<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Sep 2025 04:01:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A common zero-inflation bivariate Poisson model with comonotonic and counter-monotonic shocks</title>
      <link>https://arxiv.org/abs/2509.22798</link>
      <description>arXiv:2509.22798v1 Announce Type: new 
Abstract: There are numerous applications which involve modeling multi-dimensional count data, notably in actuarial science and risk management. When such data exhibit an excess of zeros, common count models are no longer suitable. With multivariate data, characterizing an appropriate dependence structure is equally critical in order to adequately assess the underlying risk inherent in the joint counts. In this work we propose a new bivariate zero-inflated Poisson model appropriate for modeling pairs of counts with a surplus of zeros. The proposed construction is based on a mixture model involving a common mass at zero along with a Poisson random pair. Various forms of dependence are considered for the latent Poisson pair, allowing for both negative and positive dependence. Several model properties are explored, notably the joint probability mass function and implied dependence structure. The method of moments and maximum likelihood approaches are described and implemented for estimation. The usual asymptotic properties of the estimators are derived, and their finite sample properties are explored through extensive simulations. The practical use of the proposed model is further illustrated through two real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22798v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Golshid Aflaki, Juliana Schulz, Jean-Fran\c{c}ois Plante</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Sexual Contact Networks Using Longitudinal Survey Data</title>
      <link>https://arxiv.org/abs/2509.22848</link>
      <description>arXiv:2509.22848v1 Announce Type: new 
Abstract: Characterizing sexual contact networks is essential for understanding sexually transmitted infections, but principled parameter inference for mechanistic network models remains challenging. We develop a discrete-time simulation framework that enables parameter estimation using approximate Bayesian computation. The interpretable model incorporates relationship formation, dissolution, concurrency, casual contacts, and population turnover. Applying our framework to survey data from 403 men who have sex with men in Stockholm, we provide principled uncertainty quantification for key network dynamics. Our analysis estimates the timescale for seeking a new steady relationship at 25 weeks and for relationship dissolution at 42 weeks. Casual contacts occur more frequently for single individuals (every 1.8 weeks) than for partnered individuals (every 4.5 weeks). However, while cross-sectional data constrains these parameters, migration rates remain poorly identified. We demonstrate that simple longitudinal data can resolve this issue. Tracking participant retention between survey waves directly informs migration rates, though survey dropout is a potential confounder. Furthermore, simple binary survey questions can outperform complex timeline follow-back methods for estimating contact frequencies. This framework provides a foundation for uncertainty quantification in network epidemiology and offers practical strategies to improve inference from surveys, the primary data source for studying sexual behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22848v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Till Hoffmann, Jukka-Pekka Onnela</dc:creator>
    </item>
    <item>
      <title>Global-Local Dirichlet Processes for Identifying Pan-Cancer Subpopulations Using Both Shared and Cancer-Specific Data</title>
      <link>https://arxiv.org/abs/2509.22884</link>
      <description>arXiv:2509.22884v1 Announce Type: new 
Abstract: We consider the problem of clustering grouped data for which the observations may include group-specific variables in addition to the variables that are shared across groups. This type of data is common in cancer genomics where the molecular information is usually accompanied by cancer-specific clinical information. Existing grouped clustering methods only consider the shared variables, thereby ignoring valuable information from the cancer-specific variables. To allow for these cancer-specific variables to aid in the clustering, we propose a novel Bayesian nonparametric approach, termed global-local (GLocal) Dirichlet process, that models the ``global-local'' structure of the observations across groups. We characterize the GLocal Dirichlet process using the stick-breaking representation and the representation as a limit of a finite mixture model, which leads to an efficient posterior inference algorithm. We illustrate our model with extensive simulations and a real pan-gastrointestinal cancer dataset. The cancer-specific clinical variables included carcinoembryonic antigen level, patients' body mass index, and the number of cigarettes smoked per day. These important clinical variables refine the clusters of gene expression data and allow us to identify finer sub-clusters, which is not possible in their absence. This refinement aids in the better understanding of tumor progression and heterogeneity. Moreover, our proposed method is applicable beyond the field of cancer genomics to a general grouped clustering framework in the presence of group-specific idiosyncratic variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22884v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/25-AOAS2056</arxiv:DOI>
      <arxiv:journal_reference>The Annals of Applied Statistics 19(3), 2254-2278, (September 2025)</arxiv:journal_reference>
      <dc:creator>Arhit Chakrabarti, Yang Ni, Debdeep Pati, Bani K. Mallick</dc:creator>
    </item>
    <item>
      <title>An exploration of sequential Bayesian variable selection - A comment on Garc\'{i}a-Donato et al. (2025). "Model uncertainty and missing data: An objective Bayesian perspective"</title>
      <link>https://arxiv.org/abs/2509.22901</link>
      <description>arXiv:2509.22901v1 Announce Type: new 
Abstract: Our comment on Garc\'ia-Donato et al. (2025). "Model uncertainty and missing data: An objective Bayesian perspective" explores a further extension of the proposed methodology. Specifically, we consider the sequential setting where (potentially missing) data accumulate over time, with the goal of continuously monitoring statistical evidence, as opposed to assessing it only once data collection terminates. We explore a new variable selection method based on sequential model confidence sets, as proposed by Arnold et al. (2024), and show that it can help stabilise the inference of Garc\'ia-Donato et al. (2025). To be published as "Invited discussion" in Bayesian Analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22901v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastian Arnold, Alexander Ly</dc:creator>
    </item>
    <item>
      <title>Individualized treatment regimens under correlated data with multiple outcomes</title>
      <link>https://arxiv.org/abs/2509.22905</link>
      <description>arXiv:2509.22905v1 Announce Type: new 
Abstract: Precision medicine involves developing individualized treatment regimes (ITRs) which allow for treatment decisions to be tailored to patient characteristics. Naturally, the identification of the optimal regime, that is, the rule which maximizes patient outcomes, is of interest. Several procedures for estimating optimal ITRs from observational data have been proposed; however, relatively few methods exist for estimating optimal ITRs in the presence of competing risks. Previous approaches either target one particular cause of failure, or rely on singly-robust estimators. We propose a novel doubly-robust regression-based method for estimating optimal ITRs which accounts for the uncertainty related to the unobserved cause of failure by averaging over all possible causes, or targeting the most likely cause. Our approach is straightforward to implement, and we demonstrate an extension to incorporate clustering, motivated by the question of for whom kidney transplantation with hepatitis C virus (HCV)-positive donors is safe, using data from the Organ Procurement and Transplantation Network. Our analysis suggests that a large portion of HCV-negative kidney recipients would see their overall survival unchanged if they were instead provided a kidney from an HCV-positive donor. The estimated treatment rules could be used to provide more efficient allocation of HCV-positive kidneys, increasing the donor pool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22905v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Misha Dolmatov, Erica E. M. Moodie, David A. Stephens, Dipankar Bandyopadhyay</dc:creator>
    </item>
    <item>
      <title>Structural Nested Mean Models for Modified Treatment Policies</title>
      <link>https://arxiv.org/abs/2509.22916</link>
      <description>arXiv:2509.22916v1 Announce Type: new 
Abstract: There is a growing literature on estimating effects of treatment strategies based on the natural treatment that would have been received in the absence of intervention, often dubbed `modified treatment policies' (MTPs). MTPs are sometimes of interest because they are more realistic than interventions setting exposure to an ideal level for all members of a population. In the general time-varying setting, Richardson and Robins (2013) provided exchangeability conditions for nonparametric identification of MTP effects that could be deduced from Single World Intervention Graphs (SWIGs). Diaz (2023) provided multiply robust estimators under these identification assumptions that allow for machine learning nuisance regressions. In this paper, we fill a remaining gap by extending Structural Nested Mean Models (SNMMs) to MTP settings, which enables characterization of (time-varying) heterogeneity of MTP effects. We do this both under the exchangeability assumptions of Richardson and Robins (2013) and under parallel trends assumptions, which enables investigation of (time-varying heterogeneous) MTP effects in the presence of some unobserved confounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22916v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zach Shahn</dc:creator>
    </item>
    <item>
      <title>A queuing theory-based operating capacity model for multi-modal port operations</title>
      <link>https://arxiv.org/abs/2509.22961</link>
      <description>arXiv:2509.22961v1 Announce Type: new 
Abstract: This paper investigates how "operating capacity" can be meaningfully defined in multi-modal maritime freight systems with limited data. Shipping channels and ports are complex systems that interact deeply, and the capacities of individual components may differ from the overall capacity of these systems. Traditional methods for port capacity assessment often rely on data-driven simulations, which can be difficult to calibrate due to complex interactions among port systems and the large volume of data required.
  We present a data-efficient alternative for estimating port capacities by developing a novel queuing theory-based formulation. We define the operating capacity of a port system as the maximum vessel arrival rate that can be sustained over an extended period of stable operations. Furthermore, we define terminal-level operating capacities for import and export processes in terms of the maximum achievable throughput for each process per unit time. Our approach requires only minimal data, which can be readily obtained from archival Automatic Identification System (AIS) vessel trajectories and historical port terminal logs, thereby making implementation both robust and efficient.
  We demonstrate the utility of the proposed method using data from the Port of Houston. The results suggest that the model is a viable approach for estimating port operating capacity. Specifically, the inbound operating capacity of the Port of Houston was estimated to be approximately 0.8 vessels per hour. At the Barbour's Cut Container Terminal, the operating capacities were found to be 57.8 containers per hour for import processes and 74.6 containers per hour for export processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22961v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debojjal Bagchi, Kyle Bathgate, Stephen D. Boyles, Kenneth N. Mitchell, Magdalena I. Asborno, Marin M. Kress</dc:creator>
    </item>
    <item>
      <title>Taming Variability: Randomized and Bootstrapped Conformal Risk Control for LLMs</title>
      <link>https://arxiv.org/abs/2509.23007</link>
      <description>arXiv:2509.23007v1 Announce Type: new 
Abstract: We transform the randomness of LLMs into precise assurances using an actuator at the API interface that applies a user-defined risk constraint in finite samples via Conformal Risk Control (CRC). This label-free and model-agnostic actuator manages ship/abstain/escalate actions based solely on a scalar score from opaque outputs. We enhance CRC's computational efficiency and robustness through Batched Bootstrap CRC (BB-CRC) and Randomized Batched Weighted-Average CRC (RBWA-CRC), reducing calibration calls and stabilizing thresholds while maintaining statistical validity. Additionally, we present a semantic quantification method grounded in gram matrix geometry, resulting in interpretable signal and metric design. Together these pieces deliver principled randomness control for LLM hallucination mitigation and LLM-as-judge reliability. Our framework is assessed using four datasets, demonstrating its efficacy in enhancing factual accuracy and measuring LLM-as-judge performance, yielding a simplified and computationally efficient control layer that converts variability into statistical validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23007v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lingyou Pang, Lei Huang, Jianyu Lin, Tianyu Wang, Alexander Aue, Carey E. Priebe</dc:creator>
    </item>
    <item>
      <title>Understanding How Network Geometry Influences Diffusion Processes in Complex Networks: A Focus on Cryptocurrency Blockchains and Critical Infrastructure Networks</title>
      <link>https://arxiv.org/abs/2509.23450</link>
      <description>arXiv:2509.23450v1 Announce Type: new 
Abstract: This study provides essential insights into how diffusion processes unfold in complex networks, with a focus on cryptocurrency blockchains and infrastructure networks. The structural properties of these networks, such as hub-dominated, heavy-tailed topology, network motifs, and node centrality, significantly influence diffusion speed and reach. Using epidemic diffusion models, specifically the Kertesz threshold model and the Susceptible-Infected (SI) model, we analyze key factors affecting diffusion dynamics. To assess the uncertainty in the fraction of infected nodes over time, we employ bootstrap confidence intervals, while Bayesian credible intervals are constructed to quantify parameter uncertainties in the SI models. Our findings reveal substantial variations across different network types, including Erd\H{o}s--R\'enyi networks, Geometric Random Graphs, and Delaunay Triangulation networks, emphasizing the role of network architecture in failure propagation. We identify that network motifs are crucial in diffusion. We highlight that hub-dominated networks, which dominate blockchain ecosystems, provide resilience against random failures but remain vulnerable to targeted attacks, posing significant risks to network stability. Furthermore, centrality measures such as degree, betweenness, and clustering coefficient strongly influence the transmissibility of diffusion in both blockchain and critical infrastructure networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23450v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssa/qnaf146</arxiv:DOI>
      <dc:creator>S M Mustaquim, Asim K. Dey, Abhijit Mandal</dc:creator>
    </item>
    <item>
      <title>Community Detection through Recursive Partitioning in Bayesian Framework</title>
      <link>https://arxiv.org/abs/2509.23536</link>
      <description>arXiv:2509.23536v1 Announce Type: new 
Abstract: Community detection involves grouping the nodes in the network and is one of the most-studied tasks in network science. Conventional methods usually require the specification of the number of communities $K$ in the network. This number is determined heuristically or by certain model selection criteria. In practice, different model selection criteria yield different values of $K$, leading to different results. We propose a community detection method based on recursive partitioning within the Bayesian framework. The method is compatible with a wide range of existing model-based community detection frameworks. In particular, our method does not require pre-specification of the number of communities and can capture the hierarchical structure of the network. We establish the theoretical guarantee of consistency under the stochastic block model and demonstrate the effectiveness of our method through simulations using different models that cover a broad range of scenarios. We apply our method to the California Department of Healthcare Access and Information (HCAI) data, including all Emergency Department (ED) and hospital discharges from 342 hospitals to identify regional hospital clusters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23536v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuhua Zhang, Kori S. Zachrison, Renee Y. Hsia, Jukka-Pekka Onnela</dc:creator>
    </item>
    <item>
      <title>Collaborative Indirect Treatment Comparisons with Multiple Distributed Single-arm Trials</title>
      <link>https://arxiv.org/abs/2509.23664</link>
      <description>arXiv:2509.23664v1 Announce Type: new 
Abstract: When randomized controlled trials are impractical or unethical to simultaneously compare multiple treatments, indirect treatment comparisons using single-arm trials offer valuable evidence for health technology assessments, especially for rare diseases and early-phase drug development. In practice, each sponsor conducts a single-arm trial on its own drug with restricted data-sharing and targets effects in its trial population, which can lead to unfair comparisons. This motivates methods for fair treatment comparisons across a range of target populations in distributed networks of single-arm trials sharing only aggregated data. Existing federated methods, which assume at least one site contains all treatments and allow pooling of treatment groups within the same site, cannot address this problem. We propose a novel distributed augmented calibration weighting (DAC) method to simultaneously estimate the pairwise average treatment effects (ATEs) across all trial population combinations in a distributed network of multiple single-arm trials. Using two communication rounds, DAC estimators balance covariates via calibration weighting, incorporate flexible nuisance parameter estimation, achieve doubly robust consistency, and yield results identical to pooled-data analysis. When nuisance parameters are estimated parametrically, DAC estimators are enhanced to achieve doubly robust inference with minimal squared first-order asymptotic bias. Simulations and a real-data application show good performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23664v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuru Zhu, Huiyuan Wang, Haitao Chu, Yumou Qiu, Yong Chen</dc:creator>
    </item>
    <item>
      <title>A Bayesian model-free dose-finding approach in Phase II clinical trials with the flexibility of historical borrowing</title>
      <link>https://arxiv.org/abs/2509.23777</link>
      <description>arXiv:2509.23777v1 Announce Type: new 
Abstract: Accurate dose selection in Phase II trials is critical to the success of subsequent Phase III trials, but suboptimal choices remain a leading cause of trial failure and regulatory rejection. Although MCP-Mod is widely adopted and endorsed by regulatory agencies, it requires prespecification of candidate models and is highly sensitive to model misspecification. To address these challenges, we introduce MAP-curvature, a general model-free framework for dose-response modelling that penalises the total curvature of the dose-response curve through a prior. Within this framework, LiMAP-curvature arises as the linear special case, whereas SEMAP-curvature, the focus of this work, employs the sigmoid Emax model, providing greater flexibility to capture nonlinear pharmacological patterns. Through extensive simulations, we show that SEMAP-curvature generally outperforms LiMAP-curvature and MCP-Mod in detecting the dose-response signal, estimating the dose-response curve and identifying the minimum effective dose, with particularly significant improvements under concave downward shapes resembling the sigmoid Emax model. Although SEMAP-curvature exhibits slightly greater variability, it remains robust in accuracy and reliability. We further extend MAP-curvature by integrating it with the Bayesian hierarchical model to enable flexible borrowing of historical data, which improves power and precision, particularly when dose levels overlap across studies. These results highlight MAP-curvature, and in particular SEMAP-curvature with historical borrowing, as a robust and efficient framework for dose selection in early-phase clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23777v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linxi Han, Qiqi Deng, Feng Yu, Zhangyi He</dc:creator>
    </item>
    <item>
      <title>RAPSEM: Identifying Latent Mediators Without Sequential Ignorability via a Rank-Preserving Structural Equation Model</title>
      <link>https://arxiv.org/abs/2509.23935</link>
      <description>arXiv:2509.23935v1 Announce Type: new 
Abstract: The identification of latent mediator variables is typically conducted using standard structural equation models (SEMs). When SEM is applied to mediation analysis with a causal interpretation, valid inference relies on the strong assumption of no unmeasured confounding, that is, all relevant covariates must be included in the analysis. This assumption is often violated in empirical applications, leading to biased estimates of direct and indirect effects. We address this limitation by weakening the causal assumptions and proposing a procedure that combines g-estimation with a two-stage method of moments to incorporate latent variables, thereby enabling more robust mediation analysis in settings common to the social sciences. We establish consistency and asymptotic normality of the resulting estimator. Simulation studies demonstrate that the estimator is unbiased across a wide range of settings, robust to violations of its underlying no-effect-modifier assumption, and achieves reasonable power to detect medium to large effects for sample sizes above 500, with power increasing as the strength of treatment-covariate interactions grows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23935v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sofia Morelli, Roberto Faleh, Holger Brandt</dc:creator>
    </item>
    <item>
      <title>SpeedCP: Fast Kernel-based Conditional Conformal Prediction</title>
      <link>https://arxiv.org/abs/2509.24100</link>
      <description>arXiv:2509.24100v1 Announce Type: new 
Abstract: Conformal prediction provides distribution-free prediction sets with finite-sample conditional guarantees. We build upon the RKHS-based framework of Gibbs et al. (2023), which leverages families of covariate shifts to provide approximate conditional conformal prediction intervals, an approach with strong theoretical promise, but with prohibitive computational cost. To bridge this gap, we develop a stable and efficient algorithm that computes the full solution path of the regularized RKHS conformal optimization problem, at essentially the same cost as a single kernel quantile fit. Our path-tracing framework simultaneously tunes hyperparameters, providing smoothness control and data-adaptive calibration. To extend the method to high-dimensional settings, we further integrate our approach with low-rank latent embeddings that capture conditional validity in a data-driven latent space. Empirically, our method provides reliable conditional coverage across a variety of modern black-box predictors, improving the interval length of Gibbs et al. (2023) by 30%, while achieving a 40-fold speedup.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24100v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yeo Jin Jung, Yating Liu, Zixuan Wu, So Won Jeong, Claire Donnat</dc:creator>
    </item>
    <item>
      <title>Blockwise Missingness meets AI: A Tractable Solution for Semiparametric Inference</title>
      <link>https://arxiv.org/abs/2509.24158</link>
      <description>arXiv:2509.24158v1 Announce Type: new 
Abstract: We consider parameter estimation and inference when data feature blockwise, non-monotone missingness. Our approach, rooted in semiparametric theory and inspired by prediction-powered inference, leverages off-the-shelf AI (predictive or generative) models to handle missing completely at random mechanisms, by finding an approximation of the optimal estimating equation through a novel and tractable Restricted Anova hierarchY (RAY) approximation. The resulting Inference for Blockwise Missingness(RAY), or IBM(RAY) estimator incorporates pre-trained AI models and carefully controls asymptotic variance by tuning model-specific hyperparameters. We then extend IBM(RAY) to a general class of estimators. We find the most efficient estimator in this class, which we call IBM(Adaptive), by solving a constrained quadratic programming problem. All IBM estimators are unbiased, and, crucially, asymptotically achieving guaranteed efficiency gains over a naive complete-case estimator, regardless of the predictive accuracy of the AI models used. We demonstrate the finite-sample performance and numerical stability of our method through simulation studies and an application to surface protein abundance estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24158v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qi Xu, Lorenzo Testa, Jing Lei, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Sign and signed rank tests for paired functions</title>
      <link>https://arxiv.org/abs/2509.24170</link>
      <description>arXiv:2509.24170v1 Announce Type: new 
Abstract: Simple nonparametric tests for paired functional data are an understudied area, despite recent advances in similar tests for other types of functional data. While the sign test has received limited treatment, the signed rank-type test has not previously been examined. The aim of the present work is to develop and evaluate these types of tests for functional data. We derive a simple, theoretical framework for both sign and signed rank tests for pairs of functions. In particular, we demonstrate that doubly ranked testing -- a newly developed framework for testing hypotheses involving functional data -- is a useful conduit for examining hypotheses regarding pairs o,f functions. We briefly examine the operating characteristics of all derived tests. We also use the described approaches to re-analyze pairs of functions from a randomized crossover study of heart health during simulated flight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24170v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark J. Meyer</dc:creator>
    </item>
    <item>
      <title>Equivalence Test for Mean Functions from Multi-population Functional Data</title>
      <link>https://arxiv.org/abs/2509.24242</link>
      <description>arXiv:2509.24242v1 Announce Type: new 
Abstract: Most existing methods for testing equality of means of functional data from multiple populations rely on assumptions of equal covariance and/or Gaussianity. In this work we provide a new testing method based on a statistic that is distribution-free under the null hypothesis (i.e. the statistic is pivotal), and allows different covariance structures across populations, while Gaussianity is not required. In contrast to classical methods of functional mean testing, where either observations of the full curves or projections are applied, our method allows the projection dimension to increase with the sample size to allow asymptotic recovery of full information as the sample size increases. We obtain a unified theory for the asymptotic distribution of the test statistic under local alternatives, in both the sample and bootstrap cases. The finite sample performance for both size and power have been studied via simulations and the approach has also been applied to two real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24242v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuang Xu, Andrew T. A. Wood, Yanrong Yang</dc:creator>
    </item>
    <item>
      <title>Difference-in-Differences Under Network Interference</title>
      <link>https://arxiv.org/abs/2509.24259</link>
      <description>arXiv:2509.24259v1 Announce Type: new 
Abstract: This paper develops doubly robust estimators for direct (DATT) and spillover (SATT) average treatment effects on the treated in network-based difference-in-differences (DiD) designs. Unlike standard DiD methods, the proposed approach explicitly accommodates treatment spillovers and high-dimensional network confounding arising from complex inter-unit dependencies. Identification relies on a conditional parallel-trends assumption that holds after adjusting for high-dimensional network confounders. The estimators are consistent and asymptotically normal as the network size increases, and we use graph neural networks (GNNs) to estimate nuisance functions. Simulation studies and an empirical application to U.S. county-level mask mandates and their impact on COVID-19 transmission demonstrate favorable finite-sample performance, addressing limitations of conventional DiD methods that ignore network interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24259v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kuan Sun, Zhiguo Xiao</dc:creator>
    </item>
    <item>
      <title>Robust Semiparametric Inference for Bayesian Additive Regression Trees</title>
      <link>https://arxiv.org/abs/2509.24634</link>
      <description>arXiv:2509.24634v1 Announce Type: new 
Abstract: We develop a semiparametric framework for inference on the mean response in missing-data settings using a corrected posterior distribution. Our approach is tailored to Bayesian Additive Regression Trees (BART), which is a powerful predictive method but whose nonsmoothness complicate asymptotic theory with multi-dimensional covariates. When using BART combined with Bayesian bootstrap weights, we establish a new Bernstein-von Mises theorem and show that the limit distribution generally contains a bias term. To address this, we introduce RoBART, a posterior bias-correction that robustifies BART for valid inference on the mean response. Monte Carlo studies support our theory, demonstrating reduced bias and improved coverage relative to existing procedures using BART.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24634v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Breunig, Ruixuan Liu, Zhengfei Yu</dc:creator>
    </item>
    <item>
      <title>Surjective Independence of Causal Influences for Local Bayesian Network Structures</title>
      <link>https://arxiv.org/abs/2509.24759</link>
      <description>arXiv:2509.24759v1 Announce Type: new 
Abstract: The very expressiveness of Bayesian networks can introduce fresh challenges due to the large number of relationships they often model. In many domains, it is thus often essential to supplement any available data with elicited expert judgements. This in turn leads to two key challenges: the cognitive burden of these judgements is often very high, and there are a very large number of judgements required to obtain a full probability model. We can mitigate both issues by introducing assumptions such as independence of causal influences (ICI) on the local structures throughout the network, restricting the parameter space of the model. However, the assumption of ICI is often unjustified and overly strong. In this paper, we introduce the surjective independence of causal influences (SICI) model which relaxes the ICI assumption and provides a more viable, practical alternative local structure model that facilitates efficient Bayesian network parameterisation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24759v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kieran Drury, Martine J. Barons, Jim Q. Smith</dc:creator>
    </item>
    <item>
      <title>A Greedy PDE Router for Blending Neural Operators and Classical Methods</title>
      <link>https://arxiv.org/abs/2509.24814</link>
      <description>arXiv:2509.24814v1 Announce Type: new 
Abstract: When solving PDEs, classical numerical solvers are often computationally expensive, while machine learning methods can suffer from spectral bias, failing to capture high-frequency components. Designing an optimal hybrid iterative solver--where, at each iteration, a solver is selected from an ensemble of solvers to leverage their complementary strengths--poses a challenging combinatorial problem. While the greedy selection strategy is desirable for its constant-factor approximation guarantee to the optimal solution, it requires knowledge of the true error at each step, which is generally unavailable in practice. We address this by proposing an approximate greedy router that efficiently mimics a greedy approach to solver selection. Empirical results on the Poisson and Helmholtz equations demonstrate that our method outperforms single-solver baselines and existing hybrid solver approaches, such as HINTS, achieving faster and more stable convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24814v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahana Rayan, Yash Patel, Ambuj Tewari</dc:creator>
    </item>
    <item>
      <title>A more interpretable regression model for count data with excess of zeros</title>
      <link>https://arxiv.org/abs/2509.24916</link>
      <description>arXiv:2509.24916v1 Announce Type: new 
Abstract: Count data are common in medical research. When these data have more zeros than expected by the most used count distributions, it is common to employ a zero-inflated regression model. However, the interpretability of these models is much lower than the most used count regression models. In this work, we introduce a more interpretable regression model for count data with excess of zeros based on a reparameterization of the zero-inflated Poisson distribution. We discuss inferential and diagnostic tools and perform a Monte Carlo simulation study to evaluate the performance of the maximum likelihood estimator. Finally, the usefulness of the proposed regression model is illustrated through an application on children mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24916v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gustavo H. A. Pereira, Jeremias Le\~ao, Manoel Santos-Neto, Jianwen Cai</dc:creator>
    </item>
    <item>
      <title>Unsupervised Learning in a General Semiparametric Clusterwise Index Distribution Model</title>
      <link>https://arxiv.org/abs/2509.24987</link>
      <description>arXiv:2509.24987v1 Announce Type: new 
Abstract: This study introduces a general semiparametric clusterwise index distribution model to analyze how latent clusters affect the covariate-response relationships. By employing sufficient dimension reduction to account for the effects of covariates on the cluster variable, we develop a distinct method for estimating model parameters. Building on a subjectwise representation of the underlying model, the proposed separation penalty estimation method partitions individuals and estimates cluster index coefficients. We propose a convergent algorithm for this estimation procedure and incorporate a heuristic initialization to expedite optimization. The resulting partition estimator is subsequently used to fit the cluster membership model and to construct an optimal classification rule, with both procedures iteratively updating the partition and parameter estimators. Another key contribution of our method is the development of two consistent semiparametric information criteria for selecting the number of clusters. In line with principles of classification and estimation in supervised learning, the estimated cluster structure is consistent and optimal, and the parameter estimators possess the oracle property. Comprehensive simulation studies and empirical data analyses illustrate the effectiveness of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24987v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jen-Chieh Teng, Chin-Tsang Chiang</dc:creator>
    </item>
    <item>
      <title>Efficient Difference-in-Differences Estimation when Outcomes are Missing at Random</title>
      <link>https://arxiv.org/abs/2509.25009</link>
      <description>arXiv:2509.25009v1 Announce Type: new 
Abstract: The Difference-in-Differences (DiD) method is a fundamental tool for causal inference, yet its application is often complicated by missing data. Although recent work has developed robust DiD estimators for complex settings like staggered treatment adoption, these methods typically assume complete data and fail to address the critical challenge of outcomes that are missing at random (MAR) -- a common problem that invalidates standard estimators. We develop a rigorous framework, rooted in semiparametric theory, for identifying and efficiently estimating the Average Treatment Effect on the Treated (ATT) when either pre- or post-treatment (or both) outcomes are missing at random. We first establish nonparametric identification of the ATT under two minimal sets of sufficient conditions. For each, we derive the semiparametric efficiency bound, which provides a formal benchmark for asymptotic optimality. We then propose novel estimators that are asymptotically efficient, achieving this theoretical bound. A key feature of our estimators is their multiple robustness, which ensures consistency even if some nuisance function models are misspecified. We validate the properties of our estimators and showcase their broad applicability through an extensive simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25009v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Testa, Edward H. Kennedy, Matthew Reimherr</dc:creator>
    </item>
    <item>
      <title>Strategic Play and Home Advantage: Coaches' Tactical Impact in Serie A</title>
      <link>https://arxiv.org/abs/2509.22683</link>
      <description>arXiv:2509.22683v1 Announce Type: cross 
Abstract: We analyze how coaching strategies affect goal difference and home win probabilities using hand-coded Serie A match commentary (2011/12--2013/14). Our dataset captures in-game dynamics, referee actions, and team behavior. Applying generalized linear, logit, and proportional-odds models with robust and bootstrap standard errors, we uncover stable effects across model averaging. Aggressive opening tactics consistently boost performance, while technical actions like crosses and goal-kicks show distinct patterns. Home advantage remains significant after full control. Our approach reveals the economic logic of real-time coaching, offering a novel, data-driven method to study decision-making under uncertainty in competitive environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22683v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Angelini, Massimiliano Castellani, Gery Andr\'es D\'iaz Rubio, Simone Giannerini, Greta Goracci</dc:creator>
    </item>
    <item>
      <title>Tracking the Spatiotemporal Spread of the Ohio Overdose Epidemic with Topological Data Analysis</title>
      <link>https://arxiv.org/abs/2509.22705</link>
      <description>arXiv:2509.22705v1 Announce Type: cross 
Abstract: In recent years, techniques from Topological Data Analysis (TDA) have proven effective at capturing spatial features of multidimensional data. However, applying TDA to spatiotemporal data remains relatively underexplored. In this work, we extend previous studies of disease spread by using the Mapper algorithm to analyze the Ohio drug overdose epidemic from 2007 to 2024. We introduce a novel method for constructing covers in Mapper graphs of spatiotemporal data that respects geographic structure and highlights the time-dependent variables. Finally, we generate a Mapper visualization of regional demographics to examine how these factors relate to overdose deaths. Our approach effectively reveals temporal trends, overdose hotspots, and time-lagged patterns in relation to both geography and community demographics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22705v1</guid>
      <category>stat.AP</category>
      <category>math.AT</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Bermingham, David White, Nathan Willey</dc:creator>
    </item>
    <item>
      <title>Misspecified Maximum Likelihood Estimation for Non-Uniform Group Orbit Recovery</title>
      <link>https://arxiv.org/abs/2509.22945</link>
      <description>arXiv:2509.22945v1 Announce Type: cross 
Abstract: We study maximum likelihood estimation (MLE) in the generalized group orbit recovery model, where each observation is generated by applying a random group action and a known, fixed linear operator to an unknown signal, followed by additive noise. This model is motivated by single-particle cryo-electron microscopy (cryo-EM) and can be viewed primarily as a structured continuous Gaussian mixture model. In practice, signal estimation is often performed by marginalizing over the group using a uniform distribution--an assumption that typically does not hold and renders the MLE misspecified. This raises a fundamental question: how does the misspecified MLE perform? We address this question from several angles. First, we show that in the absence of projection, the misspecified population log-likelihood has desired optimization landscape that leads to correct signal recovery. In contrast, when projections are present, the global optimizers of the misspecified likelihood deviate from the true signal, with the magnitude of the bias depending on the noise level. To address this issue, we propose a joint estimation approach tailored to the cryo-EM setting, which parameterizes the unknown distribution of the group elements and estimates both the signal and distribution parameters simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22945v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sheng Xu, Anderson Ye Zhang, Amit Singer</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Gradient Boosting Regression</title>
      <link>https://arxiv.org/abs/2509.23127</link>
      <description>arXiv:2509.23127v1 Announce Type: cross 
Abstract: Gradient boosting is widely popular due to its flexibility and predictive accuracy. However, statistical inference and uncertainty quantification for gradient boosting remain challenging and under-explored. We propose a unified framework for statistical inference in gradient boosting regression. Our framework integrates dropout or parallel training with a recently proposed regularization procedure that allows for a central limit theorem (CLT) for boosting. With these enhancements, we surprisingly find that increasing the dropout rate and the number of trees grown in parallel at each iteration substantially enhances signal recovery and overall performance. Our resulting algorithms enjoy similar CLTs, which we use to construct built-in confidence intervals, prediction intervals, and rigorous hypothesis tests for assessing variable importance. Numerical experiments demonstrate that our algorithms perform well, interpolate between regularized boosting and random forests, and confirm the validity of their built-in statistical inference procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23127v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haimo Fang, Kevin Tan, Giles Hooker</dc:creator>
    </item>
    <item>
      <title>Automatic Order, Bandwidth Selection and Flaws of Eigen Adjustment in HAC Estimation</title>
      <link>https://arxiv.org/abs/2509.23256</link>
      <description>arXiv:2509.23256v1 Announce Type: cross 
Abstract: In this paper, we propose a new heteroskedasticity and autocorrelation consistent covariance matrix estimator based on the prewhitened kernel estimator and a localized leave-one-out frequency domain cross-validation (FDCV). We adapt the cross-validated log likelihood (CVLL) function to simultaneously select the order of the prewhitening vector autoregression (VAR) and the bandwidth. The prewhitening VAR is estimated by the Burg method without eigen adjustment as we find the eigen adjustment rule of Andrews and Monahan (1992) can be triggered unnecessarily and harmfully when regressors have nonzero mean. Through Monte Carlo simulations and three empirical examples, we illustrate the flaws of eigen adjustment and the reliability of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23256v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuoxun Li, Clifford M. Hurvich</dc:creator>
    </item>
    <item>
      <title>End-to-End Deep Learning for Predicting Metric Space-Valued Outputs</title>
      <link>https://arxiv.org/abs/2509.23544</link>
      <description>arXiv:2509.23544v1 Announce Type: cross 
Abstract: Many modern applications involve predicting structured, non-Euclidean outputs such as probability distributions, networks, and symmetric positive-definite matrices. These outputs are naturally modeled as elements of general metric spaces, where classical regression techniques that rely on vector space structure no longer apply. We introduce E2M (End-to-End Metric regression), a deep learning framework for predicting metric space-valued outputs. E2M performs prediction via a weighted Fr\'echet means over training outputs, where the weights are learned by a neural network conditioned on the input. This construction provides a principled mechanism for geometry-aware prediction that avoids surrogate embeddings and restrictive parametric assumptions, while fully preserving the intrinsic geometry of the output space. We establish theoretical guarantees, including a universal approximation theorem that characterizes the expressive capacity of the model and a convergence analysis of the entropy-regularized training objective. Through extensive simulations involving probability distributions, networks, and symmetric positive-definite matrices, we show that E2M consistently achieves state-of-the-art performance, with its advantages becoming more pronounced at larger sample sizes. Applications to human mortality distributions and New York City taxi networks further demonstrate the flexibility and practical utility of the framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23544v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yidong Zhou, Su I Iao, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>Asymptotic Expansion for Nonlinear Filtering in the Small System Noise Regime</title>
      <link>https://arxiv.org/abs/2509.23920</link>
      <description>arXiv:2509.23920v1 Announce Type: cross 
Abstract: We propose a new asymptotic expansion method for nonlinear filtering, based on a small parameter in the system noise. The conditional expectation is expanded as a power series in the noise level, with each coefficient computed by solving a system of ordinary differential equations. This approach mitigates the trade-off between computational efficiency and accuracy inherent in existing methods such as Gaussian approximations and particle filters. Moreover, by incorporating an Edgeworth-type expansion, our method captures complex features of the conditional distribution, such as multimodality, with significantly lower computational cost than conventional filtering algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23920v1</guid>
      <category>eess.SP</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kurisaki</dc:creator>
    </item>
    <item>
      <title>Preference-Based Dynamic Ranking Structure Recognition</title>
      <link>https://arxiv.org/abs/2509.24493</link>
      <description>arXiv:2509.24493v1 Announce Type: cross 
Abstract: Preference-based data often appear complex and noisy but may conceal underlying homogeneous structures. This paper introduces a novel framework of ranking structure recognition for preference-based data. We first develop an approach to identify dynamic ranking groups by incorporating temporal penalties into a spectral estimation for the celebrated Bradley-Terry model. To detect structural changes, we introduce an innovative objective function and present a practicable algorithm based on dynamic programming. Theoretically, we establish the consistency of ranking group recognition by exploiting properties of a random `design matrix' induced by a reversible Markov chain. We also tailor a group inverse technique to quantify the uncertainty in item ability estimates. Additionally, we prove the consistency of structure change recognition, ensuring the robustness of the proposed framework. Experiments on both synthetic and real-world datasets demonstrate the practical utility and interpretability of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24493v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nan Lu, Jian Shi, Xin-Yu Tian</dc:creator>
    </item>
    <item>
      <title>The Limits of Inference in Complex Systems: When Stochastic Models Become Indistinguishable</title>
      <link>https://arxiv.org/abs/2509.24977</link>
      <description>arXiv:2509.24977v1 Announce Type: cross 
Abstract: Robust inference methods are essential for parameter estimation and model selection in stochastic modeling approaches, which provide interpretable frameworks for describing time series of complex phenomena. However, applying such methods is often challenging, as they typically demand either high-frequency observations or access to the model's analytical solution, resources that are rarely available in practice. Here, we address these limitations by designing a novel Monte Carlo method based on full-path statistics and bridge processes, which optimize sampling efforts and performance even under coarse sampling. We systematically investigate how experimental design -- particularly sampling frequency and dataset size -- shapes inference accuracy, revealing optimal sampling regimes and the fundamental limits of model distinguishability. We validate our approach on four datasets -- optical tweezers, human microbiome, topic mentions in social media, and forest population dynamics -- where resolution-dependent limits to inference emerge, offering fresh insight into ongoing debates about the dominant sources of noise in these systems. Overall, this work shows how combining minimal stochastic models with path-inference tools and model selection can guide the experimental design of optimized strategies in data collection and clarify the boundaries of model-based understanding in complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24977v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>physics.bio-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Javier Aguilar, Miguel A. Mu\~noz, Sandro Azaele</dc:creator>
    </item>
    <item>
      <title>Combining Experimental and Observational Data for Identification and Estimation of Long-Term Causal Effects</title>
      <link>https://arxiv.org/abs/2201.10743</link>
      <description>arXiv:2201.10743v4 Announce Type: replace 
Abstract: We study identifying and estimating the causal effect of a treatment variable on a long-term outcome using data from an observational and an experimental domain. The observational data are subject to unobserved confounding. Furthermore, subjects in the experiment are only followed for a short period; thus, long-term effects are unobserved, though short-term effects are available. Consequently, neither data source alone suffices for causal inference on the long-term outcome, necessitating a principled fusion of the two. We propose three approaches for data fusion for the purpose of identifying and estimating the causal effect. The first assumes equal confounding bias for short-term and long-term outcomes. The second weakens this assumption by leveraging an observed confounder for which the short-term and long-term potential outcomes share the same partial additive association with this confounder. The third approach employs proxy variables of the latent confounder of the treatment-outcome relationship, extending the proximal causal inference framework to the data fusion setting. For each approach, we develop influence function-based estimators and analyze their robustness properties. We illustrate our methods by estimating the effect of class size on 8th-grade SAT scores using data from the Project STAR experiment combined with observational data from the Early Childhood Longitudinal Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.10743v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>AmirEmad Ghassami, Chang Liu, Alan Yang, David Richardson, Ilya Shpitser, Eric Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Cell Type Deconvolution</title>
      <link>https://arxiv.org/abs/2202.06420</link>
      <description>arXiv:2202.06420v5 Announce Type: replace 
Abstract: Integrating heterogeneous datasets across different measurement platforms is a fundamental challenge in many scientific applications. A common example arises in deconvolution problems, such as cell type deconvolution, where one aims to estimate the composition of latent subpopulations using reference data from a different source. However, this task is complicated by systematic platform-specific scaling effects, measurement noise, and differences between data sources. For the problem of cell type deconvolution, existing methods often neglect the correlation and uncertainty in cell type proportion estimates, possibly leading to an additional concern of false positives in downstream comparisons across multiple individuals. We introduce MEAD, a statistical framework that provides both accurate estimation and valid statistical inference on the estimates. One of our key contributions is the identifiability result, which establishes the conditions under which cell type compositions are identifiable under arbitrary gene-specific scaling differences across platforms. MEAD also supports the comparison of cell type proportions across individuals after deconvolution, accounting for gene-gene correlations and biological variability. Through simulations and real-data analysis, MEAD demonstrates superior reliability for inferring cell type compositions in complex biological systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.06420v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongyue Xie, Lin Gui, Jingshu Wang</dc:creator>
    </item>
    <item>
      <title>An Economical Approach to Design with Precision Criteria</title>
      <link>https://arxiv.org/abs/2306.09476</link>
      <description>arXiv:2306.09476v3 Announce Type: replace 
Abstract: Estimation frameworks for statistical inference are preferred to hypothesis testing when quantifying uncertainty and precisely estimating effect sizes are more valuable than binary decisions about statistical significance. Study design for estimation-based investigations often uses precision criteria to select sample sizes that control the length of interval estimates with respect to a sampling distribution. In this paper, we formally define a distribution that characterizes the probability of obtaining a sufficiently narrow interval estimate as a function of the sample size. This distribution can be used to determine the smallest sample size needed to ensure an interval estimate is sufficiently narrow. We prove that this distribution is approximately normal in large-sample settings for many data generation processes. However, this approximate normality may not hold for studies with moderate sample sizes, particularly when incorporating prior information or obtaining asymmetric interval estimates. Thus, we also propose an efficient simulation-based approach to approximate the distribution for the sample size by estimating the sampling distribution of interval estimate lengths at only two sample sizes. Our methodology provides a unified framework for design with precision criteria in Bayesian and frequentist settings. We illustrate the broad applicability of this framework with several examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.09476v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luke Hagar, Nathaniel T. Stevens</dc:creator>
    </item>
    <item>
      <title>Causal inference for the expected number of recurrent events in the presence of a terminal event</title>
      <link>https://arxiv.org/abs/2306.16571</link>
      <description>arXiv:2306.16571v3 Announce Type: replace 
Abstract: While recurrent event analyses have been extensively studied, limited attention has been given to causal inference within the framework of recurrent event analysis. We develop a multiply robust estimation framework for causal inference in recurrent event data with a terminal failure event. We define our estimand as the vector comprising both the expected number of recurrent events and the failure survival function evaluated along a sequence of landmark times. We show that the estimand can be identified under a weaker condition than conditionally independent censoring and derive the associated class of influence functions under general censoring and failure distributions (i.e., without assuming absolute continuity). We propose a particular estimator within this class for further study, conduct comprehensive simulation studies to evaluate the small-sample performance of our estimator, and illustrate the proposed estimator using a large Medicare dataset to assess the causal effect of PM$_{2.5}$ on recurrent cardiovascular hospitalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16571v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin R. Baer, Trang Bui, Daniel Mork, Robert L. Strawderman, Ashkan Ertefaie</dc:creator>
    </item>
    <item>
      <title>A Double Machine Learning Approach to Combining Experimental and Observational Data</title>
      <link>https://arxiv.org/abs/2307.01449</link>
      <description>arXiv:2307.01449v3 Announce Type: replace 
Abstract: Experimental and observational studies often lack validity due to untestable assumptions. We propose a double machine learning approach to combine experimental and observational studies, allowing practitioners to test for assumption violations and estimate treatment effects consistently. Our framework proposes a falsification test for external validity and ignorability under milder assumptions. We provide consistent treatment effect estimators even when one of the assumptions is violated. However, our no-free-lunch theorem highlights the necessity of accurately identifying the violated assumption for consistent treatment effect estimation. Through comparative analyses, we show our framework's superiority over existing data fusion methods. The practical utility of our approach is further exemplified by three real-world case studies, underscoring its potential for widespread application in empirical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.01449v3</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Harsh Parikh, Marco Morucci, Vittorio Orlandi, Sudeepa Roy, Cynthia Rudin, Alexander Volfovsky</dc:creator>
    </item>
    <item>
      <title>Differentially Private Estimation and Inference in High-Dimensional Regression with FDR Control</title>
      <link>https://arxiv.org/abs/2310.16260</link>
      <description>arXiv:2310.16260v2 Announce Type: replace 
Abstract: This paper proposes new methodologies for conducting practical differentially private (DP) estimation and inference in high-dimensional linear regression. We first introduce a DP Bayesian Information Criterion (DP-BIC) for selecting the unknown sparsity parameter in differentially private sparse linear regression (DP-SLR), eliminating the need for prior knowledge of model sparsity, which is a requisite in the existing literature. Next, we develop the DP debiased algorithm that enables privacy-preserving inference on a particular subset of regression parameters. Our proposed method enables privacy-preserving inference on the regression parameters by leveraging the inherent sparsity of high-dimensional linear regression models. Additionally, we address private feature selection by considering multiple testing in high-dimensional linear regression by introducing a DP multiple testing procedure that controls the false discovery rate (FDR). This allows for accurate and privacy-preserving identification of significant predictors in the regression model. Through extensive simulations and real data analyses, we demonstrate the effectiveness of our proposed methods in conducting inference for high-dimensional linear models while safeguarding privacy and controlling the FDR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16260v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhanrui Cai, Sai Li, Xintao Xia, Linjun Zhang</dc:creator>
    </item>
    <item>
      <title>Two-Stage Nuisance Function Estimation for Causal Mediation Analysis</title>
      <link>https://arxiv.org/abs/2404.00735</link>
      <description>arXiv:2404.00735v3 Announce Type: replace 
Abstract: Tchetgen Tchetgen and Shpitser (2012) introduced an efficient, debiased, and robust influence function-based estimator for the mediation functional, which is the key component in mediation analysis. This estimator relies on the treatment, mediator, and outcome mean mechanisms. However, treating these three mechanisms as nuisance functions and fitting them as accurately as possible may not be the most effective approach. Instead, it is essential to identify the specific functionals and aspects of these mechanisms that impact the estimation of the mediation functional. In this work, we consider a specific reparametrization of the likelihood function that requires four nuisance functions. To estimate them, we propose a two-stage estimation strategy guided by the role of the nuisance functions in the bias structure of the influence function-based estimator. In particular, two of the functions are estimated using a novel nonparametric weighted balancing approach that directly targets the bias of the final mediation functional estimator. We show that the resulting estimator is consistent and asymptotically normal under certain conditions and attains multiple robustness against misspecifications of the nuisance functions. In simulations, our estimator demonstrated better stability and reduced bias and mean squared error compared to the original influence function-based estimator and a naive estimator. In an application to NHANES 2013-2014 data, our approach suggests that obesity results in a 60% higher odds of coronary heart disease, with roughly 35% higher odds attributable to the direct pathway independent of Glycohemoglobin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00735v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chang Liu, AmirEmad Ghassami</dc:creator>
    </item>
    <item>
      <title>Minimax Regret Learning for Data with Heterogeneous Subgroups</title>
      <link>https://arxiv.org/abs/2405.01709</link>
      <description>arXiv:2405.01709v2 Announce Type: replace 
Abstract: Modern complex datasets often consist of various sub-populations with known group information. In the presence of sub-population heterogeneity, it is crucial to develop robust and generalizable learning methods that (1) can enjoy robust performance on each of the training populations, and (2) is generalizable to an unseen testing population. While various min-max formulations have been proposed to achieve (1) in the robust learning literature, their generalization to an unseen testing is less explored. Moreover, a general min-max formulation can be sensitive to the noise heterogeneity, and, in the extreme case, be degenerate such that a single high-noise population dominates. The min-max-regret (MMR) can mitigate these challenges. In this work, we consider a distribution-free robust hierarchical model for the generalization from multiple training populations to an unseen testing population. Under the robust hierarchical model, the empirical MMR can enjoy the regret guarantees on each of the training populations as well as the unseen testing population. We further specialize the general MMR framework to linear regression and generalized linear model, where we characterize the geometry of MMR and its distinction from other robust methods. We demonstrate the effectiveness of MMR through extensive simulation studies and an application to image recognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01709v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weibin Mo, Weijing Tang, Songkai Xue, Yufeng Liu, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>YEAST: Yet Another Sequential Test</title>
      <link>https://arxiv.org/abs/2406.16523</link>
      <description>arXiv:2406.16523v4 Announce Type: replace 
Abstract: Online evaluation of machine learning models is typically conducted through A/B experiments. Sequential statistical tests are valuable tools for analysing these experiments, as they enable researchers to stop data collection early without increasing the risk of false discoveries. However, existing sequential tests either limit the number of interim analyses or suffer from low statistical power. In this paper, we introduce a novel sequential test designed for continuous monitoring of A/B experiments. We validate our method using semi-synthetic simulations and demonstrate that it outperforms current state-of-the-art sequential testing approaches. Our method is derived using a new technique that inverts a bound on the probability of threshold crossing, based on a classical maximal inequality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16523v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexey Kurennoy, Majed Dodin, Tural Gurbanov, Ana Peleteiro Ramallo</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference for CP Tensor Factor Models</title>
      <link>https://arxiv.org/abs/2406.17278</link>
      <description>arXiv:2406.17278v2 Announce Type: replace 
Abstract: High-dimensional tensor-valued data have recently gained attention from researchers in economics and finance. We consider the estimation and inference of high-dimensional tensor factor models, where each dimension of the tensor diverges. Our focus is on a factor model that admits CP-type tensor decomposition, which allows for non-orthogonal loading vectors. Based on the contemporary covariance matrix, we propose an iterative simultaneous projection estimation method. Our estimator is robust to weak dependence among factors and weak correlation across different dimensions in the idiosyncratic shocks. We establish an inferential theory, demonstrating both consistency and asymptotic normality under relaxed assumptions. Within a unified framework, we consider two eigenvalue ratio-based estimators for the number of factors in a tensor factor model and justify their consistency. Simulation studies confirm the theoretical results and an empirical application to sorted portfolios reveals three important factors: a market factor, a long-short factor, and a volatility factor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17278v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bin Chen, Yuefeng Han, Qiyang Yu</dc:creator>
    </item>
    <item>
      <title>Evaluating Treatment Benefit Predictors using Observational Data: Contending with Identification and Confounding Bias</title>
      <link>https://arxiv.org/abs/2407.05585</link>
      <description>arXiv:2407.05585v4 Announce Type: replace 
Abstract: A treatment benefit predictor (TBP) is a function that maps patient characteristics to an estimate of the treatment benefit for that patient. Such predictors support optimizing individualized treatment decisions, which are central to precision medicine. However, evaluating the predictive performance of a TBP is challenging, as this often must be conducted in a sample where treatment assignment is not random. After briefly reviewing several metrics for evaluating TBPs, we show conceptually how to evaluate a pre-specified TBP using observational data from the target population, for a binary treatment decision at a single time point. We exemplify with a particular measure of discrimination (the concentration of benefit index) and a particular measure of calibration (the moderate calibration curve). The population-level definitions of these metrics involve the latent treatment benefit variable, but we show identification by re-expressing the respective estimands in terms of the distribution of observable data only. We also show that in the absence of full confounding control, bias propagates in a more complex manner than when targeting more commonly encountered estimands. We find the patterns of biases are often unpredictable, and general intuition about the direction of bias in causal effect estimates does not hold in the present context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05585v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Xia, Mohsen Sadatsafavi, Paul Gustafson</dc:creator>
    </item>
    <item>
      <title>Smoothed pseudo-population bootstrap methods with applications to finite population quantiles</title>
      <link>https://arxiv.org/abs/2410.07996</link>
      <description>arXiv:2410.07996v2 Announce Type: replace 
Abstract: This paper introduces smoothed pseudo-population bootstrap methods for the purposes of variance estimation and the construction of confidence intervals for finite population quantiles. In an i.i.d. context, it has been shown that resampling from a smoothed estimate of the distribution function instead of the usual empirical distribution function can improve the convergence rate of the bootstrap variance estimator of a sample quantile. We extend the smoothed bootstrap to the survey sampling framework by implementing it in pseudo-population bootstrap methods for high entropy, single-stage survey designs, such as simple random sampling without replacement and Poisson sampling. Given a kernel function and a bandwidth, it consists of smoothing the pseudo-population from which bootstrap samples are drawn using the original sampling design. Given that the implementation of the proposed algorithms requires the specification of the bandwidth, we develop a plug-in selection method along with a grid search selection method based on a bootstrap estimate of the mean squared error. Simulation results suggest a gain in efficiency associated with the smoothed approach as compared to the standard pseudo-population bootstrap for estimating the variance of a quantile estimator together with mixed results regarding confidence interval coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07996v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Vanessa McNealis, Christian L\'eger</dc:creator>
    </item>
    <item>
      <title>Causal Data Fusion for Panel Data without a Pre-Intervention Period</title>
      <link>https://arxiv.org/abs/2410.16391</link>
      <description>arXiv:2410.16391v3 Announce Type: replace 
Abstract: Traditional panel-data causal inference frameworks, such as difference-in-differences and synthetic control methods, rely on pre-intervention data to estimate counterfactual means. However, such data may be unavailable in real-world settings when interventions are implemented in response to sudden events, such as public health crises or epidemiological shocks. In this paper, we introduce two data-fusion methods for causal inference from panel data in scenarios where pre-intervention data are unavailable. These methods leverage auxiliary reference domains with related panel data to estimate causal effects in the target domain, thereby overcoming the limitations imposed by the absence of pre-intervention data. We demonstrate the efficacy of these methods by deriving bounds on the absolute bias that converge to zero under suitable conditions, as well as through simulations across a variety of panel-data settings. Our proposed methodology renders causal inference feasible in urgent and data-constrained environments where the assumptions of existing causal inference frameworks are not met. As an application of our methodology, we evaluate the effect of a community organization vaccination intervention in Chelsea, Massachusetts on COVID-19 vaccination rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16391v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zou Yang, Seung Hee Lee, Julia R. K\"ohler, AmirEmad Ghassami</dc:creator>
    </item>
    <item>
      <title>On a risk model with tree-structured Poisson Markov random field frequency, with application to rainfall events</title>
      <link>https://arxiv.org/abs/2412.00607</link>
      <description>arXiv:2412.00607v2 Announce Type: replace 
Abstract: In many insurance contexts, dependence between risks of a portfolio may arise from their frequencies. We investigate a dependent risk model in which we assume the vector of count variables to be a tree-structured Markov random field with Poisson marginals. The tree structure translates into a wide variety of dependence schemes. We study the global risk of the portfolio and the risk allocation to all its constituents. We provide asymptotic results for portfolios defined on infinitely growing trees. To illustrate its flexibility and computational scalability to higher dimensions, we calibrate the risk model on real-world extreme rainfall data and perform a risk analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00607v2</guid>
      <category>stat.ME</category>
      <category>q-fin.RM</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H\'el\`ene Cossette, Benjamin C\^ot\'e, Alexandre Dubeau, Etienne Marceau</dc:creator>
    </item>
    <item>
      <title>Testing the Homogeneity of Proportions for Correlated Bilateral Data via the Clayton Copula</title>
      <link>https://arxiv.org/abs/2502.00523</link>
      <description>arXiv:2502.00523v2 Announce Type: replace 
Abstract: Handling highly dependent data is crucial in clinical trials, particularly in fields related to ophthalmology. Incorrectly specifying the dependency structure can lead to biased inferences. Traditionally, models rely on three fixed dependence structures, which lack flexibility and interpretation. In this article, we propose a framework using a more general model -- copulas -- to better account for dependency. We assess the performance of three different test statistics within the Clayton copula setting to demonstrate the framework's feasibility. Simulation results indicate that this method controls type I error rates and achieves reasonable power, providing a solid benchmark for future research and broader applications. Additionally, we present analyses of two real-world datasets as case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00523v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuyi Liang, Takeshi Emura, Chang-Xing Ma, Yijing Xin, Xin-Wei Huang</dc:creator>
    </item>
    <item>
      <title>Stability and performance guarantees for misspecified multivariate score-driven filters</title>
      <link>https://arxiv.org/abs/2502.05021</link>
      <description>arXiv:2502.05021v5 Announce Type: replace 
Abstract: Can stochastic gradient methods track a moving target? We address the problem of tracking multivariate time-varying parameters under noisy observations and potential model misspecification. Specifically, we examine implicit and explicit score-driven (ISD and ESD) filters, which update parameter predictions using the gradient of the logarithmic postulated observation density (commonly referred to as the score). For both filter types, we derive novel sufficient conditions that ensure the exponential stability of the filtered parameter path and the existence of a finite mean squared error (MSE) bound relative to the pseudo-true parameter path. Our (non-)asymptotic MSE bounds rely on mild moment conditions on the data-generating process, while our stability results are agnostic about the true process. For the ISD filter, concavity of the postulated log density combined with simple parameter restrictions is sufficient to guarantee stability. In contrast, the ESD filter additionally requires the score to be Lipschitz continuous and the learning rate to be sufficiently small. We validate our theoretical findings through simulation studies, showing that ISD filters outperform ESD filters in terms of accuracy and stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05021v5</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Donker van Heel, Rutger-Jan Lange, Bram van Os, Dick van Dijk</dc:creator>
    </item>
    <item>
      <title>False Discovery estimation in Record Linkage</title>
      <link>https://arxiv.org/abs/2503.20627</link>
      <description>arXiv:2503.20627v2 Announce Type: replace 
Abstract: Integrating data from multiple electronic sources allows researchers to expand studies without the cost of new data collection. However, such data are often collected for administrative or operational purposes rather than with specific future research questions in mind and, due to privacy constraints, unique identifiers are unavailable. This lack of direct identifiers requires the use of Record Linkage (RL) algorithms, which rely on partially identifying variables to probabilistically determine whether records belong to the same entity. Since these variables lack the strength to perfectly combine information, RL procedures typically yield an imperfect set of linked records. Therefore, assessing the false discovery rate (FDR) of RL is crucial for ensuring the reliability of subsequent analyses. In this paper, we introduce a novel method for estimating the FDR in RL by linking records from real and synthesised data. As synthetic records should never form links with real observations, they provide a means to estimate the FDR across different procedural settings. Notably, this method is applicable to all RL techniques. By identifying the FDR in RL results and selecting suitable model parameters, our approach enables to assess and improve the reliability of linked data. We evaluate the performance of our procedure using established RL algorithms and benchmark data sets before applying it to link siblings from the Netherlands Perinatal Registry, where the reliability of previous RL applications has never been confirmed. Through this application, we highlight the importance of accounting for linkage errors when studying mother-child dynamics in healthcare records.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20627v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/sim.70292</arxiv:DOI>
      <dc:creator>Kayan\'e Robach, Michel H. Hof, Mark A. van de Wiel</dc:creator>
    </item>
    <item>
      <title>A Nonparametric Bayesian Local-Global Model for Enhanced Adverse Event Signal Detection in Spontaneous Reporting System Data</title>
      <link>https://arxiv.org/abs/2504.10881</link>
      <description>arXiv:2504.10881v2 Announce Type: replace 
Abstract: Spontaneous reporting system databases are key resources for post-marketing surveillance, providing real-world evidence (RWE) on the adverse events (AEs) of regulated drugs or other medical products. Various statistical methods have been proposed for AE signal detection in these databases, flagging drug-specific AEs with disproportionately high observed counts compared to expected counts under independence. However, signal detection remains challenging for rare AEs or newer drugs, which receive small observed and expected counts and thus suffer from reduced statistical power. Principled information sharing on signal strengths across drugs/AEs is crucial in such cases to enhance signal detection. However, existing methods typically ignore complex between-drug associations on AE signal strengths, limiting their ability to detect signals. We propose novel local-global mixture Dirichlet process (DP) prior-based nonparametric Bayesian models to capture these associations, enabling principled information sharing between drugs while balancing flexibility and shrinkage for each drug, thereby enhancing statistical power. We develop efficient Markov chain Monte Carlo algorithms for implementation and employ a false discovery rate (FDR)-controlled, false negative rate (FNR)-optimized hypothesis testing framework for AE signal detection. Extensive simulations demonstrate our methods' superior sensitivity -- often surpassing existing approaches by a twofold or greater margin -- while strictly controlling the FDR. An application to FDA FAERS data on statin drugs further highlights our methods' effectiveness in real-world AE signal detection. Software implementing our methods is provided as supplementary material.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10881v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xin-Wei Huang, Saptarshi Chakraborty</dc:creator>
    </item>
    <item>
      <title>Over the Stability Space of a Multivariate Time Series</title>
      <link>https://arxiv.org/abs/2506.22407</link>
      <description>arXiv:2506.22407v3 Announce Type: replace 
Abstract: This paper jointly addresses the challenges of non-stationarity and high dimensionality in analysing multivariate time series. Building on the classical concept of cointegration, we introduce a more flexible notion, called stability space, aimed at capturing stationary components in settings where traditional assumptions may not hold. Based on the dimensionality reduction techniques of Partial Least Squares and Principal Component Analysis, we proposed two non-parametric procedures for estimating such a space and a targeted selection of components that prioritise stationarity. We compare these alternatives with the parametric Johansen procedure when possible. Through simulations and real-data applications, we evaluated the performance of these methodologies across various scenarios, including high-dimensional configurations where regularisation techniques are explored, considering a sparse version of the Principal Component Analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22407v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto V\'asquez-Mart\'inez, Graciela Gonz\'alez-Far\'ias, Jos\'e Ulises M\'arquez Urbina, Francisco Corona</dc:creator>
    </item>
    <item>
      <title>Power new generalized class of Kavya-Manoharan distributions with an application to exponential distribution</title>
      <link>https://arxiv.org/abs/2508.18930</link>
      <description>arXiv:2508.18930v2 Announce Type: replace 
Abstract: Recently, Verma et al. (2025) introduced a novel generalized class of Kavya-Manoharan distributions, which have demonstrated significant utility in reliability analysis and the modeling of lifetime data. This paper proposes an extension of this class by applying the power generalization technique, thereby enhancing more flexibility and applicability. We take the exponential distribution as the baseline distribution to introduce a new model capable of accommodating both monotonic and non-monotonic hazard rate functions. Our model includes eleven submodels. We present several statistical properties of the introduced model, including moments, generating and characteristic functions, mean deviations, quantile function, mean residual life function, R\'enyi entropy, order statistics, and reliability. To estimate the unknown model parameters, we use the maximum likelihood approach. A simulation study is conducted to assess the validity of the maximum likelihood estimator. The superiority of the new distribution is demonstrated through the use of a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18930v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lazhar Benkhelifa</dc:creator>
    </item>
    <item>
      <title>On the Use of Weighting for Personalized and Transparent Evidence Synthesis</title>
      <link>https://arxiv.org/abs/2509.00228</link>
      <description>arXiv:2509.00228v2 Announce Type: replace 
Abstract: Over the past few decades, statistical methods for causal inference have made impressive strides, enabling progress across a range of scientific fields. However, much of this methodological development has been confined to individual studies, limiting its ability to draw more generalizable conclusions. Achieving a thorough understanding of cause and effect typically relies on the integration, reconciliation, and synthesis from diverse study designs and multiple data sources. Furthermore, it is crucial to direct this synthesis effort toward understanding the effect of treatments for specific patient populations. To address these challenges, we present a weighting framework for evidence synthesis that handles both individual- and aggregate-level data, encompassing and extending conventional regression-based meta-analysis methods. We use this approach to tailor meta-analyses, targeting the covariate profiles of patients in a target population in a sample-bounded manner, thereby enhancing their personalization and robustness. We propose a technique to detect studies that meaningfully deviate from the target population, suggesting when it might be prudent to exclude them from the analysis. We establish multiple consistency conditions and demonstrate asymptotic normality for the proposed estimator. We demonstrate the effectiveness of the method through a simulation study and two real-world case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00228v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenqi Shi, Jos\'e R. Zubizarreta</dc:creator>
    </item>
    <item>
      <title>Simplicial clustering using the $\alpha$--transformation</title>
      <link>https://arxiv.org/abs/2509.05945</link>
      <description>arXiv:2509.05945v3 Announce Type: replace 
Abstract: We introduce two simplicial clustering approaches for compositional data, that are adaptations of the $K$--means and of the Gaussian mixture models algorithms, by employing the $\alpha$--transformation. By utilizing clustering validation indices we can decide on the number of clusters and choose the value of $\alpha$ for the $K$--means, while for the model-based clustering approach information criteria complete this task. extensive simulation studies compare the performance of these two approaches and a real data set illustrates their performance in real world settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05945v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Tsagris, Nikolaos Kontemeniotis</dc:creator>
    </item>
    <item>
      <title>Functional Regression with Nonstationarity and Error Contamination: Application to the Economic Impact of Climate Change</title>
      <link>https://arxiv.org/abs/2509.08591</link>
      <description>arXiv:2509.08591v2 Announce Type: replace 
Abstract: This paper studies a regression model with functional dependent and explanatory variables, both of which exhibit nonstationary dynamics. The model assumes that the nonstationary stochastic trends of the dependent variable are explained by those of the explanatory variables, and hence that there exists a stable long-run relationship between the two variables despite their nonstationary behavior. We also assume that the functional observations may be error-contaminated. We develop novel autocovariance-based estimation and inference methods for this model. The methodology is broadly applicable to economic and statistical functional time series with nonstationary dynamics. To illustrate our methodology and its usefulness, we apply it to evaluating the global economic impact of climate change, an issue of intrinsic importance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08591v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyungsik Nam, Won-Ki Seo</dc:creator>
    </item>
    <item>
      <title>Modified Lepage-type test statistics for the weak null hypothesis</title>
      <link>https://arxiv.org/abs/2509.19126</link>
      <description>arXiv:2509.19126v2 Announce Type: replace 
Abstract: Detecting simultaneous shifts in the location and scale of two populations is a challenging problem in statistical research. A common way to address this issue is by combining location and scale test statistics. A well-known example is the Lepage test, which combines the Wilcoxon-Mann-Whitney test for location with the Ansari-Bradley test for scale. However, the Wilcoxon-Mann-Whitney test assumes that the population variances are equal, while the Ansari-Bradley test assumes the population medians are equal. This study introduces new approaches that combine recent methodological advances to relax these assumptions. We incorporate the Fligner-Policello test, a distribution-free alternative to the Wilcoxon-Mann-Whitney test that does not require the assumption of equal variances. The Fligner-Policello test is further enhanced by the Fong-Huang method, which provides an improved variance estimation. Additionally, we propose a new variance estimator for the Ansari-Bradley test, thereby eliminating the need for the equal medians assumption. These methodological modifications are integrated into the Lepage framework to operate under a weak null hypothesis. Simulation results suggest that these new tests are promising candidates for location-scale testing. The practical utility of the proposed tests is then demonstrated through an analysis of four real-world biomedical datasets. These empirical applications confirm the robustness and reliability of the modified tests for the two-sample independent location-scale problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19126v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abid Hussain, Michail Tsagris</dc:creator>
    </item>
    <item>
      <title>Bayesian Autoregressive Online Change-Point Detection with Time-Varying Parameters</title>
      <link>https://arxiv.org/abs/2407.16376</link>
      <description>arXiv:2407.16376v2 Announce Type: replace-cross 
Abstract: Change points in real-world systems mark significant regime shifts in system dynamics, possibly triggered by exogenous or endogenous factors. These points define regimes for the time evolution of the system and are crucial for understanding transitions in financial, economic, social, environmental, and technological contexts. Building upon the Bayesian approach introduced in \cite{c:07}, we devise a new method for online change point detection in the mean of a univariate time series, which is well suited for real-time applications and is able to handle the general temporal patterns displayed by data in many empirical contexts. We first describe time series as an autoregressive process of an arbitrary order. Second, the variance and correlation of the data are allowed to vary within each regime driven by a scoring rule that updates the value of the parameters for a better fit of the observations. Finally, a change point is detected in a probabilistic framework via the posterior distribution of the current regime length. By modeling temporal dependencies and time-varying parameters, the proposed approach enhances both the estimate accuracy and the forecasting power. Empirical validations using various datasets demonstrate the method's effectiveness in capturing memory and dynamic patterns, offering deeper insights into the non-stationary dynamics of real-world systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16376v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioanna-Yvonni Tsaknaki, Fabrizio Lillo, Piero Mazzarisi</dc:creator>
    </item>
    <item>
      <title>Optimal and Provable Calibration in High-Dimensional Binary Classification: Angular Calibration and Platt Scaling</title>
      <link>https://arxiv.org/abs/2502.15131</link>
      <description>arXiv:2502.15131v2 Announce Type: replace-cross 
Abstract: We study the fundamental problem of calibrating a linear binary classifier of the form $\sigma(\hat{w}^\top x)$, where the feature vector $x$ is Gaussian, $\sigma$ is a link function, and $\hat{w}$ is an estimator of the true linear weight $w^\star$. By interpolating with a noninformative $\textit{chance classifier}$, we construct a well-calibrated predictor whose interpolation weight depends on the angle $\angle(\hat{w}, w_\star)$ between the estimator $\hat{w}$ and the true linear weight $w_\star$. We establish that this angular calibration approach is provably well-calibrated in a high-dimensional regime where the number of samples and features both diverge, at a comparable rate. The angle $\angle(\hat{w}, w_\star)$ can be consistently estimated. Furthermore, the resulting predictor is uniquely $\textit{Bregman-optimal}$, minimizing the Bregman divergence to the true label distribution within a suitable class of calibrated predictors. Our work is the first to provide a calibration strategy that satisfies both calibration and optimality properties provably in high dimensions. Additionally, we identify conditions under which a classical Platt-scaling predictor converges to our Bregman-optimal calibrated solution. Thus, Platt-scaling also inherits these desirable properties provably in high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15131v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yufan Li, Pragya Sur</dc:creator>
    </item>
    <item>
      <title>Joint Value Estimation and Bidding in Repeated First-Price Auctions</title>
      <link>https://arxiv.org/abs/2502.17292</link>
      <description>arXiv:2502.17292v2 Announce Type: replace-cross 
Abstract: We study regret minimization in repeated first-price auctions (FPAs), where a bidder observes only the realized outcome after each auction -- win or loss. This setup reflects practical scenarios in online display advertising where the actual value of an impression depends on the difference between two potential outcomes, such as clicks or conversion rates, when the auction is won versus lost. We analyze three outcome models: (1) adversarial outcomes without features, (2) linear potential outcomes with features, and (3) linear treatment effects in features. For each setting, we propose algorithms that jointly estimate private values and optimize bidding strategies, achieving near-optimal regret bounds. Notably, our framework enjoys a unique feature that the treatments are also actively chosen, and hence eliminates the need for the overlap condition commonly required in causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17292v2</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxiao Wen, Yanjun Han, Zhengyuan Zhou</dc:creator>
    </item>
    <item>
      <title>CLEAR: Calibrated Learning for Epistemic and Aleatoric Risk</title>
      <link>https://arxiv.org/abs/2507.08150</link>
      <description>arXiv:2507.08150v2 Announce Type: replace-cross 
Abstract: Existing methods typically address either aleatoric uncertainty due to measurement noise or epistemic uncertainty resulting from limited data, but not both in a balanced manner. We propose CLEAR, a calibration method with two distinct parameters, $\gamma_1$ and $\gamma_2$, to combine the two uncertainty components and improve the conditional coverage of predictive intervals for regression tasks. CLEAR is compatible with any pair of aleatoric and epistemic estimators; we show how it can be used with (i) quantile regression for aleatoric uncertainty and (ii) ensembles drawn from the Predictability-Computability-Stability (PCS) framework for epistemic uncertainty. Across 17 diverse real-world datasets, CLEAR achieves an average improvement of 28.2% and 17.4% in the interval width compared to the two individually calibrated baselines while maintaining nominal coverage. Similar improvements are observed when applying CLEAR to Deep Ensembles (epistemic) and Simultaneous Quantile Regression (aleatoric). The benefits are especially evident in scenarios dominated by high aleatoric or epistemic uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08150v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilia Azizi, Juraj Bodik, Jakob Heiss, Bin Yu</dc:creator>
    </item>
    <item>
      <title>Bayesian Predictive Inference Beyond Martingales</title>
      <link>https://arxiv.org/abs/2507.21874</link>
      <description>arXiv:2507.21874v2 Announce Type: replace-cross 
Abstract: There is a growing interest in the so-called Bayesian Predictive Inference approach, which allows to perform Bayesian inference without specifying the likelihood and prior of the model, or the need of any MCMC. Instead, only a sequence of predictive distributions for the observations is required, and inference on the unknown estimand can be performed, cheaply in parallel, using bootstrap-type schemes. Understanding which classes of predictive distributions can be used within this framework, is still a key open question. We relax commonly used probabilistic assumptions on the observations, namely exchangeability and conditional identical distribution, and on their predictive distributions, being measure-valued martingales, by introducing the new class of Almost Conditional Identically Distributed (a.c.i.d.) random variables. This class assumes that the predictive distributions are measure-valued almost supermartingales, and is parametrized by a sequence of parameters $(\xi_n)_{n&gt;0}$, which regulate the decay of conditional dependence among future observations. Under mild summability assumptions on $(\xi_n)_{n&gt;0}$, the resulting sequence of observations is shown to be asymptotically exchangeable, hence amenable to Bayesian Predictive Inference techniques. A.c.i.d. random variables arise naturally in recursive algorithms, and include classic approaches in Statistics and Learning Theory, such as kernel estimators, and more novel ones, such as the parametric Bayesian bootstraps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21874v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Battiston, Lorenzo Cappello</dc:creator>
    </item>
    <item>
      <title>Cohort-Anchored Robust Inference for Event-Study with Staggered Adoption</title>
      <link>https://arxiv.org/abs/2509.01829</link>
      <description>arXiv:2509.01829v2 Announce Type: replace-cross 
Abstract: This paper proposes a cohort-anchored framework for robust inference in event studies with staggered adoption, building on Rambachan and Roth (2023). Robust inference based on event-study coefficients aggregated across cohorts can be misleading due to the dynamic composition of treated cohorts, especially when pre-trends differ across cohorts. My approach avoids this problem by operating at the cohort-period level. To address the additional challenge posed by time-varying control groups in modern DiD estimators, I introduce the concept of block bias: the parallel-trends violation for a cohort relative to its fixed initial control group. I show that the biases of these estimators can be decomposed invertibly into block biases. Because block biases maintain a consistent comparison across pre- and post-treatment periods, researchers can impose transparent restrictions on them to conduct robust inference. In simulations and a reanalysis of minimum-wage effects on teen employment, my framework yields better-centered (and sometimes narrower) confidence sets than the aggregated approach when pre-trends vary across cohorts. The framework is most useful in settings with multiple cohorts, sufficient within-cohort precision, and substantial cross-cohort heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01829v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyi Liu</dc:creator>
    </item>
    <item>
      <title>A note on the relation between one-step, outcome regression and IPW-type estimators of parameters with the mixed bias property</title>
      <link>https://arxiv.org/abs/2509.22452</link>
      <description>arXiv:2509.22452v2 Announce Type: replace-cross 
Abstract: Bruns-Smith et al. (2025) established an algebraic identity between the one-step estimator and a specific outcome regression-type estimator for a class of parameters that forms a strict subset of the class introduced in Chernozhukov et al. (2022), assuming both nuisance functions are estimated as linear combinations of given features. They conjectured that this identity extends to the broader mixed bias class introduced in Rotnitzky et al. (2021). In this note, we prove their conjecture and further extend the result to allow one of the nuisance estimators to be non-linear. We also relate these findings to the work of Robins et al. (2007), who established other identities linking one-step estimators to outcome regression-type and IPW-type estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22452v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 30 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Rotnitzky, Ezequiel Smucler, James M. Robins</dc:creator>
    </item>
  </channel>
</rss>
