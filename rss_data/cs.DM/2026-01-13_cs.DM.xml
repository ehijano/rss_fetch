<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DM</link>
    <description>cs.DM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Jan 2026 05:00:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Improved lower bounds for the maximum size of Condorcet domains</title>
      <link>https://arxiv.org/abs/2601.07336</link>
      <description>arXiv:2601.07336v1 Announce Type: new 
Abstract: Condorcet domains are sets of linear orders with the property that, whenever voters' preferences are restricted to the domain, the pairwise majority relation (for an odd number of voters) is transitive and hence a linear order. Determining the maximum size of a Condorcet domain, sometimes under additional constraints, has been a longstanding problem in the mathematical theory of majority voting. The exact maximum is only known for $n\leq 8$ alternatives.
  In this paper we use a structural analysis of the largest domains for small $n$ to design a new inductive search method. Using an implementation of this method on a supercomputer, together with existing algorithms, we improve the size of the largest known domains for all $9 \leq n \leq 20$. These domains are then used in a separate construction to obtain the currently largest known domains for $21 \leq n \leq 25$, and to improve the best asymptotic lower bound for the maximum size of a Condorcet domain to $\Omega(2.198139^n)$. Finally, we discuss properties of the domains found and state several open problems and conjectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07336v1</guid>
      <category>cs.DM</category>
      <category>cs.GT</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Karpov, Klas Markstrom, Soren Riis, Bei Zhou</dc:creator>
    </item>
    <item>
      <title>On the complexity of the Maker-Breaker happy vertex game</title>
      <link>https://arxiv.org/abs/2601.07673</link>
      <description>arXiv:2601.07673v1 Announce Type: new 
Abstract: Given a c-colored graph G, a vertex of G is happy if it has the same color as all its neighbors. The notion of happy vertices was introduced by Zhang and Li to compute the homophily of a graph. Eto, et al. introduced the Maker-Maker version of the Happy vertex game, where two players compete to claim more happy vertices than their opponent. We introduce here the Maker-Breaker happy vertex game: two players, Maker and Breaker, alternately color the vertices of a graph with their respective colors. Maker aims to maximize the number of happy vertices at the end, while Breaker aims to prevent her. This game is also a scoring version of the Maker-Breaker Domination game introduced by Duchene, et al. as a happy vertex corresponds exactly to a vertex that is not dominated in the domination game. Therefore, this game is a very natural game on graphs and can be studied within the scope of scoring positional games. We initiate here the complexity study of this game, by proving that computing its score is PSPACE-complete on trees, NP-hard on caterpillars, and polynomial on subdivided stars. Finally, we provide the exact value of the score on graphs of maximum degree 2, and we provide an FPT-algorithm to compute the score on graphs of bounded neighborhood diversity. An important contribution of the paper is that, to achieve our hardness results, we introduce a new type of incidence graph called the literal-clause incidence graph for 2-SAT formulas. We prove that QMAX 2-SAT remains PSPACE-complete even if this graph is acyclic, and that MAX 2-SAT remains NP-complete, even if this graph is acyclic and has maximum degree 2, i.e. is a union of paths. We demonstrate the importance of this contribution by proving that Incidence, the scoring positional game played on a graph is also PSPACE-complete when restricted to forests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07673v1</guid>
      <category>cs.DM</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mathieu Hilaire, Perig Montfort, Nacim Oijid</dc:creator>
    </item>
    <item>
      <title>A Fast and Effective Method for Euclidean Anticlustering: The Assignment-Based-Anticlustering Algorithm</title>
      <link>https://arxiv.org/abs/2601.06351</link>
      <description>arXiv:2601.06351v1 Announce Type: cross 
Abstract: The anticlustering problem is to partition a set of objects into K equal-sized anticlusters such that the sum of distances within anticlusters is maximized. The anticlustering problem is NP-hard. We focus on anticlustering in Euclidean spaces, where the input data is tabular and each object is represented as a D-dimensional feature vector. Distances are measured as squared Euclidean distances between the respective vectors. Applications of Euclidean anticlustering include social studies, particularly in psychology, K-fold cross-validation in which each fold should be a good representative of the entire dataset, the creation of mini-batches for gradient descent in neural network training, and balanced K-cut partitioning. In particular, machine-learning applications involve million-scale datasets and very large values of K, making scalable anticlustering algorithms essential. Existing algorithms are either exact methods that can solve only small instances or heuristic methods, among which the most scalable is the exchange-based heuristic fast_anticlustering. We propose a new algorithm, the Assignment-Based Anticlustering algorithm (ABA), which scales to very large instances. A computational study shows that ABA outperforms fast_anticlustering in both solution quality and running time. Moreover, ABA scales to instances with millions of objects and hundreds of thousands of anticlusters within short running times, beyond what fast_anticlustering can handle. As a balanced K-cut partitioning method for tabular data, ABA is superior to the well-known METIS method in both solution quality and running time. The code of the ABA algorithm is available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06351v1</guid>
      <category>cs.LG</category>
      <category>cs.DM</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Baumann, Olivier Goldschmidt, Dorit S. Hochbaum, Jason Yang</dc:creator>
    </item>
    <item>
      <title>Approximate FKG inequalities for phase-bound spin systems</title>
      <link>https://arxiv.org/abs/2601.07169</link>
      <description>arXiv:2601.07169v1 Announce Type: cross 
Abstract: The FKG inequality is an invaluable tool in monotone spin systems satisfying the FKG lattice condition, which provides positive correlations for all coordinate-wise increasing functions of spins. However, the FKG lattice condition is somewhat brittle and is not preserved when confining a spin system to a particular phase. For instance, consider the Curie-Weiss model, which is a model of a ferromagnet with two phases at low temperature corresponding to positive and negative overall magnetization. It is not a priori clear if each phase internally has positive correlations for increasing functions, or if the positive correlations in the model arise primarily from the global choice of positive or negative magnetization.
  In this article, we show that the individual phases do indeed satisfy an approximate form of the FKG inequality in a class of generalized higher-order Curie-Weiss models (including the standard Curie-Weiss model as a special case), as well as in ferromagnetic exponential random graph models (ERGMs). To cover both of these settings, we present a general result which allows for the derivation of such approximate FKG inequalities in a straightforward manner from inputs related to metastable mixing; we expect that this general result will be widely applicable. In addition, we derive some consequences of the approximate FKG inequality, including a version of a useful covariance inequality originally due to Newman as well as Bulinski and Shabanovich. We use this to extend the proof of the central limit theorem for ERGMs within a phase at low temperatures, due to the second author, to the non-forest phase-coexistence regime, answering a question posed by Bianchi, Collet, and Magnanini for the edge-triangle model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07169v1</guid>
      <category>math.PR</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satyaki Mukherjee, Vilas Winstein</dc:creator>
    </item>
    <item>
      <title>The Secretary Problem with Predictions and a Chosen Order</title>
      <link>https://arxiv.org/abs/2601.07482</link>
      <description>arXiv:2601.07482v1 Announce Type: cross 
Abstract: We study a learning-augmented variant of the secretary problem, recently introduced by Fujii and Yoshida (2023), in which the decision-maker has access to machine-learned predictions of candidate values. The central challenge is to balance consistency and robustness: when predictions are accurate, the algorithm should select a near-optimal secretary, while under inaccurate predictions it should still guarantee a bounded competitive ratio.
  We consider both the classical Random Order Secretary Problem (ROSP), where candidates arrive in a uniformly random order, and a more natural learning-augmented model in which the decision-maker may choose the arrival order based on predicted values. We call this model the Chosen Order Secretary Problem (COSP), capturing scenarios such as interview schedules set in advance.
  We propose a new randomized algorithm applicable to both ROSP and COSP. Our method switches from fully trusting predictions to a threshold-based rule once a large prediction deviation is detected. Let $\epsilon \in [0,1]$ denote the maximum multiplicative prediction error. For ROSP, our algorithm achieves a competitive ratio of $\max\{0.221, (1-\epsilon)/(1+\epsilon)\}$, improving upon the prior bound of $\max\{0.215, (1-\epsilon)/(1+\epsilon)\}$. For COSP, we achieve $\max\{0.262, (1-\epsilon)/(1+\epsilon)\}$, surpassing the $0.25$ worst-case bound for prior approaches and moving closer to the classical secretary benchmark of $1/e \approx 0.368$. These results highlight the benefit of combining predictions with arrival-order control in online decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07482v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Helia Karisani, Mohammadreza Daneshvaramoli, Hedyeh Beyhaghi, Mohammad Hajiesmaili, Cameron Musco</dc:creator>
    </item>
    <item>
      <title>Certificates in P and Subquadratic-Time Computation of Radius, Diameter, and all Eccentricities in Graphs</title>
      <link>https://arxiv.org/abs/1803.04660</link>
      <description>arXiv:1803.04660v5 Announce Type: replace 
Abstract: In the context of fine-grained complexity, we investigate the notion of certificate enabling faster polynomial-time algorithms. We specifically target radius (minimum eccentricity), diameter (maximum eccentricity), and all-eccentricity computations for which quadratic-time lower bounds are known under plausible conjectures. In each case, we introduce a notion of certificate as a specific set of nodes from which appropriate bounds on all eccentricities can be derived in subquadratic time when this set has sublinear size. The existence of small certificates is a barrier against SETH-based lower bounds for these problems. We indeed prove that for graph classes with small certificates, there exist randomized subquadratic-time algorithms for computing the radius, the diameter, and all eccentricities respectively. Moreover, these notions of certificates are tightly related to algorithms probing the graph through one-to-all distance queries and allow to explain the efficiency of practical radius and diameter algorithms from the literature. Our formalization enables a novel primal-dual analysis of a classical approach for diameter computation that leads to algorithms for radius, diameter and all eccentricities with theoretical guarantees with respect to certain graph parameters. This is complemented by experimental results on various types of real-world graphs showing that these parameters appear to be low in practice. Finally, we obtain refined results for several graph classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:1803.04660v5</guid>
      <category>cs.DM</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.NI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 2025 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), Jan 2025, New Orleans (LA), United States. pp.2157--2193</arxiv:journal_reference>
      <dc:creator>Feodor F. Dragan (UniBuc, ICI), Guillaume Ducoffe (UniBuc, ICI), Michel Habib (IRIF), Laurent Viennot (DI-ENS, ARGO)</dc:creator>
    </item>
    <item>
      <title>Deterministic approximate counting of colorings with fewer than $2\Delta$ colors via absence of zeros</title>
      <link>https://arxiv.org/abs/2408.04727</link>
      <description>arXiv:2408.04727v3 Announce Type: replace-cross 
Abstract: Let $\Delta,q\geq 3$ be integers. We prove that there exists $\eta\geq 0.002$ such that if $q\geq (2-\eta)\Delta$, then there exists an open set $\mathcal{U}\subset \mathbb{C}$ that contains the interval $[0,1]$ such that for each $w\in \mathcal{U}$ and any graph $G=(V,E)$ of maximum degree at most $\Delta$, the partition function of the anti-ferromagnetic $q$-state Potts model evaluated at $w$ does not vanish. This provides a (modest) improvement on a result of Liu, Sinclair, and Srivastava, and breaks the $q=2\Delta$-barrier for this problem.
  As a direct consequence we obtain via Barvinok's interpolation method a deterministic polynomial time algorithm to approximate the number of proper $q$-colorings of graphs of maximum degree at most $\Delta$, provided $q\geq (2-\eta)\Delta$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04727v3</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.46298/theoretics.26.1</arxiv:DOI>
      <arxiv:journal_reference>TheoretiCS, Volume 5 (2026), Article 1, 1-41</arxiv:journal_reference>
      <dc:creator>Ferenc Bencs, Khallil Berrekkal, Guus Regts</dc:creator>
    </item>
    <item>
      <title>When is local search both effective and efficient?</title>
      <link>https://arxiv.org/abs/2410.02634</link>
      <description>arXiv:2410.02634v3 Announce Type: replace-cross 
Abstract: Combinatorial optimization problems implicitly define fitness landscapes that combine the numeric structure of the 'fitness' function to be maximized with the combinatorial structure of which assignments are 'adjacent'. Local search starts at an assignment in this landscape and successively moves assignments until no further improvement is possible among the adjacent assignments. Classic analyses of local search algorithms have focused more on the question of effectiveness ("did we find a good solution?") and often implicitly assumed that there are no doubts about their efficiency ("did we find it quickly?"). But there are many reasons to doubt the efficiency of local search. Even if we focus on fitness landscapes on the hypercube that are single peaked on every subcube (i.e., semismooth fitness landscapes) where effectiveness is obvious, many local search algorithms are known to be inefficient. Since fitness landscapes are unwieldy exponentially large objects, we focus on their polynomial-sized representations by instances of valued constraint satisfaction problems (VCSP). We define a "direction" for valued constraints such that directed VCSPs generate semismooth fitness landscapes. We call VCSPs oriented if they do not have any pair of variables with arcs in both directions. Since recognizing if a VCSP-instance is directed or oriented is coNP-complete, we generalized oriented VCSPs as conditionally-smooth fitness landscapes that are recognizable in polynomial time for a VCSP-instance. We prove that many popular local search algorithms like random ascent, simulated annealing, history-based rules, jumping rules, and the Kernighan-Lin heuristic are very efficient on conditionally-smooth landscapes. But conditionally-smooth landscapes are still expressive enough so that algorithms like steepest ascent and random facet require a super-polynomial number of steps to find the fitness peak.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02634v3</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>q-bio.PE</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Artem Kaznatcheev, Sofia Vazquez Alferez</dc:creator>
    </item>
    <item>
      <title>Disproving two conjectures on the Hamiltonicity of Venn diagrams</title>
      <link>https://arxiv.org/abs/2503.18554</link>
      <description>arXiv:2503.18554v4 Announce Type: replace-cross 
Abstract: In 1984, Winkler conjectured that every simple Venn diagram with $n$ curves can be extended to a simple Venn diagram with $n+1$ curves. His conjecture is equivalent to the statement that the dual graph of any simple Venn diagram has a Hamilton cycle. In this work, we construct counterexamples to Winkler's conjecture for all $n\geq 6$. As part of this proof, we computed all 3.430.404 simple Venn diagrams with $n=6$ curves (even their number was not previously known), among which we found 72 counterexamples. We also construct monotone Venn diagrams, i.e., diagrams that can be drawn with $n$ convex curves, and are not extendable, for all $n\geq 7$. Furthermore, we also disprove another conjecture about the Hamiltonicity of the (primal) graph of a Venn diagram. Specifically, while working on Winkler's conjecture, Pruesse and Ruskey proved that this graph has a Hamilton cycle for every simple Venn diagram with $n$ curves, and conjectured that this also holds for non-simple diagrams. We construct counterexamples to this conjecture for all $n\geq 4$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18554v4</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sofia Brenner, Linda Kleist, Torsten M\"utze, Christian Rieck, Francesco Verciani</dc:creator>
    </item>
    <item>
      <title>Efficient Online Random Sampling via Randomness Recycling</title>
      <link>https://arxiv.org/abs/2505.18879</link>
      <description>arXiv:2505.18879v3 Announce Type: replace-cross 
Abstract: This article studies the fundamental problem of using i.i.d. coin tosses from an entropy source to efficiently generate random variables $X_i \sim P_i$ $(i \ge 1)$, where $(P_1, P_2, \dots)$ is a random sequence of rational discrete probability distributions subject to an \textit{arbitrary} stochastic process. Our method achieves an amortized expected entropy cost within $\varepsilon &gt; 0$ bits of the information-theoretically optimal Shannon lower bound using $O(\log(1/\varepsilon))$ space. This result holds both pointwise in terms of the Shannon information content conditioned on $X_i$ and $P_i$, and in expectation to obtain a rate of $\mathbb{E}[H(P_1) + \dots + H(P_n)]/n + \varepsilon$ bits per sample as $n \to \infty$ (where $H$ is the Shannon entropy). The combination of space, time, and entropy properties of our method improves upon the Knuth and Yao (1976) entropy-optimal algorithm and Han and Hoshi (1997) interval algorithm for online sampling, which require unbounded space. It also uses exponentially less space than the more specialized methods of Kozen and Soloviev (2022) and Shao and Wang (2025) that generate i.i.d. samples from a fixed distribution. Our online sampling algorithm rests on a powerful algorithmic technique called \textit{randomness recycling}, which reuses a fraction of the random information consumed by a probabilistic algorithm to reduce its amortized entropy cost.
  On the practical side, we develop randomness recycling techniques to accelerate a variety of prominent sampling algorithms. We show that randomness recycling enables state-of-the-art runtime performance on the Fisher-Yates shuffle when using a cryptographically secure pseudorandom number generator, and that it reduces the entropy cost of discrete Gaussian sampling. Accompanying the manuscript is a performant software library in the C programming language.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18879v3</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1137/1.9781611978971.89</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 2026 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 2473-2511. Society for Industrial and Applied Mathematics, 2026</arxiv:journal_reference>
      <dc:creator>Thomas L. Draper, Feras A. Saad</dc:creator>
    </item>
    <item>
      <title>Covering Complete Geometric Graphs by Monotone Paths</title>
      <link>https://arxiv.org/abs/2507.10840</link>
      <description>arXiv:2507.10840v2 Announce Type: replace-cross 
Abstract: Given a set $A$ of $n$ points (vertices) in general position in the plane, the \emph{complete geometric graph} $K_n[A]$ consists of all $\binom{n}{2}$ segments (edges) between the elements of $A$. It is known that the edge set of every complete geometric graph on $n$ vertices can be partitioned into $O(n^{3/2})$ crossing-free paths (or matchings). We strengthen this result under various additional assumptions on the point set. In particular, we prove that for a set $A$ of $n$ \emph{randomly} selected points, uniformly distributed in $[0,1]^2$, with probability tending to $1$ as $n\rightarrow\infty$, the edge set of $K_n[A]$ can be covered by $O(n\log n)$ crossing-free paths and by $O(n\sqrt{\log n})$ crossing-free matchings. On the other hand, we construct $n$-element point sets such that covering the edge set of $K_n[A]$ requires a quadratic number of monotone paths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10840v2</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrian Dumitrescu, J\'anos Pach, Morteza Saghafian, Alex Scott</dc:creator>
    </item>
    <item>
      <title>GDBA Revisited: Unleashing the Power of Guided Local Search for Distributed Constraint Optimization</title>
      <link>https://arxiv.org/abs/2508.06899</link>
      <description>arXiv:2508.06899v2 Announce Type: replace-cross 
Abstract: Local search is an important class of incomplete algorithms for solving Distributed Constraint Optimization Problems (DCOPs) but it often converges to poor local optima. While Generalized Distributed Breakout Algorithm (GDBA) provides a comprehensive rule set to escape premature convergence, its empirical benefits remain marginal on general-valued problems. In this work, we systematically examine GDBA and identify three factors that potentially lead to its inferior performance, i.e., over-aggressive constraint violation conditions, unbounded penalty accumulation, and uncoordinated penalty updates. To address these issues, we propose Distributed Guided Local Search (DGLS), a novel GLS framework for DCOPs that incorporates an adaptive violation condition to selectively penalize constraints with high cost, a penalty evaporation mechanism to control the magnitude of penalization, and a synchronization scheme for coordinated penalty updates. We theoretically show that the penalty values are bounded, and agents play a potential game in DGLS. Extensive empirical results on various benchmarks demonstrate the great superiority of DGLS over state-of-the-art baselines. Compared to Damped Max-sum with high damping factors, our DGLS achieves competitive performance on general-valued problems, and outperforms by significant margins on structured problems in terms of anytime results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06899v2</guid>
      <category>cs.AI</category>
      <category>cs.DM</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanchen Deng, Xinrun Wang, Bo An</dc:creator>
    </item>
    <item>
      <title>Empirical Quantum Advantage in Constrained Optimization from Encoded Unitary Designs</title>
      <link>https://arxiv.org/abs/2511.14296</link>
      <description>arXiv:2511.14296v2 Announce Type: replace-cross 
Abstract: We introduce the Constraint-Enhanced Quantum Approximate Optimization Algorithm (CE-QAOA), a shallow, constraint-aware ansatz that operates inside the one-hot product space [n]^m, where m is the number of blocks and each block is initialized in an n-qubit W_n state. We give an ancilla-free, depth-optimal encoder that prepares W_n using n-1 two-qubit rotations per block, and a two-local block-XY mixer that preserves the one-hot manifold and has a constant spectral gap on the one-excitation sector. At the level of expressivity, we establish per-block controllability, implying approximate universality per block. At the level of distributional behavior, we show that, after natural block and symbol permutation twirls, shallow CE-QAOA realizes an encoded unitary 1-design and supports approximate second-moment (2-design) behavior; combined with a Paley-Zygmund argument, this yields finite-shot anticoncentration guarantees.
  Algorithmically, we wrap constant-depth sampling with a deterministic feasibility checker to obtain a polynomial-time hybrid quantum-classical solver (PHQC) that returns the best observed feasible solution in O(S n^2) time, where S is a polynomial shot budget. We obtain two advantages. First, when CE-QAOA fixes r &gt;= 1 locations different from the start city, we achieve a Theta(n^r) reduction in shot complexity even against a classical sampler that draws uniformly from the feasible set. Second, against a classical baseline restricted to raw bitstring sampling, we show an exp(Theta(n^2)) minimax separation. In noiseless circuit simulations of traveling salesman problem instances with n in {4,...,10} locations from the QOPTLib benchmark library, we recover the global optimum at depth p = 1 using polynomial shot budgets and coarse parameter grids defined by the problem size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14296v2</guid>
      <category>cs.ET</category>
      <category>cs.DM</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>physics.app-ph</category>
      <category>quant-ph</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chinonso Onah, Roman Firt, Kristel Michielsen</dc:creator>
    </item>
  </channel>
</rss>
