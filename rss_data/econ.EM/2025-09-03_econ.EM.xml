<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Sep 2025 01:39:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bootstrap Diagnostic Tests</title>
      <link>https://arxiv.org/abs/2509.01351</link>
      <description>arXiv:2509.01351v1 Announce Type: new 
Abstract: Violation of the assumptions underlying classical (Gaussian) limit theory frequently leads to unreliable statistical inference. This paper shows the novel result that the bootstrap can detect such violation by means of simple and powerful tests which (a) induce no pre-testing bias, (b) can be performed using the same critical values in a broad range of applications, and (c) are consistent against deviations from asymptotic normality. By focusing on the discrepancy between the conditional distribution of a bootstrap statistic and the (limiting) Gaussian distribution which obtains under valid specification, we show how to assess whether this discrepancy is large enough to indicate specification invalidity. The method, which is computationally straightforward, only requires to measure the discrepancy between the bootstrap and the Gaussian distributions based on a sample of i.i.d. draws of the bootstrap statistic. We derive sufficient conditions for the randomness in the data to mix with the randomness in the bootstrap repetitions in a way such that (a), (b) and (c) above hold. To demonstrate the practical relevance and broad applicability of our diagnostic procedure, we discuss five scenarios where the asymptotic Gaussian approximation may fail: (i) weak instruments in instrumental variable regression; (ii) non-stationarity in autoregressive time series; (iii) parameters near or at the boundary of the parameter space; (iv) infinite variance innovations in a location model for i.i.d. data; (v) invalidity of the delta method due to (near-)rank deficiency in the implied Jacobian matrix. An illustration drawn from the empirical macroeconomic literature concludes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01351v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Cavaliere, Luca Fanelli, Iliyan Georgiev</dc:creator>
    </item>
    <item>
      <title>Handling Sparse Non-negative Data in Finance</title>
      <link>https://arxiv.org/abs/2509.01478</link>
      <description>arXiv:2509.01478v1 Announce Type: new 
Abstract: We show that Poisson regression, though often recommended over log-linear regression for modeling count and other non-negative variables in finance and economics, can be far from optimal when heteroskedasticity and sparsity -- two common features of such data -- are both present. We propose a general class of moment estimators, encompassing Poisson regression, that balances the bias-variance trade-off under these conditions. A simple cross-validation procedure selects the optimal estimator. Numerical simulations and applications to corporate finance data reveal that the best choice varies substantially across settings and often departs from Poisson regression, underscoring the need for a more flexible estimation framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01478v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Agostino Capponi, Zhaonan Qu</dc:creator>
    </item>
    <item>
      <title>Using Aggregate Relational Data to Infer Social Networks</title>
      <link>https://arxiv.org/abs/2509.01503</link>
      <description>arXiv:2509.01503v2 Announce Type: new 
Abstract: This study introduces a novel approach for inferring social network structures using Aggregate Relational Data (ARD), addressing the challenge of limited detailed network data availability. By integrating ARD with variational approximation methods, we provide a computationally efficient and cost-effective solution for network analysis. Our methodology demonstrates the potential of ARD to offer insightful approximations of network dynamics, as evidenced by Monte Carlo Simulations. This paper not only showcases the utility of ARD in social network inference but also opens avenues for future research in enhancing estimation precision and exploring diverse network datasets. Through this work, we contribute to the field of network analysis by offering an alternative strategy for understanding complex social networks with constrained data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01503v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xunkang Tian</dc:creator>
    </item>
    <item>
      <title>On the Estimation of Multinomial Logit and Nested Logit Models: A Conic Optimization Approach</title>
      <link>https://arxiv.org/abs/2509.01562</link>
      <description>arXiv:2509.01562v1 Announce Type: new 
Abstract: In this paper, we revisit parameter estimation for multinomial logit (MNL), nested logit (NL), and tree-nested logit (TNL) models through the framework of convex conic optimization. Traditional approaches typically solve the maximum likelihood estimation (MLE) problem using gradient-based methods, which are sensitive to step-size selection and initialization, and may therefore suffer from slow or unstable convergence. In contrast, we propose a novel estimation strategy that reformulates these models as conic optimization problems, enabling more robust and reliable estimation procedures. Specifically, we show that the MLE for MNL admits an equivalent exponential cone program (ECP). For NL and TNL, we prove that when the dissimilarity (scale) parameters are fixed, the estimation problem is convex and likewise reducible to an ECP. Leveraging these results, we design a two-stage procedure: an outer loop that updates the scale parameters and an inner loop that solves the ECP to update the utility coefficients. The inner problems are handled by interior-point methods with iteration counts that grow only logarithmically in the target accuracy, as implemented in off-the-shelf solvers (e.g., MOSEK). Extensive experiments across estimation instances of varying size show that our conic approach attains better MLE solutions, greater robustness to initialization, and substantial speedups compared to standard gradient-based MLE, particularly on large-scale instances with high-dimensional specifications and large choice sets. Our findings establish exponential cone programming as a practical and scalable alternative for estimating a broad class of discrete choice models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01562v1</guid>
      <category>econ.EM</category>
      <category>math.OC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hoang Giang Pham, Tien Mai, Minh Ha Hoang</dc:creator>
    </item>
    <item>
      <title>Constrained Recursive Logit for Route Choice Analysis</title>
      <link>https://arxiv.org/abs/2509.01595</link>
      <description>arXiv:2509.01595v1 Announce Type: new 
Abstract: The recursive logit (RL) model has become a widely used framework for route choice modeling, but it suffers from a key limitation: it assigns nonzero probabilities to all paths in the network, including those that are unrealistic, such as routes exceeding travel time deadlines or violating energy constraints. To address this gap, we propose a novel Constrained Recursive Logit (CRL) model that explicitly incorporates feasibility constraints into the RL framework. CRL retains the main advantages of RL-no path sampling and ease of prediction-but systematically excludes infeasible paths from the universal choice set. The model is inherently non-Markovian; to address this, we develop a tractable estimation approach based on extending the state space, which restores the Markov property and enables estimation using standard value iteration methods. We prove that our estimation method admits a unique solution under positive discrete costs and establish its equivalence to a multinomial logit model defined over restricted universal path choice sets. Empirical experiments on synthetic and real networks demonstrate that CRL improves behavioral realism and estimation stability, particularly in cyclic networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01595v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hung Tran, Tien Mai, Minh Ha Hoang</dc:creator>
    </item>
    <item>
      <title>Cohort-Anchored Robust Inference for Event-Study with Staggered Adoption</title>
      <link>https://arxiv.org/abs/2509.01829</link>
      <description>arXiv:2509.01829v1 Announce Type: new 
Abstract: I propose a cohort-anchored framework for robust inference in event studies with staggered adoption. Robust inference based on aggregated event-study coefficients, as in Rambachan and Roth (2023), can be misleading because pre- and post-treatment coefficients are identified from different cohort compositions and the not-yet-treated control group changes over time. To address these issues, I work at the cohort-period level and introduce the \textit{block bias}-the parallel-trends violation for a cohort relative to its anchored initial control group-whose interpretation is consistent across pre- and post-treatment periods. For both the imputation estimator and the estimator in Callaway and Sant'Anna (2021) that uses not-yet-treated units as controls, I show an invertible decomposition linking these estimators' biases in post-treatment periods to block biases. This allows researchers to place transparent restrictions on block biases (e.g., Relative Magnitudes and Second Differences) and conduct robust inference using the algorithm from Rambachan and Roth (2023). In simulations, when parallel-trends violations differ across cohorts, my framework yields better-centered (and sometimes narrower) confidence sets than the aggregated approach. In a reanalysis of the effect of minimum-wage changes on teen employment in the Callaway and Sant'Anna (2021) application, my inference framework with the Second Differences restriction yields confidence sets centered well below zero, indicating robust negative effects, whereas inference based on aggregated coefficients yields sets centered near zero. The proposed framework is most useful when there are several cohorts, adequate within-cohort precision, and substantial cross-cohort heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01829v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyi Liu</dc:creator>
    </item>
    <item>
      <title>On the role of the design phase in a linear regression</title>
      <link>https://arxiv.org/abs/2509.01861</link>
      <description>arXiv:2509.01861v1 Announce Type: new 
Abstract: The "design phase" refers to a stage in observational studies, during which a researcher constructs a subsample that achieves a better balance in covariate distributions between the treated and untreated units. In this paper, we study the role of this preliminary phase in the context of linear regression, offering a justification for its utility. To that end, we first formalize the design phase as a process of estimand adjustment via selecting a subsample. Then, we show that covariate balance of a subsample is indeed a justifiable criterion for guiding the selection: it informs on the maximum degree of model misspecification that can be allowed for a subsample, when a researcher wishes to restrict the bias of the estimand for the parameter of interest within a target level of precision. In this sense, the pursuit of a balanced subsample in the design phase is interpreted as identifying an estimand that is less susceptible to bias in the presence of model misspecification. Also, we demonstrate that covariate imbalance can serve as a sensitivity measure in regression analysis, and illustrate how it can structure a communication between a researcher and the readers of her report.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01861v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junho Choi</dc:creator>
    </item>
    <item>
      <title>Interpretational errors with instrumental variables</title>
      <link>https://arxiv.org/abs/2509.02045</link>
      <description>arXiv:2509.02045v1 Announce Type: new 
Abstract: Instrumental variables (IV) are often used to identify causal effects in observational settings and experiments subject to non-compliance. Under canonical assumptions, IVs allow us to identify a so-called local average treatment effect (LATE). The use of IVs is often accompanied by a pragmatic decision to abandon the identification of the causal parameter that corresponds to the original research question and target the LATE instead. This pragmatic decision presents a potential source of error: an investigator mistakenly interprets findings as if they had made inference on their original causal parameter of interest. We conducted a systematic review and meta-analysis of patterns of pragmatism and interpretational errors in the applied IV literature published in leading journals of economics, political science, epidemiology, and clinical medicine (n = 309 unique studies). We found that a large fraction of studies targeted the LATE, although specific interest in this parameter was rare. Of these studies, 61% contained claims that mistakenly suggested that another parameter was targeted -- one whose value likely differs, and could even have the opposite sign, from the parameter actually estimated. Our findings suggest that the validity of conclusions drawn from IV applications is often compromised by interpretational errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02045v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luca Locher, Mats J. Stensrud, Aaron L. Sarvet</dc:creator>
    </item>
    <item>
      <title>Bayesian Shrinkage in High-Dimensional VAR Models: A Comparative Study</title>
      <link>https://arxiv.org/abs/2504.05489</link>
      <description>arXiv:2504.05489v2 Announce Type: cross 
Abstract: High-dimensional vector autoregressive (VAR) models offer a versatile framework for multivariate time series analysis, yet face critical challenges from over-parameterization and uncertain lag order. In this paper, we systematically compare three Bayesian shrinkage priors (horseshoe, lasso, and normal) and two frequentist regularization approaches (ridge and nonparametric shrinkage) under three carefully crafted simulation scenarios. These scenarios encompass (i) overfitting in a low-dimensional setting, (ii) sparse high-dimensional processes, and (iii) a combined scenario where both large dimension and overfitting complicate inference.
  We evaluate each method in quality of parameter estimation (root mean squared error, coverage, and interval length) and out-of-sample forecasting (one-step-ahead forecast RMSE). Our findings show that local-global Bayesian methods, particularly the horseshoe, dominate in maintaining accurate coverage and minimizing parameter error, even when the model is heavily over-parameterized. Frequentist ridge often yields competitive point forecasts but underestimates uncertainty, leading to sub-nominal coverage. A real-data application using macroeconomic variables from Canada illustrates how these methods perform in practice, reinforcing the advantages of local-global priors in stabilizing inference when dimension or lag order is inflated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05489v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5539/ijsp.v14n3p1</arxiv:DOI>
      <dc:creator>Harrison Katz, Robert E. Weiss</dc:creator>
    </item>
    <item>
      <title>Finite-Sample Non-Parametric Bounds with an Application to the Causal Effect of Workforce Gender Diversity on Firm Performance</title>
      <link>https://arxiv.org/abs/2509.01622</link>
      <description>arXiv:2509.01622v1 Announce Type: cross 
Abstract: Classical Manski bounds identify average treatment effects under minimal assumptions but, in finite samples, assume that latent conditional expectations are bounded by the sample's own extrema or that the population extrema are known a priori -- often untrue in firm-level data with heavy tails. We develop a finite-sample, concentration-driven band (concATE) that replaces that assumption with a Dvoretzky--Kiefer--Wolfowitz tail bound, combines it with delta-method variance, and allocates size via Bonferroni. The band extends to a group-sequential design that controls the family-wise error when the first ``significant'' diversity threshold is data-chosen. Applied to 945 listed firms (2015 Q2--2022 Q1), concATE shows that senior-level gender diversity raises Tobin's Q once representation exceeds approximately 30\% in growth sectors and approximately 65\% in cyclical sectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01622v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grace Lordan, Kaveh Salehzadeh Nobari</dc:creator>
    </item>
    <item>
      <title>Bias Correction in Factor-Augmented Regression Models with Weak Factors</title>
      <link>https://arxiv.org/abs/2509.02066</link>
      <description>arXiv:2509.02066v1 Announce Type: cross 
Abstract: In this paper, we study the asymptotic bias of the factor-augmented regression estimator and its reduction, which is augmented by the $r$ factors extracted from a large number of $N$ variables with $T$ observations. In particular, we consider general weak latent factor models with $r$ signal eigenvalues that may diverge at different rates, $N^{\alpha _{k}}$, $0&lt;\alpha _{k}\leq 1$, $k=1,\dots,r$. In the existing literature, the bias has been derived using an approximation for the estimated factors with a specific data-dependent rotation matrix $\hat{H}$ for the model with $\alpha_{k}=1$ for all $k$, whereas we derive the bias for weak factor models. In addition, we derive the bias using the approximation with a different rotation matrix $\hat{H}_q$, which generally has a smaller bias than with $\hat{H}$. We also derive the bias using our preferred approximation with a purely signal-dependent rotation $H$, which is unique and can be regarded as the population version of $\hat{H}$ and $\hat{H}_q$. Since this bias is parametrically inestimable, we propose a split-panel jackknife bias correction, and theory shows that it successfully reduces the bias. The extensive finite-sample experiments suggest that the proposed bias correction works very well, and the empirical application illustrates its usefulness in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02066v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peiyun Jiang, Yoshimasa Uematsu, Takashi Yamagata</dc:creator>
    </item>
    <item>
      <title>Normalizations and misspecification in skill formation models</title>
      <link>https://arxiv.org/abs/2104.00473</link>
      <description>arXiv:2104.00473v4 Announce Type: replace 
Abstract: An important class of structural models studies the determinants of skill formation and the optimal timing of interventions. In this paper, I provide new identification results for these models and investigate the effects of seemingly innocuous scale and location restrictions on parameters of interest. To do so, I first characterize the identified set of all parameters without these additional restrictions and show that important policy-relevant parameters are point identified under weaker assumptions than commonly used in the literature. The implications of imposing standard scale and location restrictions depend on how the model is specified, but they generally impact the interpretation of parameters and may affect counterfactuals. Importantly, with the popular CES production function, commonly used scale restrictions fix identified parameters and lead to misspecification. Consequently, simply changing the units of measurements of observed variables might yield ineffective investment strategies and misleading policy recommendations. I show how existing estimators can easily be adapted to solve these issues. As a byproduct, this paper also presents a general and formal definition of when restrictions are truly normalizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.00473v4</guid>
      <category>econ.EM</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joachim Freyberger</dc:creator>
    </item>
    <item>
      <title>Causal Effects in Matching Mechanisms with Strategically Reported Preferences</title>
      <link>https://arxiv.org/abs/2307.14282</link>
      <description>arXiv:2307.14282v3 Announce Type: replace 
Abstract: A growing number of central authorities use assignment mechanisms to allocate students to schools in a way that reflects student preferences and school priorities. However, most real-world mechanisms incentivize students to strategically misreport their preferences. Misreporting complicates the identification of causal parameters that depend on true preferences, which are necessary inputs for a broad class of counterfactual analyses. In this paper, we provide an identification approach that is robust to strategic misreporting and derive sharp bounds on causal effects of school assignment on future outcomes. Our approach applies to any mechanism as long as there exist placement scores and cutoffs that characterize that mechanism's allocation rule. We use data from a deferred acceptance mechanism that assigns students to more than 1,000 university--major combinations in Chile. Matching theory predicts and empirical evidence suggests that students behave strategically in Chile because they face constraints on their submission of preferences and have good a priori information on the schools they will have access to. Our bounds are informative enough to reveal significant heterogeneity in graduation success with respect to preferences and school assignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.14282v3</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marinho Bertanha, Margaux Luflade, Ismael Mourifi\'e</dc:creator>
    </item>
    <item>
      <title>Specification testing with grouped fixed effects</title>
      <link>https://arxiv.org/abs/2310.01950</link>
      <description>arXiv:2310.01950v3 Announce Type: replace 
Abstract: We propose a Hausman test for the correct specification of unobserved heterogeneity in both linear and nonlinear fixed-effects panel data models. The null hypothesis is that heterogeneity is either time-invariant or, symmetrically, described by homogeneous time effects. We contrast the standard one-way fixed-effects estimator with the recently developed two-way grouped fixed-effects estimator, that is consistent in the presence of time-varying heterogeneity (or heterogeneous time effects) under minimal specification and distributional assumptions for the unobserved effects. The Hausman test compares jackknife corrected estimators, removing the leading term of the incidental parameters and approximation biases, and exploits bootstrap to obtain the variance of the vector of contrasts. We provide Monte Carlo evidence on the size and power properties of the test and illustrate its application in two empirical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01950v3</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claudia Pigini, Alessandro Pionati, Francesco Valentini</dc:creator>
    </item>
    <item>
      <title>Return and Volatility Forecasting Using On-Chain Flows in Cryptocurrency Markets</title>
      <link>https://arxiv.org/abs/2411.06327</link>
      <description>arXiv:2411.06327v2 Announce Type: replace 
Abstract: We empirically examine the intraday return- and volatility-forecasting power of on-chain flow data for Bitcoin(BTC), Ethereum(ETH), and Tether(USDT). We find ETH net inflows to strongly predict ETH returns and volatility in the 2017-2023 period. Our intraday frequencies are 1-6 hours. We find that differing significantly from forecasting patterns for BTC, ETH net inflows negatively predict ETH returns and volatility. First, we find that USDT flowing out of investors wallets and into cryptocurrency exchanges, namely, USDT net inflows into the exchanges, positively predicts BTC and ETH returns at multiple intervals and negatively predicts ETH volatility at various intervals and BTC volatility at the 6-hour interval. Second, we find that ETH net inflows negatively predict ETH returns and volatility for all intraday intervals. Third, BTC net inflows generally lack predictive power for BTC returns(except at 4 hours) but are negatively associated with volatility across all intraday intervals. We illustrate our findings on return forecasting via case studies. Moreover, we develop option strategies to assess profits and losses on ETH investments based on ETH net inflows. Our findings contribute to the growing literature on on-chain activity and its asset pricing implications, offering economically relevant insights for intraday portfolio management in cryptocurrency markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06327v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yeguang Chi (Ruihua),  Qionghua (Ruihua),  Chu, Wenyan Hao</dc:creator>
    </item>
    <item>
      <title>On Extrapolation of Treatment Effects in Multiple-Cutoff Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2412.04265</link>
      <description>arXiv:2412.04265v3 Announce Type: replace 
Abstract: We investigate how to learn treatment effects away from the cutoff in multiple-cutoff regression discontinuity designs. Using a microeconomic model, we demonstrate that the parallel-trend type assumption proposed in the literature is justified when cutoff positions are assigned as if randomly and the running variable is non-manipulable (e.g., parental income). However, when the running variable is partially manipulable (e.g., test scores), extrapolations based on that assumption can be biased. As a complementary strategy, we propose a novel partial identification approach based on empirically motivated assumptions. We also develop a uniform inference procedure and provide two empirical illustrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04265v3</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuta Okamoto, Yuuki Ozaki</dc:creator>
    </item>
    <item>
      <title>VAR models with an index structure: A survey with new results</title>
      <link>https://arxiv.org/abs/2412.11278</link>
      <description>arXiv:2412.11278v2 Announce Type: replace 
Abstract: The main aim of this paper is to review recent advances in the multivariate autoregressive index model [MAI], originally proposed by Reinsel (1983), and their applications to economic and financial time series. MAI has recently gained momentum because it can be seen as a link between two popular but distinct multivariate time series approaches: vector autoregressive modeling [VAR] and the dynamic factor model [DFM]. Indeed, on the one hand, the MAI is a VAR model with a peculiar reduced-rank structure; on the other hand, it allows for identification of common components and common shocks in a similar way as the DFM. The focus is on recent developments of the MAI, which include extending the original model with individual autoregressive structures, stochastic volatility, time-varying parameters, high-dimensionality, and cointegration. In addition, new insights on previous contributions and a novel model are also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11278v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gianluca Cubadda</dc:creator>
    </item>
    <item>
      <title>Taxonomy and Estimation of Multiple Breakpoints in High-Dimensional Factor Models</title>
      <link>https://arxiv.org/abs/2503.06645</link>
      <description>arXiv:2503.06645v2 Announce Type: replace 
Abstract: This paper investigates the estimation of high-dimensional factor models in which factor loadings undergo an unknown number of structural changes over time. Given that a model with multiple changes in factor loadings can be observationally indistinguishable from one with constant loadings but varying factor variances, this reduces the high-dimensional structural change problem to a lower-dimensional one. Due to the presence of multiple breakpoints, the factor space may expand, potentially causing the pseudo factor covariance matrix within some regimes to be singular. We define two types of breakpoints: {\bf a singular change}, where the number of factors in the combined regime exceeds the minimum number of factors in the two separate regimes, and {\bf a rotational change}, where the number of factors in the combined regime equals that in each separate regime. Under a singular change, we derive the properties of the small eigenvalues and establish the consistency of the QML estimators. Under a rotational change, unlike in the single-breakpoint case, the pseudo factor covariance matrix within each regime can be either full rank or singular, yet the QML estimation error for the breakpoints remains stably bounded. We further propose an information criterion (IC) to estimate the number of breakpoints and show that, with probability approaching one, it accurately identifies the true number of structural changes. Monte Carlo simulations confirm strong finite-sample performance. Finally, we apply our method to the FRED-MD dataset, identifying five structural breaks in factor loadings between 1959 and 2024.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06645v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiangtao Duan, Jushan Bai, Xu Han</dc:creator>
    </item>
    <item>
      <title>Slope Consistency of Quasi-Maximum Likelihood Estimator for Binary Choice Models</title>
      <link>https://arxiv.org/abs/2505.02327</link>
      <description>arXiv:2505.02327v2 Announce Type: replace 
Abstract: Although QMLE is generally inconsistent, logistic regression relying on the binary choice model (BCM) with logistic errors is widely used, especially in machine learning contexts with many covariates and high-dimensional slope coefficients. This paper revisits the slope consistency of QMLE for BCMs. Ruud (1983) introduced a set of conditions under which QMLE may yield a constant multiple of the slope coefficient of BCMs asymptotically. However, he did not fully establish slope consistency of QMLE, which requires the existence of a positive multiple of slope coefficient identified as an interior maximizer of the population QMLE likelihood function over an appropriately restricted parameter space. We fill this gap by providing a formal proof of slope consistency under the same set of conditions for any binary choice model identified as in Manski (1975, 1985). Our result implies that logistic regression yields a consistent estimate for the slope coefficient of BCMs under suitable conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02327v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yoosoon Chang, Joon Y. Park, Guo Yan</dc:creator>
    </item>
    <item>
      <title>On Selection of Cross-Section Averages in Non-stationary Environments</title>
      <link>https://arxiv.org/abs/2505.08615</link>
      <description>arXiv:2505.08615v3 Announce Type: replace 
Abstract: Information criteria (IC) have been widely used in factor models to estimate an unknown number of latent factors. It has recently been shown that IC perform well in Common Correlated Effects (CCE) and related setups in selecting a set of cross-section averages (CAs) sufficient for the factor space under stationary factors. As CAs can proxy non-stationary factors, it is tempting to claim such generality of IC, too. We show formally and in simulations that IC have a severe underselection issue even under very mild forms of factor non-stationarity, which goes against the sentiment in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08615v3</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Ditzen, Ovidijus Stauskas</dc:creator>
    </item>
  </channel>
</rss>
