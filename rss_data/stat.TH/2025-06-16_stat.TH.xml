<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Jun 2025 02:28:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Inference post region selection</title>
      <link>https://arxiv.org/abs/2506.11564</link>
      <description>arXiv:2506.11564v1 Announce Type: new 
Abstract: Post-selection inference consists in providing statistical guarantees, based on a data set, that are robust to a prior model selection step on the same data set. In this paper, we address an instance of the post-selection-inference problem, where the model selection step consists in selecting a rectangular region in a spatial domain. The inference step then consists in constructing confidence intervals on the average signal of this region. This is motivated by applications such as genetics or brain imaging. Our confidence intervals are constructed in dimension one, and then extended to higher dimension. They are based on the process mapping all possible selected regions to their corresponding estimation errors on the average signal. We prove the functional convergence of this process to a limiting Gaussian process with explicit covariance. This enables us to provide confidence intervals with asymptotic guarantees. In numerical experiments with simulated data, we show that our coverage proportions are fairly close to the nominal level already for small to moderate data-set size. We also highlight the impact of various possible noise distributions and the robustness of our intervals. Finally, we illustrate the relevance of our method to a segmentation problem inspired by the analysis of DNA copy number data in cancerology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11564v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominique Bontemps (IMT), Fran\c{c}ois Bachoc (IMT, ANITI), Pierre Neuvial (IMT)</dc:creator>
    </item>
    <item>
      <title>Variance estimation after matching or re-weighting</title>
      <link>https://arxiv.org/abs/2506.11317</link>
      <description>arXiv:2506.11317v1 Announce Type: cross 
Abstract: This paper develops a variance estimation framework for matching estimators that enables valid population inference for treatment effects. We provide theoretical analysis of a variance estimator that addresses key limitations in the existing literature. While Abadie and Imbens (2006) proposed a foundational variance estimator requiring matching for both treatment and control groups, this approach is computationally prohibitive and rarely used in practice. Our method provides a computationally feasible alternative that only requires matching treated units to controls while maintaining theoretical validity for population inference.
  We make three main contributions. First, we establish consistency and asymptotic normality for our variance estimator, proving its validity for average treatment effect on the treated (ATT) estimation in settings with small treated samples. Second, we develop a generalized theoretical framework with novel regularity conditions that significantly expand the class of matching procedures for which valid inference is available, including radius matching, M-nearest neighbor matching, and propensity score matching. Third, we demonstrate that our approach extends naturally to other causal inference estimators such as stable balancing weighting methods.
  Through simulation studies across different data generating processes, we show that our estimator maintains proper coverage rates while the state-of-the-art bootstrap method can exhibit substantial undercoverage (dropping from 95% to as low as 61%), particularly in settings with extensive control unit reuse. Our framework provides researchers with both theoretical guarantees and practical tools for conducting valid population inference across a wide range of causal inference applications. An R package implementing our method is available at https://github.com/jche/scmatch2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11317v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiang Meng, Aaron Smith, Luke Miratrix</dc:creator>
    </item>
    <item>
      <title>Local empirical Bayes correction for Bayesian modeling</title>
      <link>https://arxiv.org/abs/2506.11424</link>
      <description>arXiv:2506.11424v1 Announce Type: cross 
Abstract: The James-Stein estimator has attracted much interest as a shrinkage estimator that yields better estimates than the maximum likelihood estimator. The James-Stein estimator is also very useful as an argument in favor of empirical Bayesian methods. However, for problems involving large-scale data, such as differential gene expression data, the distribution is considered a mixture distribution with different means that cannot be considered sufficiently close. Therefore, it is not appropriate to apply the James-Stein estimator. Efron (2011) proposed a local empirical Bayes correction that attempted to correct a selection bias for large-scale data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11424v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.24644/keidaironshu.68.4\_161</arxiv:DOI>
      <arxiv:journal_reference>Osaka Keidai Ronshu, vol.68, no.4, pp.161-172, 2017</arxiv:journal_reference>
      <dc:creator>Yoshiko Hayashi</dc:creator>
    </item>
    <item>
      <title>On the performance of multi-fidelity and reduced-dimensional neural emulators for inference of physiologic boundary conditions</title>
      <link>https://arxiv.org/abs/2506.11683</link>
      <description>arXiv:2506.11683v1 Announce Type: cross 
Abstract: Solving inverse problems in cardiovascular modeling is particularly challenging due to the high computational cost of running high-fidelity simulations. In this work, we focus on Bayesian parameter estimation and explore different methods to reduce the computational cost of sampling from the posterior distribution by leveraging low-fidelity approximations. A common approach is to construct a surrogate model for the high-fidelity simulation itself. Another is to build a surrogate for the discrepancy between high- and low-fidelity models. This discrepancy, which is often easier to approximate, is modeled with either a fully connected neural network or a nonlinear dimensionality reduction technique that enables surrogate construction in a lower-dimensional space. A third possible approach is to treat the discrepancy between the high-fidelity and surrogate models as random noise and estimate its distribution using normalizing flows. This allows us to incorporate the approximation error into the Bayesian inverse problem by modifying the likelihood function. We validate five different methods which are variations of the above on analytical test cases by comparing them to posterior distributions derived solely from high-fidelity models, assessing both accuracy and computational cost. Finally, we demonstrate our approaches on two cardiovascular examples of increasing complexity: a lumped-parameter Windkessel model and a patient-specific three-dimensional anatomy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11683v1</guid>
      <category>stat.ML</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chloe H. Choi, Andrea Zanoni, Daniele E. Schiavazzi, Alison L. Marsden</dc:creator>
    </item>
    <item>
      <title>Simultaneous hypothesis testing for comparing many functional means</title>
      <link>https://arxiv.org/abs/2506.11889</link>
      <description>arXiv:2506.11889v1 Announce Type: cross 
Abstract: Data with multiple functional recordings at each observational unit are increasingly common in various fields including medical imaging and environmental sciences. To conduct inference for such observations, we develop a paired two-sample test that allows to simultaneously compare the means of many functional observations while maintaining family-wise error rate control. We explicitly allow the number of functional recordings to increase, potentially much faster than the sample size. Our test is fully functional and does not rely on dimension reduction or functional PCA type approaches or the choice of tuning parameters. To provide a theoretical justification for the proposed procedure, we develop a number of new anti-concentration and Gaussian approximation results for maxima of $L^2$ statistics which might be of independent interest. The methodology is illustrated on the task-related cortical surface functional magnetic resonance imaging data from Human Connectome Project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11889v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Colin Decker, Dehan Kong, Stanislav Volgushev</dc:creator>
    </item>
    <item>
      <title>A Random Matrix Theory of Pauli Tomography</title>
      <link>https://arxiv.org/abs/2506.12010</link>
      <description>arXiv:2506.12010v1 Announce Type: cross 
Abstract: Quantum state tomography (QST), the process of reconstructing some unknown quantum state $\hat\rho$ from repeated measurements on copies of said state, is a foundationally important task in the context of quantum computation and simulation. For this reason, a detailed characterization of the error $\Delta\hat\rho = \hat\rho-\hat\rho^\prime$ in a QST reconstruction $\hat\rho^\prime$ is of clear importance to quantum theory and experiment. In this work, we develop a fully random matrix theory (RMT) treatment of state tomography in informationally-complete bases; and in doing so we reveal deep connections between QST errors $\Delta\hat\rho$ and the gaussian unitary ensemble (GUE). By exploiting this connection we prove that wide classes of functions of the spectrum of $\Delta\hat\rho$ can be evaluated by substituting samples of an appropriate GUE for realizations of $\Delta\hat\rho$. This powerful and flexible result enables simple analytic treatments of the mean value and variance of the error as quantified by the trace distance $\|\Delta\hat\rho\|_\mathrm{Tr}$ (which we validate numerically for common tomographic protocols), allows us to derive a bound on the QST sample complexity, and subsequently demonstrate that said bound doesn't change under the most widely-used rephysicalization procedure. These results collectively demonstrate the flexibility, strength, and broad applicability of our approach; and lays the foundation for broader studies of RMT treatments of QST in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12010v1</guid>
      <category>quant-ph</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Keenan, John Goold, Alex Nico-Katz</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes large-scale multiple testing for high-dimensional binary outcome data</title>
      <link>https://arxiv.org/abs/2307.05943</link>
      <description>arXiv:2307.05943v5 Announce Type: replace 
Abstract: This paper explores the multiple testing problem for sparse high-dimensional data with binary outcomes. We propose novel empirical Bayes multiple testing procedures based on a spike-and-slab posterior and then evaluate their performance in controlling the false discovery rate (FDR). A surprising finding is that the procedure using the default conjugate prior (namely, the $\ell$-value procedure) can be overly conservative in estimating the FDR. To address this, we introduce two new procedures that provide accurate FDR control. Sharp frequentist theoretical results are established for these procedures, and numerical experiments are conducted to validate our theory in finite samples. To the best of our knowledge, we obtain the first {\it uniform} FDR control result in multiple testing for high-dimensional data with binary outcomes under the sparsity assumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.05943v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yu-Chien Bo Ning</dc:creator>
    </item>
    <item>
      <title>Information theoretic limits of robust sub-Gaussian mean estimation under star-shaped constraints</title>
      <link>https://arxiv.org/abs/2412.03832</link>
      <description>arXiv:2412.03832v2 Announce Type: replace 
Abstract: We obtain the minimax rate for a mean location model with a bounded star-shaped set $K \subseteq \mathbb{R}^n$ constraint on the mean, in an adversarially corrupted data setting with Gaussian noise. We assume an unknown fraction $\epsilon \le 1/2-\kappa$ for some fixed $\kappa\in(0,1/2]$ of $N$ observations are arbitrarily corrupted. We obtain a minimax risk up to proportionality constants under the squared $\ell_2$ loss of $\max(\eta^{*2},\sigma^2\epsilon^2)\wedge d^2$ with \begin{align*}
  \eta^* = \sup \bigg\{\eta \ge 0 : \frac{N\eta^2}{\sigma^2} \leq \log \mathcal{M}_K^{\operatorname{loc}}(\eta,c)\bigg\}, \end{align*} where $\log \mathcal{M}_K^{\operatorname{loc}}(\eta,c)$ denotes the local entropy of the set $K$, $d$ is the diameter of $K$, $\sigma^2$ is the variance, and $c$ is some sufficiently large absolute constant. A variant of our algorithm achieves the same rate for settings with known or symmetric sub-Gaussian noise, with a smaller breakdown point, still of constant order. We further study the case of unknown sub-Gaussian noise and show that the rate is slightly slower: $\max(\eta^{*2},\sigma^2\epsilon^2\log(1/\epsilon))\wedge d^2$. We generalize our results to the case when $K$ is star-shaped but unbounded.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03832v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akshay Prasadan, Matey Neykov</dc:creator>
    </item>
    <item>
      <title>Covariance matrix estimation in the singular case using regularized Cholesky factor</title>
      <link>https://arxiv.org/abs/2505.16302</link>
      <description>arXiv:2505.16302v2 Announce Type: replace 
Abstract: We consider estimating the population covariance matrix when the number of available samples is less than the size of the observations. The sample covariance matrix (SCM) being singular, regularization is mandatory in this case. For this purpose we consider minimizing Stein's loss function and we investigate a method based on augmenting the partial Cholesky decomposition of the SCM. We first derive the finite sample optimum estimator which minimizes the loss for each data realization, then the Oracle estimator which minimizes the risk, i.e., the average value of the loss. Finally a practical scheme is presented where the missing part of the Cholesky decomposition is filled. We conduct a numerical performance study of the proposed method and compare it with available related methods. In particular we investigate the influence of the condition number of the covariance matrix as well as of the shape of its spectrum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.16302v2</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olivier Besson</dc:creator>
    </item>
    <item>
      <title>Learning Firmly Nonexpansive Operators</title>
      <link>https://arxiv.org/abs/2407.14156</link>
      <description>arXiv:2407.14156v2 Announce Type: replace-cross 
Abstract: This paper proposes a data-driven approach for constructing firmly nonexpansive operators. We demonstrate its applicability in Plug-and-Play (PnP) methods, where classical algorithms such as Forward-Backward splitting, Chambolle-Pock primal-dual iteration, Douglas-Rachford iteration or alternating directions method of multipliers (ADMM), are modified by replacing one proximal map by a learned firmly nonexpansive operator. We provide sound mathematical background to the problem of learning such an operator via expected and empirical risk minimization. We prove that, as the number of training points increases, the empirical risk minimization problem converges (in the sense of Gamma-convergence) to the expected risk minimization problem. Further, we derive a solution strategy that ensures firmly nonexpansive and piecewise affine operators within the convex envelope of the training set. We show that this operator converges to the best empirical solution as the number of points in the envelope increases in an appropriate way. Finally, the experimental section details practical implementations of the method and presents an application in image denoising, where we consider a novel, interpretable PnP Chambolle-Pock primal-dual iteration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14156v2</guid>
      <category>math.OC</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristian Bredies, Jonathan Chirinos-Rodriguez, Emanuele Naldi</dc:creator>
    </item>
    <item>
      <title>General-Purpose $f$-DP Estimation and Auditing in a Black-Box Setting</title>
      <link>https://arxiv.org/abs/2502.07066</link>
      <description>arXiv:2502.07066v2 Announce Type: replace-cross 
Abstract: In this paper we propose new methods to statistically assess $f$-Differential Privacy ($f$-DP), a recent refinement of differential privacy (DP) that remedies certain weaknesses of standard DP (including tightness under algorithmic composition). A challenge when deploying differentially private mechanisms is that DP is hard to validate, especially in the black-box setting. This has led to numerous empirical methods for auditing standard DP, while $f$-DP remains less explored. We introduce new black-box methods for $f$-DP that, unlike existing approaches for this privacy notion, do not require prior knowledge of the investigated algorithm. Our procedure yields a complete estimate of the $f$-DP trade-off curve, with theoretical guarantees of convergence. Additionally, we propose an efficient auditing method that empirically detects $f$-DP violations with statistical certainty, merging techniques from non-parametric estimation and optimal classification theory. Through experiments on a range of DP mechanisms, we demonstrate the effectiveness of our estimation and auditing procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07066v2</guid>
      <category>cs.CR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\"Onder Askin (Ruhr-University Bochum), Holger Dette (Ruhr-University Bochum), Martin Dunsche (Ruhr-University Bochum), Tim Kutta (Aarhus University), Yun Lu (University of Victoria), Yu Wei (Georgia Institute of Technology), Vassilis Zikas (Georgia Institute of Technology)</dc:creator>
    </item>
    <item>
      <title>Proximal Inference on Population Intervention Indirect Effect</title>
      <link>https://arxiv.org/abs/2504.11848</link>
      <description>arXiv:2504.11848v2 Announce Type: replace-cross 
Abstract: Population intervention indirect effect (PIIE) is a novel mediation effect representing the indirect component of the population intervention effect. Unlike traditional mediation measures, such as the natural indirect effect, the PIIE holds particular relevance in observational studies involving unethical exposures, when hypothetical interventions that impose harmful exposures are inappropriate. Although prior research has identified PIIE under unmeasured confounders between exposure and outcome, it has not fully addressed the confounding that affects the mediator. This paper proposes a novel PIIE identification framework in settings where unmeasured confounders influence exposure-outcome, exposure-mediator, and mediator-outcome relationships. Specifically, we leverage observed covariates as proxy variables for unmeasured confounders, constructing three proximal identification strategies. Additionally, we characterize the semiparametric efficiency bound and develop multiply robust and locally efficient estimators. To handle high-dimensional nuisance parameters, we propose a debiased machine learning approach that achieves $\sqrt{n}$-consistency and asymptotic normality to estimate the true PIIE values, even when the machine learning estimators for the nuisance functions do not converge at $\sqrt{n}$-rate. In simulations, our estimators demonstrate higher confidence interval coverage rates than conventional methods across various model misspecifications. In a real data application, our approaches reveal an indirect effect of alcohol consumption on depression risk mediated by depersonalization symptoms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11848v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Bai, Yifan Cui, Baoluo Sun</dc:creator>
    </item>
    <item>
      <title>Transformers for Learning on Noisy and Task-Level Manifolds: Approximation and Generalization Insights</title>
      <link>https://arxiv.org/abs/2505.03205</link>
      <description>arXiv:2505.03205v2 Announce Type: replace-cross 
Abstract: Transformers serve as the foundational architecture for large language and video generation models, such as GPT, BERT, SORA and their successors. Empirical studies have demonstrated that real-world data and learning tasks exhibit low-dimensional structures, along with some noise or measurement error. The performance of transformers tends to depend on the intrinsic dimension of the data/tasks, though theoretical understandings remain largely unexplored for transformers. This work establishes a theoretical foundation by analyzing the performance of transformers for regression tasks involving noisy input data on a manifold. Specifically, the input data are in a tubular neighborhood of a manifold, while the ground truth function depends on the projection of the noisy data onto the manifold. We prove approximation and generalization errors which crucially depend on the intrinsic dimension of the manifold. Our results demonstrate that transformers can leverage low-complexity structures in learning task even when the input data are perturbed by high-dimensional noise. Our novel proof technique constructs representations of basic arithmetic operations by transformers, which may hold independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03205v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaiming Shen, Alex Havrilla, Rongjie Lai, Alexander Cloninger, Wenjing Liao</dc:creator>
    </item>
  </channel>
</rss>
