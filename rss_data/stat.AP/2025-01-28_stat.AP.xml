<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Jan 2025 02:36:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On the super-efficiency and robustness of the least squares of depth-trimmed regression estimator</title>
      <link>https://arxiv.org/abs/2501.14791</link>
      <description>arXiv:2501.14791v1 Announce Type: new 
Abstract: The least squares of depth-trimmed (LST) residuals regression, proposed and studied in Zuo and Zuo (2023), serves as a robust alternative to the classic least squares (LS) regression as well as a strong competitor to the renowned robust least trimmed squares (LTS) regression of Rousseeuw (1984).
  The aim of this article is three-fold. (i) to reveal the super-efficiency of the LST and demonstrate it can be as efficient as (or even more efficient than) the LS in the scenarios with errors uncorrelated and mean zero and homoscedastic with finite variance and to explain this anti-Gaussian-Markov-Theorem phenomenon; (ii) to demonstrate that the LST can outperform the LTS, the benchmark of robust regression estimator, on robustness, and the MM of Yohai (1987), the benchmark of efficient and robust estimator, on both efficiency and robustness, consequently, could serve as an alternative to both; (iii) to promote the implementation and computation of the LST regression for a broad group of statisticians in statistical practice and to demonstrate that it can be computed as fast as (or even faster than) the LTS based on a newly improved algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14791v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijun Zuo, Hanwen Zuo</dc:creator>
    </item>
    <item>
      <title>Inverse Gaussian Distribution, Introduction and Applications:Comprehensive Analysis of Power Plant Performance: A Study of Combined Cycle and Nuclear Power Plant</title>
      <link>https://arxiv.org/abs/2501.14820</link>
      <description>arXiv:2501.14820v1 Announce Type: new 
Abstract: This paper presents a comprehensive analysis of power plant performance using the inverse Gaussian (IG) distribution framework. We combine theoretical foundations with practical applications, focusing on both combined cycle and nuclear power plant contexts. The study demonstrates the advantages of the IG distribution in modeling right-skewed industrial data, particularly in power generation. Using the UCI Combined Cycle Power Plant Dataset, we establishthe superiority of IG-based models over traditional approaches through rigorous statistical testing and model validation. The methodology developed here extends naturally to nuclear power plant applications, where similar statistical patterns emerge in operational data. Our findings suggest that IG-based models provide more accurate predictions and better capture the underlying physical processes in power generation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14820v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yen-hsuan Tseng</dc:creator>
    </item>
    <item>
      <title>Controlling Ensemble Variance in Diffusion Models: An Application for Reanalyses Downscaling</title>
      <link>https://arxiv.org/abs/2501.14822</link>
      <description>arXiv:2501.14822v1 Announce Type: new 
Abstract: In recent years, diffusion models have emerged as powerful tools for generating ensemble members in meteorology. In this work, we demonstrate that a Denoising Diffusion Implicit Model (DDIM) can effectively control ensemble variance by varying the number of diffusion steps. Introducing a theoretical framework, we relate diffusion steps to the variance expressed by the reverse diffusion process. Focusing on reanalysis downscaling, we propose an ensemble diffusion model for the full ERA5-to-CERRA domain, generating variance-calibrated ensemble members for wind speed at full spatial and temporal resolution. Our method aligns global mean variance with a reference ensemble dataset and ensures spatial variance is distributed in accordance with observed meteorological variability. Additionally, we address the lack of ensemble information in the CARRA dataset, showcasing the utility of our approach for efficient, high-resolution ensemble generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14822v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fabio Merizzi, Davide Evangelista, Harilaos Loukos</dc:creator>
    </item>
    <item>
      <title>Validation of Satellite and Reanalysis Rainfall Products in Ghana and Zambia</title>
      <link>https://arxiv.org/abs/2501.14829</link>
      <description>arXiv:2501.14829v2 Announce Type: new 
Abstract: Accurate rainfall data are crucial for effective climate services, especially in Sub-Saharan Africa, where agriculture heavily depends on rain-fed systems. However, the sparse distribution of rain-gauge networks in the region necessitates reliance on satellite and reanalysis rainfall products (REs) for rainfall estimation. This study evaluated the performance of eight REs -- CHIRPS, TAMSAT, CHIRP, ENACTS, ERA5, AgERA5, PERSIANN-CDR, and PERSIANN-CCS-CDR -- in Zambia and Ghana using a point-to-pixel validation approach. The analysis encompassed spatial consistency, annual rainfall summaries, seasonal patterns, and rainfall intensity detection across 38 ground stations. Results indicated that no single product performed optimally across all contexts, underscoring the need for application-specific recommendations. All products exhibited a high probability of detection (POD) for dry days in Zambia and northern Ghana (with 70% $&lt;$ POD $&lt;$ 100%, and 60% $&lt;$ POD $&lt;$ 85% respectively), suggesting their potential utility for drought-related studies in these areas. Conversely, all products showed limited skill in detecting heavy and violent rains (with POD close to 0%), rendering them unsuitable for extreme rainfall analysis (such as floods) in the current form. Products integrated with station data (ENACTS, CHIRPS, and TAMSAT) outperformed their counterparts under many contexts, highlighting the importance of calibration with local observations. Bias correction is strongly recommended, as varying levels of biases were evident across different rainfall summaries. A critical area for advancement is extreme rainfall detection. Future research should focus on this aspect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14829v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Bagiliko, David Stern, Denis Ndanguza, Francis Feehi Torgbor</dc:creator>
    </item>
    <item>
      <title>A characterization of uniform distribution using varextropy with application in testing uniformity</title>
      <link>https://arxiv.org/abs/2501.14797</link>
      <description>arXiv:2501.14797v1 Announce Type: cross 
Abstract: In statistical analysis, quantifying uncertainties through measures such as entropy, extropy, varentropy, and varextropy is of fundamental importance for understanding distribution functions. This paper investigates several properties of varextropy and give a new characterization of uniform distribution using varextropy. The alredy proposed estimators are used as a test statistics. Building on the characterization of the uniform distribution using varextropy, we give a uniformity test. The critical value and power of the test statistics are derived. The proposed test procedure is applied to a real-world dataset to assess its performance and effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14797v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Santosh Kumar Chaudhary, Nitin Gupta</dc:creator>
    </item>
    <item>
      <title>Sequential Methods for Error Correction of Probabilistic Wind Power Forecasts</title>
      <link>https://arxiv.org/abs/2501.14805</link>
      <description>arXiv:2501.14805v1 Announce Type: cross 
Abstract: Reliable probabilistic production forecasts are required to better manage the uncertainty that the rapid build-out of wind power capacity adds to future energy systems. In this article, we consider sequential methods to correct errors in power production forecast ensembles derived from numerical weather predictions. We propose combining neural networks with time-adaptive quantile regression to enhance the accuracy of wind power forecasts. We refer to this approach as Neural Adaptive Basis for (time-adaptive) Quantile Regression or NABQR. First, we use NABQR to correct power production ensembles with neural networks. We find that Long Short-Term Memory networks are the most effective architecture for this purpose. Second, we apply time-adaptive quantile regression to the corrected ensembles to obtain optimal median predictions along with quantiles of the forecast distribution. With the suggested method we achieve accuracy improvements up to 40% in mean absolute terms in an application to day-ahead forecasting of on- and offshore wind power production in Denmark. In addition, we explore the value of our method for applications in energy trading. We have implemented the NABQR method as an open-source Python package to support applications in renewable energy forecasting and future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14805v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bastian Schmidt J{\o}rgensen, Jan Kloppenborg M{\o}ller, Peter Nystrup, Henrik Madsen</dc:creator>
    </item>
    <item>
      <title>A Semiparametric Bayesian Method for Instrumental Variable Analysis with Partly Interval-Censored Time-to-Event Outcome</title>
      <link>https://arxiv.org/abs/2501.14837</link>
      <description>arXiv:2501.14837v1 Announce Type: cross 
Abstract: This paper develops a semiparametric Bayesian instrumental variable analysis method for estimating the causal effect of an endogenous variable when dealing with unobserved confounders and measurement errors with partly interval-censored time-to-event data, where event times are observed exactly for some subjects but left-censored, right-censored, or interval-censored for others. Our method is based on a two-stage Dirichlet process mixture instrumental variable (DPMIV) model which simultaneously models the first-stage random error term for the exposure variable and the second-stage random error term for the time-to-event outcome using a bivariate Gaussian mixture of the Dirichlet process (DPM) model. The DPM model can be broadly understood as a mixture model with an unspecified number of Gaussian components, which relaxes the normal error assumptions and allows the number of mixture components to be determined by the data. We develop an MCMC algorithm for the DPMIV model tailored for partly interval-censored data and conduct extensive simulations to assess the performance of our DPMIV method in comparison with some competing methods. Our simulations revealed that our proposed method is robust under different error distributions and can have superior performance over its parametric counterpart under various scenarios. We further demonstrate the effectiveness of our approach on an UK Biobank data to investigate the causal effect of systolic blood pressure on time-to-development of cardiovascular disease from the onset of diabetes mellitus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14837v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elvis Han Cui, Xuyang Lu, Jin Zhou, Hua Zhou, Gang Li</dc:creator>
    </item>
    <item>
      <title>Salvaging Forbidden Treasure in Medical Data: Utilizing Surrogate Outcomes and Single Records for Rare Event Modeling</title>
      <link>https://arxiv.org/abs/2501.15079</link>
      <description>arXiv:2501.15079v1 Announce Type: cross 
Abstract: The vast repositories of Electronic Health Records (EHR) and medical claims hold untapped potential for studying rare but critical events, such as suicide attempt. Conventional setups often model suicide attempt as a univariate outcome and also exclude any ``single-record'' patients with a single documented encounter due to a lack of historical information. However, patients who were diagnosed with suicide attempts at the only encounter could, to some surprise, represent a substantial proportion of all attempt cases in the data, as high as 70--80%. We innovate a hybrid and integrative learning framework to leverage concurrent outcomes as surrogates and harness the forbidden yet precious information from single-record data. Our approach employs a supervised learning component to learn the latent variables that connect primary (e.g., suicide) and surrogate outcomes (e.g., mental disorders) to historical information. It simultaneously employs an unsupervised learning component to utilize the single-record data, through the shared latent variables. As such, our approach offers a general strategy for information integration that is crucial to modeling rare conditions and events. With hospital inpatient data from Connecticut, we demonstrate that single-record data and concurrent diagnoses indeed carry valuable information, and utilizing them can substantially improve suicide risk modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15079v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaohui Yin, Shane Sacco, Robert H. Aseltine, Fei Wang, Kun Chen</dc:creator>
    </item>
    <item>
      <title>Foundations of a Knee Joint Digital Twin from qMRI Biomarkers for Osteoarthritis and Knee Replacement</title>
      <link>https://arxiv.org/abs/2501.15396</link>
      <description>arXiv:2501.15396v1 Announce Type: cross 
Abstract: This study forms the basis of a digital twin system of the knee joint, using advanced quantitative MRI (qMRI) and machine learning to advance precision health in osteoarthritis (OA) management and knee replacement (KR) prediction. We combined deep learning-based segmentation of knee joint structures with dimensionality reduction to create an embedded feature space of imaging biomarkers. Through cross-sectional cohort analysis and statistical modeling, we identified specific biomarkers, including variations in cartilage thickness and medial meniscus shape, that are significantly associated with OA incidence and KR outcomes. Integrating these findings into a comprehensive framework represents a considerable step toward personalized knee-joint digital twins, which could enhance therapeutic strategies and inform clinical decision-making in rheumatological care. This versatile and reliable infrastructure has the potential to be extended to broader clinical applications in precision health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15396v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabrielle Hoyer, Kenneth T Gao, Felix G Gassert, Johanna Luitjens, Fei Jiang, Sharmila Majumdar, Valentina Pedoia</dc:creator>
    </item>
    <item>
      <title>A New Approach to Radiocarbon Summarisation: Rigorous Identification of Variations/Changepoints in the Occurrence Rate of Radiocarbon Samples using a Poisson Process</title>
      <link>https://arxiv.org/abs/2501.15980</link>
      <description>arXiv:2501.15980v1 Announce Type: cross 
Abstract: A commonly-used paradigm to estimate changes in the frequency of past events or the size of populations is to consider the occurrence rate of archaeological/environmental samples found at a site over time. The reliability of such a "dates-as-data" approach is highly dependent upon how the occurrence rates are estimated from the underlying samples, particularly when calendar age information for the samples is obtained from radiocarbon (14C). The most frequently-used "14C-dates-as-data" approach of creating Summed Probability Distributions (SPDs) is not statistically valid or coherent and can provide highly misleading inference. Here, we provide an alternative method with a rigorous statistical underpinning that also provides valuable additional information on potential changepoints in the rate of events. Our approach ensures more reliable "14C-dates-as-data" analyses, allowing us to better assess and identify potential signals present. We model the occurrence of events, each assumed to leave a radiocarbon sample in the archaeological/environmental record, as an inhomogeneous Poisson process. The varying rate of samples over time is then estimated within a fully-Bayesian framework using reversible-jump Markov Chain Monte Carlo (RJ-MCMC). Given a set of radiocarbon samples, we reconstruct how their occurrence rate varies over calendar time and identify if that rate contains statistically-significant changes, i.e., specific times at which the rate of events abruptly changes. We illustrate our method with both a simulation study and a practical example concerning late-Pleistocene megafaunal population changes in Alaska and Yukon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15980v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy J Heaton, Sara Al-assam, Edouard Bard</dc:creator>
    </item>
    <item>
      <title>On spatial point processes with composition-valued marks</title>
      <link>https://arxiv.org/abs/2501.16049</link>
      <description>arXiv:2501.16049v1 Announce Type: cross 
Abstract: Methods for marked spatial point processes with scalar marks have seen extensive development in recent years. While the impressive progress in data collection and storage capacities has yielded an immense increase in spatial point process data with highly challenging non-scalar marks, methods for their analysis are not equally well developed. In particular, there are no methods for composition-valued marks, i.e. vector-valued marks with a sum-to-constant constrain (typically 1 or 100). Prompted by the need for a suitable methodological framework, we extend existing methods to spatial point processes with composition-valued marks and adapt common mark characteristics to this context. The proposed methods are applied to analyse spatial correlations in data on tree crown-to-base and business sector compositions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16049v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Eckardt, Mari Myllym\"aki, Sonja Greven</dc:creator>
    </item>
    <item>
      <title>Copyright and Competition: Estimating Supply and Demand with Unstructured Data</title>
      <link>https://arxiv.org/abs/2501.16120</link>
      <description>arXiv:2501.16120v1 Announce Type: cross 
Abstract: Copyright policies play a pivotal role in protecting the intellectual property of creators and companies in creative industries. The advent of cost-reducing technologies, such as generative AI, in these industries calls for renewed attention to the role of these policies. This paper studies product positioning and competition in a market of creatively differentiated products and the competitive and welfare effects of copyright protection. A common feature of products with creative elements is that their key attributes (e.g., images and text) are unstructured and thus high-dimensional. We focus on a stylized design product, fonts, and use data from the world's largest online marketplace for fonts. We use neural network embeddings to quantify unstructured attributes and measure the visual similarity. We show that this measure closely aligns with actual human perception. Based on this measure, we empirically find that competitions occur locally in the visual characteristics space. We then develop a structural model for supply and demand that integrate the embeddings. Through counterfactual analyses, we find that local copyright protection can enhance consumer welfare when products are relocated, and the interplay between copyright and cost-reducing technologies is essential in determining an optimal policy for social welfare. We believe that the embedding analysis and empirical models introduced in this paper can be applicable to a range of industries where unstructured data captures essential features of products and markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16120v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sukjin Han, Kyungho Lee</dc:creator>
    </item>
    <item>
      <title>Fault detection in propulsion motors in the presence of concept drift</title>
      <link>https://arxiv.org/abs/2406.08030</link>
      <description>arXiv:2406.08030v3 Announce Type: replace 
Abstract: Machine learning and statistical methods can improve conventional motor protection systems, providing early warning and detection of emerging failures. Data-driven methods rely on historical data to learn how the system is expected to behave under normal circumstances. An unexpected change in the underlying system may cause a change in the statistical properties of the data, and by this alter the performance of the fault detection algorithm in terms of time to detection and false alarms. This kind of change, called \textit{concept drift}, requires adaptations to maintain constant performance. In this article, we present a machine learning approach for detecting overheating in the stator windings of marine electrical propulsion motors. Using simulated overheating faults injected into operational data, the methods are shown to provide early detection compared to conventional methods based on temperature readings and fixed limits. The proposed monitors are designed to operate for a type of concept drift observed in operational data collected from a specific class of motors in a fleet of ships. Using a mix of real and simulated concept drifts, it is shown that the proposed monitors are able to provide early detections during and after concept drifts, without the need for full model retraining.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08030v3</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Tveten, Morten Stakkeland</dc:creator>
    </item>
    <item>
      <title>QuIP: Experimental design for expensive simulators with many Qualitative factors via Integer Programming</title>
      <link>https://arxiv.org/abs/2501.14616</link>
      <description>arXiv:2501.14616v2 Announce Type: replace 
Abstract: The need to explore and/or optimize expensive simulators with many qualitative factors arises in broad scientific and engineering problems. Our motivating application lies in path planning - the exploration of feasible paths for navigation, which plays an important role in robotics, surgical planning and assembly planning. Here, the feasibility of a path is evaluated via expensive virtual experiments, and its parameter space is typically discrete and high-dimensional. A carefully selected experimental design is thus essential for timely decision-making. We propose here a novel framework, called QuIP, for experimental design of Qualitative factors via Integer Programming under a Gaussian process surrogate model with an exchangeable covariance function. For initial design, we show that its asymptotic D-optimal design can be formulated as a variant of the well-known assignment problem in operations research, which can be efficiently solved to global optimality using state-of-the-art integer programming solvers. For sequential design (specifically, for active learning or black-box optimization), we show that its design criterion can similarly be formulated as an assignment problem, thus enabling efficient and reliable optimization with existing solvers. We then demonstrate the effectiveness of QuIP over existing methods in a suite of path planning experiments and an application to rover trajectory optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14616v2</guid>
      <category>stat.AP</category>
      <category>cs.RO</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yen-Chun Liu, Simon Mak</dc:creator>
    </item>
    <item>
      <title>Skew-elliptical copula based mixed models for non-Gaussian longitudinal data with application to an HIV-AIDS study</title>
      <link>https://arxiv.org/abs/2402.00651</link>
      <description>arXiv:2402.00651v2 Announce Type: replace-cross 
Abstract: This study was sparked by an extensive longitudinal dataset focusing on HIV CD4 T$^+$ cell counts from Livingstone district, Zambia. Analysis of the corresponding histogram plots reveals an absence of symmetry in the marginal distributions, while pairwise scatter plots uncover non-elliptical dependence patterns. Traditional linear mixed models designed for longitudinal data fail to capture these complexities adequately. Therefore, it appears prudent to explore a broader framework for modeling such data. In this article, we delve into generalized linear mixed models (GLMM) for the marginals (e.g., the Gamma mixed model), and we address the temporal dependency of repeated measurements by utilizing copulas associated with skew-elliptical distributions (such as the skew-normal/skew-$t$). Our proposed class of copula-based mixed models simultaneously accommodates asymmetry, between-subject variability, and non-standard temporal dependence, thus offering extensions to the standard linear mixed model based on multivariate normality. We estimate the model parameters using the IFM (inference function of margins) method and outline the process of obtaining standard errors for parameter estimates. Through extensive simulation studies covering skewed and symmetric marginal distributions and various copula choices, we assess the finite sample performance of our approach. Finally, we apply these models to the HIV dataset and present our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00651v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhajit Chattopadhyay</dc:creator>
    </item>
    <item>
      <title>Eliciting prior information from clinical trials via calibrated Bayes factor</title>
      <link>https://arxiv.org/abs/2406.19346</link>
      <description>arXiv:2406.19346v2 Announce Type: replace-cross 
Abstract: In the Bayesian framework power prior distributions are increasingly adopted in clinical trials and similar studies to incorporate external and past information, typically to inform the parameter associated to a treatment effect. Their use is particularly effective in scenarios with small sample sizes and where robust prior information is actually available. A crucial component of this methodology is represented by its weight parameter, which controls the volume of historical information incorporated into the current analysis. This parameter can be considered as either fixed or random. Although various strategies exist for its determination, eliciting the prior distribution of the weight parameter according to a full Bayesian approach remains a challenge. In general, this parameter should be carefully selected to accurately reflect the available prior information without dominating the posterior inferential conclusions. To this aim, we propose a novel method for eliciting the prior distribution of the weight parameter through a simulation-based calibrated Bayes factor procedure. This approach allows for the prior distribution to be updated based on the strength of evidence provided by the data: The goal is to facilitate the integration of historical data when it aligns with current information and to limit it when discrepancies arise in terms, for instance, of prior-data conflicts. The performance of the proposed method is tested through simulation studies and applied to real data from clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19346v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Macr\`i Demartino, Leonardo Egidi, Nicola Torelli, Ioannis Ntzoufras</dc:creator>
    </item>
    <item>
      <title>Opinion dynamics in bounded confidence models with manipulative agents: Moving the Overton window</title>
      <link>https://arxiv.org/abs/2501.12198</link>
      <description>arXiv:2501.12198v2 Announce Type: replace-cross 
Abstract: This paper focuses on the opinion dynamics under the influence of manipulative agents. This type of agents is characterized by the fact that their opinions follow a trajectory that does not respond to the dynamics of the model, although it does influence the rest of the normal agents. Simulation has been implemented to study how one manipulative group modifies the natural dynamics of some opinion models of bounded confidence. It is studied what strategies based on the number of manipulative agents and their common opinion trajectory can be carried out by a manipulative group to influence normal agents and attract them to their opinions. In certain weighted models, some effects are observed in which normal agents move in the opposite direction to the manipulator group. Moreover, the conditions which ensure the influence of a manipulative group on a group of normal agents over time are also established for the Hegselmann-Krause model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12198v2</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.physa.2025.130379</arxiv:DOI>
      <arxiv:journal_reference>Physica A: Statistical Mechanics and its Applications, Volume 660, 2025, 130379</arxiv:journal_reference>
      <dc:creator>A. Bautista</dc:creator>
    </item>
  </channel>
</rss>
