<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Aug 2024 01:41:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Multivariate Space-Time Dynamic Model for Characterizing the Atmospheric Impacts Following the Mt Pinatubo Eruptio</title>
      <link>https://arxiv.org/abs/2408.13392</link>
      <description>arXiv:2408.13392v1 Announce Type: new 
Abstract: The June 1991 Mt. Pinatubo eruption resulted in a massive increase of sulfate aerosols in the atmosphere, absorbing radiation and leading to global changes in surface and stratospheric temperatures. A volcanic eruption of this magnitude serves as a natural analog for stratospheric aerosol injection, a proposed solar radiation modification method to combat the warming climate. The impacts of such an event are multifaceted and region-specific. Our goal is to characterize the multivariate and dynamic nature of the climate impacts following the Mt. Pinatubo eruption. We developed a multivariate space-time dynamic linear model to understand the full extent of the spatially- and temporally-varying impacts. Specifically, spatial variation is modeled using a flexible set of basis functions for which the basis coefficients are allowed to vary in time through a vector autoregressive (VAR) structure. This novel model is caste in a Dynamic Linear Model (DLM) framework and estimated via a customized MCMC approach. We demonstrate how the model quantifies the relationships between key atmospheric parameters following the Mt. Pinatubo eruption with reanalysis data from MERRA-2 and highlight when such model is advantageous over univariate models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13392v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Garrett, Lyndsay Shand, J. Gabriel Huerta</dc:creator>
    </item>
    <item>
      <title>Analysis of the ICML 2023 Ranking Data: Can Authors' Opinions of Their Own Papers Assist Peer Review in Machine Learning?</title>
      <link>https://arxiv.org/abs/2408.13430</link>
      <description>arXiv:2408.13430v1 Announce Type: new 
Abstract: We conducted an experiment during the review process of the 2023 International Conference on Machine Learning (ICML) that requested authors with multiple submissions to rank their own papers based on perceived quality. We received 1,342 rankings, each from a distinct author, pertaining to 2,592 submissions. In this paper, we present an empirical analysis of how author-provided rankings could be leveraged to improve peer review processes at machine learning conferences. We focus on the Isotonic Mechanism, which calibrates raw review scores using author-provided rankings. Our analysis demonstrates that the ranking-calibrated scores outperform raw scores in estimating the ground truth ``expected review scores'' in both squared and absolute error metrics. Moreover, we propose several cautious, low-risk approaches to using the Isotonic Mechanism and author-provided rankings in peer review processes, including assisting senior area chairs' oversight of area chairs' recommendations, supporting the selection of paper awards, and guiding the recruitment of emergency reviewers. We conclude the paper by addressing the study's limitations and proposing future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13430v1</guid>
      <category>stat.AP</category>
      <category>cs.DL</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Buxin Su, Jiayao Zhang, Natalie Collina, Yuling Yan, Didong Li, Kyunghyun Cho, Jianqing Fan, Aaron Roth, Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>Change Point Detection in Pairwise Comparison Data with Covariates</title>
      <link>https://arxiv.org/abs/2408.13642</link>
      <description>arXiv:2408.13642v1 Announce Type: new 
Abstract: This paper introduces the novel piecewise stationary covariate-assisted ranking estimation (PS-CARE) model for analyzing time-evolving pairwise comparison data, enhancing item ranking accuracy through the integration of covariate information. By partitioning the data into distinct, stationary segments, the PS-CARE model adeptly detects temporal shifts in item rankings, known as change points, whose number and positions are initially unknown. Leveraging the minimum description length (MDL) principle, this paper establishes a statistically consistent model selection criterion to estimate these unknowns. The practical optimization of this MDL criterion is done with the pruned exact linear time (PELT) algorithm. Empirical evaluations reveal the method's promising performance in accurately locating change points across various simulated scenarios. An application to an NBA dataset yielded meaningful insights that aligned with significant historical events, highlighting the method's practical utility and the MDL criterion's effectiveness in capturing temporal ranking changes. To the best of the authors' knowledge, this research pioneers change point detection in pairwise comparison data with covariate information, representing a significant leap forward in the field of dynamic ranking analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13642v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Han, Thomas C. M. Lee</dc:creator>
    </item>
    <item>
      <title>A Topological Gaussian Mixture Model for Bone Marrow Morphology in Leukaemia</title>
      <link>https://arxiv.org/abs/2408.13685</link>
      <description>arXiv:2408.13685v1 Announce Type: new 
Abstract: Acute myeloid leukaemia (AML) is a type of blood and bone marrow cancer characterized by the proliferation of abnormal clonal haematopoietic cells in the bone marrow leading to bone marrow failure. Over the course of the disease, angiogenic factors released by leukaemic cells drastically alter the bone marrow vascular niches resulting in observable structural abnormalities. We use a technique from topological data analysis - persistent homology - to quantify the images and infer on the disease through the imaged morphological features. We find that persistent homology uncovers succinct dissimilarities between the control, early, and late stages of AML development. We then integrate persistent homology into stage-dependent Gaussian mixture models for the first time, proposing a new class of models which are applicable to persistent homology summaries and able to both infer patterns in morphological changes between different stages of progression as well as provide a basis for prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13685v1</guid>
      <category>stat.AP</category>
      <category>math.AT</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiquan Wang, Anna Song, Antoniana Batsivari, Dominique Bonnet, Anthea Monod</dc:creator>
    </item>
    <item>
      <title>Examining Differential Item Functioning (DIF) in Self-Reported Health Survey Data: Via Multilevel Modeling</title>
      <link>https://arxiv.org/abs/2408.13702</link>
      <description>arXiv:2408.13702v1 Announce Type: new 
Abstract: Few health-related constructs or measures have received critical evaluation in terms of measurement equivalence, such as self-reported health survey data. Differential item functioning (DIF) analysis is crucial for evaluating measurement equivalence in self-reported health surveys, which are often hierarchical in structure. While traditional DIF methods rely on single-level models, multilevel models offer a more suitable alternative for analyzing such data. In this article, we highlight the advantages of multilevel modeling in DIF analysis and demonstrate how to apply the DIF framework to self-reported health survey data using multilevel models. For demonstration, we analyze DIF associated with population density on the probability to answer "Yes" to a survey question on depression and reveal that multilevel models achieve better fit and account for more variance compared to single-level models. This article is expected to increase awareness of the usefulness of multilevel modeling for DIF analysis and assist healthcare researchers and practitioners in improving the understanding of self-reported health survey data validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13702v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dandan Chen Kaptur, Yiqing Liu, Bradley Kaptur, Nicholas Peterman, Jinming Zhang, Justin Kern, Carolyn Anderson</dc:creator>
    </item>
    <item>
      <title>Estimation of time-varying recovery and death rates from epidemiological data: A new approach</title>
      <link>https://arxiv.org/abs/2408.13872</link>
      <description>arXiv:2408.13872v1 Announce Type: new 
Abstract: The time-to-recovery or time-to-death for various infectious diseases can vary significantly among individuals, influenced by several factors such as demographic differences, immune strength, medical history, age, pre-existing conditions, and infection severity. To capture these variations, time-since-infection dependent recovery and death rates offer a detailed description of the epidemic. However, obtaining individual-level data to estimate these rates is challenging, while aggregate epidemiological data (such as the number of new infections, number of active cases, number of new recoveries, and number of new deaths) are more readily available. In this article, a new methodology is proposed to estimate time-since-infection dependent recovery and death rates using easily available data sources, accommodating irregular data collection timings reflective of real-world reporting practices. The Nadaraya-Watson estimator is utilized to derive the number of new infections. This model improves the accuracy of epidemic progression descriptions and provides clear insights into recovery and death distributions. The proposed methodology is validated using COVID-19 data and its general applicability is demonstrated by applying it to some other diseases like measles and typhoid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13872v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samiran Ghosh, Malay Banerjee, Subhra Sankar Dhar, Siuli Mukhopadhyay</dc:creator>
    </item>
    <item>
      <title>Bayesian Cointegrated Panels in Digital Marketing</title>
      <link>https://arxiv.org/abs/2408.14012</link>
      <description>arXiv:2408.14012v1 Announce Type: new 
Abstract: In this paper, we fully develop and apply a novel extension of Bayesian cointegrated panels modeling in digital marketing, particularly in modeling of a system where key ROI metrics such as clicks or impressions of a given digital campaign considered. Thus, in this context our goal is evaluating how the system reacts to investment perturbations due to changes in the investment strategy and its impact on the visibility of specific campaigns. To do so, we fit the model using a set of real marketing data with different investment campaigns over the same geographic territory. By employing forecast error variance decomposition, our findings indicate that clicks and impressions have a significant impact on session generation. Also, we evaluate our approach through a comprehensive simulation study that considers different processes. The results indicate that our proposal has substantial capabilities in terms of estimability and accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14012v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan David Carranza-S\'anchez, Juan Sosa</dc:creator>
    </item>
    <item>
      <title>Evaluating the effectiveness of public policies on COVID-19 containment: A PSM-DID approach</title>
      <link>https://arxiv.org/abs/2408.14108</link>
      <description>arXiv:2408.14108v1 Announce Type: new 
Abstract: The implementation of public policies is crucial in controlling the spread of COVID-19. However, the effectiveness of different policies can vary across different aspects of epidemic containment. Identifying the most effective policies is essential for providing informed recommendations for pandemic control. This paper examines the relationship between various public policy responses and their impact on COVID-19 containment. Using the propensity score matching-difference in differences (PSM-DID) model to address endogeneity, we analyze the causal significance of each policy on epidemic control. Our analysis reveals that that policies related to vaccine delivery, debt relief, and the cancellation of public events are the most effective measures. These findings provide key insights for policymakers, highlighting the importance of focusing on specific, high-impact measures in managing public health crises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14108v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihan Wang</dc:creator>
    </item>
    <item>
      <title>Ridge, lasso, and elastic-net estimations of the modified Poisson and least-squares regressions for binary outcome data</title>
      <link>https://arxiv.org/abs/2408.13474</link>
      <description>arXiv:2408.13474v1 Announce Type: cross 
Abstract: Logistic regression is a standard method in multivariate analysis for binary outcome data in epidemiological and clinical studies; however, the resultant odds-ratio estimates fail to provide directly interpretable effect measures. The modified Poisson and least-squares regressions are alternative standard methods that can provide risk-ratio and risk difference estimates without computational problems. However, the bias and invalid inference problems of these regression analyses under small or sparse data conditions (i.e.,the "separation" problem) have been insufficiently investigated. We show that the separation problem can adversely affect the inferences of the modified Poisson and least squares regressions, and to address these issues, we apply the ridge, lasso, and elastic-net estimating approaches to the two regression methods. As the methods are not founded on the maximum likelihood principle, we propose regularized quasi-likelihood approaches based on the estimating equations for these generalized linear models. The methods provide stable shrinkage estimates of risk ratios and risk differences even under separation conditions, and the lasso and elastic-net approaches enable simultaneous variable selection. We provide a bootstrap method to calculate the confidence intervals on the basis of the regularized quasi-likelihood estimation. The proposed methods are applied to a hematopoietic stem cell transplantation cohort study and the National Child Development Survey. We also provide an R package, regconfint, to implement these methods with simple commands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13474v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takahiro Kitano, Hisashi Noma</dc:creator>
    </item>
    <item>
      <title>Influence Networks: Bayesian Modeling and Diffusion</title>
      <link>https://arxiv.org/abs/2408.13606</link>
      <description>arXiv:2408.13606v1 Announce Type: cross 
Abstract: In this article, we make an innovative adaptation of a Bayesian latent space model based on projections in a novel way to analyze influence networks. By appropriately reparameterizing the model, we establish a formal metric for quantifying each individual's influencing capacity and estimating their latent position embedded in a social space. This modeling approach introduces a novel mechanism for fully characterizing the diffusion of an idea based on the estimated latent characteristics. It assumes that each individual takes the following states: Unknown, undecided, supporting, or rejecting an idea. This approach is demonstrated using a influence network from Twitter (now $\mathbb{X}$) related to the 2022 Tax Reform in Colombia. An exhaustive simulation exercise is also performed to evaluate the proposed diffusion process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13606v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel S\'anchez-Guti\'errez, Juan Sosa, Carolina Luque</dc:creator>
    </item>
    <item>
      <title>Enhancing Uplift Modeling in Multi-Treatment Marketing Campaigns: Leveraging Score Ranking and Calibration Techniques</title>
      <link>https://arxiv.org/abs/2408.13628</link>
      <description>arXiv:2408.13628v2 Announce Type: cross 
Abstract: Uplift modeling is essential for optimizing marketing strategies by selecting individuals likely to respond positively to specific marketing campaigns. This importance escalates in multi-treatment marketing campaigns, where diverse treatment is available and we may want to assign the customers to treatment that can make the most impact. While there are existing approaches with convenient frameworks like Causalml, there are potential spaces to enhance the effect of uplift modeling in multi treatment cases. This paper introduces a novel approach to uplift modeling in multi-treatment campaigns, leveraging score ranking and calibration techniques to improve overall performance of the marketing campaign. We review existing uplift models, including Meta Learner frameworks (S, T, X), and their application in real-world scenarios. Additionally, we delve into insights from multi-treatment studies to highlight the complexities and potential advancements in the field. Our methodology incorporates Meta-Learner calibration and a scoring rank-based offer selection strategy. Extensive experiment results with real-world datasets demonstrate the practical benefits and superior performance of our approach. The findings underscore the critical role of integrating score ranking and calibration techniques in refining the performance and reliability of uplift predictions, thereby advancing predictive modeling in marketing analytics and providing actionable insights for practitioners seeking to optimize their campaign strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13628v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoon Tae Park, Ting Xu, Mohamed Anany</dc:creator>
    </item>
    <item>
      <title>On the minimum strength of (unobserved) covariates to overturn an insignificant result</title>
      <link>https://arxiv.org/abs/2408.13901</link>
      <description>arXiv:2408.13901v1 Announce Type: cross 
Abstract: We study conditions under which the addition of variables to a regression equation can turn a previously statistically insignificant result into a significant one. Specifically, we characterize the minimum strength of association required for these variables--both with the dependent and independent variables, or with the dependent variable alone--to elevate the observed t-statistic above a specified significance threshold. Interestingly, we show that it is considerably difficult to overturn a statistically insignificant result solely by reducing the standard error. Instead, included variables must also alter the point estimate to achieve such reversals in practice. Our results can be used for sensitivity analysis and for bounding the extent of p-hacking, and may also offer algebraic explanations for patterns of reversals seen in empirical research, such as those documented by Lenz and Sahn (2021).</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13901v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danielle Tsao, Ronan Perry, Carlos Cinelli</dc:creator>
    </item>
    <item>
      <title>Bayesian functional data analysis in astronomy</title>
      <link>https://arxiv.org/abs/2408.14466</link>
      <description>arXiv:2408.14466v1 Announce Type: cross 
Abstract: Cosmic demographics -- the statistical study of populations of astrophysical objects -- has long relied on *multivariate statistics*, providing methods for analyzing data comprising fixed-length vectors of properties of objects, as might be compiled in a tabular astronomical catalog (say, with sky coordinates, and brightness measurements in a fixed number of spectral passbands). But beginning with the emergence of automated digital sky surveys, ca. ~2000, astronomers began producing large collections of data with more complex structure: light curves (brightness time series) and spectra (brightness vs. wavelength). These comprise what statisticians call *functional data* -- measurements of populations of functions. Upcoming automated sky surveys will soon provide astronomers with a flood of functional data. New methods are needed to accurately and optimally analyze large ensembles of light curves and spectra, accumulating information both along and across measured functions. Functional data analysis (FDA) provides tools for statistical modeling of functional data. Astronomical data presents several challenges for FDA methodology, e.g., sparse, irregular, and asynchronous sampling, and heteroscedastic measurement error. Bayesian FDA uses hierarchical Bayesian models for function populations, and is well suited to addressing these challenges. We provide an overview of astronomical functional data, and of some key Bayesian FDA modeling approaches, including functional mixed effects models, and stochastic process models. We briefly describe a Bayesian FDA framework combining FDA and machine learning methods to build low-dimensional parametric models for galaxy spectra.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14466v1</guid>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Loredo, Tamas Budavari, David Kent, David Ruppert</dc:creator>
    </item>
    <item>
      <title>Capturing usage patterns in bike sharing system via multilayer network fused Lasso</title>
      <link>https://arxiv.org/abs/2208.08150</link>
      <description>arXiv:2208.08150v5 Announce Type: replace 
Abstract: Data collected from a bike-sharing system exhibit complex temporal and spatial features. We analyze shared-bike usage data collected in three large cities at the level of individual stations, accounting for station-specific behavior and covariate effects. For this, we adopt a penalized regression approach with a multilayer network fused Lasso penalty. These fusion penalties are imposed on networks which embed spatio-temporal linkages, and capture the homogeneity in bike usage that is attributed to intricate spatio-temporal features without arbitrarily partitioning the data. On the real-life datasets, we demonstrate that the proposed approach yields competitive predictive performance and provides a new interpretation of the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.08150v5</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunjin Choi, Haeran Cho, Hyelim Son</dc:creator>
    </item>
    <item>
      <title>How to identify earth pressures on in-service tunnel linings: Insights from Bayesian inversion to address non-uniqueness</title>
      <link>https://arxiv.org/abs/2402.15217</link>
      <description>arXiv:2402.15217v3 Announce Type: replace 
Abstract: Identifying earth pressures on in-service transportation tunnel linings is essential for their health monitoring and performance prediction, particularly in structures that exhibit poor performance. Due to the high costs associated with pressure gauges, pressure inversion based on easily observed structural responses, such as deformations, is preferred. A significant challenge lies in the non-uniqueness of inversion results, where various pressures can yield similar structural responses. Existing approaches often overlook detailed discussions on this critical issue. In addressing this gap, this study introduces a Bayesian approach. The proposed statistical framework effectively quantifies the uncertainty induced by non-uniqueness. Further analysis identifies the uniform component in distributed pressures as the primary source of non-uniqueness. Insights into mitigation strategies are provided, including increasing the quantity of deformation data or incorporating an observation of internal normal force within the tunnel lining -- the latter proving to be notably more effective. A practical application in a numerical case study demonstrates the effectiveness of this approach. In addition, our investigation recommends maintaining deformation measurement accuracy within the range of [-1, 1] mm to ensure satisfactory outcomes. Finally, deficiencies and potential future extensions of this approach are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15217v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.trgeo.2024.101344</arxiv:DOI>
      <dc:creator>Zhiyao Tian, Shunhua Zhou, Anthony Lee, Yao Shan, Bettina Detmann</dc:creator>
    </item>
    <item>
      <title>From day-ahead to mid and long-term horizons with econometric electricity price forecasting models</title>
      <link>https://arxiv.org/abs/2406.00326</link>
      <description>arXiv:2406.00326v2 Announce Type: replace 
Abstract: The recent energy crisis starting in 2021 led to record-high gas, coal, carbon and power prices, with electricity reaching up to 40 times the pre-crisis average. This had dramatic consequences for operational and risk management prompting the need for robust econometric models for mid to long-term electricity price forecasting. After a comprehensive literature analysis, we identify key challenges and address them with novel approaches: 1) Fundamental information is incorporated by constraining coefficients with bounds derived from fundamental models offering interpretability; 2) Short-term regressors such as load and renewables can be used in long-term forecasts by incorporating their seasonal expectations to stabilize the model; 3) Unit root behavior of power prices, induced by fuel prices, can be managed by estimating same-day relationships and projecting them forward. We develop interpretable models for a range of forecasting horizons from one day to one year ahead, providing guidelines on robust modeling frameworks and key explanatory variables for each horizon. Our study, focused on Europe's largest energy market, Germany, analyzes hourly electricity prices using regularized regression methods and generalized additive models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00326v2</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Ghelasi, Florian Ziel</dc:creator>
    </item>
    <item>
      <title>Coarse Personalization</title>
      <link>https://arxiv.org/abs/2204.05793</link>
      <description>arXiv:2204.05793v3 Announce Type: replace-cross 
Abstract: Advances in estimating heterogeneous treatment effects enable firms to personalize marketing mix elements and target individuals at an unmatched level of granularity, but feasibility constraints limit such personalization. In practice, firms choose which unique treatments to offer and which individuals to offer these treatments with the goal of maximizing profits: we call this the coarse personalization problem. We propose a two-step solution that makes segmentation and targeting decisions in concert. First, the firm personalizes by estimating conditional average treatment effects. Second, the firm discretizes by utilizing treatment effects to choose which unique treatments to offer and who to assign to these treatments. We show that a combination of available machine learning tools for estimating heterogeneous treatment effects and a novel application of optimal transport methods provides a viable and efficient solution. With data from a large-scale field experiment for promotions management, we find that our methodology outperforms extant approaches that segment on consumer characteristics or preferences and those that only search over a prespecified grid. Using our procedure, the firm recoups over 99.5% of its expected incremental profits under fully granular personalization while offering only five unique treatments. We conclude by discussing how coarse personalization arises in other domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.05793v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Walter W. Zhang, Sanjog Misra</dc:creator>
    </item>
    <item>
      <title>Asymptotically Normal Estimation of Local Latent Network Curvature</title>
      <link>https://arxiv.org/abs/2211.11673</link>
      <description>arXiv:2211.11673v4 Announce Type: replace-cross 
Abstract: Network data, commonly used throughout the physical, social, and biological sciences, consist of nodes (individuals) and the edges (interactions) between them. One way to represent network data's complex, high-dimensional structure is to embed the graph into a low-dimensional geometric space. The curvature of this space, in particular, provides insights about the structure in the graph, such as the propensity to form triangles or present tree-like structures. We derive an estimating function for curvature based on triangle side lengths and the length of the midpoint of a side to the opposing corner. We construct an estimator where the only input is a distance matrix and also establish asymptotic normality. We next introduce a novel latent distance matrix estimator for networks and an efficient algorithm to compute the estimate via solving iterative quadratic programs. We apply this method to the Los Alamos National Laboratory Unified Network and Host dataset and show how curvature estimates can be used to detect a red-team attack faster than naive methods, as well as discover non-constant latent curvature in co-authorship networks in physics. The code for this paper is available at https://github.com/SteveJWR/netcurve, and the methods are implemented in the R package https://github.com/SteveJWR/lolaR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.11673v4</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven Wilkins-Reeves, Tyler McCormick</dc:creator>
    </item>
    <item>
      <title>$\ell_1$-penalized Multinomial Regression: Estimation, inference, and prediction, with an application to risk factor identification for different dementia subtypes</title>
      <link>https://arxiv.org/abs/2302.02310</link>
      <description>arXiv:2302.02310v2 Announce Type: replace-cross 
Abstract: High-dimensional multinomial regression models are very useful in practice but have received less research attention than logistic regression models, especially from the perspective of statistical inference. In this work, we analyze the estimation and prediction error of the contrast-based $\ell_1$-penalized multinomial regression model and extend the debiasing method to the multinomial case, providing a valid confidence interval for each coefficient and $p$-value of the individual hypothesis test. We also examine cases of model misspecification and non-identically distributed data to demonstrate the robustness of our method when some assumptions are violated. We apply the debiasing method to identify important predictors in the progression into dementia of different subtypes. Results from extensive simulations show the superiority of the debiasing method compared to other inference methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.02310v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Tian, Henry Rusinek, Arjun V. Masurkar, Yang Feng</dc:creator>
    </item>
    <item>
      <title>An Information-Theoretic Approach for Detecting Edits in AI-Generated Text</title>
      <link>https://arxiv.org/abs/2308.12747</link>
      <description>arXiv:2308.12747v2 Announce Type: replace-cross 
Abstract: We propose a method to determine whether a given article was written entirely by a generative language model or perhaps contains edits by a different author, possibly a human. Our process involves multiple tests for the origin of individual sentences or other pieces of text and combining these tests using a method that is sensitive to rare alternatives, i.e., non-null effects are few and scattered across the text in unknown locations. Interestingly, this method also identifies pieces of text suspected to contain edits. We demonstrate the effectiveness of the method in detecting edits through extensive evaluations using real data and provide an information-theoretic analysis of the factors affecting its success. In particular, we discuss optimality properties under a theoretical framework for text editing saying that sentences are generated mainly by the language model, except perhaps for a few sentences that might have originated via a different mechanism. Our analysis raises several interesting research questions at the intersection of information theory and data science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12747v2</guid>
      <category>cs.IT</category>
      <category>cs.AI</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1162/99608f92.5dbf3265</arxiv:DOI>
      <dc:creator>Idan Kashtan, Alon Kipnis</dc:creator>
    </item>
    <item>
      <title>Who Are We Missing? A Principled Approach to Characterizing the Underrepresented Population</title>
      <link>https://arxiv.org/abs/2401.14512</link>
      <description>arXiv:2401.14512v4 Announce Type: replace-cross 
Abstract: Randomized controlled trials (RCTs) serve as the cornerstone for understanding causal effects, yet extending inferences to target populations presents challenges due to effect heterogeneity and underrepresentation. Our paper addresses the critical issue of identifying and characterizing underrepresented subgroups in RCTs, proposing a novel framework for refining target populations to improve generalizability. We introduce an optimization-based approach, Rashomon Set of Optimal Trees (ROOT), to characterize underrepresented groups. ROOT optimizes the target subpopulation distribution by minimizing the variance of the target average treatment effect estimate, ensuring more precise treatment effect estimations. Notably, ROOT generates interpretable characteristics of the underrepresented population, aiding researchers in effective communication. Our approach demonstrates improved precision and interpretability compared to alternatives, as illustrated with synthetic data experiments. We apply our methodology to extend inferences from the Starting Treatment with Agonist Replacement Therapies (START) trial -- investigating the effectiveness of medication for opioid use disorder -- to the real-world population represented by the Treatment Episode Dataset: Admissions (TEDS-A). By refining target populations using ROOT, our framework offers a systematic approach to enhance decision-making accuracy and inform future trials in diverse populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14512v4</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Harsh Parikh, Rachael Ross, Elizabeth Stuart, Kara Rudolph</dc:creator>
    </item>
    <item>
      <title>Unveiling Nonlinear Dynamics in Catastrophe Bond Pricing: A Machine Learning Perspective</title>
      <link>https://arxiv.org/abs/2405.00697</link>
      <description>arXiv:2405.00697v2 Announce Type: replace-cross 
Abstract: This paper explores the implications of using machine learning models in the pricing of catastrophe (CAT) bonds. By integrating advanced machine learning techniques, our approach uncovers nonlinear relationships and complex interactions between key risk factors and CAT bond spreads -- dynamics that are often overlooked by traditional linear regression models. Using primary market CAT bond transaction records between January 1999 and March 2021, our findings demonstrate that machine learning models not only enhance the accuracy of CAT bond pricing but also provide a deeper understanding of how various risk factors interact and influence bond prices in a nonlinear way. These findings suggest that investors and issuers can benefit from incorporating machine learning to better capture the intricate interplay between risk factors when pricing CAT bonds. The results also highlight the potential for machine learning models to refine our understanding of asset pricing in markets characterized by complex risk structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00697v2</guid>
      <category>q-fin.CP</category>
      <category>cs.LG</category>
      <category>q-fin.PR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaowei Chen, Hong Li, Yufan Lu, Rui Zhou</dc:creator>
    </item>
    <item>
      <title>Tackling GenAI Copyright Issues: Originality Estimation and Genericization</title>
      <link>https://arxiv.org/abs/2406.03341</link>
      <description>arXiv:2406.03341v4 Announce Type: replace-cross 
Abstract: The rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers. While various techniques for mitigating copyright issues have been studied, significant risks remain. Here, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to infringe copyright. To achieve this, we introduce a metric for quantifying the level of originality of data in a manner that is consistent with the legal framework. This metric can be practically estimated by drawing samples from a generative model, which is then used for the genericization process. As a practical implementation, we introduce PREGen, which combines our genericization method with an existing mitigation technique. Experiments demonstrate that our genericization method successfully modifies the output of a text-to-image generative model so that it produces more generic, copyright-compliant images. Compared to the existing method, PREGen reduces the likelihood of generating copyrighted characters by more than half when the names of copyrighted characters are used as the prompt, dramatically improving the performance. Additionally, while generative models can produce copyrighted characters even when their names are not directly mentioned in the prompt, PREGen almost entirely prevents the generation of such characters in these cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03341v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroaki Chiba-Okabe, Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>Bayesian Rank-Clustering</title>
      <link>https://arxiv.org/abs/2406.19563</link>
      <description>arXiv:2406.19563v2 Announce Type: replace-cross 
Abstract: Traditional statistical inference on ordinal comparison data results in an overall ranking of objects, e.g., from best to worst, with each object having a unique rank. However, ranks of some objects may not be statistically distinguishable. This could happen due to insufficient data or to the true underlying object qualities being equal. Because uncertainty communication in estimates of overall rankings is notoriously difficult, we take a different approach and allow groups of objects to have equal ranks or be $\textit{rank-clustered}$ in our model. Existing models related to rank-clustering are limited by their inability to handle a variety of ordinal data types, to quantify uncertainty, or by the need to pre-specify the number and size of potential rank-clusters. We solve these limitations through our proposed Bayesian $\textit{Rank-Clustered Bradley-Terry-Luce}$ model. We accommodate rank-clustering via parameter fusion by imposing a novel spike-and-slab prior on object-specific worth parameters in Bradley-Terry-Luce family of distributions for ordinal comparisons. We demonstrate rank-clustering on simulated and real datasets in surveys, elections, and sports analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19563v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Michael Pearce, Elena A. Erosheva</dc:creator>
    </item>
    <item>
      <title>AI-Powered Energy Algorithmic Trading: Integrating Hidden Markov Models with Neural Networks</title>
      <link>https://arxiv.org/abs/2407.19858</link>
      <description>arXiv:2407.19858v5 Announce Type: replace-cross 
Abstract: In quantitative finance, machine learning methods are essential for alpha generation. This study introduces a new approach that combines Hidden Markov Models (HMM) and neural networks, integrated with Black-Litterman portfolio optimization. During the COVID period (2019-2022), this dual-model approach achieved a 83% return with a Sharpe ratio of 0.77. It incorporates two risk models to enhance risk management, showing efficiency during volatile periods. The methodology was implemented on the QuantConnect platform, which was chosen for its robust framework and experimental reproducibility. The system, which predicts future price movements, includes a three-year warm-up to ensure proper algorithm function. It targets highly liquid, large-cap energy stocks to ensure stable and predictable performance while also considering broker payments. The dual-model alpha system utilizes log returns to select the optimal state based on the historical performance. It combines state predictions with neural network outputs, which are based on historical data, to generate trading signals. This study examined the architecture of the trading system, data pre-processing, training, and performance. The full code and backtesting data are available under the QuantConnect terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19858v5</guid>
      <category>q-fin.PM</category>
      <category>cs.LG</category>
      <category>q-fin.GN</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tiago Monteiro</dc:creator>
    </item>
  </channel>
</rss>
