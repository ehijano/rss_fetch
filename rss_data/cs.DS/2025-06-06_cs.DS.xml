<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Jun 2025 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Rumors on evolving graphs through stationary times</title>
      <link>https://arxiv.org/abs/2506.04386</link>
      <description>arXiv:2506.04386v1 Announce Type: new 
Abstract: We study rumor spreading in dynamic random graphs. Starting with a single informed vertex, the information flows until it reaches all the vertices of the graph (completion), according to the following process. At each step $k$, the information is propagated to neighbors of the informed vertices, in the $k$-th generated random graph. The way this information propagates from vertex to vertex at each step will depend on the ``protocol". We provide a method based on strong stationary times to study the completion time when the graphs are Markovian time dependent, using known results of the literature for independent graphs. The concept of strong stationary times is then extended to non-Markovian Dynamics using coupling from the past algorithms. This allows to extend results on completion times for non-Markov dynamics</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04386v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.PR</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vicenzo Bonasorte</dc:creator>
    </item>
    <item>
      <title>Faster MPC Algorithms for Approximate Allocation in Uniformly Sparse Graphs</title>
      <link>https://arxiv.org/abs/2506.04524</link>
      <description>arXiv:2506.04524v1 Announce Type: new 
Abstract: We study the allocation problem in the Massively Parallel Computation (MPC) model. This problem is a special case of $b$-matching, in which the input is a bipartite graph with capacities greater than $1$ in only one part of the bipartition. We give a $(1+\epsilon)$ approximate algorithm for the problem, which runs in $\tilde{O}(\sqrt{\log \lambda})$ MPC rounds, using sublinear space per machine and $\tilde{O}(\lambda n)$ total space, where $\lambda$ is the arboricity of the input graph. Our result is obtained by providing a new analysis of a LOCAL algorithm by Agrawal, Zadimoghaddam, and Mirrokni [ICML 2018], which improves its round complexity from $O(\log n)$ to $O(\log \lambda)$. Prior to our work, no $o(\log n)$ round algorithm for constant-approximate allocation was known in either LOCAL or sublinear space MPC models for graphs with low arboricity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04524v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakub {\L}\k{a}cki, Slobodan Mitrovi\'c, Srikkanth Ramachandran, Wen-Horng Sheu</dc:creator>
    </item>
    <item>
      <title>Online matching on stochastic block model</title>
      <link>https://arxiv.org/abs/2506.04921</link>
      <description>arXiv:2506.04921v1 Announce Type: new 
Abstract: While online bipartite matching has gained significant attention in recent years, existing analyses in stochastic settings fail to capture the performance of algorithms on heterogeneous graphs, such as those incorporating inter-group affinities or other social network structures. In this work, we address this gap by studying online bipartite matching within the stochastic block model (SBM). A fixed set of offline nodes is matched to a stream of online arrivals, with connections governed probabilistically by latent class memberships. We analyze two natural algorithms: a $\tt{Myopic}$ policy that greedily matches each arrival to the most compatible class, and the $\tt{Balance}$ algorithm, which accounts for both compatibility and remaining capacity. For the $\tt{Myopic}$ algorithm, we prove that the size of the matching converges, with high probability, to the solution of an ordinary differential equation (ODE), for which we provide a tractable approximation along with explicit error bounds. For the $\tt{Balance}$ algorithm, we demonstrate convergence of the matching size to a differential inclusion and derive an explicit limiting solution. Lastly, we explore the impact of estimating the connection probabilities between classes online, which introduces an exploration-exploitation trade-off.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04921v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria Cherifa (MAP5, CREST), Cl\'ement Calauz\`enes (CREST), Vianney Perchet (CREST)</dc:creator>
    </item>
    <item>
      <title>Decomposing Words for Enhanced Compression: Exploring the Number of Runs in the Extended Burrows-Wheeler Transform</title>
      <link>https://arxiv.org/abs/2506.04926</link>
      <description>arXiv:2506.04926v1 Announce Type: new 
Abstract: The Burrows-Wheeler Transform (BWT) is a fundamental component in many data structures for text indexing and compression, widely used in areas such as bioinformatics and information retrieval. The extended BWT (eBWT) generalizes the classical BWT to multisets of strings, providing a flexible framework that captures many BWT-like constructions. Several known variants of the BWT can be viewed as instances of the eBWT applied to specific decompositions of a word. A central property of the BWT, essential for its compressibility, is the number of maximal ranges of equal letters, named runs. In this article, we explore how different decompositions of a word impact the number of runs in the resulting eBWT. First, we show that the number of decompositions of a word is exponential, even under minimal constraints on the size of the subsets in the decomposition. Second, we present an infinite family of words for which the ratio of the number of runs between the worst and best decompositions is unbounded, under the same minimal constraints. These results illustrate the potential cost of decomposition choices in eBWT-based compression and underline the challenges in optimizing run-length encoding in generalized BWT frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04926v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.FL</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Ingels, Ana\"is Denis, Bastien Cazaux</dc:creator>
    </item>
    <item>
      <title>Resilient Pattern Mining</title>
      <link>https://arxiv.org/abs/2506.04935</link>
      <description>arXiv:2506.04935v1 Announce Type: new 
Abstract: Frequent pattern mining is a flagship problem in data mining. In its most basic form, it asks for the set of substrings of a given string $S$ of length $n$ that occur at least $\tau$ times in $S$, for some integer $\tau\in[1,n]$. We introduce a resilient version of this classic problem, which we term the $(\tau, k)$-Resilient Pattern Mining (RPM) problem. Given a string $S$ of length $n$ and two integers $\tau, k\in[1,n]$, RPM asks for the set of substrings of $S$ that occur at least $\tau$ times in $S$, even when the letters at any $k$ positions of $S$ are substituted by other letters. Unlike frequent substrings, resilient ones account for the fact that changes to string $S$ are often expensive to handle or are unknown.
  We propose an exact $\mathcal{O}(n\log n)$-time and $\mathcal{O}(n)$-space algorithm for RPM, which employs advanced data structures and combinatorial insights. We then present experiments on real large-scale datasets from different domains demonstrating that: (I) The notion of resilient substrings is useful in analyzing genomic data and is more powerful than that of frequent substrings, in scenarios where resilience is required, such as in the case of versioned datasets; (II) Our algorithm is several orders of magnitude faster and more space-efficient than a baseline algorithm that is based on dynamic programming; and (III) Clustering based on resilient substrings is effective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04935v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pengxin Bian, Panagiotis Charalampopoulos, Lorraine A. K. Ayad, Manal Mohamed, Solon P. Pissis, Grigorios Loukides</dc:creator>
    </item>
    <item>
      <title>Compressing Hypergraphs using Suffix Sorting</title>
      <link>https://arxiv.org/abs/2506.05023</link>
      <description>arXiv:2506.05023v1 Announce Type: new 
Abstract: Hypergraphs model complex, non-binary relationships like co-authorships, social group memberships, and recommendations. Like traditional graphs, hypergraphs can grow large, posing challenges for storage, transmission, and query performance. We propose HyperCSA, a novel compression method for hypergraphs that maintains support for standard queries over the succinct representation. HyperCSA achieves compression ratios of 26% to 79% of the original file size on real-world hypergraphs - outperforming existing methods on all large hypergraphs in our experiments. Additionally, HyperCSA scales to larger datasets than existing approaches. Furthermore, for common real-world hypergraphs, HyperCSA evaluates neighbor queries 6 to 40 times faster than both standard data structures and other hypergraph compression approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05023v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enno Adler, Stefan B\"ottcher, Rita Hartel</dc:creator>
    </item>
    <item>
      <title>On Minimizers of Minimum Density</title>
      <link>https://arxiv.org/abs/2506.05277</link>
      <description>arXiv:2506.05277v1 Announce Type: new 
Abstract: Minimizers are sampling schemes with numerous applications in computational biology. Assuming a fixed alphabet of size $\sigma$, a minimizer is defined by two integers $k,w\ge2$ and a linear order $\rho$ on strings of length $k$ (also called $k$-mers). A string is processed by a sliding window algorithm that chooses, in each window of length $w+k-1$, its minimal $k$-mer with respect to $\rho$. A key characteristic of the minimizer is its density, which is the expected frequency of chosen $k$-mers among all $k$-mers in a random infinite $\sigma$-ary string. Minimizers of smaller density are preferred as they produce smaller samples with the same guarantee: each window is represented by a $k$-mer.
  The problem of finding a minimizer of minimum density for given input parameters $(\sigma,k,w)$ has a huge search space of $(\sigma^k)!$ and is representable by an ILP of size $\tilde\Theta(\sigma^{k+w})$, which has worst-case solution time that is doubly-exponential in $(k+w)$ under standard complexity assumptions. We solve this problem in $w\cdot 2^{\sigma^k+O(k)}$ time and provide several additional tricks reducing the practical runtime and search space. As a by-product, we describe an algorithm computing the average density of a minimizer within the same time bound. Then we propose a novel method of studying minimizers via regular languages and show how to find, via the eigenvalue/eigenvector analysis over finite automata, minimizers with the minimal density in the asymptotic case $w\to\infty$. Implementing our algorithms, we compute the minimum density minimizers for $(\sigma,k)\in\{(2,2),(2,3),(2,4),(2,5),(4,2)\}$ and \textbf{all} $w\ge 2$. The obtained densities are compared against the average density and the theoretical lower bounds, including the new bound presented in this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05277v1</guid>
      <category>cs.DS</category>
      <category>cs.FL</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arseny Shur</dc:creator>
    </item>
    <item>
      <title>Identity Testing for Circuits with Exponentiation Gates</title>
      <link>https://arxiv.org/abs/2506.04529</link>
      <description>arXiv:2506.04529v1 Announce Type: cross 
Abstract: Motivated by practical applications in the design of optimization compilers for neural networks, we initiated the study of identity testing problems for arithmetic circuits augmented with \emph{exponentiation gates} that compute the real function $x\mapsto e^x$. These circuits compute real functions of form $P(\vec x)/P'(\vec x)$, where both $P(\vec x)$ and $P'(\vec x)$ are exponential polynomials
  \[
  \sum_{i=1}^k f_i(\vec x)\cdot \exp\left(\frac{g_i(\vec x)}{h_i(\vec x)}\right),
  \]
  for polynomials $f_i(\vec x),g_i(\vec x)$, and $h_i(\vec x)$.
  We formalize a black-box query model over finite fields for this class of circuits, which is mathematical simple and reflects constraints faced by real-world neural network compilers. We proved that a simple and efficient randomized identity testing algorithm achieves perfect completeness and non-trivial soundness. Concurrent with our work, the algorithm has been implemented in the optimization compiler Mirage by Wu et al.~(OSDI 2025), demonstrating promising empirical performance in both efficiency and soundness error. Finally, we propose a number-theoretic conjecture under which our algorithm is sound with high probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04529v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiatu Li, Mengdi Wu</dc:creator>
    </item>
    <item>
      <title>Improved Byzantine Agreement under an Adaptive Adversary</title>
      <link>https://arxiv.org/abs/2506.04919</link>
      <description>arXiv:2506.04919v1 Announce Type: cross 
Abstract: Byzantine agreement is a fundamental problem in fault-tolerant distributed computing that has been studied intensively for the last four decades. Much of the research has focused on a static Byzantine adversary, where the adversary is constrained to choose the Byzantine nodes in advance of the protocol's execution. This work focuses on the harder case of an adaptive Byzantine adversary that can choose the Byzantine nodes \emph{adaptively} based on the protocol's execution. While efficient $O(\log n)$-round protocols ($n$ is the total number of nodes) are known for the static adversary (Goldwasser, Pavlov, and Vaikuntanathan, FOCS 2006) tolerating up to $t &lt; n/(3+\epsilon)$ Byzantine nodes, $\Omega(t/\sqrt{n \log n})$ rounds is a well-known lower bound for adaptive adversary [Bar-Joseph and Ben-Or, PODC 1998]. The best-known protocol for adaptive adversary runs in $O(t/\log n)$ rounds [Chor and Coan, IEEE Trans. Soft. Engg., 1985].
  This work presents a synchronous randomized Byzantine agreement protocol under an adaptive adversary that improves over previous results. Our protocol works under the powerful \emph{adaptive rushing adversary in the full information model}. That is, we assume that the Byzantine nodes can behave arbitrarily and maliciously, have knowledge about the entire state of the network at every round, including random choices made by all the nodes up to and including the current round, have unlimited computational power, and may collude among themselves. Furthermore, the adversary can \emph{adaptively} corrupt up to $t &lt; n/3$ nodes based on the protocol's execution. We present a simple randomized Byzantine agreement protocol that runs in $O(\min\{t^2\log n/n, t/\log n\})$ rounds that improves over the long-standing bound of $O(t/\log n)$ rounds due to Chor and Coan [IEEE Trans. Soft. Engg., 1985].</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04919v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabien Dufoulon, Gopal Pandurangan</dc:creator>
    </item>
    <item>
      <title>Memory Hierarchy Design for Caching Middleware in the Age of NVM</title>
      <link>https://arxiv.org/abs/2506.05071</link>
      <description>arXiv:2506.05071v1 Announce Type: cross 
Abstract: Advances in storage technology have introduced Non-Volatile Memory, NVM, as a new storage medium. NVM, along with Dynamic Random Access Memory (DRAM), Solid State Disk (SSD), and Disk present a system designer with a wide array of options in designing caching middleware. Moreover, design decisions to replicate a data item in more than one level of a caching memory hierarchy may enhance the overall system performance with a faster recovery time in the event of a memory failure. Given a fixed budget, the key configuration questions are: Which storage media should constitute the memory hierarchy? What is the storage capacity of each hierarchy? Should data be replicated or partitioned across the different levels of the hierarchy? We model these cache configuration questions as an instance of the Multiple Choice Knapsack Problem (MCKP). This model is guided by the specification of each type of memory along with an application's database characteristics and its workload. Although MCKP is NP-complete, its linear programming relaxation is efficiently solvable and can be used to closely approximate the optimal solution. We use the resulting simple algorithm to evaluate design tradeoffs in the context of a memory hierarchy for a Key-Value Store (e.g., memcached) as well as a host-side cache (e.g., Flashcache). The results show selective replication is appropriate with certain failure rates and workload characteristics. With a slim failure rate and frequent data updates, tiering of data across the different storage media that constitute the cache is superior to replication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05071v1</guid>
      <category>cs.DB</category>
      <category>cs.AR</category>
      <category>cs.DS</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICDE.2018.00155</arxiv:DOI>
      <dc:creator>Shahram Ghandeharizadeh, Sandy Irani, Jenny Lam</dc:creator>
    </item>
    <item>
      <title>The Peculiarities of Extending Queue Layouts</title>
      <link>https://arxiv.org/abs/2506.05156</link>
      <description>arXiv:2506.05156v1 Announce Type: cross 
Abstract: We consider the problem of computing $\ell$-page queue layouts, which are linear arrangements of vertices accompanied with an assignment of the edges to pages from one to $\ell$ that avoid the nesting of edges on any of the pages. Inspired by previous work in the extension of stack layouts, here we consider the setting of extending a partial $\ell$-page queue layout into a complete one and primarily analyze the problem through the refined lens of parameterized complexity. We obtain novel algorithms and lower bounds which provide a detailed picture of the problem's complexity under various measures of incompleteness, and identify surprising distinctions between queue and stack layouts in the extension setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05156v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Depian, Simon D. Fink, Robert Ganian, Martin N\"ollenburg</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Provably Efficient Algorithms to Estimate Shapley Values</title>
      <link>https://arxiv.org/abs/2506.05216</link>
      <description>arXiv:2506.05216v1 Announce Type: cross 
Abstract: Shapley values have emerged as a critical tool for explaining which features impact the decisions made by machine learning models. However, computing exact Shapley values is difficult, generally requiring an exponential (in the feature dimension) number of model evaluations. To address this, many model-agnostic randomized estimators have been developed, the most influential and widely used being the KernelSHAP method (Lundberg &amp; Lee, 2017). While related estimators such as unbiased KernelSHAP (Covert &amp; Lee, 2021) and LeverageSHAP (Musco &amp; Witter, 2025) are known to satisfy theoretical guarantees, bounds for KernelSHAP have remained elusive. We describe a broad and unified framework that encompasses KernelSHAP and related estimators constructed using both with and without replacement sampling strategies. We then prove strong non-asymptotic theoretical guarantees that apply to all estimators from our framework. This provides, to the best of our knowledge, the first theoretical guarantees for KernelSHAP and sheds further light on tradeoffs between existing estimators. Through comprehensive benchmarking on small and medium dimensional datasets for Decision-Tree models, we validate our approach against exact Shapley values, consistently achieving low mean squared error with modest sample sizes. Furthermore, we make specific implementation improvements to enable scalability of our methods to high-dimensional datasets. Our methods, tested on datasets such MNIST and CIFAR10, provide consistently better results compared to the KernelSHAP library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05216v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>quant-ph</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tyler Chen, Akshay Seshadri, Mattia J. Villani, Pradeep Niroula, Shouvanik Chakrabarti, Archan Ray, Pranav Deshpande, Romina Yalovetzky, Marco Pistoia, Niraj Kumar</dc:creator>
    </item>
    <item>
      <title>Computational complexity of the recoverable robust shortest path problem in acyclic digraphs</title>
      <link>https://arxiv.org/abs/2410.09425</link>
      <description>arXiv:2410.09425v2 Announce Type: replace 
Abstract: In this paper, the recoverable robust shortest path problem in acyclic digraphs is considered. The interval budgeted uncertainty representation is used to model the uncertain second-stage costs. The computational complexity of this problem has been open to date. In this paper, we prove that the problem is strongly NP-hard even for the case of layered acyclic digraphs. We also show that for the discrete budgeted uncertainty, the problem is not approximable unless P=NP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09425v2</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Kasperski, Pawel Zielinski</dc:creator>
    </item>
    <item>
      <title>On the Complexity of 2-club Cluster Editing with Vertex Splitting</title>
      <link>https://arxiv.org/abs/2411.04846</link>
      <description>arXiv:2411.04846v2 Announce Type: replace 
Abstract: Editing a graph to obtain a disjoint union of s-clubs is one of the models for correlation clustering, which seeks a partition of the vertex set of a graph so that elements of each resulting set are close enough according to some given criterion. For example, in the case of editing into s-clubs, the criterion is proximity since any pair of vertices (in an s-club) are within a distance of s from each other. In this work we consider the vertex splitting operation, which allows a vertex to belong to more than one cluster. This operation was studied as one of the parameters associated with the Cluster Editing problem. We study the complexity and parameterized complexity of the s-Club Cluster Edge Deletion with Vertex Splitting and s-Club Cluster Vertex Splitting problems. Both problems are shown to be NP-Complete and APX-hard. On the positive side, we show that both problems are Fixed-Parameter Tractable with respect to the number of allowed editing operations and that s-Club Cluster Vertex Splitting is solvable in polynomial-time on the class of forests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04846v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Faisal N. Abu-Khzam, Tom Davot, Lucas Isenmann, Sergio Thoumi</dc:creator>
    </item>
    <item>
      <title>Connectivity-Preserving Minimum Separator in AT-free Graphs</title>
      <link>https://arxiv.org/abs/2506.03612</link>
      <description>arXiv:2506.03612v2 Announce Type: replace 
Abstract: Let $A$ and $B$ be disjoint, non-adjacent vertex-sets in an undirected, connected graph $G$, whose vertices are associated with positive weights. We address the problem of identifying a minimum-weight subset of vertices $S\subseteq V(G)$ that, when removed, disconnects $A$ from $B$ while preserving the internal connectivity of both $A$ and $B$. We call such a subset of vertices a connectivity-preserving, or safe minimum $A,B$-separator. Deciding whether a safe $A,B$-separator exists is NP-hard by reduction from the 2-disjoint connected subgraphs problem, and remains NP-hard even for restricted graph classes that include planar graphs, and $P_\ell$-free graphs if $\ell\geq 5$. In this work, we show that if $G$ is AT-free then in polynomial time we can find a safe $A,B$-separator of minimum weight, or establish that no safe $A,B$-separator exists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03612v2</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Batya Kenig</dc:creator>
    </item>
    <item>
      <title>HashEvict: A Pre-Attention KV Cache Eviction Strategy using Locality-Sensitive Hashing</title>
      <link>https://arxiv.org/abs/2412.16187</link>
      <description>arXiv:2412.16187v3 Announce Type: replace-cross 
Abstract: Transformer-based large language models (LLMs) use the key-value (KV) cache to significantly accelerate inference by storing the key and value embeddings of past tokens. However, this cache consumes significant GPU memory. In this work, we introduce HashEvict, an algorithm that uses locality-sensitive hashing (LSH) to compress the KV cache. HashEvict quickly locates tokens in the cache that are cosine dissimilar to the current query token. This is achieved by computing the Hamming distance between binarized Gaussian projections of the current token query and cached token keys, with a projection length much smaller than the embedding dimension. We maintain a lightweight binary structure in GPU memory to facilitate these calculations. Unlike existing compression strategies that compute attention to determine token retention, HashEvict makes these decisions pre-attention, thereby reducing computational costs. Additionally, HashEvict is dynamic - at every decoding step, the key and value of the current token replace the embeddings of a token expected to produce the lowest attention score. We demonstrate that HashEvict can compress the KV cache by 30%-70% while maintaining high performance across reasoning, multiple-choice, long-context retrieval and summarization tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16187v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DS</category>
      <category>cs.PF</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minghui Liu, Tahseen Rabbani, Tony O'Halloran, Ananth Sankaralingam, Mary-Anne Hartley, Furong Huang, Cornelia Ferm\"uller, Yiannis Aloimonos</dc:creator>
    </item>
  </channel>
</rss>
