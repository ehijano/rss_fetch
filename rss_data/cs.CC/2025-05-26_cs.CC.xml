<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 26 May 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Role of Regularity in (Hyper-)Clique Detection and Implications for Optimizing Boolean CSPs</title>
      <link>https://arxiv.org/abs/2505.17314</link>
      <description>arXiv:2505.17314v1 Announce Type: new 
Abstract: Is detecting a $k$-clique in $k$-partite regular (hyper-)graphs as hard as in the general case? Intuition suggests yes, but proving this -- especially for hypergraphs -- poses notable challenges. Concretely, we consider a strong notion of regularity in $h$-uniform hypergraphs, where we essentially require that any subset of at most $h-1$ is incident to a uniform number of hyperedges. Such notions are studied intensively in the combinatorial block design literature. We show that any $f(k)n^{g(k)}$-time algorithm for detecting $k$-cliques in such graphs transfers to an $f'(k)n^{g(k)}$-time algorithm for the general case, establishing a fine-grained equivalence between the $h$-uniform hyperclique hypothesis and its natural regular analogue.
  Equipped with this regularization result, we then fully resolve the fine-grained complexity of optimizing Boolean constraint satisfaction problems over assignments with $k$ non-zeros. Our characterization depends on the maximum degree $d$ of a constraint function. Specifically, if $d\le 1$, we obtain a linear-time solvable problem, if $d=2$, the time complexity is essentially equivalent to $k$-clique detection, and if $d\ge 3$ the problem requires exhaustive-search time under the 3-uniform hyperclique hypothesis. To obtain our hardness results, the regularization result plays a crucial role, enabling a very convenient approach when applied carefully. We believe that our regularization result will find further applications in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17314v1</guid>
      <category>cs.CC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nick Fischer, Marvin K\"unnemann, Mirza Red\v{z}i\'c, Julian Stie{\ss}</dc:creator>
    </item>
    <item>
      <title>The Quasi-Polynomial Low-Degree Conjecture is False</title>
      <link>https://arxiv.org/abs/2505.17360</link>
      <description>arXiv:2505.17360v1 Announce Type: new 
Abstract: There is a growing body of work on proving hardness results for average-case estimation problems by bounding the low-degree advantage (LDA) - a quantitative estimate of the closeness of low-degree moments - between a null distribution and a related planted distribution. Such hardness results are now ubiquitous not only for foundational average-case problems but also central questions in statistics and cryptography. This line of work is supported by the low-degree conjecture of Hopkins, which postulates that a vanishing degree-$D$ LDA implies the absence of any noise-tolerant distinguishing algorithm with runtime $n^{\widetilde{O}(D)}$ whenever 1) the null distribution is product on $\{0,1\}^{\binom{n}{k}}$, and 2) the planted distribution is permutation invariant, that is, invariant under any relabeling $[n] \rightarrow [n]$.
  In this paper, we disprove this conjecture. Specifically, we show that for any fixed $\varepsilon&gt;0$ and $k\geq 2$, there is a permutation-invariant planted distribution on $\{0,1\}^{\binom{n}{k}}$ that has a vanishing degree-$n^{1-O(\varepsilon)}$ LDA with respect to the uniform distribution on $\{0,1\}^{\binom{n}{k}}$, yet the corresponding $\varepsilon$-noisy distinguishing problem can be solved in $n^{O(\log^{1/(k-1)}(n))}$ time. Our construction relies on algorithms for list-decoding for noisy polynomial interpolation in the high-error regime.
  We also give another construction of a pair of planted and (non-product) null distributions on $\mathbb{R}^{n \times n}$ with a vanishing $n^{\Omega(1)}$-degree LDA while the largest eigenvalue serves as an efficient noise-tolerant distinguisher.
  Our results suggest that while a vanishing LDA may still be interpreted as evidence of hardness, developing a theory of average-case complexity based on such heuristics requires a more careful approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17360v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rares-Darius Buhai, Jun-Ting Hsieh, Aayush Jain, Pravesh K. Kothari</dc:creator>
    </item>
    <item>
      <title>Finding d-Cuts in Claw-free Graphs</title>
      <link>https://arxiv.org/abs/2505.17993</link>
      <description>arXiv:2505.17993v1 Announce Type: cross 
Abstract: The Matching Cut problem is to decide if the vertex set of a connected graph can be partitioned into two non-empty sets $B$ and $R$ such that the edges between $B$ and $R$ form a matching, that is, every vertex in $B$ has at most one neighbour in $R$, and vice versa. If for some integer $d\geq 1$, we allow every neighbour in $B$ to have at most $d$ neighbours in $R$, and vice versa, we obtain the more general problem $d$-Cut. It is known that $d$-Cut is NP-complete for every $d\geq 1$. However, for claw-free graphs, it is only known that $d$-Cut is polynomial-time solvable for $d=1$ and NP-complete for $d\geq 3$. We resolve the missing case $d=2$ by proving NP-completeness. This follows from our more general study, in which we also bound the maximum degree. That is, we prove that for every $d\geq 2$, $d$-Cut, restricted to claw-free graphs of maximum degree $p$, is constant-time solvable if $p\leq 2d+1$ and NP-complete if $p\geq 2d+3$. Moreover, in the former case, we can find a $d$-cut in linear time. We also show how our positive results for claw-free graphs can be generalized to $S_{1^t,l}$-free graphs where $S_{1^t,l}$ is the graph obtained from a star on $t+2$ vertices by subdividing one of its edges exactly $l$ times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17993v1</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jungho Ahn, Tala Eagling-Vose, Felicia Lucke, Dani\"el Paulusma, Siani Smith</dc:creator>
    </item>
    <item>
      <title>How to fit large complexity classes into TFNP</title>
      <link>https://arxiv.org/abs/2412.09984</link>
      <description>arXiv:2412.09984v2 Announce Type: replace 
Abstract: Subclasses of TFNP (total functional NP) are usually defined by specifying a complete problem, which is necessarily in TFNP, and including all problems many-one reducible to it. We study two notions of how a TFNP problem can be reducible to an object, such as a complexity class, outside TFNP. This gives rise to subclasses of TFNP which capture some properties of that outside object. We show that well-known subclasses can arise in this way, for example PPA from reducibility to parity P and PLS from reducibility to P^NP.
  We study subclasses arising from PSPACE and the polynomial hierarchy, and show that they are characterized by the propositional proof systems Frege and constant-depth Frege, extending the known pairings between natural TFNP subclasses and proof systems.
  We study approximate counting from this point of view, and look for a subclass of TFNP that gives a natural home to combinatorial principles such as Ramsey which can be proved using approximate counting. We relate this to the recently-studied Long choice and Short choice problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09984v2</guid>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neil Thapen</dc:creator>
    </item>
    <item>
      <title>Twisted conjugacy in dihedral Artin groups I: Torus Knot groups</title>
      <link>https://arxiv.org/abs/2403.16671</link>
      <description>arXiv:2403.16671v5 Announce Type: replace-cross 
Abstract: In this paper we provide an alternative solution to a result by Juh\'{a}sz that the twisted conjugacy problem for odd dihedral Artin groups is solvable, that is, groups with presentation $G(m) = \langle a,b \; | \; _{m}(a,b) = {}_{m}(b,a) \rangle$, where $m\geq 3$ is odd, and $_{m}(a,b)$ is the word $abab \dots$ of length $m$, is solvable. Our solution provides an implementable linear time algorithm, by considering an alternative group presentation to that of a torus knot group, and working with geodesic normal forms. An application of this result is that the conjugacy problem is solvable in extensions of odd dihedral Artin groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16671v5</guid>
      <category>math.GR</category>
      <category>cs.CC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gemma Crowe</dc:creator>
    </item>
    <item>
      <title>Fundamental Limitations on Subquadratic Alternatives to Transformers</title>
      <link>https://arxiv.org/abs/2410.04271</link>
      <description>arXiv:2410.04271v2 Announce Type: replace-cross 
Abstract: The Transformer architecture is widely deployed in many popular and impactful Large Language Models. At its core is the attention mechanism for calculating correlations between pairs of tokens. Performing an attention computation takes quadratic time in the input size, and had become the time bottleneck for transformer operations. In order to circumvent this, researchers have used a variety of approaches, including designing heuristic algorithms for performing attention computations faster, and proposing alternatives to the attention mechanism which can be computed more quickly. For instance, state space models such as Mamba were designed to replace attention with an almost linear time alternative.
  In this paper, we prove that any such approach cannot perform important tasks that Transformer is able to perform (assuming a popular conjecture from fine-grained complexity theory). We focus on document similarity tasks, where one is given as input many documents and would like to find a pair which is (approximately) the most similar. We prove that Transformer is able to perform this task, and we prove that this task cannot be performed in truly subquadratic time by any algorithm. Thus, any model which can be evaluated in subquadratic time - whether because of subquadratic-time heuristics for attention, faster attention replacements like Mamba, or any other reason - cannot perform this task. In other words, in order to perform tasks that (implicitly or explicitly) involve document similarity, one may as well use Transformer and cannot avoid its quadratic running time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04271v2</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.CL</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Josh Alman, Hantao Yu</dc:creator>
    </item>
    <item>
      <title>A Little Depth Goes a Long Way: The Expressive Power of Log-Depth Transformers</title>
      <link>https://arxiv.org/abs/2503.03961</link>
      <description>arXiv:2503.03961v2 Announce Type: replace-cross 
Abstract: Recent theoretical results show transformers cannot express sequential reasoning problems over long inputs, intuitively because their computational depth is bounded. However, prior work treats the depth as a constant, leaving it unclear to what degree bounded depth may suffice for solving problems over short inputs, or how increasing the transformer's depth affects its expressive power. We address these questions by analyzing transformers whose depth can grow minimally with context length $n$. We show even highly uniform transformers with depth $\Theta(\log n)$ can express two important problems: recognizing regular languages, which captures state tracking abilities and was known to be expressible only by an unconventional, non-uniform model of transformers, and graph connectivity, which underlies multi-step reasoning. Notably, both of these problems cannot be expressed by fixed-depth transformers under standard complexity conjectures, demonstrating the expressivity benefit of growing depth. Moreover, our theory quantitatively predicts how depth must grow with input length to express these problems, showing that depth scaling is more efficient than scaling width or chain-of-thought steps. Empirically, our detailed experiments designed to bridge the expressivity vs. learnability gap reveal that our theoretical depth requirements for regular language recognition closely match the practical depth requirements for successfully training transformers. Thus, our results clarify how depth affects a transformer's reasoning capabilities, and provide practical guidance for effective depth selection for sequential reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03961v2</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <pubDate>Mon, 26 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Merrill, Ashish Sabharwal</dc:creator>
    </item>
  </channel>
</rss>
