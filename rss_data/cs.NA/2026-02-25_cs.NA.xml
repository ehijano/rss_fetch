<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Feb 2026 05:00:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Two approaches to low-parametric SimRank computation</title>
      <link>https://arxiv.org/abs/2602.20282</link>
      <description>arXiv:2602.20282v1 Announce Type: new 
Abstract: In this work, we discuss low-parametric approaches for approximating SimRank matrices, which estimate the similarity between pairs of nodes in a graph. Although SimRank matrices and their computation require a significant amount of memory, common approaches mostly address the problem of algorithmic complexity. We propose two major formats for the economical embedding of target data. The first approach adopts a non-symmetric form that can be computed using a specialized alternating optimization algorithm. The second is based on a symmetric representation and Newton-type iterations. We propose numerical implementations for both methodologies that avoid working with dense matrices and maintain low memory consumption. Furthermore, we study both types of embeddings numerically using real data from publicly available datasets. The results show that our algorithms yield a good approximation of the SimRank matrices, both in terms of the error norm (particularly the Chebyshev norm) and in preserving the average number of the most similar elements for each given node.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20282v1</guid>
      <category>math.NA</category>
      <category>cs.DM</category>
      <category>cs.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Egor P. Berezin, Robert T. Zaks, German Z. Alekhin, Stanislav V. Morozov, Sergey A. Matveev</dc:creator>
    </item>
    <item>
      <title>The largest 5th pivot may be the root of a 61st degree polynomial</title>
      <link>https://arxiv.org/abs/2602.20390</link>
      <description>arXiv:2602.20390v1 Announce Type: new 
Abstract: This paper introduces a number of new techniques in the study of the famous question from numerical linear algebra: what is the largest possible growth factor when performing Gaussian elimination with complete pivoting? This question is highly complex, due to a complicated set of polynomial inequalities that need to be simultaneously satisfied. This paper introduces the JuMP + Groebner basis + discriminant polynomial approach as well as the use of interval arithmetic computations. Thus, we are introducing a marriage of numerical and exact mathematical computations.
  In 1988, Day and Peterson performed numerical optimization on $n=5$ with NPSOL and obtained a largest seen value of $4.1325...$. This same best value was reproduced by Gould with LANCELOT in 1991. We ran extensive comparable experiments with the modern software tool JuMP and also saw the same value $4.1325...$. While the combinatorial explosion of possibilities prevents us from knowing there may not be a larger maximum, we succeed in obtaining the exact mathematical value: the number $4.1325...$ is exactly the root of a 61st degree polynomial provided in this work, and is a maximum given the equality constraints seen by JuMP. In light of the numerics, we pose the conjecture that this lower bound is indeed the maximum. We also apply this technique to $n = 6$, $7$, and $8$.
  Furthermore, in 1969, an upper bound of $4\frac{17}{18}\approx 4.94$ was produced for the maximum possible growth for $n = 5$. We slightly lower this upper bound to $4.84$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20390v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Chen, Alan Edelman, John Urschel</dc:creator>
    </item>
    <item>
      <title>A parametrix for the surface Stokes equation</title>
      <link>https://arxiv.org/abs/2602.20395</link>
      <description>arXiv:2602.20395v1 Announce Type: new 
Abstract: We introduce an integral equation formulation of the surface Stokes equations, constructed using two-dimensional Stokeslets. The resulting integral equations are Fredholm integral equations of the second kind and can be discretized to high order using standard tools. Since the resulting discrete linear systems are dense, we describe and analyze a proxy shell method to construct fast direct solvers for these systems. The properties of our integral equation, and the performance of the resulting numerical scheme, are illustrated with several representative numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20395v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tristan Goodwill, Jeremy Hoskins, Zydrunas Gimbutas, Bowei Wu</dc:creator>
    </item>
    <item>
      <title>Implicit-explicit all-speed schemes for compressible Cahn-Hilliard-Navier-Stokes equations</title>
      <link>https://arxiv.org/abs/2602.20679</link>
      <description>arXiv:2602.20679v1 Announce Type: new 
Abstract: We propose a second-order implicit-explicit (IMEX) time-stepping scheme for the isentropic, compressible Cahn-Hilliard-Navier-Stokes equations in the low Mach number regime.
  The method is based on finite differences on staggered grids and is specifically designed to handle the challenges posed by the low Mach number limit, where the system approaches to an incompressible behavior.
  In this regime, standard explicit schemes suffer from severe time-step restrictions due to fourth-order diffusion terms and the stiffness induced by fast acoustic waves.
  To overcome this, we employ an IMEX strategy which splits the governing equations into stiff and non-stiff components.
  The stiff terms, arising from pressure, viscous forces and fourth-order Cahn-Hilliard contributions, are treated implicitly, while the remaining are dealt explicitly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20679v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreu Martorell, Pep Mulet, Dionisio F. Y\'a\~nez</dc:creator>
    </item>
    <item>
      <title>Reduced-order computational homogenization for hyperelastic media using gradient based sensitivity analysis of microstructures</title>
      <link>https://arxiv.org/abs/2602.20697</link>
      <description>arXiv:2602.20697v1 Announce Type: new 
Abstract: We propose an algorithm for the computational homogenization of locally periodic hyperelastic structures undergoing large deformations due to external quasi-static loading. The algorithm performs clustering of macroscopic deformations into subsets called "centroids", and, as a new ingredient, approximates the homogenized coefficients using sensitivity analysis of micro-configurations with respect to the macroscopic deformation. The novel "model-order reduction" approach significantly reduces the number of microscopic problems that must be solved in nonlinear simulations, thereby accelerating the overall computational process. The degree of reduction can be controlled by a user-defined error tolerance parameter. The algorithm is implemented in the finite element framework SfePy, and its performance effectiveness is demonstrated using two-dimensional test examples, when compared with solutions obtained by the proper orthogonal decomposition method, and by the full "FE-square" simulations. Extensions beyond the present implementations and the scope of tractable problems are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20697v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vladim\'ir Luke\v{s}, Eduard Rohan</dc:creator>
    </item>
    <item>
      <title>The Adaptive Solution of High-Frequency Helmholtz Equations via Multi-Grade Deep Learning</title>
      <link>https://arxiv.org/abs/2602.20719</link>
      <description>arXiv:2602.20719v1 Announce Type: new 
Abstract: The Helmholtz equation is fundamental to wave modeling in acoustics, electromagnetics, and seismic imaging, yet high-frequency regimes remain challenging due to the ``pollution effect''. We propose FD-MGDL, an adaptive framework integrating finite difference schemes with Multi-Grade Deep Learning to efficiently resolve high-frequency solutions. While traditional PINNs struggle with spectral bias and automatic differentiation overhead, FD-MGDL employs a progressive training strategy, incrementally adding hidden layers to refine the solution and maintain stability. Crucially, when using ReLU activation, our algorithm recasts the highly non-convex training problem into a sequence of convex subproblems. Numerical experiments in 2D and 3D with wavenumbers up to $\kappa=200$ show that FD-MGDL significantly outperforms single-grade and conventional neural solvers in accuracy and speed. Applied to an inhomogeneous concave velocity model, the framework accurately resolves wave focusing and caustics, surpassing the 5-point finite difference method in capturing sharp phase transitions and amplitude spikes. These results establish FD-MGDL as a robust, scalable solver for high-frequency wave equations in complex domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20719v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peiyao Zhao, Rui Wang, Tingting Wu, Yuesheng Xu</dc:creator>
    </item>
    <item>
      <title>Convergence analysis of $L^{p+1}$-normalized gradient flow for action ground state of nonlinear Schr\"odinger equation</title>
      <link>https://arxiv.org/abs/2602.20820</link>
      <description>arXiv:2602.20820v1 Announce Type: new 
Abstract: This paper presents a rigorous convergence analysis of the $L^{p+1}$-normalized gradient flow with asymptotic Lagrange multiplier (GFALM) method for computing the action ground state of the nonlinear Schr\"odinger equation in the focusing case. First, a general global convergence theory is established for the semi-discrete GFALM scheme, guaranteeing the existence of an accumulation point and a convergent subsequence. Then, under additional non-degeneracy assumptions, a local exponential convergence rate is rigorously proven. This result is further extended to the fully discrete case using a Fourier pseudo-spectral discretization. The analysis is achieved by characterizing the local geometry of the $L^{p+1}$-constrained manifold near the ground state, establishing a quadratic growth property of the energy functional, and deriving a \L{}ojasiewicz-type gradient inequality. Finally, the paper also investigates the exponential convergence of the associated continuous-time gradient flow, providing a theoretical foundation for future numerical discretizations. This work extends existing convergence analyses for energy ground states, addressing the challenges posed by the $L^{p+1}$ constraint, especially the absence of an inner-product structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20820v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Liu, Tingfeng Wang, Xiaofei Zhao</dc:creator>
    </item>
    <item>
      <title>H\"older-Logarithmic Stability and Convergence Rates for an Inverse Random Source Problem</title>
      <link>https://arxiv.org/abs/2602.20822</link>
      <description>arXiv:2602.20822v1 Announce Type: new 
Abstract: In this paper, we investigate an inverse random source problem concerned with recovering the strength of a random, uncorrelated acoustic source from correlation measurements of emitted time-harmonic acoustic waves. Such problems arise in applications including aeroacoustics and seismic imaging. Unlike their deterministic counterparts, inverse random source problems are known to be uniquely solvable in the absence of noise. Nevertheless, due to their inherent ill-posedness, regularization is required to stably reconstruct the source strength.
  We derive conditional H\"older-logarithmic stability estimates under Sobolev smoothness assumptions by employing complex geometrical optics solutions. Moreover, by establishing a variational source condition, we obtain H\"older-logarithmic convergence rates for spectral regularization methods. At fixed frequency, the exponents in the logarithmic stability and convergence estimates grow unboundedly as the Sobolev regularity of the source increases. Finally, we present numerical experiments supporting our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20822v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Mickan, Thorsten Hohage</dc:creator>
    </item>
    <item>
      <title>Lanczos with compression for symmetric eigenvalue problems</title>
      <link>https://arxiv.org/abs/2602.20948</link>
      <description>arXiv:2602.20948v1 Announce Type: new 
Abstract: The Lanczos method with implicit restarting is one of the most popular methods for finding a few exterior eigenpairs of a large symmetric matrix $A$. Usually based on polynomial filtering, restarting is crucial to limit memory and the cost of orthogonalization. In this work, we propose a novel strategy for the same purpose, called Lanczos with compression. Unlike polynomial filtering, our approach compresses the Krylov subspace using rational approximation and, in doing so, it sacrifices the structure of the associated Krylov decomposition. Nevertheless, it remains compatible with subsequent Lanczos steps and the overall algorithm is still solely based on matrix-vector products with $A$. On the theoretical side, we show that compression introduces only a small error compared to standard (unrestarted) Lanczos and therefore has only a negligible impact on convergence. Comparable guarantees are not available for commonly used implicit restarting strategies, including the Krylov--Schur method. On the practical side, our numerical experiments demonstrate that compression often outperforms the Krylov--Schur method in terms of matrix-vector products.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20948v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angelo A. Casulli, Daniel Kressner, Nian Shao</dc:creator>
    </item>
    <item>
      <title>Orthonormal polynomial wavelets associated with de la Vall\'ee Poussin-type interpolation on $[-1,1]$</title>
      <link>https://arxiv.org/abs/2602.20955</link>
      <description>arXiv:2602.20955v1 Announce Type: new 
Abstract: Starting from de la Vall\'ee Poussin type (VP) interpolation, the authors have recently introduced a family of interpolating polynomial scaling and wavelet bases generating the approximation and detail spaces of a non-standard multiresolution analysis. Motivated by the fact that, in many applications, orthonormal rather than interpolating bases are preferable, the present study develops a new family of scaling and wavelet polynomials that provide well-localized and orthonormal bases for the same approximation and detail spaces.
  We show that the proposed new bases have a behavior very similar to the interpolating bases already introduced, presenting similar features although they are not interpolating but orthonormal. In particular, we study the Fourier projection corresponding to the proposed orthonormal scaling basis, and introduce a discrete version of it by approximating the Fourier--like coefficients. For both continuous and discrete orthogonal projections, we prove the uniform boundedness of the Lebesgue constants and the uniform convergence with an asymptotic rate comparable with the best uniform polynomial approximation.
  Numerical experiments confirm the theoretical results and compare the new orthonormal VP scaling and wavelet bases with the interpolating case previously treated by the authors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20955v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Woula Themistoclakis, Marc Van Barel</dc:creator>
    </item>
    <item>
      <title>Variants of Raviart-Thomas mixed elements for curved domains using straight-edged tetrahedra</title>
      <link>https://arxiv.org/abs/2602.21197</link>
      <description>arXiv:2602.21197v1 Announce Type: new 
Abstract: A numerical study of tetrahedral Raviart-Thomas mixed finite element methods is presented in the solution of model second order boundary value problems posed in a curved spatial domain. An emphasis is given to the case where normal fluxes are prescribed on a boundary portion. In this case the question on the best way to enforce known boundary degrees of freedom is raised. It seems intuitive that the normal component of the flux variable should preferably not take up corresponding prescribed values at nodes shifted to the boundary of the approximating polyhedron in the underlying normal direction. This is because an accuracy downgrade is to be expected, as shown in https://doi.org/10.1137/15M1045442 and https://doi.org/10.1051/m2an/2025028. In the former work accuracy improvement is achieved by means of a standard Galerkin formulation with parametric elements. The latter one in turn advocates the use of straight-edged triangles combined with a Petrov-Galerkin formulation, in which the aforementioned shift applies only to the test-flux space, while the shape-flux space consists of fields whose fluxes satisfy the prescribed conditions on the true boundary. The first purpose of this article is to show that the method studied in https://doi.org/10.1051/m2an/2025028 for two-dimensional problems can be extended quite naturally to the three-dimensional case. More particularly we illustrate this by carrying out numerical experimentation with such a version for the two lowest order methods of this family, as compared to the corresponding do-nothing strategy. In the case of the lowest order method this comparative study is enriched by assessing as well the performance of its Hermite analog introduced in https://doi.org/10.1016/j.cam.2012.08.027.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21197v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vittoriano Ruas</dc:creator>
    </item>
    <item>
      <title>Shape-informed cardiac mechanics surrogates in data-scarce regimes via geometric encoding and generative augmentation</title>
      <link>https://arxiv.org/abs/2602.20306</link>
      <description>arXiv:2602.20306v1 Announce Type: cross 
Abstract: High-fidelity computational models of cardiac mechanics provide mechanistic insight into the heart function but are computationally prohibitive for routine clinical use. Surrogate models can accelerate simulations, but generalization across diverse anatomies is challenging, particularly in data-scarce settings. We propose a two-step framework that decouples geometric representation from learning the physics response, to enable shape-informed surrogate modeling under data-scarce conditions. First, a shape model learns a compact latent representation of left ventricular geometries. The learned latent space effectively encodes anatomies and enables synthetic geometries generation for data augmentation. Second, a neural field-based surrogate model, conditioned on this geometric encoding, is trained to predict ventricular displacement under external loading. The proposed architecture performs positional encoding by using universal ventricular coordinates, which improves generalization across diverse anatomies. Geometric variability is encoded using two alternative strategies, which are systematically compared: a PCA-based approach suitable for working with point cloud representations of geometries, and a DeepSDF-based implicit neural representation learned directly from point clouds. Overall, our results, obtained on idealized and patient-specific datasets, show that the proposed approaches allow for accurate predictions and generalization to unseen geometries, and robustness to noisy or sparsely sampled inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20306v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Davide Carrara, Marc Hirschvogel, Francesca Bonizzoni, Stefano Pagani, Simone Pezzuto, Francesco Regazzoni</dc:creator>
    </item>
    <item>
      <title>Quantitative Approximation Rates for Group Equivariant Learning</title>
      <link>https://arxiv.org/abs/2602.20370</link>
      <description>arXiv:2602.20370v1 Announce Type: cross 
Abstract: The universal approximation theorem establishes that neural networks can approximate any continuous function on a compact set. Later works in approximation theory provide quantitative approximation rates for ReLU networks on the class of $\alpha$-H\"older functions $f: [0,1]^N \to \mathbb{R}$. The goal of this paper is to provide similar quantitative approximation results in the context of group equivariant learning, where the learned $\alpha$-H\"older function is known to obey certain group symmetries. While there has been much interest in the literature in understanding the universal approximation properties of equivariant models, very few quantitative approximation results are known for equivariant models.
  In this paper, we bridge this gap by deriving quantitative approximation rates for several prominent group-equivariant and invariant architectures. The architectures that we consider include: the permutation-invariant Deep Sets architecture; the permutation-equivariant Sumformer and Transformer architectures; joint invariance to permutations and rigid motions using invariant networks based on frame averaging; and general bi-Lipschitz invariant models. Overall, we show that equally-sized ReLU MLPs and equivariant architectures are equally expressive over equivariant functions. Thus, hard-coding equivariance does not result in a loss of expressivity or approximation power in these models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20370v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan W. Siegel, Snir Hordan, Hannah Lawrence, Ali Syed, Nadav Dym</dc:creator>
    </item>
    <item>
      <title>Entropy stable numerical schemes for divergence diminishing Chew, Goldberger &amp; Low equations for plasma flows</title>
      <link>https://arxiv.org/abs/2602.20757</link>
      <description>arXiv:2602.20757v1 Announce Type: cross 
Abstract: Chew, Goldberger &amp; Low (CGL) equations are a set of hyperbolic PDEs with non-conservative products used to model the plasma flows, when the assumption of local thermodynamic equilibrium is not valid, and the pressure tensor is assumed to be rotated by the magnetic field. This results in the pressure tensor, which is described by the two scalar components. As the magnetic field also evolves, controlling the divergence of the magnetic field is important. In this work, we consider the generalized Lagrange multiplier (GLM) technique for the CGL model. The resulting model is referred to as the GLM-CGL system. To make the system suitable for entropy-stable schemes, we reformulate the GLM-CGL system by treating some conservative terms as non-conservative. The resulting system has a non-conservative part that does not affect entropy evolution. We then propose entropy stable numerical methods for the GLM-CGL model. The numerical results for the GLM-CGL system are then compared with the CGL system without the GLM divergence diminishing approach to demonstrate that the GLM approach indeed leads to significant improvement in the magnetic field divergence diminishing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20757v1</guid>
      <category>physics.plasm-ph</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chetan Singh, Harish Kumar, Deepak Bhoriya, Dinshaw S. Balsara</dc:creator>
    </item>
    <item>
      <title>Optimal design for linear models via gradient flow</title>
      <link>https://arxiv.org/abs/2401.07806</link>
      <description>arXiv:2401.07806v3 Announce Type: replace 
Abstract: Optimal experimental design (OED) aims to choose the observations in an experiment to be as informative as possible, according to certain statistical criteria. In the linear case (when the observations depend linearly on the unknown parameters), it seeks the optimal weights over rows of the design matrix $\mA$ under certain criteria. Classical OED assumes a discrete design space and thus a design matrix with finite dimensions. In many practical situations, however, the design space is continuous-valued, so that the OED problem is one of optimizing over a continuous-valued design space. The objective becomes a functional over the probability measure, instead of a function of a finite dimensional vector. This change of perspective requires a new set of techniques to optimize over probability measures, and Wasserstein gradient flow becomes a natural candidate. Both the first-order criticality and the convexity properties of the OED objective are presented. Computationally, the Monte Carlo particle method is used to translate the gradient flow equation formulation into a numerical algorithm. This algorithm is applied to two elliptic inverse problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07806v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruhui Jin, Martin Guerra, Qin Li, Stephen Wright</dc:creator>
    </item>
    <item>
      <title>A Priori Error Estimation of Physics-Informed Neural Networks Solving Allen--Cahn and Cahn--Hilliard Equations</title>
      <link>https://arxiv.org/abs/2402.02667</link>
      <description>arXiv:2402.02667v2 Announce Type: replace 
Abstract: Physics-Informed Neural Networks (PINNs) encounter accuracy limitations when solving the Allen--Cahn (AC) and Cahn--Hilliard (CH) partial differential equations (PDEs). To overcome this, we employ a novel loss function, Residuals-weighted Region Activation Evaluation (Residuals-RAE), featuring a { pre-training weight update scheme}. { Unlike conventional self-adaptive PINNs where weights evolve simultaneously with network parameters, Residuals-RAE-PINNs computes weights from current residuals before each training step and holds them constant during gradient updates. We establish weight convergence under standard neural network optimization assumptions, which justifies analyzing the converged network with constant weights.} Based on this theoretical framework, we derive the error estimation for PINNs with Residuals-RAE when solving AC and CH equations. {The analysis is aligned with Monte-Carlo sampling for the discretization of integrals, consistent with the numerical experiments.} Numerical experiments on one- and two-dimensional AC and CH systems confirm our theoretical results. Additionally, our analysis reveals that feedforward neural networks with two hidden layers and the tanh activation function bound the approximation errors of the solution, its temporal derivative, and the nonlinear term, constrained by the training loss and the number of collocation points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02667v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guangtao Zhang, Jiani Lin, Qijia Zhai, Huiyu Yang, Xujun Chen, Ieng Tak Leong, Fang Zhu</dc:creator>
    </item>
    <item>
      <title>Sobolev-Poincar\'e inequalities for piecewise $W^{1,p}$ functions over general polytopic meshes</title>
      <link>https://arxiv.org/abs/2504.03449</link>
      <description>arXiv:2504.03449v3 Announce Type: replace 
Abstract: We establish Sobolev-Poincar\'e inequalities for piecewise $W^{1,p}$ functions over families of fairly general polytopic (thence also shape-regular simplicial and Cartesian) meshes in any dimension; amongst others, they cover the case of standard Poincar\'e inequalities for piecewise $W^{1,p}$ functions and can be useful in the analysis of nonconforming finite element discretizations of nonlinear problems. Crucial tools in their derivation are novel Sobolev-trace inequalities and $W^{1,p}$-stable right-inverses of the divergence satisfying mixed boundary conditions. We provide estimates with constants having an explicit dependence on the geometric properties of the domain and the underlying family of polytopic meshes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03449v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michele Botti, Lorenzo Mascotto</dc:creator>
    </item>
    <item>
      <title>Accelerating Newton-Schulz Iteration for Orthogonalization via Chebyshev-type Polynomials</title>
      <link>https://arxiv.org/abs/2506.10935</link>
      <description>arXiv:2506.10935v2 Announce Type: replace 
Abstract: The problem of computing optimal orthogonal approximation to a given matrix has attracted growing interest in machine learning. Notable applications include the recent Muon optimizer or Riemannian optimization on the Stiefel manifold. Among existing approaches, the Newton-Schulz iteration has emerged as a particularly effective solution, as it relies solely on matrix multiplications and thus achieves high computational efficiency on GPU hardware. Despite its efficiency, the method has inherent limitations - its coefficients are fixed and thus not optimized for a given matrix. In this paper we address this issue by proposing a Chebyshev-optimized version of Newton-Schulz (CANS). Based on the Chebyshev's alternance theorem, we theoretically derive optimal coefficients for the 3-rd order Newton-Schulz iteration and apply a Remez algorithm to compute optimal higher-degree polynomials. We leverage these polynomials to construct controlled approximate orthogonalization schemes, which is of interest in deep learning applications. Practically, we demonstrate the method's effectiveness in two key applications: orthogonalization in the Muon optimizer, and providing an efficient retraction alternative for Riemannian optimization on the Stiefel manifold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10935v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ekaterina Grishina, Matvey Smirnov, Maxim Rakhuba</dc:creator>
    </item>
    <item>
      <title>Multi-Order Runge-Kutta Methods or how to numerically solve initial value problems of any order</title>
      <link>https://arxiv.org/abs/2509.23513</link>
      <description>arXiv:2509.23513v4 Announce Type: replace 
Abstract: When one wishes to numerically solve an initial value problem, it is customary to rewrite it as an equivalent first-order system to which a method, usually from the class of Runge-Kutta methods, is applied. Directly treating higher-order initial value problems without such rewriting, however, allows for significantly greater accuracy. We therefore introduce a new generalization of Runge-Kutta methods, multi-order Runge-Kutta methods, designed to solve initial value problems of arbitrary order. We establish fundamental properties of these methods, including convergence, order of consistency, and linear stability. We also analyze the structure of the system satisfied by the approximations of a method, which enables us to provide a proper definition of explicit methods and gain a finer understanding of implicit methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23513v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Loris Petronijevic</dc:creator>
    </item>
    <item>
      <title>General transformation neural networks: A class of parametrized functions for high-dimensional function approximation</title>
      <link>https://arxiv.org/abs/2510.20142</link>
      <description>arXiv:2510.20142v3 Announce Type: replace 
Abstract: We propose a novel class of neural network-like parametrized functions, i.e., general transformation neural networks (GTNNs), for high-dimensional approximation. Conventional deep neural networks sometimes perform less accurately on learning problems trained with gradient descent, especially when the target function is oscillatory. To improve accuracy, we generalize the neuron's affine transformation to a broader class of functions that can capture complex shapes and offer greater capacity. Specifically, we discuss three types of GTNNs in detail: the cubic, quadratic and trigonometric transformation neural networks (CTNNs, QTNNs and TTNNs). We perform an approximation error analysis of GTNNs, presenting their universal approximation properties for continuous functions, error bounds for Barron-type functions and error bounds of deep architectures. Several numerical examples of regression problems are presented, demonstrating that CTNNs/QTNNs/TTNNs achieve higher accuracy than conventional fully connected neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20142v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyang Wang, Yiqi Gu</dc:creator>
    </item>
    <item>
      <title>Robust stability and preconditioning of Darcy-Forchheimer equations</title>
      <link>https://arxiv.org/abs/2510.24527</link>
      <description>arXiv:2510.24527v2 Announce Type: replace 
Abstract: We derive parameter-robust quasi-optimal error estimates for mixed finite element methods for the nonlinear Darcy--Forchheimer equations with mixed boundary conditions. Using the framework of operator preconditioning, we also design efficient block preconditioners for the linearised system, that exhibit robustness with respect to the coefficients that modulate permeability and inertia of the system. The properties of the formulation (parameter and mesh-size independence of the convergence rates) are illustrated by means of several numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24527v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rishi Das, Harsha Hutridurga, Amiya K. Pani, Ricardo Ruiz-Baier</dc:creator>
    </item>
    <item>
      <title>Numerical efficiency of explicit time integrators for phase-field models</title>
      <link>https://arxiv.org/abs/2601.16522</link>
      <description>arXiv:2601.16522v2 Announce Type: replace 
Abstract: Phase-field simulations are a practical but also expensive tool to calculate microstructural evolution. This work aims to compare explicit time integrators for a broad class of phase-field models involving coupling between the phase-field and concentration. Particular integrators are adapted to constraints on the phase-field as well as storage scheme implications. Reproducible benchmarks are defined with a focus on having exact sharp interface solutions, allowing for identification of dominant error terms. Speedups of 4 to 114 over the classic forward Euler integrator are achievable while still using a fully explicit scheme without appreciable accuracy loss. Application examples include final stage sintering with pores slowing down grain growth as they move and merge over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16522v2</guid>
      <category>math.NA</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Seiz, Tomohiro Takaki</dc:creator>
    </item>
    <item>
      <title>Two Models for Surface Segmentation using the Total Variation of the Normal Vector</title>
      <link>https://arxiv.org/abs/2412.00445</link>
      <description>arXiv:2412.00445v2 Announce Type: replace-cross 
Abstract: We consider the problem of surface segmentation, where the goal is to partition a surface represented by a triangular mesh. The segmentation is based on the similarity of the normal vector field to a given set of label vectors. We propose a variational approach and compare two different regularizers, both based on a total variation measure. The first regularizer penalizes the total variation of the assignment function directly, while the second regularizer penalizes the total variation in the label space. In order to solve the resulting optimization problems, we use variations of the split Bregman (ADMM) iteration adapted to the problem at hand. While computationally more expensive, the second regularizer yields better results in our experiments. In particular it removes noise more reliably in regions of constant curvature. In order to mitigate the computational cost, we present a manifold Newton scheme for the most expensive subproblem, which is related to the Riemannian center of mass on a sphere. This significantly improves the computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00445v2</guid>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Manuel Wei{\ss}, Lukas Baumg\"artner, Laura Weigl, Ronny Bergmann, Stephan Schmidt, Roland Herzog</dc:creator>
    </item>
    <item>
      <title>Nonlinear PDEs with modulated dispersion IV: normal form approach and unconditional uniqueness</title>
      <link>https://arxiv.org/abs/2505.24270</link>
      <description>arXiv:2505.24270v2 Announce Type: replace-cross 
Abstract: We study the modulated Korteweg-de~Vries equation (KdV) on the circle with a time non-homogeneous modulation acting on the linear dispersion term. By adapting the normal form approach to the modulated setting, we prove sharp unconditional uniqueness of solutions to the modulated KdV in $L^2(\mathbb T)$ if a modulation is sufficiently irregular. For example, this result implies that if the modulation is given by a sample path of a fractional Brownian motion with Hurst index $0 &lt; H &lt; \frac 25$, the modulated KdV on the circle is unconditionally well-posed in $L^2(\mathbb T)$. Our normal form approach provides the construction of solutions to the modulated KdV (and the associated nonlinear Young integral) {\it without} assuming any positive regularity in time. As an interesting byproduct of our normal form approach, we extend the construction of the nonlinear Young integral to a much larger class of functions, and obtain an improved Euler approximation scheme as compared to the classical sewing lemma approach.
  We also establish analogous sharp unconditional uniqueness results for the modulated Benjamin-Ono equation and the modulated derivative nonlinear Schr\"odinger equation (NLS) with a quadratic nonlinearity. In the appendix, we prove sharp unconditional uniqueness of the cubic modulated NLS on the circle in $H^{\frac 16}(\mathbb T)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24270v2</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Massimiliano Gubinelli, Guopeng Li, Jiawei Li, Tadahiro Oh</dc:creator>
    </item>
    <item>
      <title>Bayesian Active Learning for Bayesian Model Updating: the Art of Acquisition Functions and Beyond</title>
      <link>https://arxiv.org/abs/2510.08974</link>
      <description>arXiv:2510.08974v2 Announce Type: replace-cross 
Abstract: Estimating posteriors and the associated model evidences, with desired accuracy and affordable computational cost, is a core issue of Bayesian model updating, and can be of great challenge given expensive-to-evaluate models and posteriors with complex features such as multi-modalities of unequal importance, nonlinear dependencies and high sharpness. Bayesian Quadrature (BQ) equipped with active learning has emerged as a competitive framework for tackling this challenge, as it provides flexible balance between computational cost and accuracy. The performance of a BQ scheme is fundamentally dictated by the acquisition function as it exclusively governs the active generation of integration points. After reexamining one of the most advanced acquisition function from a prospective inference perspective and reformulating the quadrature rules for prediction, four new acquisition functions, inspired by distinct intuitions on expected rewards, are primarily developed, all of which are accompanied by elegant interpretations and highly efficient numerical estimators. Mathematically, these four acquisition functions measure, respectively, the prediction uncertainty of posterior, the contribution to prediction uncertainty of evidence, as well as the expected reduction of prediction uncertainties concerning posterior and evidence, and thus provide flexibility for highly effective design of integration points. These acquisition functions are further extended to the transitional BQ scheme, along with several specific refinements, to tackle the above-mentioned challenges with high efficiency and robustness. Effectiveness of the developments is ultimately demonstrated with extensive benchmark studies and application to an engineering example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08974v2</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jingwen Song, Pengfei Wei</dc:creator>
    </item>
    <item>
      <title>Adaptive Multilevel Newton: A Quadratically Convergent Optimization Method</title>
      <link>https://arxiv.org/abs/2510.24967</link>
      <description>arXiv:2510.24967v2 Announce Type: replace-cross 
Abstract: Newton's method may exhibit slower convergence than vanilla Gradient Descent in its initial phase on strongly convex problems. Classical Newton-type multilevel methods mitigate this but, like Gradient Descent, achieve only linear convergence near the minimizer. We introduce an adaptive multilevel Newton-type method with a principled automatic switch to full Newton once its quadratic phase is reached. The local quadratic convergence for strongly convex functions with Lipschitz continuous Hessians and for self-concordant functions is established and confirmed empirically. Although per-iteration cost can exceed that of classical multilevel schemes, the method is efficient and consistently outperforms Newton's method, Gradient Descent, and the multilevel Newton method, indicating that second-order methods can outperform first-order methods even when Newton's method is initially slow. The promising empirical results open new avenues for designing reduced-cost second- and high-order methods with extremely fast convergence rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24967v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick Tsipinakis, Panos Parpas, Matthias Voigt</dc:creator>
    </item>
  </channel>
</rss>
