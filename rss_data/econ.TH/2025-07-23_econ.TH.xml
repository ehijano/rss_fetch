<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.TH</link>
    <description>econ.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Jul 2025 04:07:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Robust Tournaments</title>
      <link>https://arxiv.org/abs/2507.16348</link>
      <description>arXiv:2507.16348v1 Announce Type: new 
Abstract: We characterize robust tournament design -- the prize scheme that maximizes the lowest effort in a rank-order tournament where the distribution of noise is unknown, except for an upper bound, $\bar{H}$, on its Shannon entropy. The robust tournament scheme awards positive prizes to all ranks except the last, with a distinct top prize. Asymptotically, the prizes follow the harmonic number sequence and induce an exponential distribution of noise with rate parameter $e^{-\bar{H}}$. The robust prize scheme is highly unequal, especially in small tournaments, but becomes more equitable as the number of participants grows, with the Gini coefficient approaching $1/2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16348v1</guid>
      <category>econ.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikhail Drugov, Dmitry Ryvkin</dc:creator>
    </item>
    <item>
      <title>A comparison between behavioral similarity methods vs standard deviation method in predicting time series dataset, case study of finance market</title>
      <link>https://arxiv.org/abs/2507.16655</link>
      <description>arXiv:2507.16655v1 Announce Type: new 
Abstract: In statistical modeling, prediction and explanation are two fundamental objectives. When the primary goal is forecasting, it is important to account for the inherent uncertainty associated with estimating unknown outcomes. Traditionally, confidence intervals constructed using standard deviations have served as a formal means to quantify this uncertainty and evaluate the closeness of predicted values to their true counterparts. This approach reflects an implicit aim to capture the behavioral similarity between observed and estimated values. However, advances in similarity based approaches present promising alternatives to conventional variance based techniques, particularly in contexts characterized by large datasets or a high number of explanatory variables. This study aims to investigate which methods either traditional or similarity based are capable of producing narrower confidence intervals under comparable conditions, thereby offering more precise and informative intervals. The dataset utilized in this study consists of U.S. mega cap companies, comprising 42 firms. Due to the high number of features, interdependencies among predictors are common, therefore, Ridge Regression is applied to address this issue. The research findings indicate that variance based method and LCSS exhibit the highest coverage among the analyzed methods, although they produce broader intervals. Conversely, DTW, Hausdorff, and TWED deliver narrower intervals, positioning them as the most accurate methods, despite their medium coverage rates. Ultimately, the trade off between interval width and coverage underscores the necessity for context aware decision making when selecting similarity based methods for confidence interval estimation in time series analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16655v1</guid>
      <category>econ.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Goldani</dc:creator>
    </item>
    <item>
      <title>The Root of Revenue Continuity</title>
      <link>https://arxiv.org/abs/2507.15735</link>
      <description>arXiv:2507.15735v1 Announce Type: cross 
Abstract: In the setup of selling one or more goods, various papers have shown, in various forms and for various purposes, that a small change in the distribution of a buyer's valuations may cause only a small change in the possible revenue that can be extracted. We prove a simple, clean, convenient, and general statement to this effect: let X and Y be random valuations on k additive goods, and let W(X,Y) be the Wasserstein (or "earth mover's") distance between them; then sqrt(Rev(X))-sqrt(Rev(Y)) &lt;= sqrt(W(X,Y)). This further implies that a simple explicit modification of any optimal mechanism for X, namely, "uniform discounting", is guaranteed to be almost optimal for any Y that is close to X in the Wasserstein distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15735v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sergiu Hart, Noam Nisan</dc:creator>
    </item>
    <item>
      <title>Shapley-Scarf Markets with Objective Indifferences</title>
      <link>https://arxiv.org/abs/2503.18144</link>
      <description>arXiv:2503.18144v2 Announce Type: replace 
Abstract: In many object allocation problems, some of the objects may be indistinguishable from each other. For example, in a college dormitory, rooms in the same building with the same floor plan are effectively identical. In such cases, it is reasonable to assume that agents are indifferent between identical objects, and matching mechanisms in these settings should account for the agents' indifferences. Top trading cycles (TTC) with fixed tie-breaking has been suggested and used in practice to deal with indifferences in object allocation problems. Under general indifferences, TTC with fixed tie-breaking is neither Pareto efficient nor group strategy-proof. Furthermore, it may not select an allocation in the core of the market, even when the core is non-empty. We introduce a new setting, objective indifferences, in which any indifferences are shared by all agents. In this setting, which includes strict preferences as a special case, TTC with fixed tie-breaking maintains Pareto efficiency, group strategy-proofness, and core selection. Further, we characterize objective indifferences as the most general setting where TTC with fixed tie-breaking maintains these important properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18144v2</guid>
      <category>econ.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Will Sandholtz, Andrew Tai</dc:creator>
    </item>
    <item>
      <title>Competitive Information Disclosure with Heterogeneous Consumer Search</title>
      <link>https://arxiv.org/abs/2504.04659</link>
      <description>arXiv:2504.04659v2 Announce Type: replace 
Abstract: We study a model of competitive information design in an oligopoly search market with heterogeneous consumer search costs. A unique class of equilibria -- upper-censorship equilibria -- emerges under intense competition. In equilibrium, firms balance competitive pressure with local monopoly power granted by search frictions. Notably, firms disclose only partial information even as the number of firms approaches infinity. The maximal informativeness of equilibrium decreases under first-order shifts in the search cost distribution, but varies non-monotonically under mean-preserving spreads. The model converges to the full-disclosure benchmark as search frictions vanish, and to the no-disclosure benchmark as search costs become homogeneous. Moreover, we show that the well-known discontinuities of equilibrium with respect to the search cost distribution in the price search literature carry over to information competition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04659v2</guid>
      <category>econ.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dongjin Hwang, Ilwoo Hwang</dc:creator>
    </item>
    <item>
      <title>Dynamic SINR-Guided Iterative Interference Cancellation for ODDM Systems in Doubly Dispersive Channels</title>
      <link>https://arxiv.org/abs/2507.00397</link>
      <description>arXiv:2507.00397v4 Announce Type: replace 
Abstract: Orthogonal delay-Doppler division multiplexing (ODDM) modulation has recently gained significant attention as a promising candidate to promote the communication reliability in high-mobility environments. Low complexity signal detection is one of the most significant challenges for ODDM over general physical channels, due to the large channel spreading caused by the fractional delay and Doppler shifts. In this paper, we investigate the low-complexity data detection for ODDM system by utilizing iterative interference cancellation. Based on the theoretical analysis of signal to interference plus noise ratio (SINR) during the iteration, a dynamic SINR-guided approach is proposed to provide a better initialization result. Specifically, we analyze the SINR of each time domain sample before initial estimate with consideration of off-grid delay and Doppler shifts. The iteration is then started from the multi-carrier symbol index which has the best SINR. The corresponding interference is then eliminated for other time domain samples while the SINR for symbol awaiting detection is also updated. Based on the updated SINR, the next multi-carrier symbol index is selected for the same processing until all data symbols have been initialized. Finally, we update the SINR synchronously until the end of the initialization. Simulation experiments indicate that our proposed algorithms demonstrate satisfying convergence and error performance while avoiding the huge complexity introduced by full linear minimum mean squared error (LMMSE) initialization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00397v4</guid>
      <category>econ.TH</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiasong Han, Xuehan Wang, Jintao Wang</dc:creator>
    </item>
  </channel>
</rss>
