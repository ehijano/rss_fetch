<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Sep 2024 01:40:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Stochastic Weather Model: A Case of Bono Region of Ghana</title>
      <link>https://arxiv.org/abs/2409.06731</link>
      <description>arXiv:2409.06731v1 Announce Type: new 
Abstract: The paper sought to fit an Ornstein Uhlenbeck model with seasonal mean and volatility, where the residuals are generated by a Brownian motion for Ghanian daily average temperature. This paper employed the modified Ornstein Uhlenbeck model proposed by Bhowan which has a seasonal mean and stochastic volatility process. The findings revealed that, the Bono region experiences warm temperatures and maximum precipitation up to 32.67 degree celsius and 126.51mm respectively. It was observed that the Daily Average Temperature (DAT) of the region reverts to a temperature of approximately 26 degree celsius at a rate of 18.72% with maximum and minimum temperatures of 32.67degree celsius and 19.75degree celsius respectively. Although the region is in the middle belt of Ghana, it still experiences warm(hot) temperatures daily and experiences dry seasons relatively more than wet seasons in the number of years considered for our analysis. Our model explained approximately 50% of the variations in the daily average temperature of the region which can be regarded as relatively a good model. The findings of this paper are relevant in the pricing of weather derivatives with temperature as an underlying variable in the Ghanaian financial and agricultural sector. Furthermore, it would assist in the development and design of tailored agriculture/crop insurance models which would incorporate temperature dynamics rather than extreme weather conditions/events such as floods, drought and wildfires.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06731v1</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Bernard Gyamfi</dc:creator>
    </item>
    <item>
      <title>Kramnik vs Nakamura: A Chess Scandal</title>
      <link>https://arxiv.org/abs/2409.06739</link>
      <description>arXiv:2409.06739v1 Announce Type: new 
Abstract: We provide a statistical analysis of the recent controversy between Vladimir Kramnik (ex chess world champion) and Hikaru Nakamura. Hikaru Nakamura is a chess prodigy and a five-time United States chess champion. Kramnik called into question Nakamura's 45.5 out of 46 win streak in an online blitz contest at chess.com. We assess the weight of evidence using a priori assessment of Viswanathan Anand and the streak evidence. Based on this evidence, we show that Nakamura has a 99.6 percent chance of not cheating. We study the statistical fallacies prevalent in both their analyses. On the one hand Kramnik bases his argument on the probability of such a streak is very small. This falls precisely into the Prosecutor's Fallacy. On the other hand, Nakamura tries to refute the argument using a cherry-picking argument. This violates the likelihood principle. We conclude with a discussion of the relevant statistical literature on the topic of fraud detection and the analysis of streaks in sports data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06739v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiva Maharaj, Nick Polson, Vadim Sokolov</dc:creator>
    </item>
    <item>
      <title>Dynamic Bayesian Networks, Elicitation and Data Embedding for Secure Environments</title>
      <link>https://arxiv.org/abs/2409.07389</link>
      <description>arXiv:2409.07389v1 Announce Type: new 
Abstract: Serious crime modelling typically needs to be undertaken securely behind a firewall where police knowledge and capabilities can remain undisclosed. Data informing an ongoing incident is often sparse, with a large proportion of relevant data only coming to light after the incident culminates or after police intervene - by which point it is too late to make use of the data to aid real-time decision making for the incident in question. Much of the data that is available to police to support real-time decision making is highly confidential so cannot be shared with academics, and is therefore missing to them. In this paper, we describe the development of a formal protocol where a graphical model is used as a framework for securely translating a model designed by an academic team to a model for use by a police team. We then show, for the first time, how libraries of these models can be built and used for real-time decision support to circumvent the challenges of data missingness and tardiness seen in such a secure environment. The parallel development described by this protocol ensures that any sensitive information collected by police, and missing to academics, remains secured behind a firewall. The protocol nevertheless guides police so that they are able to combine the typically incomplete data streams that are open source with their more sensitive information in a formal and justifiable way. We illustrate the application of this protocol by describing how a new entry - a suspected vehicle attack - can be embedded into such a police library of criminal plots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07389v1</guid>
      <category>stat.AP</category>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kieran Drury, Jim Q. Smith</dc:creator>
    </item>
    <item>
      <title>Stratospheric aerosol source inversion: Noise, variability, and uncertainty quantification</title>
      <link>https://arxiv.org/abs/2409.06846</link>
      <description>arXiv:2409.06846v1 Announce Type: cross 
Abstract: Stratospheric aerosols play an important role in the earth system and can affect the climate on timescales of months to years. However, estimating the characteristics of partially observed aerosol injections, such as those from volcanic eruptions, is fraught with uncertainties. This article presents a framework for stratospheric aerosol source inversion which accounts for background aerosol noise and earth system internal variability via a Bayesian approximation error approach. We leverage specially designed earth system model simulations using the Energy Exascale Earth System Model (E3SM). A comprehensive framework for data generation, data processing, dimension reduction, operator learning, and Bayesian inversion is presented where each component of the framework is designed to address particular challenges in stratospheric modeling on the global scale. We present numerical results using synthesized observational data to rigorously assess the ability of our approach to estimate aerosol sources and associate uncertainty with those estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06846v1</guid>
      <category>cs.LG</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Hart, I. Manickam, M. Gulian, L. Swiler, D. Bull, T. Ehrmann, H. Brown, B. Wagman, J. Watkins</dc:creator>
    </item>
    <item>
      <title>Toward Model-Agnostic Detection of New Physics Using Data-Driven Signal Regions</title>
      <link>https://arxiv.org/abs/2409.06960</link>
      <description>arXiv:2409.06960v1 Announce Type: cross 
Abstract: In the search for new particles in high-energy physics, it is crucial to select the Signal Region (SR) in such a way that it is enriched with signal events if they are present. While most existing search methods set the region relying on prior domain knowledge, it may be unavailable for a completely novel particle that falls outside the current scope of understanding. We address this issue by proposing a method built upon a model-agnostic but often realistic assumption about the localized topology of the signal events, in which they are concentrated in a certain area of the feature space. Considering the signal component as a localized high-frequency feature, our approach employs the notion of a low-pass filter. We define the SR as an area which is most affected when the observed events are smeared with additive random noise. We overcome challenges in density estimation in the high-dimensional feature space by learning the density ratio of events that potentially include a signal to the complementary observation of events that closely resemble the target events but are free of any signals. By applying our method to simulated $\mathrm{HH} \rightarrow 4b$ events, we demonstrate that the method can efficiently identify a data-driven SR in a high-dimensional feature space in which a high portion of signal events concentrate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06960v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soheun Yi, John Alison, Mikael Kuusela</dc:creator>
    </item>
    <item>
      <title>Testing for a Forecast Accuracy Breakdown under Long Memory</title>
      <link>https://arxiv.org/abs/2409.07087</link>
      <description>arXiv:2409.07087v1 Announce Type: cross 
Abstract: We propose a test to detect a forecast accuracy breakdown in a long memory time series and provide theoretical and simulation evidence on the memory transfer from the time series to the forecast residuals. The proposed method uses a double sup-Wald test against the alternative of a structural break in the mean of an out-of-sample loss series. To address the problem of estimating the long-run variance under long memory, a robust estimator is applied. The corresponding breakpoint results from a long memory robust CUSUM test. The finite sample size and power properties of the test are derived in a Monte Carlo simulation. A monotonic power function is obtained for the fixed forecasting scheme. In our practical application, we find that the global energy crisis that began in 2021 led to a forecast break in European electricity prices, while the results for the U.S. are mixed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07087v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jannik Kreye, Philipp Sibbertsen</dc:creator>
    </item>
    <item>
      <title>Extended-support beta regression for $[0, 1]$ responses</title>
      <link>https://arxiv.org/abs/2409.07233</link>
      <description>arXiv:2409.07233v1 Announce Type: cross 
Abstract: We introduce the XBX regression model, a continuous mixture of extended-support beta regressions for modeling bounded responses with or without boundary observations. The core building block of the new model is the extended-support beta distribution, which is a censored version of a four-parameter beta distribution with the same exceedance on the left and right of $(0, 1)$. Hence, XBX regression is a direct extension of beta regression. We prove that both beta regression with dispersion effects and heteroscedastic normal regression with censoring at both $0$ and $1$ -- known as the heteroscedastic two-limit tobit model in the econometrics literature -- are special cases of the extended-support beta regression model, depending on whether a single extra parameter is zero or infinity, respectively. To overcome identifiability issues that may arise in estimating the extra parameter due to the similarity of the beta and normal distribution for certain parameter settings, we assume that the additional parameter has an exponential distribution with an unknown mean. The associated marginal likelihood can be conveniently and accurately approximated using a Gauss-Laguerre quadrature rule, resulting in efficient estimation and inference procedures. The new model is used to analyze investment decisions in a behavioral economics experiment, where the occurrence and extent of loss aversion is of interest. In contrast to standard approaches, XBX regression can simultaneously capture the probability of rational behavior as well as the mean amount of loss aversion. Moreover, the effectiveness of the new model is illustrated through extensive numerical comparisons with alternative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07233v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Kosmidis, Achim Zeileis</dc:creator>
    </item>
    <item>
      <title>Joint Modeling of Biomarker Cascades Along An Unobserved Disease Progression with Differentiate Covariate Effects: An Application in Alzheimer's Disease</title>
      <link>https://arxiv.org/abs/2304.09754</link>
      <description>arXiv:2304.09754v3 Announce Type: replace 
Abstract: Alzheimer's Disease (AD) research has shifted to focus on biomarker trajectories and their potential use in understanding the underlying AD-related pathological process. A conceptual framework was proposed in modern AD research that hypothesized biomarker cascades as a result of underlying AD pathology. In this paper, we leveraged this idea to jointly model AD biomarker trajectories as a function of the latent AD disease progression with individual and covariate effects in the latent disease progression model and the biomarker cascade. We tailored our methods to address a number of real-data challenges that are often present in AD studies. Simulation studies were performed to investigate the proposed approach under various realistic but less-than-ideal situations. Finally, we illustrated the methods using real data from the BIOCARD and the ADNI studies. The analyses investigated cascading patterns of AD biomarkers in these datasets and presented prediction results for individual-level profiles over time. These findings highlight the potential of the conceptual biomarker cascade framework to be leveraged for diagnosis and monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.09754v3</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuojun Tang, Yuxin Zhu, Kexin Zhang, Zheyu Wang</dc:creator>
    </item>
    <item>
      <title>Recurrent Neural Networks for Multivariate Loss Reserving and Risk Capital Analysis</title>
      <link>https://arxiv.org/abs/2402.10421</link>
      <description>arXiv:2402.10421v2 Announce Type: replace 
Abstract: In the property and casualty (P&amp;C) insurance industry, reserves comprise most of a company's liabilities. These reserves are the best estimates made by actuaries for future unpaid claims. Notably, reserves for different lines of business (LOBs) are related due to dependent events or claims. While the actuarial industry has developed both parametric and non-parametric methods for loss reserving, only a few tools have been developed to capture dependence between loss reserves. This paper introduces the use of recurrent neural network (RNN) methods for loss reserving domain to model pairwise dependence and time dependence of incremental payments and generate predictive distributions for reserves. We construct an RNN model to capture the complex dependencies between LOBs by expanding the Deep Triangle (DT) model from Kuo (2019) for one LOB. We then extend generative adversarial networks (GANs) by transforming the two loss triangles into a tabular format and generating synthetic loss triangles to obtain the predictive distribution for reserves. The proposed method, which involves building a predictive distribution for the reserve along with the DT model, is called the extended Deep Triangle (EDT). To illustrate EDT, we apply and calibrate these methods using data from 30 companies from the National Association of Insurance Commissioners database (Meyers and Shi, 2011) and compare the results with copula regression models. The findings indicate that the EDT model outperforms the copula regression models in predicting total loss reserve. Furthermore, with the obtained predictive distribution for reserves, we show that risk capitals calculated from EDT are smaller than that of the copula regression models, suggesting a more considerable diversification benefit. Finally, these findings are also confirmed in a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10421v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengfei Cai, Anas Abdallah, Pratheepa Jeganathan</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference General Procedures for A Single-subject Test Study</title>
      <link>https://arxiv.org/abs/2408.15419</link>
      <description>arXiv:2408.15419v3 Announce Type: replace 
Abstract: Abnormality detection in the identification of a single-subject which deviates from the majority of the dataset that comes from a control group is a critical problem. A common approach is to assume that the control group can be characterised in terms of standard Normal statistics and the detection of single abnormal subject is in that context. But in many situations the control group can not be described in terms of Normal statistics and the use of standard statistics is inappropriate. This paper presents a Bayesian Inference General Procedures for A Single-Subject Test (BIGPAST), designed to mitigate the effects of skewness under the assumption that the dataset of control group comes from the skewed Student's \( t \) distribution. BIGPAST operates under the null hypothesis that the single-subject follows the same distribution as the control group. We assess BIGPAST's performance against other methods through a series of simulation studies. The results demonstrate that BIGPAST is robust against deviations from normality and outperforms the existing approaches in terms of accuracy. This is because BIGPAST can effectively reduce model misspecification errors under the skewed Student's \( t \) assumption. We apply BIGPAST to a MEG dataset consisting of an individual with mild traumatic brain injury and an age and gender-matched control group, demonstrating its effectiveness in detecting abnormalities in the single-subject.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15419v3</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jie Li, Gary Green, Sarah J. A. Carr, Peng Liu, Jian Zhang</dc:creator>
    </item>
    <item>
      <title>Random matching in balanced bipartite graphs: The (un)fairness of draw mechanisms used in sports</title>
      <link>https://arxiv.org/abs/2303.09274</link>
      <description>arXiv:2303.09274v4 Announce Type: replace-cross 
Abstract: The draw of some knockout tournaments requires finding a perfect matching in a balanced bipartite graph. The problem becomes challenging with draw constraints: the two field-proven procedures used in sports are known to be non-uniformly distributed (the feasible matchings are not equally likely), which may threaten fairness. We compare the biases of both mechanisms, each of them having two forms, for reasonable subsets of balanced bipartite graphs up to 16 nodes. A mechanism is found to dominate all others in the draw of quarterfinals under reasonable restrictions. The UEFA Champions League Round of 16 draw is verified to apply the best design among the four available options between the 2003/04 and 2023/24 seasons. However, considerable scope remains to improve the performance of these randomisation procedures, especially because they tend to distort the probabilities in the same direction and roughly with the same magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.09274v4</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o</dc:creator>
    </item>
    <item>
      <title>Model-assisted analysis of covariance estimators for stepped wedge cluster randomized experiments</title>
      <link>https://arxiv.org/abs/2306.11267</link>
      <description>arXiv:2306.11267v4 Announce Type: replace-cross 
Abstract: Stepped wedge cluster randomized experiments (SW-CREs) represent a class of unidirectional crossover designs. Although SW-CREs have become popular, definitions of estimands and robust methods to target estimands under the potential outcomes framework remain insufficient. To address this gap, we describe a class of estimands that explicitly acknowledge the multilevel data structure in SW-CREs and highlight three typical members of the estimand class that are interpretable. We then introduce four analysis of covariance (ANCOVA) working models to achieve estimand-aligned analyses with covariate adjustment. Each ANCOVA estimator is model-assisted, as its point estimator is consistent even when the working model is misspecified. Under the stepped wedge randomization scheme, we establish the finite population Central Limit Theorem for each estimator. We study the finite-sample operating characteristics of the ANCOVA estimators in simulations and illustrate their application by analyzing the Washington State Expedited Partner Therapy study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11267v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Chen, Fan Li</dc:creator>
    </item>
    <item>
      <title>Bayesian Quantile Regression with Subset Selection: A Posterior Summarization Perspective</title>
      <link>https://arxiv.org/abs/2311.02043</link>
      <description>arXiv:2311.02043v3 Announce Type: replace-cross 
Abstract: Quantile regression is a powerful tool in epidemiological studies where interest lies in inferring how different exposures affect specific percentiles of the distribution of a health or life outcome. Existing methods either estimate conditional quantiles separately for each quantile of interest or estimate the entire conditional distribution using semi- or non-parametric models. The former often produce inadequate models for real data and do not share information across quantiles, while the latter are characterized by complex and constrained models that can be difficult to interpret and computationally inefficient. Further, neither approach is well-suited for quantile-specific subset selection. Instead, we pose the fundamental problems of linear quantile estimation, uncertainty quantification, and subset selection from a Bayesian decision analysis perspective. For any Bayesian regression model, we derive optimal and interpretable linear estimates and uncertainty quantification for each model-based conditional quantile. Our approach introduces a quantile-focused squared error loss, which enables efficient, closed-form computing and maintains a close relationship with Wasserstein-based density estimation. In an extensive simulation study, our methods demonstrate substantial gains in quantile estimation accuracy, variable selection, and inference over frequentist and Bayesian competitors. We use these tools to identify and quantify the heterogeneous impacts of multiple social stressors and environmental exposures on educational outcomes across the full spectrum of low-, medium-, and high-achieving students in North Carolina.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02043v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph Feldman, Daniel Kowal</dc:creator>
    </item>
    <item>
      <title>A clustering approach for pairwise comparison matrices</title>
      <link>https://arxiv.org/abs/2402.06061</link>
      <description>arXiv:2402.06061v4 Announce Type: replace-cross 
Abstract: We consider clustering in group decision making where the opinions are given by pairwise comparison matrices. In particular, the k-medoids model is suggested to classify the matrices since it has a linear programming problem formulation that may contain any condition on the properties of the cluster centres. Its objective function depends on the measure of dissimilarity between the matrices but not on the weights derived from them. Our methodology provides a convenient tool for decision support, for instance, it can be used to quantify the reliability of the aggregation. The proposed theoretical framework is applied to a large-scale experimental dataset, on which it is able to automatically detect some mistakes made by the decision-makers, as well as to identify a common source of inconsistency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06061v4</guid>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kolos Csaba \'Agoston, S\'andor Boz\'oki, L\'aszl\'o Csat\'o</dc:creator>
    </item>
    <item>
      <title>Robust estimations from distribution structures: I. Mean</title>
      <link>https://arxiv.org/abs/2403.12110</link>
      <description>arXiv:2403.12110v5 Announce Type: replace-cross 
Abstract: As the most fundamental problem in statistics, robust location estimation has many prominent solutions, such as the trimmed mean, Winsorized mean, Hodges Lehmann estimator, Huber M estimator, and median of means. Recent studies suggest that their maximum biases concerning the mean can be quite different, but the underlying mechanisms largely remain unclear. This study exploited a semiparametric method to classify distributions by the asymptotic orderliness of quantile combinations with varying breakdown points, showing their interrelations and connections to parametric distributions. Further deductions explain why the Winsorized mean typically has smaller biases compared to the trimmed mean; two sequences of semiparametric robust mean estimators emerge, particularly highlighting the superiority of the median Hodges Lehmann mean. This article sheds light on the understanding of the common nature of probability distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12110v5</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Tuobang</dc:creator>
    </item>
    <item>
      <title>Robust estimations from distribution structures: V. Non-asymptotic</title>
      <link>https://arxiv.org/abs/2403.18951</link>
      <description>arXiv:2403.18951v3 Announce Type: replace-cross 
Abstract: Due to the complexity of order statistics, the finite sample behaviour of robust statistics is generally not analytically solvable. While the Monte Carlo method can provide approximate solutions, its convergence rate is typically very slow, making the computational cost to achieve the desired accuracy unaffordable for ordinary users. In this paper, we propose an approach analogous to the Fourier transformation to decompose the finite sample structure of the uniform distribution. By obtaining sets of sequences that are consistent with parametric distributions for the first four sample moments, we can approximate the finite sample behavior of other estimators with significantly reduced computational costs. This article reveals the underlying structure of randomness and presents a novel approach to integrate multiple assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18951v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <pubDate>Thu, 12 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Tuobang</dc:creator>
    </item>
  </channel>
</rss>
