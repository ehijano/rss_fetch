<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Apr 2024 04:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Using early rejection Markov chain Monte Carlo and Gaussian processes to accelerate ABC methods</title>
      <link>https://arxiv.org/abs/2404.08898</link>
      <description>arXiv:2404.08898v1 Announce Type: new 
Abstract: Approximate Bayesian computation (ABC) is a class of Bayesian inference algorithms that targets for problems with intractable or {unavailable} likelihood function. It uses synthetic data drawn from the simulation model to approximate the posterior distribution. However, ABC is computationally intensive for complex models in which simulating synthetic data is very expensive. In this article, we propose an early rejection Markov chain Monte Carlo (ejMCMC) sampler based on Gaussian processes to accelerate inference speed. We early reject samples in the first stage of the kernel using a discrepancy model, in which the discrepancy between the simulated and observed data is modeled by Gaussian process (GP). Hence, the synthetic data is generated only if the parameter space is worth exploring. We demonstrate from theory, simulation experiments, and real data analysis that the new algorithm significantly improves inference efficiency compared to existing early-rejection MCMC algorithms. In addition, we employ our proposed method within an ABC sequential Monte Carlo (SMC) sampler. In our numerical experiments, we use examples of ordinary differential equations, stochastic differential equations, and delay differential equations to demonstrate the effectiveness of the proposed algorithm. We develop an R package that is available at https://github.com/caofff/ejMCMC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08898v1</guid>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuefei Cao, Shijia Wang, Yongdao Zhou</dc:creator>
    </item>
    <item>
      <title>Central and noncentral moments of the multivariate hypergeometric distribution</title>
      <link>https://arxiv.org/abs/2404.09118</link>
      <description>arXiv:2404.09118v1 Announce Type: cross 
Abstract: In this short note, explicit formulas are developed for the central and noncentral moments of the multivariate hypergeometric distribution. A numerical implementation is provided in Mathematica for fast evaluations. This work complements the paper by Ouimet (2021), where analogous formulas were derived and implemented in Mathematica for the multinomial distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09118v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>The Challenges of Optimization For Data Science</title>
      <link>https://arxiv.org/abs/2404.09810</link>
      <description>arXiv:2404.09810v1 Announce Type: cross 
Abstract: Optimization problems arising in data science have given rise to a number of new derivative-based optimization methods. Such methods often use standard smoothness assumptions -- namely, global Lipschitz continuity of the gradient function -- to establish a convergence theory. Unfortunately, in this work, we show that common optimization problems from data science applications are not globally Lipschitz smooth, nor do they satisfy some more recently developed smoothness conditions in literature. Instead, we show that such optimization problems are better modeled as having locally Lipschitz continuous gradients. We then construct explicit examples satisfying this assumption on which existing classes of optimization methods are either unreliable or experience an explosion in evaluation complexity. In summary, we show that optimization problems arising in data science are particularly difficult to solve, and that there is a need for methods that can reliably and practically solve these problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09810v1</guid>
      <category>math.OC</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Varner, Vivak Patel</dc:creator>
    </item>
    <item>
      <title>sfislands: An R Package for Accommodating Islands and Disjoint Zones in Areal Spatial Modelling</title>
      <link>https://arxiv.org/abs/2404.09863</link>
      <description>arXiv:2404.09863v1 Announce Type: cross 
Abstract: Fitting areal models which use a spatial weights matrix to represent relationships between geographical units can be a cumbersome task, particularly when these units are not well-behaved. The two chief aims of sfislands are to simplify the process of creating an appropriate neighbourhood matrix, and to quickly visualise the predictions of subsequent models. The package uses visual aids in the form of easily-generated maps to help this process. This paper demonstrates how sfislands could be useful to researchers. It begins by describing the package's functions in the context of a proposed workflow. It then presents three worked examples showing a selection of potential use-cases. These range from earthquakes in Indonesia, to river crossings in London, and hierarchical models of output areas in Liverpool. We aim to show how the sfislands package streamlines much of the human workflow involved in creating and examining such models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09863v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Horan, Katarina Domijan, Chris Brunsdon</dc:creator>
    </item>
    <item>
      <title>Time-Heterogeneity of the F\"orster Radius from Dipole Orientational Dynamics Explains Observed Dynamic Shift</title>
      <link>https://arxiv.org/abs/2404.09883</link>
      <description>arXiv:2404.09883v1 Announce Type: cross 
Abstract: F\"orster resonance energy transfer (FRET) is a quantum mechanical phenomenon involving the non-radiative transfer of energy between coupled electric dipoles. Due to the strong dependence of FRET on the distance between the dipoles, it is frequently used as a ``molecular ruler" in biology, chemistry, and physics. This is done by placing dipolar molecules called dyes on molecules of interest. In time-resolved confocal single-molecule FRET (smFRET) experiments, the joint distribution of the FRET efficiency and the donor fluorescence lifetime can reveal underlying molecular conformational dynamics via deviation from their theoretical F\"orster relationship. This deviation is referred to as a dynamic shift. Quantifying the dynamic shift caused by the motion of the fluorescent dyes is essential to decoupling the dynamics of the studied molecules and the dyes. We develop novel Langevin models for the dye linker dynamics, including rotational dynamics, based on first physics principles and proper dye linker chemistry to match accessible volumes predicted by molecular dynamics simulations. By simulating the dyes' stochastic translational and rotational dynamics, we show that the observed dynamic shift can largely be attributed to the mutual orientational dynamics of the electric dipole moments associated with the dyes, not their accessible volume.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09883v1</guid>
      <category>physics.chem-ph</category>
      <category>physics.bio-ph</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Frost, Keisha Cook, Hugo Sanabria</dc:creator>
    </item>
    <item>
      <title>clrng: A tool set for parallel random numbergeneration on GPUs in R</title>
      <link>https://arxiv.org/abs/2201.06604</link>
      <description>arXiv:2201.06604v4 Announce Type: replace 
Abstract: We introduce the R package clrng which leverages the gpuR package and is able to generate random numbers in parallel on a Graphics Processing Unit (GPU) with the clRNG (OpenCL) library. Parallel processing with GPU's can speed up computationally intensive tasks, which when combined with R, it can largely improve R's downsides in terms of slow speed, memory usage and computation mode. clrng enables reproducible research by setting random initial seeds for streams on GPU and CPU, and can thus accelerate several types of statistical simulation and modelling. The random number generator in clrng guarantees independent parallel samples even when R is used interactively in an ad-hoc manner, with sessions being interrupted and restored. This package is portable and flexible, developers can use its random number generation kernel for various other purposes and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.06604v4</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruoyong Xu, Patrick Brown, Pierre L'Ecuyer</dc:creator>
    </item>
    <item>
      <title>mpower: An R Package for Power Analysis of Exposure Mixture Studies via Monte Carlo Simulations</title>
      <link>https://arxiv.org/abs/2209.08036</link>
      <description>arXiv:2209.08036v2 Announce Type: replace-cross 
Abstract: Estimating sample size and statistical power is an essential part of a good study design. This R package allows users to conduct power analysis based on Monte Carlo simulations in settings in which consideration of the correlations between predictors is important. It runs power analyses given a data generative model and an inference model. It can set up a data generative model that preserves dependence structures among variables given existing data (continuous, binary, or ordinal) or high-level descriptions of the associations. Users can generate power curves to assess the trade-offs between sample size, effect size, and power of a design. This paper presents tutorials and examples focusing on applications for environmental mixture studies when predictors tend to be moderately to highly correlated. It easily interfaces with several existing and newly developed analysis strategies for assessing associations between exposures and health outcomes. However, the package is sufficiently general to facilitate power simulations in a wide variety of settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.08036v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s12561-023-09385-7</arxiv:DOI>
      <arxiv:journal_reference>Stat Biosci (2023)</arxiv:journal_reference>
      <dc:creator>Phuc H. Nguyen, Stephanie M. Engel, Amy H. Herring</dc:creator>
    </item>
    <item>
      <title>Penalized Overdamped and Underdamped Langevin Monte Carlo Algorithms for Constrained Sampling</title>
      <link>https://arxiv.org/abs/2212.00570</link>
      <description>arXiv:2212.00570v2 Announce Type: replace-cross 
Abstract: We consider the constrained sampling problem where the goal is to sample from a target distribution $\pi(x)\propto e^{-f(x)}$ when $x$ is constrained to lie on a convex body $\mathcal{C}$. Motivated by penalty methods from continuous optimization, we propose penalized Langevin Dynamics (PLD) and penalized underdamped Langevin Monte Carlo (PULMC) methods that convert the constrained sampling problem into an unconstrained sampling problem by introducing a penalty function for constraint violations. When $f$ is smooth and gradients are available, we get $\tilde{\mathcal{O}}(d/\varepsilon^{10})$ iteration complexity for PLD to sample the target up to an $\varepsilon$-error where the error is measured in the TV distance and $\tilde{\mathcal{O}}(\cdot)$ hides logarithmic factors. For PULMC, we improve the result to $\tilde{\mathcal{O}}(\sqrt{d}/\varepsilon^{7})$ when the Hessian of $f$ is Lipschitz and the boundary of $\mathcal{C}$ is sufficiently smooth. To our knowledge, these are the first convergence results for underdamped Langevin Monte Carlo methods in the constrained sampling that handle non-convex $f$ and provide guarantees with the best dimension dependency among existing methods with deterministic gradient. If unbiased stochastic estimates of the gradient of $f$ are available, we propose PSGLD and PSGULMC methods that can handle stochastic gradients and are scaleable to large datasets without requiring Metropolis-Hasting correction steps. For PSGLD and PSGULMC, when $f$ is strongly convex and smooth, we obtain $\tilde{\mathcal{O}}(d/\varepsilon^{18})$ and $\tilde{\mathcal{O}}(d\sqrt{d}/\varepsilon^{39})$ iteration complexity in W2 distance. When $f$ is smooth and can be non-convex, we provide finite-time performance bounds and iteration complexity results. Finally, we illustrate the performance on Bayesian LASSO regression and Bayesian constrained deep learning problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.00570v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mert G\"urb\"uzbalaban, Yuanhan Hu, Lingjiong Zhu</dc:creator>
    </item>
    <item>
      <title>Fast Variational Inference for Bayesian Factor Analysis in Single and Multi-Study Settings</title>
      <link>https://arxiv.org/abs/2305.13188</link>
      <description>arXiv:2305.13188v2 Announce Type: replace-cross 
Abstract: Factors models are routinely used to analyze high-dimensional data in both single-study and multi-study settings. Bayesian inference for such models relies on Markov Chain Monte Carlo (MCMC) methods which scale poorly as the number of studies, observations, or measured variables increase. To address this issue, we propose variational inference algorithms to approximate the posterior distribution of Bayesian latent factor models using the multiplicative gamma process shrinkage prior. The proposed algorithms provide fast approximate inference at a fraction of the time and memory of MCMC-based implementations while maintaining comparable accuracy in characterizing the data covariance matrix. We conduct extensive simulations to evaluate our proposed algorithms and show their utility in estimating the model for high-dimensional multi-study gene expression data in ovarian cancers. Overall, our proposed approaches enable more efficient and scalable inference for factor models, facilitating their use in high-dimensional settings. An R package VIMSFA implementing our methods is available on GitHub (github.com/blhansen/VI-MSFA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.13188v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Blake Hansen, Alejandra Avalos-Pacheco, Massimiliano Russo, Roberta De Vito</dc:creator>
    </item>
    <item>
      <title>On the Computational Complexity of Private High-dimensional Model Selection</title>
      <link>https://arxiv.org/abs/2310.07852</link>
      <description>arXiv:2310.07852v3 Announce Type: replace-cross 
Abstract: We consider the problem of model selection in a high-dimensional sparse linear regression model under privacy constraints. We propose a differentially private best subset selection method with strong utility properties by adopting the well-known exponential mechanism for selecting the best model. We propose an efficient Metropolis-Hastings algorithm and establish that it enjoys polynomial mixing time to its stationary distribution. Furthermore, we also establish approximate differential privacy for the estimates of the mixed Metropolis-Hastings chain. Finally, we perform some illustrative experiments that show the strong utility of our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07852v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saptarshi Roy, Zehua Wang, Ambuj Tewari</dc:creator>
    </item>
  </channel>
</rss>
