<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Nov 2025 02:50:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Large-scale Multigrid with Adaptive Galerkin Coarsening</title>
      <link>https://arxiv.org/abs/2511.13109</link>
      <description>arXiv:2511.13109v1 Announce Type: new 
Abstract: We propose a robust, adaptive coarse-grid correction scheme for matrix-free geometric multigrid targeting PDEs with strongly varying coefficients. The method combines uniform geometric coarsening of the underlying grid with heterogeneous coarse-grid operators: Galerkin coarse grid approximation is applied locally in regions with large coefficient gradients, while lightweight, direct coarse grid approximation is used elsewhere. This selective application ensures that local Galerkin operators are computed and stored only where necessary, minimizing memory requirements while maintaining robust convergence. We demonstrate the method on a suite of sinker benchmark problems for the generalized Stokes equation, including grid-aligned and unaligned viscosity jumps, smoothly varying viscosity functions with large gradients, and different viscosity evaluation techniques. We analytically quantify the solver's memory consumption and demonstrate its efficiency by solving Stokes problems with $10^{10}$ degrees of freedom, viscosity jumps of $10^{6}$ magnitude, and more than 100{,}000 parallel processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13109v1</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabian B\"ohm, Nils Kohl, Harald K\"ostler, Ulrich R\"ude</dc:creator>
    </item>
    <item>
      <title>Evaluation of Domain-Specific Architectures for General-Purpose Applications in Apple Silicon</title>
      <link>https://arxiv.org/abs/2511.13450</link>
      <description>arXiv:2511.13450v1 Announce Type: new 
Abstract: The rise of AI and its growing computational demands have driven the integration of domain-specific accelerators (such as GPUs, TPUs, and NPUs) across the entire computing infrastructure. Following the precedent set by the GPGPU which popularized GPUs for general-purpose tasks, this research asks whether this phenomenon can be replicated with specialized accelerators like NPUs in new contexts. This paper evaluates the potential of the Apple Neural Engine (ANE) designed for high energy efficiency in Machine Learning workloads, in the context of general-purpose HPC applications. We evaluate the performance and energy consumption of classic HPC algorithms such as GEMM, Jacobi or Multigrid methods on Apple's ANE across the M1 and the latest M4 architectures. Results confirm that, when algorithms are properly adapted, the ANE achieves competitive performance (up to 3.8 TFlops on the M4-Pro, comparable to the GPU's 4.7 TFlops on the same SoC for GEMM operation) while demonstrating significantly superior energy efficiency (e.g., GEMM consumes 5.2 Watts on the ANE versus 24 Watts on GPU counterpart in M4 architectures).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13450v1</guid>
      <category>cs.PF</category>
      <category>cs.ET</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>\'Alvaro Corrochano L\'opez, Carlos Garc\'ia S\'anchez</dc:creator>
    </item>
    <item>
      <title>EcoSpa: Efficient Transformer Training with Coupled Sparsity</title>
      <link>https://arxiv.org/abs/2511.11641</link>
      <description>arXiv:2511.11641v1 Announce Type: cross 
Abstract: Transformers have become the backbone of modern AI, yet their high computational demands pose critical system challenges. While sparse training offers efficiency gains, existing methods fail to preserve critical structural relationships between weight matrices that interact multiplicatively in attention and feed-forward layers. This oversight leads to performance degradation at high sparsity levels. We introduce EcoSpa, an efficient structured sparse training method that jointly evaluates and sparsifies coupled weight matrix pairs, preserving their interaction patterns through aligned row/column removal. EcoSpa introduces a new granularity for calibrating structural component importance and performs coupled estimation and sparsification across both pre-training and fine-tuning scenarios. Evaluations demonstrate substantial improvements: EcoSpa enables efficient training of LLaMA-1B with 50\% memory reduction and 21\% faster training, achieves $2.2\times$ model compression on GPT-2-Medium with $2.4$ lower perplexity, and delivers $1.6\times$ inference speedup. The approach uses standard PyTorch operations, requiring no custom hardware or kernels, making efficient transformer training accessible on commodity hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11641v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinqi Xiao, Cheng Luo, Lingyi Huang, Cheng Yang, Yang Sui, Huy Phan, Xiao Zang, Yibiao Ying, Zhexiang Tang, Anima Anandkumar, Bo Yuan</dc:creator>
    </item>
    <item>
      <title>Looking Forward: Challenges and Opportunities in Agentic AI Reliability</title>
      <link>https://arxiv.org/abs/2511.11921</link>
      <description>arXiv:2511.11921v1 Announce Type: cross 
Abstract: This chapter presents perspectives for challenges and future development in building reliable AI systems, particularly, agentic AI systems. Several open research problems related to mitigating the risks of cascading failures are discussed. The chapter also sheds lights on research challenges and opportunities in aspects including dynamic environments, inconsistent task execution, unpredictable emergent behaviors, as well as resource-intensive reliability mechanisms. In addition, several research directions along the line of testing and evaluating reliability of agentic AI systems are also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11921v1</guid>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.PF</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liudong Xing (Jing),  Janet (Jing),  Lin</dc:creator>
    </item>
    <item>
      <title>KForge: Program Synthesis for Diverse AI Hardware Accelerators</title>
      <link>https://arxiv.org/abs/2511.13274</link>
      <description>arXiv:2511.13274v1 Announce Type: cross 
Abstract: GPU kernels are critical for ML performance but difficult to optimize across diverse accelerators. We present KForge, a platform-agnostic framework built on two collaborative LLM-based agents: a generation agent that produces and iteratively refines programs through compilation and correctness feedback, and a performance analysis agent that interprets profiling data to guide optimization. This agent-based architecture requires only a single-shot example to target new platforms.
  We make three key contributions: (1) introducing an iterative refinement system where the generation agent and performance analysis agent collaborate through functional and optimization passes, interpreting diverse profiling data (from programmatic APIs to GUI-based tools) to generate actionable recommendations that guide program synthesis for arbitrary accelerators; (2) demonstrating that the generation agent effectively leverages cross-platform knowledge transfer, where a reference implementation from one architecture substantially improves generation quality for different hardware targets; and (3) validating the platform-agnostic nature of our approach by demonstrating effective program synthesis across fundamentally different parallel computing platforms: NVIDIA CUDA and Apple Metal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13274v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>cs.PF</category>
      <category>cs.SE</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taras Sereda, Tom St. John, Burak Bartan, Natalie Serrino, Sachin Katti, Zain Asgar</dc:creator>
    </item>
    <item>
      <title>Hardware optimization on Android for inference of AI models</title>
      <link>https://arxiv.org/abs/2511.13453</link>
      <description>arXiv:2511.13453v1 Announce Type: cross 
Abstract: The pervasive integration of Artificial Intelligence models into contemporary mobile computing is notable across numerous use cases, from virtual assistants to advanced image processing. Optimizing the mobile user experience involves minimal latency and high responsiveness from deployed AI models with challenges from execution strategies that fully leverage real time constraints to the exploitation of heterogeneous hardware architecture. In this paper, we research and propose the optimal execution configurations for AI models on an Android system, focusing on two critical tasks: object detection (YOLO family) and image classification (ResNet). These configurations evaluate various model quantization schemes and the utilization of on device accelerators, specifically the GPU and NPU. Our core objective is to empirically determine the combination that achieves the best trade-off between minimal accuracy degradation and maximal inference speed-up.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13453v1</guid>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iulius Gherasim, Carlos Garc\'ia S\'anchez</dc:creator>
    </item>
    <item>
      <title>Ridgeline: A 2D Roofline Model for Distributed Systems</title>
      <link>https://arxiv.org/abs/2209.01368</link>
      <description>arXiv:2209.01368v2 Announce Type: replace-cross 
Abstract: In this short paper, we introduce the Ridgeline model, an extension of the Roofline model [4] for distributed systems. The Roofline model targets shared memory systems, bounding the performance of a kernel based on its operational intensity, and the peak compute throughput and memory bandwidth of the execution system. In a distributed setting, with multiple communicating compute entities, the network must be taken into account to model the system behavior accurately. The Ridgeline aggregates information on compute, memory, and network limits in one 2D plot to show, in an intuitive way, which of the resources is the expected bottleneck. We show the applicability of the Ridgeline in a case study based on a data-parallel Multi-Layer Perceptron (MLP) instance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.01368v2</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabio Checconi, Jesmin Jahan Tithi, Fabrizio Petrini</dc:creator>
    </item>
    <item>
      <title>Energy-Efficient Software Development: A Multi-dimensional Empirical Analysis of Stack Overflow</title>
      <link>https://arxiv.org/abs/2409.19222</link>
      <description>arXiv:2409.19222v2 Announce Type: replace-cross 
Abstract: Energy consumption of software applications has emerged as a critical concern for developers to contemplate in their daily development processes. Previous studies have surveyed a limited number of developers to understand their viewpoints on energy consumption. We complement these studies by analyzing a meticulously curated dataset of 1,193 Stack Overflow (SO) questions concerning energy consumption. These questions capture real-world energy-related challenges in practice. To understand practitioners' perceptions, we investigate the intentions behind these questions, semantic topics, and associated technologies (e.g., programming languages). Our results reveal that: (i) the most prevalent energy consumption topic is about balancing Positioning usage; (ii) efficiently handling data is particularly challenging, with these questions having the longest response times; (iii) practitioners primarily ask questions to understand a concept or API related to energy consumption; and (iv) practitioners are concerned about energy consumption across multiple levels-hardware, operating systems, and programming languages-during energy efficient software development. Our findings raise awareness about energy consumption's impact on software development. We also derive actionable implications for energy optimization at different levels (e.g., optimizing API usage or hardware accesses) during energy-aware software development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19222v2</guid>
      <category>cs.SE</category>
      <category>cs.PF</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3744916.3773177</arxiv:DOI>
      <dc:creator>Bihui Jin, Heng Li, Pengyu Nie, Ying Zou</dc:creator>
    </item>
  </channel>
</rss>
