<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Sep 2025 01:43:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Dynamical Modeling of Behaviorally Relevant Spatiotemporal Patterns in Neural Imaging Data</title>
      <link>https://arxiv.org/abs/2509.18507</link>
      <description>arXiv:2509.18507v1 Announce Type: new 
Abstract: High-dimensional imaging of neural activity, such as widefield calcium and functional ultrasound imaging, provide a rich source of information for understanding the relationship between brain activity and behavior. Accurately modeling neural dynamics in these modalities is crucial for understanding this relationship but is hindered by the high-dimensionality, complex spatiotemporal dependencies, and prevalent behaviorally irrelevant dynamics in these modalities. Existing dynamical models often employ preprocessing steps to obtain low-dimensional representations from neural image modalities. However, this process can discard behaviorally relevant information and miss spatiotemporal structure. We propose SBIND, a novel data-driven deep learning framework to model spatiotemporal dependencies in neural images and disentangle their behaviorally relevant dynamics from other neural dynamics. We validate SBIND on widefield imaging datasets, and show its extension to functional ultrasound imaging, a recent modality whose dynamical modeling has largely remained unexplored. We find that our model effectively identifies both local and long-range spatial dependencies across the brain while also dissociating behaviorally relevant neural dynamics. Doing so, SBIND outperforms existing models in neural-behavioral prediction. Overall, SBIND provides a versatile tool for investigating the neural mechanisms underlying behavior using imaging modalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18507v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ICML 2025</arxiv:journal_reference>
      <dc:creator>Mohammad Hosseini, Maryam M. Shanechi</dc:creator>
    </item>
    <item>
      <title>From Noise to Insight: Visualizing Neural Dynamics with Segmented SNR Topographies for Improved EEG-BCI Performance</title>
      <link>https://arxiv.org/abs/2509.18599</link>
      <description>arXiv:2509.18599v1 Announce Type: new 
Abstract: Electroencephalography (EEG)-based wearable brain-computer interfaces (BCIs) face challenges due to low signal-to-noise ratio (SNR) and non-stationary neural activity. We introduce in this manuscript a mathematically rigorous framework that combines data-driven noise interval evaluation with advanced SNR visualization to address these limitations. Analysis of the publicly available Eye-BCI multimodal dataset demonstrates the method's ability to recover canonical P300 characteristics across frequency bands (delta: 0.5-4 Hz, theta: 4-7.5 Hz, broadband: 1-15 Hz), with precise spatiotemporal localization of both P3a (frontocentral) and P3b (parietal) subcomponents. To the best of our knowledge, this is the first study to systematically assess the impact of noise interval selection on EEG signal quality. Cross-session correlations for four different choices of noise intervals spanning from early to late pre-stimulus phases also indicate that alertness and task engagement states modulate noise interval sensitivity, suggesting broader applications for adaptive BCI systems. While validated in healthy participants, our results represent a first step towards providing clinicians with an interpretable tool for detecting neurophysiological abnormalities and provides quantifiable metrics for system optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18599v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eva Guttmann-Flury, Shan Zhao, Jian Zhao, Mohamad Sawan</dc:creator>
    </item>
    <item>
      <title>BRAID: Input-Driven Nonlinear Dynamical Modeling of Neural-Behavioral Data</title>
      <link>https://arxiv.org/abs/2509.18627</link>
      <description>arXiv:2509.18627v1 Announce Type: new 
Abstract: Neural populations exhibit complex recurrent structures that drive behavior, while continuously receiving and integrating external inputs from sensory stimuli, upstream regions, and neurostimulation. However, neural populations are often modeled as autonomous dynamical systems, with little consideration given to the influence of external inputs that shape the population activity and behavioral outcomes. Here, we introduce BRAID, a deep learning framework that models nonlinear neural dynamics underlying behavior while explicitly incorporating any measured external inputs. Our method disentangles intrinsic recurrent neural population dynamics from the effects of inputs by including a forecasting objective within input-driven recurrent neural networks. BRAID further prioritizes the learning of intrinsic dynamics that are related to a behavior of interest by using a multi-stage optimization scheme. We validate BRAID with nonlinear simulations, showing that it can accurately learn the intrinsic dynamics shared between neural and behavioral modalities. We then apply BRAID to motor cortical activity recorded during a motor task and demonstrate that our method more accurately fits the neural-behavioral data by incorporating measured sensory stimuli into the model and improves the forecasting of neural-behavioral data compared with various baseline methods, whether input-driven or not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18627v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ICLR 2025</arxiv:journal_reference>
      <dc:creator>Parsa Vahidi, Omid G. Sani, Maryam M. Shanechi</dc:creator>
    </item>
    <item>
      <title>Apport de l'imagerie c{\'e}r{\'e}brale pour comprendre les d{\'e}ficits linguistiques</title>
      <link>https://arxiv.org/abs/2509.18688</link>
      <description>arXiv:2509.18688v1 Announce Type: new 
Abstract: Psycholinguistics and neurolinguistics are two complementary disciplines that study language from different perspectives. Psycholinguistics focuses on the cognitive processes involved in language production and comprehension, while neurolinguistics investigates the brain bases of these mechanisms. Brain imaging techniques make it possible to identify the regions and networks involved in various psycholinguistic processes, and to better understand the effects of brain lesions on language abilities. By integrating psycholinguistic and neurolinguistic approaches, research provides a deeper understanding of language processing, whether intact or impaired. This knowledge paves the way for more targeted and effective rehabilitation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18688v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Basirat et Macchi. M{\'e}thodes en psycholinguistique et neurolinguistique : Pour {\'e}tudiants et praticiens en orthophonie, De Boeck Sup{\'e}rieur, 2025, 9782807365681</arxiv:journal_reference>
      <dc:creator>Charlotte Jacquemot (DEC, IMRB), Marine Lunven</dc:creator>
    </item>
    <item>
      <title>Complexity of Activity Patterns in a Bio-Inspired Hopfield-Type Network in Different Topologies</title>
      <link>https://arxiv.org/abs/2509.18758</link>
      <description>arXiv:2509.18758v1 Announce Type: new 
Abstract: Neural network models capable of storing memory have been extensively studied in computer science and computational neuroscience. The Hopfield network is a prototypical example of a model designed for associative, or content-addressable, memory and has been analyzed in many forms. Further, ideas and methods from complex network theory have been incorporated into artificial neural networks and learning, emphasizing their structural properties. Nevertheless, the temporal dynamics also play a vital role in biological neural networks, whose temporal structure is a crucial feature to examine. Biological neural networks display complex intermittency and, thus, can be studied through the lens of the temporal complexity (TC) theory. The TC approach look at the metastability of self-organized states, characterized by a power-law decay in the inter-event time distribution and in the total activity distribution or a scaling behavior in the corresponding event-driven diffusion processes. In this study, we present a temporal complexity (TC) analysis of a biologically-inspired Hopfield-type neural network model. We conducted a comparative assessment between scale-free and random network topologies, with particular emphasis on their global activation patterns. Our parametric analysis revealed comparable dynamical behaviors across both neural network architectures. Furthermore, our investigation into temporal complexity characteristics uncovered that seemingly distinct dynamical patterns exhibit similar temporal complexity behaviors. In particular, similar power-law decay in the activity distribution and similar complexity levels are observed in both topologies, but with a much reduced noise in the scale-free topology. Notably, most of the complex dynamical profiles were consistently observed in scale-free network configurations, thus confirming the crucial role of hubs in neural network dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18758v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>nlin.AO</category>
      <category>physics.bio-ph</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marco Cafiso, Paolo Paradisi</dc:creator>
    </item>
    <item>
      <title>Exploring aperiodic, complexity and entropic brain changes during non-ordinary states of consciousness</title>
      <link>https://arxiv.org/abs/2509.19254</link>
      <description>arXiv:2509.19254v1 Announce Type: new 
Abstract: Non-ordinary states of consciousness (NOC) provide an opportunity to experience highly intense, unique, and perceptually rich subjective states. The neural mechanisms supporting these experiences remain poorly understood. This study examined brain activity associated with a self-induced, substance-free NOC known as Auto-Induced Cognitive Trance (AICT). Twenty-seven trained participants underwent high-density electroencephalography (EEG) recordings during rest and AICT. We analyzed the aperiodic component of the power spectrum (1/f), Lempel-Ziv complexity, and sample entropy from five-minute signal segments. A machine learning approach was used to classify rest and AICT, identify discriminative features, and localize their sources. We also compared EEG metrics across conditions and assessed whether baseline activity predicted the magnitude of change during AICT. Classification analyses revealed condition-specific differences in spectral exponents, complexity, and entropy. The aperiodic component showed the strongest discriminative power, followed by entropy and complexity. Source localization highlighted frontal regions, the posterior cingulate cortex, and the left parietal cortex as key contributors to the AICT state. Baseline neural activity in frontal and parietal regions predicted individual variability in the transition from rest to AICT. These findings indicate that AICT engages brain regions implicated in rich subjective experiences and provide mechanistic insights into how self-induced trance states influence neural functioning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19254v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Victor Oswald (Conscious Care Lab, GIGA Consciousness, GIGA Institute, University of Liege, Liege, Belgium, Cognitive &amp; Computational Neuroscience Lab, Psychology Department, University of Montreal, Montreal, Canada), Karim Jerbi (Cognitive &amp; Computational Neuroscience Lab, Psychology Department, University of Montreal, Montreal, Canada), Corine Sombrun (TranseScience Research Institute, Paris, France), Hamza Abdelhedi (Cognitive &amp; Computational Neuroscience Lab, Psychology Department, University of Montreal, Montreal, Canada), Annen Jitka (Coma Science Group, GIGA Consciousness, GIGA Institute, University of Liege, Liege, Belgium, NeuroRehab &amp; Consciousness Clinic, Neurology Department, University Hospital of Liege, Liege, Belgium, Department of Data Analysis, University of Ghent, Ghent, Belgium), Charlotte Martial (Coma Science Group, GIGA Consciousness, GIGA Institute, University of Liege, Liege, Belgium, NeuroRehab &amp; Consciousness Clinic, Neurology Department, University Hospital of Liege, Liege, Belgium), Audrey Vanhaudenhuyse (Conscious Care Lab, GIGA Consciousness, GIGA Institute, University of Liege, Liege, Belgium, Interdisciplinary Algology Center, University Hospital of Liege, Liege, Belgium), Olivia Gosseries (Conscious Care Lab, GIGA Consciousness, GIGA Institute, University of Liege, Liege, Belgium, Coma Science Group, GIGA Consciousness, GIGA Institute, University of Liege, Liege, Belgium, NeuroRehab &amp; Consciousness Clinic, Neurology Department, University Hospital of Liege, Liege, Belgium)</dc:creator>
    </item>
    <item>
      <title>Probabilistic Geometric Principal Component Analysis with application to neural data</title>
      <link>https://arxiv.org/abs/2509.18469</link>
      <description>arXiv:2509.18469v1 Announce Type: cross 
Abstract: Dimensionality reduction is critical across various domains of science including neuroscience. Probabilistic Principal Component Analysis (PPCA) is a prominent dimensionality reduction method that provides a probabilistic approach unlike the deterministic approach of PCA and serves as a connection between PCA and Factor Analysis (FA). Despite their power, PPCA and its extensions are mainly based on linear models and can only describe the data in a Euclidean coordinate system. However, in many neuroscience applications, data may be distributed around a nonlinear geometry (i.e., manifold) rather than lying in the Euclidean space. We develop Probabilistic Geometric Principal Component Analysis (PGPCA) for such datasets as a new dimensionality reduction algorithm that can explicitly incorporate knowledge about a given nonlinear manifold that is first fitted from these data. Further, we show how in addition to the Euclidean coordinate system, a geometric coordinate system can be derived for the manifold to capture the deviations of data from the manifold and noise. We also derive a data-driven EM algorithm for learning the PGPCA model parameters. As such, PGPCA generalizes PPCA to better describe data distributions by incorporating a nonlinear manifold geometry. In simulations and brain data analyses, we show that PGPCA can effectively model the data distribution around various given manifolds and outperforms PPCA for such data. Moreover, PGPCA provides the capability to test whether the new geometric coordinate system better describes the data than the Euclidean one. Finally, PGPCA can perform dimensionality reduction and learn the data distribution both around and on the manifold. These capabilities make PGPCA valuable for enhancing the efficacy of dimensionality reduction for analysis of high-dimensional data that exhibit noise and are distributed around a nonlinear manifold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18469v1</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ICLR 2025</arxiv:journal_reference>
      <dc:creator>Han-Lin Hsieh, Maryam M. Shanechi</dc:creator>
    </item>
    <item>
      <title>Multi-Scale Graph Theoretical Analysis of Resting-State fMRI for Classification of Alzheimer's Disease, Mild Cognitive Impairment, and Healthy Controls</title>
      <link>https://arxiv.org/abs/2409.04072</link>
      <description>arXiv:2409.04072v3 Announce Type: replace 
Abstract: Alzheimer's disease (AD) is a neurodegenerative disorder marked by memory loss and cognitive decline, making early detection vital for timely intervention. However, early diagnosis is challenging due to the heterogeneous presentation of symptoms. Resting-state functional magnetic resonance imaging (rs-fMRI) captures spontaneous brain activity and functional connectivity, which are known to be disrupted in AD and mild cognitive impairment (MCI). Traditional methods, such as Pearson's correlation, have been used to calculate association matrices, but these approaches often overlook the dynamic and non-stationary nature of brain activity. In this study, we introduce a novel method that integrates discrete wavelet transform (DWT) and graph theory to model the dynamic behavior of brain networks. Our approach captures the time-frequency representation of brain activity, allowing for a more nuanced analysis of the underlying network dynamics. Machine learning was employed to automate the discrimination of different stages of AD based on learned patterns from brain network at different frequency bands. We applied our method to a dataset of rs-fMRI images from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database, demonstrating its potential as an early diagnostic tool for AD and for monitoring disease progression. Our statistical analysis identifies specific brain regions and connections that are affected in AD and MCI, at different frequency bands, offering deeper insights into the disease's impact on brain function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04072v3</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11760-025-04768-3</arxiv:DOI>
      <arxiv:journal_reference>SIViP 19, 1192 (2025)</arxiv:journal_reference>
      <dc:creator>Ali Khazaee, Abdolreza Mohammadi, Ruairi O'Reilly</dc:creator>
    </item>
    <item>
      <title>Language Models Do Not Have Human-Like Working Memory</title>
      <link>https://arxiv.org/abs/2505.10571</link>
      <description>arXiv:2505.10571v3 Announce Type: replace 
Abstract: While Large Language Models (LLMs) exhibit remarkable reasoning abilities, we demonstrate that they lack a fundamental aspect of human cognition: working memory. Human working memory is an active cognitive system that enables not only the temporary storage of information but also its processing and utilization, enabling coherent reasoning and decision-making. Without working memory, individuals may produce unrealistic responses, exhibit self-contradictions, and struggle with tasks that require mental reasoning. Existing evaluations using N-back or context-dependent tasks fall short as they allow LLMs to exploit external context rather than retaining the reasoning process in the latent space. We introduce three novel tasks: (1) Number Guessing, (2) Yes-No Deduction, and (3) Math Magic, designed to isolate internal representation from external context. Across seventeen frontier models spanning four major model families, we consistently observe irrational or contradictory behaviors, indicating LLMs' inability to retain and manipulate latent information. Our work establishes a new benchmark for evaluating working memory in LLMs and highlights this limitation as a key bottleneck for advancing reliable reasoning systems. Code and prompts for the experiments are available at https://github.com/penguinnnnn/LLM-Working-Memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10571v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jen-tse Huang, Kaiser Sun, Wenxuan Wang, Mark Dredze</dc:creator>
    </item>
  </channel>
</rss>
