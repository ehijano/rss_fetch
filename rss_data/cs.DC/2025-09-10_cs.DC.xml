<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Sep 2025 01:18:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Crossword: Adaptive Consensus for Dynamic Data-Heavy Workloads</title>
      <link>https://arxiv.org/abs/2509.07157</link>
      <description>arXiv:2509.07157v1 Announce Type: new 
Abstract: We present Crossword, a flexible consensus protocol for dynamic data-heavy workloads, a rising challenge in the cloud where replication payload sizes span a wide spectrum and introduce sporadic bandwidth stress. Crossword applies per-instance erasure coding and distributes coded shards intelligently to reduce critical-path data transfer significantly when desirable. Unlike previous approaches that statically assign shards to servers, Crossword enables an adaptive tradeoff between the assignment of shards and quorum size in reaction to dynamic workloads and network conditions, while always retaining the availability guarantee of classic protocols. Crossword handles leader failover gracefully by employing a lazy follower gossiping mechanism that incurs minimal impact on critical-path performance. We implement Crossword (along with relevant protocols) in Gazette, a distributed, replicated, and protocol-generic key-value store written in async Rust. We evaluate Crossword comprehensively to show that it matches the best performance among previous protocols (MultiPaxos, Raft, RSPaxos, and CRaft) in static scenarios, and outperforms them by up to 2.3x under dynamic workloads and network conditions. Our integration of Crossword with CockroachDB brings 1.32x higher aggregate throughput to TPC-C under 5-way replication. We will open-source Gazette upon publication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07157v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guanzhou Hu, Yiwei Chen, Andrea Arpaci-Dusseau, Remzi Arpaci-Dusseau</dc:creator>
    </item>
    <item>
      <title>Bodega: Serving Linearizable Reads Locally from Anywhere at Anytime via Roster Leases</title>
      <link>https://arxiv.org/abs/2509.07158</link>
      <description>arXiv:2509.07158v1 Announce Type: new 
Abstract: We present Bodega, the first consensus protocol that serves linearizable reads locally from any desired node, regardless of interfering writes. Bodega achieves this via a novel roster leases algorithm that safeguards the roster, a new notion of cluster metadata. The roster is a generalization of leadership; it tracks arbitrary subsets of replicas as responder nodes for local reads. A consistent agreement on the roster is established through roster leases, an all-to-all leasing mechanism that generalizes existing all-to-one leasing approaches (Leader Leases, Quorum Leases), unlocking a new point in the protocol design space. Bodega further employs optimistic holding and early accept notifications to minimize interruption from interfering writes, and incorporates smart roster coverage and lightweight heartbeats to maximize practicality. Bodega is a non-intrusive extension to classic consensus; it imposes no special requirements on writes other than a responder-covering quorum. We implement Bodega and related works in Vineyard, a protocol-generic replicated key-value store written in async Rust. We compare it to previous protocols (Leader Leases, EPaxos, PQR, and Quorum Leases) and two production coordination services (etcd and ZooKeeper). Bodega speeds up average client read requests by 5.6x-13.1x on real WAN clusters versus previous approaches under moderate write interference, delivers comparable write performance, supports fast proactive roster changes as well as fault tolerance via leases, and closely matches the performance of sequentially-consistent etcd and ZooKeeper deployments across all YCSB workloads. We will open-source Vineyard upon publication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07158v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guanzhou Hu, Andrea Arpaci-Dusseau, Remzi Arpaci-Dusseau</dc:creator>
    </item>
    <item>
      <title>A Study on Messaging Trade-offs in Data Streaming for Scientific Workflows</title>
      <link>https://arxiv.org/abs/2509.07199</link>
      <description>arXiv:2509.07199v1 Announce Type: new 
Abstract: Memory-to-memory data streaming is essential for modern scientific workflows that require near real-time data analysis, experimental steering, and informed decision-making during experiment execution. It eliminates the latency bottlenecks associated with file-based transfers to parallel storage, enabling rapid data movement between experimental facilities and HPC systems. These tightly coupled experimental-HPC workflows demand low latency, high throughput, and reliable data delivery to support on-the-fly analysis and timely feedback for experimental control. Off-the-shelf messaging frameworks are increasingly considered viable solutions for enabling such direct memory streaming due to their maturity, broad adoption, and ability to abstract core messaging and reliability functionalities from the application layer. However, effectively meeting the workflows' requirements depends on utilizing the framework's capabilities and carefully tuning its configurations.
  In this paper, we present a study that investigates the messaging parameters, and their configuration choices that impact the streaming requirements of two representative scientific workflows. We specifically characterize throughput trade-offs associated with reliable message transmission for these workflows. Our study is conducted through streaming simulations using synthetic workloads derived from the Deleria and LCLS workflows, employing the RabbitMQ messaging framework within the context of the Data Streaming to HPC infrastructure at OLCF. Our simulations reveal several key observations and practical insights that help users understand which configurations best meet the needs of their streaming workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07199v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anjus George, Michael J. Brim, Christopher Zimmer, Tyler J. Skluzacek, A. J. Ruckman, Gustav R. Jansen, Sarp Oral</dc:creator>
    </item>
    <item>
      <title>Optimizing Task Scheduling in Fog Computing with Deadline Awareness</title>
      <link>https://arxiv.org/abs/2509.07378</link>
      <description>arXiv:2509.07378v1 Announce Type: new 
Abstract: The rise of Internet of Things (IoT) devices has led to the development of numerous applications that require quick responses and low latency. Fog computing has emerged as a solution for processing these IoT applications, but it faces challenges such as resource allocation and job scheduling. Therefore, it is crucial to determine how to assign and schedule tasks on Fog nodes. A well-designed job scheduling algorithm can help decrease energy usage and improve response times for application requests. This work aims to schedule tasks in IoT while minimizing the total energy consumption of nodes and enhancing the Quality of Service (QoS) requirements of IoT tasks, taking into account task deadlines. Initially, this paper classifies the Fog nodes into two categories based on their traffic level: low and high. It schedules low-deadline tasks on low-traffic-level nodes using an Improved Golden Eagle Optimization (IGEO) algorithm, an enhancement of the Golden Eagle Optimization Algorithm that utilizes genetic operators for discretization. High-deadline tasks are processed on high-traffic nodes using reinforcement learning (RL). This combined approach is called the Reinforcement Improved Golden Eagle Optimization (RIGEO) algorithm. Experimental results demonstrate that the proposed algorithms optimize system response time, total deadline violation time, and resource and system energy consumption compared to other state-of-the-art algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07378v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Sadegh Sirjani, Somayeh Sobati-Moghadam</dc:creator>
    </item>
    <item>
      <title>DuoServe-MoE: Dual-Phase Expert Prefetch and Cache Scheduling for Efficient MoE LLM Inference</title>
      <link>https://arxiv.org/abs/2509.07379</link>
      <description>arXiv:2509.07379v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated impressive performance across a wide range of deep learning tasks. Mixture of Experts (MoE) further enhances their capabilities by increasing model width through sparsely activated expert branches, which keeps inference computation efficient. However, the large number of expert weights introduces significant GPU memory pressure, especially in resource-constrained environments such as single-GPU servers. More importantly, MoE inference consists of two fundamentally different stages: a prefill stage where most experts are activated densely, and a decode stage where only a few experts are triggered sparsely. Treating these stages with a uniform scheduling strategy often leads to suboptimal latency and memory usage. To address this, we propose DuoServe-MoE, an inference serving system that explicitly separates prefill and decode stages and applies tailored expert scheduling strategies to each. In the prefill stage, DuoServe-MoE uses a two-stream CUDA pipeline that overlaps expert weight prefetching with the computation of non-MoE layers, limiting expert residency in GPU memory. In the decode stage, a lightweight layer-level predictor trained offline from activation traces is used to prefetch only the most likely activated experts, without requiring any changes to the model. Experiments on 4-bit Mixtral-8x7B and 8x22B models show that DuoServe-MoE improves end-to-end latency by 1.42 to 7.54 times while keeping peak memory usage at only 15 percent of the full model size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07379v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuning Zhang, Grant Pinkert, Nan Yang, Yanli Li, Dong Yuan</dc:creator>
    </item>
    <item>
      <title>Dependency-Aware Execution Mechanism in Hyperledger Fabric Architecture</title>
      <link>https://arxiv.org/abs/2509.07425</link>
      <description>arXiv:2509.07425v1 Announce Type: new 
Abstract: Hyperledger Fabric is a leading permissioned blockchain framework for enterprise use, known for its modular design and privacy features. While it strongly supports configurable consensus and access control, Fabric can face challenges in achieving high transaction throughput and low rejection rates under heavy workloads. These performance limitations are often attributed to endorsement, ordering, and validation bottlenecks. Further, optimistic concurrency control and deferred validation in Fabric may lead to resource inefficiencies and contention, as conflicting transactions are identified only during the commit phase. To address these challenges, we propose a dependency-aware execution model for Hyperledger Fabric. Our approach includes: (a) a dependency flagging system during endorsement, marking transactions as independent or dependent using a hashmap; (b) an optimized block construction in the ordering service that prioritizes independent transactions; (c) the incorporation of a Directed Acyclic Graph (DAG) within each block to represent dependencies; and (d) parallel execution of independent transactions at the committer, with dependent transactions processed according to DAG order. Incorporated in Hyperledger Fabric v2.5, our framework was tested on workloads with varying dependency levels and system loads. Results show up to 40% higher throughput and significantly reduced rejection rates in high-contention scenarios. This demonstrates that dependency-aware scheduling and DAG-based execution can substantially enhance Fabric's scalability while remaining compatible with its existing consensus and smart contract layers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07425v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sanyam Kaul, Manaswini Piduguralla, Gayathri Shreeya Patnala, Sathya Peri</dc:creator>
    </item>
    <item>
      <title>DREAMS: Decentralized Resource Allocation and Service Management across the Compute Continuum Using Service Affinity</title>
      <link>https://arxiv.org/abs/2509.07497</link>
      <description>arXiv:2509.07497v1 Announce Type: new 
Abstract: Modern manufacturing systems require adaptive computing infrastructures that can respond to highly dynamic workloads and increasingly customized production demands. The compute continuum emerges as a promising solution, enabling flexible deployment of microservices across distributed, heterogeneous domains. However, this paradigm also requires a novel approach to resource allocation and service placement, as traditional centralized solutions struggle to scale effectively, suffer from latency bottlenecks, and introduce single points of failure. In this paper, we present DREAMS, a decentralized framework that optimizes microservice placement decisions collaboratively across different computational domains. At its core, DREAMS introduces agents that operate autonomously within each domain while coordinating globally through a Raft-based consensus algorithm and cost-benefit voting. This decentralized architecture enables responsive, privacy-preserving, and fault-tolerant coordination, making it particularly suitable given the growing prevalence of multi-stakeholder scenarios across the compute continuum. In particular, within modern manufacturing environments, DREAMS achieves globally optimized service placements while maintaining high fault tolerance. Further evaluations demonstrate that key coordination operations, such as Local Domain Manager (LDM) registration and migration voting, scale sub-linearly with the number of domains, confirming the efficiency and scalability of our proposal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07497v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hai Dinh-Tuan, Tien Hung Nguyen, Sanjeet Raj Pandey</dc:creator>
    </item>
    <item>
      <title>Astra: A Multi-Agent System for GPU Kernel Performance Optimization</title>
      <link>https://arxiv.org/abs/2509.07506</link>
      <description>arXiv:2509.07506v1 Announce Type: new 
Abstract: GPU kernel optimization has long been a central challenge at the intersection of high-performance computing and machine learning. Efficient kernels are crucial for accelerating large language model (LLM) training and serving, yet attaining high performance typically requires extensive manual tuning. Compiler-based systems reduce some of this burden, but still demand substantial manual design and engineering effort. Recently, researchers have explored using LLMs for GPU kernel generation, though prior work has largely focused on translating high-level PyTorch modules into CUDA code. In this work, we introduce Astra, the first LLM-based multi-agent system for GPU kernel optimization. Unlike previous approaches, Astra starts from existing CUDA implementations extracted from SGLang, a widely deployed framework for serving LLMs, rather than treating PyTorch modules as the specification. Within Astra, specialized LLM agents collaborate through iterative code generation, testing, profiling, and planning to produce kernels that are both correct and high-performance. On kernels from SGLang, Astra achieves an average speedup of 1.32x using zero-shot prompting with OpenAI o4-mini. A detailed case study further demonstrates that LLMs can autonomously apply loop transformations, optimize memory access patterns, exploit CUDA intrinsics, and leverage fast math operations to yield substantial performance gains. Our work highlights multi-agent LLM systems as a promising new paradigm for GPU kernel optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07506v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anjiang Wei, Tianran Sun, Yogesh Seenichamy, Hang Song, Anne Ouyang, Azalia Mirhoseini, Ke Wang, Alex Aiken</dc:creator>
    </item>
    <item>
      <title>Navigating Energy Doldrums: Modeling the Impact of Energy Price Volatility on HPC Cost of Ownership</title>
      <link>https://arxiv.org/abs/2509.07567</link>
      <description>arXiv:2509.07567v1 Announce Type: new 
Abstract: Energy costs are a major factor in the total cost of ownership (TCO) for high-performance computing (HPC) systems. The rise of intermittent green energy sources and reduced reliance on fossil fuels have introduced volatility into electricity markets, complicating energy budgeting. This paper explores variable capacity as a strategy for managing HPC energy costs - dynamically adjusting compute resources in response to fluctuating electricity prices. While this approach can lower energy expenses, it risks underutilizing costly hardware. To evaluate this trade-off, we present a simple model that helps operators estimate the TCO impact of variable capacity strategies using key system parameters. We apply this model to real data from a university HPC cluster and assess how different scenarios could affect the cost-effectiveness of this approach in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07567v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Arzt, Felix Wolf</dc:creator>
    </item>
    <item>
      <title>AgentX: Towards Orchestrating Robust Agentic Workflow Patterns with FaaS-hosted MCP Services</title>
      <link>https://arxiv.org/abs/2509.07595</link>
      <description>arXiv:2509.07595v1 Announce Type: new 
Abstract: Generative Artificial Intelligence (GenAI) has rapidly transformed various fields including code generation, text summarization, image generation and so on. Agentic AI is a recent evolution that further advances this by coupling the decision making and generative capabilities of LLMs with actions that can be performed using tools. While seemingly powerful, Agentic systems often struggle when faced with numerous tools, complex multi-step tasks,and long-context management to track history and avoid hallucinations. Workflow patterns such as Chain-of-Thought (CoT) and ReAct help address this. Here, we define a novel agentic workflow pattern, AgentX, composed of stage designer, planner, and executor agents that is competitive or better than the state-of-the-art agentic patterns. We also leverage Model Context Protocol (MCP) tools, and propose two alternative approaches for deploying MCP servers as cloud Functions as a Service (FaaS). We empirically evaluate the success rate, latency and cost for AgentX and two contemporary agentic patterns, ReAct and Magentic One, using these the FaaS and local MCP server alternatives for three practical applications. This highlights the opportunities and challenges of designing and deploying agentic workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07595v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiva Sai Krishna Anand Tokal, Vaibhav Jha, Anand Eswaran, Praveen Jayachandran, Yogesh Simmhan</dc:creator>
    </item>
    <item>
      <title>Scaling atomic ordering in shared memory</title>
      <link>https://arxiv.org/abs/2509.07781</link>
      <description>arXiv:2509.07781v1 Announce Type: new 
Abstract: Atomic multicast is a communication primitive used in dependable systems to ensure consistent ordering of messages delivered to a set of replica groups. This primitive enables critical services to integrate replication and sharding (i.e., state partitioning) to achieve fault tolerance and scalability. While several atomic multicast protocols have been developed for message-passing systems, only a few are designed for the shared memory system model. This paper introduces TRAM, an atomic multicast protocol specifically designed for shared memory systems, leveraging an overlay tree architecture. Due to its simple and practical design, TRAM delivers exceptional performance, increasing throughput by more than 3$\times$ and reducing latency by more than 2.3$\times$ compared to state-of-the-art shared memory-based protocols. Additionally, it significantly outperforms message-passing-based protocols, boosting throughput by up to 5.9$\times$ and reducing latency by up to 106$\times$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07781v1</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Martignetti, Eli\~a Batista, Gianpaolo Cugola, Fernando Pedone</dc:creator>
    </item>
    <item>
      <title>veScale: Consistent and Efficient Tensor Programming with Eager-Mode SPMD</title>
      <link>https://arxiv.org/abs/2509.07003</link>
      <description>arXiv:2509.07003v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have scaled rapidly in size and complexity, requiring increasingly intricate parallelism for distributed training, such as 3D parallelism. This sophistication motivates a shift toward simpler, more debuggable programming paradigm like Single Program Multiple Data (SPMD). However, SPMD in eager execution introduces two key challenges: ensuring consistency with single-device execution and achieving high performance at scale. In this paper, we introduce veScale, an eager-mode training system that fully embraces SPMD paradigm to democratize distributed tensor programming. veScale addresses the prevalent issue of inconsistent results in systems like PyTorch by introducing a novel algorithm of distributed Random Number Generation (RNG) compatible with arbitrary sharded operators. veScale also significantly boosts training performance by reducing PyTorch primitive's overhead and improving communication efficiency. Evaluations show that veScale delivers up to 2.2x speedup over the state-of-the-art training systems, like TorchTitan, and cuts code complexity by 78.4%, while preserving single-device-equivalent results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07003v1</guid>
      <category>cs.PL</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youjie Li, Cheng Wan, Zhiqi Lin, Hongyu Zhu, Jiacheng Yang, Ziang Song, Xinyi Di, Jiawei Wu, Huiyao Shu, Wenlei Bao, Yanghua Peng, Haibin Lin, Li-Wen Chang</dc:creator>
    </item>
    <item>
      <title>Fed-REACT: Federated Representation Learning for Heterogeneous and Evolving Data</title>
      <link>https://arxiv.org/abs/2509.07198</link>
      <description>arXiv:2509.07198v1 Announce Type: cross 
Abstract: Motivated by the high resource costs and privacy concerns associated with centralized machine learning, federated learning (FL) has emerged as an efficient alternative that enables clients to collaboratively train a global model while keeping their data local. However, in real-world deployments, client data distributions often evolve over time and differ significantly across clients, introducing heterogeneity that degrades the performance of standard FL algorithms. In this work, we introduce Fed-REACT, a federated learning framework designed for heterogeneous and evolving client data. Fed-REACT combines representation learning with evolutionary clustering in a two-stage process: (1) in the first stage, each client learns a local model to extracts feature representations from its data; (2) in the second stage, the server dynamically groups clients into clusters based on these representations and coordinates cluster-wise training of task-specific models for downstream objectives such as classification or regression. We provide a theoretical analysis of the representation learning stage, and empirically demonstrate that Fed-REACT achieves superior accuracy and robustness on real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07198v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiyue Chen, Usman Akram, Chianing Wang, Haris Vikalo</dc:creator>
    </item>
    <item>
      <title>FedTeddi: Temporal Drift and Divergence Aware Scheduling for Timely Federated Edge Learning</title>
      <link>https://arxiv.org/abs/2509.07342</link>
      <description>arXiv:2509.07342v1 Announce Type: cross 
Abstract: Federated edge learning (FEEL) enables collaborative model training across distributed clients over wireless networks without exposing raw data. While most existing studies assume static datasets, in real-world scenarios clients may continuously collect data with time-varying and non-independent and identically distributed (non-i.i.d.) characteristics. A critical challenge is how to adapt models in a timely yet efficient manner to such evolving data. In this paper, we propose FedTeddi, a temporal-drift-and-divergence-aware scheduling algorithm that facilitates fast convergence of FEEL under dynamic data evolution and communication resource limits. We first quantify the temporal dynamics and non-i.i.d. characteristics of data using temporal drift and collective divergence, respectively, and represent them as the Earth Mover's Distance (EMD) of class distributions for classification tasks. We then propose a novel optimization objective and develop a joint scheduling and bandwidth allocation algorithm, enabling the FEEL system to learn from new data quickly without forgetting previous knowledge. Experimental results show that our algorithm achieves higher test accuracy and faster convergence compared to benchmark methods, improving the rate of convergence by 58.4% on CIFAR-10 and 49.2% on CIFAR-100 compared to random scheduling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07342v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxuan Bai, Yuxuan Sun, Tan Chen, Wei Chen, Sheng Zhou, Zhisheng Niu</dc:creator>
    </item>
    <item>
      <title>HYLU: Hybrid Parallel Sparse LU Factorization</title>
      <link>https://arxiv.org/abs/2509.07690</link>
      <description>arXiv:2509.07690v1 Announce Type: cross 
Abstract: This article introduces HYLU, a hybrid parallel LU factorization-based general-purpose solver designed for efficiently solving sparse linear systems (Ax=b) on multi-core shared-memory architectures. The key technical feature of HYLU is the integration of hybrid numerical kernels so that it can adapt to various sparsity patterns of coefficient matrices. Tests on 34 sparse matrices from SuiteSparse Matrix Collection reveal that HYLU outperforms Intel MKL PARDISO in the numerical factorization phase by geometric means of 1.74X (for one-time solving) and 2.26X (for repeated solving). HYLU can be downloaded from https://github.com/chenxm1986/hylu.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07690v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoming Chen</dc:creator>
    </item>
    <item>
      <title>MoE-Compression: How the Compression Error of Experts Affects the Inference Accuracy of MoE Model?</title>
      <link>https://arxiv.org/abs/2509.07727</link>
      <description>arXiv:2509.07727v1 Announce Type: cross 
Abstract: With the widespread application of Mixture of Experts (MoE) reasoning models in the field of LLM learning, efficiently serving MoE models under limited GPU memory constraints has emerged as a significant challenge. Offloading the non-activated experts to main memory has been identified as an efficient approach to address such a problem, while it brings the challenges of transferring the expert between the GPU memory and main memory. We need to explore an efficient approach to compress the expert and analyze how the compression error affects the inference performance.
  To bridge this gap, we propose employing error-bounded lossy compression algorithms (such as SZ3 and CuSZp) to compress non-activated experts, thereby reducing data transfer overhead during MoE inference. We conduct extensive experiments across various benchmarks and present a comprehensive analysis of how compression-induced errors in different experts affect overall inference accuracy. The results indicate that experts in the shallow layers, which are primarily responsible for the attention mechanism and the transformation of input tokens into vector representations, exhibit minimal degradation in inference accuracy when subjected to bounded errors. In contrast, errors in the middle-layer experts, which are central to model reasoning, significantly impair inference accuracy. Interestingly, introducing bounded errors in the deep-layer experts, which are mainly responsible for instruction following and output integration, can sometimes lead to improvements in inference accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07727v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Songkai Ma, Zhaorui Zhang, Sheng Di, Benben Liu, Xiaodong Yu, Xiaoyi Lu, Dan Wang</dc:creator>
    </item>
    <item>
      <title>Centroid Approximation with Multidimensional Approximate Agreement Protocols</title>
      <link>https://arxiv.org/abs/2306.12741</link>
      <description>arXiv:2306.12741v4 Announce Type: replace 
Abstract: In this paper, we present distributed fault-tolerant algorithms that approximate the centroid (i.e., the average) of a set of $n$ data points in $\mathbb{R}^d$. Our work falls into the broader area of multidimensional Byzantine approximate agreement. We show that state-of-the-art algorithms, such as agreeing inside the convex hull of all non-faulty vectors, or minimum-diameter averaging (MDA), in the worst case either prevent us from agreeing on a vector close to the centroid (in terms of approximation quality), or allow Byzantine parties to influence the output considerably (in terms of validity).
  To design better approximation algorithms, we propose a novel concept of defining an approximation ratio of the centroid by including the vectors of the Byzantine adversaries in the definition. We analyze the algorithms in the synchronous and asynchronous models of communication with public communication channels. We show that the standard agreement algorithms based on agreeing inside the convex hull of all non-faulty vectors do not allow us to compute a better approximation than $2d$ of the centroid. On the other hand, MDA can be used to achieve constant approximation at the cost of only satisfying strong validity. As a trade-off, we develop an approach that reaches a $2\sqrt{d}$-approximation of the centroid, while satisfying box validity. Our approach provides optimal resilience, allowing up to $t&lt;n/3$ faulty nodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.12741v4</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Melanie Cambus, Darya Melnyk</dc:creator>
    </item>
    <item>
      <title>Scaled Block Vecchia Approximation for High-Dimensional Gaussian Process Emulation on GPUs</title>
      <link>https://arxiv.org/abs/2504.12004</link>
      <description>arXiv:2504.12004v2 Announce Type: replace 
Abstract: Emulating computationally intensive scientific simulations is crucial for enabling uncertainty quantification, optimization, and informed decision-making at scale. Gaussian Processes (GPs) offer a flexible and data-efficient foundation for statistical emulation, but their poor scalability limits applicability to large datasets. We introduce the Scaled Block Vecchia (SBV) algorithm for distributed GPU-based systems. SBV integrates the Scaled Vecchia approach for anisotropic input scaling with the Block Vecchia (BV) method to reduce computational and memory complexity while leveraging GPU acceleration techniques for efficient linear algebra operations. To the best of our knowledge, this is the first distributed implementation of any Vecchia-based GP variant. Our implementation employs MPI for inter-node parallelism and the MAGMA library for GPU-accelerated batched matrix computations. We demonstrate the scalability and efficiency of the proposed algorithm through experiments on synthetic and real-world workloads, including a 50M point simulation from a respiratory disease model. SBV achieves near-linear scalability on up to 512 A100 and GH200 GPUs, handles 2.56B points, and reduces energy use relative to exact GP solvers, establishing SBV as a scalable and energy-efficient framework for emulating large-scale scientific models on GPU-based distributed systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.12004v2</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qilong Pan, Sameh Abdulah, Mustafa Abduljabbar, Hatem Ltaief, Andreas Herten, Mathis Bode, Matthew Pratola, Arindam Fadikar, Marc G. Genton, David E. Keyes, Ying Sun</dc:creator>
    </item>
    <item>
      <title>Dynamic Memory Management on GPUs with SYCL</title>
      <link>https://arxiv.org/abs/2504.18211</link>
      <description>arXiv:2504.18211v2 Announce Type: replace 
Abstract: Dynamic memory allocation is not traditionally available in kernels running on GPUs. This work aims to build on Ouroboros, an efficient dynamic memory management library for CUDA applications, by porting the code to SYCL, a cross-platform accelerator API. Since SYCL can be compiled to a CUDA backend, it is possible to compare the performance of the SYCL implementation with that of the original CUDA implementation, as well as test it on non-CUDA platforms such as Intel's Xe graphics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18211v2</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Russell K. Standish</dc:creator>
    </item>
    <item>
      <title>DistFlow: A Fully Distributed RL Framework for Scalable and Efficient LLM Post-Training</title>
      <link>https://arxiv.org/abs/2507.13833</link>
      <description>arXiv:2507.13833v3 Announce Type: replace 
Abstract: Reinforcement learning (RL) has become the pivotal post-training technique for large language model (LLM). Effectively scaling reinforcement learning is now the key to unlocking advanced reasoning capabilities and ensuring safe, goal-aligned behavior in the most powerful LLMs. Mainstream frameworks usually employ a hybrid-controller architecture where a single-controller dispatches the overall execution logic and manages overall data transfer and the multi-controller executes distributed computation. For large-scale reinforcement learning, minor load imbalances can introduce significant bottlenecks, ultimately constraining the scalability of the system. To address this limitation, we introduce DistFlow, a novel, fully distributed RL framework designed to break scaling barrier. We adopt a multi-controller paradigm that dispatches data transfer and execution tasks to all workers, which eliminates the centralized node. This allows each worker to operate independently, leading to near-linear scalability up to 1024 GPUs and dramatic efficiency gains. Furthermore, our architecture decouples resource configuration from execution logic, allowing each worker to have a unique execution flow, offering significant flexibility for rapid and cost-effective algorithmic experimentation. Extensive experiments show that DistFlow achieves excellent linear scalability and up to a 7x end-to-end throughput improvement in specific scenarios over state-of-the-art (SOTA) frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13833v3</guid>
      <category>cs.DC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhixin Wang, Tianyi Zhou, Liming Liu, Ao Li, Jiarui Hu, Dian Yang, Yinhui Lu, Jinlong Hou, Siyuan Feng, Yuan Cheng, Yuan Qi</dc:creator>
    </item>
    <item>
      <title>SAGe: A Lightweight Algorithm-Architecture Co-Design for Mitigating the Data Preparation Bottleneck in Large-Scale Genome Sequence Analysis</title>
      <link>https://arxiv.org/abs/2504.03732</link>
      <description>arXiv:2504.03732v3 Announce Type: replace-cross 
Abstract: Genome sequence analysis, which analyzes the DNA sequences of organisms, drives advances in many critical medical and biotechnological fields. Given its importance and the exponentially growing volumes of genomic sequence data, there are extensive efforts to accelerate genome sequence analysis. In this work, we demonstrate a major bottleneck that greatly limits and diminishes the benefits of state-of-the-art genome sequence analysis accelerators: the data preparation bottleneck, where genomic sequence data is stored in compressed form and needs to be decompressed and formatted first before an accelerator can operate on it. To mitigate this bottleneck, we propose SAGe, an algorithm-architecture co-design for highly-compressed storage and high-performance access of large-scale genomic sequence data. The key challenge is to improve data preparation performance while maintaining high compression ratios (comparable to genomic-specific compression algorithms) at low hardware cost. We address this challenge by leveraging key properties of genomic datasets to co-design (i) a new (de)compression algorithm, (ii) hardware that decompresses data with lightweight operations and efficient streaming accesses, (iii) storage data layout, and (iv) interface commands to access data. SAGe is highly versatile as it supports datasets from different sequencing technologies and species. Thanks to its lightweight design, SAGe can be seamlessly integrated with a broad range of genome sequence analysis hardware accelerators to mitigate their data preparation bottlenecks. Our results demonstrate that SAGe improves the average end-to-end performance and energy efficiency of two state-of-the-art genome sequence analysis accelerators by 3.0x-32.1x and 13.0x-34.0x, respectively, compared to when the accelerators rely on state-of-the-art decompression tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03732v3</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>q-bio.GN</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nika Mansouri Ghiasi, Talu G\"uloglu, Harun Mustafa, Can Firtina, Konstantina Koliogeorgi, Konstantinos Kanellopoulos, Haiyu Mao, Rakesh Nadig, Mohammad Sadrosadati, Jisung Park, Onur Mutlu</dc:creator>
    </item>
    <item>
      <title>Computing in a Faulty Congested Clique</title>
      <link>https://arxiv.org/abs/2505.11430</link>
      <description>arXiv:2505.11430v2 Announce Type: replace-cross 
Abstract: We study a Faulty Congested Clique model, in which an adversary may fail nodes in the network throughout the computation. We show that any task of $O(n\log{n})$-bit input per node can be solved in roughly $n$ rounds, where $n$ is the size of the network. This nearly matches the linear upper bound on the complexity of the non-faulty Congested Clique model for such problems, by learning the entire input, and it holds in the faulty model even with a linear number of faults.
  Our main contribution is that we establish that one can do much better by looking more closely at the computation. Given a deterministic algorithm $\mathcal{A}$ for the non-faulty Congested Clique model, we show how to transform it into an algorithm $\mathcal{A}'$ for the faulty model, with an overhead that could be as small as some logarithmic-in-$n$ factor, by considering refined complexity measures of $\mathcal{A}$.
  As an exemplifying application of our approach, we show that the $O(n^{1/3})$-round complexity of semi-ring matrix multiplication [Censor-Hillel, Kaski, Korhonen, Lenzen, Paz, Suomela, PODC 2015] remains the same up to polylog factors in the faulty model, even if the adversary can fail $99\%$ of the nodes (or any other constant fraction).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11430v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keren Censor-Hillel, Pedro Soto</dc:creator>
    </item>
  </channel>
</rss>
