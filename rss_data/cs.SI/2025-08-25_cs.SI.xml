<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SI</link>
    <description>cs.SI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Aug 2025 02:23:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Dac-Fake: A Divide and Conquer Framework for Detecting Fake News on Social Media</title>
      <link>https://arxiv.org/abs/2508.16223</link>
      <description>arXiv:2508.16223v1 Announce Type: new 
Abstract: With the rapid evolution of technology and the Internet, the proliferation of fake news on social media has become a critical issue, leading to widespread misinformation that can cause societal harm. Traditional fact checking methods are often too slow to prevent the dissemination of false information. Therefore, the need for rapid, automated detection of fake news is paramount. We introduce DaCFake, a novel fake news detection model using a divide and conquer strategy that combines content and context based features. Our approach extracts over eighty linguistic features from news articles and integrates them with either a continuous bag of words or a skipgram model for enhanced detection accuracy. We evaluated the performance of DaCFake on three datasets including Kaggle, McIntire + PolitiFact, and Reuter achieving impressive accuracy rates of 97.88%, 96.05%, and 97.32%, respectively. Additionally, we employed a ten-fold cross validation to further enhance the model's robustness and accuracy. These results highlight the effectiveness of DaCFake in early detection of fake news, offering a promising solution to curb misinformation on social media platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16223v1</guid>
      <category>cs.SI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mayank Kumar Jain, Dinesh Gopalani, Yogesh Kumar Meena, Nishant Jain</dc:creator>
    </item>
    <item>
      <title>Anti-establishment sentiment on TikTok: Implications for understanding influence(rs) and expertise on social media</title>
      <link>https://arxiv.org/abs/2508.16453</link>
      <description>arXiv:2508.16453v1 Announce Type: new 
Abstract: Distrust of public serving institutions and anti-establishment views are on the rise (especially in the U.S.). As people turn to social media for information, it is imperative to understand whether and how social media environments may be contributing to distrust of institutions. In social media, content creators, influencers, and other opinion leaders often position themselves as having expertise and authority on a range of topics from health to politics, and in many cases devalue and dismiss institutional expertise to build a following and increase their own visibility. However, the extent to which this content appears and whether such content increases engagement is unclear. This study analyzes the prevalence of anti-establishment sentiment (AES) on the social media platform TikTok. Despite its popularity as a source of information, TikTok remains relatively understudied and may provide important insights into how people form attitudes towards institutions. We employ a computational approach to label TikTok posts as containing AES or not across topical domains where content creators tend to frame themselves as experts: finance and wellness. As a comparison, we also consider the topic of conspiracy theories, where AES is expected to be common. We find that AES is most prevalent in conspiracy theory content, and relatively rare in content related to the other two topics. However, we find that engagement patterns with such content varies by area, and that there may be platform incentives for users to post content that expresses anti-establishment sentiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16453v1</guid>
      <category>cs.SI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianliang Xu, Ariel Hasell, Sabina Tomkins</dc:creator>
    </item>
    <item>
      <title>Embarrassed to observe: The effects of directive language in brand conversation</title>
      <link>https://arxiv.org/abs/2508.15826</link>
      <description>arXiv:2508.15826v1 Announce Type: cross 
Abstract: In social media, marketers attempt to influence consumers by using directive language, that is, expressions designed to get consumers to take action. While the literature has shown that directive messages in advertising have mixed results for recipients, we know little about the effects of directive brand language on consumers who see brands interacting with other consumers in social media conversations. On the basis of a field study and three online experiments, this study shows that directive language in brand conversation has a detrimental downstream effect on engagement of consumers who observe such exchanges. Specifically, in line with Goffman's facework theory, because a brand that encourages consumers to react could be perceived as face-threatening, consumers who see a brand interacting with others in a directive way may feel vicarious embarrassment and engage less (compared with a conversation without directive language). In addition, we find that when the conversation is nonproduct-centered (vs. product-centered), consumers expect more freedom, as in mundane conversations, even for others; therefore, directive language has a stronger negative effect. However, in this context, the strength of the brand relationship mitigates this effect. Thus, this study contributes to the literature on directive language and brand-consumer interactions by highlighting the importance of context in interactive communication, with direct relevance for social media and brand management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15826v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1002/mar.70018</arxiv:DOI>
      <arxiv:journal_reference>Psychology &amp; Marketing, Early View (2025)</arxiv:journal_reference>
      <dc:creator>Andria Andriuzzi, G\'eraldine Michel</dc:creator>
    </item>
    <item>
      <title>A Bayesian framework for opinion dynamics models</title>
      <link>https://arxiv.org/abs/2508.16539</link>
      <description>arXiv:2508.16539v1 Announce Type: cross 
Abstract: This work introduces a Bayesian framework that unifies a wide class of opinion dynamics models. In this framework, an individual's opinion on a topic is the expected value of their belief, represented as a random variable with a prior distribution. Upon receiving a signal, modeled as the prior belief plus a bias term and subject to zero-mean noise with a known distribution, the individual updates their belief distribution via Bayes' rule. By systematically varying the prior, bias, and noise distributions, this approach recovers a broad array of opinion dynamics models, including DeGroot, bounded confidence, bounded shift, and models exhibiting overreaction or backfire effects. Our analysis shows that the signal score is the key determinant of each model's mathematical structure, governing both small- and large-signal behavior. All models converge to DeGroot's linear update rule for small signals, but diverge in their tail behavior for large signals. This unification not only reveals theoretical linkages among previously disconnected models but also provides a systematic method for generating new ones, offering insights into the rational foundations of opinion formation under cognitive constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16539v1</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yen-Shao Chen, Tauhid Zaman</dc:creator>
    </item>
    <item>
      <title>From chambers to echo chambers: Quantifying polarization with a second-neighbor approach applied to Twitter's climate discussion</title>
      <link>https://arxiv.org/abs/2206.14501</link>
      <description>arXiv:2206.14501v3 Announce Type: replace 
Abstract: Social media platforms often foster environments where users primarily engage with content that aligns with their existing beliefs, thereby reinforcing their views and limiting exposure to opposing viewpoints. In this paper, we analyze X (formerly Twitter) discussions on climate change throughout 2019, using an unsupervised method centered on chambers--second-order information sources--to uncover ideological patterns at scale. Beyond direct connections, chambers capture shared sources of influence, revealing polarization dynamics efficiently and effectively. Analyzing retweet patterns, we identify echo chambers of climate believers and skeptics, revealing strong chamber overlap within ideological groups and minimal overlap between them, resulting in a robust bimodal structure that characterizes polarization. Our method enables us to infer the stance of high-impact users based on their audience's chamber alignment, allowing for the classification of over half the retweeting population with minimal cross-group interaction, in what we term augmented echo chamber classification. We benchmark our approach against manual labeling and a state-of-the-art latent ideology model, finding comparable performance but with nearly four times greater coverage. Moreover, we find that echo chamber structures remain stable over time, even as their members change significantly, suggesting that these structures are a persistent and emergent property of the system. Notably, polarization decreases and climate skepticism rises during the #FridaysForFuture strikes in September 2019. This chamber-based analysis offers valuable insights into the persistence and fluidity of ideological polarization on social media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.14501v3</guid>
      <category>cs.SI</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/comnet/cnaf020</arxiv:DOI>
      <dc:creator>Blas Kolic, Fabi\'an Aguirre-L\'opez, Sergio Hern\'andez-Williams, Guillermo Gardu\~no-Hern\'andez</dc:creator>
    </item>
    <item>
      <title>Are LLM-Powered Social Media Bots Realistic?</title>
      <link>https://arxiv.org/abs/2508.00998</link>
      <description>arXiv:2508.00998v2 Announce Type: replace 
Abstract: As Large Language Models (LLMs) become more sophisticated, there is a possibility to harness LLMs to power social media bots. This work investigates the realism of generating LLM-Powered social media bot networks. Through a combination of manual effort, network science and LLMs, we create synthetic bot agent personas, their tweets and their interactions, thereby simulating social media networks. We compare the generated networks against empirical bot/human data, observing that both network and linguistic properties of LLM-Powered Bots differ from Wild Bots/Humans. This has implications towards the detection and effectiveness of LLM-Powered Bots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00998v2</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lynnette Hui Xian Ng, Kathleen M. Carley</dc:creator>
    </item>
    <item>
      <title>The Chilling: Identifying Strategic Antisocial Behavior Online and Examining the Impact on Journalists</title>
      <link>https://arxiv.org/abs/2508.15061</link>
      <description>arXiv:2508.15061v2 Announce Type: replace 
Abstract: On social platforms like Twitter, strategic targeted attacks are becoming increasingly common, especially against vulnerable groups such as female journalists. Two key challenges in identifying strategic online behavior are the complex structure of online conversations and the hidden nature of potential strategies that drive user behavior. To address these, we develop a new tree structured Transformer model that categorizes replies based on their hierarchical conversation structures. Extensive experiments demonstrate that our proposed classification model can effectively detect different user groups, namely attackers, supporters, and bystanders, and their latent strategies. To demonstrate the utility of our approach, we apply this classifier to real time Twitter data and conduct a series of quantitative analyses on the interactions between journalists with different groups of users. Our classification approach allows us to not only explore strategic behaviors of attackers but also those of supporters and bystanders who engage in online interactions. When examining the impact of online attacks, we find a strong correlation between the presence of attackers' interactions and chilling effects, where journalists tend to slow their subsequent posting behavior. This paper provides a deeper understanding of how different user groups engage in online discussions and highlights the detrimental effects of attacker presence on journalists, other users, and conversational outcomes. Our findings underscore the need for social platforms to develop tools that address coordinated toxicity. By detecting patterns of coordinated attacks early, platforms could limit the visibility of toxic content to prevent escalation. Additionally, providing journalists and users with tools for real time reporting could empower them to manage hostile interactions more effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15061v2</guid>
      <category>cs.SI</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3757668</arxiv:DOI>
      <dc:creator>Yian Wang, Mukhilshankar Umashankar, Eshwar Chandrasekharan, Hari Sundaram</dc:creator>
    </item>
    <item>
      <title>Robust Graph Contrastive Learning with Information Restoration</title>
      <link>https://arxiv.org/abs/2307.12555</link>
      <description>arXiv:2307.12555v3 Announce Type: replace-cross 
Abstract: The graph contrastive learning (GCL) framework has gained remarkable achievements in graph representation learning. However, similar to graph neural networks (GNNs), GCL models are susceptible to graph structural attacks. As an unsupervised method, GCL faces greater challenges in defending against adversarial attacks. Furthermore, there has been limited research on enhancing the robustness of GCL. To thoroughly explore the failure of GCL on the poisoned graphs, we investigate the detrimental effects of graph structural attacks against the GCL framework. We discover that, in addition to the conventional observation that graph structural attacks tend to connect dissimilar node pairs, these attacks also diminish the mutual information between the graph and its representations from an information-theoretical perspective, which is the cornerstone of the high-quality node embeddings for GCL. Motivated by this theoretical insight, we propose a robust graph contrastive learning framework with a learnable sanitation view that endeavors to sanitize the augmented graphs by restoring the diminished mutual information caused by the structural attacks. Additionally, we design a fully unsupervised tuning strategy to tune the hyperparameters without accessing the label information, which strictly coincides with the defender's knowledge. Extensive experiments demonstrate the effectiveness and efficiency of our proposed method compared to competitive baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.12555v3</guid>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yulin Zhu, Xing Ai, Yevgeniy Vorobeychik, Kai Zhou</dc:creator>
    </item>
  </channel>
</rss>
