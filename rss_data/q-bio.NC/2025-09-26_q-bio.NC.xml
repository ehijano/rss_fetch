<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Sep 2025 04:00:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>More than a feeling: Expressive style influences cortical speech tracking in subjective cognitive decline</title>
      <link>https://arxiv.org/abs/2509.21277</link>
      <description>arXiv:2509.21277v1 Announce Type: new 
Abstract: Subjective cognitive decline (SCD) approximately doubles the risk of progressing to MCI and dementia. The present study investigates how one's subjective concerns of his/her own cognition are manifested in the neural dynamics during speech perception. EEG was collected from 56 Cantonese, cognitively normal older adults (aged 60 - 70) while they listened to stimuli of four expressive styles that varied in prosody: scrambled, descriptive, dialogue, and exciting. Using encoding models to predict EEG signals from acoustic, segmentation, and phonotactic features, we found that greater subjective concern was associated with weaker cortical tracking of (1) higher-level linguistic features but not acoustic features and (2) less engaging stimuli (scrambled and descriptive styles) but not prosodically rich stimuli. Overall, our results suggest that early signs of cognitive impairment can be revealed from speech perception via cortical tracking, especially while listening to prosodically flat speech.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21277v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew King-Hang Ma, Manson Cheuk-Man Fong, Yun Feng, Cloris Pui-Hang Li, William Shiyuan Wang</dc:creator>
    </item>
    <item>
      <title>Cross-Linguistic Analysis of Memory Load in Sentence Comprehension: Linear Distance and Structural Density</title>
      <link>https://arxiv.org/abs/2509.20916</link>
      <description>arXiv:2509.20916v1 Announce Type: cross 
Abstract: This study examines whether sentence-level memory load in comprehension is better explained by linear proximity between syntactically related words or by the structural density of the intervening material. Building on locality-based accounts and cross-linguistic evidence for dependency length minimization, the work advances Intervener Complexity-the number of intervening heads between a head and its dependent-as a structurally grounded lens that refines linear distance measures. Using harmonized dependency treebanks and a mixed-effects framework across multiple languages, the analysis jointly evaluates sentence length, dependency length, and Intervener Complexity as predictors of the Memory-load measure. Studies in Psycholinguistics have reported the contributions of feature interference and misbinding to memory load during processing. For this study, I operationalized sentence-level memory load as the linear sum of feature misbinding and feature interference for tractability; current evidence does not establish that their cognitive contributions combine additively. All three factors are positively associated with memory load, with sentence length exerting the broadest influence and Intervener Complexity offering explanatory power beyond linear distance. Conceptually, the findings reconcile linear and hierarchical perspectives on locality by treating dependency length as an important surface signature while identifying intervening heads as a more proximate indicator of integration and maintenance demands. Methodologically, the study illustrates how UD-based graph measures and cross-linguistic mixed-effects modelling can disentangle linear and structural contributions to processing efficiency, providing a principled path for evaluating competing theories of memory load in sentence comprehension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20916v1</guid>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Krishna Aggarwal</dc:creator>
    </item>
    <item>
      <title>Interpretable Embeddings of Speech Enhance and Explain Brain Encoding Performance of Audio Models</title>
      <link>https://arxiv.org/abs/2507.16080</link>
      <description>arXiv:2507.16080v2 Announce Type: replace 
Abstract: Speech foundation models (SFMs) are increasingly hailed as powerful computational models of human speech perception. However, since their representations are inherently black-box, it remains unclear what drives their alignment with brain responses. To remedy this, we built linear encoding models from six interpretable feature families: mel-spectrogram, Gabor filter bank features, speech presence, phonetic, syntactic, and semantic features, and contextualized embeddings from three state-of-the-art SFMs (Whisper, HuBERT, WavLM), quantifying electrocorticography (ECoG) response variance shared between feature classes. Variance-partitioning analyses revealed several key insights: First, the SFMs' alignment with the brain can be mostly explained by their ability to learn and encode simple interpretable speech features. Second, SFMs exhibit a systematic trade-off between encoding of brain-relevant low-level and high-level features across layers. Finally, our results show that SFMs learn brain-relevant semantics which cannot be explained by lower-level speech features, with this capacity increasing with model size and context length. Together, our findings suggest a principled approach to build more interpretable, accurate, and efficient encoding models of the brain by augmenting SFM embeddings with interpretable features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16080v2</guid>
      <category>q-bio.NC</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riki Shimizu, Richard J. Antonello, Chandan Singh, Nima Mesgarani</dc:creator>
    </item>
    <item>
      <title>A Quotient Homology Theory of Representation in Neural Networks</title>
      <link>https://arxiv.org/abs/2502.01360</link>
      <description>arXiv:2502.01360v3 Announce Type: replace-cross 
Abstract: Previous research has proven that the set of maps implemented by neural networks with a ReLU activation function is identical to the set of piecewise linear continuous maps. Furthermore, such networks induce a hyperplane arrangement splitting the input domain of the network into convex polyhedra $G_J$ over which a network $\Phi$ operates in an affine manner.
  In this work, we leverage these properties to define an equivalence class $\sim_\Phi$ on top of an input dataset, which can be split into two sets related to the local rank of $\Phi_J$ and the intersections $\cap \text{Im}\Phi_{J_i}$. We refer to the latter as the \textit{overlap decomposition} $\mathcal{O}_\Phi$ and prove that if the intersections between each polyhedron and an input manifold are convex, the homology groups of neural representations are isomorphic to quotient homology groups $H_k(\Phi(\mathcal{M})) \simeq H_k(\mathcal{M}/\mathcal{O}_\Phi)$. This lets us intrinsically calculate the Betti numbers of neural representations without the choice of an external metric. We develop methods to numerically compute the overlap decomposition through linear programming and a union-find algorithm.
  Using this framework, we perform several experiments on toy datasets showing that, compared to standard persistent homology, our overlap homology-based computation of Betti numbers tracks purely topological rather than geometric features. Finally, we study the evolution of the overlap decomposition during training on several classification problems while varying network width and depth and discuss some shortcomings of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01360v3</guid>
      <category>cs.LG</category>
      <category>math.AT</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kosio Beshkov</dc:creator>
    </item>
    <item>
      <title>ML-PWS: Estimating the Mutual Information Between Experimental Time Series Using Neural Networks</title>
      <link>https://arxiv.org/abs/2508.16509</link>
      <description>arXiv:2508.16509v2 Announce Type: replace-cross 
Abstract: The ability to quantify information transmission is crucial for the analysis and design of natural and engineered systems. The information transmission rate is the fundamental measure for systems with time-varying signals, yet computing it is extremely challenging. In particular, the rate cannot be obtained directly from experimental time-series data without approximations, because of the high dimensionality of the signal trajectory space. Path Weight Sampling (PWS) is a computational technique that makes it possible to obtain the information rate exactly for any stochastic system. However, it requires a mathematical model of the system of interest, be it described by a master equation or a set of differential equations. Here, we present a technique that employs Machine Learning (ML) to develop a generative model from experimental time-series data, which is then combined with PWS to obtain the information rate. We demonstrate the accuracy of this technique, called ML-PWS, by comparing its results on synthetic time-series data generated from a non-linear model against ground-truth results obtained by applying PWS directly to the same model. We illustrate the utility of ML-PWS by applying it to neuronal time-series data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16509v2</guid>
      <category>physics.bio-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 26 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manuel Reinhardt, Ga\v{s}per Tka\v{c}ik, Pieter Rein ten Wolde</dc:creator>
    </item>
  </channel>
</rss>
