<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 15 May 2025 01:23:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Time-varying Parameter Tensor Vector Autoregression</title>
      <link>https://arxiv.org/abs/2505.07975</link>
      <description>arXiv:2505.07975v1 Announce Type: new 
Abstract: Time-varying parameter vector autoregression provides a flexible framework to capture structural changes within time series. However, when applied to high-dimensional data, this model encounters challenges of over-parametrization and computational burden. We address these challenges by building on recently proposed Tensor VAR models to represent the time-varying coefficient matrix as a third-order tensor with CANDECOMP/PARAFAC (CP) decomposition, yielding three model configurations where different sets of components are specified as time-varying, each offering distinct interpretations. To select the model configuration and the decomposition rank, we evaluate multiple variants of Deviance Information Criterion (DIC) corresponding to the conditional and marginal DICs. Our simulation demonstrates that a specific conditional DIC variant provides more reliable results and accurately identifies true model configurations. We improve the accuracy of rank selection by applying knee point detection to the DICs, rather than defaulting to the minimum DIC value. Upon analyzing functional magnetic resonance imaging data from story reading tasks, our selected model configurations suggest time-varying dynamics while reducing the number of parameters by over 90% relative to standard VARs. Granger causality analysis reveals directional brain connectivity patterns that align with narrative progression, with various regions functioning as signal emitters or receivers at different time points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07975v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiyong Luo, Jim E. Griffin</dc:creator>
    </item>
    <item>
      <title>rd2d: Causal Inference in Boundary Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2505.07989</link>
      <description>arXiv:2505.07989v1 Announce Type: new 
Abstract: Boundary discontinuity designs -- also known as Multi-Score Regression Discontinuity (RD) designs, with Geographic RD designs as a prominent example -- are often used in empirical research to learn about causal treatment effects along a continuous assignment boundary defined by a bivariate score. This article introduces the R package rd2d, which implements and extends the methodological results developed in Cattaneo, Titiunik and Yu (2025) for boundary discontinuity designs. The package employs local polynomial estimation and inference using either the bivariate score or a univariate distance-to-boundary metric. It features novel data-driven bandwidth selection procedures, and offers both pointwise and uniform estimation and inference along the assignment boundary. The numerical performance of the package is demonstrated through a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07989v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rocio Titiunik, Ruiqi Rae Yu</dc:creator>
    </item>
    <item>
      <title>Separation-based causal discovery for extremes</title>
      <link>https://arxiv.org/abs/2505.08008</link>
      <description>arXiv:2505.08008v1 Announce Type: new 
Abstract: Structural causal models (SCMs), with an underlying directed acyclic graph (DAG), provide a powerful analytical framework to describe the interaction mechanisms in large-scale complex systems. However, when the system exhibits extreme events, the governing mechanisms can change dramatically, and SCMs with a focus on rare events are needed. We propose a new class of SCMs, called XSCMs, which leverage transformed-linear algebra to model causal relationships among extreme values. Similar to traditional SCMs, we prove that XSCMs satisfy the causal Markov and causal faithfulness properties with respect to partial tail (un)correlatedness. This enables estimation of the underlying DAG for extremes using separation-based tests, and makes many state-of-the-art constraint-based causal discovery algorithms directly applicable. We further consider the problem of undirected graph estimation for relationships among tail-dependent (and potentially heavy-tailed) data. The effectiveness of our method, compared to alternative approaches, is validated through simulation studies on large-scale systems with up to 50 variables, and in a well-studied application to river discharge data from the Danube basin. Finally, we apply the framework to investigate complex market-wide relationships in China's derivatives market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08008v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junshu Jiang, Jordan Richards, Rapha\"el Huser, David Bolin</dc:creator>
    </item>
    <item>
      <title>Asymptotically Efficient Data-adaptive Penalized Shrinkage Estimation with Application to Causal Inference</title>
      <link>https://arxiv.org/abs/2505.08065</link>
      <description>arXiv:2505.08065v1 Announce Type: new 
Abstract: A rich literature exists on constructing non-parametric estimators with optimal asymptotic properties. In addition to asymptotic guarantees, it is often of interest to design estimators with desirable finite-sample properties; such as reduced mean-squared error of a large set of parameters. We provide examples drawn from causal inference where this may be the case, such as estimating a large number of group-specific treatment effects. We show how finite-sample properties of non-parametric estimators, particularly their variance, can be improved by careful application of penalization. Given a target parameter of interest we derive a novel penalized parameter defined as the solution to an optimization problem that balances fidelity to the original parameter against a penalty term. By deriving the non-parametric efficiency bound for the penalized parameter, we are able to propose simple data-adaptive choices for the L1 and L2 tuning parameters designed to minimize finite-sample mean-squared error while preserving optimal asymptotic properties. The L1 and L2 penalization amounts to an adjustment that can be performed as a post-processing step applied to any asymptotically normal and efficient estimator. We show in extensive simulations that this adjustment yields estimators with lower MSE than the unpenalized estimators. Finally, we apply our approach to estimate provider quality measures of kidney dialysis providers within a causal inference framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08065v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Herbert P. Susmann, Yiting Li, Mara A. McAdams-DeMarco, Wenbo Wu, Iv\'an D\'iaz</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Fusion of Many Treatments for Policy Learning</title>
      <link>https://arxiv.org/abs/2505.08092</link>
      <description>arXiv:2505.08092v1 Announce Type: new 
Abstract: Individualized treatment rules/recommendations (ITRs) aim to improve patient outcomes by tailoring treatments to the characteristics of each individual. However, when there are many treatment groups, existing methods face significant challenges due to data sparsity within treatment groups and highly unbalanced covariate distributions across groups. To address these challenges, we propose a novel calibration-weighted treatment fusion procedure that robustly balances covariates across treatment groups and fuses similar treatments using a penalized working model. The fusion procedure ensures the recovery of latent treatment group structures when either the calibration model or the outcome model is correctly specified. In the fused treatment space, practitioners can seamlessly apply state-of-the-art ITR learning methods with the flexibility to utilize a subset of covariates, thereby achieving robustness while addressing practical concerns such as fairness. We establish theoretical guarantees, including consistency, the oracle property of treatment fusion, and regret bounds when integrated with multi-armed ITR learning methods such as policy trees. Simulation studies show superior group recovery and policy value compared to existing approaches. We illustrate the practical utility of our method using a nationwide electronic health record-derived de-identified database containing data from patients with Chronic Lymphocytic Leukemia and Small Lymphocytic Lymphoma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08092v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ke Zhu, Jianing Chu, Ilya Lipkovich, Wenyu Ye, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Beyond Basic A/B testing: Improving Statistical Efficiency for Business Growth</title>
      <link>https://arxiv.org/abs/2505.08128</link>
      <description>arXiv:2505.08128v1 Announce Type: new 
Abstract: The standard A/B testing approaches are mostly based on t-test in large scale industry applications. These standard approaches however suffers from low statistical power in business settings, due to nature of small sample-size or non-Gaussian distribution or return-on-investment (ROI) consideration. In this paper, we propose several approaches to addresses these challenges: (i) regression adjustment, generalized estimating equation, Man-Whitney U and Zero-Trimmed U that addresses each of these issues separately, and (ii) a novel doubly robust generalized U that handles ROI consideration, distribution robustness and small samples in one framework. We provide theoretical results on asymptotic normality and efficiency bounds, together with insights on the efficiency gain from theoretical analysis. We further conduct comprehensive simulation studies and apply the methods to multiple real A/B tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08128v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changshuai Wei, Phuc Nguyen, Benjamin Zelditch, Joyce Chen</dc:creator>
    </item>
    <item>
      <title>Online differentially private inference in stochastic gradient descent</title>
      <link>https://arxiv.org/abs/2505.08227</link>
      <description>arXiv:2505.08227v1 Announce Type: new 
Abstract: We propose a general privacy-preserving optimization-based framework for real-time environments without requiring trusted data curators. In particular, we introduce a noisy stochastic gradient descent algorithm for online statistical inference with streaming data under local differential privacy constraints. Unlike existing methods that either disregard privacy protection or require full access to the entire dataset, our proposed algorithm provides rigorous local privacy guarantees for individual-level data. It operates as a one-pass algorithm without re-accessing the historical data, thereby significantly reducing both time and space complexity. We also introduce online private statistical inference by conducting two construction procedures of valid private confidence intervals. We formally establish the convergence rates for the proposed estimators and present a functional central limit theorem to show the averaged solution path of these estimators weakly converges to a rescaled Brownian motion, providing a theoretical foundation for our online inference tool. Numerical simulation experiments demonstrate the finite-sample performance of our proposed procedure, underscoring its efficacy and reliability. Furthermore, we illustrate our method with an analysis of two datasets: the ride-sharing data and the US insurance data, showcasing its practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08227v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinhan Xie, Enze Shi, Bei Jiang, Linglong Kong, Xuming He</dc:creator>
    </item>
    <item>
      <title>High-dimensional Bayesian Tobit regression for censored response with Horseshoe prior</title>
      <link>https://arxiv.org/abs/2505.08288</link>
      <description>arXiv:2505.08288v1 Announce Type: new 
Abstract: Censored response variables--where outcomes are only partially observed due to known bounds--arise in numerous scientific domains and present serious challenges for regression analysis. The Tobit model, a classical solution for handling left-censoring, has been widely used in economics and beyond. However, with the increasing prevalence of high-dimensional data, where the number of covariates exceeds the sample size, traditional Tobit methods become inadequate. While frequentist approaches for high-dimensional Tobit regression have recently been developed, notably through Lasso-based estimators, the Bayesian literature remains sparse and lacks theoretical guarantees. In this work, we propose a novel Bayesian framework for high-dimensional Tobit regression that addresses both censoring and sparsity. Our method leverages the Horseshoe prior to induce shrinkage and employs a data augmentation strategy to facilitate efficient posterior computation via Gibbs sampling. We establish posterior consistency and derive concentration rates under sparsity, providing the first theoretical results for Bayesian Tobit models in high dimensions. Numerical experiments show that our approach outperforms favorably with the recent Lasso-Tobit method.
  Our method is implemented in the R package tobitbayes, which can be found on Github.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08288v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>The Tien Mai</dc:creator>
    </item>
    <item>
      <title>Bayesian Estimation of Causal Effects Using Proxies of a Latent Interference Network</title>
      <link>https://arxiv.org/abs/2505.08395</link>
      <description>arXiv:2505.08395v1 Announce Type: new 
Abstract: Network interference occurs when treatments assigned to some units affect the outcomes of others. Traditional approaches often assume that the observed network correctly specifies the interference structure. However, in practice, researchers frequently only have access to proxy measurements of the interference network due to limitations in data collection or potential mismatches between measured networks and actual interference pathways. In this paper, we introduce a framework for estimating causal effects when only proxy networks are available. Our approach leverages a structural causal model that accommodates diverse proxy types, including noisy measurements, multiple data sources, and multilayer networks, and defines causal effects as interventions on population-level treatments. Since the true interference network is latent, estimation poses significant challenges. To overcome them, we develop a Bayesian inference framework. We propose a Block Gibbs sampler with Locally Informed Proposals to update the latent network, thereby efficiently exploring the high-dimensional posterior space composed of both discrete and continuous parameters. We illustrate the performance of our method through numerical experiments, demonstrating its accuracy in recovering causal effects even when only proxies of the interference network are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08395v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.OT</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bar Weinstein, Daniel Nevo</dc:creator>
    </item>
    <item>
      <title>Extreme Conformal Prediction: Reliable Intervals for High-Impact Events</title>
      <link>https://arxiv.org/abs/2505.08578</link>
      <description>arXiv:2505.08578v1 Announce Type: new 
Abstract: Conformal prediction is a popular method to construct prediction intervals for black-box machine learning models with marginal coverage guarantees. In applications with potentially high-impact events, such as flooding or financial crises, regulators often require very high confidence for such intervals. However, if the desired level of confidence is too large relative to the amount of data used for calibration, then classical conformal methods provide infinitely wide, thus, uninformative prediction intervals. In this paper, we propose a new method to overcome this limitation. We bridge extreme value statistics and conformal prediction to provide reliable and informative prediction intervals with high-confidence coverage, which can be constructed using any black-box extreme quantile regression method. The advantages of this extreme conformal prediction method are illustrated in a simulation study and in an application to flood risk forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08578v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olivier C. Pasche, Henry Lam, Sebastian Engelke</dc:creator>
    </item>
    <item>
      <title>An Efficient Multi-scale Leverage Effect Estimator under Dependent Microstructure Noise</title>
      <link>https://arxiv.org/abs/2505.08654</link>
      <description>arXiv:2505.08654v1 Announce Type: new 
Abstract: Estimating the leverage effect from high-frequency data is vital but challenged by complex, dependent microstructure noise, often exhibiting non-Gaussian higher-order moments. This paper introduces a novel multi-scale framework for efficient and robust leverage effect estimation under such flexible noise structures. We develop two new estimators, the Subsampling-and-Averaging Leverage Effect (SALE) and the Multi-Scale Leverage Effect (MSLE), which adapt subsampling and multi-scale approaches holistically using a unique shifted window technique. This design simplifies the multi-scale estimation procedure and enhances noise robustness without requiring the pre-averaging approach. We establish central limit theorems and stable convergence, with MSLE achieving convergence rates of an optimal $n^{-1/4}$ and a near-optimal $n^{-1/9}$ for the noise-free and noisy settings, respectively. A cornerstone of our framework's efficiency is a specifically designed MSLE weighting strategy that leverages covariance structures across scales. This significantly reduces asymptotic variance and, critically, yields substantially smaller finite-sample errors than existing methods under both noise-free and realistic noisy settings. Extensive simulations and empirical analyses confirm the superior efficiency, robustness, and practical advantages of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08654v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyang Xiong, Zhao Chen, Christina Dan Wang</dc:creator>
    </item>
    <item>
      <title>Testing the Missing Completely at Random Assumption for Functional Data</title>
      <link>https://arxiv.org/abs/2505.08721</link>
      <description>arXiv:2505.08721v1 Announce Type: new 
Abstract: We consider functional data which have only been observed on a subset of their domain. This paper aims to develop statistical tests to determine whether the function and the domain over which it is observed are independent. The assumption that data is missing completely at random (MCAR) is essential for many functional data methods handling incomplete observations. However, no general testing procedures have been established to validate this assumption. We address this critical gap by introducing such tests, along with their asymptotic theory and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08721v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maximilian Ofner, Siegfried H\"ormann, David Kraus, Dominik Liebl</dc:creator>
    </item>
    <item>
      <title>Assumption-robust Causal Inference</title>
      <link>https://arxiv.org/abs/2505.08729</link>
      <description>arXiv:2505.08729v1 Announce Type: new 
Abstract: In observational causal inference, it is common to encounter multiple adjustment sets that appear equally plausible. It is often untestable which of these adjustment sets are valid to adjust for (i.e., satisfies ignorability). This discrepancy can pose practical challenges as it is typically unclear how to reconcile multiple, possibly conflicting estimates of the average treatment effect (ATE). A naive approach is to report the whole range (convex hull of the union) of the resulting confidence intervals. However, the width of this interval might not shrink to zero in large samples and can be unnecessarily wide in real applications. To address this issue, we propose a summary procedure that generates a single estimate, one confidence interval, and identifies a set of units for which the causal effect estimate remains valid, provided at least one adjustment set is valid. The width of our proposed confidence interval shrinks to zero with sample size at $n^{-1/2}$ rate, unlike the original range which is of constant order. Thus, our assumption-robust approach enables reliable causal inference on the ATE even in scenarios where most of the adjustment sets are invalid. Admittedly, this robustness comes at a cost: our inferential guarantees apply to a target population close to, but different from, the one originally intended. We use synthetic and real-data examples to demonstrate that our proposed procedure provides substantially tighter confidence intervals for the ATE as compared to the whole range. In particular, for a real-world dataset on 401(k) retirement plans our method produces a confidence interval 50\% shorter than the whole range of confidence intervals based on multiple adjustment sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08729v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Ghosh, Dominik Rothenh\"ausler</dc:creator>
    </item>
    <item>
      <title>neuralGAM: An R Package for Fitting Generalized Additive Neural Networks</title>
      <link>https://arxiv.org/abs/2505.08610</link>
      <description>arXiv:2505.08610v1 Announce Type: cross 
Abstract: Nowadays, Neural Networks are considered one of the most effective methods for various tasks such as anomaly detection, computer-aided disease detection, or natural language processing. However, these networks suffer from the ``black-box'' problem which makes it difficult to understand how they make decisions. In order to solve this issue, an R package called neuralGAM is introduced. This package implements a Neural Network topology based on Generalized Additive Models, allowing to fit an independent Neural Network to estimate the contribution of each feature to the output variable, yielding a highly accurate and interpretable Deep Learning model. The neuralGAM package provides a flexible framework for training Generalized Additive Neural Networks, which does not impose any restrictions on the Neural Network architecture. We illustrate the use of the neuralGAM package in both synthetic and real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08610v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ines Ortega-Fernandez, Marta Sestelo</dc:creator>
    </item>
    <item>
      <title>Uncertainty-Aware Surrogate-based Amortized Bayesian Inference for Computationally Expensive Models</title>
      <link>https://arxiv.org/abs/2505.08683</link>
      <description>arXiv:2505.08683v1 Announce Type: cross 
Abstract: Bayesian inference typically relies on a large number of model evaluations to estimate posterior distributions. Established methods like Markov Chain Monte Carlo (MCMC) and Amortized Bayesian Inference (ABI) can become computationally challenging. While ABI enables fast inference after training, generating sufficient training data still requires thousands of model simulations, which is infeasible for expensive models. Surrogate models offer a solution by providing approximate simulations at a lower computational cost, allowing the generation of large data sets for training. However, the introduced approximation errors and uncertainties can lead to overconfident posterior estimates. To address this, we propose Uncertainty-Aware Surrogate-based Amortized Bayesian Inference (UA-SABI) - a framework that combines surrogate modeling and ABI while explicitly quantifying and propagating surrogate uncertainties through the inference pipeline. Our experiments show that this approach enables reliable, fast, and repeated Bayesian inference for computationally expensive models, even under tight time constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08683v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stefania Scheurer, Philipp Reiser, Tim Br\"unnette, Wolfgang Nowak, Anneli Guthke, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>Continuous Temporal Learning of Probability Distributions via Neural ODEs with Applications in Continuous Glucose Monitoring Data</title>
      <link>https://arxiv.org/abs/2505.08698</link>
      <description>arXiv:2505.08698v1 Announce Type: cross 
Abstract: Modeling the continuous--time dynamics of probability distributions from time--dependent data samples is a fundamental problem in many fields, including digital health. The aim is to analyze how the distribution of a biomarker, such as glucose, evolves over time and how these changes may reflect the progression of chronic diseases such as diabetes. In this paper, we propose a novel probabilistic model based on a mixture of Gaussian distributions to capture how samples from a continuous-time stochastic process evolve over the time. To model potential distribution shifts over time, we introduce a time-dependent function parameterized by a Neural Ordinary Differential Equation (Neural ODE) and estimate it non--parametrically using the Maximum Mean Discrepancy (MMD). The proposed model is highly interpretable, detects subtle temporal shifts, and remains computationally efficient. Through simulation studies, we show that it performs competitively in terms of estimation accuracy against state-of-the-art, less interpretable methods such as normalized gradient--flows and non--parameteric kernel density estimators. Finally, we demonstrate the utility of our method on digital clinical--trial data, showing how the interventions alters the time-dependent distribution of glucose levels and enabling a rigorous comparison of control and treatment groups from novel mathematical and clinical perspectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08698v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Antonio \'Alvarez-L\'opez, Marcos Matabuena</dc:creator>
    </item>
    <item>
      <title>PCS-UQ: Uncertainty Quantification via the Predictability-Computability-Stability Framework</title>
      <link>https://arxiv.org/abs/2505.08784</link>
      <description>arXiv:2505.08784v1 Announce Type: cross 
Abstract: As machine learning (ML) models are increasingly deployed in high-stakes domains, trustworthy uncertainty quantification (UQ) is critical for ensuring the safety and reliability of these models. Traditional UQ methods rely on specifying a true generative model and are not robust to misspecification. On the other hand, conformal inference allows for arbitrary ML models but does not consider model selection, which leads to large interval sizes. We tackle these drawbacks by proposing a UQ method based on the predictability, computability, and stability (PCS) framework for veridical data science proposed by Yu and Kumbier. Specifically, PCS-UQ addresses model selection by using a prediction check to screen out unsuitable models. PCS-UQ then fits these screened algorithms across multiple bootstraps to assess inter-sample variability and algorithmic instability, enabling more reliable uncertainty estimates. Further, we propose a novel calibration scheme that improves local adaptivity of our prediction sets. Experiments across $17$ regression and $6$ classification datasets show that PCS-UQ achieves the desired coverage and reduces width over conformal approaches by $\approx 20\%$. Further, our local analysis shows PCS-UQ often achieves target coverage across subgroups while conformal methods fail to do so. For large deep-learning models, we propose computationally efficient approximation schemes that avoid the expensive multiple bootstrap trainings of PCS-UQ. Across three computer vision benchmarks, PCS-UQ reduces prediction set size over conformal methods by $20\%$. Theoretically, we show a modified PCS-UQ algorithm is a form of split conformal inference and achieves the desired coverage with exchangeable data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08784v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhineet Agarwal, Michael Xiao, Rebecca Barter, Omer Ronen, Boyu Fan, Bin Yu</dc:creator>
    </item>
    <item>
      <title>A Bayesian Framework for Multivariate Differential Analysis accounting for Missing Data</title>
      <link>https://arxiv.org/abs/2307.08975</link>
      <description>arXiv:2307.08975v2 Announce Type: replace 
Abstract: Current statistical methods in differential proteomics analysis generally leave aside several challenges, such as missing values, correlations between peptide intensities and uncertainty quantification. Moreover, they provide point estimates, such as the mean intensity for a given peptide or protein in a given condition. The decision of whether an analyte should be considered as differential is then based on comparing the p-value to a significance threshold, usually 5%. In the state-of-the-art limma approach, a hierarchical model is used to deduce the posterior distribution of the variance estimator for each analyte. The expectation of this distribution is then used as a moderated estimation of variance and is injected directly into the expression of the t-statistic. However, instead of merely relying on the moderated estimates, we could provide more powerful and intuitive results by leveraging a fully Bayesian approach and hence allow the quantification of uncertainty. The present work introduces this idea by taking advantage of standard results from Bayesian inference with conjugate priors in hierarchical models to derive a methodology tailored to handle multiple imputation contexts. Furthermore, we aim to tackle a more general problem of multivariate differential analysis, to account for possible inter-peptide correlations. By defining a hierarchical model with prior distributions on both mean and variance parameters, we achieve a global quantification of uncertainty for differential analysis. The inference is thus performed by computing the posterior distribution for the difference in mean peptide intensities between two experimental conditions. In contrast to more flexible models that can be achieved with hierarchical structures, our choice of conjugate priors maintains analytical expressions for direct sampling from posterior distributions without requiring expensive MCMC methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.08975v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marie Chion, Arthur Leroy</dc:creator>
    </item>
    <item>
      <title>Outlier-robust neural network training: variation regularization meets trimmed loss to prevent functional breakdown</title>
      <link>https://arxiv.org/abs/2308.02293</link>
      <description>arXiv:2308.02293v4 Announce Type: replace 
Abstract: In this study, we tackle the challenge of outlier-robust predictive modeling using highly expressive neural networks. Our approach integrates two key components: (1) a transformed trimmed loss (TTL), a computationally efficient variant of the classical trimmed loss, and (2) higher-order variation regularization (HOVR), which imposes smoothness constraints on the prediction function. While traditional robust statistics typically assume low-complexity models such as linear and kernel models, applying TTL alone to modern neural networks may fail to ensure robustness, as their high expressive power allows them to fit both inliers and outliers, even when a robust loss is used. To address this, we revisit the traditional notion of breakdown point and adapt it to the nonlinear function setting, introducing a regularization scheme via HOVR that controls the model's capacity and suppresses overfitting to outliers. We theoretically establish that our training procedure retains a high functional breakdown point, thereby ensuring robustness to outlier contamination. We develop a stochastic optimization algorithm tailored to this framework and provide a theoretical guarantee of its convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.02293v4</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akifumi Okuno, Shotaro Yagishita</dc:creator>
    </item>
    <item>
      <title>Detecting Spatial Health Disparities Using Disease Maps</title>
      <link>https://arxiv.org/abs/2309.02086</link>
      <description>arXiv:2309.02086v2 Announce Type: replace 
Abstract: Epidemiologists commonly use regional aggregates of health outcomes to map mortality or incidence rates and identify geographic disparities. However, to detect health disparities across regions, it is necessary to identify "difference boundaries" that separate neighboring regions with significantly different spatial effects. This can be particularly challenging when dealing with multiple outcomes for each unit and accounting for dependence among diseases and across areal units. In this study, we address the issue of multivariate difference boundary detection for correlated diseases by formulating the problem in terms of Bayesian pairwise multiple comparisons by extending it through the introduction of adjacency modeling and disease graph dependencies. Specifically, we seek the posterior probabilities of neighboring spatial effects being different. To accomplish this, we adopt a class of multivariate areally referenced Dirichlet process models that accommodate spatial and interdisease dependence by endowing the spatial random effects with a discrete probability law. Our method is evaluated through simulation studies and applied to detect difference boundaries for multiple cancers using data from the Surveillance, Epidemiology, and End Results Program of the National Cancer Institute.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02086v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luca Aiello, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Robust Quickest Change Detection in Non-Stationary Processes</title>
      <link>https://arxiv.org/abs/2310.09673</link>
      <description>arXiv:2310.09673v3 Announce Type: replace 
Abstract: Optimal algorithms are developed for robust detection of changes in non-stationary processes. These are processes in which the distribution of the data after change varies with time. The decision-maker does not have access to precise information on the post-change distribution. It is shown that if the post-change non-stationary family has a distribution that is least favorable in a well-defined sense, then the algorithms designed using the least favorable distributions are robust and optimal. Non-stationary processes are encountered in public health monitoring and space and military applications. The robust algorithms are applied to real and simulated data to show their effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09673v3</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingze Hou, Yousef Oleyaeimotlagh, Rahul Mishra, Hoda Bidkhori, Taposh Banerjee</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametrics for Principal Stratification with Continuous Post-Treatment Variables</title>
      <link>https://arxiv.org/abs/2405.17669</link>
      <description>arXiv:2405.17669v2 Announce Type: replace 
Abstract: Principal stratification provides a causal inference framework for investigating treatment effects in the presence of a post-treatment variable. Principal strata play a key role in characterizing the treatment effect by identifying groups of units with the same or similar values for the potential post-treatment variable under both treatment levels. The literature has focused mainly on binary post-treatment variables. Few papers considered continuous post-treatment variables. In the presence of a continuous post-treatment, a challenge is how to identify and characterize meaningful coarsening of the latent principal strata that lead to interpretable principal causal effects. This paper introduces the confounders-aware shared-atom Bayesian mixture, a novel approach for principal stratification with binary treatment and continuous post-treatment variables. Our method leverages Bayesian nonparametric priors with an innovative hierarchical structure for the potential post-treatment variable that overcomes some of the limitations of previous works. Specifically, the novel features of our method allow for (i) identifying coarsened principal strata through a data-adaptive approach and (ii) providing a comprehensive quantification of the uncertainty surrounding stratum membership. Through Monte Carlo simulations, we show that the proposed methodology performs better than existing methods in characterizing the principal strata and estimating principal effects of the treatment. Finally, our proposed model is applied to a case study in which we estimate the causal effects of US national air quality regulations on pollution levels and health outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17669v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dafne Zorzetto, Antonio Canale, Fabrizia Mealli, Francesca Dominici, Falco J. Bargagli-Stoffi</dc:creator>
    </item>
    <item>
      <title>Cellwise outlier detection in heterogeneous populations</title>
      <link>https://arxiv.org/abs/2409.07881</link>
      <description>arXiv:2409.07881v3 Announce Type: replace 
Abstract: Real-world applications may be affected by outlying values. In the model-based clustering literature, several methodologies have been proposed to detect units that deviate from the majority of the data (rowwise outliers) and trim them from the parameter estimates. However, the discarded observations can encompass valuable information in some observed features. Following the more recent cellwise contamination paradigm, we introduce a Gaussian mixture model for cellwise outlier detection. The proposal is estimated via an Expectation-Maximization (EM) algorithm with an additional step for flagging the contaminated cells of a data matrix and then imputing - instead of discarding - them before the parameter estimation. This procedure adheres to the spirit of the EM algorithm by treating the contaminated cells as missing values. We analyze the performance of the proposed model in comparison with other existing methodologies through a simulation study with different scenarios and illustrate its potential use for clustering, outlier detection, and imputation on three real data sets. Additional applications include socio-economic studies, environmental analysis, healthcare, and any domain where the aim is to cluster data affected by missing information and outlying values within features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07881v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/00401706.2025.2497822</arxiv:DOI>
      <dc:creator>Giorgia Zaccaria, Luis A. Garc\'ia-Escudero, Francesca Greselin, Agust\'in Mayo-\'Iscar</dc:creator>
    </item>
    <item>
      <title>Estimating treatment effects with competing intercurrent events in randomized controlled trials</title>
      <link>https://arxiv.org/abs/2503.03049</link>
      <description>arXiv:2503.03049v2 Announce Type: replace 
Abstract: The analysis of randomized controlled trials is often complicated by intercurrent events (ICEs) -- events that occur after treatment initiation and affect either the interpretation or existence of outcome measurements. Examples include treatment discontinuation or the use of additional medications. In two recent clinical trials for systemic lupus erythematosus with complications of ICEs, we classify the ICEs into two broad categories: treatment-related (e.g., treatment discontinuation due to adverse events or lack of efficacy) and treatment-unrelated (e.g., treatment discontinuation due to external factors such as pandemics or relocation). To define a clinically meaningful estimand, we adopt tailored strategies for each category of ICEs. For treatment-related ICEs, which are often informative about a patient's outcome, we use the composite variable strategy that assigns an outcome value indicative of treatment failure. For treatment-unrelated ICEs, we apply the hypothetical strategy, assuming their timing is conditionally independent of the outcome given treatment and baseline covariates, and hypothesizing a scenario in which such events do not occur. A central yet previously overlooked challenge is the presence of competing ICEs, where the first ICE censors all subsequent ones. Despite its ubiquity in practice, this issue has not been explicitly recognized or addressed in previous data analyses due to the lack of rigorous statistical methodology. In this paper, we propose a principled framework to formulate the estimand, establish its nonparametric identification and semiparametric estimation theory, and introduce weighting, outcome regression, and doubly robust estimators. We apply our methods to analyze the two systemic lupus erythematosus trials, demonstrating the robustness and practical utility of the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03049v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sizhu Lu, Yanyao Yi, Yongming Qu, Huayu Karen Liu, Ting Ye, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Bayesian Model Averaging in Causal Instrumental Variable Models</title>
      <link>https://arxiv.org/abs/2504.13520</link>
      <description>arXiv:2504.13520v3 Announce Type: replace 
Abstract: Instrumental variables are a popular tool to infer causal effects under unobserved confounding, but choosing suitable instruments is challenging in practice. We propose gIVBMA, a Bayesian model averaging procedure that addresses this challenge by averaging across different sets of instrumental variables and covariates in a structural equation model. Our approach extends previous work through a scale-invariant prior structure and accommodates non-Gaussian outcomes and treatments, offering greater flexibility than existing methods. The computational strategy uses conditional Bayes factors to update models separately for the outcome and treatments. We prove that this model selection procedure is consistent. By explicitly accounting for model uncertainty, gIVBMA allows instruments and covariates to switch roles and provides robustness against invalid instruments. In simulation experiments, gIVBMA outperforms current state-of-the-art methods. We demonstrate its usefulness in two empirical applications: the effects of malaria and institutions on income per capita and the returns to schooling. A software implementation of gIVBMA is available in Julia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13520v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gregor Steiner, Mark Steel</dc:creator>
    </item>
    <item>
      <title>An Introduction to Topological Data Analysis Ball Mapper in R</title>
      <link>https://arxiv.org/abs/2504.14081</link>
      <description>arXiv:2504.14081v2 Announce Type: replace 
Abstract: The Topological Data Analysis Ball Mapper (TDABM) algorithm of Dlotko (2019) provides a model free means to visualize multi-dimensional data. The visualizations are abstract two-dimensional representations of covers of the dataset. To construct a TDABM plot, each variable in the dataset should be ordinal and suitable for representing as an axis of a scatter plot. The graphs produced by TDABM provide a map of the dataset on which outcomes may be charted, models assessed and new models formed. The benefits of TDABM are powering a growing literature. This document provides a step-by-step introduction to the algorithm with code in R.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14081v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Rudkin</dc:creator>
    </item>
    <item>
      <title>An Introduction to Topological Data Analysis Ball Mapper in Python</title>
      <link>https://arxiv.org/abs/2505.03022</link>
      <description>arXiv:2505.03022v2 Announce Type: replace 
Abstract: Visualization of data is an important step in the understanding of data and the evaluation of statistical models. Topological Data Analysis Ball Mapper (TDABM) after Dlotko (2019), provides a model free means to visualize multivariate datasets without information loss. To permit the construction of a TDABM graph, each variable must be ordinal and have sufficiently many values to make a scatterplot of interest. Where a scatterplot works with two, or three, axes, the TDABM graph can handle any number of axes simultaneously. The result is a visualization of the structure of data. The TDABM graph also permits coloration by additional variables, enabling the mapping of outcomes across the joint distribution of axes. The strengths of TDABM for understanding data, and evaluating models, lie behind a rapidly expanding literature. This guide provides an introduction to TDABM with code in Python.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03022v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Rudkin</dc:creator>
    </item>
    <item>
      <title>Sparse High-Dimensional Vector Autoregressive Bootstrap</title>
      <link>https://arxiv.org/abs/2302.01233</link>
      <description>arXiv:2302.01233v2 Announce Type: replace-cross 
Abstract: We introduce a high-dimensional multiplier bootstrap for time series data based on capturing dependence through a sparsely estimated vector autoregressive model. We prove its consistency for inference on high-dimensional means under two different moment assumptions on the errors, namely sub-gaussian moments and a finite number of absolute moments. In establishing these results, we derive a Gaussian approximation for the maximum mean of a linear process, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.01233v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Adamek, Stephan Smeekes, Ines Wilms</dc:creator>
    </item>
    <item>
      <title>Data-driven stochastic 3D modeling of the nanoporous binder-conductive additive phase in battery cathodes</title>
      <link>https://arxiv.org/abs/2409.11080</link>
      <description>arXiv:2409.11080v2 Announce Type: replace-cross 
Abstract: A stochastic 3D modeling approach for the nanoporous binder-conductive additive phase in hierarchically structured cathodes of lithium-ion batteries is presented. The binder-conductive additive phase of these electrodes consists of carbon black, polyvinylidene difluoride binder and graphite particles. For its stochastic 3D modeling, a three-step procedure based on methods from stochastic geometry is used. First, the graphite particles are described by a Boolean model with ellipsoidal grains. Second, the mixture of carbon black and binder is modeled by an excursion set of a Gaussian random field in the complement of the graphite particles. Third, large pore regions within the mixture of carbon black and binder are described by a Boolean model with spherical grains. The model parameters are calibrated to 3D image data of cathodes in lithium-ion batteries acquired by focused ion beam scanning electron microscopy. Subsequently, model validation is performed by comparing model realizations with measured image data in terms of various morphological descriptors that are not used for model fitting. Finally, we use the stochastic 3D model for predictive simulations, where we generate virtual, yet realistic, image data of nanoporous binder-conductive additives with varying amounts of graphite particles. Based on these virtual nanostructures, we can investigate structure-property relationships. In particular, we quantitatively study the influence of graphite particles on effective transport properties in the nanoporous binder-conductive additive phase, which have a crucial impact on electrochemical processes in the cathode and thus on the performance of battery cells.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11080v2</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Phillip Gr\"afensteiner, Markus Osenberg, Andr\'e Hilger, Nicole Bohn, Joachim R. Binder, Ingo Manke, Volker Schmidt, Matthias Neumann</dc:creator>
    </item>
  </channel>
</rss>
