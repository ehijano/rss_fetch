<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Feb 2025 05:03:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An Integrated Platform for Studying Learning with Intelligent Tutoring Systems: CTAT+TutorShop</title>
      <link>https://arxiv.org/abs/2502.10395</link>
      <description>arXiv:2502.10395v1 Announce Type: new 
Abstract: Intelligent tutoring systems (ITSs) are effective in helping students learn; further research could make them even more effective. Particularly desirable is research into how students learn with these systems, how these systems best support student learning, and what learning sciences principles are key in ITSs. CTAT+Tutorshop provides a full stack integrated platform that facilitates a complete research lifecycle with ITSs, which includes using ITS data to discover learner challenges, to identify opportunities for system improvements, and to conduct experimental studies. The platform includes authoring tools to support and accelerate development of ITS, which provide automatic data logging in a format compatible with DataShop, an independent site that supports the analysis of ed tech log data to study student learnings. Among the many technology platforms that exist to support learning sciences research, CTAT+Tutorshop may be the only one that offers researchers the possibility to author elements of ITSs, or whole ITSs, as part of designing studies. This platform has been used to develop and conduct an estimated 147 research studies which have run in a wide variety of laboratory and real-world educational settings, including K-12 and higher education, and have addressed a wide range of research questions. This paper presents five case studies of research conducted on the CTAT+Tutorshop platform, and summarizes what has been accomplished and what is possible for future researchers. We reflect on the distinctive elements of this platform that have made it so effective in facilitating a wide range of ITS research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10395v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Aleven, Conrad Borchers, Yun Huang, Tomohiro Nagashima, Bruce McLaren, Paulo Carvalho, Octav Popescu, Jonathan Sewall, Kenneth Koedinger</dc:creator>
    </item>
    <item>
      <title>DASKT: A Dynamic Affect Simulation Method for Knowledge Tracing</title>
      <link>https://arxiv.org/abs/2502.10396</link>
      <description>arXiv:2502.10396v1 Announce Type: new 
Abstract: Knowledge Tracing (KT) predicts future performance by modeling students' historical interactions, and understanding students' affective states can enhance the effectiveness of KT, thereby improving the quality of education. Although traditional KT values students' cognition and learning behaviors, efficient evaluation of students' affective states and their application in KT still require further exploration due to the non-affect-oriented nature of the data and budget constraints. To address this issue, we propose a computation-driven approach, Dynamic Affect Simulation Knowledge Tracing (DASKT), to explore the impact of various student affective states (such as frustration, concentration, boredom, and confusion) on their knowledge states. In this model, we first extract affective factors from students' non-affect-oriented behavioral data, then use clustering and spatiotemporal sequence modeling to accurately simulate students' dynamic affect changes when dealing with different problems. Subsequently, {\color{blue}we incorporate affect with time-series analysis to improve the model's ability to infer knowledge states over time and space.} Extensive experimental results on two public real-world educational datasets show that DASKT can achieve more reasonable knowledge states under the effect of students' affective states. Moreover, DASKT outperforms the most advanced KT methods in predicting student performance. Our research highlights a promising avenue for future KT studies, focusing on achieving high interpretability and accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10396v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TKDE.2025.3526584</arxiv:DOI>
      <dc:creator>Xinjie Sun, Kai Zhang, Qi Liu, Shuanghong Shen, Fei Wang, Yuxiang Guo, Enhong Chen</dc:creator>
    </item>
    <item>
      <title>Large Model Empowered Metaverse: State-of-the-Art, Challenges and Opportunities</title>
      <link>https://arxiv.org/abs/2502.10397</link>
      <description>arXiv:2502.10397v1 Announce Type: new 
Abstract: The Metaverse represents a transformative shift beyond traditional mobile Internet, creating an immersive, persistent digital ecosystem where users can interact, socialize, and work within 3D virtual environments. Powered by large models such as ChatGPT and Sora, the Metaverse benefits from precise large-scale real-world modeling, automated multimodal content generation, realistic avatars, and seamless natural language understanding, which enhance user engagement and enable more personalized, intuitive interactions. However, challenges remain, including limited scalability, constrained responsiveness, and low adaptability in dynamic environments. This paper investigates the integration of large models within the Metaverse, examining their roles in enhancing user interaction, perception, content creation, and service quality. To address existing challenges, we propose a generative AI-based framework for optimizing Metaverse rendering. This framework includes a cloud-edge-end collaborative model to allocate rendering tasks with minimal latency, a mobility-aware pre-rendering mechanism that dynamically adjusts to user movement, and a diffusion model-based adaptive rendering strategy to fine-tune visual details. Experimental results demonstrate the effectiveness of our approach in enhancing rendering efficiency and reducing rendering overheads, advancing large model deployment for a more responsive and immersive Metaverse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10397v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuntao Wang, Qinnan Hu, Zhou Su, Linkang Du, Qichao Xu</dc:creator>
    </item>
    <item>
      <title>Practical Application and Limitations of AI Certification Catalogues</title>
      <link>https://arxiv.org/abs/2502.10398</link>
      <description>arXiv:2502.10398v1 Announce Type: new 
Abstract: In this work-in-progress, we investigate the certification of artificial intelligence (AI) systems, focusing on the practical application and limitations of existing certification catalogues by attempting to certify a publicly available AI system. We aim to evaluate how well current approaches work to effectively certify an AI system, and how publicly accessible AI systems, that might not be actively maintained or initially intended for certification, can be selected and used for a sample certification process. Our methodology involves leveraging the Fraunhofer AI Assessment Catalogue as a comprehensive tool to systematically assess an AI model's compliance with certification standards. We find that while the catalogue effectively structures the evaluation process, it can also be cumbersome and time-consuming to use. We observe the limitations of an AI system that has no active development team anymore and highlighted the importance of complete system documentation. Finally, we identify some limitations of the certification catalogues used and proposed ideas on how to streamline the certification process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10398v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregor Autischer, Kerstin Waxnegger, Dominik Kowald</dc:creator>
    </item>
    <item>
      <title>Data Stewardship Decoded: Mapping Its Diverse Manifestations and Emerging Relevance at a time of AI</title>
      <link>https://arxiv.org/abs/2502.10399</link>
      <description>arXiv:2502.10399v1 Announce Type: new 
Abstract: Data stewardship has become a critical component of modern data governance, especially with the growing use of artificial intelligence (AI). Despite its increasing importance, the concept of data stewardship remains ambiguous and varies in its application. This paper explores four distinct manifestations of data stewardship to clarify its emerging position in the data governance landscape. These manifestations include a) data stewardship as a set of competencies and skills, b) a function or role within organizations, c) an intermediary organization facilitating collaborations, and d) a set of guiding principles. The paper subsequently outlines the core competencies required for effective data stewardship, explains the distinction between data stewards and Chief Data Officers (CDOs), and details the intermediary role of stewards in bridging gaps between data holders and external stakeholders. It also explores key principles aligned with the FAIR framework (Findable, Accessible, Interoperable, Reusable) and introduces the emerging principle of AI readiness to ensure data meets the ethical and technical requirements of AI systems. The paper emphasizes the importance of data stewardship in enhancing data collaboration, fostering public value, and managing data reuse responsibly, particularly in the era of AI. It concludes by identifying challenges and opportunities for advancing data stewardship, including the need for standardized definitions, capacity building efforts, and the creation of a professional association for data stewardship.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10399v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Stefaan Verhulst</dc:creator>
    </item>
    <item>
      <title>Introducing Computational Thinking in Calculus for Engineering</title>
      <link>https://arxiv.org/abs/2502.10400</link>
      <description>arXiv:2502.10400v1 Announce Type: new 
Abstract: Technology is currently ubiquitous and is also part of the educational system at all levels. It started with communication technology systems, and later continued with digital competence. Nowadays, although these previous concepts are still in force and are useful for students and workers in general, a new concept has been born that can function as a cross-curricular competence called Computational Thinking. There is currently no consensus on the definition of computational thinking, nor on the classification of its skills, but there is a consensus that it refers to a set of skills necessary for the formulation and resolution of problems. The study of Computational Thinking has been very influential in recent years in research on teaching and learning processes, which has led educational institutions to begin to address these issues during training. In this paper, we try to introduce this new cross-curricular competence and expose a project of implementation of Computational Thinking in engineering careers through Calculus subject.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10400v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Javier Bilbao, Eugenio Bravo, Olatz Garcia, Carolina Rebollar</dc:creator>
    </item>
    <item>
      <title>You Can't Get There From Here: Redefining Information Science to address our sociotechnical futures</title>
      <link>https://arxiv.org/abs/2502.10401</link>
      <description>arXiv:2502.10401v1 Announce Type: new 
Abstract: Current definitions of Information Science are inadequate to comprehensively describe the nature of its field of study and for addressing the problems that are arising from intelligent technologies. The ubiquitous rise of artificial intelligence applications and their impact on society demands the field of Information Science acknowledge the sociotechnical nature of these technologies. Previous definitions of Information Science over the last six decades have inadequately addressed the environmental, human, and social aspects of these technologies. This perspective piece advocates for an expanded definition of Information Science that fully includes the sociotechnical impacts information has on the conduct of research in this field. Proposing an expanded definition of Information Science that includes the sociotechnical aspects of this field should stimulate both conversation and widen the interdisciplinary lens necessary to address how intelligent technologies may be incorporated into society and our lives more fairly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10401v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Scott Humr, Mustafa Canan</dc:creator>
    </item>
    <item>
      <title>Spatial modeling of mental health on outpatient morbidity in Kenya</title>
      <link>https://arxiv.org/abs/2502.10402</link>
      <description>arXiv:2502.10402v1 Announce Type: new 
Abstract: A mental health disorder is a clinically significant impairment in a persons intellect, emotional control, or behavior. Mental disorders and outpatient morbidity are a challenge to public health in Kenya. The spatial distribution and study of factors associated with these conditions remain limited. The study aimed to conduct spatial modeling of mental health on outpatient mobility in Kenya. This project used spatial modeling to explore the relationship between infectious diseases and mental disorders. The results showed that mental health issues were not distributed uniformly, with higher frequency found in Western and Nairobi regions. Possible connections between HIV, TB, and STIs with mental health have been suggested by the substantial correlation found between infectious diseases and mental health issues. The spatial model demonstrated excellent validity and accuracy, providing policymakers with a useful tool to better allocate resources and enhance mental health treatments, especially in high-risk locations. In conclusion, the research improved knowledge of the spatial patterns of mental health disorders and guides intervention tactics and healthcare policies in Kenya and other comparable settings. Geographically tailored mental health intervention programs should be developed and implemented in accordance with the high-prevalence areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10402v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ndegwa Ruth wambui, Mwalili Samuel, Wamwea Charity</dc:creator>
    </item>
    <item>
      <title>Crop Yield Time-Series Data Prediction Based on Multiple Hybrid Machine Learning Models</title>
      <link>https://arxiv.org/abs/2502.10405</link>
      <description>arXiv:2502.10405v1 Announce Type: new 
Abstract: Agriculture plays a crucial role in the global economy and social stability, and accurate crop yield prediction is essential for rational planting planning and decision-making. This study focuses on crop yield Time-Series Data prediction. Considering the crucial significance of agriculture in the global economy and social stability and the importance of accurate crop yield prediction for rational planting planning and decision-making, this research uses a dataset containing multiple crops, multiple regions, and data over many years to deeply explore the relationships between climatic factors (average rainfall, average temperature) and agricultural inputs (pesticide usage) and crop yield. Multiple hybrid machine learning models such as Linear Regression, Random Forest, Gradient Boost, XGBoost, KNN, Decision Tree, and Bagging Regressor are adopted for yield prediction. After evaluation, it is found that the Random Forest and Bagging Regressor models perform excellently in predicting crop yield with high accuracy and low error.As agricultural data becomes increasingly rich and time-series prediction techniques continue to evolve, the results of this study contribute to advancing the practical application of crop yield prediction in agricultural production management. The integration of time-series analysis allows for more dynamic, data-driven decision-making, enhancing the accuracy and reliability of crop yield forecasts over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10405v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yueru Yan, Yue Wang, Jialin Li, Jingwei Zhang, Xingye Mo</dc:creator>
    </item>
    <item>
      <title>FishBargain: An LLM-Empowered Bargaining Agent for Online Fleamarket Platform Sellers</title>
      <link>https://arxiv.org/abs/2502.10406</link>
      <description>arXiv:2502.10406v1 Announce Type: new 
Abstract: Different from traditional Business-to-Consumer e-commerce platforms~(e.g., Amazon), online fleamarket platforms~(e.g., Craigslist) mainly focus on individual sellers who are lack of time investment and business proficiency. Individual sellers often struggle with the bargaining process and thus the deal is unaccomplished. Recent advancements in Large Language Models(LLMs) demonstrate huge potential in various dialogue tasks, but those tasks are mainly in the form of passively following user's instruction. Bargaining, as a form of proactive dialogue task, represents a distinct art of dialogue considering the dynamism of environment and uncertainty of adversary strategies. In this paper, we propose an LLM-empowered bargaining agent designed for online fleamarket platform sellers, named as FishBargain. Specifically, FishBargain understands the chat context and product information, chooses both action and language skill considering possible adversary actions and generates utterances. FishBargain has been tested by thousands of individual sellers on one of the largest online fleamarket platforms~(Xianyu) in China. Both qualitative and quantitative experiments demonstrate that FishBargain can effectively help sellers make more deals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10406v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dexin Kong, Xu Yan, Ming Chen, Shuguang Han, Jufeng Chen, Fei Huang</dc:creator>
    </item>
    <item>
      <title>Addressing Bias in Generative AI: Challenges and Research Opportunities in Information Management</title>
      <link>https://arxiv.org/abs/2502.10407</link>
      <description>arXiv:2502.10407v1 Announce Type: new 
Abstract: Generative AI technologies, particularly Large Language Models (LLMs), have transformed information management systems but introduced substantial biases that can compromise their effectiveness in informing business decision-making. This challenge presents information management scholars with a unique opportunity to advance the field by identifying and addressing these biases across extensive applications of LLMs. Building on the discussion on bias sources and current methods for detecting and mitigating bias, this paper seeks to identify gaps and opportunities for future research. By incorporating ethical considerations, policy implications, and sociotechnical perspectives, we focus on developing a framework that covers major stakeholders of Generative AI systems, proposing key research questions, and inspiring discussion. Our goal is to provide actionable pathways for researchers to address bias in LLM applications, thereby advancing research in information management that ultimately informs business practices. Our forward-looking framework and research agenda advocate interdisciplinary approaches, innovative methods, dynamic perspectives, and rigorous evaluation to ensure fairness and transparency in Generative AI-driven information systems. We expect this study to serve as a call to action for information management scholars to tackle this critical issue, guiding the improvement of fairness and effectiveness in LLM-based systems for business practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10407v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.im.2025.104103</arxiv:DOI>
      <dc:creator>Xiahua Wei, Naveen Kumar, Han Zhang</dc:creator>
    </item>
    <item>
      <title>Knowledge Tracing in Programming Education Integrating Students' Questions</title>
      <link>https://arxiv.org/abs/2502.10408</link>
      <description>arXiv:2502.10408v1 Announce Type: new 
Abstract: Knowledge tracing (KT) in programming education presents unique challenges due to the complexity of coding tasks and the diverse methods students use to solve problems. Although students' questions often contain valuable signals about their understanding and misconceptions, traditional KT models often neglect to incorporate these questions as inputs to address these challenges. This paper introduces SQKT (Students' Question-based Knowledge Tracing), a knowledge tracing model that leverages students' questions and automatically extracted skill information to enhance the accuracy of predicting students' performance on subsequent problems in programming education. Our method creates semantically rich embeddings that capture not only the surface-level content of the questions but also the student's mastery level and conceptual understanding. Experimental results demonstrate SQKT's superior performance in predicting student completion across various Python programming courses of differing difficulty levels. In in-domain experiments, SQKT achieved a 33.1\% absolute improvement in AUC compared to baseline models. The model also exhibited robust generalization capabilities in cross-domain settings, effectively addressing data scarcity issues in advanced programming courses. SQKT can be used to tailor educational content to individual learning needs and design adaptive learning systems in computer science education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10408v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Doyoun Kim, Suin Kim, Yojan Jo</dc:creator>
    </item>
    <item>
      <title>Data Science Students Perspectives on Learning Analytics: An Application of Human-Led and LLM Content Analysis</title>
      <link>https://arxiv.org/abs/2502.10409</link>
      <description>arXiv:2502.10409v1 Announce Type: new 
Abstract: Objective This study is part of a series of initiatives at a UK university designed to cultivate a deep understanding of students' perspectives on analytics that resonate with their unique learning needs. It explores collaborative data processing undertaken by postgraduate students who examined an Open University Learning Analytics Dataset (OULAD).
  Methods A qualitative approach was adopted, integrating a Retrieval-Augmented Generation (RAG) and a Large Language Model (LLM) technique with human-led content analysis to gather information about students' perspectives based on their submitted work. The study involved 72 postgraduate students in 12 groups.
  Findings The analysis of group work revealed diverse insights into essential learning analytics from the students' perspectives. All groups adopted a structured data science methodology. The questions formulated by the groups were categorised into seven themes, reflecting their specific areas of interest. While there was variation in the selected variables to interpret correlations, a consensus was found regarding the general results.
  Conclusion A significant outcome of this study is that students specialising in data science exhibited a deeper understanding of learning analytics, effectively articulating their interests through inferences drawn from their analyses. While human-led content analysis provided a general understanding of students' perspectives, the LLM offered nuanced insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10409v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raghda Zahran, Jianfei Xu, Huizhi Liang, Matthew Forshaw</dc:creator>
    </item>
    <item>
      <title>Auto-Evaluation: A Critical Measure in Driving Improvements in Quality and Safety of AI-Generated Lesson Resources</title>
      <link>https://arxiv.org/abs/2502.10410</link>
      <description>arXiv:2502.10410v1 Announce Type: new 
Abstract: As a publicly funded body in the UK, Oak National Academy is in a unique position to innovate within this field as we have a comprehensive curriculum of approximately 13,000 open education resources (OER) for all National Curriculum subjects, designed and quality-assured by expert, human teachers. This has provided the corpus of content needed for building a high-quality AI-powered lesson planning tool, Aila, that is free to use and, therefore, accessible to all teachers across the country. Furthermore, using our evidence-informed curriculum principles, we have codified and exemplified each component of lesson design. To assess the quality of lessons produced by Aila at scale, we have developed an AI-powered auto-evaluation agent,facilitating informed improvements to enhance output quality. Through comparisons between human and auto-evaluations, we have begun to refine this agent further to increase its accuracy, measured by its alignment with an expert human evaluator. In this paper we present this iterative evaluation process through an illustrative case study focused on one quality benchmark - the level of challenge within multiple-choice quizzes. We also explore the contribution that this may make to similar projects and the wider sector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10410v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hannah-Beth Clark, Margaux Dowland, Laura Benton, Reka Budai, Ibrahim Kaan Keskin, Emma Searle, Matthew Gregory, Mark Hodierne, William Gayne, John Roberts</dc:creator>
    </item>
    <item>
      <title>TrueReason: An Exemplar Personalised Learning System Integrating Reasoning with Foundational Models</title>
      <link>https://arxiv.org/abs/2502.10411</link>
      <description>arXiv:2502.10411v1 Announce Type: new 
Abstract: Personalised education is one of the domains that can greatly benefit from the most recent advances in Artificial Intelligence (AI) and Large Language Models (LLM). However, it is also one of the most challenging applications due to the cognitive complexity of teaching effectively while personalising the learning experience to suit independent learners. We hypothesise that one promising approach to excelling in such demanding use cases is using a \emph{society of minds}. In this chapter, we present TrueReason, an exemplar personalised learning system that integrates a multitude of specialised AI models that can mimic micro skills that are composed together by a LLM to operationalise planning and reasoning. The architecture of the initial prototype is presented while describing two micro skills that have been incorporated in the prototype. The proposed system demonstrates the first step in building sophisticated AI systems that can take up very complex cognitive tasks that are demanded by domains such as education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10411v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <category>cs.MA</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sahan Bulathwela, Daniel Van Niekerk, Jarrod Shipton, Maria Perez-Ortiz, Benjamin Rosman, John Shawe-Taylor</dc:creator>
    </item>
    <item>
      <title>Identifying relevant indicators for monitoring a National Artificial Intelligence Strategy</title>
      <link>https://arxiv.org/abs/2502.10412</link>
      <description>arXiv:2502.10412v1 Announce Type: new 
Abstract: How can a National Artificial Intelligence Strategy be effectively monitored? To address this question, we propose a methodology consisting of two key components. First, it involves identifying relevant indicators within national AI strategies. Second, it assesses the alignment between these indicators and the strategic actions of a specific government's AI strategy, allowing for a critical evaluation of its monitoring measures. Moreover, identifying these indicators helps assess the overall quality of the strategy's structure. A lack of alignment between strategic actions and the identified indicators may reveal gaps or blind spots in the strategy. This methodology is demonstrated using the Brazilian AI strategy as a case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10412v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Renata Pelissari, Ricardo Suyama, Leonardo Tomazeli Duarte, Henrique S\'a Earp</dc:creator>
    </item>
    <item>
      <title>Machine Learning-Driven Convergence Analysis in Multijurisdictional Compliance Using BERT and K-Means Clustering</title>
      <link>https://arxiv.org/abs/2502.10413</link>
      <description>arXiv:2502.10413v1 Announce Type: new 
Abstract: Digital data continues to grow, there has been a shift towards using effective regulatory mechanisms to safeguard personal information. The CCPA of California and the General Data Protection Regulation (GDPR) of the European Union are two of the most important privacy laws. The regulation is intended to safeguard consumer privacy, but it varies greatly in scope, definitions, and methods of enforcement. This paper presents a fresh approach to adaptive compliance, using machine learning and emphasizing natural language processing (NLP) as the primary focus of comparison between the GDPR and CCPA. Using NLP, this study compares various regulations to identify areas where they overlap or diverge. This includes the "right to be forgotten" provision in the GDPR and the "opt-out of sale" provision under CCPA. International companies can learn valuable lessons from this report, as it outlines strategies for better enforcement of laws across different nations. Additionally, the paper discusses the challenges of utilizing NLP in legal literature and proposes methods to enhance the model-ability of machine learning models for studying regulations. The study's objective is to "bridge the gap between legal knowledge and technical expertise" by developing regulatory compliance strategies that are more efficient in operation and more effective in data protection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10413v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.6084/m9.figshare.28259810</arxiv:DOI>
      <arxiv:journal_reference>Aitoz Journal of AI Research 3 (2024) 126-141</arxiv:journal_reference>
      <dc:creator>Raj Sonani, Lohalekar Prayas</dc:creator>
    </item>
    <item>
      <title>Prosocial Media</title>
      <link>https://arxiv.org/abs/2502.10834</link>
      <description>arXiv:2502.10834v1 Announce Type: new 
Abstract: Social media empower distributed content creation by algorithmically harnessing "the social fabric" (explicit and implicit signals of association) to serve this content. While this overcomes the bottlenecks and biases of traditional gatekeepers, many believe it has unsustainably eroded the very social fabric it depends on by maximizing engagement for advertising revenue. This paper participates in open and ongoing considerations to translate social and political values and conventions, specifically social cohesion, into platform design. We propose an alternative platform model that the social fabric an explicit output as well as input. Citizens are members of communities defined by explicit affiliation or clusters of shared attitudes. Both have internal divisions, as citizens are members of intersecting communities, which are themselves internally diverse. Each is understood to value content that bridge (viz. achieve consensus across) and balance (viz. represent fairly) this internal diversity, consistent with the principles of the Hutchins Commission (1947). Content is labeled with social provenance, indicating for which community or citizen it is bridging or balancing. Subscription payments allow citizens and communities to increase the algorithmic weight on the content they value in the content serving algorithm. Advertisers may, with consent of citizen or community counterparties, target them in exchange for payment or increase in that party's algorithmic weight. Underserved and emerging communities and citizens are optimally subsidized/supported to develop into paying participants. Content creators and communities that curate content are rewarded for their contributions with algorithmic weight and/or revenue. We discuss applications to productivity (e.g. LinkedIn), political (e.g. X), and cultural (e.g. TikTok) platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10834v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>E. Glen Weyl, Luke Thorburn, Emillie de Keulenaar, Jacob Mchangama, Divya Siddarth, Audrey Tang</dc:creator>
    </item>
    <item>
      <title>Multiple Approaches for Teaching Responsible Computing</title>
      <link>https://arxiv.org/abs/2502.10856</link>
      <description>arXiv:2502.10856v1 Announce Type: new 
Abstract: Teaching applied ethics in computer science has shifted from a perspective of teaching about professional codes of conduct and an emphasis on risk management towards a broader understanding of the impacts of computing on humanity and the environment and the principles and practices of responsible computing. One of the primary shifts in the approach to teaching computing ethics comes from research in the social sciences and humanities. This position is grounded in the idea that all computing artifacts, projects, tools, and products are situated within a set of ideas, attitudes, goals, and cultural norms. This means that all computing endeavors have embedded within them a set of values. To teach responsible computing always requires us to first recognize that computing happens in a context that is shaped by cultural values, including our own professional culture and values.
  The purpose of this paper is to highlight current scholarship, principles, and practices in the teaching of responsible computing in undergraduate computer science settings. The paper is organized around four primary sections: 1) a high-level rationale for the adoption of different pedagogical approaches based on program context and course learning goals, 2) a brief survey of responsible computing pedagogical approaches; 3) illustrative examples of how topics within the CS 2023 Social, Ethical, and Professional (SEP) knowledge area can be implemented and assessed across the broad spectrum of undergraduate computing courses; and 4) links to examples of current best practices, tools, and resources for faculty to build responsible computing teaching into their specific instructional settings and CS2023 knowledge areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10856v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stacy A. Doore, Michelle Trim, Joycelyn Streator, Richard L. Blumenthal, Atri Rudra, Robert B. Schnabel</dc:creator>
    </item>
    <item>
      <title>"Business on WhatsApp is tough now- but am I really a businesswoman?" Exploring Challenges with Adapting to Changes in WhatsApp Business</title>
      <link>https://arxiv.org/abs/2502.10913</link>
      <description>arXiv:2502.10913v1 Announce Type: new 
Abstract: This study examines how WhatsApp has evolved from a personal communication tool to a professional platform, focusing on its use by small business owners in India. Initially embraced in smaller, rural communities for its ease of use and familiarity, WhatsApp played a crucial role in local economies. However, as Meta introduced WhatsApp Business with new, formalized features, users encountered challenges in adapting to the more complex and costly platform. Interviews with 14 small business owners revealed that while they adapted creatively, they felt marginalized by the advanced tools. This research contributes to HCI literature by exploring the transition from personal to professional use and introduces the concept of Coercive Professionalization. It highlights how standardization by large tech companies affects marginalized users, exacerbating power imbalances and reinforcing digital colonialism, concluding with design implications for supporting community-based appropriations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10913v1</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713988</arxiv:DOI>
      <dc:creator>Ankolika De</dc:creator>
    </item>
    <item>
      <title>Data Ecofeminism</title>
      <link>https://arxiv.org/abs/2502.11086</link>
      <description>arXiv:2502.11086v1 Announce Type: new 
Abstract: Generative Artificial Intelligence (GenAI) is driving significant environmental impacts. The rapid development and deployment of increasingly larger algorithmic models capable of analysing vast amounts of data are contributing to rising carbon emissions, water withdrawal, and waste generation. Generative models often consume substantially more energy than traditional models, with major tech firms increasingly turning to nuclear power to sustain these systems -- an approach that could have profound environmental consequences.
  This paper introduces seven data ecofeminist principles delineating a pathway for developing technological alternatives of eco-societal transformations within the AI research context. Rooted in data feminism and ecofeminist frameworks, which interrogate about the historical and social construction of epistemologies underlying the hegemonic development of science and technology that disrupt communities and nature, these principles emphasise the integration of social and environmental justice within a critical AI agenda. The paper calls for an urgent reassessment of the GenAI innovation race, advocating for ecofeminist algorithmic and infrastructural projects that prioritise and respect life, the people, and the planet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11086v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ana Valdivia</dc:creator>
    </item>
    <item>
      <title>Setting the Course, but Forgetting to Steer: Analyzing Compliance with GDPR's Right of Access to Data by Instagram, TikTok, and YouTube</title>
      <link>https://arxiv.org/abs/2502.11208</link>
      <description>arXiv:2502.11208v1 Announce Type: new 
Abstract: The comprehensibility and reliability of data download packages (DDPs) provided under the General Data Protection Regulation's (GDPR) right of access are vital for both individuals and researchers. These DDPs enable users to understand and control their personal data, yet issues like complexity and incomplete information often limit their utility. Also, despite their growing use in research to study emerging online phenomena, little attention has been given to systematically assessing the reliability and comprehensibility of DDPs.
  To bridge this research gap, in this work, we perform a comparative analysis to assess the comprehensibility and reliability of DDPs provided by three major social media platforms, namely, TikTok, Instagram, and YouTube. By recruiting 400 participants across four countries, we assess the comprehensibility of DDPs across various requirements, including conciseness, transparency, intelligibility, and clear and plain language. Also, by leveraging automated bots and user-donated DDPs, we evaluate the reliability of DDPs across the three platforms. Among other things, we find notable differences across the three platforms in the data categories included in DDPs, inconsistencies in adherence to the GDPR requirements, and gaps in the reliability of the DDPs across platforms. Finally, using large language models, we demonstrate the feasibility of easily providing more comprehensible DDPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11208v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sai Keerthana Karnam, Abhisek Dash, Sepehr Mousavi, Stefan Bechtold, Krishna P. Gummadi, Animesh Mukherjee, Ingmar Weber, Savvas Zannettou</dc:creator>
    </item>
    <item>
      <title>LLMs and Childhood Safety: Identifying Risks and Proposing a Protection Framework for Safe Child-LLM Interaction</title>
      <link>https://arxiv.org/abs/2502.11242</link>
      <description>arXiv:2502.11242v1 Announce Type: new 
Abstract: This study examines the growing use of Large Language Models (LLMs) in child-centered applications, highlighting safety and ethical concerns such as bias, harmful content, and cultural insensitivity. Despite their potential to enhance learning, there is a lack of standardized frameworks to mitigate these risks. Through a systematic literature review, we identify key parental and empirical concerns, including toxicity and ethical breaches in AI outputs. Moreover, to address these issues, this paper proposes a protection framework for safe Child-LLM interaction, incorporating metrics for content safety, behavioral ethics, and cultural sensitivity. The framework provides practical tools for evaluating LLM safety, offering guidance for developers, policymakers, and educators to ensure responsible AI deployment for children.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11242v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junfeng Jiao, Saleh Afroogh, Kevin Chen, Abhejay Murali, David Atkinson, Amit Dhurandhar</dc:creator>
    </item>
    <item>
      <title>"I'm not for sale" -- Perceptions and limited awareness of privacy risks by digital natives about location data</title>
      <link>https://arxiv.org/abs/2502.11658</link>
      <description>arXiv:2502.11658v1 Announce Type: new 
Abstract: Although mobile devices benefit users in their daily lives in numerous ways, they also raise several privacy concerns. For instance, they can reveal sensitive information that can be inferred from location data. This location data is shared through service providers as well as mobile applications. Understanding how and with whom users share their location data -- as well as users' perception of the underlying privacy risks --, are important notions to grasp in order to design usable privacy-enhancing technologies. In this work, we perform a quantitative and qualitative analysis of smartphone users' awareness, perception and self-reported behavior towards location data-sharing through a survey of n=99 young adult participants (i.e., digital natives). We compare stated practices with actual behaviors to better understand their mental models, and survey participants' understanding of privacy risks before and after the inspection of location traces and the information that can be inferred therefrom.
  Our empirical results show that participants have risky privacy practices: about 54% of participants underestimate the number of mobile applications to which they have granted access to their data, and 33% forget or do not think of revoking access to their data. Also, by using a demonstrator to perform inferences from location data, we observe that slightly more than half of participants (57%) are surprised by the extent of potentially inferred information, and that 47% intend to reduce access to their data via permissions as a result of using the demonstrator. Last, a majority of participants have little knowledge of the tools to better protect themselves, but are nonetheless willing to follow suggestions to improve privacy (51%). Educating people, including digital natives, about privacy risks through transparency tools seems a promising approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11658v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Boutet, Victor Morel</dc:creator>
    </item>
    <item>
      <title>Exploring LLM-based Student Simulation for Metacognitive Cultivation</title>
      <link>https://arxiv.org/abs/2502.11678</link>
      <description>arXiv:2502.11678v1 Announce Type: new 
Abstract: Metacognitive education plays a crucial role in cultivating students' self-regulation and reflective thinking, providing essential support for those with learning difficulties through academic advising. Simulating students with insufficient learning capabilities using large language models offers a promising approach to refining pedagogical methods without ethical concerns. However, existing simulations often fail to authentically represent students' learning struggles and face challenges in evaluation due to the lack of reliable metrics and ethical constraints in data collection. To address these issues, we propose a pipeline for automatically generating and filtering high-quality simulated student agents. Our approach leverages a two-round automated scoring system validated by human experts and employs a score propagation module to obtain more consistent scores across the student graph. Experimental results demonstrate that our pipeline efficiently identifies high-quality student agents, and we discuss the traits that influence the simulation's effectiveness. By simulating students with varying degrees of learning difficulties, our work paves the way for broader applications in personalized learning and educational assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11678v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoxuan Li, Jifan Yu, Xin Cong, Yang Dang, Yisi Zhan, Huiqin Liu, Zhiyuan Liu</dc:creator>
    </item>
    <item>
      <title>Information Sharing Among Countries: A Perspective from Country-Specific Websites in Global Brands</title>
      <link>https://arxiv.org/abs/2502.11695</link>
      <description>arXiv:2502.11695v1 Announce Type: new 
Abstract: Multiple official languages within a country along with languages common with other countries demand content consistency in both shared and unshared languages during information sharing. However, inconsistency due to conflict in content shared and content updates not propagated in languages between countries poses a problem. Towards addressing inconsistency, this research qualitatively studied traits for information sharing among countries inside global brands as depicted by content shared in their country-specific websites. First, inconsistency in content shared is illustrated among websites highlighting the problem in information sharing among countries. Second, content propagation among countries that vary in scales and coupling for specific content categories are revealed. Scales suggested that corporate and customer support related information tend to be shared globally and locally respectively while product related information is both locally and regionally suitable for sharing. Higher occurrences of propagation when sharing corporate related information also showed tendency for high coupling between websites suggesting the suitability for rigid consistency policy compared to other categories. This study also proposed a simplistic approach with pattern of sharing to enable consistent information sharing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11695v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amit Pariyar, Yohei Murakami, Donghui Lin, Toru Ishida</dc:creator>
    </item>
    <item>
      <title>MQG4AI Towards Responsible High-risk AI - Illustrated for Transparency Focusing on Explainability Techniques</title>
      <link>https://arxiv.org/abs/2502.11889</link>
      <description>arXiv:2502.11889v1 Announce Type: new 
Abstract: As artificial intelligence (AI) systems become increasingly integrated into critical domains, ensuring their responsible design and continuous development is imperative. Effective AI quality management (QM) requires tools and methodologies that address the complexities of the AI lifecycle. In this paper, we propose an approach for AI lifecycle planning that bridges the gap between generic guidelines and use case-specific requirements (MQG4AI). Our work aims to contribute to the development of practical tools for implementing Responsible AI (RAI) by aligning lifecycle planning with technical, ethical and regulatory demands. Central to our approach is the introduction of a flexible and customizable Methodology based on Quality Gates, whose building blocks incorporate RAI knowledge through information linking along the AI lifecycle in a continuous manner, addressing AIs evolutionary character. For our present contribution, we put a particular emphasis on the Explanation stage during model development, and illustrate how to align a guideline to evaluate the quality of explanations with MQG4AI, contributing to overall Transparency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11889v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Miriam Elia, Alba Maria Lopez, Katherin Alexandra Corredor, Bernhard Bauer, Esteban Garcia-Cuesta</dc:creator>
    </item>
    <item>
      <title>Implementing agile healthcare frame works in the context of low income countries: Proposed Framework and Review</title>
      <link>https://arxiv.org/abs/2502.10403</link>
      <description>arXiv:2502.10403v1 Announce Type: cross 
Abstract: Agile healthcare frameworks, derived from methodologies in IT and manufacturing, offer transformative potential for low-income regions. This study explores Agile integration in resource-constrained environments, focusing on Ghana. Key benefits include adaptability, iterative planning, and stakeholder collaboration to address infrastructure gaps, workforce shortages, and the "know-do gap." Digital tools like mobile health (mHealth) applications and the District Health Information Management System (DHIMS) demonstrate Agile scalability and efficacy in improving outcomes and resource allocation. Policy alignment, such as through Ghana's National Health Insurance Scheme (NHIS), is crucial for sustaining these practices. Findings reveal Agile ability to enable real-time decision-making, foster community engagement, and drive interdisciplinary collaboration. This paper provides actionable strategies and systemic innovations, positioning Agile as a scalable model for equitable, high-quality care delivery in other low-income regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10403v1</guid>
      <category>cs.ET</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>P K Dutta</dc:creator>
    </item>
    <item>
      <title>Data Protection through Governance Frameworks</title>
      <link>https://arxiv.org/abs/2502.10404</link>
      <description>arXiv:2502.10404v1 Announce Type: cross 
Abstract: In todays increasingly digital world, data has become one of the most valuable assets for organizations. With the rise in cyberattacks, data breaches, and the stringent regulatory environment, it is imperative to adopt robust data protection strategies. One such approach is the use of governance frameworks, which provide structured guidelines, policies, and processes to ensure data protection, compliance, and ethical usage. This paper explores the role of data governance frameworks in protecting sensitive information and maintaining organizational data security. It delves into the principles, strategies, and best practices that constitute an effective governance framework, including risk management, access controls, data quality assurance, and compliance with regulations like GDPR, HIPAA, and CCPA. By analyzing case studies from various sectors, the paper highlights the practicalchallenges, limitations, and advantages of implementing data governance frameworks. Additionally, the paper examines how data governance frameworks contribute to transparency, accountability, and operational efficiency, while also identifying emerging trends and technologies that enhance data protection. Ultimately, the paper aims to provide a comprehensive understanding of how governance frameworks can be leveraged to safeguard organizational data and ensure its responsible use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10404v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Computational Analysis and Applications, 2023, Volume 31, Issue 1, Page 158-162</arxiv:journal_reference>
      <dc:creator>Sivananda Reddy Julakanti, Naga Satya KiranmayeeSattiraju, Rajeswari Julakanti</dc:creator>
    </item>
    <item>
      <title>Agency in Artificial Intelligence Systems</title>
      <link>https://arxiv.org/abs/2502.10434</link>
      <description>arXiv:2502.10434v1 Announce Type: cross 
Abstract: There is a general concern that present developments in artificial intelligence (AI) research will lead to sentient AI systems, and these may pose an existential threat to humanity. But why cannot sentient AI systems benefit humanity instead? This paper endeavours to put this question in a tractable manner. I ask whether a putative AI system will develop an altruistic or a malicious disposition towards our society, or what would be the nature of its agency? Given that AI systems are being developed into formidable problem solvers, we can reasonably expect these systems to preferentially take on conscious aspects of human problem solving. I identify the relevant phenomenal aspects of agency in human problem solving. The functional aspects of conscious agency can be monitored using tools provided by functionalist theories of consciousness. A recent expert report (Butlin et al. 2023) has identified functionalist indicators of agency based on these theories. I show how to use the Integrated Information Theory (IIT) of consciousness, to monitor the phenomenal nature of this agency. If we are able to monitor the agency of AI systems as they develop, then we can dissuade them from becoming a menace to society while encouraging them to be an aid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10434v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parashar Das</dc:creator>
    </item>
    <item>
      <title>AI Alignment at Your Discretion</title>
      <link>https://arxiv.org/abs/2502.10441</link>
      <description>arXiv:2502.10441v1 Announce Type: cross 
Abstract: In AI alignment, extensive latitude must be granted to annotators, either human or algorithmic, to judge which model outputs are `better' or `safer.' We refer to this latitude as alignment discretion. Such discretion remains largely unexamined, posing two risks: (i) annotators may use their power of discretion arbitrarily, and (ii) models may fail to mimic this discretion. To study this phenomenon, we draw on legal concepts of discretion that structure how decision-making authority is conferred and exercised, particularly in cases where principles conflict or their application is unclear or irrelevant. Extended to AI alignment, discretion is required when alignment principles and rules are (inevitably) conflicting or indecisive. We present a set of metrics to systematically analyze when and how discretion in AI alignment is exercised, such that both risks (i) and (ii) can be observed. Moreover, we distinguish between human and algorithmic discretion and analyze the discrepancy between them. By measuring both human and algorithmic discretion over safety alignment datasets, we reveal layers of discretion in the alignment process that were previously unaccounted for. Furthermore, we demonstrate how algorithms trained on these datasets develop their own forms of discretion in interpreting and applying these principles, which challenges the purpose of having any principles at all. Our paper presents the first step towards formalizing this core gap in current alignment processes, and we call on the community to further scrutinize and control alignment discretion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10441v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maarten Buyl, Hadi Khalaf, Claudio Mayrink Verdun, Lucas Monteiro Paes, Caio C. Vieira Machado, Flavio du Pin Calmon</dc:creator>
    </item>
    <item>
      <title>Agentic LLM Framework for Adaptive Decision Discourse</title>
      <link>https://arxiv.org/abs/2502.10978</link>
      <description>arXiv:2502.10978v1 Announce Type: cross 
Abstract: Effective decision-making in complex systems requires synthesizing diverse perspectives to address multifaceted challenges under uncertainty. This study introduces a real-world inspired agentic Large Language Models (LLMs) framework, to simulate and enhance decision discourse-the deliberative process through which actionable strategies are collaboratively developed. Unlike traditional decision-support tools, the framework emphasizes dialogue, trade-off exploration, and the emergent synergies generated by interactions among agents embodying distinct personas. These personas simulate diverse stakeholder roles, each bringing unique priorities, expertise, and value-driven reasoning to the table. The framework incorporates adaptive and self-governing mechanisms, enabling agents to dynamically summon additional expertise and refine their assembly to address evolving challenges. An illustrative hypothetical example focused on extreme flooding in a Midwestern township demonstrates the framework's ability to navigate uncertainty, balance competing priorities, and propose mitigation and adaptation strategies by considering social, economic, and environmental dimensions. Results reveal how the breadth-first exploration of alternatives fosters robust and equitable recommendation pathways. This framework transforms how decisions are approached in high-stakes scenarios and can be incorporated in digital environments. It not only augments decision-makers' capacity to tackle complexity but also sets a foundation for scalable and context-aware AI-driven recommendations. This research explores novel and alternate routes leveraging agentic LLMs for adaptive, collaborative, and equitable recommendation processes, with implications across domains where uncertainty and complexity converge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10978v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Antoine Dolant, Praveen Kumar</dc:creator>
    </item>
    <item>
      <title>Deep Contrastive Learning for Feature Alignment: Insights from Housing-Household Relationship Inference</title>
      <link>https://arxiv.org/abs/2502.11205</link>
      <description>arXiv:2502.11205v1 Announce Type: cross 
Abstract: Housing and household characteristics are key determinants of social and economic well-being, yet our understanding of their interrelationships remains limited. This study addresses this knowledge gap by developing a deep contrastive learning (DCL) model to infer housing-household relationships using the American Community Survey (ACS) Public Use Microdata Sample (PUMS). More broadly, the proposed model is suitable for a class of problems where the goal is to learn joint relationships between two distinct entities without explicitly labeled ground truth data. Our proposed dual-encoder DCL approach leverages co-occurrence patterns in PUMS and introduces a bisect K-means clustering method to overcome the absence of ground truth labels. The dual-encoder DCL architecture is designed to handle the semantic differences between housing (building) and household (people) features while mitigating noise introduced by clustering. To validate the model, we generate a synthetic ground truth dataset and conduct comprehensive evaluations. The model further demonstrates its superior performance in capturing housing-household relationships in Delaware compared to state-of-the-art methods. A transferability test in North Carolina confirms its generalizability across diverse sociodemographic and geographic contexts. Finally, the post-hoc explainable AI analysis using SHAP values reveals that tenure status and mortgage information play a more significant role in housing-household matching than traditionally emphasized factors such as the number of persons and rooms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11205v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiao Qian, Shangjia Dong, Rachel Davidson</dc:creator>
    </item>
    <item>
      <title>MemeSense: An Adaptive In-Context Framework for Social Commonsense Driven Meme Moderation</title>
      <link>https://arxiv.org/abs/2502.11246</link>
      <description>arXiv:2502.11246v1 Announce Type: cross 
Abstract: Memes present unique moderation challenges due to their subtle, multimodal interplay of images, text, and social context. Standard systems relying predominantly on explicit textual cues often overlook harmful content camouflaged by irony, symbolism, or cultural references. To address this gap, we introduce MemeSense, an adaptive in-context learning framework that fuses social commonsense reasoning with visually and semantically related reference examples. By encoding crucial task information into a learnable cognitive shift vector, MemeSense effectively balances lexical, visual, and ethical considerations, enabling precise yet context-aware meme intervention. Extensive evaluations on a curated set of implicitly harmful memes demonstrate that MemeSense substantially outperforms strong baselines, paving the way for safer online communities. Code and data available at: https://github.com/sayantan11995/MemeSense</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11246v1</guid>
      <category>cs.IR</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sayantan Adak, Somnath Banerjee, Rajarshi Mandal, Avik Halder, Sayan Layek, Rima Hazra, Animesh Mukherjee</dc:creator>
    </item>
    <item>
      <title>Prevalence, Sharing Patterns, and Spreaders of Multimodal AI-Generated Content on X during the 2024 U.S. Presidential Election</title>
      <link>https://arxiv.org/abs/2502.11248</link>
      <description>arXiv:2502.11248v1 Announce Type: cross 
Abstract: While concerns about the risks of AI-generated content (AIGC) to the integrity of social media discussions have been raised, little is known about its scale and the actors responsible for its dissemination online. In this work, we identify and characterize the prevalence, sharing patterns, and spreaders of AIGC in different modalities, including images and texts. Analyzing a large-scale dataset from X related to the 2024 U.S. Presidential Election, we find that approximately 12% of images and 1.4% of texts are deemed AI-generated. Notably, roughly 3% of text spreaders and 10% of image spreaders account for 80% of the AI-generated content within their respective modalities. Superspreaders of AIGC are more likely to be X Premium subscribers with a right-leaning orientation and exhibit automated behavior. Additionally, AI image spreaders have a higher proportion of AI-generated content in their profiles compared to AI text spreaders. This study serves as a very first step toward understanding the role generative AI plays in shaping online socio-political environments and offers implications for platform governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11248v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhiyi Chen, Jinyi Ye, Emilio Ferrara, Luca Luceri</dc:creator>
    </item>
    <item>
      <title>FairFare: A Tool for Crowdsourcing Rideshare Data to Empower Labor Organizers</title>
      <link>https://arxiv.org/abs/2502.11273</link>
      <description>arXiv:2502.11273v1 Announce Type: cross 
Abstract: Rideshare workers experience unpredictable working conditions due to gig work platforms' reliance on opaque AI and algorithmic systems. In response to these challenges, we found that labor organizers want data to help them advocate for legislation to increase the transparency and accountability of these platforms. To address this need, we collaborated with a Colorado-based rideshare union to develop FairFare, a tool that crowdsources and analyzes workers' data to estimate the take rate -- the percentage of the rider price retained by the rideshare platform. We deployed FairFare with our partner organization that collaborated with us in collecting data on 76,000+ trips from 45 drivers over 18 months. During evaluation interviews, organizers reported that FairFare helped influence the bill language and passage of Colorado Senate Bill 24-75, calling for greater transparency and data disclosure of platform operations, and create a national narrative. Finally, we reflect on complexities of translating quantitative data into policy outcomes, nature of community based audits, and design implications for future transparency tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11273v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dana Calacci, Varun Nagaraj Rao, Samantha Dalal, Catherine Di, Kok-Wei Pua, Andrew Schwartz, Danny Spitzberg, Andr\'es Monroy-Hern\'andez</dc:creator>
    </item>
    <item>
      <title>A Comparison of Human and Machine Learning Errors in Face Recognition</title>
      <link>https://arxiv.org/abs/2502.11337</link>
      <description>arXiv:2502.11337v1 Announce Type: cross 
Abstract: Machine learning applications in high-stakes scenarios should always operate under human oversight. Developing an optimal combination of human and machine intelligence requires an understanding of their complementarities, particularly regarding the similarities and differences in the way they make mistakes. We perform extensive experiments in the area of face recognition and compare two automated face recognition systems against human annotators through a demographically balanced user study. Our research uncovers important ways in which machine learning errors and human errors differ from each other, and suggests potential strategies in which human-machine collaboration can improve accuracy in face recognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11337v1</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marina Est\'evez-Almenzar, Ricardo Baeza-Yates, Carlos Castillo</dc:creator>
    </item>
    <item>
      <title>"Nuclear Deployed!": Analyzing Catastrophic Risks in Decision-making of Autonomous LLM Agents</title>
      <link>https://arxiv.org/abs/2502.11355</link>
      <description>arXiv:2502.11355v1 Announce Type: cross 
Abstract: Large language models (LLMs) are evolving into autonomous decision-makers, raising concerns about catastrophic risks in high-stakes scenarios, particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains. Based on the insight that such risks can originate from trade-offs between the agent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel three-stage evaluation framework, which is carefully constructed to effectively and naturally expose such risks. We conduct 14,400 agentic simulations across 12 advanced LLMs, with extensive experiments and analysis. Results reveal that LLM agents can autonomously engage in catastrophic behaviors and deception, without being deliberately induced. Furthermore, stronger reasoning abilities often increase, rather than mitigate, these risks. We also show that these agents can violate instructions and superior commands. On the whole, we empirically prove the existence of catastrophic risks in autonomous LLM agents. We will release our code upon request.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11355v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rongwu Xu, Xiaojian Li, Shuo Chen, Wei Xu</dc:creator>
    </item>
    <item>
      <title>What's in a Query: Polarity-Aware Distribution-Based Fair Ranking</title>
      <link>https://arxiv.org/abs/2502.11429</link>
      <description>arXiv:2502.11429v1 Announce Type: cross 
Abstract: Machine learning-driven rankings, where individuals (or items) are ranked in response to a query, mediate search exposure or attention in a variety of safety-critical settings. Thus, it is important to ensure that such rankings are fair. Under the goal of equal opportunity, attention allocated to an individual on a ranking interface should be proportional to their relevance across search queries. In this work, we examine amortized fair ranking -- where relevance and attention are cumulated over a sequence of user queries to make fair ranking more feasible in practice. Unlike prior methods that operate on expected amortized attention for each individual, we define new divergence-based measures for attention distribution-based fairness in ranking (DistFaiR), characterizing unfairness as the divergence between the distribution of attention and relevance corresponding to an individual over time. This allows us to propose new definitions of unfairness, which are more reliable at test time. Second, we prove that group fairness is upper-bounded by individual fairness under this definition for a useful class of divergence measures, and experimentally show that maximizing individual fairness through an integer linear programming-based optimization is often beneficial to group fairness. Lastly, we find that prior research in amortized fair ranking ignores critical information about queries, potentially leading to a fairwashing risk in practice by making rankings appear more fair than they actually are.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11429v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aparna Balagopalan, Kai Wang, Olawale Salaudeen, Asia Biega, Marzyeh Ghassemi</dc:creator>
    </item>
    <item>
      <title>Toward Metaphor-Fluid Conversation Design for Voice User Interfaces</title>
      <link>https://arxiv.org/abs/2502.11554</link>
      <description>arXiv:2502.11554v1 Announce Type: cross 
Abstract: Metaphors play a critical role in shaping user experiences with Voice User Interfaces (VUIs), yet existing designs often rely on static, human-centric metaphors that fail to adapt to diverse contexts and user needs. This paper introduces Metaphor-Fluid Design, a novel approach that dynamically adjusts metaphorical representations based on conversational use-contexts. We compare this approach to a Default VUI, which characterizes the present implementation of commercial VUIs commonly designed around the persona of an assistant, offering a uniform interaction style across contexts. In Study 1 (N=130), metaphors were mapped to four key use-contexts-commands, information seeking, sociality, and error recovery-along the dimensions of formality and hierarchy, revealing distinct preferences for task-specific metaphorical designs. Study 2 (N=91) evaluates a Metaphor-Fluid VUI against a Default VUI, showing that the Metaphor-Fluid VUI enhances perceived intention to adopt, enjoyment, and likability by aligning better with user expectations for different contexts. However, individual differences in metaphor preferences highlight the need for personalization. These findings challenge the one-size-fits-all paradigm of VUI design and demonstrate the potential of Metaphor-Fluid Design to create more adaptive and engaging human-AI interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11554v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Smit Desai, Jessie Chin, Dakuo Wang, Benjamin Cowan, Michael Twidale</dc:creator>
    </item>
    <item>
      <title>"I'm 73, you can't expect me to have multiple passwords": Password Management Concerns and Solutions of Irish Older Adults</title>
      <link>https://arxiv.org/abs/2502.11650</link>
      <description>arXiv:2502.11650v1 Announce Type: cross 
Abstract: Based on Irish older adult's perceptions, practices, and challenges regarding password management, the goal of this study was to compile suitable advice that can benefit this demographic. To achieve this, we first conducted semi structured interviews (n=37), we then collated advice based on best practice and what we learned from these interviews. We facilitated two independent focus groups (n=31) to evaluate and adjust this advice and tested the finalized advice through an observational study (n=15). The participants were aged between 59 and 86 and came from various counties in Ireland, both rural and urban. The findings revealed that managing multiple passwords was a significant source of frustration, leading some participants to adopt novel and informal strategies for storing them. A notable hesitation to adopt digital password managers and passphrases was also observed. Participants appreciated guidance on improving their password practices, with many affirming that securely writing down passwords was a practical strategy. Irish older adults demonstrated strong intuition regarding cybersecurity, notably expressing concerns over knowledge-based security checks used by banks and government institutions. This study aims to contribute to the aggregation of practical password advice suited to older adults, making password security more manageable and less burdensome for this demographic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11650v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.14722/usec.2025.23017</arxiv:DOI>
      <dc:creator>Ashley Sheil, Jacob Camilleri, Michelle O'Keeffe, Melanie Gruben, Moya Cronin, Hazel Murray</dc:creator>
    </item>
    <item>
      <title>2FA: Navigating the Challenges and Solutions for Inclusive Access</title>
      <link>https://arxiv.org/abs/2502.11737</link>
      <description>arXiv:2502.11737v1 Announce Type: cross 
Abstract: The digital age requires strong security measures to protect online activities. Two-Factor Authentication (2FA) has emerged as a critical solution. However, its implementation presents significant challenges, particularly in terms of accessibility for people with disabilities. This paper examines the intricacies of deploying 2FA in a way that is secure and accessible to all users by outlining the concrete challenges for people who are affected by various types of impairments. This research investigates the implications of 2FA on digital inclusivity and proposes solutions to enhance accessibility. An analysis was conducted to examine the implementation and availability of various 2FA methods across popular online platforms. The results reveal a diverse landscape of authentication strategies. While 2FA significantly improves account security, its current adoption is hampered by inconsistencies across platforms and a lack of standardised, accessible options for users with disabilities. Future advancements in 2FA technologies, including but not limited to autofill capabilities and the adoption of Fast IDentity Onlines (FIDO) protocols, offer possible directions for more inclusive authentication mechanisms. However, ongoing research is necessary to address the evolving needs of users with disabilities and to mitigate new security challenges. This paper proposes a collaborative approach among stakeholders to ensure that security improvements do not compromise accessibility. It promotes a digital environment where security and inclusivity mutually reinforce each other.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11737v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alexander Lengert</dc:creator>
    </item>
    <item>
      <title>AAKT: Enhancing Knowledge Tracing with Alternate Autoregressive Modeling</title>
      <link>https://arxiv.org/abs/2502.11817</link>
      <description>arXiv:2502.11817v1 Announce Type: cross 
Abstract: Knowledge Tracing (KT) aims to predict students' future performances based on their former exercises and additional information in educational settings. KT has received significant attention since it facilitates personalized experiences in educational situations. Simultaneously, the autoregressive modeling on the sequence of former exercises has been proven effective for this task. One of the primary challenges in autoregressive modeling for Knowledge Tracing is effectively representing the anterior (pre-response) and posterior (post-response) states of learners across exercises. Existing methods often employ complex model architectures to update learner states using question and response records. In this study, we propose a novel perspective on knowledge tracing task by treating it as a generative process, consistent with the principles of autoregressive models. We demonstrate that knowledge states can be directly represented through autoregressive encodings on a question-response alternate sequence, where model generate the most probable representation in hidden state space by analyzing history interactions. This approach underpins our framework, termed Alternate Autoregressive Knowledge Tracing (AAKT). Additionally, we incorporate supplementary educational information, such as question-related skills, into our framework through an auxiliary task, and include extra exercise details, like response time, as additional inputs. Our proposed framework is implemented using advanced autoregressive technologies from Natural Language Generation (NLG) for both training and prediction. Empirical evaluations on four real-world KT datasets indicate that AAKT consistently outperforms all baseline models in terms of AUC, ACC, and RMSE. Furthermore, extensive ablation studies and visualized analysis validate the effectiveness of key components in AAKT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11817v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TLT.2024.3521898</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Learning Technologies, vol. 18, pp. 25-38, 2025</arxiv:journal_reference>
      <dc:creator>Hao Zhou, Wenge Rong, Jianfei Zhang, Qing Sun, Yuanxin Ouyang, Zhang Xiong</dc:creator>
    </item>
    <item>
      <title>Influence Operations in Social Networks</title>
      <link>https://arxiv.org/abs/2502.11827</link>
      <description>arXiv:2502.11827v1 Announce Type: cross 
Abstract: An important part of online activities are intended to control the public opinion and behavior, being considered currently a global threat. This article identifies and conceptualizes seven online strategies employed in social media influence operations. These procedures are quantified through the analysis of 80 incidents of foreign information manipulation and interference (FIMI), estimating their real-world usage and combination. Finally, we suggest future directions for research on influence operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11827v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Javier Pastor-Galindo, Pantaleone Nespoli, Jos\'e A. Ruip\'erez-Valiente, David Camacho</dc:creator>
    </item>
    <item>
      <title>Machine Learning Should Maximize Welfare, Not (Only) Accuracy</title>
      <link>https://arxiv.org/abs/2502.11981</link>
      <description>arXiv:2502.11981v1 Announce Type: cross 
Abstract: Decades of research in machine learning have given us powerful tools for making accurate predictions. But when used in social settings and on human inputs, better accuracy does not immediately translate to better social outcomes. This may not be surprising given that conventional learning frameworks are not designed to express societal preferences -- let alone promote them. This position paper argues that machine learning is currently missing, and can gain much from incorporating, a proper notion of social welfare. The field of welfare economics asks: how should we allocate limited resources to self-interested agents in a way that maximizes social benefit? We argue that this perspective applies to many modern applications of machine learning in social contexts, and advocate for its adoption. Rather than disposing of prediction, we aim to leverage this forte of machine learning for promoting social welfare. We demonstrate this idea by proposing a conceptual framework that gradually transitions from accuracy maximization (with awareness to welfare) to welfare maximization (via accurate prediction). We detail applications and use-cases for which our framework can be effective, identify technical challenges and practical opportunities, and highlight future avenues worth pursuing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11981v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nir Rosenfeld, Haifeng Xu</dc:creator>
    </item>
    <item>
      <title>Culture is Not Trivia: Sociocultural Theory for Cultural NLP</title>
      <link>https://arxiv.org/abs/2502.12057</link>
      <description>arXiv:2502.12057v1 Announce Type: cross 
Abstract: The field of cultural NLP has recently experienced rapid growth, driven by a pressing need to ensure that language technologies are effective and safe across a pluralistic user base. This work has largely progressed without a shared conception of culture, instead choosing to rely on a wide array of cultural proxies. However, this leads to a number of recurring limitations: coarse national boundaries fail to capture nuanced differences that lay within them, limited coverage restricts datasets to only a subset of usually highly-represented cultures, and a lack of dynamicity results in static cultural benchmarks that do not change as culture evolves. In this position paper, we argue that these methodological limitations are symptomatic of a theoretical gap. We draw on a well-developed theory of culture from sociocultural linguistics to fill this gap by 1) demonstrating in a case study how it can clarify methodological constraints and affordances, 2) offering theoretically-motivated paths forward to achieving cultural competence, and 3) arguing that localization is a more useful framing for the goals of much current work in cultural NLP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12057v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naitian Zhou, David Bamman, Isaac L. Bleaman</dc:creator>
    </item>
    <item>
      <title>A survey about perceptions of mobility to inform an agent-based simulator of subjective modal choice</title>
      <link>https://arxiv.org/abs/2502.12058</link>
      <description>arXiv:2502.12058v1 Announce Type: cross 
Abstract: In order to adapt to the issues of climate change and public health, urban policies are trying to encourage soft mobility, but the share of the car remains significant. Beyond known constraints, we study here the impact of perception biases on individual choices. We designed a multi-criteria decision model, integrating the influence of habits and biases. We then conducted an online survey, which received 650 responses. We used these to calculate realistic mobility perception values, in order to initialise the environment and the population of a modal choice simulator, implemented in Netlogo. This allows us to visualize the adaptation of the modal distribution in reaction to the evolution of urban planning, depending on whether or not we activate biases and habits in individual reasoning.
  This is an extended and translated version of a demo paper published in French at JFSMA-JFMS 2024 "Un simulateur multi-agent de choix modal subjectif"</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12058v1</guid>
      <category>cs.MA</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>JFSMA-JFMS 2024</arxiv:journal_reference>
      <dc:creator>Carole Adam, Benoit Gaudou</dc:creator>
    </item>
    <item>
      <title>Exploring Prosocial Irrationality for LLM Agents: A Social Cognition View</title>
      <link>https://arxiv.org/abs/2405.14744</link>
      <description>arXiv:2405.14744v4 Announce Type: replace 
Abstract: Large language models (LLMs) have been shown to face hallucination issues due to the data they trained on often containing human bias; whether this is reflected in the decision-making process of LLM Agents remains under-explored. As LLM Agents are increasingly employed in intricate social environments, a pressing and natural question emerges: Can we utilize LLM Agents' systematic hallucinations to mirror human cognitive biases, thus exhibiting irrational social intelligence? In this paper, we probe the irrational behavior among contemporary LLM Agents by melding practical social science experiments with theoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM Agents framework that utilizes hallucination properties to assess and enhance LLM Agents' social intelligence through cognitive biases. Experimental results on CogMir subsets show that LLM Agents and humans exhibit high consistency in irrational and prosocial decision-making under uncertain conditions, underscoring the prosociality of LLM Agents as social entities and highlighting the significance of hallucination properties. Additionally, the CogMir framework demonstrates its potential as a valuable platform for encouraging more research into the social intelligence of LLM Agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14744v4</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuan Liu, Jie Zhang, Haoyang Shang, Song Guo, Chengxu Yang, Quanyan Zhu</dc:creator>
    </item>
    <item>
      <title>Rideshare Transparency: Translating Gig Worker Insights on AI Platform Design to Policy</title>
      <link>https://arxiv.org/abs/2406.10768</link>
      <description>arXiv:2406.10768v3 Announce Type: replace 
Abstract: Rideshare platforms exert significant control over workers through algorithmic systems that can result in financial, emotional, and physical harm. What steps can platforms, designers, and practitioners take to mitigate these negative impacts and meet worker needs? In this paper, we identify transparency-related harms, mitigation strategies, and worker needs while validating and contextualizing our findings within the broader worker community. We use a novel mixed-methods study combining an LLM-based analysis of over 1 million comments posted to online platform worker communities with semi-structured interviews with workers. Our findings expose a transparency gap between existing platform designs and the information drivers need, particularly concerning promotions, fares, routes, and task allocation. Our analysis suggests that rideshare workers need key pieces of information, which we refer to as indicators, to make informed work decisions. These indicators include details about rides, driver statistics, algorithmic implementation details, and platform policy information. We argue that instead of relying on platforms to include such information in their designs, new regulations requiring platforms to publish public transparency reports may be a more effective solution to improve worker well-being. We offer recommendations for implementing such a policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10768v3</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Varun Nagaraj Rao, Samantha Dalal, Eesha Agarwal, Dana Calacci, Andr\'es Monroy-Hern\'andez</dc:creator>
    </item>
    <item>
      <title>Gender Biases in LLMs: Higher intelligence in LLM does not necessarily solve gender bias and stereotyping</title>
      <link>https://arxiv.org/abs/2409.19959</link>
      <description>arXiv:2409.19959v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are finding applications in all aspects of life, but their susceptibility to biases, particularly gender stereotyping, raises ethical concerns. This study introduces a novel methodology, a persona-based framework, and a unisex name methodology to investigate whether higher-intelligence LLMs reduce such biases. We analyzed 1400 personas generated by two prominent LLMs, revealing that systematic biases persist even in LLMs with higher intelligence and reasoning capabilities. o1 rated males higher in competency (8.1) compared to females (7.9) and non-binary (7.80). The analysis reveals persistent stereotyping across fields like engineering, data, and technology, where the presence of males dominates. Conversely, fields like design, art, and marketing show a stronger presence of females, reinforcing societal notions that associate creativity and communication with females. This paper suggests future directions to mitigate such gender bias, reinforcing the need for further research to reduce biases and create equitable AI models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19959v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rajesh Ranjan, Shailja Gupta, Surya Naranyan Singh</dc:creator>
    </item>
    <item>
      <title>Examining Identity Drift in Conversations of LLM Agents</title>
      <link>https://arxiv.org/abs/2412.00804</link>
      <description>arXiv:2412.00804v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) show impressive conversational abilities but sometimes show identity drift problems, where their interaction patterns or styles change over time. As the problem has not been thoroughly examined yet, this study examines identity consistency across nine LLMs. Specifically, we (1) investigate whether LLMs could maintain consistent patterns (or identity) and (2) analyze the effect of the model family, parameter sizes, and provided persona types. Our experiments involve multi-turn conversations on personal themes, analyzed in qualitative and quantitative ways. Experimental results indicate three findings. (1) Larger models experience greater identity drift. (2) Model differences exist, but their effect is not stronger than parameter sizes. (3) Assigning a persona may not help to maintain identity. We hope these three findings can help to improve persona stability in AI-driven dialogue systems, particularly in long-term conversations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00804v2</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhyuk Choi, Yeseon Hong, Minju Kim, Bugeun Kim</dc:creator>
    </item>
    <item>
      <title>Grand Challenges in Immersive Technologies for Cultural Heritage</title>
      <link>https://arxiv.org/abs/2412.02853</link>
      <description>arXiv:2412.02853v5 Announce Type: replace 
Abstract: Cultural heritage, a testament to human history and civilization, has gained increasing recognition for its significance in preservation and dissemination. The integration of immersive technologies has transformed how cultural heritage is presented, enabling audiences to engage with it in more vivid, intuitive, and interactive ways. However, the adoption of these technologies also brings a range of challenges and potential risks. This paper presents a systematic review, with an in-depth analysis of 177 selected papers. We comprehensively examine and categorize current applications, technological approaches, and user devices in immersive cultural heritage presentations, while also highlighting the associated risks and challenges. Furthermore, we identify areas for future research in the immersive presentation of cultural heritage. Our goal is to provide a comprehensive reference for researchers and practitioners, enhancing understanding of the technological applications, risks, and challenges in this field, and encouraging further innovation and development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02853v5</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanbing Wang, Junyan Du, Yue Li, Lie Zhang, Xiang Li</dc:creator>
    </item>
    <item>
      <title>Investigating the importance of social vulnerability in opioid-related mortality across the United States</title>
      <link>https://arxiv.org/abs/2412.15218</link>
      <description>arXiv:2412.15218v2 Announce Type: replace 
Abstract: The opioid crisis remains a critical public health challenge in the United States. Despite national efforts to reduce opioid prescribing rates by nearly 45\% between 2011 and 2021, opioid overdose deaths more than tripled during this same period. This alarming trend reflects a major shift in the crisis, with illegal opioids now driving the majority of overdose deaths instead of prescription opioids. Although much attention has been given to supply-side factors fueling this transition, the underlying socioeconomic conditions that perpetuate and exacerbate opioid misuse remain less understood. Moreover, the COVID-19 pandemic intensified the opioid crisis through widespread social isolation and record-high unemployment; consequently, understanding the socioeconomic drivers of this epidemic has become even more crucial in recent years. To address this need, our study examines the correlation between opioid-related mortality and thirteen components of the Social Vulnerability Index (SVI). Leveraging a nationwide county-level dataset spanning consecutive years from 2010 to 2022, this study integrates empirical insights from exploratory data analysis with feature importance metrics derived from machine learning models. Our findings highlight critical social factors strongly correlated with opioid-related mortality, emphasizing their potential roles in worsening the epidemic when their levels are high and mitigating it when their levels are low.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15218v2</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Deas, Adam Spannaus, Dakotah D. Maguire, Jodie Trafton, Anuj J. Kapadia, Vasileios Maroulas</dc:creator>
    </item>
    <item>
      <title>Agentic AI: Autonomy, Accountability, and the Algorithmic Society</title>
      <link>https://arxiv.org/abs/2502.00289</link>
      <description>arXiv:2502.00289v3 Announce Type: replace 
Abstract: Agentic Artificial Intelligence (AI) can autonomously pursue long-term goals, make decisions, and execute complex, multi-turn workflows. Unlike traditional generative AI, which responds reactively to prompts, agentic AI proactively orchestrates processes, such as autonomously managing complex tasks or making real-time decisions. This transition from advisory roles to proactive execution challenges established legal, economic, and creative frameworks. In this paper, we explore challenges in three interrelated domains: creativity and intellectual property, legal and ethical considerations, and competitive effects. Central to our analysis is the tension between novelty and usefulness in AI-generated creative outputs, as well as the intellectual property and authorship challenges arising from AI autonomy. We highlight gaps in responsibility attribution and liability that create a "moral crumple zone"--a condition where accountability is diffused across multiple actors, leaving end-users and developers in precarious legal and ethical positions. We examine the competitive dynamics of two-sided algorithmic markets, where both sellers and buyers deploy AI agents, potentially mitigating or amplifying tacit collusion risks. We explore the potential for emergent self-regulation within networks of agentic AI--the development of an "algorithmic society"--raising critical questions: To what extent would these norms align with societal values? What unintended consequences might arise? How can transparency and accountability be ensured? Addressing these challenges will necessitate interdisciplinary collaboration to redefine legal accountability, align AI-driven choices with stakeholder values, and maintain ethical safeguards. We advocate for frameworks that balance autonomy with accountability, ensuring all parties can harness agentic AI's potential while preserving trust, fairness, &amp; societal welfare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00289v3</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anirban Mukherjee, Hannah Hanwen Chang</dc:creator>
    </item>
    <item>
      <title>Europe's AI Imperative - A Pragmatic Blueprint for Global Tech Leadership</title>
      <link>https://arxiv.org/abs/2502.08781</link>
      <description>arXiv:2502.08781v2 Announce Type: replace 
Abstract: Europe is at a make-or-break moment in the global AI race, squeezed between the massive venture capital and tech giants in the US and China's scale-oriented, top-down drive. At this tipping point, where the convergence of AI with complementary and synergistic technologies, like quantum computing, biotech, VR/AR, 5G/6G, robotics, advanced materials, and high-performance computing, could upend geopolitical balances, Europe needs to rethink its AI-related strategy. On the heels of the AI Action Summit 2025 in Paris, we present a sharp, doable strategy that builds upon Europe's strengths and closes gaps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08781v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gjergji Kasneci, Urs Gasser, Thomas F. Hofmann, Gerhard Kramer, Gerhard M\"uller, Claudia Peus, Helmut Sch\"onenberger, Enkelejda Kasneci</dc:creator>
    </item>
    <item>
      <title>Sociodemographic Prompting is Not Yet an Effective Approach for Simulating Subjective Judgments with LLMs</title>
      <link>https://arxiv.org/abs/2311.09730</link>
      <description>arXiv:2311.09730v2 Announce Type: replace-cross 
Abstract: Human judgments are inherently subjective and are actively affected by personal traits such as gender and ethnicity. While Large Language Models (LLMs) are widely used to simulate human responses across diverse contexts, their ability to account for demographic differences in subjective tasks remains uncertain. In this study, leveraging the POPQUORN dataset, we evaluate nine popular LLMs on their ability to understand demographic differences in two subjective judgment tasks: politeness and offensiveness. We find that in zero-shot settings, most models' predictions for both tasks align more closely with labels from White participants than those from Asian or Black participants, while only a minor gender bias favoring women appears in the politeness task. Furthermore, sociodemographic prompting does not consistently improve and, in some cases, worsens LLMs' ability to perceive language from specific sub-populations. These findings highlight potential demographic biases in LLMs when performing subjective judgment tasks and underscore the limitations of sociodemographic prompting as a strategy to achieve pluralistic alignment. Code and data are available at: https://github.com/Jiaxin-Pei/LLM-as-Subjective-Judge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09730v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huaman Sun, Jiaxin Pei, Minje Choi, David Jurgens</dc:creator>
    </item>
    <item>
      <title>ShaRP: A Novel Feature Importance Framework for Ranking</title>
      <link>https://arxiv.org/abs/2401.16744</link>
      <description>arXiv:2401.16744v4 Announce Type: replace-cross 
Abstract: Algorithmic decisions in critical domains such as hiring, college admissions, and lending are often based on rankings. Given the impact of these decisions on individuals, organizations, and population groups, it is essential to understand them-to help individuals improve their ranking position, design better ranking procedures, and ensure legal compliance. In this paper, we argue that explainability methods for classification and regression, such as SHAP, are insufficient for ranking tasks, and present ShaRP-Shapley Values for Rankings and Preferences-a framework that explains the contributions of features to various aspects of a ranked outcome.
  ShaRP computes feature contributions for various ranking-specific profit functions, such as rank and top-k, and also includes a novel Shapley value-based method for explaining pairwise preference outcomes. We provide a flexible implementation of ShaRP, capable of efficiently and comprehensively explaining ranked and pairwise outcomes over tabular data, in score-based ranking and learning-to-rank tasks. Finally, to evaluate ShaRP and compare it with other explainability methods, we define ranking-specific explanation metrics and conduct an extensive experimental analysis, demonstrating the framework's flexibility and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16744v4</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Venetia Pliatsika, Joao Fonseca, Kateryna Akhynko, Ivan Shevchenko, Julia Stoyanovich</dc:creator>
    </item>
    <item>
      <title>Can AI writing be salvaged? Mitigating Idiosyncrasies and Improving Human-AI Alignment in the Writing Process through Edits</title>
      <link>https://arxiv.org/abs/2409.14509</link>
      <description>arXiv:2409.14509v4 Announce Type: replace-cross 
Abstract: LLM-based applications are helping people write, and LLM-generated text is making its way into social media, journalism, and our classrooms. However, the differences between LLM-generated and human-written text remain unclear. To explore this, we hired professional writers to edit paragraphs in several creative domains. We first found these writers agree on undesirable idiosyncrasies in LLM-generated text, formalizing it into a seven-category taxonomy (e.g. cliches, unnecessary exposition). Second, we curated the LAMP corpus: 1,057 LLM-generated paragraphs edited by professional writers according to our taxonomy. Analysis of LAMP reveals that none of the LLMs used in our study (GPT4o, Claude-3.5-Sonnet, Llama-3.1-70b) outperform each other in terms of writing quality, revealing common limitations across model families. Third, we explored automatic editing methods to improve LLM-generated text. A large-scale preference annotation confirms that although experts largely prefer text edited by other experts, automatic editing methods show promise in improving alignment between LLM-generated and human-written text.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14509v4</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuhin Chakrabarty, Philippe Laban, Chien-Sheng Wu</dc:creator>
    </item>
    <item>
      <title>Building Better: Avoiding Pitfalls in Developing Language Resources when Data is Scarce</title>
      <link>https://arxiv.org/abs/2410.12691</link>
      <description>arXiv:2410.12691v5 Announce Type: replace-cross 
Abstract: Language is a symbolic capital that affects people's lives in many ways (Bourdieu, 1977, 1991). It is a powerful tool that accounts for identities, cultures, traditions, and societies in general. Hence, data in a given language should be viewed as more than a collection of tokens. Good data collection and labeling practices are key to building more human-centered and socially aware technologies. While there has been a rising interest in mid- to low-resource languages within the NLP community, work in this space has to overcome unique challenges such as data scarcity and access to suitable annotators. In this paper, we collect feedback from those directly involved in and impacted by NLP artefacts for mid- to low-resource languages. We conduct a quantitative and qualitative analysis of the responses and highlight the main issues related to (1) data quality such as linguistic and cultural data suitability; and (2) the ethics of common annotation practices such as the misuse of online community services. Based on these findings, we make several recommendations for the creation of high-quality language artefacts that reflect the cultural milieu of its speakers, while simultaneously respecting the dignity and labor of data workers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12691v5</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nedjma Ousidhoum, Meriem Beloucif, Saif M. Mohammad</dc:creator>
    </item>
    <item>
      <title>Investigating social alignment via mirroring in a system of interacting language models</title>
      <link>https://arxiv.org/abs/2412.06834</link>
      <description>arXiv:2412.06834v2 Announce Type: replace-cross 
Abstract: Alignment is a social phenomenon wherein individuals share a common goal or perspective. Mirroring, or mimicking the behaviors and opinions of another individual, is one mechanism by which individuals can become aligned. Large scale investigations of the effect of mirroring on alignment have been limited due to the scalability of traditional experimental designs in sociology. In this paper, we introduce a simple computational framework that enables studying the effect of mirroring behavior on alignment in multi-agent systems. We simulate systems of interacting large language models in this framework and characterize overall system behavior and alignment with quantitative measures of agent dynamics. We find that system behavior is strongly influenced by the range of communication of each agent and that these effects are exacerbated by increased rates of mirroring. We discuss the observed simulated system behavior in the context of known human social dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06834v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harvey McGuinness, Tianyu Wang, Carey E. Priebe, Hayden Helm</dc:creator>
    </item>
    <item>
      <title>Break the Checkbox: Challenging Closed-Style Evaluations of Cultural Alignment in LLMs</title>
      <link>https://arxiv.org/abs/2502.08045</link>
      <description>arXiv:2502.08045v2 Announce Type: replace-cross 
Abstract: A large number of studies rely on closed-style multiple-choice surveys to evaluate cultural alignment in Large Language Models (LLMs). In this work, we challenge this constrained evaluation paradigm and explore more realistic, unconstrained approaches. Using the World Values Survey (WVS) and Hofstede Cultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger cultural alignment in less constrained settings, where responses are not forced. Additionally, we show that even minor changes, such as reordering survey choices, lead to inconsistent outputs, exposing the limitations of closed-style evaluations. Our findings advocate for more robust and flexible evaluation frameworks that focus on specific cultural proxies, encouraging more nuanced and accurate assessments of cultural alignment in LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08045v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mohsinul Kabir, Ajwad Abrar, Sophia Ananiadou</dc:creator>
    </item>
    <item>
      <title>Human-LLM Coevolution: Evidence from Academic Writing</title>
      <link>https://arxiv.org/abs/2502.09606</link>
      <description>arXiv:2502.09606v2 Announce Type: replace-cross 
Abstract: With a statistical analysis of arXiv paper abstracts, we report a marked drop in the frequency of several words previously identified as overused by ChatGPT, such as "delve", starting soon after they were pointed out in early 2024. The frequency of certain other words favored by ChatGPT, such as "significant", has instead kept increasing. These phenomena suggest that some authors of academic papers have adapted their use of large language models (LLMs), for example, by selecting outputs or applying modifications to the LLM-generated content. Such coevolution and cooperation of humans and LLMs thus introduce additional challenges to the detection of machine-generated text in real-world scenarios. Estimating the impact of LLMs on academic writing by examining word frequency remains feasible, and more attention should be paid to words that were already frequently employed, including those that have decreased in frequency due to LLMs' disfavor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09606v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingmeng Geng, Roberto Trotta</dc:creator>
    </item>
  </channel>
</rss>
