<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.GN updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.GN</link>
    <description>econ.GN updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.GN" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Sep 2025 04:01:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Economics and Game Theory of OSINT Frontline Photography: Risk, Attention, and the Collective Dilemma</title>
      <link>https://arxiv.org/abs/2509.10548</link>
      <description>arXiv:2509.10548v1 Announce Type: new 
Abstract: This paper develops an economic model of the Open Source Intelligence (OSINT) attention economy in contemporary armed conflict. We conceptualize attention (e.g. social media views, followers, likes) as revenue, and time and risk spent in analysis as costs. Using utility functions and simple game theoretic setups, we show how OSINT actors (amateurs, journalists, analysts, and state operatives) allocate effort to maximize net attention benefit. We incorporate strategic behaviors such as a first mover advantage (racing to publish) and prisoner's dilemma scenarios (to share information or hold it back). In empirical case studies, especially the Ukraine conflict actors like the UAV unit Madyar's Birds and volunteer channels like Kavkazfighter, illustrate how battlefront reporting translates into digital revenue (attention) at real cost. We draw on recent literature and data (e.g., public follower counts, viral posts) to examine trends such as OSINT virality. Finally, we discuss policy implications for balancing transparency with operational security, citing calls for verification ethics and attention sustaining narratives. Our analysis bridges conflict studies and economics, highlighting OSINT as both a public good and a competitive product in today's information war.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10548v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Teagan</dc:creator>
    </item>
    <item>
      <title>The impact on health system expenditure in Australia and OECD countries from accelerated NCD mortality decline through prevention or treatment strategies to achieve Sustainable Development Goal Target 3.4</title>
      <link>https://arxiv.org/abs/2509.10795</link>
      <description>arXiv:2509.10795v1 Announce Type: new 
Abstract: Background: It is unclear what the relative impacts of prevention or treatment of NCDs are on future health system expenditure. First, we estimated expenditure in Australia for prevention vs treatment pathways to achieve SDG target 3.4. Second, we applied the method to 34 other OECD countries.
  Methods: We used GBD data to estimate average annual percentage changes in disease incidence, remission, and CFRs from 1990-2021, and projected to 2030 to estimate business-as-usual (BAU) reductions in NCD mortality risk (40q30). For countries not on track to meet SDG3.4 under BAU, we modelled two intervention scenarios commencing in 2022 to achieve SDG3.4: (1) prevention via accelerated incidence reduction; (2) treatment via accelerated increases in remission and decreases in CFRs. Australian disease expenditure data were input into a PMSLT model to estimate expenditure changes from 2022 to 2040. Assuming similar expenditure patterns, the method was applied across OECD countries.
  Findings: In Australia, current trends project a 25% reduction in 40q30 by 2030, short of the 33.3% SDG3.4 target. Achieving this requires a 2.53 percentage point (pp) annual acceleration in incidence decline (prevention) or 1.56pp acceleration in CFR reduction and remission increase (treatment). Prevention reduces disease expenditure by 0.72%-3.17% by 2030 and 2040; treatment initially increase expenditure by 0.16%, before reducing it by 0.98%. A treatment scenario reducing only CFRs increased expenditure initially; increasing remission alone achieved savings similar to prevention. Only Sweden, Ireland, and South Korea were on track to meet SDG3.4. Other OECD countries showed similar expenditure impacts to Australia.
  Interpretation: Whether reducing NCD mortality saves money depends on pathway taken (prevention or treatment). Care is needed when linking NCD mortality reduction to health system savings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10795v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bibha Dhungel, Jingjing Yang, Tim Wilson, Samantha Grimshaw, Emily Bourke, Stephanie Khuu, Tony Blakely</dc:creator>
    </item>
    <item>
      <title>Bailouts and Redistribution</title>
      <link>https://arxiv.org/abs/2509.10933</link>
      <description>arXiv:2509.10933v1 Announce Type: new 
Abstract: What is the best macroprudential regulation when households differ in their exposure to profits from the financial sector? To answer the question, I study a real business cycle model with household heterogeneity and market incompleteness. In the model, shocks are amplified in states with high leverage, leading to lower investment. I consider the problem of a Ramsey planner who can finance transfers with a distortive tax on labor and levy taxes on the balance sheet components of experts. I show that the optimal tax on capital purchases is zero and the optimal policy relies mostly on a tax on deposit issuance. The latter redistributes between agents by affecting the equilibrium rate on deposits. The welfare gains from optimal policy are due to both redistribution and insurance and are larger the more unequal the initial distribution is. A simple tax rule that targets a level of leverage can achieve most of the welfare gains from optimal policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10933v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikayel Sukiasyan</dc:creator>
    </item>
    <item>
      <title>Out-of-sample gravity predictions and trade policy counterfactuals</title>
      <link>https://arxiv.org/abs/2509.11271</link>
      <description>arXiv:2509.11271v1 Announce Type: new 
Abstract: Gravity equations are often used to evaluate counterfactual trade policy scenarios, such as the effect of regional trade agreements on trade flows. In this paper, we argue that the suitability of gravity equations for this purpose crucially depends on their out-of-sample predictive power. We propose a methodology that compares different versions of the gravity equation, both among themselves and with machine learning-based forecast methods such as random forests and neural networks. We find that the 3-way gravity model is difficult to beat in terms of out-of-sample average predictive performance, further justifying its place as the predominant tool for applied trade policy analysis. However, when the goal is to predict individual bilateral trade flows, the 3-way model can be outperformed by an ensemble machine learning method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11271v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicolas Apfel, Holger Breinlich, Nick Green, Dennis Novy, J. M. C. Santos Silva, Tom Zylkin</dc:creator>
    </item>
    <item>
      <title>The Price of Disaster: Estimating the Impact of Hurricane Harvey on the Texas Construction Labor Market</title>
      <link>https://arxiv.org/abs/2509.11501</link>
      <description>arXiv:2509.11501v1 Announce Type: new 
Abstract: This paper estimates the effect of Hurricane Harvey on wages and employment in the construction labor industry across impacted counties in Texas. Based on data from the Quarterly Census of Employment and Wages (QCEW) for the period 2016-2019, I adopted a difference-in-differences event study approach by comparing results in 41 FEMA-designated disaster counties with a set of unaffected southern control counties. I find that Hurricane Harvey had a large and long-lasting impact on labor market outcomes in the construction industry. More precisely, average log wages in treated counties rose by around 7.2 percent compared to control counties two quarters after the hurricane and remained high for the next two years. Employment effects were more gradual, showing a statistically significant increase only after six quarters, in line with the lagged nature of large-scale reconstruction activities. These results imply that natural disasters can generate persistent labor demand shocks to local construction markets, with policy implications for disaster recovery planning and workforce mobilization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11501v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kartik Ganesh</dc:creator>
    </item>
    <item>
      <title>Human or Robot? Evidence from Last-Mile Delivery Service</title>
      <link>https://arxiv.org/abs/2509.11562</link>
      <description>arXiv:2509.11562v1 Announce Type: new 
Abstract: As platforms increasingly deploy robots alongside human labor in last-mile logistics, little is known about how contextual features like product attributes, environmental conditions, and psychological mechanisms shape consumer preference in real-world settings. To address this gap, this paper conducts an empirical study on consumer choice between human versus robot service, analyzing 241,517 package-level choices from Alibaba's last-mile delivery stations. We identify how product privacy sensitivity, product value, and environmental complexity affect consumer preference. Our findings reveal that consumers are significantly more likely to choose robot delivery for privacy-sensitive packages (11.49%) and high-value products (0.97% per 1% increase in value), but prefer human couriers under adverse weather conditions (1.63%). These patterns are robust to alternative specifications and controls. These results also underscore that delivery choices are shaped not only by functional considerations but also by psychological concerns, highlighting the need for context-aware service design that aligns strategies with consumer perceptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11562v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Baorui Li, Xincheng Ma, Brian Rongqing Han, Daizhong Tang, Lei Fu</dc:creator>
    </item>
    <item>
      <title>Geopolitical Barriers to Globalization</title>
      <link>https://arxiv.org/abs/2509.12084</link>
      <description>arXiv:2509.12084v1 Announce Type: new 
Abstract: This paper systematically estimates and quantifies how geopolitical alignment shapes global trade across three distinct eras: the Cold War, hyper-globalization, and contemporary fragmentation. We construct a novel measure of bilateral alignment using large language models to compile and analyze 833,485 political events spanning 193 countries from 1950 to 2024. Our analysis reveals that trade flows systematically track geopolitical alignment in both bilateral relationships and aggregate patterns. Using local projections within a gravity framework, we estimate that a one-standard-deviation improvement in geopolitical alignment increases bilateral trade by 20 percent over ten years. Integrating these elasticities into a quantitative general equilibrium model, we find that deteriorating geopolitical relations have reduced global trade by 7 percentage points between 1995 and 2020. Our findings provide empirical benchmarks for evaluating the costs of geopolitical fragmentation in an era of renewed great power competition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12084v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tianyu Fan, Mai Wo, Wei Xiang</dc:creator>
    </item>
    <item>
      <title>Bilevel subsidy-enabled mobility hub network design with perturbed utility coalitional choice-based assignment</title>
      <link>https://arxiv.org/abs/2509.10465</link>
      <description>arXiv:2509.10465v1 Announce Type: cross 
Abstract: Urban mobility is undergoing rapid transformation with the emergence of new services. Mobility hubs (MHs) have been proposed as physical-digital convergence points, offering a range of public and private mobility options in close proximity. By supporting Mobility-as-a-Service, these hubs can serve as focal points where travel decisions intersect with operator strategies. We develop a bilevel MH platform design model that treats MHs as control levers. The upper level (platform) maximizes revenue or flow by setting subsidies to incentivize last-mile operators; the lower level captures joint traveler-operator decisions with a link-based Perturbed Utility Route Choice (PURC) assignment, yielding a strictly convex quadratic program. We reformulate the bilevel problem to a single-level program via the KKT conditions of the lower level and solve it with a gap-penalty method and an iterative warm-start scheme that exploits the computationally cheap lower-level problem. Numerical experiments on a toy network and a Long Island Rail Road (LIRR) case (244 nodes, 469 links, 78 ODs) show that the method attains sub-1% optimality gaps in minutes. In the base LIRR case, the model allows policymakers to quantify the social surplus value of a MH, or the value of enabling subsidy or regulating the microtransit operator's pricing. Comparing link-based subsidies to hub-based subsidies, the latter is computationally more expensive but offers an easier mechanism for comparison and control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10465v1</guid>
      <category>math.OC</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hai Yang, Joseph Y. J. Chow</dc:creator>
    </item>
    <item>
      <title>Statistical Model Checking of NetLogo Models</title>
      <link>https://arxiv.org/abs/2509.10977</link>
      <description>arXiv:2509.10977v1 Announce Type: cross 
Abstract: Agent-based models (ABMs) are gaining increasing traction in several domains, due to their ability to represent complex systems that are not easily expressible with classical mathematical models. This expressivity and richness come at a cost: ABMs can typically be analyzed only through simulation, making their analysis challenging. Specifically, when studying the output of ABMs, the analyst is often confronted with practical questions such as: (i) how many independent replications should be run? (ii) how many initial time steps should be discarded as a warm-up? (iii) after the warm-up, how long should the model run? (iv) what are the right parameter values? Analysts usually resort to rules of thumb and experimentation, which lack statistical rigor. This is mainly because addressing these points takes time, and analysts prefer to spend their limited time improving the model. In this paper, we propose a methodology, drawing on the field of Statistical Model Checking, to automate the process and provide guarantees of statistical rigor for ABMs written in NetLogo, one of the most popular ABM platforms. We discuss MultiVeStA, a tool that dramatically reduces the time and human intervention needed to run statistically rigorous checks on ABM outputs, and introduce its integration with NetLogo. Using two ABMs from the NetLogo library, we showcase MultiVeStA's analysis capabilities for NetLogo ABMs, as well as a novel application to statistically rigorous calibration. Our tool-chain makes it immediate to perform statistical checks with NetLogo models, promoting more rigorous and reliable analyses of ABM outputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10977v1</guid>
      <category>cs.MA</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marco Pangallo, Daniele Giachini, Andrea Vandin</dc:creator>
    </item>
    <item>
      <title>Health Impacts of Public Pawnshops in Industrializing Tokyo</title>
      <link>https://arxiv.org/abs/2305.09352</link>
      <description>arXiv:2305.09352v3 Announce Type: replace 
Abstract: This study is the first to investigate whether pawnshops, financial institutions for low-income populations, have contributed to the decline in mortality in the early twentieth century. Using ward-level panel data from Tokyo City, this study revealed that the popularity of public pawnshops was associated with a 4% and 5% decrease in infant mortality and fetal death rates, respectively, during 1927-1935. The historical context implies that the potential channels of the relationships were improving nutrition and hygiene and covering childbirth costs. Moreover, a cost-effectiveness calculation highlighted that the establishment of public pawnshops was a cost-effective public investment for better public health. Contrarily, for-profit private pawnshops showed no significant association with health improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.09352v3</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tatsuki Inoue</dc:creator>
    </item>
    <item>
      <title>Strategic Investment to Mitigate Transition Risks</title>
      <link>https://arxiv.org/abs/2501.02383</link>
      <description>arXiv:2501.02383v2 Announce Type: replace 
Abstract: This paper investigates strategic investments needed to mitigate transition risks, particularly focusing on sectors significantly impacted by the shift to a low-carbon economy. It emphasizes the importance of tailored sector-specific strategies and the role of government interventions, such as carbon taxes and subsidies, in shaping corporate behavior. In providing a multi-period framework, this paper evaluates the economic and operational trade-offs companies face under four various decarbonization scenarios: immediate, quick, slow, and no transitions. The analysis provides practical insights for both policymakers and business leaders, demonstrating how regulatory frameworks and strategic investments can be aligned to manage transition risks while optimizing long-term sustainability effectively. The findings contribute to a deeper understanding of the economic impacts of regulatory policies and offer a comprehensive framework to navigate the complexities of transitioning to a low-carbon economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02383v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiayue Zhang, Tony S. Wirjanto, Lysa Porth, Ken Seng Tan</dc:creator>
    </item>
    <item>
      <title>Evaluating and Aligning Human Economic Risk Preferences in LLMs</title>
      <link>https://arxiv.org/abs/2503.06646</link>
      <description>arXiv:2503.06646v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly used in decision-making scenarios that involve risk assessment, yet their alignment with human economic rationality remains unclear. In this study, we investigate whether LLMs exhibit risk preferences consistent with human expectations across different personas. Specifically, we assess whether LLM-generated responses reflect appropriate levels of risk aversion or risk-seeking behavior based on individual's persona. Our results reveal that while LLMs make reasonable decisions in simplified, personalized risk contexts, their performance declines in more complex economic decision-making tasks. To address this, we propose an alignment method designed to enhance LLM adherence to persona-specific risk preferences. Our approach improves the economic rationality of LLMs in risk-related applications, offering a step toward more human-aligned AI decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06646v2</guid>
      <category>econ.GN</category>
      <category>cs.CL</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaxin Liu, Yixuan Tang, Yi Yang, Kar Yan Tam</dc:creator>
    </item>
    <item>
      <title>Automation, AI, and the Intergenerational Transmission of Knowledge</title>
      <link>https://arxiv.org/abs/2507.16078</link>
      <description>arXiv:2507.16078v5 Announce Type: replace 
Abstract: Recent advances in Artificial Intelligence (AI) have sparked expectations of unprecedented economic growth. Yet, by enabling senior workers to accomplish more tasks independently, AI may inadvertently reduce entry-level opportunities, raising concerns about how future generations will acquire essential expertise. This paper develops a model to examine how advanced automation affects the intergenerational transmission of tacit knowledge -- practical insights that resist codification and are critical for workplace success. The analysis shows that the competitive equilibrium features socially excessive automation of early-career tasks and reveals a critical trade-off: while such automation delivers immediate productivity gains, it can undermine long-term growth by hindering younger workers' acquisition of tacit skills. Back-of-the-envelope calculations suggest AI-driven entry-level automation could lower the long-run annual growth rate of U.S. per capita output by 0.05 to 0.35 percentage points, depending on its scale. The analysis further shows that AI co-pilots -- systems providing access to tacit-like expertise once obtained only through direct experience -- can partially offset these losses by assisting individuals who fail to develop adequate skills early in their careers. However, co-pilots are not always beneficial, as they may also weaken junior workers' incentives to engage in hands-on learning. These findings challenge the view that AI will automatically lead to higher economic growth, highlighting the need to safeguard -- or deliberately create -- entry-level opportunities to fully realize AI's potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16078v5</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enrique Ide</dc:creator>
    </item>
    <item>
      <title>Analysis and Study of Smart Growth</title>
      <link>https://arxiv.org/abs/2509.04529</link>
      <description>arXiv:2509.04529v2 Announce Type: replace 
Abstract: In the mid-1990s, the concept of smart growth emerged in the United States as a critical response to the phenomenon of suburban sprawl. To promote sustainable urban development, it is necessary to further investigate the principles and applications of smart growth. In this paper, we propose a Smart Growth Index (SGI) as a standard for measuring the degree of responsible urban development. Based on this index, we construct a comprehensive 3E evaluation model (covering economic prosperity, social equity, and environmental sustainability) to systematically assess the level of smart growth. For empirical analysis, we selected two medium-sized cities from different continents: Wuhu County, China, and Colima, Mexico. Using an improved entropy method, we evaluated the degree of smart growth in recent years and analyzed the contributions of various policies to sustainable urban development. Guided by the ten principles of smart growth, we further linked theoretical insights to practical challenges and formulated a development plan for both cities. To forecast long-term trends, we employed trend extrapolation based on historical data, enabling the prediction of SGI values for 2020, 2030, and 2050. The results indicate that Wuhu demonstrates greater potential for smart growth compared with Colima. We also simulated a scenario in which the population of both cities increased by 50 percent and re-evaluated the SGI. The analysis suggests that while rapid population growth tends to slow the pace of smart growth, it does not necessarily exert a negative impact on the overall trajectory of sustainable development. Finally, we conducted a study on the application of Transit-Oriented Development (TOD) theory in Wuhu County and proposed several policy recommendations aimed at enhancing the city's sustainable urban development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04529v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rongyan Chen, Ci Chen, Ziyang Yan</dc:creator>
    </item>
    <item>
      <title>Leveraging Large Language Models to Democratize Access to Costly Datasets for Academic Research</title>
      <link>https://arxiv.org/abs/2412.02065</link>
      <description>arXiv:2412.02065v3 Announce Type: replace-cross 
Abstract: Unequal access to costly datasets essential for empirical research has long hindered researchers from disadvantaged institutions, limiting their ability to contribute to their fields and advance their careers. Recent breakthroughs in Large Language Models (LLMs) have the potential to democratize data access by automating data collection from unstructured sources. We develop and evaluate a novel methodology using GPT-4o-mini within a Retrieval-Augmented Generation (RAG) framework to collect data from corporate disclosures. Our approach achieves human-level accuracy in collecting CEO pay ratios from approximately 10,000 proxy statements and Critical Audit Matters (CAMs) from more than 12,000 10-K filings, with LLM processing times of 9 and 40 minutes respectively, each at a cost under US $10. This stands in stark contrast to the hundreds of hours needed for manual collection or the thousands of dollars required for commercial database subscriptions. To foster a more inclusive research community by empowering researchers with limited resources to explore new avenues of inquiry, we share our methodology and the resulting datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02065v3</guid>
      <category>q-fin.GN</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Julian Junyan Wang, Victor Xiaoqi Wang</dc:creator>
    </item>
  </channel>
</rss>
