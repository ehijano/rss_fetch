<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Nov 2025 02:45:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Condition-Invariant fMRI Decoding of Speech Intelligibility with Deep State Space Model</title>
      <link>https://arxiv.org/abs/2511.01868</link>
      <description>arXiv:2511.01868v1 Announce Type: new 
Abstract: Clarifying the neural basis of speech intelligibility is critical for computational neuroscience and digital speech processing. Recent neuroimaging studies have shown that intelligibility modulates cortical activity beyond simple acoustics, primarily in the superior temporal and inferior frontal gyri. However, previous studies have been largely confined to clean speech, leaving it unclear whether the brain employs condition-invariant neural codes across diverse listening environments. To address this gap, we propose a novel architecture built upon a deep state space model for decoding intelligibility from fMRI signals, specifically tailored to their high-dimensional temporal structure. We present the first attempt to decode intelligibility across acoustically distinct conditions, showing our method significantly outperforms classical approaches. Furthermore, region-wise analysis highlights contributions from auditory, frontal, and parietal regions, and cross-condition transfer indicates the presence of condition-invariant neural codes, thereby advancing understanding of abstract linguistic representations in the brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01868v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>eess.SP</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ching-Chih Sung, Shuntaro Suzuki, Francis Pingfan Chien, Komei Sugiura, Yu Tsao</dc:creator>
    </item>
    <item>
      <title>CytoNet: A Foundation Model for the Human Cerebral Cortex</title>
      <link>https://arxiv.org/abs/2511.01870</link>
      <description>arXiv:2511.01870v1 Announce Type: new 
Abstract: To study how the human brain works, we need to explore the organization of the cerebral cortex and its detailed cellular architecture. We introduce CytoNet, a foundation model that encodes high-resolution microscopic image patches of the cerebral cortex into highly expressive feature representations, enabling comprehensive brain analyses. CytoNet employs self-supervised learning using spatial proximity as a powerful training signal, without requiring manual labelling. The resulting features are anatomically sound and biologically relevant. They encode general aspects of cortical architecture and unique brain-specific traits. We demonstrate top-tier performance in tasks such as cortical area classification, cortical layer segmentation, cell morphology estimation, and unsupervised brain region mapping. As a foundation model, CytoNet offers a consistent framework for studying cortical microarchitecture, supporting analyses of its relationship with other structural and functional brain features, and paving the way for diverse neuroscientific investigations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01870v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Schiffer, Zeynep Boztoprak, Jan-Oliver Kropp, Julia Th\"onni{\ss}en, Katia Berr, Hannah Spitzer, Katrin Amunts, Timo Dickscheid</dc:creator>
    </item>
    <item>
      <title>Neural dynamics of cognitive control: Current tensions and future promise</title>
      <link>https://arxiv.org/abs/2511.02063</link>
      <description>arXiv:2511.02063v1 Announce Type: new 
Abstract: Cognitive control is a suite of processes that helps individuals pursue goals despite resistance or uncertainty about what to do. Although cognitive control has been extensively studied as a dynamic feedback loop of perception, valuation, and action, it remains incompletely understood as a cohesive dynamic and distributed neural process. Here, we critically examine the history of and advances in the study of cognitive control, including how metaphors and cultural norms of power, morality, and rationality are intertwined with definitions of control, to consider holistically how different models explain which brain regions act as controllers. Controllers, the source of top-down signals, are typically localized in regions whose neural activations implement elementary component processes of control, including conflict monitoring and behavioral inhibition. Top-down signals from these regions guide the activation of other task-specific regions, biasing them towards task-specific activity patterns. A relatively new approach, network control theory, has roots in dynamical systems theory and systems engineering. This approach can mathematically show that controllers are regions with strongly nested and recurrent anatomical connectivity that efficiently propagate top-down signals, and precisely estimate the amount, location, and timing of signaling required to bias global activity to task-specific patterns. The theory converges with prior evidence, offers new mathematical tools and intuitions for understanding control loops across levels of analysis, and naturally produces graded predictions of control across brain regions and modules of psychological function that have been unconsidered or marginalized. We describe how prior approaches converge and diverge, noting directions for future integration to improve understanding of how the brain instantiates cognitive control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02063v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dale Zhou, Danielle Cosme, Yoona Kang, Ovidia Stanoi, David M. Lydon-Staley, Peter J. Mucha, Emily B. Falk, Kevin N. Ochsner, Dani S. Bassett</dc:creator>
    </item>
    <item>
      <title>Association-sensory spatiotemporal hierarchy and functional gradient-regularised recurrent neural network with implications for schizophrenia</title>
      <link>https://arxiv.org/abs/2511.02722</link>
      <description>arXiv:2511.02722v1 Announce Type: new 
Abstract: The human neocortex is functionally organised at its highest level along a continuous sensory-to-association (AS) hierarchy. This study characterises the AS hierarchy of patients with schizophrenia in a comparison with controls. Using a large fMRI dataset (N=355), we extracted individual AS gradients via spectral analysis of brain connectivity, quantified hierarchical specialisation by gradient spread, and related this spread with connectivity geometry. We found that schizophrenia compresses the AS hierarchy indicating reduced functional differentiation. By modelling neural timescale with the Ornstein-Uhlenbeck process, we observed that the most specialised, locally cohesive regions at the gradient extremes exhibit dynamics with a longer time constant, an effect that is attenuated in schizophrenia. To study computation, we used the gradients to regularise subject-specific recurrent neural networks (RNNs) trained on working memory tasks. Networks endowed with greater gradient spread learned more efficiently, plateaued at lower task loss, and maintained stronger alignment to the prescribed AS hierarchical geometry. Fixed point linearisation showed that high-range networks settled into more stable neural states during memory delay, evidenced by lower energy and smaller maximal Jacobian eigenvalues. This gradient-regularised RNN framework therefore links large-scale cortical architecture with fixed point stability, providing a mechanistic account of how gradient de-differentiation could destabilise neural computations in schizophrenia, convergently supported by empirical timescale flattening and model-based evidence of less stable fixed points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02722v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subati Abulikemu, Puria Radmard, Michail Mamalakis, John Suckling</dc:creator>
    </item>
    <item>
      <title>Microbes in the Moonlight: How the Gut Microbiota Influences Sleep</title>
      <link>https://arxiv.org/abs/2511.02766</link>
      <description>arXiv:2511.02766v2 Announce Type: new 
Abstract: The gut microbiota has emerged as a fundamental regulator of sleep physiology, influencing neural, endocrine, and immune pathways through the gut-microbiota-brain axis (GMBA). This bidirectional communication system modulates neurotransmitter production, circadian rhythms, and metabolic homeostasis, while disruptions in microbial composition have been linked to sleep disorders, neuroinflammation, and systemic immune dysfunction. Recent findings suggest that gut dysbiosis contributes to sleep disturbances by altering serotonin, GABA, and short-chain fatty acid (SCFA) metabolism, with implications for neurodegenerative diseases, metabolic syndromes, and mood disorders. Additionally, the gut microbiota interacts with the endocrine and immune systems, shaping inflammatory responses and stress adaptation mechanisms. This review explores the intricate connections between sleep and the gut microbiota, integrating emerging research on microbiota-targeted therapies, such as probiotics, fecal microbiota transplantation (FMT), and chrononutrition, as potential interventions to restore sleep homeostasis and improve health outcomes</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02766v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enso Onill Torres Alegre</dc:creator>
    </item>
    <item>
      <title>Mirror-Neuron Patterns in AI Alignment</title>
      <link>https://arxiv.org/abs/2511.01885</link>
      <description>arXiv:2511.01885v2 Announce Type: cross 
Abstract: As artificial intelligence (AI) advances toward superhuman capabilities, aligning these systems with human values becomes increasingly critical. Current alignment strategies rely largely on externally specified constraints that may prove insufficient against future super-intelligent AI capable of circumventing top-down controls.
  This research investigates whether artificial neural networks (ANNs) can develop patterns analogous to biological mirror neurons cells that activate both when performing and observing actions, and how such patterns might contribute to intrinsic alignment in AI. Mirror neurons play a crucial role in empathy, imitation, and social cognition in humans. The study therefore asks: (1) Can simple ANNs develop mirror-neuron patterns? and (2) How might these patterns contribute to ethical and cooperative decision-making in AI systems?
  Using a novel Frog and Toad game framework designed to promote cooperative behaviors, we identify conditions under which mirror-neuron patterns emerge, evaluate their influence on action circuits, introduce the Checkpoint Mirror Neuron Index (CMNI) to quantify activation strength and consistency, and propose a theoretical framework for further study.
  Our findings indicate that appropriately scaled model capacities and self/other coupling foster shared neural representations in ANNs similar to biological mirror neurons. These empathy-like circuits support cooperative behavior and suggest that intrinsic motivations modeled through mirror-neuron dynamics could complement existing alignment techniques by embedding empathy-like mechanisms directly within AI architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01885v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robyn Wyrick</dc:creator>
    </item>
    <item>
      <title>Structural Plasticity as Active Inference: A Biologically-Inspired Architecture for Homeostatic Control</title>
      <link>https://arxiv.org/abs/2511.02241</link>
      <description>arXiv:2511.02241v1 Announce Type: cross 
Abstract: Traditional neural networks, while powerful, rely on biologically implausible learning mechanisms such as global backpropagation. This paper introduces the Structurally Adaptive Predictive Inference Network (SAPIN), a novel computational model inspired by the principles of active inference and the morphological plasticity observed in biological neural cultures. SAPIN operates on a 2D grid where processing units, or cells, learn by minimizing local prediction errors. The model features two primary, concurrent learning mechanisms: a local, Hebbian-like synaptic plasticity rule based on the temporal difference between a cell's actual activation and its learned expectation, and a structural plasticity mechanism where cells physically migrate across the grid to optimize their information-receptive fields. This dual approach allows the network to learn both how to process information (synaptic weights) and also where to position its computational resources (network topology). We validated the SAPIN model on the classic Cart Pole reinforcement learning benchmark. Our results demonstrate that the architecture can successfully solve the CartPole task, achieving robust performance. The network's intrinsic drive to minimize prediction error and maintain homeostasis was sufficient to discover a stable balancing policy. We also found that while continual learning led to instability, locking the network's parameters after achieving success resulted in a stable policy. When evaluated for 100 episodes post-locking (repeated over 100 successful agents), the locked networks maintained an average 82% success rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02241v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brennen A. Hill</dc:creator>
    </item>
    <item>
      <title>Forecasting Future Anatomies: Longitudianl Brain Mri-to-Mri Prediction</title>
      <link>https://arxiv.org/abs/2511.02558</link>
      <description>arXiv:2511.02558v1 Announce Type: cross 
Abstract: Predicting future brain state from a baseline magnetic resonance image (MRI) is a central challenge in neuroimaging and has important implications for studying neurodegenerative diseases such as Alzheimer's disease (AD). Most existing approaches predict future cognitive scores or clinical outcomes, such as conversion from mild cognitive impairment to dementia. Instead, here we investigate longitudinal MRI image-to-image prediction that forecasts a participant's entire brain MRI several years into the future, intrinsically modeling complex, spatially distributed neurodegenerative patterns. We implement and evaluate five deep learning architectures (UNet, U2-Net, UNETR, Time-Embedding UNet, and ODE-UNet) on two longitudinal cohorts (ADNI and AIBL). Predicted follow-up MRIs are directly compared with the actual follow-up scans using metrics that capture global similarity and local differences. The best performing models achieve high-fidelity predictions, and all models generalize well to an independent external dataset, demonstrating robust cross-cohort performance. Our results indicate that deep learning can reliably predict participant-specific brain MRI at the voxel level, offering new opportunities for individualized prognosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02558v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ali Farki, Elaheh Moradi, Deepika Koundal, Jussi Tohka</dc:creator>
    </item>
    <item>
      <title>Modulation of metastable ensemble dynamics explains the inverted-U relationship between tone discriminability and arousal in auditory cortex</title>
      <link>https://arxiv.org/abs/2404.03902</link>
      <description>arXiv:2404.03902v3 Announce Type: replace 
Abstract: Past work has reported inverted-U relationships between arousal and auditory task performance, but the underlying neural network mechanisms remain unclear. To make progress, we recorded auditory cortex activity from behaving mice during passive tone presentation and simultaneously monitored pupil-indexed arousal. In these experiments, neural discriminability of tones was maximized at intermediate arousal, revealing a neural correlate of the inverted-U. We explained this arousal-dependent sound processing using a spiking model with clusters. In the model, stimulus discriminability peaked as the network transitioned from a multi-attractor phase exhibiting slow switching between metastable cluster activations (low arousal) to a single-attractor phase with uniform activity (high arousal). This transition also qualitatively captured arousal-induced reductions of neural variability observed in the data. Altogether, this study elucidates computational principles to explain interactions between arousal, neural discriminability, and variability, and suggests that transitions in the dynamical regime of cortical networks could underlie nonlinear modulations of sensory processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03902v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lia Papadopoulos, Suhyun Jo, Kevin Zumwalt, Michael Wehr, Santiago Jaramillo, David A. McCormick, Luca Mazzucato</dc:creator>
    </item>
    <item>
      <title>A Single-Equation Approach to Classifying Neuronal Operational Modes</title>
      <link>https://arxiv.org/abs/2510.01386</link>
      <description>arXiv:2510.01386v2 Announce Type: replace 
Abstract: The neural coding is yet to be discovered. The neuronal operational modes that arise with fixed inputs but with varying degrees of stimulation help to elucidate their coding properties. In neurons receiving {\it in vivo} stimulation, we show that two operation modes can be described with simplified models: the coincidence detection mode and the integration mode. Our derivations include a simplified polynomial model with non-linear coefficients ($\beta_i$) that capture the subthreshold dynamics of these modes of operation. The resulting model can explain these transitions with the sign and size of the smallest nonlinear coefficient of the polynomial alone. Defining neuronal operational modes provides insight into the processing and transmission of information through electrical currents. Requisite operational modes for proper neuronal functioning may explain disorders involving dysfunction of electrophysiological behavior, such as channelopathies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01386v2</guid>
      <category>q-bio.NC</category>
      <category>math.DS</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lindsey Knowles, Cesar Ceballos, Rodrigo Pena</dc:creator>
    </item>
    <item>
      <title>Desynchronization Index: a New Connectivity Approach for Exploring Epileptogenic Networks</title>
      <link>https://arxiv.org/abs/2408.16347</link>
      <description>arXiv:2408.16347v5 Announce Type: replace-cross 
Abstract: Objective: This work presents a new computational framework to assist neurophysiologists in Stereoelectroencephalography (SEEG) analysis, with the goal of improving the definition of the Epileptogenic Zone (EZ) in patients with drug-resistant epilepsy. Methods and procedures: We consider the Phase Transfer Entropy (PTE) to estimate the effective connectivity between SEEG channels, and design a novel algorithm, named the Desynchronization Index (DI), that identifies the EZ as the group of channels showing independent behavior with respect to the rest of the network during the seconds preceding the seizure propagation. Results: We test the proposed DI algorithm against the Epileptogenicity Index (EI) on a clinical dataset of 20 patients, considering the channels that were thermocoagulated at the end of SEEG monitoring as the detection target. Our results indicate that DI overcomes EI in terms of area under the ROC curve (AUC=0.85 vs. AUC=0.83), while combining the two algorithms as a unique tool leads to the best performance (AUC=0.87). Conclusion: The DI algorithm underscores connectivity dynamics that can hardly be identified with a pure visual analysis, increasing the accuracy in the EZ definition compared to traditional methods. Clinical impact: The integration of connectivity- and energy-based features can lead to the definition of a new biomarker of epileptogenic channels, reducing the burden required by the SEEG review in the case of extensive implants and improving the understanding of the dynamics behind the generation of seizures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16347v5</guid>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Mason, Lorenzo Ferri, Lidia Di Vito, Lara Alvisi, Luca Zanuttini, Matteo Martinoni, Roberto Mai, Francesco Cardinale, Paolo Tinuper, Roberto Michelucci, Elena Pasini, Francesca Bisulli</dc:creator>
    </item>
    <item>
      <title>Contrastive Consolidation of Top-Down Modulations Achieves Sparsely Supervised Continual Learning</title>
      <link>https://arxiv.org/abs/2505.14125</link>
      <description>arXiv:2505.14125v2 Announce Type: replace-cross 
Abstract: Biological brains learn continually from a stream of unlabeled data, while integrating specialized information from sparsely labeled examples without compromising their ability to generalize. Meanwhile, machine learning methods are susceptible to catastrophic forgetting in this natural learning setting, as supervised specialist fine-tuning degrades performance on the original task. We introduce task-modulated contrastive learning (TMCL), which takes inspiration from the biophysical machinery in the neocortex, using predictive coding principles to integrate top-down information continually and without supervision. We follow the idea that these principles build a view-invariant representation space, and that this can be implemented using a contrastive loss. Then, whenever labeled samples of a new class occur, new affine modulations are learned that improve separation of the new class from all others, without affecting feedforward weights. By co-opting the view-invariance learning mechanism, we then train feedforward weights to match the unmodulated representation of a data sample to its modulated counterparts. This introduces modulation invariance into the representation space, and, by also using past modulations, stabilizes it. Our experiments show improvements in both class-incremental and transfer learning over state-of-the-art unsupervised approaches, as well as over comparable supervised approaches, using as few as 1% of available labels. Taken together, our work suggests that top-down modulations play a crucial role in balancing stability and plasticity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14125v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Viet Anh Khoa Tran, Emre Neftci, Willem A. M. Wybo</dc:creator>
    </item>
    <item>
      <title>Emergence: from physics to biology, sociology, and computer science</title>
      <link>https://arxiv.org/abs/2508.08548</link>
      <description>arXiv:2508.08548v3 Announce Type: replace-cross 
Abstract: Many systems involve numerous interacting parts and the whole system can have properties that the individual parts do not. I take this novelty as the defining characteristic of an emergent property. Other characteristics associated with emergence discussed include universality, order, complexity, unpredictability, irreducibility, diversity, self-organisation, discontinuities, and singularities. Emergent phenomena are widespread across physics, biology, social sciences, and computing, and are central to major scientific and societal challenges. Understanding emergence involves considering the stratification of reality across different scales (energy, time, length, complexity), each with its distinct ontology and epistemology, leading to semi-autonomous scientific disciplines. A central challenge is bridging the gap between macroscopic emergent properties and microscopic component interactions. Identifying an intermediate mesoscopic scale where new, weakly interacting entities or modular structures emerge is key. Theoretical approaches, such as effective theories (describing phenomena at a specific scale) and toy models (simplified systems for analysis), are vital. The Ising model exemplifies how toy models can elucidate emergence characteristics. Emergence is central to condensed matter physics, chaotic systems, fluid dynamics, nuclear physics, quantum gravity, neural networks, protein folding, and social segregation. An emergent perspective should influence scientific strategy by shaping research questions, methodologies, priorities, and resource allocation. An elusive goal is the design and control of emergent properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08548v3</guid>
      <category>physics.hist-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>q-bio.NC</category>
      <category>quant-ph</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ross H. McKenzie</dc:creator>
    </item>
    <item>
      <title>Intermittent localization and fast spatial learning by non-Markov random walks with decaying memory</title>
      <link>https://arxiv.org/abs/2509.01806</link>
      <description>arXiv:2509.01806v3 Announce Type: replace-cross 
Abstract: Random walks on lattices with preferential relocation to previously visited sites provide a simple framework for modeling the displacements of animals and humans. When the lattice contains a few impurities or resource sites where the walker spends more time on average at each visit than on the other sites, the long range memory can suppress diffusion and induce by reinforcement a steady state localized around a resource. This phenomenon can be identified with a spatial learning process. Here we study theoretically and numerically how the decay of memory impacts learning in a model with one impurity. If memory decays as $1/\tau$ or slower, where $\tau$ is the time backward into the past, the localized solution is the same as with perfect, non-decaying memory and it is linearly stable. If forgetting is faster than $1/\tau$, for instance exponential, an unusual regime of intermittent localization is observed, where well localized periods of exponentially distributed duration are disrupted by possibly long intervals of diffusive motion. At the transition between the two regimes, for a kernel in $1/\tau$, the approach to the stable localized state is the fastest, opposite to the expected critical slowing down effect. Hence, forgetting can allow the walker to save a lot of memory without compromising learning and to achieve a faster learning process. These findings agree with biological evidence on the benefits of forgetting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01806v3</guid>
      <category>cond-mat.stat-mech</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paulina R. Mart\'in-Cornejo, Denis Boyer</dc:creator>
    </item>
    <item>
      <title>The Physical Basis of Prediction: World Model Formation in Neural Organoids via an LLM-Generated Curriculum</title>
      <link>https://arxiv.org/abs/2509.04633</link>
      <description>arXiv:2509.04633v3 Announce Type: replace-cross 
Abstract: The capacity of an embodied agent to understand, predict, and interact with its environment is fundamentally contingent on an internal world model. This paper introduces a novel framework for investigating the formation and adaptation of such world models within a biological substrate: human neural organoids. We present a curriculum of three scalable, closed-loop virtual environments designed to train these biological agents and probe the underlying synaptic mechanisms of learning, such as long-term potentiation (LTP) and long-term depression (LTD). We detail the design of three distinct task environments that demand progressively more sophisticated world models for successful decision-making: (1) a conditional avoidance task for learning static state-action contingencies, (2) a one-dimensional predator-prey scenario for goal-directed interaction, and (3) a replication of the classic Pong game for modeling dynamic, continuous-time systems. For each environment, we formalize the state and action spaces, the sensory encoding and motor decoding mechanisms, and the feedback protocols based on predictable (reward) and unpredictable (punishment) stimulation, which serve to drive model refinement. In a significant methodological advance, we propose a meta-learning approach where a Large Language Model automates the generative design and optimization of experimental protocols, thereby scaling the process of environment and curriculum design. Finally, we outline a multi-modal evaluation strategy that moves beyond task performance to directly measure the physical correlates of the learned world model by quantifying synaptic plasticity at electrophysiological, cellular, and molecular levels. This work bridges the gap between model-based reinforcement learning and computational neuroscience, offering a unique platform for studying embodiment, decision-making, and the physical basis of intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04633v3</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brennen Hill</dc:creator>
    </item>
  </channel>
</rss>
