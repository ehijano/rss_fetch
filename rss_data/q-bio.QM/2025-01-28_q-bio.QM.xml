<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Jan 2025 02:37:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Foundations of a Knee Joint Digital Twin from qMRI Biomarkers for Osteoarthritis and Knee Replacement</title>
      <link>https://arxiv.org/abs/2501.15396</link>
      <description>arXiv:2501.15396v1 Announce Type: new 
Abstract: This study forms the basis of a digital twin system of the knee joint, using advanced quantitative MRI (qMRI) and machine learning to advance precision health in osteoarthritis (OA) management and knee replacement (KR) prediction. We combined deep learning-based segmentation of knee joint structures with dimensionality reduction to create an embedded feature space of imaging biomarkers. Through cross-sectional cohort analysis and statistical modeling, we identified specific biomarkers, including variations in cartilage thickness and medial meniscus shape, that are significantly associated with OA incidence and KR outcomes. Integrating these findings into a comprehensive framework represents a considerable step toward personalized knee-joint digital twins, which could enhance therapeutic strategies and inform clinical decision-making in rheumatological care. This versatile and reliable infrastructure has the potential to be extended to broader clinical applications in precision health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15396v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabrielle Hoyer, Kenneth T Gao, Felix G Gassert, Johanna Luitjens, Fei Jiang, Sharmila Majumdar, Valentina Pedoia</dc:creator>
    </item>
    <item>
      <title>Diffusion Generative Modeling for Spatially Resolved Gene Expression Inference from Histology Images</title>
      <link>https://arxiv.org/abs/2501.15598</link>
      <description>arXiv:2501.15598v1 Announce Type: new 
Abstract: Spatial Transcriptomics (ST) allows a high-resolution measurement of RNA sequence abundance by systematically connecting cell morphology depicted in Hematoxylin and Eosin (H&amp;E) stained histology images to spatially resolved gene expressions. ST is a time-consuming, expensive yet powerful experimental technique that provides new opportunities to understand cancer mechanisms at a fine-grained molecular level, which is critical for uncovering new approaches for disease diagnosis and treatments. Here, we present $\textbf{Stem}$ ($\textbf{S}$pa$\textbf{T}$ially resolved gene $\textbf{E}$xpression inference with diffusion $\textbf{M}$odel), a novel computational tool that leverages a conditional diffusion generative model to enable in silico gene expression inference from H&amp;E stained images. Through better capturing the inherent stochasticity and heterogeneity in ST data, $\textbf{Stem}$ achieves state-of-the-art performance on spatial gene expression prediction and generates biologically meaningful gene profiles for new H&amp;E stained images at test time. We evaluate the proposed algorithm on datasets with various tissue sources and sequencing platforms, where it demonstrates clear improvement over existing approaches. $\textbf{Stem}$ generates high-fidelity gene expression predictions that share similar gene variation levels as ground truth data, suggesting that our method preserves the underlying biological heterogeneity. Our proposed pipeline opens up the possibility of analyzing existing, easily accessible H&amp;E stained histology images from a genomics point of view without physically performing gene expression profiling and empowers potential biological discovery from H&amp;E stained histology images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15598v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sichen Zhu, Yuchen Zhu, Molei Tao, Peng Qiu</dc:creator>
    </item>
    <item>
      <title>HECLIP: Histology-Enhanced Contrastive Learning for Imputation of Transcriptomics Profiles</title>
      <link>https://arxiv.org/abs/2501.14948</link>
      <description>arXiv:2501.14948v1 Announce Type: cross 
Abstract: Histopathology, particularly hematoxylin and eosin (H\&amp;E) staining, plays a critical role in diagnosing and characterizing pathological conditions by highlighting tissue morphology. However, H\&amp;E-stained images inherently lack molecular information, requiring costly and resource-intensive methods like spatial transcriptomics to map gene expression with spatial resolution. To address these challenges, we introduce HECLIP (Histology-Enhanced Contrastive Learning for Imputation of Profiles), an innovative deep learning framework that bridges the gap between histological imaging and molecular profiling. HECLIP is specifically designed to infer gene expression profiles directly from H\&amp;E-stained images, eliminating the need for expensive spatial transcriptomics assays. HECLIP leverages an advanced image-centric contrastive loss function to optimize image representation learning, ensuring that critical morphological patterns in histology images are effectively captured and translated into accurate gene expression profiles. This design enhances the predictive power of the image modality while minimizing reliance on gene expression data. Through extensive benchmarking on publicly available datasets, HECLIP demonstrates superior performance compared to existing approaches, delivering robust and biologically meaningful predictions. Detailed ablation studies further underscore its effectiveness in extracting molecular insights from histology images. Additionally, HECLIP's scalable and cost-efficient approach positions it as a transformative tool for both research and clinical applications, driving advancements in precision medicine. The source code for HECLIP is openly available at https://github.com/QSong-github/HECLIP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14948v1</guid>
      <category>cs.CE</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qing Wang, Wen-jie Chen, Bo Li, Jing Su, Guangyu Wang, Qianqian Song</dc:creator>
    </item>
    <item>
      <title>Controllable Protein Sequence Generation with LLM Preference Optimization</title>
      <link>https://arxiv.org/abs/2501.15007</link>
      <description>arXiv:2501.15007v1 Announce Type: cross 
Abstract: Designing proteins with specific attributes offers an important solution to address biomedical challenges. Pre-trained protein large language models (LLMs) have shown promising results on protein sequence generation. However, to control sequence generation for specific attributes, existing work still exhibits poor functionality and structural stability. In this paper, we propose a novel controllable protein design method called CtrlProt. We finetune a protein LLM with a new multi-listwise preference optimization strategy to improve generation quality and support multi-attribute controllable generation. Experiments demonstrate that CtrlProt can meet functionality and structural stability requirements effectively, achieving state-of-the-art performance in both single-attribute and multi-attribute protein sequence generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15007v1</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiangyu Liu, Yi Liu, Silei Chen, Wei Hu</dc:creator>
    </item>
    <item>
      <title>Dynamic Estimation of Tea Flowering Based on an Improved YOLOv5 and ANN Model</title>
      <link>https://arxiv.org/abs/2501.15262</link>
      <description>arXiv:2501.15262v1 Announce Type: cross 
Abstract: Tea flowers play a crucial role in taxonomic research and hybrid breeding for the tea plant. Tea flowering consumes the plant's nutrients, and flower thinning can regulate carbon-nitrogen metabolism, enhancing the yield and quality of young shoots. As traditional methods of observing tea flower traits are labor-intensive and inaccurate, we propose an effective framework for tea flowering quantifying. In this study, a highly representative and diverse dataset was constructed by collecting flower images from 29 tea accessions. Based on this dataset, the TflosYOLO model was built on the YOLOv5 architecture and enhanced with the Squeeze-and-Excitation (SE) network, which is the first model to offer a viable solution for detecting tea flowers and predicting flower quantities. The TflosYOLO model achieved an mAP50 of 0.874, outperforming YOLOv5, YOLOv7 and YOLOv8. Furthermore, this model was tested on 34 datasets encompassing 26 tea accessions, five flowering stages, various lighting conditions, and pruned/unpruned plants, demonstrating high generalization and robustness. The correlation coefficient ($ R^2 $) between the predicted and actual flower counts was 0.974. Additionally, the TFSC (Tea Flowering Stage Classification) model - a novel Artificial Neural Network (ANN) was designed for automatic classification of the flowering stages. TFSC achieved an accuracy of 0.899. Dynamic analysis of flowering across 29 tea accessions in 2023 and 2024 was conducted, revealed significant variability in flower quantity and dynamics, with genetically similar accessions showing more consistent flowering patterns. This framework provides a solution for quantifying tea flowering, and can serve as a reference for precision horticulture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15262v1</guid>
      <category>cs.CV</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianxi Mi, Pengcheng Yuan, Chunlei Ma, Jiedan Chen, Mingzhe Yao</dc:creator>
    </item>
    <item>
      <title>PhoTorch: A robust and generalized biochemical photosynthesis model fitting package based on PyTorch</title>
      <link>https://arxiv.org/abs/2501.15484</link>
      <description>arXiv:2501.15484v1 Announce Type: cross 
Abstract: Advancements in artificial intelligence (AI) have greatly benefited plant phenotyping and predictive modeling. However, unrealized opportunities exist in leveraging AI advancements in model parameter optimization for parameter fitting in complex biophysical models. This work developed novel software, PhoTorch, for fitting parameters of the Farquhar, von Caemmerer, and Berry (FvCB) biochemical photosynthesis model based the parameter optimization components of the popular AI framework PyTorch. The primary novelty of the software lies in its computational efficiency, robustness of parameter estimation, and flexibility in handling different types of response curves and sub-model functional forms. PhoTorch can fit both steady-state and non-steady-state gas exchange data with high efficiency and accuracy. Its flexibility allows for optional fitting of temperature and light response parameters, and can simultaneously fit light response curves and standard A/Ci curves. These features are not available within presently available A/Ci curve fitting packages. Results illustrated the robustness and efficiency of PhoTorch in fitting A/Ci curves with high variability and some level of artifacts and noise. PhoTorch is more than four times faster than benchmark software, which may be relevant when processing many non-steady-state A/Ci curves with hundreds of data points per curve. PhoTorch provides researchers from various fields with a reliable and efficient tool for analyzing photosynthetic data. The Python package is openly accessible from the repository: https://github.com/GEMINI-Breeding/photorch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15484v1</guid>
      <category>cs.CE</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tong Lei, Kyle T. Rizzo, Brian N. Bailey</dc:creator>
    </item>
    <item>
      <title>AI in Oncology: Transforming Cancer Detection through Machine Learning and Deep Learning Applications</title>
      <link>https://arxiv.org/abs/2501.15489</link>
      <description>arXiv:2501.15489v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) has potential to revolutionize the field of oncology by enhancing the precision of cancer diagnosis, optimizing treatment strategies, and personalizing therapies for a variety of cancers. This review examines the limitations of conventional diagnostic techniques and explores the transformative role of AI in diagnosing and treating cancers such as lung, breast, colorectal, liver, stomach, esophageal, cervical, thyroid, prostate, and skin cancers. The primary objective of this paper is to highlight the significant advancements that AI algorithms have brought to oncology within the medical industry. By enabling early cancer detection, improving diagnostic accuracy, and facilitating targeted treatment delivery, AI contributes to substantial improvements in patient outcomes. The integration of AI in medical imaging, genomic analysis, and pathology enhances diagnostic precision and introduces a novel, less invasive approach to cancer screening. This not only boosts the effectiveness of medical facilities but also reduces operational costs. The study delves into the application of AI in radiomics for detailed cancer characterization, predictive analytics for identifying associated risks, and the development of algorithm-driven robots for immediate diagnosis. Furthermore, it investigates the impact of AI on addressing healthcare challenges, particularly in underserved and remote regions. The overarching goal of this platform is to support the development of expert recommendations and to provide universal, efficient diagnostic procedures. By reviewing existing research and clinical studies, this paper underscores the pivotal role of AI in improving the overall cancer care system. It emphasizes how AI-enabled systems can enhance clinical decision-making and expand treatment options, thereby underscoring the importance of AI in advancing precision oncology</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15489v1</guid>
      <category>cs.AI</category>
      <category>eess.IV</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Aftab, Faisal Mehmood, Chengjuan Zhang, Alishba Nadeem, Zigang Dong, Yanan Jiang, Kangdongs Liu</dc:creator>
    </item>
    <item>
      <title>Estimating Committor Functions via Deep Adaptive Sampling on Rare Transition Paths</title>
      <link>https://arxiv.org/abs/2501.15522</link>
      <description>arXiv:2501.15522v1 Announce Type: cross 
Abstract: The committor functions are central to investigating rare but important events in molecular simulations. It is known that computing the committor function suffers from the curse of dimensionality. Recently, using neural networks to estimate the committor function has gained attention due to its potential for high-dimensional problems. Training neural networks to approximate the committor function needs to sample transition data from straightforward simulations of rare events, which is very inefficient. The scarcity of transition data makes it challenging to approximate the committor function. To address this problem, we propose an efficient framework to generate data points in the transition state region that helps train neural networks to approximate the committor function. We design a Deep Adaptive Sampling method for TRansition paths (DASTR), where deep generative models are employed to generate samples to capture the information of transitions effectively. In particular, we treat a non-negative function in the integrand of the loss functional as an unnormalized probability density function and approximate it with the deep generative model. The new samples from the deep generative model are located in the transition state region and fewer samples are located in the other region. This distribution provides effective samples for approximating the committor function and significantly improves the accuracy. We demonstrate the effectiveness of the proposed method through both simulations and realistic examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15522v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yueyang Wang, Kejun Tang, Xili Wang, Xiaoliang Wan, Weiqing Ren, Chao Yang</dc:creator>
    </item>
    <item>
      <title>Integrating Probabilistic Trees and Causal Networks for Clinical and Epidemiological Data</title>
      <link>https://arxiv.org/abs/2501.15973</link>
      <description>arXiv:2501.15973v1 Announce Type: cross 
Abstract: Healthcare decision-making requires not only accurate predictions but also insights into how factors influence patient outcomes. While traditional Machine Learning (ML) models excel at predicting outcomes, such as identifying high risk patients, they are limited in addressing what-if questions about interventions. This study introduces the Probabilistic Causal Fusion (PCF) framework, which integrates Causal Bayesian Networks (CBNs) and Probability Trees (PTrees) to extend beyond predictions. PCF leverages causal relationships from CBNs to structure PTrees, enabling both the quantification of factor impacts and simulation of hypothetical interventions. PCF was validated on three real-world healthcare datasets i.e. MIMIC-IV, Framingham Heart Study, and Diabetes, chosen for their clinically diverse variables. It demonstrated predictive performance comparable to traditional ML models while providing additional causal reasoning capabilities. To enhance interpretability, PCF incorporates sensitivity analysis and SHapley Additive exPlanations (SHAP). Sensitivity analysis quantifies the influence of causal parameters on outcomes such as Length of Stay (LOS), Coronary Heart Disease (CHD), and Diabetes, while SHAP highlights the importance of individual features in predictive modeling. By combining causal reasoning with predictive modeling, PCF bridges the gap between clinical intuition and data-driven insights. Its ability to uncover relationships between modifiable factors and simulate hypothetical scenarios provides clinicians with a clearer understanding of causal pathways. This approach supports more informed, evidence-based decision-making, offering a robust framework for addressing complex questions in diverse healthcare settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15973v1</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sheresh Zahoor, Pietro Li\`o, Ga\"el Dias, Mohammed Hasanuzzaman</dc:creator>
    </item>
    <item>
      <title>Translating and evaluating single-cell Boolean network interventions in the multiscale setting</title>
      <link>https://arxiv.org/abs/2501.16052</link>
      <description>arXiv:2501.16052v1 Announce Type: cross 
Abstract: Intracellular networks process cellular-level information and control cell fate. They can be computationally modeled using Boolean networks, which are implicit-time causal models of discrete binary events. These networks can be embedded in computational agents to drive cellular behavior. To explore this integration, we construct a set of candidate interventions that induce apoptosis in a cell-survival network of a rare leukemia using exhaustive search simulation, stable motif control, and an individual-based mean field approach (IBMFA). Due to inherent algorithmic limitations, these interventions are most suitable for cell-level determinations, not the more realistic multicellular setting. To address these limitations, we treat the target control solutions as putative targets for therapeutic interventions and develop a pipeline to translate them to continuous-time multicellular, agent-based models. We set the discrete-to-continuous transitions between the Boolean network and multicellular model via thresholding and produce simple computational simulations designed to emulate situations in experimental and translational biology. These include a series of simulations: constant substrate gradients, global substrate pulses, and time-varying boundary conditions. We find that interventions that perform equally well in the implicit-time single-cell setting are separable in the multiscale setting in ability to impact population growth and spatial distribution. Further analysis shows that the population and spatial distribution differences arise from differences in internal dynamics (stable motif controls versus target controls) and network distance between the intervention and output nodes. This proof of concept work demonstrates the importance of accounting for internal dynamics in multicellular simulations as well as impacts on understanding of Boolean network control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16052v1</guid>
      <category>q-bio.MN</category>
      <category>q-bio.CB</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Metzcar, Katie Pletz, Heber L. Rocha, Jordan C Rozum</dc:creator>
    </item>
    <item>
      <title>Knowledge-Infused Prompting: Assessing and Advancing Clinical Text Data Generation with Large Language Models</title>
      <link>https://arxiv.org/abs/2311.00287</link>
      <description>arXiv:2311.00287v2 Announce Type: replace-cross 
Abstract: Clinical natural language processing requires methods that can address domain-specific challenges, such as complex medical terminology and clinical contexts. Recently, large language models (LLMs) have shown promise in this domain. Yet, their direct deployment can lead to privacy issues and are constrained by resources. To address this challenge, we delve into synthetic clinical text generation using LLMs for clinical NLP tasks. We propose an innovative, resource-efficient approach, ClinGen, which infuses knowledge into the process. Our model involves clinical knowledge extraction and context-informed LLM prompting. Both clinical topics and writing styles are drawn from external domain-specific knowledge graphs and LLMs to guide data generation. Our extensive empirical study across 7 clinical NLP tasks and 16 datasets reveals that ClinGen consistently enhances performance across various tasks, effectively aligning the distribution of real datasets and significantly enriching the diversity of generated training instances. Our code is available at \url{https://github.com/ritaranx/ClinGen}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00287v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ACL 2024</arxiv:journal_reference>
      <dc:creator>Ran Xu, Hejie Cui, Yue Yu, Xuan Kan, Wenqi Shi, Yuchen Zhuang, Wei Jin, Joyce Ho, Carl Yang</dc:creator>
    </item>
    <item>
      <title>Dimensions underlying the representational alignment of deep neural networks with humans</title>
      <link>https://arxiv.org/abs/2406.19087</link>
      <description>arXiv:2406.19087v2 Announce Type: replace-cross 
Abstract: Determining the similarities and differences between humans and artificial intelligence (AI) is an important goal both in computational cognitive neuroscience and machine learning, promising a deeper understanding of human cognition and safer, more reliable AI systems. Much previous work comparing representations in humans and AI has relied on global, scalar measures to quantify their alignment. However, without explicit hypotheses, these measures only inform us about the degree of alignment, not the factors that determine it. To address this challenge, we propose a generic framework to compare human and AI representations, based on identifying latent representational dimensions underlying the same behavior in both domains. Applying this framework to humans and a deep neural network (DNN) model of natural images revealed a low-dimensional DNN embedding of both visual and semantic dimensions. In contrast to humans, DNNs exhibited a clear dominance of visual over semantic properties, indicating divergent strategies for representing images. While in-silico experiments showed seemingly consistent interpretability of DNN dimensions, a direct comparison between human and DNN representations revealed substantial differences in how they process images. By making representations directly comparable, our results reveal important challenges for representational alignment and offer a means for improving their comparability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19087v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Florian P. Mahner, Lukas Muttenthaler, Umut G\"u\c{c}l\"u, Martin N. Hebart</dc:creator>
    </item>
    <item>
      <title>Enhancing Glucose Level Prediction of ICU Patients through Hierarchical Modeling of Irregular Time-Series</title>
      <link>https://arxiv.org/abs/2411.01418</link>
      <description>arXiv:2411.01418v2 Announce Type: replace-cross 
Abstract: Accurately predicting blood glucose (BG) levels of ICU patients is critical, as both hypoglycemia (BG &lt; 70 mg/dL) and hyperglycemia (BG &gt; 180 mg/dL) are associated with increased morbidity and mortality. This study presents a proof-of-concept machine learning framework, the Multi-source Irregular Time-Series Transformer (MITST), designed to predict blood glucose (BG) levels in ICU patients. Unlike existing approaches that rely on manual feature engineering or are limited to a small number of Electronic Health Record (EHR) data sources, MITST demonstrates the feasibility of integrating diverse clinical data (e.g., lab results, medications, vital signs) and handling irregular time-series data without predefined aggregation. MITST employs a hierarchical architecture of Transformers, comprising feature-level, timestamp-level, and source-level components, to capture fine-grained temporal dynamics and enable learning-based data integration. This eliminates the need for traditional aggregation and manual feature engineering. In a large-scale evaluation using the eICU database (200,859 ICU stays across 208 hospitals), MITST achieves an average improvement of 1.7% (p &lt; 0.001) in AUROC and 1.8% (p &lt; 0.001) in AUPRC over a state-of-the-art baseline. For hypoglycemia, MITST achieves an AUROC of 0.915 and an AUPRC of 0.247, both significantly outperforming the baseline. The flexible architecture of MITST allows seamless integration of new data sources without retraining the entire model, enhancing its adaptability for clinical decision support. While this study focuses on predicting BG levels, MITST can easily be extended to other critical event prediction tasks in ICU settings, offering a robust solution for analyzing complex, multi-source, irregular time-series data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01418v2</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hadi Mehdizavareh, Arijit Khan, Simon Lebech Cichosz</dc:creator>
    </item>
  </channel>
</rss>
