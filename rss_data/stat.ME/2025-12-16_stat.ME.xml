<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Dec 2025 02:40:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A fine-grained look at causal effects in causal spaces</title>
      <link>https://arxiv.org/abs/2512.11919</link>
      <description>arXiv:2512.11919v2 Announce Type: new 
Abstract: The notion of causal effect is fundamental across many scientific disciplines. Traditionally, quantitative researchers have studied causal effects at the level of variables; for example, how a certain drug dose (W) causally affects a patient's blood pressure (Y). However, in many modern data domains, the raw variables-such as pixels in an image or tokens in a language model-do not have the semantic structure needed to formulate meaningful causal questions. In this paper, we offer a more fine-grained perspective by studying causal effects at the level of events, drawing inspiration from probability theory, where core notions such as independence are first given for events and sigma-algebras, before random variables enter the picture. Within the measure-theoretic framework of causal spaces, a recently introduced axiomatisation of causality, we first introduce several binary definitions that determine whether a causal effect is present, as well as proving some properties of them linking causal effect to (in)dependence under an intervention measure. Further, we provide quantifying measures that capture the strength and nature of causal effects on events, and show that we can recover the common measures of treatment effect as special cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11919v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhyung Park, Yuqing Zhou</dc:creator>
    </item>
    <item>
      <title>Debiased Inference for High-Dimensional Regression Models Based on Profile M-Estimation</title>
      <link>https://arxiv.org/abs/2512.12003</link>
      <description>arXiv:2512.12003v1 Announce Type: new 
Abstract: Debiased inference for high-dimensional regression models has received substantial recent attention to ensure regularized estimators have valid inference. All existing methods focus on achieving Neyman orthogonality through explicitly constructing projections onto the space of nuisance parameters, which is infeasible when an explicit form of the projection is unavailable. We introduce a general debiasing framework, Debiased Profile M-Estimation (DPME), which applies to a broad class of models and does not require model-specific Neyman orthogonalization or projection derivations as in existing methods. Our approach begins by obtaining an initial estimator of the parameters by optimizing a penalized objective function. To correct for the bias introduced by penalization, we construct a one-step estimator using the Newton-Raphson update, applied to the gradient of a profile function defined as the optimal objective function with the parameter of interest held fixed. We use numerical differentiation without requiring the explicit calculation of the gradients. The resulting DPME estimator is shown to be asymptotically linear and normally distributed. Through extensive simulations, we demonstrate that the proposed method achieves better coverage rates than existing alternatives, with largely reduced computational cost. Finally, we illustrate the utility of our method with an application to estimating a treatment rule for multiple myeloma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12003v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Wang, Yuhao Deng, Yu Gu, Yuanjia Wang, Donglin Zeng</dc:creator>
    </item>
    <item>
      <title>Proximal Causal Inference for Modified Treatment Policies</title>
      <link>https://arxiv.org/abs/2512.12038</link>
      <description>arXiv:2512.12038v1 Announce Type: new 
Abstract: The proximal causal inference framework enables the identification and estimation of causal effects in the presence of unmeasured confounding by leveraging two disjoint sets of observed strong proxies: negative control treatments and negative control outcomes. In the point exposure setting, this framework has primarily been applied to estimands comparing counterfactual outcomes under a static fixed intervention or, possibly randomized, regime that depends on baseline covariates. For continuous exposures, alternative hypothetical scenarios can enrich our understanding of causal effects, such as those where each individual receives their observed treatment dose modified in a pre-specified manner - commonly referred to as modified treatment regimes. In this work, we extend the proximal causal inference framework to identify and estimate the mean outcome under a modified treatment regime, addressing this gap in the literature. We propose a flexible strategy that does not rely on the assumption that all confounders have been measured - unlike existing estimators - and leverages modern debiased machine learning techniques using non-parametric estimators of nuisance functions to avoid restrictive parametric assumptions. Our methodology was motivated by immunobridging studies of COVID-19 vaccines aimed at identifying correlates of protection, where the individual's underlying immune capacity is an important unmeasured confounder. We demonstrate its applicability using data from such a study and evaluate its finite-sample performance through simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12038v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio Olivas-Martinez, Peter B. Gilbert, Andrea Rotnitzky</dc:creator>
    </item>
    <item>
      <title>Sparse Bayesian Partially Identified Models for Sequence Count Data</title>
      <link>https://arxiv.org/abs/2512.12040</link>
      <description>arXiv:2512.12040v1 Announce Type: new 
Abstract: In genomics, differential abundance and expression analyses are complicated by the compositional nature of sequence count data, which reflect only relative-not absolute-abundances or expression levels. Many existing methods attempt to address this limitation through data normalizations, but we have shown that such approaches imply strong, often biologically implausible assumptions about total microbial load or total gene expression. Even modest violations of these assumptions can inflate Type I and Type II error rates to over 70%. Sparse estimators have been proposed as an alternative, leveraging the assumption that only a small subset of taxa (or genes) change between conditions. However, we show that current sparse methods suffer from similar pathologies because they treat sparsity assumptions as fixed and ignore the uncertainty inherent in these assumptions. We introduce a sparse Bayesian Partially Identified Model (PIM) that addresses this limitation by explicitly modeling uncertainty in sparsity assumptions. Our method extends the Scale-Reliant Inference (SRI) framework to the sparse setting, providing a principled approach to differential analysis under scale uncertainty. We establish theoretical consistency of the proposed estimator and, through extensive simulations and real data analyses, demonstrate substantial reductions in both Type I and Type II errors compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12040v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Won Gu, Francesca Chiaromonte, Justin D. Silverman</dc:creator>
    </item>
    <item>
      <title>Meta-analysis of diagnostic test accuracy with multiple disease stages: combining stage-specific and merged-stage data</title>
      <link>https://arxiv.org/abs/2512.12065</link>
      <description>arXiv:2512.12065v1 Announce Type: new 
Abstract: For many conditions, it is of clinical importance to know not just the ability of a test to distinguish between those with and without the disease, but also the sensitivity to detect disease at different stages: in particular, the test's ability to detect disease at a stage most amenable to treatment. In a systematic review of test accuracy, pooled stage-specific estimates can be produced using subgroup analysis or meta-regression. However, this requires stage-specific data from each study, which is often not reported. Studies may however report test sensitivity for merged stage categories (e.g. stages I-II) or merged across all stages, together with information on the proportion of patients with disease at each stage. We demonstrate how to incorporate studies reporting merged stage data alongside studies reporting stage-specific data, to allow the inclusion of more studies in the meta-analysis. We consider both meta-analysis of tests with binary results, and meta-analysis of tests with continuous results, where the sensitivity to detect disease of each stage across the whole range of observed thresholds is estimated. The methods are demonstrated using a series of simulated datasets and applied to data from a systematic review of the accuracy of tests used to screen for hepatocellular carcinoma in people with liver cirrhosis. We show that incorporating studies with merged stage data can lead to more precise estimates and, in some cases, corrects biologically implausible results that can arise when the availability of stage-specific data is limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12065v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Efthymia Derezea, Nicky J Welton, Gabriel Rogers, Hayley E Jones</dc:creator>
    </item>
    <item>
      <title>Safe, Always-Valid Alpha-Investing Rules For Doubly Sequential Online Inference</title>
      <link>https://arxiv.org/abs/2512.12244</link>
      <description>arXiv:2512.12244v1 Announce Type: new 
Abstract: Dynamic decision-making in rapidly evolving research domains, including marketing, finance, and pharmaceutical development, presents a significant challenge. Researchers frequently confront the need for real-time action within a doubly sequential framework characterized by the continuous influx of high-volume data streams and the intermittent arrival of novel tasks. This calls for the development and implementation of new online inference protocols capable of handling both the continuous processing of incoming information and the efficient allocation of resources to address emerging priorities. We introduce a novel class of Safe and Always-Valid Alpha-investing (SAVA) rules that leverages powerful tools including always valid p-values, e-processes, and online false discovery rate methods. The SAVA algorithm effectively integrates information across all tasks, mitigates the alpha-death problem, and controls the false selection rate (FSR) at all decision points. We validate the efficacy of the SAVA framework through rigorous theoretical analysis and extensive numerical experiments. Our results demonstrate that SAVA not only offers effective control of the FSR but also significantly improves statistical power compared to traditional online testing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12244v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeyu Yao, Bowen Gang, Wenguang Sun</dc:creator>
    </item>
    <item>
      <title>Robust Outlier Detection and Low-Latency Concept Drift Adaptation for Data Stream Regression: A Dual-Channel Architecture</title>
      <link>https://arxiv.org/abs/2512.12289</link>
      <description>arXiv:2512.12289v1 Announce Type: new 
Abstract: Outlier detection and concept drift detection represent two challenges in data analysis. Most studies address these issues separately. However, joint detection mechanisms in regression remain underexplored, where the continuous nature of output spaces makes distinguishing drifts from outliers inherently challenging. To address this, we propose a novel robust regression framework for joint outlier and concept drift detection. Specifically, we introduce a dual-channel decision process that orchestrates prediction residuals into two coupled logic flows: a rapid response channel for filtering point outliers and a deep analysis channel for diagnosing drifts. We further develop the Exponentially Weighted Moving Absolute Deviation with Distinguishable Types (EWMAD-DT) detector to autonomously differentiate between abrupt and incremental drifts via dynamic thresholding. Comprehensive experiments on both synthetic and real-world datasets demonstrate that our unified framework, enhanced by EWMAD-DT, exhibits superior detection performance even when point outliers and concept drifts coexist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12289v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingbing Wang, Shengyan Sun, Jiaqi Wang, Yu Tang</dc:creator>
    </item>
    <item>
      <title>Quantile regression with generalized multiquadric loss function</title>
      <link>https://arxiv.org/abs/2512.12340</link>
      <description>arXiv:2512.12340v1 Announce Type: new 
Abstract: Quantile regression (QR) is now widely used to analyze the effect of covariates on the conditional distribution of a response variable. It provides a more comprehensive picture of the relationship between a response and covariates compared with classical least squares regression. However, the non-differentiability of the check loss function precludes the use of gradient-based methods to solve the optimization problem in quantile regression estimation. To this end, This paper constructs a smoothed loss function based on multiquadric (MQ) function. The proposed loss function leads to a globally convex optimization problem that can be efficiently solved via (stochastic) gradient descent methods. As an example, we apply the Barzilai-Borwein gradient descent method to obtain the estimation of quantile regression. We establish the theoretical results of the proposed estimator under some regularity conditions, and compare it with other estimation methods using Monte Carlo simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12340v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenwu Gao, Dongyi Zheng, Hanbing Zhu</dc:creator>
    </item>
    <item>
      <title>Asymmetric Laplace distribution regression model for fitting heterogeneous longitudinal response</title>
      <link>https://arxiv.org/abs/2512.12362</link>
      <description>arXiv:2512.12362v1 Announce Type: new 
Abstract: The systematic collection of longitudinal data is very common in practice, making mixed models widely used. Most developments around these models focus on modeling the mean trajectory of repeated measurements, typically under the assumption of homoskedasticity. However, as data become increasingly rich through intensive collection over time, these models can become limiting and may introduce biases in analysis. In fact, such data are often heterogeneous, with the presence of outliers, heteroskedasticity, and asymmetry in the distribution of individual measurements. Therefore, ignoring these characteristics can lead to biased modeling results. In this work, we propose a mixed-effect distributional regression model based on the asymmetric Laplace distribution to: (1) address the presence of outliers, heteroskedasticity, and asymmetry in longitudinal measurements; (2) model the entire individual distribution of the heterogeneous longitudinal response over time, rather than just its conditional expectation; and (3) give a more comprehensive evaluation of the impact of covariates on the distribution of the responses through meaningful indicator. A Bayesian estimation procedure is presented. In order to choose between two distributional regression models, we also propose a new model selection criterion for longitudinal data. It measures the proximity between the individual distribution estimated by the model and the empirical individual distribution of the data over time, using a set of quantiles. The estimation procedure and the selection criterion are validated in a simulation study and the proposed model is compared to a distributional regression mixed model based on the Gaussian distribution and a location-scale linear quantile mixed model. Finally, the proposed model is applied to analyze blood pressure over time for hospitalized patients in the intensive care unit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12362v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Barbieri, Angelo Alcaraz, Mouna Abed, Hugues de Courson, H\'el\`ene Jacqmin-Gadda</dc:creator>
    </item>
    <item>
      <title>The Morphemic Origin of Zipf's Law: A Factorized Combinatorial Framework</title>
      <link>https://arxiv.org/abs/2512.12394</link>
      <description>arXiv:2512.12394v1 Announce Type: new 
Abstract: We present a simple structure based model of how words are formed from morphemes. The model explains two major empirical facts: the typical distribution of word lengths and the appearance of Zipf like rank frequency curves. In contrast to classical explanations based on random text or communication efficiency, our approach uses only the combinatorial organization of prefixes, roots, suffixes and inflections. In this Morphemic Combinatorial Word Model, a word is created by activating several positional slots. Each slot turns on with a certain probability and selects one morpheme from its inventory. Morphemes are treated as stable building blocks that regularly appear in word formation and have characteristic positions. This mechanism produces realistic word length patterns with a concentrated middle zone and a thin long tail, closely matching real languages. Simulations with synthetic morpheme inventories also generate rank frequency curves with Zipf like exponents around 1.1-1.4, similar to English, Russian and Romance languages. The key result is that Zipf like behavior can emerge without meaning, communication pressure or optimization principles. The internal structure of morphology alone, combined with probabilistic activation of slots, is sufficient to create the robust statistical patterns observed across languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12394v1</guid>
      <category>stat.ME</category>
      <category>cs.CL</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Berman</dc:creator>
    </item>
    <item>
      <title>Scalable Spatial Stream Network (S3N) Models</title>
      <link>https://arxiv.org/abs/2512.12398</link>
      <description>arXiv:2512.12398v1 Announce Type: new 
Abstract: Understanding how habitats shape species distributions and abundances across spatially complex, dendritic freshwater networks remains a longstanding and fundamental challenge in ecology, with direct implications for effective biodiversity management and conservation. Existing spatial stream network (SSN) models adapt spatial process models to river networks by creating covariance functions that account for stream distance, but preprocessing and estimation with these models is both computationally and time intensive, thus precluding the application of these models to regional or continental scales. This paper introduces a new class of Scalable Spatial Stream Network (S3N) models, which extend nearest-neighbor Gaussian processes to incorporate ecologically relevant spatial dependence while greatly improving computational efficiency. The S3N framework enables scalable modeling of spatial stream networks, demonstrated here for 285 fish species in the Ohio River Basin (&gt;4,000 river km). Validation analyses show that S3N accurately recovers spatial and covariance parameters, even with reduced bias and variance compared to standard SSN implementations. These results represent a key advancement toward large-scale mapping of freshwater fish distributions and quantifying the influence of environmental drivers across extensive river networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12398v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jessica P. Kunke, Julian D. Olden, Tyler H. McCormick</dc:creator>
    </item>
    <item>
      <title>Design-Based Weighted Regression Estimators for Average and Conditional Spillover Effects</title>
      <link>https://arxiv.org/abs/2512.12452</link>
      <description>arXiv:2512.12452v1 Announce Type: new 
Abstract: When individuals engage in social or physical interactions, a unit's outcome may depend on the treatments received by others. In such interference environments, we provide a unified framework characterizing a broad class of spillover estimands as weighted averages of unit-to-unit spillover effects, with estimand-specific weights. We then develop design-based weighted least squares (WLS) estimators for both average and conditional spillover effects. We introduce three nonparametric estimators under the dyadic, sender, and receiver perspectives, which distribute the estimand weights differently across the outcome vector, design matrix, and weight matrix. For the average-type estimands, we show that all three estimators are equivalent to the Hajek estimator. For conditional spillover effects, we establish conditions under which the estimands are consistent for the target conditional spillover effects. We further derive concentration inequalities, a central limit theorem, and conservative variance estimators in an asymptotic regime where both the number of clusters and cluster sizes grow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12452v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fei Fang, Laura Forastiere</dc:creator>
    </item>
    <item>
      <title>Sleep pattern profiling using a finite mixture of contaminated multivariate skew-normal distributions on incomplete data</title>
      <link>https://arxiv.org/abs/2512.12464</link>
      <description>arXiv:2512.12464v1 Announce Type: new 
Abstract: Medical data often exhibit characteristics that make cluster analysis particularly challenging, such as missing values, outliers, and cluster features like skewness. Typically, such data would need to be preprocessed -- by cleaning outliers and missing values -- before clustering could be performed. However, these preliminary steps rely on objective functions different from those used in the clustering stage. In this paper, we propose a unified model-based clustering approach that simultaneously handles atypical observations, missing values, and cluster-wise skewness within a single framework. Each cluster is modelled using a contaminated multivariate skew-normal distribution -- a convenient two-component mixture of multivariate skew-normal densities -- in which one component represents the main data (the "bulk") and the other captures potential outliers. From an inferential perspective, we implement and use a variant of the EM algorithm to obtain the maximum likelihood estimates of the model parameters. Simulation studies demonstrate that the proposed model outperforms existing approaches in both clustering accuracy and outlier detection, across low- and high-dimensional settings, even in the presence of substantial missingness. The method is further applied to the Cleveland Children's Sleep and Health Study (CCSHS), a dataset characterised by incomplete observations. Without any preprocessing, the proposed approach identifies five distinct groups of sleepers, revealing meaningful differences in sleeper typologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12464v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jason Pillay, Cristina Tortora, Antonio Punzo, Andriette Bekker</dc:creator>
    </item>
    <item>
      <title>Robust Variational Bayes by Min-Max Median Aggregation</title>
      <link>https://arxiv.org/abs/2512.12676</link>
      <description>arXiv:2512.12676v1 Announce Type: new 
Abstract: We propose a robust and scalable variational Bayes (VB) framework designed to effectively handle contamination and outliers in dataset. Our approach partitions the data into $m$ disjoint subsets and formulates a joint optimization problem based on robust aggregation principles. A key insight is that the full posterior distribution is equivalent to the minimizer of the mean Kullback-Leibler (KL) divergence from the $m$-powered local posterior distributions. To enhance robustness, we replace the mean KL divergence with a min-max median formulation. The min-max formulation not only ensures consistency between the KL minimizer and the Evidence Lower Bound (ELBO) maximizer but also facilitates the establishment of improved statistical rates for the mean of variational posterior. We observe a notable discrepancy in the $m$-powered marginal log likelihood function contingent on the presence of local latent variables. To address this, we treat these two scenarios separately to guarantee the consistency of the aggregated variational posterior. Specifically, when local latent variables are present, we introduce an aggregate-and-rescale strategy. Theoretically, we provide a non-asymptotic analysis of our proposed posterior, incorporating a refined analysis of Bernstein-von Mises (BvM) theorem to accommodate a diverging number of subsets $m$. Our findings indicate that the two-stage approach yields a smaller approximation error compared to directly aggregating the $m$-powered local posteriors. Furthermore, we establish a nearly optimal statistical rate for the mean of the proposed posterior, advancing existing theories related to min-max median estimators. The efficacy of our method is demonstrated through extensive simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12676v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Yan, Ju Liu, Weidong Liu, Jiyuan Tu</dc:creator>
    </item>
    <item>
      <title>Variational Inference for Fully Bayesian Hierarchical Linear Models</title>
      <link>https://arxiv.org/abs/2512.12857</link>
      <description>arXiv:2512.12857v1 Announce Type: new 
Abstract: Bayesian hierarchical linear models provide a natural framework to analyze nested and clustered data. Classical estimation with Markov chain Monte Carlo produces well calibrated posterior distributions but becomes computationally expensive in high dimensional or large sample settings. Variational Inference and Stochastic Variational Inference offer faster optimization based alternatives, but their accuracy in hierarchical structures is uncertain when group separation is weak. This paper compares these two paradigms across three model classes, the Linear Regression Model, the Hierarchical Linear Regression Model, and a Clustered Hierarchical Linear Regression Model. Through simulation studies and an application to real data, the results show that variational methods recover global regression effects and clustering structure with a fraction of the computing time, but distort posterior dependence and yield unstable values of information criteria such as WAIC and DIC. The findings clarify when variational methods can serve as practical surrogates for Markov chain Monte Carlo and when their limitations make full Bayesian sampling necessary, and they provide guidance for extending the same variational framework to generalized linear models and other members of the exponential family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12857v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cristian Parra-Aldana, Juan Sosa</dc:creator>
    </item>
    <item>
      <title>Robust tests for parameter change in conditionally heteroscedastic time series models</title>
      <link>https://arxiv.org/abs/2512.12946</link>
      <description>arXiv:2512.12946v1 Announce Type: new 
Abstract: Structural changes and outliers often coexist, complicating statistical inference. This paper addresses the problem of testing for parameter changes in conditionally heteroscedastic time series models, particularly in the presence of outliers. To mitigate the impact of outliers, we introduce a two-step procedure comprising robust estimation and residual truncation. Based on this procedure, we propose a residual-based robust CUSUM test and its self-normalized counterpart. We derive the limiting null distributions of the proposed robust tests and establish their consistency. Simulation results demonstrate the strong robustness of the tests against outliers. To illustrate the practical application, we analyze Bitcoin data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12946v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junmo Song</dc:creator>
    </item>
    <item>
      <title>Asymptotic Inference for Constrained Regression</title>
      <link>https://arxiv.org/abs/2512.12953</link>
      <description>arXiv:2512.12953v1 Announce Type: new 
Abstract: We consider statistical inference in high-dimensional regression problems under affine constraints on the parameter space. The theoretical study of this is motivated by the study of genetic determinants of diseases, such as diabetes, using external information from mediating protein expression levels. Specifically, we develop rigorous methods for estimating genetic effects on diabetes-related continuous outcomes when these associations are constrained based on external information about genetic determinants of proteins, and genetic relationships between proteins and the outcome of interest. In this regard, we discuss multiple candidate estimators and study their theoretical properties, sharp large sample optimality, and numerical qualities under a high-dimensional proportional asymptotic framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12953v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Madhav Sankaranarayanan, Yana Hrytsenko, Jerome I. Rotter, Tamar Sofer, Rajarshi Mukherjee</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach to learning mixtures of nonparametric components</title>
      <link>https://arxiv.org/abs/2512.12988</link>
      <description>arXiv:2512.12988v1 Announce Type: new 
Abstract: Mixture models are widely used in modeling heterogeneous data populations. A standard approach of mixture modeling is to assume that the mixture component takes a parametric kernel form, while the flexibility of the model can be obtained by using a large or possibly unbounded number of such parametric kernels. In many applications, making parametric assumptions on the latent subpopulation distributions may be unrealistic, which motivates the need for nonparametric modeling of the mixture components themselves. In this paper we study finite mixtures with nonparametric mixture components, using a Bayesian nonparametric modeling approach. In particular, it is assumed that the data population is generated according to a finite mixture of latent component distributions, where each component is endowed with a Bayesian nonparametric prior such as the Dirichlet process mixture. We present conditions under which the individual mixture component's distributions can be identified, and establish posterior contraction behavior for the data population's density, as well as densities of the latent mixture components. We develop an efficient MCMC algorithm for posterior inference and demonstrate via simulation studies and real-world data illustrations that it is possible to efficiently learn complex distributions for the latent subpopulations. In theory, the posterior contraction rate of the component densities is nearly polynomial, which is a significant improvement over the logarithm convergence rate of estimating mixing measures via deconvolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12988v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yilei Zhang, Yun Wei, Aritra Guha, XuanLong Nguyen</dc:creator>
    </item>
    <item>
      <title>Clinical transfusion-outcomes research: A practical guide</title>
      <link>https://arxiv.org/abs/2512.13155</link>
      <description>arXiv:2512.13155v1 Announce Type: new 
Abstract: Clinical transfusion-outcomes research faces unique methodological challenges compared with other areas of clinical research. These challenges arise because patients frequently receive multiple transfusions, each unit originates from a different donor, and the probability of receiving specific blood product characteristics is influenced by external, often uncontrollable, factors. These complexities complicate causal inference in observational studies of transfusion effectiveness and safety. This guide addresses key challenges in observational transfusion research, with a focus on time-varying exposure, time-varying confounding, and treatment-confounder feedback. Using the example of donor sex and pregnancy history in relation to recipient mortality, we illustrate the strengths and limitations of commonly used analytical approaches. We compare restriction-based analyses, time-varying Cox regression, and inverse probability weighted marginal structural models using a large observational dataset of male transfusion recipients. In the applied example, restriction and conventional time-varying approaches suggested an increased mortality risk associated with transfusion of red blood cells from ever-pregnant female donors compared with male-only donors (hazard ratio [HR] 1.22; 95% CI 1.05-1.42 and HR 1.21; 95% CI 1.04-1.41, respectively). In contrast, inverse probability of treatment and censoring weighted analyses, which account for treatment-confounder feedback, showed no evidence of an association (HR 1.01; 95% CI 0.85-1.20). These findings demonstrate how conventional methods can yield biased estimates when complex longitudinal structures are not adequately handled. We provide practical guidance on study design, target trial emulation, and the use of g-methods, including a reproducible tutorial and example dataset, to support valid causal inference in clinical transfusion research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13155v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sarah J Valk, Camila Caram-Deelder, Rolf. H. H. Groenwold, Johanna G van der Bom</dc:creator>
    </item>
    <item>
      <title>Automatic Quality Control for Agricultural Field Trials -- Detection of Nonstationarity in Grid-indexed Data</title>
      <link>https://arxiv.org/abs/2512.13383</link>
      <description>arXiv:2512.13383v1 Announce Type: new 
Abstract: A common assumption in the spatial analysis of agricultural field trials is stationarity. In practice, however, this assumption is often violated due to unaccounted field effects. For instance, in plant breeding field trials, this can lead to inaccurate estimates of plant performance. Based on such inaccurate estimates, breeders may be impeded in selecting the best performing plant varieties, slowing breeding progress. We propose a method to automatically verify the hypothesis of stationarity. The method is sensitive towards mean as well as variance-covariance nonstationarity. It is specifically developed for the two-dimensional grid-structure of field trials. The method relies on the hypothesis that we can detect nonstationarity by partitioning the field into areas, within which stationarity holds. We applied the method to a large number of simulated datasets and a real-data example. The method reliably points out which trials exhibit quality issues and gives an indication about the severity of nonstationarity. This information can significantly reduce the time spent on manual quality control and enhance its overall reliability. Furthermore, the output of the method can be used to improve the analysis of conducted trials as well as the experimental design of future trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13383v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karen Wolf, Pierre Fernique, Hans-Peter Piepho</dc:creator>
    </item>
    <item>
      <title>A Metadata-Only Feature-Augmented Method Factor for Ex-Post Correction and Attribution of Common Method Variance</title>
      <link>https://arxiv.org/abs/2512.13446</link>
      <description>arXiv:2512.13446v1 Announce Type: new 
Abstract: Common Method Variance (CMV) is a recurring problem that reduces survey accuracy. Popular fixes such as the Harman single-factor test, correlated uniquenesses, common latent factor models, and marker variable approaches have well known flaws. These approaches either poorly identify issues, rely too heavily on researchers' choices, omit real information, or require special marker items that many datasets lack. This paper introduces a metadata-only Feature-Augmented Method Factor (FAMF-SEM): a single extra method factor with fixed, item-specific weights based on questionnaire details like reverse coding, page and item order, scale width, wording direction, and item length. These weights are set using ridge regression, based on residual correlations in a basic CFA, and remain fixed in the model. The method avoids the need for additional data or marker variables and provides CMV-adjusted results with clear links to survey design features. An AMOS/LISREL-friendly, no-code Excel workflow demonstrates the method. The paper explains the rationale, provides model details, outlines setup, presents step-by-step instructions, describes checks and reliability tests, and notes limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13446v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Murat Yaslioglu</dc:creator>
    </item>
    <item>
      <title>Parsimonious Ultrametric Manly Mixture Models</title>
      <link>https://arxiv.org/abs/2512.13473</link>
      <description>arXiv:2512.13473v1 Announce Type: new 
Abstract: A family of parsimonious ultrametric mixture models with the Manly transformation is developed for clustering high-dimensional and asymmetric data. Advances in Gaussian mixture modeling sufficiently handle high-dimensional data but struggle with the common presence of skewness. While these advances reduce the number of free parameters, they often provide limited insight into the structure and interpretation of the clusters. To address this shortcoming, this research implements the extended ultrametric covariance structure and the Manly transformation resulting in the parsimonious ultrametric Manly mixture model family. The ultrametric covariance structure reduces the number of free parameters while identifying latent hierarchical relationships between and within groups of variables. This phenomenon allows the visualization of hierarchical relationships within individual clusters, improving cluster interpretability. Additionally, as with many classes of mixture models, model selection remains a fundamental challenge; a two-step model selection procedure is proposed herein. With simulation studies and real data analyses, we demonstrate improved model selection via the proposed two-step method, and the effective clustering performance for the proposed family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13473v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexa A. Sochaniwsky, Paul D. McNicholas</dc:creator>
    </item>
    <item>
      <title>Actively Learning Joint Contours of Multiple Computer Experiments</title>
      <link>https://arxiv.org/abs/2512.13530</link>
      <description>arXiv:2512.13530v1 Announce Type: new 
Abstract: Contour location$\unicode{x2014}$the process of sequentially training a surrogate model to identify the design inputs that result in a pre-specified response value from a single computer experiment$\unicode{x2014}$is a well-studied active learning problem. Here, we tackle a related but distinct problem: identifying the input configuration that returns pre-specified values of multiple independent computer experiments simultaneously. Motivated by computer experiments of the rotational torques acting upon a vehicle in flight, we aim to identify stable flight conditions which result in zero torque forces. We propose a "joint contour location" (jCL) scheme that strikes a strategic balance between exploring the multiple response surfaces while exploiting learning of the intersecting contours. We employ both shallow and deep Gaussian process surrogates, but our jCL procedure is applicable to any surrogate that can provide posterior predictive distributions. Our jCL designs significantly outperform existing (single response) CL strategies, enabling us to efficiently locate the joint contour of our motivating computer experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13530v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shih-Ni Prim, Kevin R. Quinlan, Paul Hawkins, Jagadeesh Movva, Annie S. Booth</dc:creator>
    </item>
    <item>
      <title>Machine learning to optimize precision in the analysis of randomized trials: A journey in pre-specified, yet data-adaptive learning</title>
      <link>https://arxiv.org/abs/2512.13610</link>
      <description>arXiv:2512.13610v1 Announce Type: new 
Abstract: Covariate adjustment is an approach to improve the precision of trial analyses by adjusting for baseline variables that are prognostic of the primary endpoint. Motivated by the SEARCH Universal HIV Test-and-Treat Trial (2013-2017), we tell our story of developing, evaluating, and implementing a machine learning-based approach for covariate adjustment. We provide the rationale for as well as the practical concerns with such an approach for estimating marginal effects. Using schematics, we illustrate our procedure: targeted machine learning estimation (TMLE) with Adaptive Pre-specification. Briefly, sample-splitting is used to data-adaptively select the combination of estimators of the outcome regression (i.e., the conditional expectation of the outcome given the trial arm and covariates) and known propensity score (i.e., the conditional probability of being randomized to the intervention given the covariates) that minimizes the cross-validated variance estimate and, thereby, maximizes empirical efficiency. We discuss our approach for evaluating finite sample performance with parametric and plasmode simulations, pre-specifying the Statistical Analysis Plan, and unblinding in real-time on video conference with our colleagues from around the world. We present the results from applying our approach in the primary, pre-specified analysis of 8 recently published trials (2022-2024). We conclude with practical recommendations and an invitation to implement our approach in the primary analysis of your next trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13610v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Laura B. Balzer, Mark J. van der Laan, Maya L. Petersen</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes learning from selectively reported confidence intervals</title>
      <link>https://arxiv.org/abs/2512.13622</link>
      <description>arXiv:2512.13622v1 Announce Type: new 
Abstract: We develop a statistical framework for empirical Bayes learning from selectively reported confidence intervals, applied here to provide context for interpreting results published in MEDLINE abstracts. A collection of 326,060 z-scores from MEDLINE abstracts (2000-2018) provides context for interpreting individual studies; we formalize this as an empirical Bayes task complicated by selection bias. We address selection bias through a selective tilting approach that extends empirical Bayes confidence intervals to truncated sampling mechanisms. Sign information is unreliable (a positive z-score need not indicate benefit, and investigators may choose contrast directions post hoc), so we work with absolute z-scores and identify only the distribution of absolute signal-to-noise ratios (SNRs). Our framework provides coverage guarantees for functionals including posterior estimands describing idealized replications and the symmetrized posterior mean, which we justify decision-theoretically as optimal among sign-equivariant (odd) estimators and minimax among priors inducing the same absolute SNR distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13622v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hunter Chen, Junming Guan, Erik van Zwet, Nikolaos Ignatiadis</dc:creator>
    </item>
    <item>
      <title>A comparative overview of win ratio and joint frailty models for recurrent event endpoints with applications in oncology and cardiology</title>
      <link>https://arxiv.org/abs/2512.13629</link>
      <description>arXiv:2512.13629v1 Announce Type: new 
Abstract: Composite endpoints that combine recurrent non-fatal events with a terminal event are increasingly used in randomized clinical trials, yet conventional time-to-first event analyses may obscure clinically relevant information. We compared two statistical frameworks tailored to such endpoints: the joint frailty model (JFM) and the last-event assisted recurrent-event win ratio (LWR). The JFM specifies proportional hazards for the recurrent and terminal events linked through a shared frailty, yielding covariate-adjusted, component-specific hazard ratios that account for informative recurrences and dependence with death. The LWR is a nonparametric, prioritized pairwise comparison that incorporates all observed events over follow-up and summarizes a population-level benefit of treatment while respecting a pre-specified hierarchy between death and recurrences. We first assessed the performance of the methods using simulations that varied both the gamma-frailty variance and the event rates. Next, we investigated these two frameworks using practical clinical applications, to assess the performance of the methods and to estimate the sample size required to achieve adequate power. These two approaches delivered complementary insights. The JFM provided component-specific estimates, while the LWR led to a summary measure of treatment effect with direction. Power was systematically improved with JFM, which thus appeared as the most reliable approach for inference and sample size estimation. Methodological extensions of the LWR to appropriately handle censoring and to formalize causal estimands remain a promising direction for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13629v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Oru\'e, Derek Dinart, Laurent Billot, Carine Bellera, Virginie Rondeau</dc:creator>
    </item>
    <item>
      <title>Amortized Causal Discovery with Prior-Fitted Networks</title>
      <link>https://arxiv.org/abs/2512.11840</link>
      <description>arXiv:2512.11840v1 Announce Type: cross 
Abstract: In recent years, differentiable penalized likelihood methods have gained popularity, optimizing the causal structure by maximizing its likelihood with respect to the data. However, recent research has shown that errors in likelihood estimation, even on relatively large sample sizes, disallow the discovery of proper structures. We propose a new approach to amortized causal discovery that addresses the limitations of likelihood estimator accuracy. Our method leverages Prior-Fitted Networks (PFNs) to amortize data-dependent likelihood estimation, yielding more reliable scores for structure learning. Experiments on synthetic, simulated, and real-world datasets show significant gains in structure recovery compared to standard baselines. Furthermore, we demonstrate directly that PFNs provide more accurate likelihood estimates than conventional neural network-based approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11840v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mateusz Sypniewski, Mateusz Olko, Mateusz Gajewski, Piotr Mi{\l}o\'s</dc:creator>
    </item>
    <item>
      <title>Scalable branch-and-bound model selection with non-monotonic criteria including AIC, BIC and Mallows's $\mathit{C_p}$</title>
      <link>https://arxiv.org/abs/2512.12221</link>
      <description>arXiv:2512.12221v1 Announce Type: cross 
Abstract: Model selection is a pivotal process in the quantitative sciences, where researchers must navigate between numerous candidate models of varying complexity. Traditional information criteria, such as the corrected Akaike Information Criterion (AICc), Bayesian Information Criterion (BIC), and Mallows's $\mathit{C_p}$, are valuable tools for identifying optimal models. However, the exponential increase in candidate models with each additional model parameter renders the evaluation of these criteria for all models -- a strategy known as exhaustive, or brute-force, searches -- computationally prohibitive. Consequently, heuristic approaches like stepwise regression are commonly employed, albeit without guarantees of finding the globally-optimal model.
  In this study, we challenge the prevailing notion that non-monotonicity in information criteria precludes bounds on the search space. We introduce a simple but novel bound that enables the development of branch-and-bound algorithms tailored for these non-monotonic functions. We demonstrate that our approach guarantees identification of the optimal model(s) across diverse model classes, sizes, and applications, often with orders of magnitude computational speedups. For instance, in one previously-published model selection task involving $2^{32}$ (approximately 4 billion) candidate models, our method achieves a computational speedup exceeding 6,000. These findings have broad implications for the scalability and effectiveness of model selection in complex scientific domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12221v1</guid>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakob Vanhoefer (Life and Medical Sciences), Antonia K\"orner (Life and Medical Sciences), Domagoj Doresic (Life and Medical Sciences), Jan Hasenauer (Life and Medical Sciences), Dilan Pathirana (Life and Medical Sciences)</dc:creator>
    </item>
    <item>
      <title>Towards a pretrained deep learning estimator of the Linfoot informational correlation</title>
      <link>https://arxiv.org/abs/2512.12358</link>
      <description>arXiv:2512.12358v1 Announce Type: cross 
Abstract: We develop a supervised deep-learning approach to estimate mutual information between two continuous random variables. As labels, we use the Linfoot informational correlation, a transformation of mutual information that has many important properties. Our method is based on ground truth labels for Gaussian and Clayton copulas. We compare our method with estimators based on kernel density, k-nearest neighbours and neural estimators. We show generally lower bias and lower variance. As a proof of principle, future research could look into training the model with a more diverse set of examples from other copulas for which ground truth labels are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12358v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>St\'ephanie M. van den Berg, Ulrich Halekoh, S\"oren M\"oller, Andreas Kryger Jensen, Jacob von Bornemann Hjelmborg</dc:creator>
    </item>
    <item>
      <title>On the epsilon-delta Structure Underlying Chatterjee's Rank Correlation</title>
      <link>https://arxiv.org/abs/2512.12363</link>
      <description>arXiv:2512.12363v1 Announce Type: cross 
Abstract: We provide an epsilon-delta interpretation of Chatterjee's rank correlation by tracing its origin to a notion of local dependence between random variables. Starting from a primitive epsilon-delta construction, we show that rank-based dependence measures arise naturally as epsilon to zero limits of local averaging procedures. Within this framework, Chatterjee's rank correlation admits a transparent interpretation as an empirical realization of a local L1 residual.
  We emphasize that the probability integral transform plays no structural role in the underlying epsilon-delta mechanism, and is introduced only as a normalization step that renders the final expression distribution-free. We further consider a moment-based analogue obtained by replacing the absolute deviation with a squared residual. This L2 formulation is independent of rank transformations and, under a Gaussian assumption, recovers Pearson's coefficient of determination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12363v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeusu Sato</dc:creator>
    </item>
    <item>
      <title>Spatial-Network Treatment Effects: A Continuous Functional Approach</title>
      <link>https://arxiv.org/abs/2512.12653</link>
      <description>arXiv:2512.12653v2 Announce Type: cross 
Abstract: This paper develops a continuous functional framework for treatment effects that propagate through geographic space and economic networks. We derive a master equation governing propagation from three economic foundations -- heterogeneous agent aggregation, market equilibrium, and cost minimization -- establishing that the framework rests on fundamental principles rather than ad hoc specifications. A key result shows that the spatial-network interaction coefficient equals the mutual information between geographic and market coordinates. The Feynman-Kac representation decomposes effects into inherited and accumulated components along stochastic paths representing economic linkages. The framework nests the no-spillover case as a testable restriction. Monte Carlo simulations demonstrate that conventional estimators -- two-way fixed effects, difference-in-differences, and generalized propensity score -- exhibit 25-38% bias and severe undercoverage when spillovers exist, while our estimator maintains correct inference regardless of whether spillovers are present. Applying the framework to U.S. minimum wage policy, we reject the no-spillover null and find total effects at state borders four times larger than direct effects -- conventional methods capture only one-quarter of policy impact. Structural estimates reveal spatial diffusion consistent with commuting-distance labor mobility, network diffusion consistent with quarterly supply chain adjustment, and significant spatial-network interaction reflecting geographic clustering of industries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12653v2</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>TRACER: Transfer Learning based Real-time Adaptation for Clinical Evolving Risk</title>
      <link>https://arxiv.org/abs/2512.12795</link>
      <description>arXiv:2512.12795v1 Announce Type: cross 
Abstract: Clinical decision support tools built on electronic health records often experience performance drift due to temporal population shifts, particularly when changes in the clinical environment initially affect only a subset of patients, resulting in a transition to mixed populations. Such case-mix changes commonly arise following system-level operational updates or the emergence of new diseases, such as COVID-19. We propose TRACER (Transfer Learning-based Real-time Adaptation for Clinical Evolving Risk), a framework that identifies encounter-level transition membership and adapts predictive models using transfer learning without full retraining. In simulation studies, TRACER outperformed static models trained on historical or contemporary data. In a real-world application predicting hospital admission following emergency department visits across the COVID-19 transition, TRACER improved both discrimination and calibration. TRACER provides a scalable approach for maintaining robust predictive performance under evolving and heterogeneous clinical conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12795v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mengying Yan, Ziye Tian, Siqi Li, Nan Liu, Benjamin A. Goldstein, Molei Liu, Chuan Hong</dc:creator>
    </item>
    <item>
      <title>Correcting exponentiality test for binned earthquake magnitudes</title>
      <link>https://arxiv.org/abs/2512.13599</link>
      <description>arXiv:2512.13599v1 Announce Type: cross 
Abstract: In theory, earthquake magnitudes follow an exponential distribution. In practice, however, earthquake catalogs report magnitudes with finite resolution, resulting in a discrete (geometric) distribution. To determine the lowest magnitude above which seismic events are completely recorded, the Lilliefors test is commonly applied. Because this test assumes continuous data, it is standard practice to add uniform noise to binned magnitudes prior to testing exponentiality.
  This work shows analytically that uniform dithering cannot recover the exponential distribution from its geometric form. It instead returns a piecewise-constant residual lifetime distribution, whose deviation from the exponential model becomes detectable as catalog size or bin width increases. Numerical experiments confirm that this deviation yields an overestimation of the magnitude of completeness in large catalogs.
  We therefore derive the exact noise distribution - a truncated exponential on the bin interval - that correctly restores the continuous exponential distribution over the whole magnitude range. Numerical tests show that this correction yields Lilliefors rejection rates consistent with the significance level for all bin widths and catalog sizes. The proposed solution eliminates a methodological bias in completeness estimation, which especially impacts high-resolution catalogs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13599v1</guid>
      <category>physics.geo-ph</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angela Stallone, Ilaria Spassiani</dc:creator>
    </item>
    <item>
      <title>Switchback Experiments under Geometric Mixing</title>
      <link>https://arxiv.org/abs/2209.00197</link>
      <description>arXiv:2209.00197v4 Announce Type: replace 
Abstract: The switchback is an experimental design that measures treatment effects by repeatedly turning an intervention on and off for a whole system. Switchback experiments are a robust way to overcome cross-unit spillover effects; however, they are vulnerable to bias from temporal carryovers. In this paper, we consider properties of switchback experiments in Markovian systems that mix at a geometric rate. We find that, in this setting, standard switchback designs suffer considerably from carryover bias: Their estimation error decays as $T^{-1/3}$ in terms of the experiment horizon $T$, whereas in the absence of carryovers a faster rate of $T^{-1/2}$ would have been possible. We also show, however, that judicious use of burn-in periods can considerably improve the situation, and enables errors that decay as $\log(T)^{1/2}T^{-1/2}$. Our formal results are mirrored in an empirical evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.00197v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchen Hu, Stefan Wager</dc:creator>
    </item>
    <item>
      <title>Bayesian spatial+: A joint model perspective</title>
      <link>https://arxiv.org/abs/2309.05496</link>
      <description>arXiv:2309.05496v3 Announce Type: replace 
Abstract: Spatial confounding is a common issue in spatial regression models, occurring when spatially varying covariates correlate with the spatial effect included in the model. This dependence, particularly at high spatial frequencies, can introduce bias in regression coefficient estimates when combined with smoothing penalties.
  The spatial+ framework is a widely used two-stage frequentist approach that mitigates spatial confounding by explicitly modeling and removing the spatial structure in the confounding covariate, then using the corresponding residuals in the second-stage model for the response. However, it does not propagate first-stage uncertainty, does not discuss a general inferential framework, and, crucially, cannot guarantee that covariate residuals and spatial effects in the response model are free of shared high-frequency structure, so confounding may persist. We propose Bayesian spatial+, a joint modeling approach that simultaneously addresses these limitations. Our framework naturally propagates uncertainty and enables straightforward posterior inference, while ensuring separation of spatial frequencies through specialized joint priors on smoothness parameters. We further introduce a cut-feedback strategy that prevents feedback between model components from reintroducing confounding. Simulation studies and real-world applications show substantial gains in bias reduction and interval coverage relative to existing approaches. Notably, in our comparisons, Bayesian spatial+ is the only method for which credible interval coverage remains stable as the sample size increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.05496v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isa Marques, Paul F. V. Wiemann</dc:creator>
    </item>
    <item>
      <title>False Discovery Rate and Localizing Power</title>
      <link>https://arxiv.org/abs/2401.03554</link>
      <description>arXiv:2401.03554v3 Announce Type: replace 
Abstract: False discovery rate (FDR) is commonly used for correction for multiple testing in neuroimaging studies. However, when using two-tailed tests, making directional inferences about the results can lead to a vastly inflated error rate, even approaching 100% in some cases. This happens because FDR controls the error rate only globally, over all tests, not within subsets, such as among those in only one or another direction. Here we consider and evaluate different strategies for FDR control in such cases, using both synthetic and real imaging data. Approaches that separate the tests by direction of the hypothesis test, or by the direction of the resulting test statistic, more properly control the directional error rate and preserve FDR benefits, albeit with a doubled risk of errors under complete absence of signal. Strategies that combine tests in both directions, or that use simple two-tailed p-values, can lead to invalid directional conclusions, even if these tests remain globally valid. A solution to this problem is through the use of selective inference, whereby positive and negative tails are treated as sets (families), which are screened locally, then subjected to FDR at a modified level that controls average FDR over those that survive the initial screening. Moreover, the BKY procedure can be used in place of the well-known Benjamini-Hochberg, yielding additional power. These methods are easy to implement. Finally, to enable valid thresholding for directional inference, we suggest that imaging software should allow the user to set asymmetrical thresholds for the two sides of the statistical map. While FDR continues to be a valid, powerful procedure for multiple testing correction, care is needed when making directional inferences for two-tailed tests, or more broadly, when making any localized inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03554v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anderson M. Winkler, Paul A. Taylor, Thomas E. Nichols, Chris Rorden</dc:creator>
    </item>
    <item>
      <title>Efficient Nonparametric Inference for Mediation Analysis with Nonignorable Missing Confounders</title>
      <link>https://arxiv.org/abs/2402.05384</link>
      <description>arXiv:2402.05384v2 Announce Type: replace 
Abstract: Mediation analysis is widely used for exploring treatment mechanisms; however, it faces challenges when nonignorable missing confounders are present. Efficient inference of mediation effects and the efficiency loss due to nonignorable missingness have been rarely studied in the literature because of the difficulties arising from the ill-posed inverse problem. In this paper, we propose a general shadow variable framework for identifying mediation effects, allowing shadow variables to be selected from either observed covariates or externally collected auxiliary data. We then propose a Sieve-based Iterative Outward (SIO) approach for estimation. We establish large-sample theory, particularly asymptotic normality, for the proposed estimator despite the ill-posedness of the problem. We show that our estimator is locally efficient and attains the semiparametric efficiency bound under certain conditions. Building on the efficient influence function, we explicitly quantify the efficiency loss attributable to missingness and propose a debiased machine learning approach for estimation and inference. We examine the finite-sample performance of the proposed approach using extensive simulation studies and showcase its practical applicability through an empirical analysis of CFPS data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05384v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Shan, Wei Li, Chunrong Ai</dc:creator>
    </item>
    <item>
      <title>Admissible online closed testing must employ e-values</title>
      <link>https://arxiv.org/abs/2407.15733</link>
      <description>arXiv:2407.15733v4 Announce Type: replace 
Abstract: In contemporary research, data scientists often test an infinite sequence of hypotheses $H_1,H_2,\ldots$ one by one, and are required to make real-time decisions without knowing the future hypotheses or data. In this paper, we consider such an online multiple testing problem with the goal of providing simultaneous lower bounds for the number of true discoveries in data-adaptively chosen rejection sets. Employing the recent online closure principle, we show that for this task it is necessary to use an anytime-valid test for each intersection hypothesis. This connects two distinct branches of the literature: online testing of multiple hypotheses (where the hypotheses appear online), and sequential anytime-valid testing of a single hypothesis (where the data for a fixed hypothesis appears online). Motivated by this result, we construct a new online closed testing procedure and a corresponding short-cut with a true discovery guarantee based on multiplying sequential e-values. This general but simple procedure gives uniform improvements over the state-of-the-art methods but also allows to construct entirely new and powerful procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15733v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lasse Fischer, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>On Nonparanormal Likelihoods</title>
      <link>https://arxiv.org/abs/2408.17346</link>
      <description>arXiv:2408.17346v2 Announce Type: replace 
Abstract: Nonparanormal models describe the joint distribution of multivariate responses via latent Gaussian, and thus parametric, copulae while allowing flexible nonparametric marginals. Some aspects of such distributions, for example conditional independence, are formulated parametrically. Other features, such as marginal distributions, can be formulated non- or semiparametrically. Such models are attractive when multivariate normality is questionable.
  Most estimation procedures perform two steps, first estimating the nonparametric part. The copula parameters come second, treating the marginal estimates as known. This is sufficient for some applications. For other applications, e.g. when a semiparametric margin features parameters of interest or when standard errors are important, a simultaneous estimation of all parameters might be more advantageous.
  We present suitable parameterisations of nonparanormal models, possibly including semiparametric effects, and define four novel nonparanormal log-likelihood functions. In general, the corresponding one-step optimization problems are shown to be non-convex. In some cases, however, biconvex problems emerge. Several convex approximations are discussed.
  From a low-level computational point of view, the core contribution is the score function for multivariate normal log-probabilities computed via Genz' procedure. We present transformation discriminant analysis when some biomarkers are subject to limit-of-detection problems as an application and illustrate possible empirical gains in semiparametric efficient polychoric correlation analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17346v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Torsten Hothorn</dc:creator>
    </item>
    <item>
      <title>Fast Estimation of the Composite Link Model for Multidimensional Grouped Counts</title>
      <link>https://arxiv.org/abs/2412.04956</link>
      <description>arXiv:2412.04956v2 Announce Type: replace 
Abstract: This paper presents a significant advancement in the estimation of the Composite Link Model within a penalized likelihood framework, specifically designed to address indirect observations of grouped count data. While the model is effective in these contexts, its application becomes computationally challenging in large, high-dimensional settings. To overcome this, we propose a reformulated iterative estimation procedure that leverages Generalized Linear Array Models, enabling the disaggregation and smooth estimation of latent distributions in multidimensional data. Through simulation studies and applications to high-dimensional mortality datasets, we demonstrate the model's capability to capture fine-grained patterns while comparing its computational performance to the conventional algorithm. The proposed methodology offers notable improvements in computational speed, storage efficiency, and practical applicability, making it suitable for a wide range of fields in which high-dimensional data are provided in grouped formats.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04956v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Carlo G. Camarda, Mar\'ia Durb\'an</dc:creator>
    </item>
    <item>
      <title>The Group R2D2 Shrinkage Prior for Sparse Linear Models with Grouped Covariates</title>
      <link>https://arxiv.org/abs/2412.15293</link>
      <description>arXiv:2412.15293v3 Announce Type: replace 
Abstract: Shrinkage priors are a popular Bayesian paradigm to handle sparsity in high-dimensional regression. Still limited, however, is a flexible class of shrinkage priors to handle grouped sparsity, where covariates exhibit some natural grouping structure. This paper proposes a novel extension of the $R^2$-induced Dirichlet Decomposition (R2D2) prior to accommodate grouped variable selection in linear regression models. The proposed method, called the Group R2D2 prior, employs a Dirichlet prior distribution on the coefficient of determination for each group, allowing for a flexible and adaptive shrinkage that operates at both group and individual variable levels. This approach improves the original R2D2 prior to handle grouped predictors, providing a balance between within-group dependence and group-level sparsity. We present several theoretical properties of this proposed prior distribution while also developing a Markov Chain Monte Carlo algorithm. Through simulation studies and real-data analysis, we demonstrate that our method outperforms traditional shrinkage priors in terms of both estimation accuracy, inference and prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15293v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Yanchenko, Kaoru Irie, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Active multiple testing with proxy p-values and e-values</title>
      <link>https://arxiv.org/abs/2502.05715</link>
      <description>arXiv:2502.05715v2 Announce Type: replace 
Abstract: Researchers often lack the resources to test every hypothesis of interest directly or compute test statistics comprehensively, but often possess auxiliary data from which we can compute an estimate of the experimental outcome. We introduce a novel approach for selecting which hypotheses to query a statistic (e.g., run an experiment, perform expensive computation, etc.) in a hypothesis testing setup by leveraging estimates to compute proxy statistics. Our framework allows a scientist to propose a proxy statistic and then query the true statistic with some probability based on the value of the proxy. We make no assumptions about how the proxy is derived, and it can be arbitrarily dependent on the true statistic. If the true statistic is not queried, the proxy is used in its place. We characterize "active" methods that produce valid p-values and e-values in this setting and utilize this framework in the multiple testing setting to create procedures with false discovery rate (FDR) control. Through simulations and real data analysis of causal effects in scCRISPR screen experiments, we empirically demonstrate that our proxy framework has both high power and low resource usage when our proxies are accurate estimates of the respective true statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05715v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyu Xu, Catherine Wang, Larry Wasserman, Kathryn Roeder, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Coherent Disaggregation and Uncertainty Quantification for Spatially Misaligned Data</title>
      <link>https://arxiv.org/abs/2502.10584</link>
      <description>arXiv:2502.10584v4 Announce Type: replace 
Abstract: Spatial misalignment arises when datasets are aggregated or collected at different spatial scales, leading to information loss. We develop a Bayesian disaggregation framework that links misaligned data to a continuous-domain model through an iteratively linearised integration scheme implemented with the Integrated Nested Laplace Approximation (INLA). The framework accommodates different ways of handling observations depending on the application, resulting in four variants: (i) \textit{Raster at Full Resolution}, (ii) \textit{Raster Aggregation}, (iii) \textit{Polygon Aggregation} (PolyAgg), and (iv) \textit{Point Values} (PointVal). The first three represent increasing levels of spatial averaging, while the last two address situations with incomplete covariate information. For PolyAgg and PointVal, we reconstruct the covariate field using three strategies -- \textit{Value Plugin}, \textit{Joint Uncertainty}, and \textit{Uncertainty Plugin} -- with the latter two propagating uncertainty.
  We illustrate the framework with an example motivated by landslide modelling, focusing on methodology rather than interpreting landslide processes. Simulations show that uncertainty-propagating approaches outperform \textit{Value Plugin} method and remain robust under model misspecification. Point-pattern observations and full-resolution covariates are therefore preferable, and when covariate fields are incomplete, uncertainty-aware methods are most reliable. The framework is well suited to landslide susceptibility modelling and other spatial mapping tasks, and integrates seamlessly with INLA-based tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10584v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Man Ho Suen, Mark Naylor, Finn Lindgren</dc:creator>
    </item>
    <item>
      <title>Predicting data value before collection: A coefficient for prioritizing sources under random distribution shift</title>
      <link>https://arxiv.org/abs/2504.06570</link>
      <description>arXiv:2504.06570v4 Announce Type: replace 
Abstract: Researchers often face choices between multiple data sources that differ in quality, cost, and representativeness. Which sources will most improve predictive performance? We study this data prioritization problem under a random distribution shift model, where candidate sources arise from random perturbations to a target population. We propose the Data Usefulness Coefficient (DUC), which predicts the reduction in prediction error from adding a dataset to training, using only covariate summary statistics and no outcome data. We prove that under random shifts, covariate differences between sources are informative about outcome prediction quality. Through theory and experiments on synthetic and real data, we demonstrate that DUC-based selection outperforms alternative strategies, allowing more efficient resource allocation across heterogeneous data sources. The method provides interpretable rankings between candidate datasets and works for any data modality, including ordinal, categorical, and continuous data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06570v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivy Zhang, Dominik Rothenh\"ausler</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Non-Gaussian Simultaneous Autoregressive Models with Missing Data</title>
      <link>https://arxiv.org/abs/2505.23070</link>
      <description>arXiv:2505.23070v2 Announce Type: replace 
Abstract: Standard simultaneous autoregressive (SAR) models typically assume normally distributed errors, an assumption often violated in real-world datasets that frequently exhibit non-normal, skewed, or heavy-tailed characteristics. New SAR models are proposed to capture these non-Gaussian features. The spatial error model (SEM), a widely used SAR-type model, is considered. Three novel SEMs are introduced, extending the standard Gaussian SEM. These extensions incorporate Student's $t$-distributed errors to accommodate heavy-tailed behaviour, one-to-one transformations of the response variable to address skewness, or a combination of both. Variational Bayes (VB) estimation methods are developed for these models, and the framework is further extended to handle missing response data under the missing not at random (MNAR) mechanism. Standard VB methods perform well with complete datasets; however, handling missing data requires a hybrid VB (HVB) approach, which integrates a Markov chain Monte Carlo (MCMC) sampler to generate missing values. The proposed VB methods are evaluated using both simulated and real-world datasets, demonstrating their robustness and effectiveness in dealing with non-Gaussian data and missing data in spatial models. Although the method is demonstrated using SAR models, the proposed model specifications and estimation approaches are widely applicable to various types of models for handling non-Gaussian data with missing values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23070v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anjana Wijayawardhana, David Gunawan, Thomas Suesse</dc:creator>
    </item>
    <item>
      <title>Projected Bayesian Spatial Factor Models</title>
      <link>https://arxiv.org/abs/2506.01098</link>
      <description>arXiv:2506.01098v2 Announce Type: replace 
Abstract: Factor models balance flexibility, identifiability, and computational efficiency, with Bayesian spatial factor models particularly prone to identifiability challenges and scaling limitations. This work introduces Projected Bayesian Spatial Factor (PBSF) models, a new class of models designed to achieve scalability and robust identifiability for spatial factor analysis. PBSF models are defined through a novel Markov chain Monte Carlo construction, Projected MCMC (ProjMC$^2$), which leverages conditional conjugacy and projection to improve posterior stability and mixing by constraining factor sampling to a scaled Stiefel manifold. Theoretical results establish convergence of ProjMC$^2$ irrespective of initialisation. By integrating scalable univariate spatial modelling, PBSF provides a flexible and interpretable framework for low-dimensional spatial representation learning of massive spatial data. Simulation studies demonstrate substantial efficiency and robustness gains, and an application to human kidney spatial transcriptomics data highlights the practical utility of the proposed methodology for improving interpretability in spatial omics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01098v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lu Zhang</dc:creator>
    </item>
    <item>
      <title>Goodness-of-fit testing for the stationary density of a size-structured PDE</title>
      <link>https://arxiv.org/abs/2506.05103</link>
      <description>arXiv:2506.05103v2 Announce Type: replace 
Abstract: We consider two division models for structured cell populations, where cells can grow, age and divide. These models have been introduced in the literature under the denomination of `mitosis' and `adder' models. In the recent years, there has been an increasing interest in biology to understand whether the cells divide equally or not, as this can be related to important mechanisms in cellular aging or recovery. We are therefore interested in testing the null hypothesis $H_0$ where the division of a mother cell results into two daughters of equal size, against the alternative hypothesis $H_1$ where the division is asymmetric and ruled by a kernel that is absolutely continuous with respect to the Lebesgue measure. The sample consists of i.i.d. observations of cell sizes and ages drawn from the population, and the division is not directly observed. The hypotheses of the test are reformulated as hypotheses on the stationary size and age distributions of the models, which we assume are also the distributions of the observations. We propose a goodness-of-fit test that we study numerically on simulated data before applying it on real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05103v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Van Ha Hoang, Phu Thanh Nguyen, Thanh Mai Pham Ngoc, Vincent Rivoirard, Viet Chi Tran</dc:creator>
    </item>
    <item>
      <title>Differential Distance Correlation and Its Applications</title>
      <link>https://arxiv.org/abs/2507.00524</link>
      <description>arXiv:2507.00524v2 Announce Type: replace 
Abstract: In this paper, we propose a novel Euclidean-distance-based coefficient, named differential distance correlation, to measure the strength of dependence between a random variable $ Y \in \mathbb{R} $ and a random vector $ \boldsymbol{X} \in \mathbb{R}^p $. The coefficient has a concise expression and is invariant to arbitrary orthogonal transformations of the random vector. Moreover, the coefficient is a strongly consistent estimator of a simple and interpretable dependent measure, which is 0 if and only if $ \boldsymbol{X} $ and $ Y $ are independent and equal to 1 if and only if $ Y $ determines $ \boldsymbol{X} $ almost surely. An alternative approach is also proposed to address the limitation that the coefficient is non-robust to outliers. Furthermore, the coefficient exhibits asymptotic normality with a simple variance under the independent hypothesis, facilitating fast and accurate estimation of $ p $-value for testing independence. Three simulation experiments show that the proposed coefficient is more computationally efficient for independence testing and more effective in detecting oscillatory relationships than several competing methods. We also apply our method to analyze a real data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00524v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixiao Liu, Pengjian Shang</dc:creator>
    </item>
    <item>
      <title>Minority representation and fairness in network ranking: An application to school contact diary data</title>
      <link>https://arxiv.org/abs/2507.01136</link>
      <description>arXiv:2507.01136v2 Announce Type: replace 
Abstract: Considerations of bias, fairness and representation are a prerequisite of responsible modern statistics. In statistical network analysis, observed networks are often incomplete or systematically biased, which can lead to systematic underrepresentation of protected groups, and affect any downstream ranking or decision based on the observed network. In this paper, we study a high school contact network constructed from self-reported contact diaries and introduce a formal measure of minority representation, defined as the proportion of minority nodes among the top-ranked individuals. We model systematic bias through group-dependent missing edge mechanisms and develop statistical methods to estimate and test for such bias. When bias is detected, we propose a re-ranking procedure based on an asymptotic approximation that improves group representation. Applying the framework to the high school contact network reveals systematic underreporting of cross-group contacts consistent with recall bias. These findings highlight the importance of modeling and correcting systematic bias in social networks with heterogeneous groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01136v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hui Shen, Peter W. MacDonald, Eric D. Kolaczyk</dc:creator>
    </item>
    <item>
      <title>A New Integrative Learning Framework for Integrating Multiple Secondary Outcomes into Primary Outcome Analysis: A Case Study on Liver Health</title>
      <link>https://arxiv.org/abs/2507.18865</link>
      <description>arXiv:2507.18865v2 Announce Type: replace 
Abstract: In the era of big data, secondary outcomes have become increasingly important alongside primary outcomes. These secondary outcomes, which can be derived from traditional endpoints in clinical trials, compound measures, or risk prediction scores, hold the potential to enhance the analysis of primary outcomes. Our method is motivated by the challenge of utilizing multiple secondary outcomes, such as blood biochemistry markers and urine assays, to improve the analysis of the primary outcome related to liver health. Current integration methods often fall short, as they impose strong model assumptions or require prior knowledge to construct over-identified working functions. This paper addresses these statistical challenges and potentially opens a new avenue in data integration by introducing a novel integrative learning framework that is applicable in a general setting. The proposed framework allows for the robust, data-driven integration of information from multiple secondary outcomes, promotes the development of efficient learning algorithms, and ensures optimal use of available data. Extensive simulation studies demonstrate that the proposed method significantly reduces variance in primary outcome analysis, outperforming existing integration approaches. Additionally, applying this method to UK Biobank (UKB) reveals that cigarette smoking is associated with increased fatty liver measures, with these effects being particularly pronounced in the older adult cohort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18865v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daxuan Deng, Peisong Han, Shuo Chen, Ming Wang, Chixiang Chen</dc:creator>
    </item>
    <item>
      <title>Regularized Reduced Rank Regression for mixed predictor and response variables</title>
      <link>https://arxiv.org/abs/2511.16718</link>
      <description>arXiv:2511.16718v2 Announce Type: replace 
Abstract: In this paper, we introduce the Generalized Mixed Regularized Reduced Rank Regression model (GMR4), an extension of the GMR3 model designed to improve performance in high-dimensional settings. GMR3 is a regression method for a mix of numeric, binary and ordinal response variables, while also allowing for mixed-type predictors through optimal scaling. GMR4 extends this approach by incorporating regularization techniques, such as Ridge, Lasso, Group Lasso, or any combination thereof, making the model suitable for datasets with a large number of predictors or collinearity among them. In addition, we propose a cross-validation procedure that enables the estimation of the rank S and the penalty parameter lambda. Through a simulation study, we evaluate the performance of the model under different scenarios, varying the sample size, the number of non-informative predictors and response dimension. The results of the simulation study guide the choice of the penalty parameter lambda in the empirical application ISSP: Health and Healthcare I-II (2023), which includes mixed-type predictors and ordinal responses. In this application, the model results in a sparse and interpretable solution, with a limited set of influential predictors that provide insights into public attitudes toward healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16718v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenza Cotugno, Mark de Rooij, Roberta Siciliano</dc:creator>
    </item>
    <item>
      <title>Differentially Private Fisher Randomization Tests for Binary Outcomes</title>
      <link>https://arxiv.org/abs/2511.20884</link>
      <description>arXiv:2511.20884v2 Announce Type: replace 
Abstract: Across many disciplines, causal inference often relies on randomized experiments with binary outcomes. In such experiments, the Fisher randomization test provides exact, assumption-free tests for causal effects. Sometimes the outcomes are sensitive and must be kept confidential, for example, when they comprise physical or mental health measurements. Releasing test statistics or p-values computed with the confidential outcomes can leak information about the individuals in the study. Those responsible for sharing the analysis results may wish to bound this information leakage, which they can do by ensuring the released outputs satisfy differential privacy. In this article, we develop several differentially private versions of the Fisher randomization test for binary outcomes. Specifically, we consider direct perturbation approaches that inject calibrated noise into test statistics or p-values, as well as a mechanism-aware, Bayesian denoising framework that explicitly models the privacy mechanism. We further develop decision-making procedures under privacy constraints, including a Bayes risk-optimal rule and a frequentist-calibrated significance test. Through theoretical results, simulation studies, and an application to the ADAPTABLE clinical trial, we demonstrate that our methods can achieve valid and interpretable causal inference while ensuring the differential privacy guarantee.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20884v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qingyang Sun, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>Reyes's I: Measuring Spatial Autocorrelation in Compositions</title>
      <link>https://arxiv.org/abs/2512.04289</link>
      <description>arXiv:2512.04289v2 Announce Type: replace 
Abstract: Compositional observations arise when measurements are recorded as parts of a whole, so that only relative information is meaningful and the natural sample space is the simplex equipped with Aitchison geometry. Despite extensive development of compositional methods, a direct analogue of Moran's \(I\) for assessing spatial autocorrelation in areal compositional data has been lacking. We propose Reyes's \(I\), a Moran type statistic defined through the Aitchison inner product and norm, which is invariant to scale, to permutations of the parts, and to the choice of the \(\operatorname{ilr}\) contrast matrix. Under the randomization assumption, we derive an upper bound, the expected value, and the noncentral second moment, and we describe exact and Monte Carlo permutation procedures for inference. Through simulations covering identical, independent, and spatially correlated compositions under multiple covariance structures and neighborhood definitions, we show that Reyes's \(I\) provides stable behavior, competitive calibration, and improved efficiency relative to a naive alternative based on averaging componentwise Moran statistics. We illustrate practical utility by studying the spatial dependence of a composition measuring COVID-19 severity across Colombian departments during January 2021, documenting significant positive autocorrelation early in the month that attenuates over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04289v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lina Buitrago, Juan Sosa, Oscar Melo</dc:creator>
    </item>
    <item>
      <title>ADOPT: Additive Optimal Transport Regression</title>
      <link>https://arxiv.org/abs/2512.08118</link>
      <description>arXiv:2512.08118v3 Announce Type: replace 
Abstract: Regression analysis for responses taking values in general metric spaces has received increasing attention, particularly for settings with Euclidean predictors $X \in \mathbb{R}^p$ and non-Euclidean responses $Y$ in metric spaces. While additive regression is a powerful tool for enhancing interpretability and mitigating the curse of dimensionality in the presence of multivariate predictors, its direct extension is hindered by the absence of vector space operations in general metric spaces. We propose a novel framework for additive optimal transport regression, which incorporates additive structure through optimal geodesic transports. A key idea is to extend the notion of optimal transports in Wasserstein spaces to general geodesic metric spaces. This unified approach accommodates a wide range of responses, including probability distributions, symmetric positive definite (SPD) matrices with various metrics and spherical data. The practical utility of the method is illustrated with correlation matrices derived from resting state fMRI brain imaging data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08118v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wookyeong Song, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>Model-robust Inference for Seamless II/III Trials with Covariate Adaptive Randomization</title>
      <link>https://arxiv.org/abs/2512.09430</link>
      <description>arXiv:2512.09430v3 Announce Type: replace 
Abstract: Seamless phase II/III trials have become a cornerstone of modern drug development, offering a means to accelerate evaluation while maintaining statistical rigor. However, most existing inference procedures are model-based, designed primarily for continuous outcomes, and often neglect the stratification used in covariate-adaptive randomization (CAR), limiting their practical relevance. In this paper, we propose a unified, model-robust framework for seamless phase II/III trials grounded in generalized linear models (GLMs), enabling valid inference across diverse outcome types, estimands, and CAR schemes. Using Z-estimation, we derive the asymptotic properties of treatment effect estimators and explicitly characterize how their variance depends on the underlying randomization procedure. Based on these results, we develop adjusted Wald tests that, together with Dunnett's multiple-comparison procedure and the inverse chi-square combination method, ensure valid overall Type I error. Extensive simulation studies and a trial example demonstrate that the proposed model-robust tests achieve superior power and reliable inference compared to conventional approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.09430v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Yi, Lucy Xia</dc:creator>
    </item>
    <item>
      <title>Learning Time-Varying Correlation Networks with FDR Control via Time-Varying P-values</title>
      <link>https://arxiv.org/abs/2512.10467</link>
      <description>arXiv:2512.10467v2 Announce Type: replace 
Abstract: This paper presents a systematic framework for controlling false discovery rate in learning time-varying correlation networks from high-dimensional, non-linear, non-Gaussian and non-stationary time series with an increasing number of potential abrupt change points in means. We propose a bootstrap-assisted approach to derive dependent and time-varying P-values from a robust estimate of time-varying correlation functions, which are not sensitive to change points. Our procedure is based on a new high-dimensional Gaussian approximation result for the uniform approximation of P-values across time and different coordinates. Moreover, we establish theoretically guaranteed Benjamini--Hochberg and Benjamini--Yekutieli procedures for the dependent and time-varying P-values, which can achieve uniform false discovery rate control. The proposed methods are supported by rigorous mathematical proofs and simulation studies. We also illustrate the real-world application of our framework using both brain electroencephalogram and financial time series data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10467v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bufan Li, Lujia Bai, Weichi Wu</dc:creator>
    </item>
    <item>
      <title>Autotune: fast, accurate, and automatic tuning parameter selection for Lasso</title>
      <link>https://arxiv.org/abs/2512.11139</link>
      <description>arXiv:2512.11139v2 Announce Type: replace 
Abstract: Least absolute shrinkage and selection operator (Lasso), a popular method for high-dimensional regression, is now used widely for estimating high-dimensional time series models such as the vector autoregression (VAR). Selecting its tuning parameter efficiently and accurately remains a challenge, despite the abundance of available methods for doing so. We propose $\mathsf{autotune}$, a strategy for Lasso to automatically tune itself by optimizing a penalized Gaussian log-likelihood alternately over regression coefficients and noise standard deviation. Using extensive simulation experiments on regression and VAR models, we show that $\mathsf{autotune}$ is faster, and provides better generalization and model selection than established alternatives in low signal-to-noise regimes. In the process, $\mathsf{autotune}$ provides a new estimator of noise standard deviation that can be used for high-dimensional inference, and a new visual diagnostic procedure for checking the sparsity assumption on regression coefficients. Finally, we demonstrate the utility of $\mathsf{autotune}$ on a real-world financial data set. An R package based on C++ is also made publicly available on Github.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11139v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tathagata Sadhukhan, Ines Wilms, Stephan Smeekes, Sumanta Basu</dc:creator>
    </item>
    <item>
      <title>Information Based Inference in Models with Set-Valued Predictions and Misspecification</title>
      <link>https://arxiv.org/abs/2401.11046</link>
      <description>arXiv:2401.11046v2 Announce Type: replace-cross 
Abstract: This paper proposes an information-based inference method for partially identified parameters in incomplete models that is valid both when the model is correctly specified and when it is misspecified. Key features of the method are: (i) it is based on minimizing a suitably defined Kullback-Leibler information criterion that accounts for incompleteness of the model and delivers a non-empty pseudo-true set; (ii) it is computationally tractable; (iii) its implementation is the same for both correctly and incorrectly specified models; (iv) it exploits all information provided by variation in discrete and continuous covariates; (v) it relies on Rao's score statistic, which is shown to be asymptotically pivotal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11046v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroaki Kaido, Francesca Molinari</dc:creator>
    </item>
    <item>
      <title>Fused $L_{1/2}$ prior for large scale linear inverse problem with Gibbs bouncy particle sampler</title>
      <link>https://arxiv.org/abs/2409.07874</link>
      <description>arXiv:2409.07874v3 Announce Type: replace-cross 
Abstract: In this paper, we study Bayesian approach for solving large scale linear inverse problems arising in various scientific and engineering fields. We propose a fused $L_{1/2}$ prior with edge-preserving and sparsity-promoting properties and show that it can be formulated as a Gaussian mixture Markov random field. Since the density function of this family of prior is neither log-concave nor Lipschitz, gradient-based Markov chain Monte Carlo methods can not be applied to sample the posterior. Thus, we present a Gibbs sampler in which all the conditional posteriors involved have closed form expressions. The Gibbs sampler works well for small size problems but it is computationally intractable for large scale problems due to the need for sample high dimensional Gaussian distribution. To reduce the computation burden, we construct a Gibbs bouncy particle sampler (Gibbs-BPS) based on a piecewise deterministic Markov process. This new sampler combines elements of Gibbs sampler with bouncy particle sampler and its computation complexity is an order of magnitude smaller. We show that the new sampler converges to the target distribution. With computed tomography examples, we demonstrate that the proposed method shows competitive performance with existing popular Bayesian methods and is highly efficient in large scale problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07874v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiongwen Ke, Yanan Fan, Qingping Zhou</dc:creator>
    </item>
    <item>
      <title>Incorporating Interventional Independence Improves Robustness against Interventional Distribution Shift</title>
      <link>https://arxiv.org/abs/2507.05412</link>
      <description>arXiv:2507.05412v3 Announce Type: replace-cross 
Abstract: We study the problem of learning robust discriminative representations of causally related latent variables given the underlying causal graph and a training set comprising passively collected observational data and interventional data obtained through targeted interventions on some of these latent variables. We desire to learn representations that are robust against the resulting interventional distribution shifts. Existing approaches treat observational and interventional data alike, ignoring the independence relations arising from these interventions, even with known underlying causal models. As a result, their representations lead to large predictive performance disparities between observational and interventional data. This performance disparity worsens when interventional training data is scarce. In this paper, (1) we first identify a strong correlation between this performance disparity and the representations' violation of statistical independence induced during interventions. (2) For linear models, we derive sufficient conditions on the proportion of interventional training data, for which enforcing statistical independence between representations of the intervened node and its non-descendants during interventions lowers the test-time error on interventional data. Combining these insights, (3) we propose RepLIn, a training algorithm that explicitly enforces this statistical independence between interventional representations. We demonstrate the utility of RepLIn on a synthetic dataset, and on real image and text datasets on facial attribute classification and toxicity detection, respectively, with semi-synthetic causal structures. Our experiments show that RepLIn is scalable with the number of nodes in the causal graph and is suitable to improve robustness against interventional distribution shifts of both continuous and discrete latent variables compared to the ERM baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05412v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gautam Sreekumar, Vishnu Naresh Boddeti</dc:creator>
    </item>
    <item>
      <title>Polynomial Log-Marginals and Tweedie's Formula : When Is Bayes Possible?</title>
      <link>https://arxiv.org/abs/2509.05823</link>
      <description>arXiv:2509.05823v2 Announce Type: replace-cross 
Abstract: Motivated by Tweedie's formula for the Compound Decision problem, we examine the theoretical foundations of empirical Bayes estimators that directly model the marginal density $m(y)$. Our main result shows that polynomial log-marginals of degree $k \ge 3 $ cannot arise from any valid prior distribution in exponential family models, while quadratic forms correspond exactly to Gaussian priors. This provides theoretical justification for why certain empirical Bayes decision rules, while practically useful, do not correspond to any formal Bayes procedures. We also strengthen the diagnostic by showing that a marginal is a Gaussian convolution only if it extends to a bounded solution of the heat equation in a neighborhood of the smoothing parameter, beyond the convexity of $c(y)=\tfrac12 y^2+\log m(y)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05823v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jyotishka Datta, Nicholas G. Polson</dc:creator>
    </item>
    <item>
      <title>Nonparametric Uniform Inference in Binary Classification and Policy Values</title>
      <link>https://arxiv.org/abs/2511.14700</link>
      <description>arXiv:2511.14700v2 Announce Type: replace-cross 
Abstract: We develop methods for nonparametric uniform inference in cost-sensitive binary classification, a framework that encompasses maximum score estimation, predicting utility maximizing actions, and policy learning. These problems are well known for slow convergence rates and non-standard limiting behavior, even under point identified parametric frameworks. In nonparametric settings, they may further suffer from failures of identification. To address these challenges, we introduce a strictly convex surrogate loss that point-identifies a representative nonparametric policy function. We then estimate this representative policy function to conduct inference on both the optimal classification policy and the optimal policy value. This approach enables Gaussian inference, substantially simplifying empirical implementation relative to working directly with the original classification problem. In particular, we establish root-$n$ asymptotic normality for the optimal policy value and derive a Gaussian approximation for the optimal classification policy at the standard nonparametric rate. Extensive simulation studies corroborate the theoretical findings. We apply our method to the National JTPA Study to conduct inference on the optimal treatment assignment policy and its associated welfare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14700v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Nan Liu, Yanbo Liu, Yuya Sasaki, Yuanyuan Wan</dc:creator>
    </item>
    <item>
      <title>Developing synthetic microdata through machine learning for firm-level business surveys</title>
      <link>https://arxiv.org/abs/2512.05948</link>
      <description>arXiv:2512.05948v2 Announce Type: replace-cross 
Abstract: Public-use microdata samples (PUMS) from the United States (US) Census Bureau on individuals have been available for decades. However, large increases in computing power and the greater availability of Big Data have dramatically increased the probability of re-identifying anonymized data, potentially violating the pledge of confidentiality given to survey respondents. Data science tools can be used to produce synthetic data that preserve critical moments of the empirical data but do not contain the records of any existing individual respondent or business. Developing public-use firm data from surveys presents unique challenges different from demographic data, because there is a lack of anonymity and certain industries can be easily identified in each geographic area. This paper briefly describes a machine learning model used to construct a synthetic PUMS based on the Annual Business Survey (ABS) and discusses various quality metrics. Although the ABS PUMS is currently being refined and results are confidential, we present two synthetic PUMS developed for the 2007 Survey of Business Owners, similar to the ABS business data. Econometric replication of a high impact analysis published in Small Business Economics demonstrates the verisimilitude of the synthetic data to the true data and motivates discussion of possible ABS use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05948v2</guid>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jorge Cisneros, Timothy Wojan, Matthew Williams, Jennifer Ozawa, Robert Chew, Kimberly Janda, Timothy Navarro, Michael Floyd, Christine Task, Damon Streat</dc:creator>
    </item>
  </channel>
</rss>
