<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Aug 2025 04:01:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Sampling by averaging: A multiscale approach to score estimation</title>
      <link>https://arxiv.org/abs/2508.15069</link>
      <description>arXiv:2508.15069v1 Announce Type: new 
Abstract: We introduce a novel framework for efficient sampling from complex, unnormalised target distributions by exploiting multiscale dynamics. Traditional score-based sampling methods either rely on learned approximations of the score function or involve computationally expensive nested Markov chain Monte Carlo (MCMC) loops. In contrast, the proposed approach leverages stochastic averaging within a slow-fast system of stochastic differential equations (SDEs) to estimate intermediate scores along a diffusion path without training or inner-loop MCMC. Two algorithms are developed under this framework: MultALMC, which uses multiscale annealed Langevin dynamics, and MultCDiff, based on multiscale controlled diffusions for the reverse-time Ornstein-Uhlenbeck process. Both overdamped and underdamped variants are considered, with theoretical guarantees of convergence to the desired diffusion path. The framework is extended to handle heavy-tailed target distributions using Student's t-based noise models and tailored fast-process dynamics. Empirical results across synthetic and real-world benchmarks, including multimodal and high-dimensional distributions, demonstrate that the proposed methods are competitive with existing samplers in terms of accuracy and efficiency, without the need for learned models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15069v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Paula Cordero-Encinar, Andrew B. Duncan, Sebastian Reich, O. Deniz Akyildiz</dc:creator>
    </item>
    <item>
      <title>CSTEapp: An interactive R-Shiny application of the covariate-specific treatment effect curve for visualizing individualized treatment rule</title>
      <link>https://arxiv.org/abs/2508.15265</link>
      <description>arXiv:2508.15265v1 Announce Type: new 
Abstract: In precision medicine, deriving the individualized treatment rule (ITR) is crucial for recommending the optimal treatment based on patients' baseline covariates. The covariate-specific treatment effect (CSTE) curve presents a graphical method to visualize an ITR within a causal inference framework. Recent advancements have enhanced the causal interpretation of the CSTE curves and provided methods for deriving simultaneous confidence bands for various study types. To facilitate the implementation of these methods and make ITR estimation more accessible, we developed CSTEapp, a web-based application built on the R Shiny framework. CSTEapp allows users to upload data and create CSTE curves through simple point and click operations, making it the first application for estimating the ITRs. CSTEapp simplifies the analytical process by providing interactive graphical user interfaces with dynamic results, enabling users to easily report optimal treatments for individual patients based on their covariates information. Currently, CSTEapp is applicable to studies with binary and time-to-event outcomes, and we continually expand its capabilities to accommodate other outcome types as new methods emerge. We demonstrate the utility of CSTEapp using real-world examples and simulation datasets. By making advanced statistical methods more accessible, CSTEapp empowers researchers and practitioners across various fields to advance precision medicine and improve patient outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15265v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Zhou, Yuhao Deng, Yu-Shi Tian, Peng Wu, Wenjie Hu, Haoxiang Wang, Ewout Steyerberg, Xiao-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>Efficient sampling from a multivariate normal distribution subject to linear equality and inequality constraints</title>
      <link>https://arxiv.org/abs/2508.15292</link>
      <description>arXiv:2508.15292v1 Announce Type: new 
Abstract: Sampling from multivariate normal distributions, subjected to a variety of restrictions, is a problem that is recurrent in statistics and computing. In the present work, we demonstrate a general framework to efficiently sample a multivariate normal distribution subject to any set of linear inequality constraints and/or linear equality constraints simultaneously. In the approach we detail, sampling a multivariate random variable from the domain formed by the intersection of linear constraints proceeds via a combination of elliptical slice sampling to address the inequality constraints, and linear mapping to address the equality constraints. We also detail a linear programming method for finding an initial sample on the linearly constrained domain; such a method is critical for sampling problems where the domain has small probability.
  We demonstrate the validity of our methods on an arbitrarily chosen four-dimensional multivariate normal distribution subject to five inequality constraints and/or two equality constraints. Our approach compares favourably to direct sampling and/or accept-reject sampling methods; the latter methods vary widely in their efficiency, whereas the methods in the present work are rejection-free. Where practical we compare predictions of probability density functions between our sampling methods and analytical computation. For all simulations we demonstrate that our methods yield accurate computation of the mean and covariance of the multivariate normal distributions restricted by the imposed linear constraints. MATLAB codes to implement our methods are readily available at https://dx.doi.org/10.6084/m9.figshare.29956304 .</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15292v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew P. Adams, Gloria M. Monsalve-Bravo, Lucy G. Dowdell, Scott A. Sisson, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>Twin-Boot: Uncertainty-Aware Optimization via Online Two-Sample Bootstrapping</title>
      <link>https://arxiv.org/abs/2508.15019</link>
      <description>arXiv:2508.15019v1 Announce Type: cross 
Abstract: Standard gradient descent methods yield point estimates with no measure of confidence. This limitation is acute in overparameterized and low-data regimes, where models have many parameters relative to available data and can easily overfit. Bootstrapping is a classical statistical framework for uncertainty estimation based on resampling, but naively applying it to deep learning is impractical: it requires training many replicas, produces post-hoc estimates that cannot guide learning, and implicitly assumes comparable optima across runs - an assumption that fails in non-convex landscapes. We introduce Twin-Bootstrap Gradient Descent (Twin-Boot), a resampling-based training procedure that integrates uncertainty estimation into optimization. Two identical models are trained in parallel on independent bootstrap samples, and a periodic mean-reset keeps both trajectories in the same basin so that their divergence reflects local (within-basin) uncertainty. During training, we use this estimate to sample weights in an adaptive, data-driven way, providing regularization that favors flatter solutions. In deep neural networks and complex high-dimensional inverse problems, the approach improves calibration and generalization and yields interpretable uncertainty maps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15019v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Stein Brito</dc:creator>
    </item>
    <item>
      <title>On Prior Distributions for Orthogonal Function Sequences</title>
      <link>https://arxiv.org/abs/2508.15552</link>
      <description>arXiv:2508.15552v1 Announce Type: cross 
Abstract: We propose a novel class of prior distributions for sequences of orthogonal functions, which are frequently required in various statistical models such as functional principal component analysis (FPCA). Our approach constructs priors sequentially by imposing adaptive orthogonality constraints through a hierarchical formulation of conditionally normal distributions. The orthogonality is controlled via hyperparameters, allowing for flexible trade-offs between exactness and smoothness, which can be learned from the observed data. We illustrate the properties of the proposed prior and show that it leads to nearly orthogonal posterior estimates. The proposed prior is employed in Bayesian FPCA, providing more interpretable principal functions and efficient low-rank representations. Through simulation studies and analysis of human mobility data in Tokyo, we demonstrate the superior performance of our approach in inducing orthogonality and improving functional component estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15552v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shonosuke Sugasawa, Daichi Mochihashi</dc:creator>
    </item>
    <item>
      <title>Flexible yet Sparse Bayesian Survival Models with Time-Varying Coefficients and Unobserved Heterogeneity</title>
      <link>https://arxiv.org/abs/2206.11320</link>
      <description>arXiv:2206.11320v3 Announce Type: replace-cross 
Abstract: Survival analysis is an important area of medical research, yet existing models often struggle to balance simplicity with flexibility. Simple models require minimal adjustments but come with strong assumptions, while more flexible models require significant input and tuning from researchers. We present a survival model using a Bayesian hierarchical shrinkage method that automatically determines whether each covariate should be treated as static, time-varying, or excluded altogether. This approach strikes a balance between simplicity and flexibility, minimizes the need for tuning, and naturally quantifies uncertainty. The method is supported by an efficient Markov chain Monte Carlo sampler, implemented in the R package shrinkDSM. Comprehensive simulation studies and an application to a clinical dataset involving patients with adenocarcinoma of the gastroesophageal junction showcase the advantages of our approach compared to existing models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.11320v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Knaus, Daniel Winkler, Sebastian F. Schoppmann, Gerd Jomrich</dc:creator>
    </item>
    <item>
      <title>Scalable Time-Series Causal Discovery with Approximate Causal Ordering</title>
      <link>https://arxiv.org/abs/2409.05500</link>
      <description>arXiv:2409.05500v3 Announce Type: replace-cross 
Abstract: Causal discovery in time-series data presents a significant computational challenge. Standard algorithms are often prohibitively expensive for datasets with many variables or samples. This study introduces and validates a heuristic approximation of the VarLiNGAM algorithm to address this scalability problem. The standard VarLiNGAM method relies on an iterative search, recalculating statistical dependencies after each step. Our heuristic modifies this procedure by omitting the iterative refinement. This change permits a one-time precomputation of all necessary statistical values. The algorithmic modification reduces the time complexity from $O(m^3n)$ to $O(m^2n + m^3)$ while keeping the space complexity at $O(m^2)$, where $m$ is the number of variables and $n$ is the number of samples. While an approximation, our approach retains VarLiNGAM's essential structure and empirical reliability. On large-scale financial data with up to 400 variables, our algorithm achieves a 7--13x speedup over the standard implementation and a 4.5x speedup over a GPU-accelerated version. Evaluations across medical imaging, web server monitoring, and finance demonstrate the heuristic's robustness and practical scalability. This work offers a validated balance between computational efficiency and discovery quality, making large-scale causal analysis feasible on personal computers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05500v3</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <category>stat.CO</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyang Jiao, Ce Guo, Wayne Luk</dc:creator>
    </item>
    <item>
      <title>Scalable Bayesian Monte Carlo: fast uncertainty estimation beyond deep ensembles</title>
      <link>https://arxiv.org/abs/2505.13585</link>
      <description>arXiv:2505.13585v2 Announce Type: replace-cross 
Abstract: This work introduces a new method designed for Bayesian deep learning called scalable Bayesian Monte Carlo (SBMC). The method is comprised of a model and an algorithm. The model interpolates between a point estimator and the posterior. The algorithm is a parallel implementation of sequential Monte Carlo sampler (SMC$_\parallel$) or Markov chain Monte Carlo (MCMC$_\parallel$). We collectively refer to these consistent (asymptotically unbiased) algorithms as Bayesian Monte Carlo (BMC), and any such algorithm can be used in our SBMC method. The utility of the method is demonstrated on practical examples: MNIST, CIFAR, IMDb. A systematic numerical study reveals that for the same wall-clock time as state-of-the-art (SOTA) methods like deep ensembles (DE), SBMC achieves comparable or better accuracy and substantially improved uncertainty quantification (UQ)--in particular, epistemic UQ. This is demonstrated on the downstream task of estimating the confidence in predictions, which can be used for reliability assessment or abstention decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13585v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinzhu Liang, Joseph M. Lukens, Sanjaya Lohani, Brian T. Kirby, Thomas A. Searles, Xin Qiu, Kody J. H. Law</dc:creator>
    </item>
    <item>
      <title>The ECME Algorithm Using Factor Analysis for DOA Estimation in Nonuniform Noise</title>
      <link>https://arxiv.org/abs/2508.02223</link>
      <description>arXiv:2508.02223v2 Announce Type: replace-cross 
Abstract: Maximum likelihood factor analysis has been applied to direction of arrival (DOA) estimation in unknown nonuniform noise and a variety of iterative approaches have been developed. In particular, the Factor Analysis for Anisotropic Noise (FAAN) method proposed by Stoica and Babu has excellent convergence properties. In this article, the Expectation/Conditional Maximization Either (ECME) algorithm, an extension of the expectation-maximization algorithm, is designed, which has almost the same computational complexity at each iteration as the FAAN method. However, numerical results show that the ECME algorithm yields faster stable convergence and is computationally more efficient. Importantly, the signal subspace estimated by the ECME algorithm can be employed for the subspace based DOA estimation in unknown nonuniform noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02223v2</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyan Gong</dc:creator>
    </item>
  </channel>
</rss>
