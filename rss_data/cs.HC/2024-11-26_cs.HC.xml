<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Nov 2024 02:56:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>ChatBCI: A P300 Speller BCI Leveraging Large Language Models for Improved Sentence Composition in Realistic Scenarios</title>
      <link>https://arxiv.org/abs/2411.15395</link>
      <description>arXiv:2411.15395v1 Announce Type: new 
Abstract: P300 speller BCIs allow users to compose sentences by selecting target keys on a GUI through the detection of P300 component in their EEG signals following visual stimuli. Most P300 speller BCIs require users to spell words letter by letter, or the first few initial letters, resulting in high keystroke demands that increase time, cognitive load, and fatigue. This highlights the need for more efficient, user-friendly methods for faster sentence composition. In this work, we introduce ChatBCI, a P300 speller BCI that leverages the zero-shot learning capabilities of large language models (LLMs) to suggest words from user-spelled initial letters or predict the subsequent word(s), reducing keystrokes and accelerating sentence composition. ChatBCI retrieves word suggestions through remote queries to the GPT-3.5 API. A new GUI, displaying GPT-3.5 word suggestions as extra keys is designed. SWLDA is used for the P300 classification. Seven subjects completed two online spelling tasks: 1) copy-spelling a self-composed sentence using ChatBCI, and 2) improvising a sentence using ChatBCI's word suggestions. Results demonstrate that in Task 1, on average, ChatBCI outperforms letter-by-letter BCI spellers, reducing time and keystrokes by 62.14% and 53.22%, respectively, and increasing information transfer rate by 198.96%. In Task 2, ChatBCI achieves 80.68% keystroke savings and a record 8.53 characters/min for typing speed. Overall, ChatBCI, by employing remote LLM queries, enhances sentence composition in realistic scenarios, significantly outperforming traditional spellers without requiring local model training or storage. ChatBCI's (multi-) word predictions, combined with its new GUI, pave the way for developing next-generation speller BCIs that are efficient and effective for real-time communication, especially for users with communication and motor disabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15395v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiazhen Hong, Weinan Wang, Laleh Najafizadeh</dc:creator>
    </item>
    <item>
      <title>Continuity Reinforcement Skeleton for Pixel-based Haptic Display</title>
      <link>https://arxiv.org/abs/2411.15445</link>
      <description>arXiv:2411.15445v1 Announce Type: new 
Abstract: Haptic displays are crucial for facilitating an immersive experience within virtual reality. However, when displaying continuous movements of contact, such as stroking and exploration, pixel-based haptic devices suffer from losing haptic information between pixels, leading to discontinuity. The trade-off between the travel distance of haptic elements and their pixel size in thin wearable devices hinders solutions that solely rely on increasing pixel density. Here we introduce a continuity reinforcement skeleton (CRS), which employs physically driven interpolation to enhance haptic information. The CRS enables the off-plane displacement to move conformally and display haptic information between pixel gaps. Efforts are made to quantify haptic display quality using geometric, mechanical, and psychological criteria. The development and integration of one-dimensional (1D), two-dimensional (2D), and curved CRS devices with virtual reality systems highlight the impact of CRS on haptic display, showcasing its potential for improving haptic experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15445v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xinyuan Wang, Zhiqiang Meng, Chang Qing Chen</dc:creator>
    </item>
    <item>
      <title>That Flick is Sick: Gyroscope Integration in Xbox Controllers</title>
      <link>https://arxiv.org/abs/2411.15538</link>
      <description>arXiv:2411.15538v1 Announce Type: new 
Abstract: Gyroscope integration in Xbox controllers offers new possibilities for enhancing gaming experiences, particularly in first-person shooter (FPS) games. To investigate its potential, we conducted an empirical study with 11 participants, comparing aim precision and reaction times across three input methods: a computer mouse, a standard Xbox controller, and a gyroscope-enabled controller. Participants completed an aim training task, revealing the mouse as the most accurate device, followed by the standard controller. Interestingly, the gyroscope-enabled controller showed reduced accuracy and slower reaction times, attributed to challenges in sensitivity and control. Participant feedback highlighted areas for improvement, including refined sensitivity settings, control stability, and software design. These findings underscore the need for design innovations, such as camera rotation limits and optimized sensitivity thresholds, to make gyroscope-enabled controllers more competitive. Future work should consider diverse gamer profiles and extended evaluation contexts to better understand the role of gyroscopes in gaming interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15538v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of CHIRP 2024: Transforming HCI Research in the Philippines Workshop</arxiv:journal_reference>
      <dc:creator>Jhervey Edric Cheng, Stacy Selena Kalaw, James Patrick Kok, Alyssa Ysabelle Meneses, Richard Sy, Jordan Aiko Deja</dc:creator>
    </item>
    <item>
      <title>Exploring Viewing Modalities in Cinematic Virtual Reality: A Systematic Review and Meta-Analysis of Challenges in Evaluating User Experience</title>
      <link>https://arxiv.org/abs/2411.15583</link>
      <description>arXiv:2411.15583v1 Announce Type: new 
Abstract: Cinematic Virtual Reality (CVR) is a narrative-driven VR experience that uses head-mounted displays with a 360-degree field of view. Previous research has explored different viewing modalities to enhance viewers' CVR experience. This study conducted a systematic review and meta-analysis focusing on how different viewing modalities, including intervened rotation, avatar assistance, guidance cues, and perspective shifting, influence the CVR experience. The study has screened 3444 papers (between 01/01/2013 and 17/06/2023) and selected 45 for systematic review, 13 of which also for meta-analysis. We conducted separate random-effects meta-analysis and applied Robust Variance Estimation to examine CVR viewing modalities and user experience outcomes. Evidence from experiments was synthesized as differences between standardized mean differences (SMDs) of user experience of control group ("Swivel-Chair" CVR) and experiment groups. To our surprise, we found inconsistencies in the effect sizes across different studies, even with the same viewing modalities. Moreover, in these studies, terms such as "presence," "immersion," and "narrative engagement" were often used interchangeably. Their irregular use of questionnaires, overreliance on self-developed questionnaires, and incomplete data reporting may have led to unrigorous evaluations of CVR experiences. This study contributes to Human-Computer Interaction (HCI) research by identifying gaps in CVR research, emphasizing the need for standardization of terminologies and methodologies to enhance the reliability and comparability of future CVR research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15583v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yawen Zhang, Han Zhou, Zhoumingju Jiang, Zilu Tang, Tao Luo, Qinyuan Lei</dc:creator>
    </item>
    <item>
      <title>Medillustrator: Improving Retrospective Learning in Physicians' Continuous Medical Education via Multimodal Diagnostic Data Alignment and Representation</title>
      <link>https://arxiv.org/abs/2411.15593</link>
      <description>arXiv:2411.15593v1 Announce Type: new 
Abstract: Continuous Medical Education (CME) plays a vital role in physicians' ongoing professional development. Beyond immediate diagnoses, physicians utilize multimodal diagnostic data for retrospective learning, engaging in self-directed analysis and collaborative discussions with peers. However, learning from such data effectively poses challenges for novice physicians, including screening and identifying valuable research cases, achieving fine-grained alignment and representation of multimodal data at the semantic level, and conducting comprehensive contextual analysis aided by reference data. To tackle these challenges, we introduce Medillustrator, a visual analytics system crafted to facilitate novice physicians' retrospective learning. Our structured approach enables novice physicians to explore and review research cases at an overview level and analyze specific cases with consistent alignment of multimodal and reference data. Furthermore, physicians can record and review analyzed results to facilitate further retrospection. The efficacy of Medillustrator in enhancing physicians' retrospective learning processes is demonstrated through a comprehensive case study and a controlled in-lab between-subject user study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15593v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuansong Xu, Jiahe Dong, Yijie Fan, Yuheng Shao, Chang Jiang, Lixia Jin, Yuanwu Cao, Quan Li</dc:creator>
    </item>
    <item>
      <title>Chatting with a Learning Analytics Dashboard: The Role of Generative AI Literacy on Learner Interaction with Conventional and Scaffolding Chatbots</title>
      <link>https://arxiv.org/abs/2411.15597</link>
      <description>arXiv:2411.15597v1 Announce Type: new 
Abstract: Learning analytics dashboards (LADs) simplify complex learner data into accessible visualisations, providing actionable insights for educators and students. However, their educational effectiveness has not always matched the sophistication of the technology behind them. Explanatory and interactive LADs, enhanced by generative AI (GenAI) chatbots, hold promise by enabling dynamic, dialogue-based interactions with data visualisations and offering personalised feedback through text. Yet, the effectiveness of these tools may be limited by learners' varying levels of GenAI literacy, a factor that remains underexplored in current research. This study investigates the role of GenAI literacy in learner interactions with conventional (reactive) versus scaffolding (proactive) chatbot-assisted LADs. Through a comparative analysis of 81 participants, we examine how GenAI literacy is associated with learners' ability to interpret complex visualisations and their cognitive processes during interactions with chatbot-assisted LADs. Results show that while both chatbots significantly improved learner comprehension, those with higher GenAI literacy benefited the most, particularly with conventional chatbots, demonstrating diverse prompting strategies. Findings highlight the importance of considering learners' GenAI literacy when integrating GenAI chatbots in LADs and educational technologies. Incorporating scaffolding techniques within GenAI chatbots can be an effective strategy, offering a more guided experience that reduces reliance on learners' GenAI literacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15597v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yueqiao Jin, Kaixun Yang, Lixiang Yan, Vanessa Echeverria, Linxuan Zhao, Riordan Alfredo, Mikaela Milesi, Jie Fan, Xinyu Li, Dragan Ga\v{s}evi\'c, Roberto Martinez-Maldonado</dc:creator>
    </item>
    <item>
      <title>Optimizing Gesture Recognition for Seamless UI Interaction Using Convolutional Neural Networks</title>
      <link>https://arxiv.org/abs/2411.15598</link>
      <description>arXiv:2411.15598v1 Announce Type: new 
Abstract: This study introduces an advanced gesture recognition and user interface (UI) interaction system powered by deep learning, highlighting its transformative impact on UI design and functionality. By utilizing optimized convolutional neural networks (CNNs), the system achieves high-precision gesture recognition, significantly improving user interactions with digital interfaces. The process begins with preprocessing collected gesture images to meet CNN input requirements, followed by sophisticated feature extraction and classification techniques. To address class imbalance, we employ Focal Loss as the loss function, ensuring robust model performance across diverse gesture types. Experimental results demonstrate notable improvements in model metrics, with the Area Under the Curve (AUC) and Recall metrics improving as we transition from simpler models like VGG16 to more advanced ones such as DenseNet. Our enhanced model achieves strong AUC and Recall values, outperforming standard benchmarks. Notably, the system's ability to support real-time and efficient gesture recognition paves the way for a new era in UI design, where intuitive user gestures can be seamlessly integrated into everyday technology use, reducing the learning curve and enhancing user satisfaction. The implications of this development extend beyond technical performance to fundamentally reshape user-technology interactions, underscoring the critical role of gesture-based interfaces in the next generation of UI development. Such advancements promise to significantly enhance smart life experiences, positioning gesture recognition as a key driver in the evolution of user-centric interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15598v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi Sun, Tong Zhang, Shang Gao, Liuqingqing Yang, Fenghua Shao</dc:creator>
    </item>
    <item>
      <title>Limitations of Online Play Content for Parents of Infants and Toddlers</title>
      <link>https://arxiv.org/abs/2411.15783</link>
      <description>arXiv:2411.15783v1 Announce Type: new 
Abstract: Play is a fundamental aspect of developmental growth, yet many parents encounter significant challenges in fulfilling their caregiving roles in this area. As online content increasingly serves as the primary source of parental guidance, this study investigates the difficulties parents face related to play and evaluates the limitations of current online content. We identified ten findings through in-depth interviews with nine parents who reported struggles in engaging with their children during play. Based on these findings, we discuss the major limitations of online play content and suggest how they can be improved. These recommendations include minimizing parental anxiety, accommodating diverse play scenarios, providing credible and personalized information, encouraging creativity, and delivering the same content in multiple formats.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15783v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keunwoo Park, Subin Ahn, Mina Jung, You Jung Cho, Seulah Jeong, Cheong-Ah Huh</dc:creator>
    </item>
    <item>
      <title>Over-the-Air Federated Adaptive Data Analysis: Preserving Accuracy via Opportunistic Differential Privacy</title>
      <link>https://arxiv.org/abs/2411.15948</link>
      <description>arXiv:2411.15948v1 Announce Type: new 
Abstract: Adaptive data analysis (ADA) involves a dynamic interaction between an analyst and a dataset owner, where the analyst submits queries sequentially, adapting them based on previous answers. This process can become adversarial, as the analyst may attempt to overfit by targeting non-generalizable patterns in the data. To counteract this, the dataset owner introduces randomization techniques, such as adding noise to the responses. This noise not only helps prevent overfitting but also enhances data privacy. However, it must be carefully calibrated to ensure that the statistical reliability of the responses is not compromised. In this paper, we extend the ADA problem to the context of distributed datasets. Specifically, we consider a scenario where a potentially adversarial analyst interacts with multiple distributed responders through adaptive queries. We assume that the responses are subject to noise introduced by the channel connecting the responders and the analyst. We demonstrate how, through a federated mechanism, this noise can be opportunistically leveraged to enhance the generalizability of ADA, thereby increasing the number of query-response interactions between the analyst and the responders. We illustrate that careful tuning of the transmission amplitude, based on the theoretically achievable bounds, can significantly impact the number of accurately answerable queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15948v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Amir Hossein Hadavi, Mohammad M. Mojahedian, Mohammad Reza Aref</dc:creator>
    </item>
    <item>
      <title>Auto-calibrated Wearable System for Load Vertical Location Estimation During Manual Lifting</title>
      <link>https://arxiv.org/abs/2411.16097</link>
      <description>arXiv:2411.16097v1 Announce Type: new 
Abstract: Lifting during manual material handling is a major cause of low-back pain (LBP). As an important risk factor that directly influences the risk of LBP, the Load vertical location (LVL) during lifting needs to be measured and controlled. However, existing solutions for LVL measurement are inefficient, inaccurate, and impractical for real-world workplace environments. To address these problems, an unobtrusive wearable system, including smart insoles and smart wristbands, was proposed to measure LVL accurately in workplace environments. Different from traditional methods which rely on Inertial Measurement Unit (IMU) and suffer from integral drifting errors over time, a novel barometer-based LVL measurement method was proposed in this study. To correct the environment-induced LVL measurement errors in the barometer-based method, a novel Known Vertical Location Update (KVLU) method was proposed. This method calibrates the measured LVL using a known wrist vertical location at known postures during frequently used daily activities such as standing and walking. The proposed wearable system achieved a mean absolute error (MAE) of 5.71 cm in LVL measurement. This result indicates that the proposed system has the potential to reliably measure LVL and assess the risk of LBP in manual lifting tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16097v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Diliang Chen, Nozhan Ghoreishi, John LaCourse, Sajay Arthanat, Dain LaRoche</dc:creator>
    </item>
    <item>
      <title>Collaboration in Virtual Reality: Survey and Perspectives</title>
      <link>https://arxiv.org/abs/2411.16124</link>
      <description>arXiv:2411.16124v1 Announce Type: new 
Abstract: The application of Virtual Reality Environments (VRE) has been gaining momentum as a relatively new tool to assist with mitigating various difficulties including abstractness of concepts, lack of user engagement, perception of disconnection from other users. A VRE may offer both synchronous and asynchronous experiences, in addition to an immersive environment which promotes users' engagement. Past research has shown that, in general, VRE do improve the experiences they try to enhance in many aspects of human activity. Terms like immersiveness and 3D representation of real life objects and environments are, as it appears, the two most obvious positive effects of Virtual Reality (VR) applications. However, despite these benefits it does not come without challenges. The main three concepts/challenges are the spatial design, the collaboration interaction between its members and the VRE, and the audio and video fidelity. Each of the three includes a number of other components that should be addressed for the total experience to be fine-tuned. These include mutual embodiment and shared perspectives, teleportation, gestural interaction, symmetric and asymmetric collaboration, physical and virtual co-location, inventory, and time and spatial synchronization. This paper comprises a survey of the literature, that identifies and explains the features introduced and the challenges involved with the VREs, and furthermore provides various interesting future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16124v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ourania Koutzampasopoulou Xanthidou, Nadine Aburumman, Han\^ene Ben-Abdallah</dc:creator>
    </item>
    <item>
      <title>A No Free Lunch Theorem for Human-AI Collaboration</title>
      <link>https://arxiv.org/abs/2411.15230</link>
      <description>arXiv:2411.15230v1 Announce Type: cross 
Abstract: The gold standard in human-AI collaboration is complementarity -- when combined performance exceeds both the human and algorithm alone. We investigate this challenge in binary classification settings where the goal is to maximize 0-1 accuracy. Given two or more agents who can make calibrated probabilistic predictions, we show a "No Free Lunch"-style result. Any deterministic collaboration strategy (a function mapping calibrated probabilities into binary classifications) that does not essentially always defer to the same agent will sometimes perform worse than the least accurate agent. In other words, complementarity cannot be achieved "for free." The result does suggest one model of collaboration with guarantees, where one agent identifies "obvious" errors of the other agent. We also use the result to understand the necessary conditions enabling the success of other collaboration techniques, providing guidance to human-AI collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15230v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenny Peng, Nikhil Garg, Jon Kleinberg</dc:creator>
    </item>
    <item>
      <title>Is Attention All You Need For Actigraphy? Foundation Models of Wearable Accelerometer Data for Mental Health Research</title>
      <link>https://arxiv.org/abs/2411.15240</link>
      <description>arXiv:2411.15240v2 Announce Type: cross 
Abstract: Wearable accelerometry (actigraphy) has provided valuable data for clinical insights since the 1970s and is increasingly important as wearable devices continue to become widespread. The effectiveness of actigraphy in research and clinical contexts is heavily dependent on the modeling architecture utilized. To address this, we developed the Pretrained Actigraphy Transformer (PAT)--the first pretrained and fully attention-based model designed specifically to handle actigraphy. PAT was pretrained on actigraphy from 29,307 participants in NHANES, enabling it to deliver state-of-the-art performance when fine-tuned across various actigraphy prediction tasks in the mental health domain, even in data-limited scenarios. For example, when trained to predict benzodiazepine usage using actigraphy from only 500 labeled participants, PAT achieved an 8.8 percentage-point AUC improvement over the best baseline. With fewer than 2 million parameters and built-in model explainability, PAT is robust yet easy to deploy in health research settings.
  GitHub: https://github.com/njacobsonlab/Pretrained-Actigraphy-Transformer/</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15240v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Franklin Y. Ruan, Aiwei Zhang, Jenny Y. Oh, SouYoung Jin, Nicholas C Jacobson</dc:creator>
    </item>
    <item>
      <title>The Decoy Dilemma in Online Medical Information Evaluation: A Comparative Study of Credibility Assessments by LLM and Human Judges</title>
      <link>https://arxiv.org/abs/2411.15396</link>
      <description>arXiv:2411.15396v1 Announce Type: cross 
Abstract: Can AI be cognitively biased in automated information judgment tasks? Despite recent progresses in measuring and mitigating social and algorithmic biases in AI and large language models (LLMs), it is not clear to what extent LLMs behave "rationally", or if they are also vulnerable to human cognitive bias triggers. To address this open problem, our study, consisting of a crowdsourcing user experiment and a LLM-enabled simulation experiment, compared the credibility assessments by LLM and human judges under potential decoy effects in an information retrieval (IR) setting, and empirically examined the extent to which LLMs are cognitively biased in COVID-19 medical (mis)information assessment tasks compared to traditional human assessors as a baseline. The results, collected from a between-subject user experiment and a LLM-enabled replicate experiment, demonstrate that 1) Larger and more recent LLMs tend to show a higher level of consistency and accuracy in distinguishing credible information from misinformation. However, they are more likely to give higher ratings for misinformation due to the presence of a more salient, decoy misinformation result; 2) While decoy effect occurred in both human and LLM assessments, the effect is more prevalent across different conditions and topics in LLM judgments compared to human credibility ratings. In contrast to the generally assumed "rationality" of AI tools, our study empirically confirms the cognitive bias risks embedded in LLM agents, evaluates the decoy impact on LLMs against human credibility assessments, and thereby highlights the complexity and importance of debiasing AI agents and developing psychology-informed AI audit techniques and policies for automated judgment tasks and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15396v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiqun Liu, Jiangen He</dc:creator>
    </item>
    <item>
      <title>Instruct or Interact? Exploring and Eliciting LLMs' Capability in Code Snippet Adaptation Through Prompt Engineering</title>
      <link>https://arxiv.org/abs/2411.15501</link>
      <description>arXiv:2411.15501v1 Announce Type: cross 
Abstract: Code snippet adaptation is a fundamental activity in the software development process. Unlike code generation, code snippet adaptation is not a "free creation", which requires developers to tailor a given code snippet in order to fit specific requirements and the code context. Recently, large language models (LLMs) have confirmed their effectiveness in the code generation task with promising results. However, their performance on adaptation, a reuse-oriented and context-dependent code change prediction task, is still unclear. To bridge this gap, we conduct an empirical study to investigate the performance and issues of LLMs on the adaptation task. We first evaluate the adaptation performances of three popular LLMs and compare them to the code generation task. Our result indicates that their adaptation ability is weaker than generation, with a nearly 15% decrease on pass@1 and more context-related errors. By manually inspecting 200 cases, we further investigate the causes of LLMs' sub-optimal performance, which can be classified into three categories, i.e., Unclear Requirement, Requirement Misalignment and Context Misapplication. Based on the above empirical research, we propose an interactive prompting approach to eliciting LLMs' adaptation ability. Experimental result reveals that our approach greatly improve LLMs' adaptation performance. The best-performing Human-LLM interaction successfully solves 159 out of the 202 identified defects and improves the pass@1 and pass@5 by over 40% compared to the initial instruction-based prompt. Considering human efforts, we suggest multi-agent interaction as a trade-off, which can achieve comparable performance with excellent generalization ability. We deem that our approach could provide methodological assistance for autonomous code snippet reuse and adaptation with LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15501v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tanghaoran Zhang, Yue Yu, Xinjun Mao, Shangwen Wang, Kang Yang, Yao Lu, Zhang Zhang, Yuxin Zhao</dc:creator>
    </item>
    <item>
      <title>QEQR: An Exploration of Query Expansion Methods for Question Retrieval in CQA Services</title>
      <link>https://arxiv.org/abs/2411.15530</link>
      <description>arXiv:2411.15530v1 Announce Type: cross 
Abstract: CQA services are valuable sources of knowledge that can be used to find answers to users' information needs. In these services, question retrieval aims to help users with their information needs by finding similar questions to theirs. However, finding similar questions is obstructed by the lexical gap that exists between relevant questions. In this work, we target this problem by using query expansion methods. We use word-similarity-based methods, propose a question-similarity-based method and selective expansion of these methods to expand a question that's been submitted and mitigate the lexical gap problem. Our best method achieves a significant relative improvement of 1.8\% compared to the best-performing baseline without query expansion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15530v1</guid>
      <category>cs.IR</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yasin Ghafourian, Sajad Movahedi, Azadeh Shakery</dc:creator>
    </item>
    <item>
      <title>Development of a Low-Cost Prosthetic Hand Using Electromyography and Machine Learning</title>
      <link>https://arxiv.org/abs/2411.15533</link>
      <description>arXiv:2411.15533v1 Announce Type: cross 
Abstract: Electromyography (EMG) is a measure of muscular electrical activity and is used in many clinical/biomedical disciplines and modern human computer interaction. Myo-electric prosthetics analyze and classify the electrical signals recorded from the residual limb. The classified output is then used to control the position of motors in a robotic hand and a movement is produced. The aim of this project is to develop a low-cost and effective myo-electric prosthetic hand that would meet the needs of amputees in developing countries. The proposed prosthetic hand should be able to accurately classify five different patterns (gestures) using EMG recordings from three muscles and control a robotic hand accordingly. The robotic hand is composed of two servo motors allowing for two degrees of freedom. After establishing an efficient signal acquisition and amplification system, EMG signals were thoroughly analyzed in the frequency and time domain. Features were extracted from both domains and a shallow neural network was trained on the two sets of data. Results yielded an average classification accuracy of 97.25% and 95.85% for the time and frequency domains respectively. Furthermore, results showed a faster computation and response for the time domain analysis; hence, it was adopted for the classification system. A wrist rotation mechanism was designed and tested to add significant functionality to the prosthetic. The mechanism is controlled by two of the five gestures, one for each direction. Which added a third degree of freedom to the overall design. Finally, a tactile sensory feedback system which uses force sensors and vibration motors was developed to enable sensation of the force inflicted on the hand for the user.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15533v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mosab Diab, Ashraf Mohammed, Yinlai Jiang</dc:creator>
    </item>
    <item>
      <title>Teaching Shortest Path Algorithms With a Robot and Overlaid Projections</title>
      <link>https://arxiv.org/abs/2411.15535</link>
      <description>arXiv:2411.15535v1 Announce Type: cross 
Abstract: Robots have the potential to enhance teaching of advanced computer science topics, making abstract concepts more tangible and interactive. In this paper, we present Timmy-a GoPiGo robot augmented with projections to demonstrate shortest path algorithms in an interactive learning environment. We integrated a JavaScript-based application that is projected around the robot, which allows users to construct graphs and visualise three different shortest path algorithms with colour-coded edges and vertices. Animated graph exploration and traversal are augmented by robot movements. To evaluate Timmy, we conducted two user studies. An initial study (n=10) to explore the feasibility of this type of teaching where participants were just observing both robot-synced and the on-screen-only visualisations. And a pilot study (n=6) where participants actively interacted with the system, constructed graphs and selected desired algorithms. In both studies we investigated the preferences towards the system and not the teaching outcome. Initial findings suggest that robots offer an engaging tool for teaching advanced algorithmic concepts, but highlight the need for further methodological refinements and larger-scale studies to fully evaluate their effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15535v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>HCI SI 2024: Human-Computer Interaction Slovenia 2024, November 8th, 2024, Ljubljana, Slovenia</arxiv:journal_reference>
      <dc:creator>Pavel Jolakoski, Jordan Aiko Deja, Klen \v{C}opi\v{c} Pucihar, Matja\v{z} Kljun</dc:creator>
    </item>
    <item>
      <title>From Complexity to Parsimony: Integrating Latent Class Analysis to Uncover Multimodal Learning Patterns in Collaborative Learning</title>
      <link>https://arxiv.org/abs/2411.15590</link>
      <description>arXiv:2411.15590v1 Announce Type: cross 
Abstract: Multimodal Learning Analytics (MMLA) leverages advanced sensing technologies and artificial intelligence to capture complex learning processes, but integrating diverse data sources into cohesive insights remains challenging. This study introduces a novel methodology for integrating latent class analysis (LCA) within MMLA to map monomodal behavioural indicators into parsimonious multimodal ones. Using a high-fidelity healthcare simulation context, we collected positional, audio, and physiological data, deriving 17 monomodal indicators. LCA identified four distinct latent classes: Collaborative Communication, Embodied Collaboration, Distant Interaction, and Solitary Engagement, each capturing unique monomodal patterns. Epistemic network analysis compared these multimodal indicators with the original monomodal indicators and found that the multimodal approach was more parsimonious while offering higher explanatory power regarding students' task and collaboration performances. The findings highlight the potential of LCA in simplifying the analysis of complex multimodal data while capturing nuanced, cross-modality behaviours, offering actionable insights for educators and enhancing the design of collaborative learning interventions. This study proposes a pathway for advancing MMLA, making it more parsimonious and manageable, and aligning with the principles of learner-centred education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15590v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lixiang Yan, Dragan Ga\v{s}evi\'c, Linxuan Zhao, Vanessa Echeverria, Yueqiao Jin, Roberto Martinez-Maldonado</dc:creator>
    </item>
    <item>
      <title>Achieving employees agile response in e-governance: Exploring the synergy of technology and group collaboration</title>
      <link>https://arxiv.org/abs/2411.15875</link>
      <description>arXiv:2411.15875v1 Announce Type: cross 
Abstract: The transformation of technology and collaboration methods driven by the e-government system forces government employees to reconsider their daily workflow and collaboration with colleagues. Despite the extensive existing knowledge of technology usage and collaboration, there are limitations in explaining the synergy between technology usage and group collaboration in achieving agile response from the perspective of government employees, particularly in the e-government setting. To address these challenges, this study provides a holistic understanding of the successful pathway to agile response in e-governance from the perspective of government employees. This study explores a dual path to achieve agile response in e-governance through qualitative analysis, involving 34 in-depth semi-structured interviews with government employees in several government sectors in China. By employing three rounds of coding processes and adopting Interpretative Structural Modeling (ISM), this study identifies the five-layer mechanisms leading to agile response in e-governance, considering both government employee technology usage and group collaboration perspectives. Findings of this study provides suggestions and implications for achieving agile response in e-governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15875v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10726-024-09911-y</arxiv:DOI>
      <arxiv:journal_reference>Group Decision and Negotiation (2024)</arxiv:journal_reference>
      <dc:creator>Ying Bao, Xusen Cheng, Linlin Su, Alex Zarifis</dc:creator>
    </item>
    <item>
      <title>Can AI grade your essays? A comparative analysis of large language models and teacher ratings in multidimensional essay scoring</title>
      <link>https://arxiv.org/abs/2411.16337</link>
      <description>arXiv:2411.16337v1 Announce Type: cross 
Abstract: The manual assessment and grading of student writing is a time-consuming yet critical task for teachers. Recent developments in generative AI, such as large language models, offer potential solutions to facilitate essay-scoring tasks for teachers. In our study, we evaluate the performance and reliability of both open-source and closed-source LLMs in assessing German student essays, comparing their evaluations to those of 37 teachers across 10 pre-defined criteria (i.e., plot logic, expression). A corpus of 20 real-world essays from Year 7 and 8 students was analyzed using five LLMs: GPT-3.5, GPT-4, o1, LLaMA 3-70B, and Mixtral 8x7B, aiming to provide in-depth insights into LLMs' scoring capabilities. Closed-source GPT models outperform open-source models in both internal consistency and alignment with human ratings, particularly excelling in language-related criteria. The novel o1 model outperforms all other LLMs, achieving Spearman's $r = .74$ with human assessments in the overall score, and an internal consistency of $ICC=.80$. These findings indicate that LLM-based assessment can be a useful tool to reduce teacher workload by supporting the evaluation of essays, especially with regard to language-related criteria. However, due to their tendency for higher scores, the models require further refinement to better capture aspects of content quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16337v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kathrin Se{\ss}ler, Maurice F\"urstenberg, Babette B\"uhler, Enkelejda Kasneci</dc:creator>
    </item>
    <item>
      <title>Inference-Time Policy Steering through Human Interactions</title>
      <link>https://arxiv.org/abs/2411.16627</link>
      <description>arXiv:2411.16627v1 Announce Type: cross 
Abstract: Generative policies trained with human demonstrations can autonomously accomplish multimodal, long-horizon tasks. However, during inference, humans are often removed from the policy execution loop, limiting the ability to guide a pre-trained policy towards a specific sub-goal or trajectory shape among multiple predictions. Naive human intervention may inadvertently exacerbate distribution shift, leading to constraint violations or execution failures. To better align policy output with human intent without inducing out-of-distribution errors, we propose an Inference-Time Policy Steering (ITPS) framework that leverages human interactions to bias the generative sampling process, rather than fine-tuning the policy on interaction data. We evaluate ITPS across three simulated and real-world benchmarks, testing three forms of human interaction and associated alignment distance metrics. Among six sampling strategies, our proposed stochastic sampling with diffusion policy achieves the best trade-off between alignment and distribution shift. Videos are available at https://yanweiw.github.io/itps/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16627v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yanwei Wang, Lirui Wang, Yilun Du, Balakumar Sundaralingam, Xuning Yang, Yu-Wei Chao, Claudia Perez-D'Arpino, Dieter Fox, Julie Shah</dc:creator>
    </item>
    <item>
      <title>Text2Gestures: A Transformer-Based Network for Generating Emotive Body Gestures for Virtual Agents</title>
      <link>https://arxiv.org/abs/2101.11101</link>
      <description>arXiv:2101.11101v3 Announce Type: replace 
Abstract: We present Text2Gestures, a transformer-based learning method to interactively generate emotive full-body gestures for virtual agents aligned with natural language text inputs. Our method generates emotionally expressive gestures by utilizing the relevant biomechanical features for body expressions, also known as affective features. We also consider the intended task corresponding to the text and the target virtual agents' intended gender and handedness in our generation pipeline. We train and evaluate our network on the MPI Emotional Body Expressions Database and observe that our network produces state-of-the-art performance in generating gestures for virtual agents aligned with the text for narration or conversation. Our network can generate these gestures at interactive rates on a commodity GPU. We conduct a web-based user study and observe that around 91% of participants indicated our generated gestures to be at least plausible on a five-point Likert Scale. The emotions perceived by the participants from the gestures are also strongly positively correlated with the corresponding intended emotions, with a minimum Pearson coefficient of 0.77 in the valence dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.11101v3</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/VR50410.2021.00037</arxiv:DOI>
      <arxiv:journal_reference>IEEEVR 2021, pp. 1-10</arxiv:journal_reference>
      <dc:creator>Uttaran Bhattacharya, Nicholas Rewkowski, Abhishek Banerjee, Pooja Guhan, Aniket Bera, Dinesh Manocha</dc:creator>
    </item>
    <item>
      <title>Understanding Parents' Perceptions and Practices Toward Children's Security and Privacy in Virtual Reality</title>
      <link>https://arxiv.org/abs/2403.06172</link>
      <description>arXiv:2403.06172v3 Announce Type: replace 
Abstract: Recent years have seen a sharp increase in the number of underage users in virtual reality (VR), where security and privacy (S\&amp;P) risks such as data surveillance and self-disclosure in social interaction have been increasingly prominent. Prior work shows children largely rely on parents to mitigate S\&amp;P risks in their technology use. Therefore, understanding parents' S\&amp;P knowledge, perceptions, and practices is critical for identifying the gaps for parents, technology designers, and policymakers to enhance children's S\&amp;P. While such empirical knowledge is substantial in other consumer technologies, it remains largely unknown in the context of VR. To address the gap, we conducted in-depth semi-structured interviews with 20 parents of children under the age of 18 who use VR at home. Our findings highlight parents generally lack S\&amp;P awareness due to the perception that VR is still in its infancy. To protect their children's interactions with VR, parents currently primarily rely on active strategies such as verbal education about S\&amp;P. Passive strategies such as using parental controls in VR are not commonly used among our interviewees, mainly due to their perceived technical constraints. Parents also highlight that a multi-stakeholder ecosystem must be established towards more S\&amp;P support for children in VR. Based on the findings, we propose actionable S\&amp;P recommendations for critical stakeholders, including parents, educators, VR companies, and governments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06172v3</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/SP54263.2024.00222</arxiv:DOI>
      <arxiv:journal_reference>2024 IEEE Symposium on Security and Privacy (SP), 2024, pp. 1554-1572</arxiv:journal_reference>
      <dc:creator>Jiaxun Cao, Abhinaya SB, Anupam Das, Pardis Emami-Naeini</dc:creator>
    </item>
    <item>
      <title>Metabook: A System to Automatically Generate Interactive AR Storybooks to Improve Children's Reading</title>
      <link>https://arxiv.org/abs/2405.13701</link>
      <description>arXiv:2405.13701v2 Announce Type: replace 
Abstract: Reading is important for children to acquire knowledge, enhance cognitive abilities, and improve language skills. However, current reading methods either offer limited visual presentation, making them less interesting to children, or lack channels for children to share insights and ask questions during reading. AR/VR books provide rich visual cues that address the issue of children's lack of interest in reading, but the high production costs and need for professional expertise limit the volume of AR/VR books and children's choices. We propose Metabook, a system to automatically generate interactive AR storybooks to improve children's reading. Metabook introduces a story-to-3D-book generation scheme and a 3D avatar that combines multiple AI models as a reading companion. We invited six primary and secondary school teachers to conduct a formative study to explore the design considerations for an ideal children's AR reading tool. In the user study, we invited relevant professionals (art, computer science professionals, and a semanticist), 44 children, and six teachers to evaluate Metabook. Our user study shows that Metabook can significantly increase children's interest in reading and deepen their impression of reading materials and vocabulary in books. Teachers acknowledged Metabook's effectiveness in facilitating reading communication and enhancing reading enthusiasm by connecting verbal and visual thinking, expressing high expectations for its future potential in education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13701v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yibo Wang, Yuanyuan Mao, Shi-ting Ni, Zeyu Want, Pan Hui</dc:creator>
    </item>
    <item>
      <title>The Human-GenAI Value Loop in Human-Centered Innovation: Beyond the Magical Narrative</title>
      <link>https://arxiv.org/abs/2407.17495</link>
      <description>arXiv:2407.17495v3 Announce Type: replace 
Abstract: Organizations across various industries are still exploring the potential of Generative Artificial Intelligence (GenAI) to enhance knowledge work. While innovation is often viewed as a product of individual creativity, it more commonly unfolds through a highly structured, collaborative process where creativity intertwines with knowledge work. However, the extent and effectiveness of GenAI in supporting this process remain open questions. Our study investigates this issue using a collaborative practice research approach focused on three GenAI-enabled innovation projects conducted over a year within three different organizations. We explored how, why, and when GenAI could be integrated into design sprints, a highly structured, collaborative, and human-centered innovation method. Our research identified challenges and opportunities in synchronizing AI capabilities with human intelligence and creativity. To translate these insights into practical strategies, we propose four recommendations for organizations eager to leverage GenAI to both streamline and bring more value to their innovation processes: (1) establish a collaborative intelligence value loop with GenAI; (2) build trust in GenAI, (3) develop robust data collection and curation workflows, and (4) cultivate a craftsmanship mindset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17495v3</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Camille Grange (HEC Montreal), Theophile Demazure (HEC Montreal), Mickael Ringeval (HEC Montreal), Simon Bourdeau (UQAM), Cedric Martineau (Carverinno)</dc:creator>
    </item>
    <item>
      <title>CliMB: An AI-enabled Partner for Clinical Predictive Modeling</title>
      <link>https://arxiv.org/abs/2410.03736</link>
      <description>arXiv:2410.03736v2 Announce Type: replace 
Abstract: Despite its significant promise and continuous technical advances, real-world applications of artificial intelligence (AI) remain limited. We attribute this to the "domain expert-AI-conundrum": while domain experts, such as clinician scientists, should be able to build predictive models such as risk scores, they face substantial barriers in accessing state-of-the-art (SOTA) tools. While automated machine learning (AutoML) has been proposed as a partner in clinical predictive modeling, many additional requirements need to be fulfilled to make machine learning accessible for clinician scientists.
  To address this gap, we introduce CliMB, a no-code AI-enabled partner designed to empower clinician scientists to create predictive models using natural language. CliMB guides clinician scientists through the entire medical data science pipeline, thus empowering them to create predictive models from real-world data in just one conversation. CliMB also creates structured reports and interpretable visuals. In evaluations involving clinician scientists and systematic comparisons against a baseline GPT-4, CliMB consistently demonstrated superior performance in key areas such as planning, error prevention, code execution, and model performance. Moreover, in blinded assessments involving 45 clinicians from diverse specialties and career stages, more than 80% preferred CliMB over GPT-4. Overall, by providing a no-code interface with clear guidance and access to SOTA methods in the fields of data-centric AI, AutoML, and interpretable ML, CliMB empowers clinician scientists to build robust predictive models.
  The proof-of-concept version of CliMB is available as open-source software on GitHub: https://github.com/vanderschaarlab/climb.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03736v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evgeny Saveliev, Tim Schubert, Thomas Pouplin, Vasilis Kosmoliaptsis, Mihaela van der Schaar</dc:creator>
    </item>
    <item>
      <title>Beyond correlation: The impact of human uncertainty in measuring the effectiveness of automatic evaluation and LLM-as-a-judge</title>
      <link>https://arxiv.org/abs/2410.03775</link>
      <description>arXiv:2410.03775v2 Announce Type: replace 
Abstract: The effectiveness of automatic evaluation of generative models is typically measured by comparing it to human evaluation using correlation metrics. However, metrics like Krippendorff's $\alpha$ and Randolph's $\kappa$, originally designed to measure the reliability of human labeling, make assumptions about human behavior and the labeling process. In this paper, we show how *relying on a single aggregate correlation score* can obscure fundamental differences between human behavior and automatic evaluation methods, including LLM-as-a-Judge. Specifically, we demonstrate that when the proportion of samples with variation or uncertainty in human labels (gathered during human evaluation) is relatively high, machine labels (generated by automatic evaluation methods) may superficially appear to have similar or better correlation with the human majority label compared to human-to-human (HH) correlation. This can create the illusion that automatic evaluation approximates the human majority label. However, as the proportion of samples with consistent human labels increases, the correlation between machine and human labels fall well below HH correlation. Based on these findings, we first propose stratifying results by human label uncertainty to provide a more robust analysis of automatic evaluation performance. Second, recognizing that uncertainty and variation are inherent in perception-based human evaluations, such as those involving attitudes or preferences, we introduce a new metric - *binned Jensen-Shannon Divergence for perception* for such scenarios to better measure the effectiveness of automatic evaluations. Third, we present visualization techniques -- *perception charts*, to compare the strengths and limitations of automatic evaluation and to contextualize correlation measures appropriately</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03775v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aparna Elangovan, Jongwoo Ko, Lei Xu, Mahsa Elyasi, Ling Liu, Sravan Bodapati, Dan Roth</dc:creator>
    </item>
    <item>
      <title>Scaling up the Evaluation of Collaborative Problem Solving: Promises and Challenges of Coding Chat Data with ChatGPT</title>
      <link>https://arxiv.org/abs/2411.10246</link>
      <description>arXiv:2411.10246v2 Announce Type: replace 
Abstract: Collaborative problem solving (CPS) is widely recognized as a critical 21st century skill. Efficiently coding communication data is a big challenge in scaling up research on assessing CPS. This paper reports the findings on using ChatGPT to directly code CPS chat data by benchmarking performance across multiple datasets and coding frameworks. We found that ChatGPT-based coding outperformed human coding in tasks where the discussions were characterized by colloquial languages but fell short in tasks where the discussions dealt with specialized scientific terminology and contexts. The findings offer practical guidelines for researchers to develop strategies for efficient and scalable analysis of communication data from CPS tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10246v2</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiangang Hao, Wenju Cui, Patrick Kyllonen, Emily Kerzabi, Lei Liu, Michael Flor</dc:creator>
    </item>
    <item>
      <title>Human-AI Co-Creativity: Exploring Synergies Across Levels of Creative Collaboration</title>
      <link>https://arxiv.org/abs/2411.12527</link>
      <description>arXiv:2411.12527v2 Announce Type: replace 
Abstract: Human-AI co-creativity represents a transformative shift in how humans and generative AI tools collaborate in creative processes. This chapter explores the synergies between human ingenuity and AI capabilities across four levels of interaction: Digital Pen, AI Task Specialist, AI Assistant, and AI Co-Creator. While earlier digital tools primarily facilitated creativity, generative AI systems now contribute actively, demonstrating autonomous creativity in producing novel and valuable outcomes. Empirical evidence from mathematics showcases how AI can extend human creative potential, from computational problem-solving to co-creative partnerships yielding breakthroughs in longstanding challenges. By analyzing these collaborations, the chapter highlights AI's potential to enhance human creativity without replacing it, underscoring the importance of balancing AI's contributions with human oversight and contextual understanding. This integration pushes the boundaries of creative achievements, emphasizing the need for human-centered AI systems that foster collaboration while preserving the unique qualities of human creativity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12527v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jennifer Haase, Sebastian Pokutta</dc:creator>
    </item>
    <item>
      <title>A qualitative analysis of remote patient monitoring: how a paradox mindset can support balancing emotional tensions in the design of healthcare technologies</title>
      <link>https://arxiv.org/abs/2411.14233</link>
      <description>arXiv:2411.14233v2 Announce Type: replace 
Abstract: Remote patient monitoring (RPM) is the use of digital technologies to improve patient care at a distance. However, current RPM solutions are often biased toward tech-savvy patients. To foster health equity, researchers have studied how to address the socio-economic and cognitive needs of diverse patient groups, but their emotional needs have remained largely neglected. We perform the first qualitative study to explore the emotional needs of diverse patients around RPM. Specifically, we conduct a thematic analysis of 18 interviews and 4 focus groups at a large US healthcare organization. We identify emotional needs that lead to four emotional tensions within and across stakeholder groups when applying an equity focus to the design and implementation of RPM technologies. The four emotional tensions are making diverse patients feel: (i) heard vs. exploited; (ii) seen vs. deprioritized for efficiency; (iii) empowered vs. anxious; and (iv) cared for vs. detached from care. To manage these emotional tensions across stakeholders, we develop design recommendations informed by a paradox mindset (i.e., "both-and" rather than "and-or" strategies).</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14233v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zoe Jonassen, Katharine Lawrence, Batia Mishan Wiesenfeld, Stefan Feuerriegel, Devin Mann</dc:creator>
    </item>
    <item>
      <title>Decoding the Meaning of Success on Digital Labor Platforms: Worker-Centered Perspectives</title>
      <link>https://arxiv.org/abs/2411.14298</link>
      <description>arXiv:2411.14298v2 Announce Type: replace 
Abstract: What does work and career success mean for those who secure their work using digital labor platforms? Traditional research on success predominantly relies on organizationally-centric benchmarks, such as promotions and income. These measures provide limited insights into the evolving nature of work and careers shaped at the intersection of digital labor platform technologies and workers' evolving perspectives. Drawing on data from a longitudinal study of 108 digital labor platform workers on Upwork, we (1) identify seven dimensions of success indicators that reflect workers' definitions of success in platform-mediated work and careers, (2) delineate three dimensions of digital labor platforms mediating workers' experiences of success and (3) examine the shifting perspectives of these workers relative to success. Based on these findings, we discuss the implications of platform-mediated success in workers' labor experiences, marked by platformic management, standardization, precarity and ongoing evolution. Our discussion intertwines CSCW scholarship with career studies, advancing a more nuanced understanding of the evolving perspectives on success in platform-mediated work and careers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14298v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pyeonghwa Kim, Charis Asante-Agyei, Isabel Munoz, Michael Dunn, Steve Sawyer</dc:creator>
    </item>
    <item>
      <title>A Systematic Review of Echo Chamber Research: Comparative Analysis of Conceptualizations, Operationalizations, and Varying Outcomes</title>
      <link>https://arxiv.org/abs/2407.06631</link>
      <description>arXiv:2407.06631v2 Announce Type: replace-cross 
Abstract: This systematic review synthesizes current research on echo chambers and filter bubbles to highlight the reasons for the dissent in echo chamber research on the existence, antecedents, and effects of the phenomenon. The review of 112 studies reveals that the lack of consensus in echo chamber research is based on different conceptualizations and operationalizations of echo chambers. While studies that have conceptualized echo chambers with homophily and utilized data-driven computational social science (CSS) methods have confirmed the echo chamber hypothesis and polarization effects in social media, content exposure studies and surveys that have explored the full spectrum of media exposure have rejected it.
  Most of these studies have been conducted in the United States, and the review emphasizes the need for a more comprehensive understanding of how echo chambers work in systems with more than two parties and outside the Global North. To advance our understanding of this phenomenon, future research should prioritize conducting more cross-platform studies, considering algorithmic filtering changes through continuous auditing, and examining the causal direction of the association between polarization, fragmentation, and the establishment of online echo chambers. The review also provides the advantages and disadvantages of different operationalizations and makes recommendations for studies in the European Union (EU), which will become possible with the upcoming Digital Services Act (DSA). Overall, this systematic review contributes to the ongoing scholarly discussion on the existence, antecedents, and effects of echo chambers and filter bubbles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06631v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Hartmann, Lena Pohlmann, Sonja Mei Wang, Bettina Berendt</dc:creator>
    </item>
    <item>
      <title>Understanding cyclists' perception of driverless vehicles through eye-tracking and interviews</title>
      <link>https://arxiv.org/abs/2408.10064</link>
      <description>arXiv:2408.10064v3 Announce Type: replace-cross 
Abstract: As automated vehicles (AVs) become increasingly popular, the question arises as to how cyclists will interact with such vehicles. This study investigated (1) whether cyclists spontaneously notice if a vehicle is driverless, (2) how well they perform a driver-detection task when explicitly instructed, and (3) how they carry out these tasks. Using a Wizard-of-Oz method, 37 participants cycled a designated route and encountered an AV multiple times in two experimental sessions. In Session 1, participants cycled the route uninstructed, while in Session 2, they were instructed to verbally report whether they detected the presence or absence of a driver. Additionally, we recorded participants' gaze behaviour with eye-tracking and their responses in post-session interviews. The interviews revealed that 30% of the cyclists spontaneously mentioned the absence of a driver (Session 1), and when instructed (Session 2), they detected the absence and presence of the driver with 93% accuracy. The eye-tracking data showed that cyclists looked more frequently and for longer at the vehicle in Session 2 compared to Session 1. Additionally, participants exhibited intermittent sampling of the vehicle, and they looked at the area in front of the vehicle when it was far away and towards the windshield region when it was closer. The post-session interviews also indicated that participants were curious, but felt safe, and reported a need to receive information about the AV's driving state. In conclusion, cyclists can detect the absence of a driver in the AV, and this detection may influence their perception of safety. Further research is needed to explore these findings in real-world traffic conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10064v3</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siri Hegna Berge, Joost de Winter, Dimitra Dodou, Amir Pooyan Afghari, Eleonora Papadimitriou, Nagarjun Reddy, Yongqi Dong, Narayana Raju, Haneen Farah</dc:creator>
    </item>
    <item>
      <title>Personalized Speech Emotion Recognition in Human-Robot Interaction using Vision Transformers</title>
      <link>https://arxiv.org/abs/2409.10687</link>
      <description>arXiv:2409.10687v2 Announce Type: replace-cross 
Abstract: Emotions are an essential element in verbal communication, so understanding individuals' affect during a human-robot interaction (HRI) becomes imperative. This paper investigates the application of vision transformer models, namely ViT (Vision Transformers) and BEiT (BERT Pre-Training of Image Transformers) pipelines, for Speech Emotion Recognition (SER) in HRI. The focus is to generalize the SER models for individual speech characteristics by fine-tuning these models on benchmark datasets and exploiting ensemble methods. For this purpose, we collected audio data from different human subjects having pseudo-naturalistic conversations with the NAO robot. We then fine-tuned our ViT and BEiT-based models and tested these models on unseen speech samples from the participants. In the results, we show that fine-tuning vision transformers on benchmark datasets and and then using either these already fine-tuned models or ensembling ViT/BEiT models gets us the highest classification accuracies per individual when it comes to identifying four primary emotions from their speech: neutral, happy, sad, and angry, as compared to fine-tuning vanilla-ViTs or BEiTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10687v2</guid>
      <category>eess.AS</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <category>cs.SD</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruchik Mishra, Andrew Frye, Madan Mohan Rayguru, Dan O. Popa</dc:creator>
    </item>
    <item>
      <title>Speech to Reality: On-Demand Production using Natural Language, 3D Generative AI, and Discrete Robotic Assembly</title>
      <link>https://arxiv.org/abs/2409.18390</link>
      <description>arXiv:2409.18390v3 Announce Type: replace-cross 
Abstract: We present a system that transforms speech into physical objects by combining 3D generative Artificial Intelligence with robotic assembly. The system leverages natural language input to make design and manufacturing more accessible, enabling individuals without expertise in 3D modeling or robotic programming to create physical objects. We propose utilizing discrete robotic assembly of lattice-based voxel components to address the challenges of using generative AI outputs in physical production, such as design variability, fabrication speed, structural integrity, and material waste. The system interprets speech to generate 3D objects, discretizes them into voxel components, computes an optimized assembly sequence, and generates a robotic toolpath. The results are demonstrated through the assembly of various objects, ranging from chairs to shelves, which are prompted via speech and realized within 5 minutes using a 6-axis robotic arm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18390v3</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Htet Kyaw, Se Hwan Jeon, Miana Smith, Neil Gershenfeld</dc:creator>
    </item>
    <item>
      <title>R2I-rPPG: A Robust Region of Interest Selection Method for Remote Photoplethysmography to Extract Heart Rate</title>
      <link>https://arxiv.org/abs/2410.15851</link>
      <description>arXiv:2410.15851v2 Announce Type: replace-cross 
Abstract: The COVID-19 pandemic has underscored the need for low-cost, scalable approaches to measuring contactless vital signs, either during initial triage at a healthcare facility or virtual telemedicine visits. Remote photoplethysmography (rPPG) can accurately estimate heart rate (HR) when applied to close-up videos of healthy volunteers in well-lit laboratory settings. However, results from such highly optimized laboratory studies may not be readily translated to healthcare settings. One significant barrier to the practical application of rPPG in health care is the accurate localization of the region of interest (ROI). Clinical or telemedicine visits may involve sub-optimal lighting, movement artifacts, variable camera angle, and subject distance. This paper presents an rPPG ROI selection method based on 3D facial landmarks and patient head yaw angle. We then demonstrate the robustness of this ROI selection method when coupled to the Plane-Orthogonal-to-Skin (POS) rPPG method when applied to videos of patients presenting to an Emergency Department for respiratory complaints. Our results demonstrate the effectiveness of our proposed approach in improving the accuracy and robustness of rPPG in a challenging clinical environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15851v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sandeep Nagar, Mark Hasegawa-Johnson, David G. Beiser, Narendra Ahuja</dc:creator>
    </item>
    <item>
      <title>Understanding the Effect of Algorithm Transparency of Model Explanations in Text-to-SQL Semantic Parsing</title>
      <link>https://arxiv.org/abs/2410.16283</link>
      <description>arXiv:2410.16283v2 Announce Type: replace-cross 
Abstract: Explaining the decisions of AI has become vital for fostering appropriate user trust in these systems. This paper investigates explanations for a structured prediction task called ``text-to-SQL Semantic Parsing'', which translates a natural language question into a structured query language (SQL) program. In this task setting, we designed three levels of model explanation, each exposing a different amount of the model's decision-making details (called ``algorithm transparency''), and investigated how different model explanations could potentially yield different impacts on the user experience. Our study with $\sim$100 participants shows that (1) the low-/high-transparency explanations often lead to less/more user reliance on the model decisions, whereas the medium-transparency explanations strike a good balance. We also show that (2) only the medium-transparency participant group was able to engage further in the interaction and exhibit increasing performance over time, and that (3) they showed the least changes in trust before and after the study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16283v2</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daking Rai, Rydia R. Weiland, Kayla Margaret Gabriella Herrera, Tyler H. Shaw, Ziyu Yao</dc:creator>
    </item>
    <item>
      <title>Test Security in Remote Testing Age: Perspectives from Process Data Analytics and AI</title>
      <link>https://arxiv.org/abs/2411.13699</link>
      <description>arXiv:2411.13699v2 Announce Type: replace-cross 
Abstract: The COVID-19 pandemic has accelerated the implementation and acceptance of remotely proctored high-stake assessments. While the flexible administration of the tests brings forth many values, it raises test security-related concerns. Meanwhile, artificial intelligence (AI) has witnessed tremendous advances in the last five years. Many AI tools (such as the very recent ChatGPT) can generate high-quality responses to test items. These new developments require test security research beyond the statistical analysis of scores and response time. Data analytics and AI methods based on clickstream process data can get us deeper insight into the test-taking process and hold great promise for securing remotely administered high-stakes tests. This chapter uses real-world examples to show that this is indeed the case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13699v2</guid>
      <category>cs.CR</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Machine learning, natural language processing and psychometrics, H. Jiao and R. W. Lissitz (Eds.) (2024)</arxiv:journal_reference>
      <dc:creator>Jiangang Hao, Michael Fauss</dc:creator>
    </item>
  </channel>
</rss>
