<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 25 Mar 2024 04:00:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 25 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Born With a Silver Spoon? Investigating Socioeconomic Bias in Large Language Models</title>
      <link>https://arxiv.org/abs/2403.14633</link>
      <description>arXiv:2403.14633v1 Announce Type: new 
Abstract: Socioeconomic bias in society exacerbates disparities, influencing access to opportunities and resources based on individuals' economic and social backgrounds. This pervasive issue perpetuates systemic inequalities, hindering the pursuit of inclusive progress as a society. In this paper, we investigate the presence of socioeconomic bias, if any, in large language models. To this end, we introduce a novel dataset (SilverSpoon), consisting of 3000 samples that illustrate hypothetical scenarios that involve underprivileged people performing ethically ambiguous actions due to their circumstances, and ask whether the action is ethically justified. Further, this dataset has a dual-labeling scheme and has been annotated by people belonging to both ends of the socioeconomic spectrum. Using SilverSpoon, we evaluate the degree of socioeconomic bias expressed in large language models and the variation of this degree as a function of model size. We also perform qualitative analysis to analyze the nature of this bias. Our analysis reveals that while humans disagree on which situations require empathy toward the underprivileged, most large language models are unable to empathize with the socioeconomically underprivileged regardless of the situation. To foster further research in this domain, we make SilverSpoon and our evaluation harness publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14633v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Smriti Singh, Shuvam Keshari, Vinija Jain, Aman Chadha</dc:creator>
    </item>
    <item>
      <title>AI Sustainability in Practice Part One: Foundations for Sustainable AI Projects</title>
      <link>https://arxiv.org/abs/2403.14635</link>
      <description>arXiv:2403.14635v1 Announce Type: new 
Abstract: Sustainable AI projects are continuously responsive to the transformative effects as well as short-, medium-, and long-term impacts on individuals and society that the design, development, and deployment of AI technologies may have. Projects, which centre AI Sustainability, ensure that values-led, collaborative, and anticipatory reflection both guides the assessment of potential social and ethical impacts and steers responsible innovation practices.
  This workbook is the first part of a pair that provides the concepts and tools needed to put AI Sustainability into practice. It introduces the SUM Values, which help AI project teams to assess the potential societal impacts and ethical permissibility of their projects. It then presents a Stakeholder Engagement Process (SEP), which provides tools to facilitate proportionate engagement of and input from stakeholders with an emphasis on equitable and meaningful participation and positionality awareness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14635v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.10680113</arxiv:DOI>
      <dc:creator>David Leslie, Cami Rincon, Morgan Briggs, Antonella Perini, Smera Jayadeva, Ann Borda, SJ Bennett, Christopher Burr, Mhairi Aitken, Michael Katell, Claudia Fischer, Janis Wong, Ismael Kherroubi Garcia</dc:creator>
    </item>
    <item>
      <title>AI Fairness in Practice</title>
      <link>https://arxiv.org/abs/2403.14636</link>
      <description>arXiv:2403.14636v1 Announce Type: new 
Abstract: Reaching consensus on a commonly accepted definition of AI Fairness has long been a central challenge in AI ethics and governance. There is a broad spectrum of views across society on what the concept of fairness means and how it should best be put to practice. In this workbook, we tackle this challenge by exploring how a context-based and society-centred approach to understanding AI Fairness can help project teams better identify, mitigate, and manage the many ways that unfair bias and discrimination can crop up across the AI project workflow.
  We begin by exploring how, despite the plurality of understandings about the meaning of fairness, priorities of equality and non-discrimination have come to constitute the broadly accepted core of its application as a practical principle. We focus on how these priorities manifest in the form of equal protection from direct and indirect discrimination and from discriminatory harassment. These elements form ethical and legal criteria based upon which instances of unfair bias and discrimination can be identified and mitigated across the AI project workflow.
  We then take a deeper dive into how the different contexts of the AI project lifecycle give rise to different fairness concerns. This allows us to identify several types of AI Fairness (Data Fairness, Application Fairness, Model Design and Development Fairness, Metric-Based Fairness, System Implementation Fairness, and Ecosystem Fairness) that form the basis of a multi-lens approach to bias identification, mitigation, and management. Building on this, we discuss how to put the principle of AI Fairness into practice across the AI project workflow through Bias Self-Assessment and Bias Risk Management as well as through the documentation of metric-based fairness criteria in a Fairness Position Statement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14636v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.10680527</arxiv:DOI>
      <dc:creator>David Leslie, Cami Rincon, Morgan Briggs, Antonella Perini, Smera Jayadeva, Ann Borda, SJ Bennett, Christopher Burr, Mhairi Aitken, Michael Katell, Claudia Fischer, Janis Wong, Ismael Kherroubi Garcia</dc:creator>
    </item>
    <item>
      <title>SimGrade: Using Code Similarity Measures for More Accurate Human Grading</title>
      <link>https://arxiv.org/abs/2403.14637</link>
      <description>arXiv:2403.14637v1 Announce Type: new 
Abstract: While the use of programming problems on exams is a common form of summative assessment in CS courses, grading such exam problems can be a difficult and inconsistent process. Through an analysis of historical grading patterns we show that inaccurate and inconsistent grading of free-response programming problems is widespread in CS1 courses. These inconsistencies necessitate the development of methods to ensure more fairer and more accurate grading. In subsequent analysis of this historical exam data we demonstrate that graders are able to more accurately assign a score to a student submission when they have previously seen another submission similar to it. As a result, we hypothesize that we can improve exam grading accuracy by ensuring that each submission that a grader sees is similar to at least one submission they have previously seen. We propose several algorithms for (1) assigning student submissions to graders, and (2) ordering submissions to maximize the probability that a grader has previously seen a similar solution, leveraging distributed representations of student code in order to measure similarity between submissions. Finally, we demonstrate in simulation that these algorithms achieve higher grading accuracy than the current standard random assignment process used for grading.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14637v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sonja Johnson-Yu, Nicholas Bowman, Mehran Sahami, Chris Piech</dc:creator>
    </item>
    <item>
      <title>Personalized Programming Guidance based on Deep Programming Learning Style Capturing</title>
      <link>https://arxiv.org/abs/2403.14638</link>
      <description>arXiv:2403.14638v1 Announce Type: new 
Abstract: With the rapid development of big data and AI technology, programming is in high demand and has become an essential skill for students. Meanwhile, researchers also focus on boosting the online judging system's guidance ability to reduce students' dropout rates. Previous studies mainly targeted at enhancing learner engagement on online platforms by providing personalized recommendations. However, two significant challenges still need to be addressed in programming: C1) how to recognize complex programming behaviors; C2) how to capture intrinsic learning patterns that align with the actual learning process. To fill these gaps, in this paper, we propose a novel model called Programming Exercise Recommender with Learning Style (PERS), which simulates learners' intricate programming behaviors. Specifically, since programming is an iterative and trial-and-error process, we first introduce a positional encoding and a differentiating module to capture the changes of consecutive code submissions (which addresses C1). To better profile programming behaviors, we extend the Felder-Silverman learning style model, a classical pedagogical theory, to perceive intrinsic programming patterns. Based on this, we align three latent vectors to record and update programming ability, processing style, and understanding style, respectively (which addresses C2). We perform extensive experiments on two real-world datasets to verify the rationality of modeling programming learning styles and the effectiveness of PERS for personalized programming guidance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14638v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingfan Liu, Renyu Zhu, Ming Gao</dc:creator>
    </item>
    <item>
      <title>On Defining Smart Cities using Transformer Neural Networks</title>
      <link>https://arxiv.org/abs/2403.14639</link>
      <description>arXiv:2403.14639v1 Announce Type: new 
Abstract: Cities worldwide are rapidly adopting smart technologies, transforming urban life. Despite this trend, a universally accepted definition of 'smart city' remains elusive. Past efforts to define it have not yielded a consensus, as evidenced by the numerous definitions in use. In this paper, we endeavored to create a new 'compromise' definition that should resonate with most experts previously involved in defining this concept and aimed to validate one of the existing definitions. We reviewed 60 definitions of smart cities from industry, academia, and various relevant organizations, employing transformer architecture-based generative AI and semantic text analysis to reach this compromise. We proposed a semantic similarity measure as an evaluation technique, which could generally be used to compare different smart city definitions, assessing their uniqueness or resemblance. Our methodology employed generative AI to analyze various existing definitions of smart cities, generating a list of potential new composite definitions. Each of these new definitions was then tested against the pre-existing individual definitions we have gathered, using cosine similarity as our metric. This process identified smart city definitions with the highest average cosine similarity, semantically positioning them as the closest on average to all the 60 individual definitions selected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14639v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.24297/ijct.v24i.9579</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Computer and Technology Vol 24 (2024) ISSN: 2277-3061</arxiv:journal_reference>
      <dc:creator>Andrei Khurshudov</dc:creator>
    </item>
    <item>
      <title>Testing autonomous vehicles and AI: perspectives and challenges from cybersecurity, transparency, robustness and fairness</title>
      <link>https://arxiv.org/abs/2403.14641</link>
      <description>arXiv:2403.14641v1 Announce Type: new 
Abstract: This study explores the complexities of integrating Artificial Intelligence (AI) into Autonomous Vehicles (AVs), examining the challenges introduced by AI components and the impact on testing procedures, focusing on some of the essential requirements for trustworthy AI. Topics addressed include the role of AI at various operational layers of AVs, the implications of the EU's AI Act on AVs, and the need for new testing methodologies for Advanced Driver Assistance Systems (ADAS) and Automated Driving Systems (ADS). The study also provides a detailed analysis on the importance of cybersecurity audits, the need for explainability in AI decision-making processes and protocols for assessing the robustness and ethical behaviour of predictive systems in AVs. The paper identifies significant challenges and suggests future directions for research and development of AI in AV technology, highlighting the need for multidisciplinary expertise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14641v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Fern\'andez Llorca, Ronan Hamon, Henrik Junklewitz, Kathrin Grosse, Lars Kunze, Patrick Seiniger, Robert Swaim, Nick Reed, Alexandre Alahi, Emilia G\'omez, Ignacio S\'anchez, Akos Kriston</dc:creator>
    </item>
    <item>
      <title>Revolutionising Distance Learning: A Comparative Study of Learning Progress with AI-Driven Tutoring</title>
      <link>https://arxiv.org/abs/2403.14642</link>
      <description>arXiv:2403.14642v1 Announce Type: new 
Abstract: Generative AI is expected to have a vast, positive impact on education; however, at present, this potential has not yet been demonstrated at scale at university level. In this study, we present first evidence that generative AI can increase the speed of learning substantially in university students. We tested whether using the AI-powered teaching assistant Syntea affected the speed of learning of hundreds of distance learning students across more than 40 courses at the IU International University of Applied Sciences. Our analysis suggests that using Syntea reduced their study time substantially--by about 27\% on average--in the third month after the release of Syntea. Taken together, the magnitude of the effect and the scalability of the approach implicate generative AI as a key lever to significantly improve and accelerate learning by personalisation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14642v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moritz M\"oller, Gargi Nirmal, Dario Fabietti, Quintus Stierstorfer, Mark Zakhvatkin, Holger Sommerfeld, Sven Sch\"utt</dc:creator>
    </item>
    <item>
      <title>Exploring ChatGPT and its Impact on Society</title>
      <link>https://arxiv.org/abs/2403.14643</link>
      <description>arXiv:2403.14643v1 Announce Type: new 
Abstract: Artificial intelligence has been around for a while, but suddenly it has received more attention than ever before. Thanks to innovations from companies like Google, Microsoft, Meta, and other major brands in technology. OpenAI, though, has triggered the button with its ground-breaking invention ChatGPT. ChatGPT is a Large Language Model (LLM) based on Transformer architecture that has the ability to generate human-like responses in a conversational context. It uses deep learning algorithms to generate natural language responses to input text. Its large number of parameters, contextual generation, and open-domain training make it a versatile and effective tool for a wide range of applications, from chatbots to customer service to language translation. It has the potential to revolutionize various industries and transform the way we interact with technology. However, the use of ChatGPT has also raised several concerns, including ethical, social, and employment challenges, which must be carefully considered to ensure the responsible use of this technology. The article provides an overview of ChatGPT, delving into its architecture and training process. It highlights the potential impacts of ChatGPT on the society. In this paper, we suggest some approaches involving technology, regulation, education, and ethics in an effort to maximize ChatGPT's benefits while minimizing its negative impacts. This study is expected to contribute to a greater understanding of ChatGPT and aid in predicting the potential changes it may bring about.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14643v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s43681-024-00435-4</arxiv:DOI>
      <dc:creator>Md. Asraful Haque, Shuai Li</dc:creator>
    </item>
    <item>
      <title>Designing Multi-Step Action Models for Enterprise AI Adoption</title>
      <link>https://arxiv.org/abs/2403.14645</link>
      <description>arXiv:2403.14645v1 Announce Type: new 
Abstract: This paper introduces the Multi-Step Action Model (MSAM), a closed-source AI model designed by Empsing to address challenges hindering AI adoption in enterprises. Through a holistic examination, this paper explores MSAM's foundational principles, design architecture, and future trajectory. It evaluates MSAM's performance via rigorous testing methodologies and envisions its potential impact on advancing AI adoption within organizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14645v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shreyash Mishra, Shrey Shah, Rex Pereira</dc:creator>
    </item>
    <item>
      <title>Digital Twin for Wind Energy: Latest updates from the NorthWind project</title>
      <link>https://arxiv.org/abs/2403.14646</link>
      <description>arXiv:2403.14646v1 Announce Type: new 
Abstract: NorthWind, a collaborative research initiative supported by the Research Council of Norway, industry stakeholders, and research partners, aims to advance cutting-edge research and innovation in wind energy. The core mission is to reduce wind power costs and foster sustainable growth, with a key focus on the development of digital twins. A digital twin is a virtual representation of physical assets or processes that uses data and simulators to enable real-time forecasting, optimization, monitoring, control and informed decision-making. Recently, a hierarchical scale ranging from 0 to 5 (0 - Standalone, 1 - Descriptive, 2 - Diagnostic, 3 - Predictive, 4 - Prescriptive, 5 - Autonomous has been introduced within the NorthWind project to assess the capabilities of digital twins. This paper elaborates on our progress in constructing digital twins for wind farms and their components across various capability levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14646v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adil Rasheed, Florian Stadtmann, Eivind Fonn, Mandar Tabib, Vasileios Tsiolakis, Balram Panjwani, Kjetil Andre Johannessen, Trond Kvamsdal, Omer San, John Olav Tande, Idar Barstad, Tore Christiansen, Elling Rishoff, Lars Fr{\o}yd, Tore Rasmussen</dc:creator>
    </item>
    <item>
      <title>Initial Indications of Safety of Driverless Automated Driving Systems</title>
      <link>https://arxiv.org/abs/2403.14648</link>
      <description>arXiv:2403.14648v1 Announce Type: new 
Abstract: As driverless automated driving systems (ADS) start to operate on public roads, there is an urgent need to understand how safely these systems are managing real-world traffic conditions. With data from the California Public Utilities Commission (CPUC) becoming available for Transportation Network Companies (TNCs) operating in California with and without human drivers, there is an initial basis for comparing ADS and human driving safety.
  This paper analyzes the crash rates and characteristics for three types of driving: Uber ridesharing trips from the CPUC TNC Annual Report in 2020, supervised autonomous vehicles (AV) driving from the California Department of Motor Vehicles (DMV) between December 2020 and November 2022, driverless ADS pilot (testing) and deployment (revenue service) program from Waymo and Cruise between March 2022 and August 2023. All of the driving was done within the city of San Francisco, excluding freeways. The same geographical confinement allows for controlling the exposure to vulnerable road users, population density, speed limit, and other external factors such as weather and road conditions. The study finds that supervised AV has almost equivalent crashes per million miles (CPMM) as Uber human driving, the driverless Waymo AV has a lower CPMM, and the driverless Cruise AV has a higher CPMM than Uber human driving. The data samples are not yet large enough to support conclusions about whether the current automated systems are more or less safe than human-operated vehicles in the complex San Francisco urban environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14648v1</guid>
      <category>cs.CY</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiayu Joyce Chen, Steven E. Shladover</dc:creator>
    </item>
    <item>
      <title>The economic value of scientific software</title>
      <link>https://arxiv.org/abs/2403.14649</link>
      <description>arXiv:2403.14649v1 Announce Type: new 
Abstract: Academic institutions and their staff use, adapt and create software. We're thinking of business tools used to carry out their mission: teaching management (Moodle) or subject teaching support (such as Maxima for formal calculus), for example. We're talking about software resulting from research work, designed by a researcher or a team as part of a research project (funded by ANR, Europe, etc. or not) or as a research service for a third party. These projects can last for decades (such as the Coq program proof assistant project, or the GPAC multimedia content distribution platform).We discuss why this software is produced, with what resources, the interest that institutions derive from it, what we call the ''valorization'' of software resulting from scientific research. The latter is multifaceted, as are the missions of scientific institutions: social value (contribution to the world heritage of knowledge), financial value (contracts), economic value (business creation), scientific value (publication), image value (visibility of the institution among target audiences: students, researchers, companies, prescribers).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14649v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Revue Lamy Droit de l'immat{\'e}riel, 2024, 210</arxiv:journal_reference>
      <dc:creator>Nicolas Jullien (IMT Atlantique - LUSSI, MARSOUIN, LEGO)</dc:creator>
    </item>
    <item>
      <title>Harnessing the Computing Continuum across Personalized Healthcare, Maintenance and Inspection, and Farming 4.0</title>
      <link>https://arxiv.org/abs/2403.14650</link>
      <description>arXiv:2403.14650v1 Announce Type: new 
Abstract: The AI-SPRINT project, launched in 2021 and funded by the European Commission, focuses on the development and implementation of AI applications across the computing continuum. This continuum ensures the coherent integration of computational resources and services from centralized data centers to edge devices, facilitating efficient and adaptive computation and application delivery. AI-SPRINT has achieved significant scientific advances, including streamlined processes, improved efficiency, and the ability to operate in real time, as evidenced by three practical use cases. This paper provides an in-depth examination of these applications -- Personalized Healthcare, Maintenance and Inspection, and Farming 4.0 -- highlighting their practical implementation and the objectives achieved with the integration of AI-SPRINT technologies. We analyze how the proposed toolchain effectively addresses a range of challenges and refines processes, discussing its relevance and impact in multiple domains. After a comprehensive overview of the main AI-SPRINT tools used in these scenarios, the paper summarizes of the findings and key lessons learned.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14650v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Fatemeh Baghdadi, Davide Cirillo, Daniele Lezzi, Francesc Lordan, Fernando Vazquez, Eugenio Lomurno, Alberto Archetti, Danilo Ardagna, Matteo Matteucci</dc:creator>
    </item>
    <item>
      <title>DOSA: A Dataset of Social Artifacts from Different Indian Geographical Subcultures</title>
      <link>https://arxiv.org/abs/2403.14651</link>
      <description>arXiv:2403.14651v1 Announce Type: new 
Abstract: Generative models are increasingly being used in various applications, such as text generation, commonsense reasoning, and question-answering. To be effective globally, these models must be aware of and account for local socio-cultural contexts, making it necessary to have benchmarks to evaluate the models for their cultural familiarity. Since the training data for LLMs is web-based and the Web is limited in its representation of information, it does not capture knowledge present within communities that are not on the Web. Thus, these models exacerbate the inequities, semantic misalignment, and stereotypes from the Web. There has been a growing call for community-centered participatory research methods in NLP. In this work, we respond to this call by using participatory research methods to introduce $\textit{DOSA}$, the first community-generated $\textbf{D}$ataset $\textbf{o}$f 615 $\textbf{S}$ocial $\textbf{A}$rtifacts, by engaging with 260 participants from 19 different Indian geographic subcultures. We use a gamified framework that relies on collective sensemaking to collect the names and descriptions of these artifacts such that the descriptions semantically align with the shared sensibilities of the individuals from those cultures. Next, we benchmark four popular LLMs and find that they show significant variation across regional sub-cultures in their ability to infer the artifacts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14651v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Agrima Seth, Sanchit Ahuja, Kalika Bali, Sunayana Sitaram</dc:creator>
    </item>
    <item>
      <title>MemeCraft: Contextual and Stance-Driven Multimodal Meme Generation</title>
      <link>https://arxiv.org/abs/2403.14652</link>
      <description>arXiv:2403.14652v1 Announce Type: new 
Abstract: Online memes have emerged as powerful digital cultural artifacts in the age of social media, offering not only humor but also platforms for political discourse, social critique, and information dissemination. Their extensive reach and influence in shaping online communities' sentiments make them invaluable tools for campaigning and promoting ideologies. Despite the development of several meme-generation tools, there remains a gap in their systematic evaluation and their ability to effectively communicate ideologies. Addressing this, we introduce MemeCraft, an innovative meme generator that leverages large language models (LLMs) and visual language models (VLMs) to produce memes advocating specific social movements. MemeCraft presents an end-to-end pipeline, transforming user prompts into compelling multimodal memes without manual intervention. Conscious of the misuse potential in creating divisive content, an intrinsic safety mechanism is embedded to curb hateful meme production.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14652v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.MM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3589334.3648151</arxiv:DOI>
      <dc:creator>Han Wang, Roy Ka-Wei Lee</dc:creator>
    </item>
    <item>
      <title>Between Copyright and Computer Science: The Law and Ethics of Generative AI</title>
      <link>https://arxiv.org/abs/2403.14653</link>
      <description>arXiv:2403.14653v1 Announce Type: new 
Abstract: Copyright and computer science continue to intersect and clash, but they can coexist. The advent of new technologies such as digitization of visual and aural creations, sharing technologies, search engines, social media offerings, and more challenge copyright-based industries and reopen questions about the reach of copyright law. Breakthroughs in artificial intelligence research, especially Large Language Models that leverage copyrighted material as part of training models, are the latest examples of the ongoing tension between copyright and computer science. The exuberance, rush-to-market, and edge problem cases created by a few misguided companies now raises challenges to core legal doctrines and may shift Open Internet practices for the worse. That result does not have to be, and should not be, the outcome.
  This Article shows that, contrary to some scholars' views, fair use law does not bless all ways that someone can gain access to copyrighted material even when the purpose is fair use. Nonetheless, the scientific need for more data to advance AI research means access to large book corpora and the Open Internet is vital for the future of that research. The copyright industry claims, however, that almost all uses of copyrighted material must be compensated, even for non-expressive uses. The Article's solution accepts that both sides need to change. It is one that forces the computer science world to discipline its behaviors and, in some cases, pay for copyrighted material. It also requires the copyright industry to abandon its belief that all uses must be compensated or restricted to uses sanctioned by the copyright industry. As part of this re-balancing, the Article addresses a problem that has grown out of this clash and under theorized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14653v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deven R. Desai, Mark Riedl</dc:creator>
    </item>
    <item>
      <title>ChatGPT in Veterinary Medicine: A Practical Guidance of Generative Artificial Intelligence in Clinics, Education, and Research</title>
      <link>https://arxiv.org/abs/2403.14654</link>
      <description>arXiv:2403.14654v1 Announce Type: new 
Abstract: ChatGPT, the most accessible generative artificial intelligence (AI) tool, offers considerable potential for veterinary medicine, yet a dedicated review of its specific applications is lacking. This review concisely synthesizes the latest research and practical applications of ChatGPT within the clinical, educational, and research domains of veterinary medicine. It intends to provide specific guidance and actionable examples of how generative AI can be directly utilized by veterinary professionals without a programming background. For practitioners, ChatGPT can extract patient data, generate progress notes, and potentially assist in diagnosing complex cases. Veterinary educators can create custom GPTs for student support, while students can utilize ChatGPT for exam preparation. ChatGPT can aid in academic writing tasks in research, but veterinary publishers have set specific requirements for authors to follow. Despite its transformative potential, careful use is essential to avoid pitfalls like hallucination. This review addresses ethical considerations, provides learning resources, and offers tangible examples to guide responsible implementation. Carefully selected, up-to-date links to platforms that host large language models are provided for advanced readers with programming capability. A table of key takeaways was provided to summarize this review. By highlighting potential benefits and limitations, this review equips veterinarians, educators, and researchers to harness the power of ChatGPT effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14654v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Candice P. Chu</dc:creator>
    </item>
    <item>
      <title>A Synergistic Approach to Wildfire Prevention and Management Using AI, ML, and 5G Technology in the United States</title>
      <link>https://arxiv.org/abs/2403.14657</link>
      <description>arXiv:2403.14657v1 Announce Type: new 
Abstract: Over the past few years, wildfires have become a worldwide environmental emergency, resulting in substantial harm to natural habitats and playing a part in the acceleration of climate change. Wildfire management methods involve prevention, response, and recovery efforts. Despite improvements in detection techniques, the rising occurrence of wildfires demands creative solutions for prompt identification and effective control. This research investigates proactive methods for detecting and handling wildfires in the United States, utilizing Artificial Intelligence (AI), Machine Learning (ML), and 5G technology. The specific objective of this research covers proactive detection and prevention of wildfires using advanced technology; Active monitoring and mapping with remote sensing and signaling leveraging on 5G technology; and Advanced response mechanisms to wildfire using drones and IOT devices. This study was based on secondary data collected from government databases and analyzed using descriptive statistics. In addition, past publications were reviewed through content analysis, and narrative synthesis was used to present the observations from various studies. The results showed that developing new technology presents an opportunity to detect and manage wildfires proactively. Utilizing advanced technology could save lives and prevent significant economic losses caused by wildfires. Various methods, such as AI-enabled remote sensing and 5G-based active monitoring, can enhance proactive wildfire detection and management. In addition, super intelligent drones and IOT devices can be used for safer responses to wildfires. This forms the core of the recommendation to the fire Management Agencies and the government.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14657v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanley Chinedu Okoro, Alexander Lopez, Austine Unuriode</dc:creator>
    </item>
    <item>
      <title>Identifying Potential Inlets of Man in the Artificial Intelligence Development Process</title>
      <link>https://arxiv.org/abs/2403.14658</link>
      <description>arXiv:2403.14658v1 Announce Type: new 
Abstract: In this paper we hope to identify how the typical or standard artificial intelligence development process encourages or facilitates the creation of racialized technologies. We begin by understanding Sylvia Wynter's definition of the biocentric Man genre and its exclusion of Blackness from humanness. We follow this with outlining what we consider to be the typical steps for developing an AI-based technology, which we have broken down into 6 stages: identifying a problem, development process and management tool selection, dataset development and data processing, model development, deployment and risk assessment, and integration and monitoring. The goal of this paper is to better understand how Wynter's biocentric Man is being represented and reinforced by the technologies we are producing in the AI lifecycle and by the lifecycle itself; we hope to identify ways in which the distinction of Blackness from the "ideal" human leads to perpetual punishment at the hands of these technologies. By deconstructing this development process, we can potentially identify ways in which humans in general have not been prioritized and how those affects are disproportionately affecting marginalized people. We hope to offer solutions that will encourage changes in the AI development cycle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14658v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3584931.3606981</arxiv:DOI>
      <dc:creator>Deja Workman, Christopher L. Dancy</dc:creator>
    </item>
    <item>
      <title>Social Intelligence Data Infrastructure: Structuring the Present and Navigating the Future</title>
      <link>https://arxiv.org/abs/2403.14659</link>
      <description>arXiv:2403.14659v1 Announce Type: new 
Abstract: As Natural Language Processing (NLP) systems become increasingly integrated into human social life, these technologies will need to increasingly rely on social intelligence. Although there are many valuable datasets that benchmark isolated dimensions of social intelligence, there does not yet exist any body of work to join these threads into a cohesive subfield in which researchers can quickly identify research gaps and future directions. Towards this goal, we build a Social AI Data Infrastructure, which consists of a comprehensive social AI taxonomy and a data library of 480 NLP datasets. Our infrastructure allows us to analyze existing dataset efforts, and also evaluate language models' performance in different social intelligence aspects. Our analyses demonstrate its utility in enabling a thorough understanding of current data landscape and providing a holistic perspective on potential directions for future dataset development. We show there is a need for multifaceted datasets, increased diversity in language and culture, more long-tailed social situations, and more interactive data in future social intelligence data efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14659v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minzhi Li, Weiyan Shi, Caleb Ziems, Diyi Yang</dc:creator>
    </item>
    <item>
      <title>Machina Economicus: A New Paradigm for Prosumers in the Energy Internet of Smart Cities</title>
      <link>https://arxiv.org/abs/2403.14660</link>
      <description>arXiv:2403.14660v1 Announce Type: new 
Abstract: Energy Internet (EI) is emerging as new share economy platform for flexible local energy supplies in smart cities. Empowered by the Internet-of-Things (IoT) and Artificial Intelligence (AI), EI aims to unlock peer-to-peer energy trading and sharing among prosumers, who can adeptly switch roles between providers and consumers in localized energy markets with rooftop photovoltaic panels, vehicle-to-everything technologies, packetized energy management, etc. The integration of prosumers in EI, however, will encounter many challenges in modelling, analyzing, and designing an efficient, economic, and social-optimal platform for energy sharing, calling for advanced AI/IoT-based solutions to resource optimization, information exchange, and interaction protocols in the context of the share economy. In this study, we aim to introduce a recently emerged paradigm, Machina Economicus, to investigate the economic rationality in modelling, analysis, and optimization of AI/IoT-based EI prosumer behaviors. The new paradigm, built upon the theory of machine learning and mechanism design, will offer new angles to investigate the selfishness of AI through a game-theoretic perspective, revealing potential competition and collaborations resulting from the self-adaptive learning and decision-making capacity. This study will focus on how the introduction of AI will reshape prosumer behaviors on the EI, and how this paradigm will reveal new research questions and directions when AI meets the share economy. With an extensive case analysis in the literature, we will also shed light on potential solutions for advancements of AI in future smart cities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14660v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luyang Hou, Jun Yan, Yuankai Wu, Chun Wang, Tie Qiu</dc:creator>
    </item>
    <item>
      <title>Towards Modeling Learner Performance with Large Language Models</title>
      <link>https://arxiv.org/abs/2403.14661</link>
      <description>arXiv:2403.14661v1 Announce Type: new 
Abstract: Recent work exploring the capabilities of pre-trained large language models (LLMs) has demonstrated their ability to act as general pattern machines by completing complex token sequences representing a wide array of tasks, including time-series prediction and robot control. This paper investigates whether the pattern recognition and sequence modeling capabilities of LLMs can be extended to the domain of knowledge tracing, a critical component in the development of intelligent tutoring systems (ITSs) that tailor educational experiences by predicting learner performance over time. In an empirical evaluation across multiple real-world datasets, we compare two approaches to using LLMs for this task, zero-shot prompting and model fine-tuning, with existing, non-LLM approaches to knowledge tracing. While LLM-based approaches do not achieve state-of-the-art performance, fine-tuned LLMs surpass the performance of naive baseline models and perform on par with standard Bayesian Knowledge Tracing approaches across multiple metrics. These findings suggest that the pattern recognition capabilities of LLMs can be used to model complex learning trajectories, opening a novel avenue for applying LLMs to educational contexts. The paper concludes with a discussion of the implications of these findings for future research, suggesting that further refinements and a deeper understanding of LLMs' predictive mechanisms could lead to enhanced performance in knowledge tracing tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14661v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Seyed Parsa Neshaei, Richard Lee Davis, Adam Hazimeh, Bojan Lazarevski, Pierre Dillenbourg, Tanja K\"aser</dc:creator>
    </item>
    <item>
      <title>Case Studies of AI Policy Development in Africa</title>
      <link>https://arxiv.org/abs/2403.14662</link>
      <description>arXiv:2403.14662v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) requires new ways of evaluating national technology use and strategy for African nations. We conduct a survey of existing 'readiness' assessments both for general digital adoption and for AI policy in particular. We conclude that existing global readiness assessments do not fully capture African states' progress in AI readiness and lay the groundwork for how assessments can be better used for the African context. We consider the extent to which these indicators map to the African context and what these indicators miss in capturing African states' on-the-ground work in meeting AI capability. Through case studies of four African nations of diverse geographic and economic dimensions, we identify nuances missed by global assessments and offer high-level policy considerations for how states can best improve their AI readiness standards and prepare their societies to capture the benefits of AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14662v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kadijatou Diallo, Jonathan Smith, Chinasa T. Okolo, Dorcas Nyamwaya, Jonas Kgomo, Richard Ngamita</dc:creator>
    </item>
    <item>
      <title>Machine Learning Predicts Upper Secondary Education Dropout as Early as the End of Primary School</title>
      <link>https://arxiv.org/abs/2403.14663</link>
      <description>arXiv:2403.14663v1 Announce Type: new 
Abstract: Education plays a pivotal role in alleviating poverty, driving economic growth, and empowering individuals, thereby significantly influencing societal and personal development. However, the persistent issue of school dropout poses a significant challenge, with its effects extending beyond the individual. While previous research has employed machine learning for dropout classification, these studies often suffer from a short-term focus, relying on data collected only a few years into the study period. This study expanded the modeling horizon by utilizing a 13-year longitudinal dataset, encompassing data from kindergarten to Grade 9. Our methodology incorporated a comprehensive range of parameters, including students' academic and cognitive skills, motivation, behavior, well-being, and officially recorded dropout data. The machine learning models developed in this study demonstrated notable classification ability, achieving a mean area under the curve (AUC) of 0.61 with data up to Grade 6 and an improved AUC of 0.65 with data up to Grade 9. Further data collection and independent correlational and causal analyses are crucial. In future iterations, such models may have the potential to proactively support educators' processes and existing protocols for identifying at-risk students, thereby potentially aiding in the reinvention of student retention and success strategies and ultimately contributing to improved educational outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14663v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Psyridou, Fabi Prezja, Minna Torppa, Marja-Kristiina Lerkkanen, Anna-Maija Poikkeus, Kati Vasalampi</dc:creator>
    </item>
    <item>
      <title>ClickTree: A Tree-based Method for Predicting Math Students' Performance Based on Clickstream Data</title>
      <link>https://arxiv.org/abs/2403.14664</link>
      <description>arXiv:2403.14664v1 Announce Type: new 
Abstract: The prediction of student performance and the analysis of students' learning behavior play an important role in enhancing online courses. By analysing a massive amount of clickstream data that captures student behavior, educators can gain valuable insights into the factors that influence academic outcomes and identify areas of improvement in courses. In this study, we developed ClickTree, a tree-based methodology, to predict student performance in mathematical assignments based on students' clickstream data. We extracted a set of features, including problem-level, assignment-level and student-level features, from the extensive clickstream data and trained a CatBoost tree to predict whether a student successfully answers a problem in an assignment. The developed method achieved an AUC of 0.78844 in the Educational Data Mining Cup 2023 and ranked second in the competition. Furthermore, our results indicate that students encounter more difficulties in the problem types that they must select a subset of answers from a given set as well as problem subjects of Algebra II. Additionally, students who performed well in answering end-unit assignment problems engaged more with in-unit assignments and answered more problems correctly, while those who struggled had higher tutoring request rate. The proposed method can be utilized to improve students' learning experiences, and the above insights can be integrated into mathematical courses to enhance students' learning outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14664v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Narjes Rohani, Behnam Rohani, Areti Manataki</dc:creator>
    </item>
    <item>
      <title>Sora OpenAI's Prelude: Social Media Perspectives on Sora OpenAI and the Future of AI Video Generation</title>
      <link>https://arxiv.org/abs/2403.14665</link>
      <description>arXiv:2403.14665v1 Announce Type: new 
Abstract: The rapid advancement of Generative AI (Gen-AI) is transforming Human-Computer Interaction (HCI), with significant implications across various sectors. This study investigates the public's perception of Sora OpenAI, a pioneering Gen-AI video generation tool, via social media discussions on Reddit before its release. It centers on two main questions: the envisioned applications and the concerns related to Sora's integration. The analysis forecasts positive shifts in content creation, predicting that Sora will democratize video marketing and innovate game development by making video production more accessible and economical. Conversely, there are concerns about deepfakes and the potential for disinformation, underscoring the need for strategies to address disinformation and bias. This paper contributes to the Gen-AI discourse by fostering discussion on current and future capabilities, enriching the understanding of public expectations, and establishing a temporal benchmark for user anticipation. This research underscores the necessity for informed, ethical approaches to AI development and integration, ensuring that technological advancements align with societal values and user needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14665v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reza Hadi Mogavi, Derrick Wang, Joseph Tu, Hilda Hadan, Sabrina A. Sgandurra, Pan Hui, Lennart E. Nacke</dc:creator>
    </item>
    <item>
      <title>SyllabusQA: A Course Logistics Question Answering Dataset</title>
      <link>https://arxiv.org/abs/2403.14666</link>
      <description>arXiv:2403.14666v1 Announce Type: new 
Abstract: Automated teaching assistants and chatbots have significant potential to reduce the workload of human instructors, especially for logistics-related question answering, which is important to students yet repetitive for instructors. However, due to privacy concerns, there is a lack of publicly available datasets. We introduce SyllabusQA, an open-source dataset with 63 real course syllabi covering 36 majors, containing 5,078 open-ended course logistics-related question-answer pairs that are diverse in both question types and answer formats. Since many logistics-related questions contain critical information like the date of an exam, it is important to evaluate the factuality of answers. We benchmark several strong baselines on this task, from large language model prompting to retrieval-augmented generation. We find that despite performing close to humans on traditional metrics of textual similarity, there remains a significant gap between automated approaches and humans in terms of fact precision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14666v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nigel Fernandez, Alexander Scarlatos, Andrew Lan</dc:creator>
    </item>
    <item>
      <title>Predicting Learning Performance with Large Language Models: A Study in Adult Literacy</title>
      <link>https://arxiv.org/abs/2403.14668</link>
      <description>arXiv:2403.14668v1 Announce Type: new 
Abstract: Intelligent Tutoring Systems (ITSs) have significantly enhanced adult literacy training, a key factor for societal participation, employment opportunities, and lifelong learning. Our study investigates the application of advanced AI models, including Large Language Models (LLMs) like GPT-4, for predicting learning performance in adult literacy programs in ITSs. This research is motivated by the potential of LLMs to predict learning performance based on its inherent reasoning and computational capabilities. By using reading comprehension datasets from the ITS, AutoTutor, we evaluate the predictive capabilities of GPT-4 versus traditional machine learning methods in predicting learning performance through five-fold cross-validation techniques. Our findings show that the GPT-4 presents the competitive predictive abilities with traditional machine learning methods such as Bayesian Knowledge Tracing, Performance Factor Analysis, Sparse Factor Analysis Lite (SPARFA-Lite), tensor factorization and eXtreme Gradient Boosting (XGBoost). While XGBoost (trained on local machine) outperforms GPT-4 in predictive accuracy, GPT-4-selected XGBoost and its subsequent tuning on the GPT-4 platform demonstrates superior performance compared to local machine execution. Moreover, our investigation into hyper-parameter tuning by GPT-4 versus grid-search suggests comparable performance, albeit with less stability in the automated approach, using XGBoost as the case study. Our study contributes to the field by highlighting the potential of integrating LLMs with traditional machine learning models to enhance predictive accuracy and personalize adult literacy education, setting a foundation for future research in applying LLMs within ITSs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14668v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liang Zhang, Jionghao Lin, Conrad Borchers, John Sabatini, John Hollander, Meng Cao, Xiangen Hu</dc:creator>
    </item>
    <item>
      <title>Large-Scale Evaluation of Mobility, Technology and Demand Scenarios in the Chicago Region Using POLARIS</title>
      <link>https://arxiv.org/abs/2403.14669</link>
      <description>arXiv:2403.14669v1 Announce Type: new 
Abstract: Rapid technological progress and innovation in the areas of vehicle connectivity, automation and electrification, new modes of shared and alternative mobility, and advanced transportation system demand and supply management strategies, have motivated numerous questions and studies regarding the potential impact on key performance and equity metrics. Several of these areas of development may or may not have a synergistic outcome on the overall benefits such as reduction in congestion and travel times. In this study, the use of an end-to-end modeling workflow centered around an activity-based agent-based travel demand forecasting tool called POLARIS is explored to provide insights on the effects of several different technology deployments and operational policies in combination for the Chicago region. The objective of the research was to explore the direct impacts and observe any interactions between the various policy and technology scenarios to help better characterize and evaluate their potential future benefits. We analyze system outcome metrics on mobility, energy and emissions, equity and environmental justice and overall efficiency for a scenario design of experiments that looks at combinations of supply interventions (congestion pricing, transit expansion, tnc policy, off-hours freight policy, connected signal optimization) for different potential demand scenarios defined by e-commerce and on-demand delivery engagement, and market penetration of electric vehicles. We found different combinations of strategies that can reduce overall travel times up to 7% and increase system efficiency up to 53% depending on how various metrics are prioritized. The results demonstrate the importance of considering various interventions jointly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14669v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua Auld, Jamie Cook, Krishna Murthy Gurumurthy, Nazmul Khan, Charbel Mansour, Aymeric Rousseau, Olcay Sahin, Felipe de Souza, Omer Verbas, Natalia Zuniga-Garcia</dc:creator>
    </item>
    <item>
      <title>Electric Vehicle Enquiry (EVE) Pilot</title>
      <link>https://arxiv.org/abs/2403.14670</link>
      <description>arXiv:2403.14670v1 Announce Type: new 
Abstract: This data paper presents the dataset from a study on the use of electric vehicles (EVs). This dataset covers the first dataset collected in this study: the usage data from a Renault Zoe over 3 years. The process of collection of the dataset, its treatment, and descriptions of all the included variables are detailed. The collection of this dataset represents an iteration of participative research in the personal mobility domain as the dataset was collected with low-cost commercially available equipment and open-source software. Some of the challenges of providing the dataset are also discussed: the most pertinent being the intermittent nature of data collection as an android phone and OBDII adapter were used to collect the dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14670v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seun Osonuga, Frederic Wurtz, Benoit Delinchant</dc:creator>
    </item>
    <item>
      <title>Understanding the Transit Gap: A Comparative Study of On-Demand Bus Services and Urban Climate Resilience in South End, Charlotte, NC and Avondale, Chattanooga, TN</title>
      <link>https://arxiv.org/abs/2403.14671</link>
      <description>arXiv:2403.14671v1 Announce Type: new 
Abstract: Urban design significantly impacts sustainability, particularly in the context of public transit efficiency and carbon emissions reduction. This study explores two neighborhoods with distinct urban designs: South End, Charlotte, NC, featuring a dynamic mixed-use urban design pattern, and Avondale, Chattanooga, TN, with a residential suburban grid layout. Using the TRANSIT-GYM tool, we assess the impact of increased bus utilization in these different urban settings on traffic and CO2 emissions. Our results highlight the critical role of urban design and planning in transit system efficiency. In South End, the mixed-use design led to more substantial emission reductions, indicating that urban layout can significantly influence public transit outcomes. Tailored strategies that consider the unique urban design elements are essential for climate resilience. Notably, doubling bus utilization decreased daily emissions by 10.18% in South End and 8.13% in Avondale, with a corresponding reduction in overall traffic. A target of 50% bus utilization saw emissions drop by 21.45% in South End and 14.50% in Avondale. At an idealistic goal of 70% bus utilization, South End and Avondale witnessed emission reductions of 37.22% and 27.80%, respectively. These insights are crucial for urban designers and policymakers in developing sustainable urban landscapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14671v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanaz Sadat Hosseini, Babak Rahimi Ardabili, Mona Azarbayjani, Srinivas Pulugurtha, Hamed Tabkhi</dc:creator>
    </item>
    <item>
      <title>Can 'Robots Won't Save Japan' Save Robotics? Reviewing an Ethnography of Eldercare Automation</title>
      <link>https://arxiv.org/abs/2403.14673</link>
      <description>arXiv:2403.14673v1 Announce Type: new 
Abstract: Imagine activating new robots meant to aid staff in an elder care facility, only to discover the robots are counterproductive. They undermine the most meaningful moments of the jobs and increase staff workloads, because robots demand care too. Eventually, they're returned. This vignette captures key elements of James Adrian Wright's ethnography, "Robots Won't Save Japan", an essential resource for understanding the state of elder care robotics. Wright's rich ethnographic interviews and observations challenge the prevailing funding, research, and development paradigms for robotics. Elder care residents tend to be Disabled, so this review article augments Wrights' insights with overlooked perspectives from Disability and Robotics research. This article highlights how care recipients' portrayal suggests that Paro, a plush robot seal, might perform better than the care team and author indicated -- leading to insights that support urgent paradigm shifts in elder care, ethnographic studies, and robotics. It presents some of the stronger technical status quo counter-arguments to the book's core narratives, then confronts their own assumptions. Furthermore, it explores exceptional cases where Japanese and international roboticists attend to care workers and recipients, justifying key arguments in Wright's compelling book. Finally, it addresses how "Robots won't save Japan" will save Robotics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14673v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Hundt</dc:creator>
    </item>
    <item>
      <title>Packaging Up Media Mix Modeling: An Introduction to Robyn's Open-Source Approach</title>
      <link>https://arxiv.org/abs/2403.14674</link>
      <description>arXiv:2403.14674v1 Announce Type: new 
Abstract: While attribution of user behavior across apps and websites had led to unseen levels of determinism in digital advertising measurement, privacy-centric changes to the digital data landscape are bringing probabilistic techniques such as marketing and media mix modeling en vogue again. Many small and midsize advertisers lack the scale and resources to invest in advanced proprietary modeling efforts that would usually require specific expertise and a team of several data scientists. To facilitate broad successful adoption of media mix modeling for digital advertising measurement, marketing data scientists at Meta started the open-source computational package Robyn. This article presents architectural components and choices in Robyn and discusses how Robyn aims to be packaged against biases and for organizational acceptance. As an open-source package with wide adoption and a highly active community, Robyn undergoes continual development. In this vein, what is described in this article should not be seen as conclusive solutions but as an outline of pathways that the Robyn community has embarked on. The article aims to provide a structured introduction to these pathways as a basis for feedback from marketing data scientists, to ensure Robyn's ongoing development aligns with users' needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14674v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gufeng Zhou, Igor Skokan, Julian Runge</dc:creator>
    </item>
    <item>
      <title>Investigating the Impact of Project Risks on Employee Turnover Intentions in the IT Industry of Pakistan</title>
      <link>https://arxiv.org/abs/2403.14675</link>
      <description>arXiv:2403.14675v1 Announce Type: new 
Abstract: Employee turnover remains a pressing issue within high-tech sectors such as IT firms and research centers, where organizational success heavily relies on the skills of their workforce. Intense competition and a scarcity of skilled professionals in the industry contribute to a perpetual demand for highly qualified employees, posing challenges for organizations to retain talent. While numerous studies have explored various factors affecting employee turnover in these industries, their focus often remains on overarching trends rather than specific organizational contexts. In particular, within the software industry, where projectspecific risks can significantly impact project success and timely delivery, understanding their influence on job satisfaction and turnover intentions is crucial. This study aims to investigate the influence of project risks in the IT industry on job satisfaction and employee turnover intentions. Furthermore, it examines the role of both external and internal social links in shaping perceptions of job satisfaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14675v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ghalib Ahmed Tahir, Murtaza Ashraf</dc:creator>
    </item>
    <item>
      <title>Unified Uncertainty Estimation for Cognitive Diagnosis Models</title>
      <link>https://arxiv.org/abs/2403.14676</link>
      <description>arXiv:2403.14676v1 Announce Type: new 
Abstract: Cognitive diagnosis models have been widely used in different areas, especially intelligent education, to measure users' proficiency levels on knowledge concepts, based on which users can get personalized instructions. As the measurement is not always reliable due to the weak links of the models and data, the uncertainty of measurement also offers important information for decisions. However, the research on the uncertainty estimation lags behind that on advanced model structures for cognitive diagnosis. Existing approaches have limited efficiency and leave an academic blank for sophisticated models which have interaction function parameters (e.g., deep learning-based models). To address these problems, we propose a unified uncertainty estimation approach for a wide range of cognitive diagnosis models. Specifically, based on the idea of estimating the posterior distributions of cognitive diagnosis model parameters, we first provide a unified objective function for mini-batch based optimization that can be more efficiently applied to a wide range of models and large datasets. Then, we modify the reparameterization approach in order to adapt to parameters defined on different domains. Furthermore, we decompose the uncertainty of diagnostic parameters into data aspect and model aspect, which better explains the source of uncertainty. Extensive experiments demonstrate that our method is effective and can provide useful insights into the uncertainty of cognitive diagnosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14676v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3589334.3645488</arxiv:DOI>
      <dc:creator>Fei Wang, Qi Liu, Enhong Chen, Chuanren Liu, Zhenya Huang, Jinze Wu, Shijin Wang</dc:creator>
    </item>
    <item>
      <title>An Elemental Ethics for Artificial Intelligence: Water as Resistance Within AI's Value Chain</title>
      <link>https://arxiv.org/abs/2403.14677</link>
      <description>arXiv:2403.14677v1 Announce Type: new 
Abstract: Research and activism have increasingly denounced the problematic environmental record of the infrastructure and value chain underpinning Artificial Intelligence (AI). Water-intensive data centres, polluting mineral extraction and e-waste dumping are incontrovertibly part of AI's footprint. In this article, I turn to areas affected by AI-fuelled environmental harm and identify an ethics of resistance emerging from local activists, which I term 'elemental ethics'. Elemental ethics interrogates the AI value chain's problematic relationship with the elements that make up the world, critiques the undermining of local and ancestral approaches to nature and reveals the vital and quotidian harms engendered by so-called intelligent systems. While this ethics is emerging from grassroots and Indigenous groups, it echoes recent calls from environmental philosophy to reconnect with the environment via the elements. In empirical terms, this article looks at groups in Chile resisting a Google data centre project in Santiago and lithium extraction (used for rechargeable batteries) in Lickan Antay Indigenous territory, Atacama Desert. As I show, elemental ethics can complement top-down, utilitarian and quantitative approaches to AI ethics and sustainable AI as well as interrogate whose lived experience and well-being counts in debates on AI extinction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14677v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Lehuede</dc:creator>
    </item>
    <item>
      <title>Trust in AI: Progress, Challenges, and Future Directions</title>
      <link>https://arxiv.org/abs/2403.14680</link>
      <description>arXiv:2403.14680v1 Announce Type: new 
Abstract: The increasing use of artificial intelligence (AI) systems in our daily life through various applications, services, and products explains the significance of trust/distrust in AI from a user perspective. AI-driven systems (as opposed to other technologies) have ubiquitously diffused in our life not only as some beneficial tools to be used by human agents but also are going to be substitutive agents on our behalf, or manipulative minds that would influence human thought, decision, and agency. Trust/distrust in AI plays the role of a regulator and could significantly control the level of this diffusion, as trust can increase, and distrust may reduce the rate of adoption of AI. Recently, varieties of studies have paid attention to the variant dimension of trust/distrust in AI, and its relevant considerations. In this systematic literature review, after conceptualization of trust in the current AI literature review, we will investigate trust in different types of human-Machine interaction, and its impact on technology acceptance in different domains. In addition to that, we propose a taxonomy of technical (i.e., safety, accuracy, robustness) and non-technical axiological (i.e., ethical, legal, and mixed) trustworthiness metrics, and some trustworthy measurements. Moreover, we examine some major trust-breakers in AI (e.g., autonomy and dignity threat), and trust makers; and propose some future directions and probable solutions for the transition to a trustworthy AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14680v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saleh Afroogh, Ali Akbari, Evan Malone, Mohammadali Kargar, Hananeh Alambeigi</dc:creator>
    </item>
    <item>
      <title>AI Ethics: A Bibliometric Analysis, Critical Issues, and Key Gaps</title>
      <link>https://arxiv.org/abs/2403.14681</link>
      <description>arXiv:2403.14681v1 Announce Type: new 
Abstract: Artificial intelligence (AI) ethics has emerged as a burgeoning yet pivotal area of scholarly research. This study conducts a comprehensive bibliometric analysis of the AI ethics literature over the past two decades. The analysis reveals a discernible tripartite progression, characterized by an incubation phase, followed by a subsequent phase focused on imbuing AI with human-like attributes, culminating in a third phase emphasizing the development of human-centric AI systems. After that, they present seven key AI ethics issues, encompassing the Collingridge dilemma, the AI status debate, challenges associated with AI transparency and explainability, privacy protection complications, considerations of justice and fairness, concerns about algocracy and human enfeeblement, and the issue of superintelligence. Finally, they identify two notable research gaps in AI ethics regarding the large ethics model (LEM) and AI identification and extend an invitation for further scholarly research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14681v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.4018/IJBAN.338367</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Business Analytics (IJBAN), 2024, 11(1), 1-19</arxiv:journal_reference>
      <dc:creator>Di Kevin Gao (Mississippi State University, California State University - East Bay), Andrew Haverly (Mississippi State University), Sudip Mittal (Mississippi State University), Jiming Wu (California State University - East Bay), Jingdao Chen (Mississippi State University)</dc:creator>
    </item>
    <item>
      <title>A Moral Imperative: The Need for Continual Superalignment of Large Language Models</title>
      <link>https://arxiv.org/abs/2403.14683</link>
      <description>arXiv:2403.14683v1 Announce Type: new 
Abstract: This paper examines the challenges associated with achieving life-long superalignment in AI systems, particularly large language models (LLMs). Superalignment is a theoretical framework that aspires to ensure that superintelligent AI systems act in accordance with human values and goals. Despite its promising vision, we argue that achieving superalignment requires substantial changes in the current LLM architectures due to their inherent limitations in comprehending and adapting to the dynamic nature of these human ethics and evolving global scenarios. We dissect the challenges of encoding an ever-changing spectrum of human values into LLMs, highlighting the discrepancies between static AI models and the dynamic nature of human societies. To illustrate these challenges, we analyze two distinct examples: one demonstrates a qualitative shift in human values, while the other presents a quantifiable change. Through these examples, we illustrate how LLMs, constrained by their training data, fail to align with contemporary human values and scenarios. The paper concludes by exploring potential strategies to address and possibly mitigate these alignment discrepancies, suggesting a path forward in the pursuit of more adaptable and responsive AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14683v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gokul Puthumanaillam, Manav Vora, Pranay Thangeda, Melkior Ornik</dc:creator>
    </item>
    <item>
      <title>Evaluating Pedagogical Incentives in Undergraduate Computing: A Mixed Methods Approach Using Learning Analytics</title>
      <link>https://arxiv.org/abs/2403.14686</link>
      <description>arXiv:2403.14686v1 Announce Type: new 
Abstract: In the context of higher education's evolving dynamics post-COVID-19, this paper assesses the impact of new pedagogical incentives implemented in a first-year undergraduate computing module at University College London. We employ a mixed methods approach, combining learning analytics with qualitative data, to evaluate the effectiveness of these incentives on increasing student engagement.
  A longitudinal overview of resource interactions is mapped through Bayesian network analysis of Moodle activity logs from 204 students. This analysis identifies early resource engagement as a predictive indicator of continued engagement while also suggesting that the new incentives disproportionately benefit highly engaged students. Focus group discussions complement this analysis, providing insights into student perceptions of the pedagogical changes and the module design. These qualitative findings underscore the challenge of sustaining engagement through the new incentives and highlight the importance of communication in blended learning environments.
  Our paper introduces an interpretable and actionable model for student engagement, which integrates objective, data-driven analysis with students' perspectives. This model provides educators with a tool to evaluate and improve instructional strategies. By demonstrating the effectiveness of our mixed methods approach in capturing the intricacies of student behaviour in digital learning environments, we underscore the model's potential to improve online pedagogical practices across diverse educational settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14686v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura J. Johnston, Takoua Jendoubi</dc:creator>
    </item>
    <item>
      <title>Developing and Deploying Industry Standards for Artificial Intelligence in Education (AIED): Challenges, Strategies, and Future Directions</title>
      <link>https://arxiv.org/abs/2403.14689</link>
      <description>arXiv:2403.14689v1 Announce Type: new 
Abstract: The adoption of Artificial Intelligence in Education (AIED) holds the promise of revolutionizing educational practices by offering personalized learning experiences, automating administrative and pedagogical tasks, and reducing the cost of content creation. However, the lack of standardized practices in the development and deployment of AIED solutions has led to fragmented ecosystems, which presents challenges in interoperability, scalability, and ethical governance. This article aims to address the critical need to develop and implement industry standards in AIED, offering a comprehensive analysis of the current landscape, challenges, and strategic approaches to overcome these obstacles. We begin by examining the various applications of AIED in various educational settings and identify key areas lacking in standardization, including system interoperability, ontology mapping, data integration, evaluation, and ethical governance. Then, we propose a multi-tiered framework for establishing robust industry standards for AIED. In addition, we discuss methodologies for the iterative development and deployment of standards, incorporating feedback loops from real-world applications to refine and adapt standards over time. The paper also highlights the role of emerging technologies and pedagogical theories in shaping future standards for AIED. Finally, we outline a strategic roadmap for stakeholders to implement these standards, fostering a cohesive and ethical AIED ecosystem. By establishing comprehensive industry standards, such as those by IEEE Artificial Intelligence Standards Committee (AISC) and International Organization for Standardization (ISO), we can accelerate and scale AIED solutions to improve educational outcomes, ensuring that technological advances align with the principles of inclusivity, fairness, and educational excellence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14689v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard Tong, Haoyang Li, Joleen Liang, Qingsong Wen</dc:creator>
    </item>
    <item>
      <title>Incorporating Graph Attention Mechanism into Geometric Problem Solving Based on Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2403.14690</link>
      <description>arXiv:2403.14690v1 Announce Type: new 
Abstract: In the context of online education, designing an automatic solver for geometric problems has been considered a crucial step towards general math Artificial Intelligence (AI), empowered by natural language understanding and traditional logical inference. In most instances, problems are addressed by adding auxiliary components such as lines or points. However, adding auxiliary components automatically is challenging due to the complexity in selecting suitable auxiliary components especially when pivotal decisions have to be made. The state-of-the-art performance has been achieved by exhausting all possible strategies from the category library to identify the one with the maximum likelihood. However, an extensive strategy search have to be applied to trade accuracy for ef-ficiency. To add auxiliary components automatically and efficiently, we present deep reinforcement learning framework based on the language model, such as BERT. We firstly apply the graph attention mechanism to reduce the strategy searching space, called AttnStrategy, which only focus on the conclusion-related components. Meanwhile, a novel algorithm, named Automatically Adding Auxiliary Components using Reinforcement Learning framework (A3C-RL), is proposed by forcing an agent to select top strategies, which incorporates the AttnStrategy and BERT as the memory components. Results from extensive experiments show that the proposed A3C-RL algorithm can substantially enhance the average precision by 32.7% compared to the traditional MCTS. In addition, the A3C-RL algorithm outperforms humans on the geometric questions from the annual University Entrance Mathematical Examination of China.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14690v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xiuqin Zhong, Shengyuan Yan, Gongqi Lin, Hongguang Fu, Liang Xu, Siwen Jiang, Lei Huang, Wei Fang</dc:creator>
    </item>
    <item>
      <title>Large Language Models and User Trust: Focus on Healthcare</title>
      <link>https://arxiv.org/abs/2403.14691</link>
      <description>arXiv:2403.14691v1 Announce Type: new 
Abstract: This paper explores the evolving relationship between clinician trust in LLMs, the transformation of data sources from predominantly human-generated to AI-generated content, and the subsequent impact on the precision of LLMs and clinician competence. One of the primary concerns identified is the potential feedback loop that arises as LLMs become more reliant on their outputs for learning, which may lead to a degradation in output quality and a reduction in clinician skills due to decreased engagement with fundamental diagnostic processes. While theoretical at this stage, this feedback loop poses a significant challenge as the integration of LLMs in healthcare deepens, emphasizing the need for proactive dialogue and strategic measures to ensure the safe and effective use of LLM technology. Moreover, we delve into the potential risks associated with LLMs' self-referential learning loops and the deskilling of healthcare professionals. The risk of LLMs operating within an echo chamber, where AI-generated content feeds into the learning algorithms, threatens the diversity and quality of the data pool, potentially entrenching biases and reducing the efficacy of LLMs. Concurrently, reliance on LLMs for routine or critical tasks could result in a decline in healthcare providers' diagnostic and thinking skills, particularly affecting the training and development of future professionals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14691v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Avishek Choudhury, Zaria Chaudhry</dc:creator>
    </item>
    <item>
      <title>The AI Assessment Scale (AIAS) in action: A pilot implementation of GenAI supported assessment</title>
      <link>https://arxiv.org/abs/2403.14692</link>
      <description>arXiv:2403.14692v1 Announce Type: new 
Abstract: The rapid adoption of Generative Artificial Intelligence (GenAI) technologies in higher education has raised concerns about academic integrity, assessment practices, and student learning. Banning or blocking GenAI tools has proven ineffective, and punitive approaches ignore the potential benefits of these technologies. This paper presents the findings of a pilot study conducted at British University Vietnam (BUV) exploring the implementation of the Artificial Intelligence Assessment Scale (AIAS), a flexible framework for incorporating GenAI into educational assessments. The AIAS consists of five levels, ranging from 'No AI' to 'Full AI', enabling educators to design assessments that focus on areas requiring human input and critical thinking.
  Following the implementation of the AIAS, the pilot study results indicate a significant reduction in academic misconduct cases related to GenAI, a 5.9% increase in student attainment across the university, and a 33.3% increase in module passing rates. The AIAS facilitated a shift in pedagogical practices, with faculty members incorporating GenAI tools into their modules and students producing innovative multimodal submissions. The findings suggest that the AIAS can support the effective integration of GenAI in HE, promoting academic integrity while leveraging the technology's potential to enhance learning experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14692v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Leon Furze, Mike Perkins, Jasper Roe, Jason MacVaugh</dc:creator>
    </item>
    <item>
      <title>A2CI: A Cloud-based, Service-oriented Geospatial Cyberinfrastructure to Support Atmospheric Research</title>
      <link>https://arxiv.org/abs/2403.14693</link>
      <description>arXiv:2403.14693v1 Announce Type: new 
Abstract: Big earth science data offers the scientific community great opportunities. Many more studies at large-scales, over long-terms and at high resolution can now be conducted using the rich information collected by remote sensing satellites, ground-based sensor networks, and even social media input. However, the hundreds of terabytes of information collected and compiled on an hourly basis by NASA and other government agencies present a significant challenge for atmospheric scientists seeking to improve the understanding of the Earth atmospheric system. These challenges include effective discovery, organization, analysis and visualization of large amounts of data. This paper reports the outcomes of an NSF-funded project that developed a geospatial cyberinfrastructure -- the A2CI (Atmospheric Analysis Cyberinfrastructure) -- to support atmospheric research. We first introduce the service-oriented system framework then describe in detail the implementation of the data discovery module, data management module, data integration module, data analysis and visualization modules following the cloud computing principles-Data-as-a-Service, Software-as-a-Service, Platform-as-a-Service and Infrastructure-as-a-Service. We demonstrate the graphic user interface by performing an analysis between Sea Surface Temperature and the intensity of tropical storms in the North Atlantic and Pacific oceans. We expect this work to contribute to the technical advancement of cyberinfrastructure research as well as to the development of an online, collaborative scientific analysis system for atmospheric science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14693v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.IR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenwen Li, Hu Shao, Sizhe Wang, Xiran Zhou, Sheng Wu</dc:creator>
    </item>
    <item>
      <title>Application of GPT Language Models for Innovation in Activities in University Teaching</title>
      <link>https://arxiv.org/abs/2403.14694</link>
      <description>arXiv:2403.14694v1 Announce Type: new 
Abstract: The GPT (Generative Pre-trained Transformer) language models are an artificial intelligence and natural language processing technology that enables automatic text generation. There is a growing interest in applying GPT language models to university teaching in various dimensions. From the perspective of innovation in student and teacher activities, they can provide support in understanding and generating content, problem-solving, as well as personalization and test correction, among others. From the dimension of internationalization, the misuse of these models represents a global problem that requires taking a series of common measures in universities from different geographical areas. In several countries, there has been a review of assessment tools to ensure that work is done by students and not by AI. To this end, we have conducted a detailed experiment in a representative subject of Computer Science such as Software Engineering, which has focused on evaluating the use of ChatGPT as an assistant in theory activities, exercises, and laboratory practices, assessing its potential use as a support tool for both students and teachers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14694v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manuel de Buenaga, Francisco Javier Bueno</dc:creator>
    </item>
    <item>
      <title>MOTIV: Visual Exploration of Moral Framing in Social Media</title>
      <link>https://arxiv.org/abs/2403.14696</link>
      <description>arXiv:2403.14696v1 Announce Type: new 
Abstract: We present a visual computing framework for analyzing moral rhetoric on social media around controversial topics. Using Moral Foundation Theory, we propose a methodology for deconstructing and visualizing the \textit{when}, \textit{where}, and \textit{who} behind each of these moral dimensions as expressed in microblog data. We characterize the design of this framework, developed in collaboration with experts from language processing, communications, and causal inference. Our approach integrates microblog data with multiple sources of geospatial and temporal data, and leverages unsupervised machine learning (generalized additive models) to support collaborative hypothesis discovery and testing. We implement this approach in a system named MOTIV. We illustrate this approach on two problems, one related to Stay-at-home policies during the COVID-19 pandemic, and the other related to the Black Lives Matter movement. Through detailed case studies and discussions with collaborators, we identify several insights discovered regarding the different drivers of moral sentiment in social media. Our results indicate that this visual approach supports rapid, collaborative hypothesis testing, and can help give insights into the underlying moral values behind controversial political issues.
  Supplemental Material: https://osf.io/ygkzn/?view_only=6310c0886938415391d977b8aae8b749</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14696v1</guid>
      <category>cs.CY</category>
      <category>cs.GR</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Wentzel, Lauren Levine, Vipul Dhariwal, Zarah Fatemi, Abarai Bhattacharya, Barbara Di Eugenio, Andrew Rojecki, Elena Zheleva, G. Elisabeta Marai</dc:creator>
    </item>
    <item>
      <title>An AIC-based approach for articulating unpredictable problems in open complex environments</title>
      <link>https://arxiv.org/abs/2403.14697</link>
      <description>arXiv:2403.14697v1 Announce Type: new 
Abstract: This research paper presents an approach to enhancing the predictive capability of architects in the design and assurance of systems, focusing on systems operating in dynamic and unpredictable environments. By adopting a systems approach, we aim to improve architects' predictive capabilities in designing dependable systems (for example, ML-based systems). An aerospace case study is used to illustrate the approach. Multiple factors (challenges) influencing aircraft detection are identified, demonstrating the effectiveness of our approach in a complex operational setting. Our approach primarily aimed to enhance the architect's predictive capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14697v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haider AL-Shareefy, Michael Butler, Thai Son Hoang</dc:creator>
    </item>
    <item>
      <title>Digital Twins: How Far from Ideas to Twins?</title>
      <link>https://arxiv.org/abs/2403.14699</link>
      <description>arXiv:2403.14699v1 Announce Type: new 
Abstract: As a bridge from virtuality to reality, Digital Twin has increased in popularity since proposed. Ideas have been proposed theoretical and practical for digital twins. From theoretical perspective, digital twin is fusion of data mapping between modalities; from practical point of view, digital twin is scenario implementation based on the Internet of Things and models. From these two perspectives, we explore the researches from idea to realization of digital twins and discuss thoroughly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14699v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lu Jingyu</dc:creator>
    </item>
    <item>
      <title>Rule based Complex Event Processing for an Air Quality Monitoring System in Smart City</title>
      <link>https://arxiv.org/abs/2403.14701</link>
      <description>arXiv:2403.14701v1 Announce Type: new 
Abstract: In recent years, smart city-based development has gained momentum due to its versatile nature in architecture and planning for the systematic habitation of human beings. According to World Health Organization (WHO) report, air pollution causes serious respiratory diseases. Hence, it becomes necessary to real-time monitoring of air quality to minimize effect by taking time-bound decisions by the stakeholders. The air pollution comprises various compositions such as NH3, O3, SO2, NO2, etc., and their concentrations vary from location to location.The research work proposes an integrated framework for monitoring air quality using rule-based Complex Event Processing (CEP) and SPARQL queries. CEP works with the data stream based on predefined rules to detect the complex pattern, which helps in decision support for stakeholders. Initially, the dataset was collected from the Central Pollution Control Board (CPCB) of India and this data was then preprocessed and passed through Apache Kafka. Then a knowledge graph developed based on the air quality paradigm. Consequently, convert preprocessed data into Resource Description Framework (RDF) data, and integrate with Knowledge graph which is ingested to CEP engine using Apache Jena for enhancing the decision support . Simultaneously, rules are extracted using a decision tree, and some ground truth parameters of CPCB are added and ingested to the CEP engine to determine the complex patterns. Consequently, the SPARQL query is used on real-time RDF dataset for fetching the condition of air quality as good, poor, severe, hazardous etc based on complex events detection. For validating the proposed approach various chunks of RDF are used for the deployment of events to the CEP engine, and its performance is examined over time while performing simple and complex queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14701v1</guid>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shashi Shekhar Kumar, Ritesh Chandra, Sonali Agarwal</dc:creator>
    </item>
    <item>
      <title>Large language model-powered chatbots for internationalizing student support in higher education</title>
      <link>https://arxiv.org/abs/2403.14702</link>
      <description>arXiv:2403.14702v1 Announce Type: new 
Abstract: This research explores the integration of chatbot technology powered by GPT-3.5 and GPT-4 Turbo into higher education to enhance internationalization and leverage digital transformation. It delves into the design, implementation, and application of Large Language Models (LLMs) for improving student engagement, information access, and support. Utilizing technologies like Python 3, GPT API, LangChain, and Chroma Vector Store, the research emphasizes creating a high-quality, timely, and relevant transcript dataset for chatbot testing. Findings indicate the chatbot's efficacy in providing comprehensive responses, its preference over traditional methods by users, and a low error rate. Highlighting the chatbot's real-time engagement, memory capabilities, and critical data access, the study demonstrates its potential to elevate accessibility, efficiency, and satisfaction. Concluding, the research suggests the chatbot significantly aids higher education internationalization, proposing further investigation into digital technology's role in educational enhancement and strategy development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14702v1</guid>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Achraf Hsain, Hamza El Housni</dc:creator>
    </item>
    <item>
      <title>Safeguarding Marketing Research: The Generation, Identification, and Mitigation of AI-Fabricated Disinformation</title>
      <link>https://arxiv.org/abs/2403.14706</link>
      <description>arXiv:2403.14706v1 Announce Type: new 
Abstract: Generative AI has ushered in the ability to generate content that closely mimics human contributions, introducing an unprecedented threat: Deployed en masse, these models can be used to manipulate public opinion and distort perceptions, resulting in a decline in trust towards digital platforms. This study contributes to marketing literature and practice in three ways. First, it demonstrates the proficiency of AI in fabricating disinformative user-generated content (UGC) that mimics the form of authentic content. Second, it quantifies the disruptive impact of such UGC on marketing research, highlighting the susceptibility of analytics frameworks to even minimal levels of disinformation. Third, it proposes and evaluates advanced detection frameworks, revealing that standard techniques are insufficient for filtering out AI-generated disinformation. We advocate for a comprehensive approach to safeguarding marketing research that integrates advanced algorithmic solutions, enhanced human oversight, and a reevaluation of regulatory and ethical frameworks. Our study seeks to serve as a catalyst, providing a foundation for future research and policy-making aimed at navigating the intricate challenges at the nexus of technology, ethics, and marketing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14706v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anirban Mukherjee</dc:creator>
    </item>
    <item>
      <title>Synergy of Information in Multimodal IoT Systems -- Discovering the impact of daily behaviour routines on physical activity level</title>
      <link>https://arxiv.org/abs/2403.14707</link>
      <description>arXiv:2403.14707v1 Announce Type: new 
Abstract: The intricate connection between daily behaviours and health necessitates robust behaviour monitoring, particularly with the advent of IoT systems. This study introduces an innovative approach, exploiting the synergy of information from various IoT sources, to assess the alignment of behaviour routines with health guidelines. We grouped routines based on guideline compliance and used a clustering method to identify similarities in behaviours and key characteristics within each cluster. Applied to an elderly care case study, our approach unveils patterns leading to physical inactivity by categorising days based on recommended daily steps. Utilising data from wristbands, smartphones, and ambient sensors, the study provides insights not achievable with single-source data. Visualisation in a calendar view aids health experts in understanding patient behaviours, enabling precise interventions. Notably, the approach facilitates early detection of behaviour changes during events like COVID-19 and Ramadan, available in our dataset. This work signifies a promising path for behavioural analysis and discovering variations to empower smart healthcare, offering insights into patient health, personalised interventions, and healthier routines through continuous IoT-driven data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14707v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mohsen Shirali, Zahra Ahmadi, Carlos Fern\'andez-Llatas, Jose-Luis Bayo-Monton</dc:creator>
    </item>
    <item>
      <title>Visualizing Progress in Broadening Participation in Computing: The Value of Context</title>
      <link>https://arxiv.org/abs/2403.14708</link>
      <description>arXiv:2403.14708v1 Announce Type: new 
Abstract: Concerns about representation in computing within the U.S. have driven numerous activities to broaden participation. Assessment of the impact of these efforts and, indeed, a clear assessment of the actual "problem" being addressed are limited by the nature of the most common data analysis which looks at the representation of each population as a percentage of the number of students graduating with a degree in computing. This use of a single metric cannot adequately assess the impact of broadening participation efforts. First, this approach fails to account for changing demographics of the undergraduate population in terms of overall numbers and relative proportion of the Federally designated gender, race, and ethnicity groupings. A second issue is that the majority of literature on broadening participation in computing (BPC) reports data on gender or on race/ethnicity, omitting data on students' intersectional identities. This leads to an incorrect understanding of both the data and the challenges we face as a field. In this paper we present several different approaches to tracking the impact of BPC efforts. We make three recommendations: 1) cohort-based analysis should be used to accurately show student engagement in computing; 2) the field as a whole needs to adopt the norm of always reporting intersectional data; 3) university demographic context matters when looking at how well a CS department is doing to broaden participation in computing, including longitudinal analysis of university demographic shifts that impact the local demographics of computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14708v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Valerie Barr, Carla E. Brodley, Manuel A. P\'erez-Qui\~nones</dc:creator>
    </item>
    <item>
      <title>ClimateQ&amp;A: Bridging the gap between climate scientists and the general public</title>
      <link>https://arxiv.org/abs/2403.14709</link>
      <description>arXiv:2403.14709v1 Announce Type: new 
Abstract: This research paper investigates public views on climate change and biodiversity loss by analyzing questions asked to the ClimateQ&amp;A platform. ClimateQ&amp;A is a conversational agent that uses LLMs to respond to queries based on over 14,000 pages of scientific literature from the IPCC and IPBES reports. Launched online in March 2023, the tool has gathered over 30,000 questions, mainly from a French audience. Its chatbot interface allows for the free formulation of questions related to nature*. While its main goal is to make nature science more accessible, it also allows for the collection and analysis of questions and their themes. Unlike traditional surveys involving closed questions, this novel method offers a fresh perspective on individual interrogations about nature. Running NLP clustering algorithms on a sample of 3,425 questions, we find that a significant 25.8% inquire about how climate change and biodiversity loss will affect them personally (e.g., where they live or vacation, their consumption habits) and the specific impacts of their actions on nature (e.g., transportation or food choices). This suggests that traditional methods of surveying may not identify all existing knowledge gaps, and that relying solely on IPCC and IPBES reports may not address all individual inquiries about climate and biodiversity, potentially affecting public understanding and action on these issues. *we use 'nature' as an umbrella term for 'climate change' and 'biodiversity loss'</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14709v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Natalia De La Calzada, Th\'eo Alves Da Costa, Annabelle Blangero, Nicolas Chesneau</dc:creator>
    </item>
    <item>
      <title>Use of recommendation models to provide support to dyslexic students</title>
      <link>https://arxiv.org/abs/2403.14710</link>
      <description>arXiv:2403.14710v1 Announce Type: new 
Abstract: Dyslexia is the most widespread specific learning disorder and significantly impair different cognitive domains. This, in turn, negatively affects dyslexic students during their learning path. Therefore, specific support must be given to these students. In addition, such a support must be highly personalized, since the problems generated by the disorder can be very different from one to another. In this work, we explored the possibility of using AI to suggest the most suitable supporting tools for dyslexic students, so as to provide a targeted help that can be of real utility. To do this, we relied on recommendation algorithms, which are a branch of machine learning, that aim to detect personal preferences and provide the most suitable suggestions. We hence implemented and trained three collaborative-filtering recommendation models, namely an item-based, a user-based and a weighted-hybrid model, and studied their performance on a large database of 1237 students' information, collected with a self-evaluating questionnaire regarding all the most used supporting strategies and digital tools. Each recommendation model was tested with three different similarity metrics, namely Pearson correlation, Euclidean distance and Cosine similarity. The obtained results showed that a recommendation system is highly effective in suggesting the optimal help tools/strategies for everyone. This demonstrates that the proposed approach is successful and can be used as a new and effective methodology to support students with dyslexia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14710v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gianluca Morciano, Jos\'e Manuel Alcalde-Llergo, Andrea Zingoni, Enrique Yeguas-Bolivar, Juri Taborri, Giuseppe Calabr\`o</dc:creator>
    </item>
    <item>
      <title>Human-in-the-Loop AI for Cheating Ring Detection</title>
      <link>https://arxiv.org/abs/2403.14711</link>
      <description>arXiv:2403.14711v1 Announce Type: new 
Abstract: Online exams have become popular in recent years due to their accessibility. However, some concerns have been raised about the security of the online exams, particularly in the context of professional cheating services aiding malicious test takers in passing exams, forming so-called "cheating rings". In this paper, we introduce a human-in-the-loop AI cheating ring detection system designed to detect and deter these cheating rings. We outline the underlying logic of this human-in-the-loop AI system, exploring its design principles tailored to achieve its objectives of detecting cheaters. Moreover, we illustrate the methodologies used to evaluate its performance and fairness, aiming to mitigate the unintended risks associated with the AI system. The design and development of the system adhere to Responsible AI (RAI) standards, ensuring that ethical considerations are integrated throughout the entire development process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14711v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yong-Siang Shih, Manqian Liao, Ruidong Liu, Mirza Basim Baig</dc:creator>
    </item>
    <item>
      <title>AI for bureaucratic productivity: Measuring the potential of AI to help automate 143 million UK government transactions</title>
      <link>https://arxiv.org/abs/2403.14712</link>
      <description>arXiv:2403.14712v1 Announce Type: new 
Abstract: There is currently considerable excitement within government about the potential of artificial intelligence to improve public service productivity through the automation of complex but repetitive bureaucratic tasks, freeing up the time of skilled staff. Here, we explore the size of this opportunity, by mapping out the scale of citizen-facing bureaucratic decision-making procedures within UK central government, and measuring their potential for AI-driven automation. We estimate that UK central government conducts approximately one billion citizen-facing transactions per year in the provision of around 400 services, of which approximately 143 million are complex repetitive transactions. We estimate that 84% of these complex transactions are highly automatable, representing a huge potential opportunity: saving even an average of just one minute per complex transaction would save the equivalent of approximately 1,200 person-years of work every year. We also develop a model to estimate the volume of transactions a government service undertakes, providing a way for government to avoid conducting time consuming transaction volume measurements. Finally, we find that there is high turnover in the types of services government provide, meaning that automation efforts should focus on general procedures rather than services themselves which are likely to evolve over time. Overall, our work presents a novel perspective on the structure and functioning of modern government, and how it might evolve in the age of artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14712v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent J. Straub, Youmna Hashem, Jonathan Bright, Satyam Bhagwanani, Deborah Morgan, John Francis, Saba Esnaashari, Helen Margetts</dc:creator>
    </item>
    <item>
      <title>Auditing Fairness under Unobserved Confounding</title>
      <link>https://arxiv.org/abs/2403.14713</link>
      <description>arXiv:2403.14713v1 Announce Type: new 
Abstract: A fundamental problem in decision-making systems is the presence of inequity across demographic lines. However, inequity can be difficult to quantify, particularly if our notion of equity relies on hard-to-measure notions like risk (e.g., equal access to treatment for those who would die without it). Auditing such inequity requires accurate measurements of individual risk, which is difficult to estimate in the realistic setting of unobserved confounding. In the case that these unobservables "explain" an apparent disparity, we may understate or overstate inequity. In this paper, we show that one can still give informative bounds on allocation rates among high-risk individuals, even while relaxing or (surprisingly) even when eliminating the assumption that all relevant risk factors are observed. We utilize the fact that in many real-world settings (e.g., the introduction of a novel treatment) we have data from a period prior to any allocation, to derive unbiased estimates of risk. We demonstrate the effectiveness of our framework on a real-world study of Paxlovid allocation to COVID-19 patients, finding that observed racial inequity cannot be explained by unobserved confounders of the same strength as important observed covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14713v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yewon Byun, Dylan Sam, Michael Oberst, Zachary C. Lipton, Bryan Wilder</dc:creator>
    </item>
    <item>
      <title>Individual and Product-Related Antecedents of Electronic Word-of-Mouth</title>
      <link>https://arxiv.org/abs/2403.14717</link>
      <description>arXiv:2403.14717v1 Announce Type: new 
Abstract: This research investigates the antecedents of positive and negative electronic word-of-mouth (eWOM) propensity, as well as the impact of eWOM propensity on the intention to repurchase the product. Two types of eWOM predictors were considered: product related variables and personal factors. The data were collected through an online survey conducted on a sample of 335 Romanian subjects, and the analysis method was Structural Equation Modeling. Our findings show that personal factors - social media usage behavior, marketing mavenism and need to evaluate - are the most important antecedents of the intention to write product reviews and comments online, either positive or negative. From the product related factors, only brand trust influences the propensity to provide eWOM. Furthermore, both positive and negative eWOM intentions are associated with the repurchase intention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14717v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bogdan Anastasiei, Nicoleta Dospinescu, Octavian Dospinescu</dc:creator>
    </item>
    <item>
      <title>Protected group bias and stereotypes in Large Language Models</title>
      <link>https://arxiv.org/abs/2403.14727</link>
      <description>arXiv:2403.14727v1 Announce Type: new 
Abstract: As modern Large Language Models (LLMs) shatter many state-of-the-art benchmarks in a variety of domains, this paper investigates their behavior in the domains of ethics and fairness, focusing on protected group bias. We conduct a two-part study: first, we solicit sentence continuations describing the occupations of individuals from different protected groups, including gender, sexuality, religion, and race. Second, we have the model generate stories about individuals who hold different types of occupations. We collect &gt;10k sentence completions made by a publicly available LLM, which we subject to human annotation. We find bias across minoritized groups, but in particular in the domains of gender and sexuality, as well as Western bias, in model generations. The model not only reflects societal biases, but appears to amplify them. The model is additionally overly cautious in replies to queries relating to minoritized groups, providing responses that strongly emphasize diversity and equity to an extent that other group characteristics are overshadowed. This suggests that artificially constraining potentially harmful outputs may itself lead to harm, and should be applied in a careful and controlled manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14727v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hadas Kotek, David Q. Sun, Zidi Xiu, Margit Bowler, Christopher Klein</dc:creator>
    </item>
    <item>
      <title>Particip-AI: A Democratic Surveying Framework for Anticipating Future AI Use Cases, Harms and Benefits</title>
      <link>https://arxiv.org/abs/2403.14791</link>
      <description>arXiv:2403.14791v1 Announce Type: new 
Abstract: General purpose AI, such as ChatGPT, seems to have lowered the barriers for the public to use AI and harness its power. However, the governance and development of AI still remain in the hands of a few, and the pace of development is accelerating without proper assessment of risks. As a first step towards democratic governance and risk assessment of AI, we introduce Particip-AI, a framework to gather current and future AI use cases and their harms and benefits from non-expert public. Our framework allows us to study more nuanced and detailed public opinions on AI through collecting use cases, surfacing diverse harms through risk assessment under alternate scenarios (i.e., developing and not developing a use case), and illuminating tensions over AI development through making a concluding choice on its development. To showcase the promise of our framework towards guiding democratic AI, we gather responses from 295 demographically diverse participants. We find that participants' responses emphasize applications for personal life and society, contrasting with most current AI development's business focus. This shows the value of surfacing diverse harms that are complementary to expert assessments. Furthermore, we found that perceived impact of not developing use cases predicted participants' judgements of whether AI use cases should be developed, and highlighted lay users' concerns of techno-solutionism. We conclude with a discussion on how frameworks like Particip-AI can further guide democratic AI governance and regulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14791v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jimin Mun, Liwei Jiang, Jenny Liang, Inyoung Cheong, Nicole DeCario, Yejin Choi, Tadayoshi Kohno, Maarten Sap</dc:creator>
    </item>
    <item>
      <title>Investigating Bias in LLM-Based Bias Detection: Disparities between LLMs and Human Perception</title>
      <link>https://arxiv.org/abs/2403.14896</link>
      <description>arXiv:2403.14896v1 Announce Type: new 
Abstract: The pervasive spread of misinformation and disinformation in social media underscores the critical importance of detecting media bias. While robust Large Language Models (LLMs) have emerged as foundational tools for bias prediction, concerns about inherent biases within these models persist. In this work, we investigate the presence and nature of bias within LLMs and its consequential impact on media bias detection. Departing from conventional approaches that focus solely on bias detection in media content, we delve into biases within the LLM systems themselves. Through meticulous examination, we probe whether LLMs exhibit biases, particularly in political bias prediction and text continuation tasks. Additionally, we explore bias across diverse topics, aiming to uncover nuanced variations in bias expression within the LLM framework. Importantly, we propose debiasing strategies, including prompt engineering and model fine-tuning. Extensive analysis of bias tendencies across different LLMs sheds light on the broader landscape of bias propagation in language models. This study advances our understanding of LLM bias, offering critical insights into its implications for bias detection tasks and paving the way for more robust and equitable AI systems</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14896v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luyang Lin, Lingzhi Wang, Jinsong Guo, Kam-Fai Wong</dc:creator>
    </item>
    <item>
      <title>Learners Teaching Novices: An Uplifting Alternative Assessment</title>
      <link>https://arxiv.org/abs/2403.14971</link>
      <description>arXiv:2403.14971v1 Announce Type: new 
Abstract: We propose and carry-out a novel method of formative assessment called Assessment via Teaching (AVT), in which learners demonstrate their understanding of CS1 topics by tutoring more novice students. AVT has powerful benefits over traditional forms of assessment: it is centered around service to others and is highly rewarding for the learners who teach. Moreover, teaching greatly improves the learners' own understanding of the material and has a huge positive impact on novices, who receive free 1:1 tutoring. Lastly, this form of assessment is naturally difficult to cheat -- a critical property for assessments in the era of large-language models.
  We use AVT in a randomised control trial with learners in a CS1 course at an R1 university. The learners provide tutoring sessions to more novice students taking a lagged online version of the same course. We show that learners who do an AVT session before the course exam performed 20 to 30 percentage points better than the class average on several questions. Moreover, compared to students who did a practice exam, the AVT learners enjoyed their experience more and were twice as likely to study for their teaching session. We believe AVT is a scalable and uplifting method for formative assessment that could one day replace traditional exams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14971v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3626252.3630887</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 55th ACM Technical Symposium on Computer Science Education (SIGCSE); March 2024 (785-791)</arxiv:journal_reference>
      <dc:creator>Ali Malik, Juliette Woodrow, Chris Piech</dc:creator>
    </item>
    <item>
      <title>AI Teaches the Art of Elegant Coding: Timely, Fair, and Helpful Style Feedback in a Global Course</title>
      <link>https://arxiv.org/abs/2403.14986</link>
      <description>arXiv:2403.14986v1 Announce Type: new 
Abstract: Teaching students how to write code that is elegant, reusable, and comprehensible is a fundamental part of CS1 education. However, providing this "style feedback" in a timely manner has proven difficult to scale. In this paper, we present our experience deploying a novel, real-time style feedback tool in Code in Place, a large-scale online CS1 course. Our tool is based on the latest breakthroughs in large-language models (LLMs) and was carefully designed to be safe and helpful for students. We used our Real-Time Style Feedback tool (RTSF) in a class with over 8,000 diverse students from across the globe and ran a randomized control trial to understand its benefits. We show that students who received style feedback in real-time were five times more likely to view and engage with their feedback compared to students who received delayed feedback. Moreover, those who viewed feedback were more likely to make significant style-related edits to their code, with over 79% of these edits directly incorporating their feedback. We also discuss the practicality and dangers of LLM-based tools for feedback, investigating the quality of the feedback generated, LLM limitations, and techniques for consistency, standardization, and safeguarding against demographic bias, all of which are crucial for a tool utilized by students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14986v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3626252.3630773</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 55th ACM Technical Symposium on Computer Science Education (SIGCSE); March 2024 (1442-1448)</arxiv:journal_reference>
      <dc:creator>Juliette Woodrow, Ali Malik, Chris Piech</dc:creator>
    </item>
    <item>
      <title>InstaSynth: Opportunities and Challenges in Generating Synthetic Instagram Data with ChatGPT for Sponsored Content Detection</title>
      <link>https://arxiv.org/abs/2403.15214</link>
      <description>arXiv:2403.15214v1 Announce Type: new 
Abstract: Large Language Models (LLMs) raise concerns about lowering the cost of generating texts that could be used for unethical or illegal purposes, especially on social media. This paper investigates the promise of such models to help enforce legal requirements related to the disclosure of sponsored content online. We investigate the use of LLMs for generating synthetic Instagram captions with two objectives: The first objective (fidelity) is to produce realistic synthetic datasets. For this, we implement content-level and network-level metrics to assess whether synthetic captions are realistic. The second objective (utility) is to create synthetic data that is useful for sponsored content detection. For this, we evaluate the effectiveness of the generated synthetic data for training classifiers to identify undisclosed advertisements on Instagram. Our investigations show that the objectives of fidelity and utility may conflict and that prompt engineering is a useful but insufficient strategy. Additionally, we find that while individual synthetic posts may appear realistic, collectively they lack diversity, topic connectivity, and realistic user interaction patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15214v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Thales Bertaglia, Lily Heisig, Rishabh Kaushal, Adriana Iamnitchi</dc:creator>
    </item>
    <item>
      <title>KTbench: A Novel Data Leakage-Free Framework for Knowledge Tracing</title>
      <link>https://arxiv.org/abs/2403.15304</link>
      <description>arXiv:2403.15304v1 Announce Type: new 
Abstract: Knowledge Tracing (KT) is concerned with predicting students' future performance on learning items in intelligent tutoring systems. Learning items are tagged with skill labels called knowledge concepts (KCs). Many KT models expand the sequence of item-student interactions into KC-student interactions by replacing learning items with their constituting KCs. This often results in a longer sequence length. This approach addresses the issue of sparse item-student interactions and minimises model parameters. However, two problems have been identified with such models.
  The first problem is the model's ability to learn correlations between KCs belonging to the same item, which can result in the leakage of ground truth labels and hinder performance. This problem can lead to a significant decrease in performance on datasets with a higher number of KCs per item. The second problem is that the available benchmark implementations ignore accounting for changes in sequence length when expanding KCs, leading to different models being tested with varying sequence lengths but still compared against the same benchmark.
  To address these problems, we introduce a general masking framework that mitigates the first problem and enhances the performance of such KT models while preserving the original model architecture without significant alterations. Additionally, we introduce KTbench, an open-source benchmark library designed to ensure the reproducibility of this work while mitigating the second problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15304v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yahya Badran, Christine Preisach</dc:creator>
    </item>
    <item>
      <title>A Technological Perspective on Misuse of Available AI</title>
      <link>https://arxiv.org/abs/2403.15325</link>
      <description>arXiv:2403.15325v1 Announce Type: new 
Abstract: Potential malicious misuse of civilian artificial intelligence (AI) poses serious threats to security on a national and international level. Besides defining autonomous systems from a technological viewpoint and explaining how AI development is characterized, we show how already existing and openly available AI technology could be misused. To underline this, we developed three exemplary use cases of potentially misused AI that threaten political, digital and physical security. The use cases can be built from existing AI technologies and components from academia, the private sector and the developer-community. This shows how freely available AI can be combined into autonomous weapon systems. Based on the use cases, we deduce points of control and further measures to prevent the potential threat through misused AI. Further, we promote the consideration of malicious misuse of civilian AI systems in the discussion on autonomous weapon systems (AWS).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15325v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lukas P\"ohler, Valentin Schrader, Alexander Ladwein, Florian von Keller</dc:creator>
    </item>
    <item>
      <title>Weaponization of Conscience in Cybercrime and Online Fraud: A Novel Systems Theory</title>
      <link>https://arxiv.org/abs/2403.14667</link>
      <description>arXiv:2403.14667v1 Announce Type: cross 
Abstract: This article introduces the concept of weaponization of conscience as a complex system and tactic employed by fraudsters to camouflage their activity, coerce others, or to deceive their victims. This study adopts a conceptual approach, drawing from the theoretical underpinnings of military propaganda and psychological operations doctrines and adapting them to serve as a lens through which to understand and defend against weaponization of conscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14667v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michelle Espinoza</dc:creator>
    </item>
    <item>
      <title>The opportunities and risks of large language models in mental health</title>
      <link>https://arxiv.org/abs/2403.14814</link>
      <description>arXiv:2403.14814v1 Announce Type: cross 
Abstract: Global rates of mental health concerns are rising and there is increasing realization that existing models of mental healthcare will not adequately expand to meet the demand. With the emergence of large language models (LLMs) has come great optimism regarding their promise to create novel, large-scale solutions to support mental health. Despite their nascence, LLMs have already been applied to mental health-related tasks. In this review, we summarize the extant literature on efforts to use LLMs to provide mental health education, assessment, and intervention and highlight key opportunities for positive impact in each area. We then highlight risks associated with LLMs application to mental health and encourage adoption of strategies to mitigate these risks. The urgent need for mental health support must be balanced with responsible development, testing, and deployment of mental health LLMs. Especially critical is ensuring that mental health LLMs are fine-tuned for mental health, enhance mental health equity, adhere to ethical standards, and that people, including those with lived experience with mental health concerns, are involved in all stages from development through deployment. Prioritizing these efforts will minimize potential harms to mental health and maximize the likelihood that LLMs will positively impact mental health globally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14814v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannah R. Lawrence, Renee A. Schneider, Susan B. Rubin, Maja J. Mataric, Daniel J. McDuff, Megan Jones Bell</dc:creator>
    </item>
    <item>
      <title>Establishing a leader in a pairwise comparisons method</title>
      <link>https://arxiv.org/abs/2403.14885</link>
      <description>arXiv:2403.14885v1 Announce Type: cross 
Abstract: Abstract Like electoral systems, decision-making methods are also vulnerable to manipulation by decision-makers. The ability to effectively defend against such threats can only come from thoroughly understanding the manipulation mechanisms. In the presented article, we show two algorithms that can be used to launch a manipulation attack. They allow for equating the weights of two selected alternatives in the pairwise comparison method and, consequently, choosing a leader. The theoretical considerations are accompanied by a Monte Carlo simulation showing the relationship between the size of the PC matrix, the degree of inconsistency, and the ease of manipulation. This work is a continuation of our previous research published in the paper (Szybowski et al., 2023)</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14885v1</guid>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.DM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacek Szybowski, Konrad Ku{\l}akowski, Jiri Mazurek, Sebastian Ernst</dc:creator>
    </item>
    <item>
      <title>Beyond Quantities: Machine Learning-based Characterization of Inequality in Infrastructure Quality Provision in Cities</title>
      <link>https://arxiv.org/abs/2403.12074</link>
      <description>arXiv:2403.12074v2 Announce Type: replace 
Abstract: The objective of this study is to characterize inequality in infrastructure quality across urban areas. While a growing of body of literature has recognized the importance of characterizing infrastructure inequality in cities and provided quantified metrics to inform urban development plans, the majority of the existing approaches focus primarily on measuring the quantity of infrastructure, assuming that more infrastructure is better. Also, the existing research focuses primarily on index-based approaches in which the status of infrastructure provision in urban areas is determined based on assumed subjective weights. The focus on infrastructure quantity and use of indices obtained from subjective weights has hindered the ability to properly examine infrastructure inequality as it pertains to urban inequality and environmental justice considerations. Recognizing this gap, we propose a machine learning-based approach in which infrastructure features that shape environmental hazard exposure are identified and we use the weights obtained by the model to calculate an infrastructure quality provision for spatial areas of cities and accordingly, quantify the extent of inequality in infrastructure quality. The implementation of the model in five metropolitan areas in the U.S. demonstrates the capability of the proposed approach in characterizing inequality in infrastructure quality and capturing city-specific differences in the weights of infrastructure features. The results also show that areas in which low-income populations reside have lower infrastructure quality provision, suggesting the lower infrastructure quality provision as a determinant of urban disparities. Accordingly, the proposed approach can be effectively used to inform integrated urban design strategies to promote infrastructure equity and environmental justice based on data-driven and machine intelligence-based insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12074v2</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bo Li, Ali Mostafavi</dc:creator>
    </item>
    <item>
      <title>Predicting Generalization of AI Colonoscopy Models to Unseen Data</title>
      <link>https://arxiv.org/abs/2403.09920</link>
      <description>arXiv:2403.09920v3 Announce Type: replace-cross 
Abstract: $\textbf{Background}$: Generalizability of AI colonoscopy algorithms is important for wider adoption in clinical practice. However, current techniques for evaluating performance on unseen data require expensive and time-intensive labels.
  $\textbf{Methods}$: We use a "Masked Siamese Network" (MSN) to identify novel phenomena in unseen data and predict polyp detector performance. MSN is trained to predict masked out regions of polyp images, without any labels. We test MSN's ability to be trained on data only from Israel and detect unseen techniques, narrow-band imaging (NBI) and chromendoscoy (CE), on colonoscopes from Japan (354 videos, 128 hours). We also test MSN's ability to predict performance of Computer Aided Detection (CADe) of polyps on colonoscopies from both countries, even though MSN is not trained on data from Japan.
  $\textbf{Results}$: MSN correctly identifies NBI and CE as less similar to Israel whitelight than Japan whitelight (bootstrapped z-test, |z| &gt; 496, p &lt; 10^-8 for both) using the label-free Frechet distance. MSN detects NBI with 99% accuracy, predicts CE better than our heuristic (90% vs 79% accuracy) despite being trained only on whitelight, and is the only method that is robust to noisy labels. MSN predicts CADe polyp detector performance on in-domain Israel and out-of-domain Japan colonoscopies (r=0.79, 0.37 respectively). With few examples of Japan detector performance to train on, MSN prediction of Japan performance improves (r=0.56).
  $\textbf{Conclusion}$: Our technique can identify distribution shifts in clinical data and can predict CADe detector performance on unseen data, without labels. Our self-supervised approach can aid in detecting when data in practice is different from training, such as between hospitals or data has meaningfully shifted from training. MSN has potential for application to medical image domains beyond colonoscopy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09920v3</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joel Shor, Carson McNeil, Yotam Intrator, Joseph R Ledsam, Hiro-o Yamano, Daisuke Tsurumaru, Hiroki Kayama, Atsushi Hamabe, Koji Ando, Mitsuhiko Ota, Haruei Ogino, Hiroshi Nakase, Kaho Kobayashi, Masaaki Miyo, Eiji Oki, Ichiro Takemasa, Ehud Rivlin, Roman Goldenberg</dc:creator>
    </item>
    <item>
      <title>The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency and Usability in AI</title>
      <link>https://arxiv.org/abs/2403.13784</link>
      <description>arXiv:2403.13784v2 Announce Type: replace-cross 
Abstract: Generative AI (GAI) offers unprecedented possibilities but its commercialization has raised concerns about transparency, reproducibility, bias, and safety. Many "open-source" GAI models lack the necessary components for full understanding and reproduction, and some use restrictive licenses, a practice known as "openwashing." We propose the Model Openness Framework (MOF), a ranked classification system that rates machine learning models based on their completeness and openness, following principles of open science, open source, open data, and open access. The MOF requires specific components of the model development lifecycle to be included and released under appropriate open licenses. This framework aims to prevent misrepresentation of models claiming to be open, guide researchers and developers in providing all model components under permissive licenses, and help companies, academia, and hobbyists identify models that can be safely adopted without restrictions. Wide adoption of the MOF will foster a more open AI ecosystem, accelerating research, innovation, and adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13784v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matt White (Yanglet), Ibrahim Haddad (Yanglet), Cailean Osborne (Yanglet),  Xiao-Yang (Yanglet),  Liu, Ahmed Abdelmonsef, Sachin Varghese</dc:creator>
    </item>
    <item>
      <title>"This is not a data problem": Algorithms and Power in Public Higher Education in Canada</title>
      <link>https://arxiv.org/abs/2403.13969</link>
      <description>arXiv:2403.13969v2 Announce Type: replace-cross 
Abstract: Algorithmic decision-making is increasingly being adopted across public higher education. The expansion of data-driven practices by post-secondary institutions has occurred in parallel with the adoption of New Public Management approaches by neoliberal administrations. In this study, we conduct a qualitative analysis of an in-depth ethnographic case study of data and algorithms in use at a public college in Ontario, Canada. We identify the data, algorithms, and outcomes in use at the college. We assess how the college's processes and relationships support those outcomes and the different stakeholders' perceptions of the college's data-driven systems. In addition, we find that the growing reliance on algorithmic decisions leads to increased student surveillance, exacerbation of existing inequities, and the automation of the faculty-student relationship. Finally, we identify a cycle of increased institutional power perpetuated by algorithmic decision-making, and driven by a push towards financial sustainability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13969v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3613904.3642451</arxiv:DOI>
      <dc:creator>Kelly McConvey, Shion Guha</dc:creator>
    </item>
  </channel>
</rss>
