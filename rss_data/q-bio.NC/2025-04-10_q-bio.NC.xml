<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 10 Apr 2025 04:00:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Multihead self-attention in cortico-thalamic circuits</title>
      <link>https://arxiv.org/abs/2504.06354</link>
      <description>arXiv:2504.06354v1 Announce Type: new 
Abstract: Both biological cortico-thalamic networks and artificial transformer networks use canonical computations to perform a wide range of cognitive tasks. In this work, we propose that the structure of cortico-thalamic circuits is well suited to realize a computation analogous to multihead self-attention, the main algorithmic innovation of transformers. We start with the concept of a cortical unit module or microcolumn, and propose that superficial and deep pyramidal cells carry distinct computational roles. Specifically, superficial pyramidal cells encode an attention mask applied onto deep pyramidal cells to compute attention-modulated values. We show how to wire such microcolumns into a circuit equivalent to a single head of self-attention. We then suggest the parallel between one head of attention and a cortical area. On this basis, we show how to wire cortico-thalamic circuits to perform multihead self-attention. Along these constructions, we refer back to existing experimental data, and find noticeable correspondence. Finally, as a first step towards a mechanistic theory of synaptic learning in this framework, we derive formal gradients of a tokenwise mean squared error loss for a multihead linear self-attention block.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06354v1</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arno Granier, Walter Senn</dc:creator>
    </item>
    <item>
      <title>An introduction to memory competitions, records and techniques</title>
      <link>https://arxiv.org/abs/2504.06747</link>
      <description>arXiv:2504.06747v1 Announce Type: new 
Abstract: This article provides an overview of memory competitions, analyzes differences between disciplines and explains current state-of-the-art techniques. Performances have increased dramatically over the past three decades. Nowadays, information processing reaches up to 42 bit/s in short disciplines with most of the time spent on reading, suggesting that mental associations are formed even faster. Records show a remarkable concordance across all time scales: the processing speed depends on memorization time as a power law.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06747v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bastian Wiederhold</dc:creator>
    </item>
    <item>
      <title>Retinotopic Mechanics derived using classical physics</title>
      <link>https://arxiv.org/abs/2109.11632</link>
      <description>arXiv:2109.11632v5 Announce Type: replace 
Abstract: The concept of a cell$'$s receptive field is a bedrock in systems neuroscience, and the classical static description of the receptive field has had enormous success in explaining the fundamental mechanisms underlying visual processing. Borne out by the spatio-temporal dynamics of visual sensitivity to probe stimuli in primates, I build on top of this static account with the introduction of a new computational field of research, retinotopic mechanics. At its core, retinotopic mechanics assumes that during active sensing receptive fields are not static but can shift beyond their classical extent. Specifically, the canonical computations and the neural architecture that supports these computations are inherently mediated by a neurobiologically inspired force field (e.g.,$R_s\propto \sim 1 /\Delta M$). For example, when the retina is displaced because of a saccadic eye movement from one point in space to another, cells across retinotopic brain areas are tasked with discounting the retinal disruptions such active surveillance inherently introduces. This neural phenomenon is known as spatial constancy. Using retinotopic mechanics, I propose that to achieve spatial constancy or any active visually mediated task, retinotopic cells, namely their receptive fields, are constrained by eccentricity dependent elastic fields. I propose that elastic fields are self-generated by the visual system and allow receptive fields the ability to predictively shift beyond their classical extent to future post-saccadic location such that neural sensitivity which would otherwise support intermediate eccentric locations likely to contain retinal disruptions is transiently blunted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.11632v5</guid>
      <category>q-bio.NC</category>
      <category>physics.bio-ph</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ifedayo-EmmanuEL Adeyefa-Olasupo</dc:creator>
    </item>
    <item>
      <title>A Concise Mathematical Description of Active Inference in Discrete Time</title>
      <link>https://arxiv.org/abs/2406.07726</link>
      <description>arXiv:2406.07726v3 Announce Type: replace-cross 
Abstract: In this paper we present a concise mathematical description of active inference in discrete time. The main part of the paper serves as a basic introduction to the topic, including a detailed example of the action selection mechanism. The appendix discusses the more subtle mathematical details, targeting readers who have already studied the active inference literature but struggle to make sense of the mathematical details and derivations. Throughout, we emphasize precise and standard mathematical notation, ensuring consistency with existing texts and linking all equations to widely used references on active inference. Additionally, we provide Python code that implements the action selection and learning mechanisms described in this paper and is compatible with pymdp environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07726v3</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 10 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse van Oostrum, Carlotta Langer, Nihat Ay</dc:creator>
    </item>
  </channel>
</rss>
