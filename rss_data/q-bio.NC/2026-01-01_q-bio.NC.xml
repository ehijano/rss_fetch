<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Jan 2026 05:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Spike-Timing-Dependent Plasticity for Bernoulli Message Passing</title>
      <link>https://arxiv.org/abs/2512.23728</link>
      <description>arXiv:2512.23728v1 Announce Type: new 
Abstract: Bayesian inference provides a principled framework for understanding brain function, while neural activity in the brain is inherently spike-based. This paper bridges these two perspectives by designing spiking neural networks that simulate Bayesian inference through message passing for Bernoulli messages. To train the networks, we employ spike-timing-dependent plasticity, a biologically plausible mechanism for synaptic plasticity which is based on the Hebbian rule. Our results demonstrate that the network's performance closely matches the true numerical solution. We further demonstrate the versatility of our approach by implementing a factor graph example from coding theory, illustrating signal transmission over an unreliable channel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23728v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sepideh Adamiat, Wouter M. Kouw, Bert de Vries</dc:creator>
    </item>
    <item>
      <title>SymSeqBench: a unified framework for the generation and analysis of rule-based symbolic sequences and datasets</title>
      <link>https://arxiv.org/abs/2512.24977</link>
      <description>arXiv:2512.24977v1 Announce Type: new 
Abstract: Sequential structure is a key feature of multiple domains of natural cognition and behavior, such as language, movement and decision-making. Likewise, it is also a central property of tasks to which we would like to apply artificial intelligence. It is therefore of great importance to develop frameworks that allow us to evaluate sequence learning and processing in a domain agnostic fashion, whilst simultaneously providing a link to formal theories of computation and computability. To address this need, we introduce two complementary software tools: SymSeq, designed to rigorously generate and analyze structured symbolic sequences, and SeqBench, a comprehensive benchmark suite of rule-based sequence processing tasks to evaluate the performance of artificial learning systems in cognitively relevant domains. In combination, SymSeqBench offers versatility in investigating sequential structure across diverse knowledge domains, including experimental psycholinguistics, cognitive psychology, behavioral analysis, neuromorphic computing and artificial intelligence. Due to its basis in Formal Language Theory (FLT), SymSeqBench provides researchers in multiple domains with a convenient and practical way to apply the concepts of FLT to conceptualize and standardize their experiments, thus advancing our understanding of cognition and behavior through shared computational frameworks and formalisms. The tool is modular, openly available and accessible to the research community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24977v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barna Zajzon, Younes Bouhadjar, Maxime Fabre, Felix Schmidt, Noah Ostendorf, Emre Neftci, Abigail Morrison, Renato Duarte</dc:creator>
    </item>
    <item>
      <title>Non-stationary dynamics of interspike intervals in neuronal populations</title>
      <link>https://arxiv.org/abs/2512.23922</link>
      <description>arXiv:2512.23922v1 Announce Type: cross 
Abstract: We study the joint dynamics of membrane potential and time since the last spike in a population of integrate-and-fire neurons using a population density framework. This leads to a two-dimensional Fokker-Planck equation that captures the evolution of the full neuronal state, along with a one-dimensional hierarchy of equations for the moments of the inter-spike interval (ISI). The formalism allows us to characterize the time-dependent ISI distribution, even when the population is far from stationarity, such as under time-varying external input or during network oscillations. By performing a perturbative expansion around the stationary state, we also derive an analytic expression for the linear response of the ISI distribution to weak input modulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23922v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luca Falorsi, Gianni V. Vinci, Maurizio Mattia</dc:creator>
    </item>
    <item>
      <title>Complexity and dynamics of partially symmetric random neural networks</title>
      <link>https://arxiv.org/abs/2512.24439</link>
      <description>arXiv:2512.24439v1 Announce Type: cross 
Abstract: Neural circuits exhibit structured connectivity, including an overrepresentation of reciprocal connections between neuron pairs. Despite important advances, a full understanding of how such partial symmetry in connectivity shapes neural dynamics remains elusive. Here we ask how correlations between reciprocal connections in a random, recurrent neural network affect phase-space complexity, defined as the exponential proliferation rate (with network size) of the number of fixed points that accompanies the transition to chaotic dynamics. We find a striking pattern: partial anti-symmetry strongly amplifies complexity, while partial symmetry suppresses it. These opposing trends closely track changes in other measures of dynamical behavior, such as dimensionality, Lyapunov exponents, and transient path length, supporting the view that fixed-point structure is a key determinant of network dynamics. Thus, positive reciprocal correlations favor low-dimensional, slowly varying activity, whereas negative correlations promote high-dimensional, rapidly fluctuating chaotic activity. These results yield testable predictions about the link between connection reciprocity, neural dynamics and function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24439v1</guid>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nimrod Sherf, Si Tang, Dylan Hafner, Jonathan D. Touboul, Xaq Pitkow, Kevin E. Bassler, Kre\v{s}imir Josi\'c</dc:creator>
    </item>
    <item>
      <title>Muscle Synergy Patterns During Running: Coordinative Mechanisms From a Neuromechanical Perspective</title>
      <link>https://arxiv.org/abs/2512.24654</link>
      <description>arXiv:2512.24654v1 Announce Type: cross 
Abstract: Running is a fundamental form of human locomotion and a key task for evaluating neuromuscular control and lower-limb coordination. In recent years, muscle synergy analysis based on surface electromyography (sEMG) has become an important approach in this area. This review focuses on muscle synergies during running, outlining core neural control theories and biomechanical optimization hypotheses, summarizing commonly used decomposition methods (e.g., PCA, ICA, FA, NMF) and emerging autoencoder-based approaches. We synthesize findings on the development and evolution of running-related synergies across the lifespan, examine how running surface, speed, foot-strike pattern, fatigue, and performance level modulate synergy patterns, and describe characteristic alterations in populations with knee osteoarthritis, patellofemoral pain, and stroke. Current evidence suggests that the number and basic structure of lower-limb synergies during running are relatively stable, whereas spatial muscle weightings and motor primitives are highly plastic and sensitive to task demands, fatigue, and pathology. However, substantial methodological variability remains in EMG channel selection, preprocessing pipelines, and decomposition algorithms, and direct neurophysiological validation and translational application are still limited. Future work should prioritize standardized processing protocols, integration of multi-source neuromusculoskeletal data, nonlinear modeling, and longitudinal intervention studies to better exploit muscle synergy analysis in sports biomechanics, athletic training, and rehabilitation medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24654v1</guid>
      <category>q-bio.QM</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ye Ma, Shixin Lin, Shengxing Fu, Yuwei Liu, Chenyi Guo, Dongwei Liu, Meijin Hou</dc:creator>
    </item>
    <item>
      <title>Large language models and the entropy of English</title>
      <link>https://arxiv.org/abs/2512.24969</link>
      <description>arXiv:2512.24969v1 Announce Type: cross 
Abstract: We use large language models (LLMs) to uncover long-ranged structure in English texts from a variety of sources. The conditional entropy or code length in many cases continues to decrease with context length at least to $N\sim 10^4$ characters, implying that there are direct dependencies or interactions across these distances. A corollary is that there are small but significant correlations between characters at these separations, as we show from the data independent of models. The distribution of code lengths reveals an emergent certainty about an increasing fraction of characters at large $N$. Over the course of model training, we observe different dynamics at long and short context lengths, suggesting that long-ranged structure is learned only gradually. Our results constrain efforts to build statistical physics models of LLMs or language itself.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24969v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cs.CL</category>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Colin Scheibner, Lindsay M. Smith, William Bialek</dc:creator>
    </item>
    <item>
      <title>From self-organizing systems to subjective temporal extension</title>
      <link>https://arxiv.org/abs/2404.12895</link>
      <description>arXiv:2404.12895v5 Announce Type: replace 
Abstract: The self-simulational theory of temporal extension describes an information-theoretically formalized mechanism by which the width of subjective temporality emerges from the architecture of self-modelling. In this paper, the perspective of the free energy principle will be assumed to cast the emergence of subjective temporal extension from first principles of the physics of self-organization and to formalize subjective temporal extension using information geometry. Using active inference, a deep parametric generative model of temporal inference is simulated, which realizes the described dynamics on a computational level. Two variations of time-perception naturally emerge from the simulated computational model. This concerns the intentional binding effect (i.e., the compression of the temporal interval between voluntarily initiated actions and subsequent sensory consequences) and empirically documented alterations of subjective time experience in deep states of meditative absorption (i.e., in minimal phenomenal experience). Generally, numerous systematic and domain-specific alterations of subjective temporal experience are computationally explained in a unified manner, as enabled by integration with current active inference accounts mapping onto the respective domains. This concerns, next to attentional and central tendency effects, the temporality-modulating role of valence, impulsivity, boredom, flow-states, near death-experiences, and various psychopathologies, amongst others. The self-simulational theory of temporal extension, from the perspective of the free energy principle, explains how the width of the subjective temporal moment emerges and varies from first principles, accounting for why sometimes, subjective time seems to fly, and sometimes, moments feel like eternities; with the computational mechanism being readily deployable synthetically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12895v5</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Erik Bellingrath</dc:creator>
    </item>
    <item>
      <title>Fast weight programming and linear transformers: from machine learning to neurobiology</title>
      <link>https://arxiv.org/abs/2508.08435</link>
      <description>arXiv:2508.08435v3 Announce Type: replace-cross 
Abstract: Recent advances in artificial neural networks for machine learning, and language modeling in particular, have established a family of recurrent neural network (RNN) architectures that, unlike conventional RNNs with vector-form hidden states, use two-dimensional (2D) matrix-form hidden states. Such 2D-state RNNs, known as Fast Weight Programmers (FWPs), can be interpreted as a neural network whose synaptic weights (called fast weights) dynamically change over time as a function of input observations, and serve as short-term memory storage; corresponding synaptic weight modifications are controlled or programmed by another network (the programmer) whose parameters are trained (e.g., by gradient descent). In this Primer, we review the technical foundations of FWPs, their computational characteristics, and their connections to transformers and state space models. We also discuss connections between FWPs and models of synaptic plasticity in the brain, suggesting a convergence of natural and artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08435v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kazuki Irie, Samuel J. Gershman</dc:creator>
    </item>
    <item>
      <title>Dynamical Learning in Deep Asymmetric Recurrent Neural Networks</title>
      <link>https://arxiv.org/abs/2509.05041</link>
      <description>arXiv:2509.05041v2 Announce Type: replace-cross 
Abstract: We investigate recurrent neural networks with asymmetric interactions and demonstrate that the inclusion of self-couplings or sparse excitatory inter-module connections leads to the emergence of a densely connected manifold of dynamically accessible stable configurations. This representation manifold is exponentially large in system size and is reachable through simple local dynamics, despite constituting a subdominant subset of the global configuration space. We further show that learning can be implemented directly on this structure via a fully local, gradient-free mechanism that selectively stabilizes a single task-relevant network configuration. Unlike error-driven or contrastive learning schemes, this approach does not require explicit comparisons between network states obtained with and without output supervision. Instead, transient supervisory signals bias the dynamics toward the representation manifold, after which local plasticity consolidates the attained configuration, effectively shaping the latent representation space. Numerical evaluations on standard image classification benchmarks indicate performance comparable to that of multilayer perceptrons trained using backpropagation. More generally, these results suggest that the dynamical accessibility of fixed points and the stabilization of internal network dynamics constitute viable alternative principles for learning in recurrent systems, with conceptual links to statistical physics and potential implications for biologically motivated and neuromorphic computing architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05041v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Davide Badalotti, Carlo Baldassi, Marc M\'ezard, Mattia Scardecchia, Riccardo Zecchina</dc:creator>
    </item>
  </channel>
</rss>
