<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Apr 2024 04:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Complete Quantitative Axiomatisation of Behavioural Distance of Regular Expressions</title>
      <link>https://arxiv.org/abs/2404.13352</link>
      <description>arXiv:2404.13352v1 Announce Type: new 
Abstract: Deterministic automata have been traditionally studied through the point of view of language equivalence, but another perspective is given by the canonical notion of shortest-distinguishing-word distance quantifying the of states. Intuitively, the longer the word needed to observe a difference between two states, then the closer their behaviour is. In this paper, we give a sound and complete axiomatisation of shortest-distinguishing-word distance between regular languages. Our axiomatisation relies on a recently developed quantitative analogue of equational logic, allowing to manipulate rational-indexed judgements of the form $e \equiv_\varepsilon f$ meaning term $e$ is approximately equivalent to term $f$ within the error margin of $\varepsilon$. The technical core of the paper is dedicated to the completeness argument that draws techniques from order theory and Banach spaces to simplify the calculation of the behavioural distance to the point it can be then mimicked by axiomatic reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13352v1</guid>
      <category>cs.LO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wojciech R\'o\.zowski</dc:creator>
    </item>
    <item>
      <title>Proceedings 18th International Workshop on Logical and Semantic Frameworks, with Applications and 10th Workshop on Horn Clauses for Verification and Synthesis</title>
      <link>https://arxiv.org/abs/2404.13672</link>
      <description>arXiv:2404.13672v1 Announce Type: new 
Abstract: This volume contains
  * The post-proceedings of the Eighteenth Logical and Semantic Frameworks with Applications (LSFA 2023). The meeting was held on July 1-2, 2023, organised by the Sapienza Universit\`a di Roma, Italy. LSFA aims to bring researchers and students interested in theoretical and practical aspects of logical and semantic frameworks and their applications. The covered topics include proof theory, type theory and rewriting theory, specification and deduction languages, and formal semantics of languages and systems.
  * The post-proceedings of the Tenth Workshop on Horn clauses for Verification and Synthesis (HCVS 2023). The meeting was held on April 23, 2023 at the Institut Henri Poincar\'e in Paris. HCVS aims to bring together researchers working in the two communities of constraint/ logic programming (e.g., ICLP and CP), program verification (e.g., CAV, TACAS, and VMCAI), and automated deduction (e.g., CADE, IJCAR), on the topics of Horn clause based analysis, verification, and synthesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13672v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <category>cs.SC</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.402</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 402, 2024</arxiv:journal_reference>
      <dc:creator>Temur Kutsia (RISC, Johannes Kepler University Linz), Daniel Ventura (INF, Universidade Federal de Goi\'as), David Monniaux (CNRS - Verimag), Jos\'e F. Morales (IMDEA)</dc:creator>
    </item>
    <item>
      <title>The Characterization of Abstract Truth and its Factorization</title>
      <link>https://arxiv.org/abs/2404.13782</link>
      <description>arXiv:2404.13782v1 Announce Type: new 
Abstract: Human knowledge is made up of the conceptual structures of many communities of interest. In order to establish coherence in human knowledge representation, it is important to enable communication between the conceptual structures of different communities The conceptual structures of any particular community is representable in an ontology. Such a ontology provides a formal linguistic standard for that community. However, a standard community ontology is established for various purposes, and makes choices that force a given interpretation, while excluding others that may be equally valid for other purposes. Hence, a given representation is relative to the purpose for that representation. Due to this relativity of representation, in the larger scope of all human knowledge it is more important to standardize methods and frameworks for relating ontologies than to standardize any particular choice of ontology. The standardization of methods and frameworks is called the semantic integration of ontologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13782v1</guid>
      <category>cs.LO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert E. Kent</dc:creator>
    </item>
    <item>
      <title>Error Credits: Resourceful Reasoning about Error Bounds for Higher-Order Probabilistic Programs</title>
      <link>https://arxiv.org/abs/2404.14223</link>
      <description>arXiv:2404.14223v1 Announce Type: new 
Abstract: Probabilistic programs often trade accuracy for efficiency, and are thus only approximately correct. It is important to obtain precise error bounds for these approximations, but existing approaches rely on simplifications that make the error bounds excesively coarse, or only apply to first-order programs. In this paper we present Eris, a higher-order separation logic for probabilistic programs written in an expressive higher-order language.
  Our key novelty is the introduction of error credits, a separation logic resource that tracks the error bound of a program. By representing error bounds as a resource, we recover the benefits of separation logic, including compositionality, modularity, and dependency between errors and program terms, allowing for more precise specifications. Moreover, we enable novel reasoning principles such as expectation-preserving error composition, amortized error reasoning, and proving almost-sure termination by induction on the error.
  We illustrate the advantages of our approach by proving amortized error bounds on a range of examples, including collision probabilities in hash functions, which allows us to write more modular specifications for data structures that use them as clients. We also use our logic to prove correctness and almost-sure termination of rejection sampling algorithms. All of our results have been mechanized in the Coq proof assistant using the Iris separation logic framework and the Coquelicot real analysis library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14223v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Aguirre, Philipp G. Haselwarter, Markus de Medeiros, Kwing Hei Li, Simon Oddershede Gregersen, Joseph Tassarotti, Lars Birkedal</dc:creator>
    </item>
    <item>
      <title>Hybrid Intersection Types for PCF (Extended Version)</title>
      <link>https://arxiv.org/abs/2404.14340</link>
      <description>arXiv:2404.14340v1 Announce Type: new 
Abstract: Intersection type systems have been independently applied to different evaluation strategies, such as call-by-name (CBN) and call-by-value (CBV). These type systems have been then generalized to different subsuming paradigms being able, in particular, to encode CBN and CBV in a unique unifying framework. However, there are no intersection type systems that explicitly enable CBN and CBV to cohabit together without making use of an encoding into a common target framework. This work proposes an intersection type system for PCF with a specific notion of evaluation, called PCFH. Evaluation in PCFH actually has a hybrid nature, in the sense that CBN and CBV operational behaviors cohabit together. Indeed, PCFH combines a CBV-like operational behavior for function application with a CBN-like behavior for recursion. This hybrid nature is reflected in the type system, which turns out to be sound and complete with respect to PCFH: not only typability implies normalization, but also the converse holds. Moreover, the type system is quantitative, in the sense that the size of typing derivations provides upper bounds for the length of the reduction sequences to normal form. This type system is then refined to a tight one, offering exact information regarding the length of normalization sequences. This is the first time that a sound and complete quantitative type system has been designed for a hybrid computational model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14340v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Barenbaum, Delia Kesner, Mariana Milicich</dc:creator>
    </item>
    <item>
      <title>Fuzzychain: An Equitable Consensus Mechanism for Blockchain Networks</title>
      <link>https://arxiv.org/abs/2404.13337</link>
      <description>arXiv:2404.13337v1 Announce Type: cross 
Abstract: Blockchain technology has become a trusted method for establishing secure and transparent transactions through a distributed, encrypted network. The operation of blockchain is governed by consensus algorithms, among which Proof of Stake (PoS) is popular yet has its drawbacks, notably the potential for centralising power in nodes with larger stakes or higher rewards. Fuzzychain, our proposed solution, introduces the use of fuzzy sets to define stake semantics, promoting decentralised and distributed processing control. This system selects validators based on their degree of membership to the stake fuzzy sets rather than just the size of their stakes. As a pioneer proposal in applying fuzzy sets to blockchain, Fuzzychain aims to rectify PoS's limitations. Our results indicate that Fuzzychain not only matches PoS in functionality but also ensures a fairer distribution of stakes among validators, leading to more inclusive validator selection and a better-distributed network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13337v1</guid>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <category>cs.LO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bruno Ramos-Cruz, Javier Andreu-P\'erez, Francisco J. Quesada, Luis Mart\'inez</dc:creator>
    </item>
    <item>
      <title>Faster Game Solving by Fixpoint Acceleration</title>
      <link>https://arxiv.org/abs/2404.13687</link>
      <description>arXiv:2404.13687v1 Announce Type: cross 
Abstract: We propose a method for solving parity games with acyclic (DAG) sub-structures by computing nested fixpoints of a DAG attractor function that lives over the non-DAG parts of the game, thereby restricting the domain of the involved fixpoint operators. Intuitively, this corresponds to accelerating fixpoint computation by inlining cycle-free parts during the solution of parity games, leading to earlier convergence. We also present an economic later-appearence-record construction that takes Emerson-Lei games to parity games, and show that it preserves DAG sub-structures; it follows that the proposed method can be used also for the accelerated solution of Emerson-Lei games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13687v1</guid>
      <category>cs.GT</category>
      <category>cs.LO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Hausmann</dc:creator>
    </item>
    <item>
      <title>A Reinforcement Learning based Reset Policy for CDCL SAT Solvers</title>
      <link>https://arxiv.org/abs/2404.03753</link>
      <description>arXiv:2404.03753v2 Announce Type: replace 
Abstract: Restart policy is an important technique used in modern Conflict-Driven Clause Learning (CDCL) solvers, wherein some parts of the solver state are erased at certain intervals during the run of the solver. In most solvers, variable activities are preserved across restart boundaries, resulting in solvers continuing to search parts of the assignment tree that are not far from the one immediately prior to a restart. To enable the solver to search possibly "distant" parts of the assignment tree, we study the effect of resets, a variant of restarts which not only erases the assignment trail, but also randomizes the activity scores of the variables of the input formula after reset, thus potentially enabling a better global exploration of the search space.
  In this paper, we model the problem of whether to trigger reset as a multi-armed bandit (MAB) problem, and propose two reinforcement learning (RL) based adaptive reset policies using the Upper Confidence Bound (UCB) and Thompson sampling algorithms. These two algorithms balance the exploration-exploitation tradeoff by adaptively choosing arms (reset vs. no reset) based on their estimated rewards during the solver's run. We implement our reset policies in four baseline SOTA CDCL solvers and compare the baselines against the reset versions on Satcoin benchmarks and SAT Competition instances. Our results show that RL-based reset versions outperform the corresponding baseline solvers on both Satcoin and the SAT competition instances, suggesting that our RL policy helps to dynamically and profitably adapt the reset frequency for any given input instance. We also introduce the concept of a partial reset, where at least a constant number of variable activities are retained across reset boundaries. Building on previous results, we show that there is an exponential separation between O(1) vs. $\Omega(n)$-length partial resets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03753v2</guid>
      <category>cs.LO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chunxiao Li, Charlie Liu, Jonathan Chung, Zhengyang Lu, Piyush Jha, Vijay Ganesh</dc:creator>
    </item>
    <item>
      <title>From Muller to Parity and Rabin Automata: Optimal Transformations Preserving (History) Determinism</title>
      <link>https://arxiv.org/abs/2305.04323</link>
      <description>arXiv:2305.04323v3 Announce Type: replace-cross 
Abstract: We study transformations of automata and games using Muller conditions into equivalent ones using parity or Rabin conditions. We present two transformations, one that turns a deterministic Muller automaton into an equivalent deterministic parity automaton, and another that provides an equivalent history-deterministic Rabin automaton. We show a strong optimality result: the obtained automata are minimal amongst those that can be derived from the original automaton by duplication of states. We introduce the notions of locally bijective morphisms and history-deterministic mappings to formalise the correctness and optimality of these transformations.
  The proposed transformations are based on a novel structure, called the alternating cycle decomposition, inspired by and extending Zielonka trees. In addition to providing optimal transformations of automata, the alternating cycle decomposition offers fundamental information on their structure. We use this information to give crisp characterisations on the possibility of relabelling automata with different acceptance conditions and to perform a systematic study of a normal form for parity automata.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04323v3</guid>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.46298/theoretics.24.12</arxiv:DOI>
      <arxiv:journal_reference>TheoretiCS, Volume 3 (2024), Article 12, 1-110</arxiv:journal_reference>
      <dc:creator>Antonio Casares, Thomas Colcombet, Nathana\"el Fijalkow, Karoliina Lehtinen</dc:creator>
    </item>
    <item>
      <title>On the Length of Strongly Monotone Descending Chains over $\mathbb{N}^d$</title>
      <link>https://arxiv.org/abs/2310.02847</link>
      <description>arXiv:2310.02847v2 Announce Type: replace-cross 
Abstract: A recent breakthrough by K\"unnemann, Mazowiecki, Sch\"utze, Sinclair-Banks, and Wegrzycki (ICALP, 2023) bounds the running time for the coverability problem in $d$-dimensional vector addition systems under unary encoding to $n^{2^{O(d)}}$, improving on Rackoff's $n^{2^{O(d\lg d)}}$ upper bound (Theor. Comput. Sci., 1978), and provides conditional matching lower bounds.
  In this paper, we revisit Lazi\'c and Schmitz' "ideal view" of the backward coverability algorithm (Inform. Comput., 2021) in the light of this breakthrough. We show that the controlled strongly monotone descending chains of downwards-closed sets over $\mathbb{N}^d$ that arise from the dual backward coverability algorithm of Lazi\'c and Schmitz on $d$-dimensional unary vector addition systems also enjoy this tight $n^{2^{O(d)}}$ upper bound on their length, and that this also translates into the same bound on the running time of the backward coverability algorithm.
  Furthermore, our analysis takes place in a more general setting than that of Lazi\'c and Schmitz, which allows to show the same results and improve on the 2EXPSPACE upper bound derived by Benedikt, Duff, Sharad, and Worrell (LICS, 2017) for the coverability problem in invertible affine nets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02847v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sylvain Schmitz, Lia Sch\"utze</dc:creator>
    </item>
    <item>
      <title>Lemur: Integrating Large Language Models in Automated Program Verification</title>
      <link>https://arxiv.org/abs/2310.04870</link>
      <description>arXiv:2310.04870v4 Announce Type: replace-cross 
Abstract: The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that demands high-level abstract reasoning about program properties that is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of transition rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure and demonstrate practical improvements on a set of synthetic and competition benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04870v4</guid>
      <category>cs.FL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Haoze Wu, Clark Barrett, Nina Narodytska</dc:creator>
    </item>
  </channel>
</rss>
