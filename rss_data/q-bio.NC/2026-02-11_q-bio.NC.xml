<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Feb 2026 02:47:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Recovering Whole-Brain Causal Connectivity under Indirect Observation with Applications to Human EEG and fMRI</title>
      <link>https://arxiv.org/abs/2602.09034</link>
      <description>arXiv:2602.09034v1 Announce Type: new 
Abstract: Inferring directed connectivity from neuroimaging is an ill-posed inverse problem: recorded signals are distorted by hemodynamic filtering and volume conduction, which can mask true neural interactions. Many existing methods conflate these observation artifacts with genuine neural influence, risking spurious causal graphs driven by the measurement process. We introduce INCAMA (INdirect CAusal MAmba), a latent-space causal discovery framework that explicitly accounts for measurement physics to separate neural dynamics from indirect observations. INCAMA integrates a physics-aware inversion module with a nonstationarity-driven, delay-sensitive causal discovery model based on selective state-space sequences. Leveraging nonstationary mechanism shifts as soft interventions, we establish identifiability of delayed causal structure from indirect measurements and a stability bound that quantifies how inversion error affects graph recovery. We validate INCAMA on large-scale biophysical simulations across EEG and fMRI, where it significantly outperforms standard pipelines. We further demonstrate zero-shot generalization to real-world fMRI from the Human Connectome Project: without domain-specific fine-tuning, INCAMA recovers canonical visuo-motor pathways (e.g., $V1 \to V2$ and $M1 \leftrightarrow S1$) consistent with established neuroanatomy, supporting its use for whole-brain causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09034v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sangyoon Bae, Miruna Oprescu, David Keetae Park, Shinjae Yoo, Jiook Cha</dc:creator>
    </item>
    <item>
      <title>Open diffusion MRI and connectivity data for epilepsy and surgery: The IDEAS II release</title>
      <link>https://arxiv.org/abs/2602.09852</link>
      <description>arXiv:2602.09852v1 Announce Type: new 
Abstract: Epileptic seizures are generated in cerebral networks that propagate ictal and interictal activity. The structure of cerebral networks underpinning epileptic activity can be inferred from diffusion-weighted MRI (DWI). However, publicly available DWI data in individuals with epilepsy are scarce, and processing is technically challenging due to scan-specific artifacts, limiting research progress. Here, we release raw DWI data from 216 individuals with epilepsy and 98 healthy controls. Subject identifiers align with our previous data release (IDEAS), which includes T1-weighted and FLAIR MRI, surgical details, and long-term seizure outcomes after surgery. Preprocessing reduced distortions and artifacts, while fully processed data include diffusion metric maps in native and template space. We also provide parcellated structural connectomes using multiple atlases and connectivity measures. To illustrate the utility of this IDEAS II data, we replicated ENIGMA consortium findings, observing widespread reductions of fractional anisotropy, particularly ipsilateral to the area of seizure onset. We further demonstrate localised abnormality, and network connectivity using streamline tractography in a patient who subsequently underwent temporal lobe resection. This open dataset offers a comprehensive resource to advance research on structural connectivity and surgical outcomes in epilepsy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09852v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter N. Taylor, Gerard Hall, Jonathan Horsley, Yujiang Wang, Sjoerd B. Vos, Gavin P Winston, Andrew W McEvoy, Anna Miserocchi, Jane de Tisi, John S Duncan</dc:creator>
    </item>
    <item>
      <title>Finite integration time can shift optimal sensitivity away from criticality</title>
      <link>https://arxiv.org/abs/2602.09491</link>
      <description>arXiv:2602.09491v1 Announce Type: cross 
Abstract: Sensitivity to small changes in the environment is crucial for many real-world tasks, enabling living and artificial systems to make correct behavioral decisions. It has been shown that such sensitivity is maximized when a system operates near the critical point of a phase transition. However, proximity to criticality introduces large fluctuations and diverging timescales. Hence, to leverage the maximal sensitivity, it would require impractically long integration periods. Here, we analytically and computationally demonstrate how the optimal tuning of a recurrent neural network is determined given a finite integration time. Rather than maximizing the theoretically available sensitivity, we find networks attain different sensitivities depending on the available time. Consequently, the optimal dynamic regime can shift away from criticality when integration times are finite, highlighting the necessity of incorporating finite-time considerations into studies of information processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09491v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sahel Azizpour, Viola Priesemann, Johannes Zierenberg, Anna Levina</dc:creator>
    </item>
    <item>
      <title>Popularity Feedback Constrains Innovation in Cultural Markets</title>
      <link>https://arxiv.org/abs/2602.09997</link>
      <description>arXiv:2602.09997v1 Announce Type: cross 
Abstract: Real-world creative processes ranging from art to science rely on social feedback-loops between selection and creation. Yet, the effects of popularity feedback on collective creativity remain poorly understood. We investigate how popularity ratings influence cultural dynamics in a large-scale online experiment where participants ($N = 1\,008$) iteratively \textit{select} images from evolving markets and \textit{produce} their own modifications. Results show that exposing the popularity of images reduces cultural diversity and slows innovation, delaying aesthetic improvements. These findings are mediated by alterations of both selection and creation. During selection, popularity information triggers cumulative advantage, with participants preferentially building upon popular images, reducing diversity. During creation, participants make less disruptive changes, and are more likely to expand existing visual patterns. Feedback loops in cultural markets thus not only shape selection, but also, directly or indirectly, the form and direction of cultural innovation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09997v1</guid>
      <category>cs.SI</category>
      <category>q-bio.NC</category>
      <category>q-bio.PE</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Gautheron, Raja Marjieh, Dalton C. Conley, Seth Frey, Hannah Rubin, Mike D. Schneider, Ofer Tchernichovski, Nori Jacoby</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence Software Structured to Simulate Human Working Memory, Mental Imagery, and Mental Continuity</title>
      <link>https://arxiv.org/abs/2204.05138</link>
      <description>arXiv:2204.05138v3 Announce Type: replace 
Abstract: This article presents an artificial intelligence (AI) architecture intended to simulate the iterative updating of the human working memory system. It features several interconnected neural networks designed to emulate the specialized modules of the cerebral cortex. These are structured hierarchically and integrated into a global workspace. They are capable of temporarily maintaining high-level representational patterns akin to the psychological items maintained in working memory. This maintenance is made possible by persistent neural activity in the form of two modalities: sustained neural firing (resulting in a focus of attention) and synaptic potentiation (resulting in a short-term store). Representations held in persistent activity are recursively replaced resulting in incremental changes to the content of the working memory system. As this content gradually evolves, successive processing states overlap and are continuous with one another. The present article will explore how this architecture can lead to iterative shift in the distribution of coactive representations, ultimately leading to mental continuity between processing states, and thus to human-like thought and cognition. Taken together, these components outline a biologically motivated route toward synthetic consciousness or artificial sentience and subjectivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.05138v3</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>cs.SC</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jared Edward Reser</dc:creator>
    </item>
    <item>
      <title>Clarifying the conceptual dimensions of representation in neuroscience</title>
      <link>https://arxiv.org/abs/2403.14046</link>
      <description>arXiv:2403.14046v4 Announce Type: replace 
Abstract: Despite the centrality of the notion of representation in neuroscience, the field lacks a unified framework for the concepts used to characterize representation, leading to disparate use of both terminology and measures associated with it. To offer clarification, we propose a core set of conceptual dimensions that characterize representations in neuroscience. These dimensions describe relations between a neural response, features that may be represented, and downstream effects of the neural response. A neural response may be shown to be sensitive or specific to a feature, invariant to other features, or functional (it is used downstream in the brain). We use information-theoretic measures to illustrate these conceptual dimensions and explain how they relate to data analysis methods such as correlational analyses, decoding and encoding models, representational similarity analysis, and tests of statistical dependence or adaptation. We consider several canonical examples, including models of the representation of orientation, numerosity, and spatial location, which illustrate how the evidence put forth in support or criticism of these models is systematized by our framework. By offering a unified conceptual framework to characterize representation in neuroscience, we hope to aid the comparison and integration of results across studies and research groups and to help determine when evidence for a neural representation is strong.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14046v4</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephan Pohl, Edgar Y. Walker, David L. Barack, Jennifer Lee, Rachel N. Denison, Ned Block, Florent Meyniel, Wei Ji Ma</dc:creator>
    </item>
    <item>
      <title>A Brain Graph Foundation Model: Pre-Training and Prompt-Tuning across Broad Atlases and Disorders</title>
      <link>https://arxiv.org/abs/2506.02044</link>
      <description>arXiv:2506.02044v3 Announce Type: replace 
Abstract: As large language models (LLMs) continue to revolutionize AI research, there is a growing interest in building large-scale brain foundation models to advance neuroscience. While most existing brain foundation models are pre-trained on time-series signals or connectome features, we propose a novel graph-based pre-training paradigm for constructing a brain graph foundation model. In this paper, we introduce the Brain Graph Foundation Model, termed BrainGFM, a unified framework that leverages graph contrastive learning and graph masked autoencoders for large-scale fMRI-based pre-training. BrainGFM is pre-trained on a diverse mixture of brain atlases with varying parcellations, significantly expanding the pre-training corpus and enhancing the model's ability to generalize across heterogeneous fMRI-derived brain representations. To support efficient and versatile downstream transfer, we integrate both graph prompts and language prompts into the model design, enabling BrainGFM to flexibly adapt to a wide range of atlases, neurological and psychiatric disorders, and task settings. Furthermore, we employ meta-learning to optimize the graph prompts, facilitating strong generalization to previously unseen disorders under both few-shot and zero-shot learning conditions via language-guided prompting. BrainGFM is pre-trained on 27 neuroimaging datasets spanning 25 common neurological and psychiatric disorders, encompassing 2 types of brain atlases (functional and anatomical) across 8 widely-used parcellations, and covering over 25,000 subjects, 60,000 fMRI scans, and a total of 400,000 graph samples aggregated across all atlases and parcellations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02044v3</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinxu Wei, Kanhao Zhao, Yong Jiao, Lifang He, Yu Zhang</dc:creator>
    </item>
    <item>
      <title>State Space Models Naturally Produce Time Cell and Oscillatory Behaviors and Scale to Abstract Cognitive Functions</title>
      <link>https://arxiv.org/abs/2507.13638</link>
      <description>arXiv:2507.13638v3 Announce Type: replace 
Abstract: A grand challenge in modern neuroscience is to bridge the gap between the detailed mapping of microscale neural circuits and mechanistic understanding of cognitive functions. While extensive knowledge exists about neuronal connectivity and biophysics, how these low-level phenomena eventually produce abstract behaviors remains largely unresolved. Here, we propose that a model based on State Space Models, an emerging class of deep learning architectures, can be a potential biological model for analysis. We suggest that the differential equations governing elements in a State Space Model are conceptually consistent with the dynamics of biophysical processes, while the model offers a scalable framework to build on the dynamics to produce emergent behaviors observed in experimental neuroscience. We test this model by training a network employing a diagonal state transition matrix on temporal discrimination tasks with reinforcement learning. Our results suggest that neural behaviors such as time cells naturally emerge from two fundamental principles: optimal pre-configuration and rotational dynamics. These features are shown mathematically to optimize history compression, and naturally generate structured temporal dynamics even prior to training, mirroring recent findings in biological circuits. We show that learning acts primarily as a selection mechanism that fine-tunes these pre-configured oscillatory modes, rather than constructing temporal codes de novo. The model can be readily scaled to abstract cognitive functions such as event counting, supporting the use of State Space Models as a computationally tractable framework for understanding neural activities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13638v3</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sen Lu, Xiaoyu Zhang, Mingtao Hu, Eric Yeu-Jer Lee, Soohyeon Kim, Wei D. Lu</dc:creator>
    </item>
    <item>
      <title>Beyond Grid-Locked Voxels: Neural Response Functions for Continuous Brain Encoding</title>
      <link>https://arxiv.org/abs/2510.07342</link>
      <description>arXiv:2510.07342v2 Announce Type: replace 
Abstract: Neural encoding models aim to predict fMRI-measured brain responses to natural images. fMRI data is acquired as a 3D volume of voxels, where each voxel has a defined spatial location in the brain. However, conventional encoding models often flatten this volume into a 1D vector and treat voxel responses as independent outputs. This removes spatial context, discards anatomical information, and ties each model to a subject-specific voxel grid. We introduce the Neural Response Function (NRF), a framework that models fMRI activity as a continuous function over anatomical space rather than a flat vector of voxels. NRF represents brain activity as a continuous implicit function: given an image and a spatial coordinate (x, y, z) in standardized MNI space, the model predicts the response at that location. This formulation decouples predictions from the training grid, supports querying at arbitrary spatial resolutions, and enables resolution-agnostic analyses. By grounding the model in anatomical space, NRF exploits two key properties of brain responses: (1) local smoothness -- neighboring voxels exhibit similar response patterns; modeling responses continuously captures these correlations and improves data efficiency, and (2) cross-subject alignment -- MNI coordinates unify data across individuals, allowing a model pretrained on one subject to be fine-tuned on new subjects. In experiments, NRF outperformed baseline models in both intrasubject encoding and cross-subject adaptation, achieving high performance while reducing the data size needed by orders of magnitude. To our knowledge, NRF is the first anatomically aware encoding model to move beyond flattened voxels, learning a continuous mapping from images to brain responses in 3D space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07342v2</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haomiao Chen, Keith W Jamison, Mert R. Sabuncu, Amy Kuceyeski</dc:creator>
    </item>
    <item>
      <title>Beyond Expertise: Stable Individual Differences in Predictive Eye-Hand Coordination</title>
      <link>https://arxiv.org/abs/2602.07816</link>
      <description>arXiv:2602.07816v2 Announce Type: replace 
Abstract: Human eye-hand coordination relies on internal forward models that predict future states and compensate for sensory delays. During line tracing, the gaze typically leads the hand through predictive saccades, yet the extent to which this predictive window reflects expertise or intrinsic individual traits remains unclear. In this study, I examined eye-hand coordination in professional calligraphers and non-experts performing a controlled line tracing task. The temporal coupling between saccade distance (SD) and pen speed (PS) revealed substantial interpersonal variability: SD-PS peak times ranged from approximately -50 to 400 ms, forming stable, participant-specific predictive windows that were consistent across trials. These predictive windows closely matched each individual's pen catch-up time, indicating that the oculomotor system stabilizes fixation in anticipation of the hand's future velocity rather than relying on reactive pursuit. Neither the spatial indices (mean gaze-pen distance, mean saccade distance) nor the temporal index (SD-PS peak time) differed between calligraphers and non-calligraphers, and none of these predictive parameters correlated with tracing accuracy. These findings suggest that diverse predictive strategies can achieve equivalent performance, consistent with the minimum intervention principle of optimal feedback control. Together, the results indicate that predictive timing in eye-hand coordination reflects a stable, idiosyncratic Predictive Protocol shaped by individual neuromotor constraints rather than by expertise or training history.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07816v2</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emiko Shishido</dc:creator>
    </item>
    <item>
      <title>Offline World Models as Imagination Networks in Cognitive Agents</title>
      <link>https://arxiv.org/abs/2510.04391</link>
      <description>arXiv:2510.04391v4 Announce Type: replace-cross 
Abstract: The computational role of imagination remains debated. While classical accounts emphasize reward maximization, emerging evidence suggests it accesses internal world models (IWMs). We employ psychological network analysis to compare IWMs in humans and large language models (LLMs) via imagination vividness ratings, distinguishing offline world models (persistent memory structures accessed independent of immediate goals) from online models (task-specific representations). Analyzing 2,743 humans across three populations and six LLM variants, we find human imagination networks exhibit robust structural consistency, with high centrality correlations and aligned clustering. LLMs show minimal clustering and weak correlations with human networks, even with conversational memory, across environmental and sensory contexts. These differences highlight disparities in how biological and artificial systems organize internal representations. Our framework offers quantitative metrics for evaluating offline world models in cognitive agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04391v4</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.SI</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saurabh Ranjan, Brian Odegaard</dc:creator>
    </item>
    <item>
      <title>Structural Plasticity as Active Inference: A Biologically-Inspired Architecture for Homeostatic Control</title>
      <link>https://arxiv.org/abs/2511.02241</link>
      <description>arXiv:2511.02241v4 Announce Type: replace-cross 
Abstract: Traditional neural networks, while powerful, rely on biologically implausible learning mechanisms such as global backpropagation. This paper introduces the Structurally Adaptive Predictive Inference Network (SAPIN), a novel computational model inspired by the principles of active inference and the morphological plasticity observed in biological neural cultures. SAPIN operates on a 2D grid where processing units, or cells, learn by minimizing local prediction errors. The model features two primary, concurrent learning mechanisms: a local, Hebbian-like synaptic plasticity rule based on the temporal difference between a cell's actual activation and its learned expectation, and a structural plasticity mechanism where cells physically migrate across the grid to optimize their information-receptive fields. This dual approach allows the network to learn both how to process information (synaptic weights) and also where to position its computational resources (network topology). We validated the SAPIN model on the classic Cart Pole reinforcement learning benchmark. Our results demonstrate that the architecture can successfully solve the CartPole task, achieving robust performance. The network's intrinsic drive to minimize prediction error and maintain homeostasis was sufficient to discover a stable balancing policy. We also found that while continual learning led to instability, locking the network's parameters after achieving success resulted in a stable policy. When evaluated for 100 episodes post-locking (repeated over 100 successful agents), the locked networks maintained an average 82% success rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02241v4</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brennen A. Hill</dc:creator>
    </item>
  </channel>
</rss>
