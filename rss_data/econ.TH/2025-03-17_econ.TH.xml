<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.TH</link>
    <description>econ.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Mar 2025 03:26:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Statistical Impossibility and Possibility of Aligning LLMs with Human Preferences: From Condorcet Paradox to Nash Equilibrium</title>
      <link>https://arxiv.org/abs/2503.10990</link>
      <description>arXiv:2503.10990v1 Announce Type: cross 
Abstract: Aligning large language models (LLMs) with diverse human preferences is critical for ensuring fairness and informed outcomes when deploying these models for decision-making. In this paper, we seek to uncover fundamental statistical limits concerning aligning LLMs with human preferences, with a focus on the probabilistic representation of human preferences and the preservation of diverse preferences in aligned LLMs. We first show that human preferences can be represented by a reward model if and only if the preference among LLM-generated responses is free of any Condorcet cycle. Moreover, we prove that Condorcet cycles exist with probability converging to one exponentially fast under a probabilistic preference model, thereby demonstrating the impossibility of fully aligning human preferences using reward-based approaches such as reinforcement learning from human feedback. Next, we explore the conditions under which LLMs would employ mixed strategies -- meaning they do not collapse to a single response -- when aligned in the limit using a non-reward-based approach, such as Nash learning from human feedback (NLHF). We identify a necessary and sufficient condition for mixed strategies: the absence of a response that is preferred over all others by a majority. As a blessing, we prove that this condition holds with high probability under the probabilistic preference model, thereby highlighting the statistical possibility of preserving minority preferences without explicit regularization in aligning LLMs. Finally, we leverage insights from our statistical results to design a novel, computationally efficient algorithm for finding Nash equilibria in aligning LLMs with NLHF. Our experiments show that Llama-3.2-1B, aligned with our algorithm, achieves a win rate of 60.55\% against the base model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10990v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaizhao Liu, Qi Long, Zhekun Shi, Weijie J. Su, Jiancong Xiao</dc:creator>
    </item>
    <item>
      <title>Stable matching as transport</title>
      <link>https://arxiv.org/abs/2402.13378</link>
      <description>arXiv:2402.13378v2 Announce Type: replace 
Abstract: This paper links matching markets with aligned preferences to optimal transport theory. We show that stability, efficiency, and fairness emerge as solutions to a parametric family of optimal transport problems. The parameter reflects society's preferences for inequality. This link offers insights into structural properties of matchings and trade-offs between objectives; showing how stability can lead to welfare inequalities, even among similar agents. Our model captures supply-demand imbalances in contexts like spatial markets, school choice, and ride-sharing. We also show that large markets with idiosyncratic preferences can be well approximated by aligned preferences, expanding the applicability of our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13378v2</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Echenique, Joseph Root, Fedor Sandomirskiy</dc:creator>
    </item>
    <item>
      <title>Robust Comparative Statics with Misspecified Bayesian Learning</title>
      <link>https://arxiv.org/abs/2407.17037</link>
      <description>arXiv:2407.17037v2 Announce Type: replace 
Abstract: We present novel monotone comparative statics results for steady-state behavior in a dynamic optimization environment with misspecified Bayesian learning. Building on \cite{ep21a}, we analyze a Bayesian learner whose prior is over parameterized transition models but is misspecified in the sense that the true process does not belong to this set. We characterize conditions that ensure monotonicity in the steady-state distribution over states, actions, and inferred models. Additionally, we provide a new monotonicity-based proof of steady-state existence, derive an upper bound on the cost of misspecification, and illustrate the applicability of our results to several environments of general interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17037v2</guid>
      <category>econ.TH</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Aniruddha Ghosh</dc:creator>
    </item>
    <item>
      <title>Strategic Analysis of Fair Rank-Minimizing Mechanisms with Agent Refusal Option</title>
      <link>https://arxiv.org/abs/2408.01673</link>
      <description>arXiv:2408.01673v5 Announce Type: replace 
Abstract: This study examines strategic issues in fair rank-minimizing mechanisms, which choose an assignment that minimizes the average rank of object types to which agents are assigned and satisfy a fairness property called equal treatment of equals. As one of these fair mechanisms, the uniform rank-minimizing mechanism is considered. We focus on the case where agents can refuse their assignment and obtain the outside option instead. Without the refusal option, truth-telling is not strategically dominated by any strategies if a fair rank-minimizing mechanism is employed. However, if agents have the option and the uniform rank-minimizing mechanism is employed, then a strategy called an outside option demotion strategy strategically dominates truth-telling. Moreover, we show that adopting this strategy may lead to inefficient assignments. To counter this, we consider a modification of the uniform rank-minimizing mechanism, though it may lead agents to strategically reduce the number of acceptable types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01673v5</guid>
      <category>econ.TH</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yasunori Okumura</dc:creator>
    </item>
  </channel>
</rss>
