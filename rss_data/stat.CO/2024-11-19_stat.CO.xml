<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Nov 2024 05:01:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>LikelihoodGeometry: Macaulay2 Package</title>
      <link>https://arxiv.org/abs/2411.11165</link>
      <description>arXiv:2411.11165v1 Announce Type: new 
Abstract: This note introduces the $\texttt{LikelihoodGeometry}$ package for the computer algebra system $\textit{Macaulay2}$. This package gives tools to construct the likelihood correspondence of a discrete algebraic statistical model, a variety that that ties together data and their maximum likelihood estimators. This includes methods for constructing and combining popular statistical models and calculating their ML-degree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11165v1</guid>
      <category>stat.CO</category>
      <category>math.AC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Barnhill, John Cobb, Matthew Faust</dc:creator>
    </item>
    <item>
      <title>Understanding Learning with Sliced-Wasserstein Requires Rethinking Informative Slices</title>
      <link>https://arxiv.org/abs/2411.10651</link>
      <description>arXiv:2411.10651v1 Announce Type: cross 
Abstract: The practical applications of Wasserstein distances (WDs) are constrained by their sample and computational complexities. Sliced-Wasserstein distances (SWDs) provide a workaround by projecting distributions onto one-dimensional subspaces, leveraging the more efficient, closed-form WDs for one-dimensional distributions. However, in high dimensions, most random projections become uninformative due to the concentration of measure phenomenon. Although several SWD variants have been proposed to focus on \textit{informative} slices, they often introduce additional complexity, numerical instability, and compromise desirable theoretical (metric) properties of SWD. Amidst the growing literature that focuses on directly modifying the slicing distribution, which often face challenges, we revisit the classical Sliced-Wasserstein and propose instead to rescale the 1D Wasserstein to make all slices equally informative. Importantly, we show that with an appropriate data assumption and notion of \textit{slice informativeness}, rescaling for all individual slices simplifies to \textbf{a single global scaling factor} on the SWD. This, in turn, translates to the standard learning rate search for gradient-based learning in common machine learning workflows. We perform extensive experiments across various machine learning tasks showing that the classical SWD, when properly configured, can often match or surpass the performance of more complex variants. We then answer the following question: "Is Sliced-Wasserstein all you need for common learning tasks?"</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10651v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huy Tran, Yikun Bai, Ashkan Shahbazi, John R. Hershey, Soheil Kolouri</dc:creator>
    </item>
    <item>
      <title>Integrated Machine Learning and Survival Analysis Modeling for Enhanced Chronic Kidney Disease Risk Stratification</title>
      <link>https://arxiv.org/abs/2411.10754</link>
      <description>arXiv:2411.10754v1 Announce Type: cross 
Abstract: Chronic kidney disease (CKD) is a significant public health challenge, often progressing to end-stage renal disease (ESRD) if not detected and managed early. Early intervention, warranted by silent disease progression, can significantly reduce associated morbidity, mortality, and financial burden. In this study, we propose a novel approach to modeling CKD progression using a combination of machine learning techniques and classical statistical models. Building on the work of Liu et al. (2023), we evaluate linear models, tree-based methods, and deep learning models to extract novel predictors for CKD progression, with feature importance assessed using Shapley values. These newly identified predictors, integrated with established clinical features from the Kidney Failure Risk Equation, are then applied within the framework of Cox proportional hazards models to predict CKD progression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10754v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zachary Dana, Ahmed Ammar Naseer, Botros Toro, Sumanth Swaminathan</dc:creator>
    </item>
    <item>
      <title>Variance bounds and robust tuning for pseudo-marginal Metropolis--Hastings algorithms</title>
      <link>https://arxiv.org/abs/2411.10785</link>
      <description>arXiv:2411.10785v1 Announce Type: cross 
Abstract: The general applicability and ease of use of the pseudo-marginal Metropolis--Hastings (PMMH) algorithm, and particle Metropolis--Hastings in particular, makes it a popular method for inference on discretely observed Markovian stochastic processes. The performance of these algorithms and, in the case of particle Metropolis--Hastings, the trade off between improved mixing through increased accuracy of the estimator and the computational cost were investigated independently in two papers, both published in 2015. Each suggested choosing the number of particles so that the variance of the logarithm of the estimator of the posterior at a fixed sensible parameter value is approximately 1. This advice has been widely and successfully adopted. We provide new, remarkably simple upper and lower bounds on the asymptotic variance of PMMH algorithms. The bounds explain how blindly following the 2015 advice can hide serious issues with the algorithm and they strongly suggest an alternative criterion. In most situations our guidelines and those from 2015 closely coincide; however, when the two differ it is safer to follow the new guidance. An extension of one of our bounds shows how the use of correlated proposals can fundamentally shift the properties of pseudo-marginal algorithms, so that asymptotic variances that were infinite under the PMMH kernel become finite.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10785v1</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chris Sherlock</dc:creator>
    </item>
    <item>
      <title>A novel density-based approach for estimating unknown means, distribution visualisations and meta-analyses of quantiles</title>
      <link>https://arxiv.org/abs/2411.10971</link>
      <description>arXiv:2411.10971v1 Announce Type: cross 
Abstract: In meta-analysis with continuous outcomes, the use of effect sizes based on the means is the most common. It is often found, however, that only the quantile summary measures are reported in some studies, and in certain scenarios, a meta-analysis of the quantiles themselves are of interest. We propose a novel density-based approach to support the implementation of a comprehensive meta-analysis, when only the quantile summary measures are reported. The proposed approach uses flexible quantile-based distributions and percentile matching to estimate the unknown parameters without making any prior assumptions about the underlying distributions. Using simulated and real data, we show that the proposed novel density-based approach works as well as or better than the widely-used methods in estimating the means using quantile summaries without assuming a distribution apriori, and provides a novel tool for distribution visualisations. In addition to this, we introduce quantile-based meta-analysis methods for situations where a comparison of quantiles between groups themselves are of interest and found to be more suitable. Using both real and simulated data, we also demonstrate the applicability of these quantile-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10971v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alysha M De Livera, Luke Prendergast, Udara Kumaranathunga</dc:creator>
    </item>
    <item>
      <title>singleRcapture: An R Package for Single-Source Capture-Recapture Models</title>
      <link>https://arxiv.org/abs/2411.11032</link>
      <description>arXiv:2411.11032v1 Announce Type: cross 
Abstract: Population size estimation is a major challenge in official statistics, social sciences, and natural sciences. The problem can be tackled by applying capture-recapture methods, which vary depending on the number of sources used, particularly on whether a single or multiple sources are involved. This paper focuses on the first group of methods and introduces a novel R package: singleRcapture. The package implements state-of-the-art single-source capture-recapture (SSCR) models (e.g.zero-truncated one-inflated regression) together with new developments proposed by the authors, and provides a user-friendly application programming interface (API). This self-contained package can be used to produce point estimates and their variance and implements several bootstrap variance estimators or diagnostics to assess quality and conduct sensitivity analysis. It is intended for users interested in estimating the size of populations, particularly those that are difficult to reach or measure, for which information is available only from one source and dual/multiple system estimation is not applicable. Our package serves to bridge a significant gap, as the SSCR methods are either not available at all or are only partially implemented in existing R packages and other open-source software. Furthermore, since many R users are familiar with countreg or VGAM packages, we have implemented a lightweight extension called singleRcaptureExtra which can be used to integrate singleRcapture with these packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11032v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Piotr Chlebicki, Maciej Ber\k{e}sewicz</dc:creator>
    </item>
    <item>
      <title>Unbiased Approximations for Stationary Distributions of McKean-Vlasov SDEs</title>
      <link>https://arxiv.org/abs/2411.11270</link>
      <description>arXiv:2411.11270v1 Announce Type: cross 
Abstract: We consider the development of unbiased estimators, to approximate the stationary distribution of Mckean-Vlasov stochastic differential equations (MVSDEs). These are an important class of processes, which frequently appear in applications such as mathematical finance, biology and opinion dynamics. Typically the stationary distribution is unknown and indeed one cannot simulate such processes exactly. As a result one commonly requires a time-discretization scheme which results in a discretization bias and a bias from not being able to simulate the associated stationary distribution. To overcome this bias, we present a new unbiased estimator taking motivation from the literature on unbiased Monte Carlo. We prove the unbiasedness of our estimator, under assumptions. In order to prove this we require developing ergodicity results of various discrete time processes, through an appropriate discretization scheme, towards the invariant measure. Numerous numerical experiments are provided, on a range of MVSDEs, to demonstrate the effectiveness of our unbiased estimator. Such examples include the Currie-Weiss model, a 3D neuroscience model and a parameter estimation problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11270v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Elsiddig Awadelkarim, Neil K. Chada, Ajay Jasra</dc:creator>
    </item>
    <item>
      <title>Efficient smoothness selection for nonparametric Markov-switching models via quasi restricted maximum likelihood</title>
      <link>https://arxiv.org/abs/2411.11498</link>
      <description>arXiv:2411.11498v1 Announce Type: cross 
Abstract: Markov-switching models are powerful tools that allow capturing complex patterns from time series data driven by latent states. Recent work has highlighted the benefits of estimating components of these models nonparametrically, enhancing their flexibility and reducing biases, which in turn can improve state decoding, forecasting, and overall inference. Formulating such models using penalized splines is straightforward, but practically feasible methods for a data-driven smoothness selection in these models are still lacking. Traditional techniques, such as cross-validation and information criteria-based selection suffer from major drawbacks, most importantly their reliance on computationally expensive grid search methods, hampering practical usability for Markov-switching models. Michelot (2022) suggested treating spline coefficients as random effects with a multivariate normal distribution and using the R package TMB (Kristensen et al., 2016) for marginal likelihood maximization. While this method avoids grid search and typically results in adequate smoothness selection, it entails a nested optimization problem, thus being computationally demanding. We propose to exploit the simple structure of penalized splines treated as random effects, thereby greatly reducing the computational burden while potentially improving fixed effects parameter estimation accuracy. Our proposed method offers a reliable and efficient mechanism for smoothness selection, rendering the estimation of Markov-switching models involving penalized splines feasible for complex data structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11498v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan-Ole Koslik</dc:creator>
    </item>
    <item>
      <title>Metric Oja Depth, New Statistical Tool for Estimating the Most Central Objects</title>
      <link>https://arxiv.org/abs/2411.11580</link>
      <description>arXiv:2411.11580v1 Announce Type: cross 
Abstract: The Oja depth (simplicial volume depth) is one of the classical statistical techniques for measuring the central tendency of data in multivariate space. Despite the widespread emergence of object data like images, texts, matrices or graphs, a well-developed and suitable version of Oja depth for object data is lacking. To address this shortcoming, in this study we propose a novel measure of statistical depth, the metric Oja depth applicable to any object data. Then, we develop two competing strategies for optimizing metric depth functions, i.e., finding the deepest objects with respect to them. Finally, we compare the performance of the metric Oja depth with three other depth functions (half-space, lens, and spatial) in diverse data scenarios.
  Keywords: Object Data, Metric Oja depth, Statistical depth, Optimization, Genetic algorithm, Metric statistics</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11580v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vida Zamanifarizhandi, Joni Virta</dc:creator>
    </item>
    <item>
      <title>Sequential Kalman Tuning of the $t$-preconditioned Crank-Nicolson algorithm: efficient, adaptive and gradient-free inference for Bayesian inverse problems</title>
      <link>https://arxiv.org/abs/2407.07781</link>
      <description>arXiv:2407.07781v2 Announce Type: replace 
Abstract: Ensemble Kalman Inversion (EKI) has been proposed as an efficient method for the approximate solution of Bayesian inverse problems with expensive forward models. However, when applied to the Bayesian inverse problem EKI is only exact in the regime of Gaussian target measures and linear forward models. In this work we propose embedding EKI and Flow Annealed Kalman Inversion (FAKI), its normalizing flow (NF) preconditioned variant, within a Bayesian annealing scheme as part of an adaptive implementation of the $t$-preconditioned Crank-Nicolson (tpCN) sampler. The tpCN sampler differs from standard pCN in that its proposal is reversible with respect to the multivariate $t$-distribution. The more flexible tail behaviour allows for better adaptation to sampling from non-Gaussian targets. Within our Sequential Kalman Tuning (SKT) adaptation scheme, EKI is used to initialize and precondition the tpCN sampler for each annealed target. The subsequent tpCN iterations ensure particles are correctly distributed according to each annealed target, avoiding the accumulation of errors that would otherwise impact EKI. We demonstrate the performance of SKT for tpCN on three challenging numerical benchmarks, showing significant improvements in the rate of convergence compared to adaptation within standard SMC with importance weighted resampling at each temperature level, and compared to similar adaptive implementations of standard pCN. The SKT scheme applied to tpCN offers an efficient, practical solution for solving the Bayesian inverse problem when gradients of the forward model are not available. Code implementing the SKT schemes for tpCN is available at \url{https://github.com/RichardGrumitt/KalmanMC}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07781v2</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/1361-6420/ad934b</arxiv:DOI>
      <dc:creator>Richard D. P. Grumitt, Minas Karamanis, Uro\v{s} Seljak</dc:creator>
    </item>
    <item>
      <title>Bayesian Quantile Regression with Subset Selection: A Decision Analysis Perspective</title>
      <link>https://arxiv.org/abs/2311.02043</link>
      <description>arXiv:2311.02043v4 Announce Type: replace-cross 
Abstract: Quantile regression is a powerful tool for inferring how covariates affect specific percentiles of the response distribution. Existing methods either estimate conditional quantiles separately for each quantile of interest or estimate the entire conditional distribution using semi- or non-parametric models. The former often produce inadequate models for real data and do not share information across quantiles, while the latter are characterized by complex and constrained models that can be difficult to interpret and computationally inefficient. Neither approach is well-suited for quantile-specific subset selection. Instead, we pose the fundamental problems of linear quantile estimation, uncertainty quantification, and subset selection from a Bayesian decision analysis perspective. For any Bayesian regression model -- including, but not limited to existing Bayesian quantile regression models -- we derive optimal point estimates, interpretable uncertainty quantification, and scalable subset selection techniques for all model-based conditional quantiles. Our approach introduces a quantile-focused squared error loss that enables efficient, closed-form computing and maintains a close relationship with Wasserstein-based density estimation. In an extensive simulation study, our methods demonstrate substantial gains in quantile estimation accuracy, inference, and variable selection over frequentist and Bayesian competitors. We use these tools to identify and quantify the heterogeneous impacts of multiple social stressors and environmental exposures on educational outcomes across the full spectrum of low-, medium-, and high-achieving students in North Carolina.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02043v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph Feldman, Daniel Kowal</dc:creator>
    </item>
    <item>
      <title>Localized Schr\"odinger Bridge Sampler</title>
      <link>https://arxiv.org/abs/2409.07968</link>
      <description>arXiv:2409.07968v3 Announce Type: replace-cross 
Abstract: We consider the problem of sampling from an unknown distribution for which only a sufficiently large number of training samples are available. In this paper, we build on previous work combining Schr\"odinger bridges and plug &amp; play Langevin samplers. A key bottleneck of these approaches is the exponential dependence of the required training samples on the dimension, $d$, of the ambient state space. We propose a localization strategy which exploits conditional independence of conditional expectation values. Localization thus replaces a single high-dimensional Schr\"odinger bridge problem by $d$ low-dimensional Schr\"odinger bridge problems over the available training samples. In this context, a connection to multi-head self attention transformer architectures is established. As for the original Schr\"odinger bridge sampling approach, the localized sampler is stable and geometric ergodic. The sampler also naturally extends to conditional sampling and to Bayesian inference. We demonstrate the performance of our proposed scheme through experiments on a high-dimensional Gaussian problem, on a temporal stochastic process, and on a stochastic subgrid-scale parametrization conditional sampling problem. We also extend the idea of localization to plug &amp; play Langevin samplers using kernel-based denoising in combination with Tweedie's formula.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07968v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georg A. Gottwald, Sebastian Reich</dc:creator>
    </item>
    <item>
      <title>Multiscale Multi-Type Spatial Bayesian Analysis for High-Dimensional Data with Application to Wildfires and Migration</title>
      <link>https://arxiv.org/abs/2410.02905</link>
      <description>arXiv:2410.02905v2 Announce Type: replace-cross 
Abstract: Wildfires have significantly increased in the United States (U.S.), making certain areas harder to live in. This motivates us to jointly analyze active fires and population changes in the U.S. from July 2020 to June 2021. The available data are recorded on different scales (or spatial resolutions) and by different types of distributions (referred to as multi-type data). Moreover, wildfires are known to have feedback mechanism that creates signal-to-noise dependence. We analyze point-referenced remote sensing fire data from National Aeronautics and Space Administration (NASA) and county-level population change data provided by U.S. Census Bureau's Population Estimates Program (PEP). We develop a multiscale multi-type spatial Bayesian model that assumes the average number of fires is zero-inflated normal, the incidence of fire as Bernoulli, and the percentage population change as normally distributed. This high-dimensional dataset makes Markov chain Monte Carlo (MCMC) implementation infeasible. We bypass MCMC by extending a recently introduced computationally efficient Bayesian framework to directly sample from the exact posterior distribution, which includes a term to model signal-to-noise dependence. Such signal-to-noise dependence is known to be present in wildfire data, but is commonly not accounted for. A simulation study is used to highlight the computational performance of our method. In our analysis, we obtained predictions of wildfire probabilities, identified several useful covariates, and found that regions with many fires were associated with population change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02905v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Zhou, Jonathan R. Bradley</dc:creator>
    </item>
  </channel>
</rss>
