<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Mar 2024 14:40:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 05 Mar 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Resolution of Simpson's paradox via the common cause principle</title>
      <link>https://arxiv.org/abs/2403.00957</link>
      <description>arXiv:2403.00957v1 Announce Type: new 
Abstract: Simpson's paradox is an obstacle to establishing a probabilistic association between two events $a_1$ and $a_2$, given the third (lurking) random variable $B$. We focus on scenarios when the random variables $A$ (which combines $a_1$, $a_2$, and their complements) and $B$ have a common cause $C$ that need not be observed. Alternatively, we can assume that $C$ screens out $A$ from $B$. For such cases, the correct association between $a_1$ and $a_2$ is to be defined via conditioning over $C$. This set-up generalizes the original Simpson's paradox. Now its two contradicting options simply refer to two particular and different causes $C$. We show that if $B$ and $C$ are binary and $A$ is quaternary (the minimal and the most widespread situation for valid Simpson's paradox), the conditioning over any binary common cause $C$ establishes the same direction of the association between $a_1$ and $a_2$ as the conditioning over $B$ in the original formulation of the paradox. Thus, for the minimal common cause, one should choose the option of Simpson's paradox that assumes conditioning over $B$ and not its marginalization. For tertiary (unobserved) common causes $C$ all three options of Simpson's paradox become possible (i.e. marginalized, conditional, and none of them), and one needs prior information on $C$ to choose the right option.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00957v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. Hovhannisyan, A. E. Allahverdyan</dc:creator>
    </item>
    <item>
      <title>The Bridged Posterior: Optimization, Profile Likelihood and a New Approach to Generalized Bayes</title>
      <link>https://arxiv.org/abs/2403.00968</link>
      <description>arXiv:2403.00968v1 Announce Type: new 
Abstract: Optimization is widely used in statistics, thanks to its efficiency for delivering point estimates on useful spaces, such as those satisfying low cardinality or combinatorial structure. To quantify uncertainty, Gibbs posterior exponentiates the negative loss function to form a posterior density. Nevertheless, Gibbs posteriors are supported in a high-dimensional space, and do not inherit the computational efficiency or constraint formulations from optimization. In this article, we explore a new generalized Bayes approach, viewing the likelihood as a function of data, parameters, and latent variables conditionally determined by an optimization sub-problem. Marginally, the latent variable given the data remains stochastic, and is characterized by its posterior distribution. This framework, coined ``bridged posterior'', conforms to the Bayesian paradigm. Besides providing a novel generative model, we obtain a positively surprising theoretical finding that under mild conditions, the $\sqrt{n}$-adjusted posterior distribution of the parameters under our model converges to the same normal distribution as that of the canonical integrated posterior. Therefore, our result formally dispels a long-held belief that partial optimization of latent variables may lead to under-estimation of parameter uncertainty. We demonstrate the practical advantages of our approach under several settings, including maximum-margin classification, latent normal models, and harmonization of multiple networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00968v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Cheng Zeng, Eleni Dilma, Jason Xu, Leo L Duan</dc:creator>
    </item>
    <item>
      <title>A linear mixed model approach for measurement error adjustment: applications to sedentary behavior assessment from wearable devices</title>
      <link>https://arxiv.org/abs/2403.01000</link>
      <description>arXiv:2403.01000v1 Announce Type: new 
Abstract: In recent years, wearable devices have become more common to capture a wide range of health behaviors, especially for physical activity and sedentary behavior. These sensor-based measures are deemed to be objective and thus less prone to self-reported biases, inherent in questionnaire assessments. While this is undoubtedly a major advantage, there can still be measurement errors from the device recordings, which pose serious challenges for conducting statistical analysis and obtaining unbiased risk estimates. There is a vast literature proposing statistical methods for adjusting for measurement errors in self-reported behaviors, such as in dietary intake. However, there is much less research on error correction for sensor-based device measures, especially sedentary behavior. In this paper, we address this gap. Exploiting the excessive multiple-day assessments typically collected when sensor devices are deployed, we propose a two-stage linear mixed effect model (LME) based approach to correct bias caused by measurement errors. We provide theoretical proof of the debiasing process using the Best Linear Unbiased Predictors (BLUP), and use both simulation and real data from a cohort study to demonstrate the performance of the proposed approach while comparing to the na\"ive plug-in approach that directly uses device measures without appropriately adjusting measurement errors. Our results indicate that employing our easy-to-implement BLUP correction method can greatly reduce biases in disease risk estimates and thus enhance the validity of study findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01000v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruohui Chen, Dori Rosenberg, Chongzhi Di, Rong Zablocki, Sheri J Hartman, Andrea Lacroix, Xin Tu, Loki Natarajan, Lin Liu</dc:creator>
    </item>
    <item>
      <title>Error Analysis of a Simple Quaternion Estimator: the Gaussian Case</title>
      <link>https://arxiv.org/abs/2403.01150</link>
      <description>arXiv:2403.01150v1 Announce Type: new 
Abstract: Reference [1] introduces a novel closed-form quaternion estimator from two vector observations. The simplicity of the estimator enables clear physical insights and a closed-form expression for the bias as a function of the quaternion error covariance matrix. The latter could be approximated up to second order with respect to the underlying measurement noise assuming arbitrary probability distribution. The current note relaxes the second-order assumption and provides an expression for the error covariance that is exact to the fourth order, under the assumption of Gaussian distribution. This not only provides increased accuracy but also alleviates issues related to singularity. This technical note presents a comprehensive derivation of the individual components of the quaternion additive error covariance matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01150v1</guid>
      <category>stat.ME</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caitong Peng, Daniel Choukroun</dc:creator>
    </item>
    <item>
      <title>Re-evaluating the impact of hormone replacement therapy on heart disease using match-adaptive randomization inference</title>
      <link>https://arxiv.org/abs/2403.01330</link>
      <description>arXiv:2403.01330v1 Announce Type: new 
Abstract: Matching is an appealing way to design observational studies because it mimics the data structure produced by stratified randomized trials, pairing treated individuals with similar controls. After matching, inference is often conducted using methods tailored for stratified randomized trials in which treatments are permuted within matched pairs. However, in observational studies, matched pairs are not predetermined before treatment; instead, they are constructed based on observed treatment status. This introduces a challenge as the permutation distributions used in standard inference methods do not account for the possibility that permuting treatments might lead to a different selection of matched pairs ($Z$-dependence). To address this issue, we propose a novel and computationally efficient algorithm that characterizes and enables sampling from the correct conditional distribution of treatment after an optimal propensity score matching, accounting for $Z$-dependence. We show how this new procedure, called match-adaptive randomization inference, corrects for an anticonservative result in a well-known observational study investigating the impact of hormone replacement theory (HRT) on coronary heart disease and corroborates experimental findings about heterogeneous effects of HRT across different ages of initiation in women. Keywords: matching, causal inference, propensity score, permutation test, Type I error, graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01330v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel D. Pimentel, Ruoqi Yu</dc:creator>
    </item>
    <item>
      <title>Minimax-Regret Sample Selection in Randomized Experiments</title>
      <link>https://arxiv.org/abs/2403.01386</link>
      <description>arXiv:2403.01386v1 Announce Type: new 
Abstract: Randomized controlled trials (RCTs) are often run in settings with many subpopulations that may have differential benefits from the treatment being evaluated. We consider the problem of sample selection, i.e., whom to enroll in an RCT, such as to optimize welfare in a heterogeneous population. We formalize this problem within the minimax-regret framework, and derive optimal sample-selection schemes under a variety of conditions. We also highlight how different objectives and decisions can lead to notably different guidance regarding optimal sample allocation through a synthetic experiment leveraging historical COVID-19 trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01386v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchen Hu, Henry Zhu, Emma Brunskill, Stefan Wager</dc:creator>
    </item>
    <item>
      <title>Two-phase rejective sampling</title>
      <link>https://arxiv.org/abs/2403.01477</link>
      <description>arXiv:2403.01477v1 Announce Type: new 
Abstract: Rejective sampling improves design and estimation efficiency of single-phase sampling when auxiliary information in a finite population is available. When such auxiliary information is unavailable, we propose to use two-phase rejective sampling (TPRS), which involves measuring auxiliary variables for the sample of units in the first phase, followed by the implementation of rejective sampling for the outcome in the second phase. We explore the asymptotic design properties of double expansion and regression estimators under TPRS. We show that TPRS enhances the efficiency of the double expansion estimator, rendering it comparable to a regression estimator. We further refine the design to accommodate varying importance of covariates and extend it to multi-phase sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01477v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shu Yang, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Dendrogram of mixing measures: Learning latent hierarchy and model selection for finite mixture models</title>
      <link>https://arxiv.org/abs/2403.01684</link>
      <description>arXiv:2403.01684v1 Announce Type: new 
Abstract: We present a new way to summarize and select mixture models via the hierarchical clustering tree (dendrogram) of an overfitted latent mixing measure. Our proposed method bridges agglomerative hierarchical clustering and mixture modeling. The dendrogram's construction is derived from the theory of convergence of the mixing measures, and as a result, we can both consistently select the true number of mixing components and obtain the pointwise optimal convergence rate for parameter estimation from the tree, even when the model parameters are only weakly identifiable. In theory, it explicates the choice of the optimal number of clusters in hierarchical clustering. In practice, the dendrogram reveals more information on the hierarchy of subpopulations compared to traditional ways of summarizing mixture models. Several simulation studies are carried out to support our theory. We also illustrate the methodology with an application to single-cell RNA sequence analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01684v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dat Do, Linh Do, Scott A. McKinley, Jonathan Terhorst, XuanLong Nguyen</dc:creator>
    </item>
    <item>
      <title>Graphical n-sample tests of correspondence of distributions</title>
      <link>https://arxiv.org/abs/2403.01838</link>
      <description>arXiv:2403.01838v1 Announce Type: new 
Abstract: Classical tests are available for the two-sample test of correspondence of distribution functions. From these, the Kolmogorov-Smirnov test provides also the graphical interpretation of the test results, in different forms. Here, we propose modifications of the Kolmogorov-Smirnov test with higher power. The proposed tests are based on the so-called global envelope test which allows for graphical interpretation, similarly as the Kolmogorov-Smirnov test. The tests are based on rank statistics and are suitable also for the comparison of $n$ samples, with $n \geq 2$. We compare the alternatives for the two-sample case through an extensive simulation study and discuss their interpretation. Finally, we apply the tests to real data. Specifically, we compare the height distributions between boys and girls at different ages, as well as sepal length distributions of different flower species using the proposed methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01838v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konstantinos Konstantinou, Tom\'a\v{s} Mrkvi\v{c}ka, Mari Myllym\"aki</dc:creator>
    </item>
    <item>
      <title>On Fractional Moment Estimation from Polynomial Chaos Expansion</title>
      <link>https://arxiv.org/abs/2403.01948</link>
      <description>arXiv:2403.01948v1 Announce Type: new 
Abstract: Fractional statistical moments are utilized for various tasks of uncertainty quantification, including the estimation of probability distributions. However, an estimation of fractional statistical moments of costly mathematical models by statistical sampling is challenging since it is typically not possible to create a large experimental design due to limitations in computing capacity. This paper presents a novel approach for the analytical estimation of fractional moments, directly from polynomial chaos expansions. Specifically, the first four statistical moments obtained from the deterministic PCE coefficients are used for an estimation of arbitrary fractional moments via H\"{o}lder's inequality. The proposed approach is utilized for an estimation of statistical moments and probability distributions in three numerical examples of increasing complexity. Obtained results show that the proposed approach achieves a superior performance in estimating the distribution of the response, in comparison to a standard Latin hypercube sampling in the presented examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01948v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luk\'a\v{s} Nov\'ak, Marcos Valdebenito, Matthias Faes</dc:creator>
    </item>
    <item>
      <title>Utility-based optimization of Fujikawa's basket trial design - Pre-specified protocol of a comparison study</title>
      <link>https://arxiv.org/abs/2403.02058</link>
      <description>arXiv:2403.02058v1 Announce Type: new 
Abstract: Basket trial designs are a type of master protocol in which the same therapy is tested in several strata of the patient cohort. Many basket trial designs implement borrowing mechanisms. These allow sharing information between similar strata with the goal of increasing power in responsive strata while at the same time constraining type-I error inflation to a bearable threshold. These borrowing mechanisms can be tuned using numerical tuning parameters. The optimal choice of these tuning parameters is subject to research. In a comparison study using simulations and numerical calculations, we are planning to investigate the use of utility functions for quantifying the compromise between power and type-I error inflation and the use of numerical optimization algorithms for optimizing these functions. The present document is the protocol of this comparison study, defining each step of the study in accordance with the ADEMP scheme for pre-specification of simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02058v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas D Sauer, Alexander Ritz, Meinhard Kieser</dc:creator>
    </item>
    <item>
      <title>Expectile Periodograms</title>
      <link>https://arxiv.org/abs/2403.02060</link>
      <description>arXiv:2403.02060v1 Announce Type: new 
Abstract: In this paper, we introduce a periodogram-like function, called expectile periodograms, for detecting and estimating hidden periodicity from observations with asymmetrically distributed noise. The expectile periodograms are constructed from trigonometric expectile regression where a specially designed objective function is used to substitute the squared $l_2$ norm that leads to the ordinary periodograms. The expectile periodograms have properties which are analogous to quantile periodograms, which provide a broader view of the time series by examining different expectile levels, but are much faster to calculate. The asymptotic properties are discussed and simulations show its efficiency and robustness in the presence of hidden periodicities with asymmetric or heavy-tailed noise. Finally, we leverage the inherent two-dimensional characteristics of the expectile periodograms and train a deep-learning (DL) model to classify the earthquake waveform data. Remarkably, our approach achieves heightened classification testing accuracy when juxtaposed with alternative periodogram-based methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02060v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianbo Chen</dc:creator>
    </item>
    <item>
      <title>Double trouble: Predicting new variant counts across two heterogeneous populations</title>
      <link>https://arxiv.org/abs/2403.02154</link>
      <description>arXiv:2403.02154v1 Announce Type: new 
Abstract: Collecting genomics data across multiple heterogeneous populations (e.g., across different cancer types) has the potential to improve our understanding of disease. Despite sequencing advances, though, resources often remain a constraint when gathering data. So it would be useful for experimental design if experimenters with access to a pilot study could predict the number of new variants they might expect to find in a follow-up study: both the number of new variants shared between the populations and the total across the populations. While many authors have developed prediction methods for the single-population case, we show that these predictions can fare poorly across multiple populations that are heterogeneous. We prove that, surprisingly, a natural extension of a state-of-the-art single-population predictor to multiple populations fails for fundamental reasons. We provide the first predictor for the number of new shared variants and new total variants that can handle heterogeneity in multiple populations. We show that our proposed method works well empirically using real cancer and population genetics data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02154v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunyi Shen, Lorenzo Masoero, Joshua G. Schraiber, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>Boosting Distributional Copula Regression for Bivariate Binary, Discrete and Mixed Responses</title>
      <link>https://arxiv.org/abs/2403.02194</link>
      <description>arXiv:2403.02194v1 Announce Type: new 
Abstract: Motivated by challenges in the analysis of biomedical data and observational studies, we develop statistical boosting for the general class of bivariate distributional copula regression with arbitrary marginal distributions, which is suited to model binary, count, continuous or mixed outcomes. In our framework, the joint distribution of arbitrary, bivariate responses is modelled through a parametric copula. To arrive at a model for the entire conditional distribution, not only the marginal distribution parameters but also the copula parameters are related to covariates through additive predictors. We suggest efficient and scalable estimation by means of an adapted component-wise gradient boosting algorithm with statistical models as base-learners. A key benefit of boosting as opposed to classical likelihood or Bayesian estimation is the implicit data-driven variable selection mechanism as well as shrinkage without additional input or assumptions from the analyst. To the best of our knowledge, our implementation is the only one that combines a wide range of covariate effects, marginal distributions, copula functions, and implicit data-driven variable selection. We showcase the versatility of our approach on data from genetic epidemiology, healthcare utilization and childhood undernutrition. Our developments are implemented in the R package gamboostLSS, fostering transparent and reproducible research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02194v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillermo Brise\~no Sanchez, Nadja Klein, Hannah Klinkhammer, Andreas Mayr</dc:creator>
    </item>
    <item>
      <title>Dynamic programming principle in cost-efficient sequential design: application to switching measurements</title>
      <link>https://arxiv.org/abs/2403.02245</link>
      <description>arXiv:2403.02245v1 Announce Type: new 
Abstract: We study sequential cost-efficient design in a situation where each update of covariates involves a fixed time cost typically considerable compared to a single measurement time. The problem arises from parameter estimation in switching measurements on superconducting Josephson junctions which are components needed in quantum computers and other superconducting electronics. In switching measurements, a sequence of current pulses is applied to the junction and a binary voltage response is observed. The measurement requires a very low temperature that can be kept stable only for a relatively short time, and therefore it is essential to use an efficient design. We use the dynamic programming principle from the mathematical theory of optimal control to solve the optimal update times. Our simulations demonstrate the cost-efficiency compared to the previously used methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02245v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeongmin Han, Juha Karvanen, Mikko Parviainen</dc:creator>
    </item>
    <item>
      <title>A framework for understanding data science</title>
      <link>https://arxiv.org/abs/2403.00776</link>
      <description>arXiv:2403.00776v1 Announce Type: cross 
Abstract: The objective of this research is to provide a framework with which the data science community can understand, define, and develop data science as a field of inquiry. The framework is based on the classical reference framework (axiology, ontology, epistemology, methodology) used for 200 years to define knowledge discovery paradigms and disciplines in the humanities, sciences, algorithms, and now data science. I augmented it for automated problem-solving with (methods, technology, community). The resulting data science reference framework is used to define the data science knowledge discovery paradigm in terms of the philosophy of data science addressed in previous papers and the data science problem-solving paradigm, i.e., the data science method, and the data science problem-solving workflow, both addressed in this paper. The framework is a much called for unifying framework for data science as it contains the components required to define data science. For insights to better understand data science, this paper uses the framework to define the emerging, often enigmatic, data science problem-solving paradigm and workflow, and to compare them with their well-understood scientific counterparts, scientific problem-solving paradigm and workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00776v1</guid>
      <category>stat.OT</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael L Brodie</dc:creator>
    </item>
    <item>
      <title>Asymptotic expansion of the drift estimator for the fractional Ornstein-Uhlenbeck process</title>
      <link>https://arxiv.org/abs/2403.00967</link>
      <description>arXiv:2403.00967v1 Announce Type: cross 
Abstract: We present an asymptotic expansion formula of an estimator for the drift coefficient of the fractional Ornstein-Uhlenbeck process. As the machinery, we apply the general expansion scheme for Wiener functionals recently developed by the authors [26]. The central limit theorem in the principal part of the expansion has the classical scaling T^{1/2}. However, the asymptotic expansion formula is a complex in that the order of the correction term becomes the classical T^{-1/2} for H in (1/2,5/8), but T^{4H-3} for H in [5/8, 3/4).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00967v1</guid>
      <category>math.PR</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ciprian A. Tudor, Nakahiro Yoshida</dc:creator>
    </item>
    <item>
      <title>The Science of Data Collection: Insights from Surveys can Improve Machine Learning Models</title>
      <link>https://arxiv.org/abs/2403.01208</link>
      <description>arXiv:2403.01208v1 Announce Type: cross 
Abstract: Whether future AI models make the world safer or less safe for humans rests in part on our ability to efficiently collect accurate data from people about what they want the models to do. However, collecting high quality data is difficult, and most AI/ML researchers are not trained in data collection methods. The growing emphasis on data-centric AI highlights the potential of data to enhance model performance. It also reveals an opportunity to gain insights from survey methodology, the science of collecting high-quality survey data.
  In this position paper, we summarize lessons from the survey methodology literature and discuss how they can improve the quality of training and feedback data, which in turn improve model performance. Based on the cognitive response process model, we formulate specific hypotheses about the aspects of label collection that may impact training data quality. We also suggest collaborative research ideas into how possible biases in data collection can be mitigated, making models more accurate and human-centric.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01208v1</guid>
      <category>cs.HC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stephanie Eckman, Barbara Plank, Frauke Kreuter</dc:creator>
    </item>
    <item>
      <title>Greedy selection of optimal location of sensors for uncertainty reduction in seismic moment tensor inversion</title>
      <link>https://arxiv.org/abs/2403.01403</link>
      <description>arXiv:2403.01403v1 Announce Type: cross 
Abstract: We address an optimal sensor placement problem through Bayesian experimental design for seismic full waveform inversion for the recovery of the associated moment tensor. The objective is that of optimally choosing the location of the sensors (stations) from which to collect the observed data. The Shannon expected information gain is used as the objective function to search for the optimal network of sensors. A closed form for such objective is available due to the linear structure of the forward problem, as well as the Gaussian modeling of the observational errors and prior distribution. The resulting problem being inherently combinatorial, a greedy algorithm is deployed to sequentially select the sensor locations that form the best network for learning the moment tensor. Numerical results are presented and analyzed under several instances of the problem, including: use of full three-dimensional velocity-models, cases in which the earthquake-source location is unknown, as well as moment tensor inversion under model misspecification</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01403v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ben Mansour Dia, Michael Fehler, SanLinn I. Kaka, Andrea Scarinci, Umair bin Waheed, Chen Gu</dc:creator>
    </item>
    <item>
      <title>Improving generalisation via anchor multivariate analysis</title>
      <link>https://arxiv.org/abs/2403.01865</link>
      <description>arXiv:2403.01865v1 Announce Type: cross 
Abstract: We introduce a causal regularisation extension to anchor regression (AR) for improved out-of-distribution (OOD) generalisation. We present anchor-compatible losses, aligning with the anchor framework to ensure robustness against distribution shifts. Various multivariate analysis (MVA) algorithms, such as (Orthonormalized) PLS, RRR, and MLR, fall within the anchor framework. We observe that simple regularisation enhances robustness in OOD settings. Estimators for selected algorithms are provided, showcasing consistency and efficacy in synthetic and real-world climate science problems. The empirical validation highlights the versatility of anchor regularisation, emphasizing its compatibility with MVA approaches and its role in enhancing replicability while guarding against distribution shifts. The extended AR framework advances causal inference methodologies, addressing the need for reliable OOD generalisation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01865v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Homer Durand, Gherardo Varando, Gustau Camps-Valls, Nathan Mankovich</dc:creator>
    </item>
    <item>
      <title>Kpop: A kernel balancing approach for reducing specification assumptions in survey weighting</title>
      <link>https://arxiv.org/abs/2107.08075</link>
      <description>arXiv:2107.08075v2 Announce Type: replace 
Abstract: With the precipitous decline in response rates, researchers and pollsters have been left with highly non-representative samples, relying on constructed weights to make these samples representative of the desired target population. Though practitioners employ valuable expert knowledge to choose what variables, $X$ must be adjusted for, they rarely defend particular functional forms relating these variables to the response process or the outcome. Unfortunately, commonly-used calibration weights -- which make the weighted mean $X$ in the sample equal that of the population -- only ensure correct adjustment when the portion of the outcome and the response process left unexplained by linear functions of $X$ are independent. To alleviate this functional form dependency, we describe kernel balancing for population weighting (kpop). This approach replaces the design matrix $\mathbf{X}$ with a kernel matrix, $\mathbf{K}$ encoding high-order information about $\mathbf{X}$. Weights are then found to make the weighted average row of $\mathbf{K}$ among sampled units approximately equal that of the target population. This produces good calibration on a wide range of smooth functions of $X$, without relying on the user to decide which $X$ or what functions of them to include. We describe the method and illustrate it by application to polling data from the 2016 U.S. presidential election.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.08075v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erin Hartman, Chad Hazlett, Ciara Sterbenz</dc:creator>
    </item>
    <item>
      <title>Multivariate Tie-breaker Designs</title>
      <link>https://arxiv.org/abs/2202.10030</link>
      <description>arXiv:2202.10030v4 Announce Type: replace 
Abstract: In a tie-breaker design (TBD), subjects with high values of a running variable are given some (usually desirable) treatment, subjects with low values are not, and subjects in the middle are randomized. TBDs are intermediate between regression discontinuity designs (RDDs) and randomized controlled trials (RCTs) by allowing a tradeoff between the resource allocation efficiency of an RDD and the statistical efficiency of an RCT. We study a model where the expected response is one multivariate regression for treated subjects and another for control subjects. For given covariates, we show how to use convex optimization to choose treatment probabilities that optimize a D-optimality criterion. We can incorporate a variety of constraints motivated by economic and ethical considerations. In our model, D-optimality for the treatment effect coincides with D-optimality for the whole regression, and without economic constraints, an RCT is globally optimal. We show that a monotonicity constraint favoring more deserving subjects induces sparsity in the number of distinct treatment probabilities and this is different from preexisting sparsity results for constrained designs. We also study a prospective D-optimality, analogous to Bayesian optimal design, to understand design tradeoffs without reference to a specific data set. We apply the convex optimization solution to a semi-synthetic example involving triage data from the MIMIC-IV-ED database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.10030v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim P. Morrison, Art B. Owen</dc:creator>
    </item>
    <item>
      <title>The Effect of Alcohol Consumption on Brain Ageing: A New Causal Inference Framework for Incomplete and Massive Phenomic Data</title>
      <link>https://arxiv.org/abs/2303.03520</link>
      <description>arXiv:2303.03520v2 Announce Type: replace 
Abstract: Although substance use, such as alcohol consumption, is known to be associated with cognitive decline during ageing, its direct influence on the central nervous system remains unclear. In this study, we aim to investigate the potential influence of alcohol intake frequency on accelerated brain ageing by estimating the mean potential brain-age gap (BAG) index, the difference between brain age and actual age, under different alcohol intake frequencies in a large UK Biobank (UKB) cohort with extensive phenomic data reflecting a comprehensive life-style profile. We face two major challenges: (1) a large number of phenomic variables as potential confounders and (2) a small proportion of participants with complete phenomic data. To address these challenges, we first develop a new ensemble learning framework to establish robust estimation of mean potential outcome in the presence of many confounders. We then construct a data integration step to borrow information from UKB participants with incomplete phenomic data to improve efficiency. Our analysis results reveal that daily intake or even a few times a week may have significant effects on accelerating brain ageing. Moreover, extensive numerical studies demonstrate the superiority of our method over competing methods, in terms of smaller estimation bias and variability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.03520v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chixiang Chen, Shuo Chen, Zhenyao Ye, Xu Shi, Tianzhou Ma</dc:creator>
    </item>
    <item>
      <title>Empirical sandwich variance estimator for iterated conditional expectation g-computation</title>
      <link>https://arxiv.org/abs/2306.10976</link>
      <description>arXiv:2306.10976v2 Announce Type: replace 
Abstract: Iterated conditional expectation (ICE) g-computation is an estimation approach for addressing time-varying confounding for both longitudinal and time-to-event data. Unlike other g-computation implementations, ICE avoids the need to specify models for each time-varying covariate. For variance estimation, previous work has suggested the bootstrap. However, bootstrapping can be computationally intense and sensitive to the number of resamples used. Here, we present ICE g-computation as a set of stacked estimating equations. Therefore, the variance for the ICE g-computation estimator can be consistently estimated using the empirical sandwich variance estimator. Performance of the variance estimator was evaluated empirically with a simulation study. The proposed approach is also demonstrated with an illustrative example on the effect of cigarette smoking on the prevalence of hypertension. In the simulation study, the empirical sandwich variance estimator appropriately estimated the variance. When comparing runtimes between the sandwich variance estimator and the bootstrap for the applied example, the sandwich estimator was substantially faster, even when bootstraps were run in parallel. The empirical sandwich variance estimator is a viable option for variance estimation with ICE g-computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10976v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul N Zivich, Rachael K Ross, Bonnie E Shook-Sa, Stephen R Cole, Jessie K Edwards</dc:creator>
    </item>
    <item>
      <title>The Causal Roadmap and Simulations to Improve the Rigor and Reproducibility of Real-Data Applications</title>
      <link>https://arxiv.org/abs/2309.03952</link>
      <description>arXiv:2309.03952v4 Announce Type: replace 
Abstract: The Causal Roadmap outlines a systematic approach to asking and answering questions of cause-and-effect: define quantity of interest, evaluate needed assumptions, conduct statistical estimation, and carefully interpret results. It is paramount that the algorithm for statistical estimation and inference be carefully pre-specified to optimize its expected performance for the specific real-data application. Simulations that realistically reflect the application, including key characteristics such as strong confounding and dependent or missing outcomes, can help us gain a better understanding of an estimator's applied performance. We illustrate this with two examples, using the Causal Roadmap and realistic simulations to inform estimator selection and full specification of the Statistical Analysis Plan. First, in an observational longitudinal study, outcome-blind simulations are used to inform nuisance parameter estimation and variance estimation for longitudinal targeted maximum likelihood estimation (TMLE). Second, in a cluster-randomized controlled trial with missing outcomes, treatment-blind simulations are used to ensure control for Type-I error in Two-Stage TMLE. In both examples, realistic simulations empower us to pre-specify an estimator that is expected to have strong finite sample performance and also yield quality-controlled computing code for the actual analysis. Together, this process helps to improve the rigor and reproducibility of our research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.03952v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nerissa Nance, Maya L. Petersen, Mark van der Laan, Laura B. Balzer</dc:creator>
    </item>
    <item>
      <title>Second-order group knockoffs with applications to GWAS</title>
      <link>https://arxiv.org/abs/2310.15069</link>
      <description>arXiv:2310.15069v2 Announce Type: replace 
Abstract: Conditional testing via the knockoff framework allows one to identify -- among large number of possible explanatory variables -- those that carry unique information about an outcome of interest, and also provides a false discovery rate guarantee on the selection. This approach is particularly well suited to the analysis of genome wide association studies (GWAS), which have the goal of identifying genetic variants which influence traits of medical relevance.
  While conditional testing can be both more powerful and precise than traditional GWAS analysis methods, its vanilla implementation encounters a difficulty common to all multivariate analysis methods: it is challenging to distinguish among multiple, highly correlated regressors. This impasse can be overcome by shifting the object of inference from single variables to groups of correlated variables. To achieve this, it is necessary to construct "group knockoffs." While successful examples are already documented in the literature, this paper substantially expands the set of algorithms and software for group knockoffs. We focus in particular on second-order knockoffs, for which we describe correlation matrix approximations that are appropriate for GWAS data and that result in considerable computational savings. We illustrate the effectiveness of the proposed methods with simulations and with the analysis of albuminuria data from the UK Biobank.
  The described algorithms are implemented in an open-source Julia package Knockoffs.jl, for which both R and Python wrappers are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.15069v2</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin B Chu, Jiaqi Gu, Zhaomeng Chen, Tim Morrison, Emmanuel Candes, Zihuai He, Chiara Sabatti</dc:creator>
    </item>
    <item>
      <title>Degree-heterogeneous Latent Class Analysis for High-dimensional Discrete Data</title>
      <link>https://arxiv.org/abs/2402.18745</link>
      <description>arXiv:2402.18745v2 Announce Type: replace 
Abstract: The latent class model is a widely used mixture model for multivariate discrete data. Besides the existence of qualitatively heterogeneous latent classes, real data often exhibit additional quantitative heterogeneity nested within each latent class. The modern latent class analysis also faces extra challenges, including the high-dimensionality, sparsity, and heteroskedastic noise inherent in discrete data. Motivated by these phenomena, we introduce the Degree-heterogeneous Latent Class Model and propose a spectral approach to clustering and statistical inference in the challenging high-dimensional sparse data regime. We propose an easy-to-implement HeteroClustering algorithm. It uses heteroskedastic PCA with L2 normalization to remove degree effects and perform clustering in the top singular subspace of the data matrix. We establish an exponential error rate for HeteroClustering, leading to exact clustering under minimal signal-to-noise conditions. We further investigate the estimation and inference of the high-dimensional continuous item parameters in the model, which are crucial to interpreting and finding useful markers for latent classes. We provide comprehensive procedures for global testing and multiple testing of these parameters with valid error controls. The superior performance of our methods is demonstrated through extensive simulations and applications to three diverse real-world datasets from political voting records, genetic variations, and single-cell sequencing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18745v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongyuan Lyu, Ling Chen, Yuqi Gu</dc:creator>
    </item>
    <item>
      <title>Private Prediction Sets</title>
      <link>https://arxiv.org/abs/2102.06202</link>
      <description>arXiv:2102.06202v3 Announce Type: replace-cross 
Abstract: In real-world settings involving consequential decision-making, the deployment of machine learning systems generally requires both reliable uncertainty quantification and protection of individuals' privacy. We present a framework that treats these two desiderata jointly. Our framework is based on conformal prediction, a methodology that augments predictive models to return prediction sets that provide uncertainty quantification -- they provably cover the true response with a user-specified probability, such as 90%. One might hope that when used with privately-trained models, conformal prediction would yield privacy guarantees for the resulting prediction sets; unfortunately, this is not the case. To remedy this key problem, we develop a method that takes any pre-trained predictive model and outputs differentially private prediction sets. Our method follows the general approach of split conformal prediction; we use holdout data to calibrate the size of the prediction sets but preserve privacy by using a privatized quantile subroutine. This subroutine compensates for the noise introduced to preserve privacy in order to guarantee correct coverage. We evaluate the method on large-scale computer vision datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2102.06202v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1162/99608f92.16c71dad</arxiv:DOI>
      <arxiv:journal_reference>Harvard Data Science Review, 4(2). 2022</arxiv:journal_reference>
      <dc:creator>Anastasios N. Angelopoulos, Stephen Bates, Tijana Zrnic, Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>Yurinskii's Coupling for Martingales</title>
      <link>https://arxiv.org/abs/2210.00362</link>
      <description>arXiv:2210.00362v2 Announce Type: replace-cross 
Abstract: Yurinskii's coupling is a popular theoretical tool for non-asymptotic distributional analysis in mathematical statistics and applied probability, offering a Gaussian strong approximation with an explicit error bound under easily verified conditions. Originally stated in $\ell^2$-norm for sums of independent random vectors, it has recently been extended both to the $\ell^p$-norm, for $1 \leq p \leq \infty$, and to vector-valued martingales in $\ell^2$-norm, under some strong conditions. We present as our main result a Yurinskii coupling for approximate martingales in $\ell^p$-norm, under substantially weaker conditions than those previously imposed. Our formulation further allows for the coupling variable to follow a more general Gaussian mixture distribution, and we provide a novel third-order coupling method which gives tighter approximations in certain settings. We specialize our main result to mixingales, martingales, and independent data, and derive uniform Gaussian mixture strong approximations for martingale empirical processes. Applications to nonparametric partitioning-based and local polynomial regression procedures are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.00362v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Ricardo P. Masini, William G. Underwood</dc:creator>
    </item>
    <item>
      <title>Approximately optimal domain adaptation with Fisher's Linear Discriminant</title>
      <link>https://arxiv.org/abs/2302.14186</link>
      <description>arXiv:2302.14186v3 Announce Type: replace-cross 
Abstract: We propose a class of models based on Fisher's Linear Discriminant (FLD) in the context of domain adaptation. The class is the convex combination of two hypotheses: i) an average hypothesis representing previously seen source tasks and ii) a hypothesis trained on a new target task. For a particular generative setting we derive the optimal convex combination of the two models under 0-1 loss, propose a computable approximation, and study the effect of various parameter settings on the relative risks between the optimal hypothesis, hypothesis i), and hypothesis ii). We demonstrate the effectiveness of the proposed optimal classifier in the context of EEG- and ECG-based classification settings and argue that the optimal classifier can be computed without access to direct information from any of the individual source tasks. We conclude by discussing further applications, limitations, and possible future directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.14186v3</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hayden S. Helm, Ashwin De Silva, Joshua T. Vogelstein, Carey E. Priebe, Weiwei Yang</dc:creator>
    </item>
    <item>
      <title>Simulation-based, Finite-sample Inference for Privatized Data</title>
      <link>https://arxiv.org/abs/2303.05328</link>
      <description>arXiv:2303.05328v4 Announce Type: replace-cross 
Abstract: Privacy protection methods, such as differentially private mechanisms, introduce noise into resulting statistics which often produces complex and intractable sampling distributions. In this paper, we propose a simulation-based "repro sample" approach to produce statistically valid confidence intervals and hypothesis tests, which builds on the work of Xie and Wang (2022). We show that this methodology is applicable to a wide variety of private inference problems, appropriately accounts for biases introduced by privacy mechanisms (such as by clamping), and improves over other state-of-the-art inference methods such as the parametric bootstrap in terms of the coverage and type I error of the private inference. We also develop significant improvements and extensions for the repro sample methodology for general models (not necessarily related to privacy), including 1) modifying the procedure to ensure guaranteed coverage and type I errors, even accounting for Monte Carlo error, and 2) proposing efficient numerical algorithms to implement the confidence intervals and $p$-values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.05328v4</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Awan, Zhanyu Wang</dc:creator>
    </item>
    <item>
      <title>Designing Decision Support Systems Using Counterfactual Prediction Sets</title>
      <link>https://arxiv.org/abs/2306.03928</link>
      <description>arXiv:2306.03928v2 Announce Type: replace-cross 
Abstract: Decision support systems for classification tasks are predominantly designed to predict the value of the ground truth labels. However, since their predictions are not perfect, these systems also need to make human experts understand when and how to use these predictions to update their own predictions. Unfortunately, this has been proven challenging. In this context, it has been recently argued that an alternative type of decision support systems may circumvent this challenge. Rather than providing a single label prediction, these systems provide a set of label prediction values constructed using a conformal predictor, namely a prediction set, and forcefully ask experts to predict a label value from the prediction set. However, the design and evaluation of these systems have so far relied on stylized expert models, questioning their promise. In this paper, we revisit the design of this type of systems from the perspective of online learning and develop a methodology that does not require, nor assumes, an expert model. Our methodology leverages the nested structure of the prediction sets provided by any conformal predictor and a natural counterfactual monotonicity assumption to achieve an exponential improvement in regret in comparison to vanilla bandit algorithms. We conduct a large-scale human subject study ($n = 2{,}751$) to compare our methodology to several competitive baselines. The results show that, for decision support systems based on prediction sets, limiting experts' level of agency leads to greater performance than allowing experts to always exercise their own agency. We have made available the data gathered in our human subject study as well as an open source implementation of our system at https://github.com/Networks-Learning/counterfactual-prediction-sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.03928v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eleni Straitouri, Manuel Gomez Rodriguez</dc:creator>
    </item>
    <item>
      <title>Distribution-free inference with hierarchical data</title>
      <link>https://arxiv.org/abs/2306.06342</link>
      <description>arXiv:2306.06342v3 Announce Type: replace-cross 
Abstract: This paper studies distribution-free inference in settings where the data set has a hierarchical structure -- for example, groups of observations, or repeated measurements. In such settings, standard notions of exchangeability may not hold. To address this challenge, a hierarchical form of exchangeability is derived, facilitating extensions of distribution-free methods, including conformal prediction and jackknife+. While the standard theoretical guarantee obtained by the conformal prediction framework is a marginal predictive coverage guarantee, in the special case of independent repeated measurements, it is possible to achieve a stronger form of coverage -- the "second-moment coverage" property -- to provide better control of conditional miscoverage rates, and distribution-free prediction sets that achieve this property are constructed. Simulations illustrate that this guarantee indeed leads to uniformly small conditional miscoverage rates. Empirically, this stronger guarantee comes at the cost of a larger width of the prediction set in scenarios where the fitted model is poorly calibrated, but this cost is very mild in cases where the fitted model is accurate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06342v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghoon Lee, Rina Foygel Barber, Rebecca Willett</dc:creator>
    </item>
    <item>
      <title>Post-hoc and Anytime Valid Permutation and Group Invariance Testing</title>
      <link>https://arxiv.org/abs/2310.01153</link>
      <description>arXiv:2310.01153v3 Announce Type: replace-cross 
Abstract: We study post-hoc ($e$-value-based) and post-hoc anytime valid inference for testing exchangeability and general group invariance. Our methods satisfy a generalized Type I error control that permits a data-dependent selection of both the number of observations $n$ and the significance level $\alpha$. We derive a simple analytical expression for all exact post-hoc valid $p$-values for group invariance, which allows for a flexible plug-in of the test statistic. For post-hoc anytime validity, we derive sequential $p$-processes by multiplying post-hoc $p$-values. In sequential testing, it is key to specify how the number of observations may depend on the data. We propose two approaches, and show how they nest existing efforts. To construct good post-hoc $p$-values, we develop the theory of likelihood ratios for group invariance, and generalize existing optimality results. These likelihood ratios turn out to exist in different flavors depending on which space we specify our alternative. We illustrate our methods by testing against a Gaussian location shift, which yields an improved optimality result for the $t$-test when testing sphericity, connections to the softmax function when testing exchangeability, and an improved method for testing sign-symmetry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01153v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick W. Koning</dc:creator>
    </item>
    <item>
      <title>Semiparametric Efficient Inference in Adaptive Experiments</title>
      <link>https://arxiv.org/abs/2311.18274</link>
      <description>arXiv:2311.18274v3 Announce Type: replace-cross 
Abstract: We consider the problem of efficient inference of the Average Treatment Effect in a sequential experiment where the policy governing the assignment of subjects to treatment or control can change over time. We first provide a central limit theorem for the Adaptive Augmented Inverse-Probability Weighted estimator, which is semiparametric efficient, under weaker assumptions than those previously made in the literature. This central limit theorem enables efficient inference at fixed sample sizes. We then consider a sequential inference setting, deriving both asymptotic and nonasymptotic confidence sequences that are considerably tighter than previous methods. These anytime-valid methods enable inference under data-dependent stopping times (sample sizes). Additionally, we use propensity score truncation techniques from the recent off-policy estimation literature to reduce the finite sample variance of our estimator without affecting the asymptotic variance. Empirical results demonstrate that our methods yield narrower confidence sequences than those previously developed in the literature while maintaining time-uniform error control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18274v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Cook, Alan Mishler, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Structure-agnostic Optimality of Doubly Robust Learning for Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2402.14264</link>
      <description>arXiv:2402.14264v2 Announce Type: replace-cross 
Abstract: Average treatment effect estimation is the most central problem in causal inference with application to numerous disciplines. While many estimation strategies have been proposed in the literature, the statistical optimality of these methods has still remained an open area of investigation, especially in regimes where these methods do not achieve parametric rates. In this paper, we adopt the recently introduced structure-agnostic framework of statistical lower bounds, which poses no structural properties on the nuisance functions other than access to black-box estimators that achieve some statistical estimation rate. This framework is particularly appealing when one is only willing to consider estimation strategies that use non-parametric regression and classification oracles as black-box sub-processes. Within this framework, we prove the statistical optimality of the celebrated and widely used doubly robust estimators for both the Average Treatment Effect (ATE) and the Average Treatment Effect on the Treated (ATT), as well as weighted variants of the former, which arise in policy evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14264v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jikai Jin, Vasilis Syrgkanis</dc:creator>
    </item>
  </channel>
</rss>
