<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.EC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.EC</link>
    <description>q-fin.EC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.EC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Nov 2024 05:00:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Can an increase in productivity cause a decrease in production? Insights from a model economy with AI automation</title>
      <link>https://arxiv.org/abs/2411.15718</link>
      <description>arXiv:2411.15718v1 Announce Type: new 
Abstract: It is widely assumed that increases in economic productivity necessarily lead to economic growth. In this paper, it is shown that this is not always the case. An idealized model of an economy is presented in which a new technology allows capital to be utilized autonomously without labor input. This is motivated by the possibility that advances in artificial intelligence (AI) will give rise to AI agents that act autonomously in the economy. The economic model involves a single profit-maximizing firm which is a monopolist in the product market and a monopsonist in the labor market. The new automation technology causes the firm to replace labor with capital in such a way that its profit increases while total production decreases. The model is not intended to capture the structure of a real economy, but rather to illustrate how basic economic mechanisms can give rise to counterintuitive and undesirable outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15718v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Casey O. Barkan</dc:creator>
    </item>
    <item>
      <title>Inter-firm Heterogeneity in Production</title>
      <link>https://arxiv.org/abs/2411.15980</link>
      <description>arXiv:2411.15980v1 Announce Type: new 
Abstract: This paper studies inter-firm heterogeneity in production. Unlike much of the existing research, which primarily addresses heterogeneous production through unobserved fixed effects, our approach also focuses on differences in factors' output elasticities. Using manufacturing data from Chile, Colombia, and Japan, we apply an innovative Empirical Bayes methodology to estimate heterogeneous Cobb-Douglas production functions. We uncover substantial heterogeneity in both factor neutral productivity and factor elasticities, with a strong negative correlation between them. These findings are consistently observed across datasets and remain robust when using CES and intensive Cobb-Douglas specifications. We show that accounting for these features has significant implications for issues such as markup estimation, firms' technology adoption, and productivity measurement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15980v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michele Battisti, Valentino Dardanoni, Stefano Demichelis</dc:creator>
    </item>
    <item>
      <title>FinML-Chain: A Blockchain-Integrated Dataset for Enhanced Financial Machine Learning</title>
      <link>https://arxiv.org/abs/2411.16277</link>
      <description>arXiv:2411.16277v1 Announce Type: new 
Abstract: Machine learning is critical for innovation and efficiency in financial markets, offering predictive models and data-driven decision-making. However, challenges such as missing data, lack of transparency, untimely updates, insecurity, and incompatible data sources limit its effectiveness. Blockchain technology, with its transparency, immutability, and real-time updates, addresses these challenges. We present a framework for integrating high-frequency on-chain data with low-frequency off-chain data, providing a benchmark for addressing novel research questions in economic mechanism design. This framework generates modular, extensible datasets for analyzing economic mechanisms such as the Transaction Fee Mechanism, enabling multi-modal insights and fairness-driven evaluations. Using four machine learning techniques, including linear regression, deep neural networks, XGBoost, and LSTM models, we demonstrate the framework's ability to produce datasets that advance financial research and improve understanding of blockchain-driven systems. Our contributions include: (1) proposing a research scenario for the Transaction Fee Mechanism and demonstrating how the framework addresses previously unexplored questions in economic mechanism design; (2) providing a benchmark for financial machine learning by open-sourcing a sample dataset generated by the framework and the code for the pipeline, enabling continuous dataset expansion; and (3) promoting reproducibility, transparency, and collaboration by fully open-sourcing the framework and its outputs. This initiative supports researchers in extending our work and developing innovative financial machine-learning models, fostering advancements at the intersection of machine learning, blockchain, and economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16277v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingfeng Chen, Wanlin Deng, Dangxing Chen, Luyao Zhang</dc:creator>
    </item>
    <item>
      <title>Naive Algorithmic Collusion: When Do Bandit Learners Cooperate and When Do They Compete?</title>
      <link>https://arxiv.org/abs/2411.16574</link>
      <description>arXiv:2411.16574v1 Announce Type: new 
Abstract: Algorithmic agents are used in a variety of competitive decision settings, notably in making pricing decisions in contexts that range from online retail to residential home rentals. Business managers, algorithm designers, legal scholars, and regulators alike are all starting to consider the ramifications of "algorithmic collusion." We study the emergent behavior of multi-armed bandit machine learning algorithms used in situations where agents are competing, but they have no information about the strategic interaction they are engaged in. Using a general-form repeated Prisoner's Dilemma game, agents engage in online learning with no prior model of game structure and no knowledge of competitors' states or actions (e.g., no observation of competing prices). We show that these context-free bandits, with no knowledge of opponents' choices or outcomes, still will consistently learn collusive behavior - what we call "naive collusion." We primarily study this system through an analytical model and examine perturbations to the model through simulations.
  Our findings have several notable implications for regulators. First, calls to limit algorithms from conditioning on competitors' prices are insufficient to prevent algorithmic collusion. This is a direct result of collusion arising even in the naive setting. Second, symmetry in algorithms can increase collusion potential. This highlights a new, simple mechanism for "hub-and-spoke" algorithmic collusion. A central distributor need not imbue its algorithm with supra-competitive tendencies for apparent collusion to arise; it can simply arise by using certain (common) machine learning algorithms. Finally, we highlight that collusive outcomes depend starkly on the specific algorithm being used, and we highlight market and algorithmic conditions under which it will be unknown a priori whether collusion occurs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16574v1</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Connor Douglas, Foster Provost, Arun Sundararajan</dc:creator>
    </item>
    <item>
      <title>Belief Bias Identification</title>
      <link>https://arxiv.org/abs/2404.09297</link>
      <description>arXiv:2404.09297v2 Announce Type: replace 
Abstract: This paper proposes a unified theoretical model to identify and test a comprehensive set of probabilistic updating biases within a single framework. The model achieves separate identification by focusing on the updating of belief distributions, rather than classic point-belief measurements. Testing the model in a laboratory experiment reveals significant heterogeneity at the individual level: All tested biases are present, and each participant exhibits at least one identifiable bias. Notably, motivated-belief biases (optimism and pessimism) and sequence-related biases (gambler's fallacy and hot hand fallacy) are identified as key drivers of biased inference. Moreover, at the population level, base rate neglect emerges as a persistent influence. This study contributes to the belief-updating literature by providing a methodological toolkit for researchers examining links between different conflicting biases, or exploring connections between updating biases and other behavioral phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09297v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro Gonzalez-Fernandez</dc:creator>
    </item>
    <item>
      <title>Single Transferable Vote and Paradoxes of Negative and Positive Involvement</title>
      <link>https://arxiv.org/abs/2406.20045</link>
      <description>arXiv:2406.20045v2 Announce Type: replace 
Abstract: We analyze a type of voting paradox which we term an involvement paradox, in which a candidate who loses an election could be made into a winner if more of the candidate's non-supporters participated in the election, or a winner could be made into a loser if more of the candidate's supporters participated. Such paradoxical outcomes are possible under the voting method of single transferable vote (STV), which is widely used for political elections throughout the world. We provide a worst-case analysis of involvement paradoxes under STV and show several interesting examples of these paradoxes from elections in Scotland.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.20045v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David McCune</dc:creator>
    </item>
    <item>
      <title>Estimating the Cost of Informal Care with a Novel Two-Stage Approach to Individual Synthetic Control</title>
      <link>https://arxiv.org/abs/2411.10314</link>
      <description>arXiv:2411.10314v2 Announce Type: replace 
Abstract: Informal carers provide the majority of care for people living with challenges related to older age, long-term illness, or disability. However, the care they provide often results in a significant income penalty for carers, a factor largely overlooked in the economics literature and policy discourse. Leveraging data from the UK Household Longitudinal Study, this paper provides the first robust causal estimates of the caring income penalty using a novel individual synthetic control based method that accounts for unit-level heterogeneity in post-treatment trajectories over time. Our baseline estimates identify an average relative income gap of up to 45%, with an average decrease of {\pounds}162 in monthly income, peaking at {\pounds}192 per month after 4 years, based on the difference between informal carers providing the highest-intensity of care and their synthetic counterparts. We find that the income penalty is more pronounced for women than for men, and varies by ethnicity and age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10314v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Petrillo, Daniel Valdenegro, Charles Rahal, Yanan Zhang, Gwilym Pryce, Matthew R. Bennett</dc:creator>
    </item>
    <item>
      <title>The Role of Accuracy and Validation Effectiveness in Conversational Business Analytics</title>
      <link>https://arxiv.org/abs/2411.12128</link>
      <description>arXiv:2411.12128v3 Announce Type: replace-cross 
Abstract: This study examines conversational business analytics, an approach that utilizes AI to address the technical competency gaps that hinder end users from effectively using traditional self-service analytics. By facilitating natural language interactions, conversational business analytics aims to empower end users to independently retrieve data and generate insights. The analysis focuses on Text-to-SQL as a representative technology for translating natural language requests into SQL statements. Developing theoretical models grounded in expected utility theory, this study identifies the conditions under which conversational business analytics, through partial or full support, can outperform delegation to human experts. The results indicate that partial support, focusing solely on information generation by AI, is viable when the accuracy of AI-generated SQL queries leads to a profit that surpasses the performance of a human expert. In contrast, full support includes not only information generation but also validation through explanations provided by the AI, and requires sufficiently high validation effectiveness to be reliable. However, user-based validation presents challenges, such as misjudgment and rejection of valid SQL queries, which may limit the effectiveness of conversational business analytics. These challenges underscore the need for robust validation mechanisms, including improved user support, automated processes, and methods for assessing quality independent of the technical competency of end users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12128v3</guid>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Adem Alparslan</dc:creator>
    </item>
  </channel>
</rss>
