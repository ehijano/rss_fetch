<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Nov 2024 02:41:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Asymptotics in Multiple Hypotheses Testing under Dependence: beyond Normality</title>
      <link>https://arxiv.org/abs/2411.12119</link>
      <description>arXiv:2411.12119v1 Announce Type: new 
Abstract: Correlated observations are ubiquitous phenomena in a plethora of scientific avenues. Tackling this dependence among test statistics has been one of the pertinent problems in simultaneous inference. However, very little literature exists that elucidates the effect of correlation on different testing procedures under general distributional assumptions. In this work, we address this gap in a unified way by considering the multiple testing problem under a general correlated framework. We establish an upper bound on the family-wise error rate(FWER) of Bonferroni's procedure for equicorrelated test statistics. Consequently, we find that for a quite general class of distributions, Bonferroni FWER asymptotically tends to zero when the number of hypotheses approaches infinity. We extend this result to general positively correlated elliptically contoured setups. We also present examples of distributions for which Bonferroni FWER has a strictly positive limit under equicorrelation. We extend the limiting zero results to the class of step-down procedures under quite general correlated setups. Specifically, the probability of rejecting at least one hypothesis approaches zero asymptotically for any step-down procedure. The results obtained in this work generalize existing results for correlated Normal test statistics and facilitate new insights into the performances of multiple testing procedures under dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12119v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Monitirtha Dey</dc:creator>
    </item>
    <item>
      <title>Different PCA approaches for vector functional time series with applications to resistive switching processes</title>
      <link>https://arxiv.org/abs/2411.12366</link>
      <description>arXiv:2411.12366v1 Announce Type: new 
Abstract: This paper is motivated by modeling the cycle-to-cycle variability associated with the resistive switching operation behind memristors. As the data are by nature curves, functional principal component analysis is a suitable candidate to explain the main modes of variability. Taking into account this data-driven motivation, in this paper we propose two new forecasting approaches based on studying the sequential cross-dependence between and within a multivariate functional time series in terms of vector autoregressive modeling of the most explicative functional principal component scores. The main difference between the two methods lies in whether a univariate or multivariate PCA is performed so that we have a different set of principal component scores for each functional time series or the same one for all of them. Finally, the sample performance of the proposed methodologies is illustrated by an application on a bivariate functional time series of reset-set curves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12366v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.matcom.2024.04.017</arxiv:DOI>
      <arxiv:journal_reference>Mathematics and Computers in Simulation, Volume 223, 2024, 288-298</arxiv:journal_reference>
      <dc:creator>C. Acal, A. M. Aguilera, F. J. Alonso, J. E. Ruiz-Castro, J. B. Rold\'an</dc:creator>
    </item>
    <item>
      <title>Towards a theory for testing statistical hypothesis: Multivariate mean with nuisance covariance matrix</title>
      <link>https://arxiv.org/abs/2411.12532</link>
      <description>arXiv:2411.12532v1 Announce Type: new 
Abstract: Under a multinormal distribution with an arbitrary unknown covariance matrix, the main purpose of this paper is to propose a framework to achieve the goal of reconciliation of Bayesian, frequentist, and Fisher's reporting $p$-values, Neyman-Pearson's optimal theory and Wald's decision theory for the problems of testing mean against restricted alternatives (closed convex cones). To proceed, the tests constructed via the likelihood ratio (LR) and the union-intersection (UI) principles are studied. For the problems of testing against restricted alternatives, first, we show that the LRT and the UIT are not the proper Bayes tests, however, they are shown to be the integrated LRT and the integrated UIT, respectively. For the problem of testing against the positive orthant space alternative, both the null distributions of the LRT and the UIT depend on the unknown nuisance covariance matrix. Hence we have difficulty adopting Fisher's approach to reporting $p$-values. On the other hand, according to the definition of the level of significance, both the LRT and the UIT are shown to be power-dominated by the corresponding LRT and UIT for testing against the half-space alternative, respectively. Hence, both the LRT and the UIT are $\alpha$-inadmissible, these results are against the common statistical sense. Neither Fisher's approach of reporting $p$-values alone nor Neyman-Pearson's optimal theory for power function alone is a satisfactory criterion for evaluating the performance of tests. Wald's decision theory via $d$-admissibility may shed light on resolving these challenging issues of imposing the balance between type 1 error and power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12532v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming-Tien Tsai</dc:creator>
    </item>
    <item>
      <title>Testing parametric models for the angular measure for bivariate extremes</title>
      <link>https://arxiv.org/abs/2411.12673</link>
      <description>arXiv:2411.12673v1 Announce Type: new 
Abstract: The angular measure on the unit sphere characterizes the first-order dependence structure of the components of a random vector in extreme regions and is defined in terms of standardized margins. Its statistical recovery is an important step in learning problems involving observations far away from the center. In this paper, we test the goodness-of-fit of a given parametric model to the extremal dependence structure of a bivariate random sample. The proposed test statistic consists of a weighted $L_1$-Wasserstein distance between a nonparametric, rank-based estimator of the true angular measure obtained by maximizing a Euclidean likelihood on the one hand, and a parametric estimator of the angular measure on the other hand. The asymptotic distribution of the test statistic under the null hypothesis is derived and is used to obtain critical values for the proposed testing procedure via a parametric bootstrap. Consistency of the bootstrap algorithm is proved. A simulation study illustrates the finite-sample performance of the test for the logistic and H\"usler--Reiss models. We apply the method to test for the H\"usler--Reiss model in the context of river discharge data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12673v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>St\'ephane Lhaut, Johan Segers</dc:creator>
    </item>
    <item>
      <title>On the Efficiency of ERM in Feature Learning</title>
      <link>https://arxiv.org/abs/2411.12029</link>
      <description>arXiv:2411.12029v1 Announce Type: cross 
Abstract: Given a collection of feature maps indexed by a set $\mathcal{T}$, we study the performance of empirical risk minimization (ERM) on regression problems with square loss over the union of the linear classes induced by these feature maps. This setup aims at capturing the simplest instance of feature learning, where the model is expected to jointly learn from the data an appropriate feature map and a linear predictor. We start by studying the asymptotic quantiles of the excess risk of sequences of empirical risk minimizers. Remarkably, we show that when the set $\mathcal{T}$ is not too large and when there is a unique optimal feature map, these quantiles coincide, up to a factor of two, with those of the excess risk of the oracle procedure, which knows a priori this optimal feature map and deterministically outputs an empirical risk minimizer from the associated optimal linear class. We complement this asymptotic result with a non-asymptotic analysis that quantifies the decaying effect of the global complexity of the set $\mathcal{T}$ on the excess risk of ERM, and relates it to the size of the sublevel sets of the suboptimality of the feature maps. As an application of our results, we obtain new guarantees on the performance of the best subset selection procedure in sparse linear regression under general assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12029v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayoub El Hanchi, Chris J. Maddison, Murat A. Erdogdu</dc:creator>
    </item>
    <item>
      <title>The Statistical Accuracy of Neural Posterior and Likelihood Estimation</title>
      <link>https://arxiv.org/abs/2411.12068</link>
      <description>arXiv:2411.12068v1 Announce Type: cross 
Abstract: Neural posterior estimation (NPE) and neural likelihood estimation (NLE) are machine learning approaches that provide accurate posterior, and likelihood, approximations in complex modeling scenarios, and in situations where conducting amortized inference is a necessity. While such methods have shown significant promise across a range of diverse scientific applications, the statistical accuracy of these methods is so far unexplored. In this manuscript, we give, for the first time, an in-depth exploration on the statistical behavior of NPE and NLE. We prove that these methods have similar theoretical guarantees to common statistical methods like approximate Bayesian computation (ABC) and Bayesian synthetic likelihood (BSL). While NPE and NLE methods are just as accurate as ABC and BSL, we prove that this accuracy can often be achieved at a vastly reduced computational cost, and will therefore deliver more attractive approximations than ABC and BSL in certain problems. We verify our results theoretically and in several examples from the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12068v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David T. Frazier, Ryan Kelly, Christopher Drovandi, David J. Warne</dc:creator>
    </item>
    <item>
      <title>Fine-Grained Uncertainty Quantification via Collisions</title>
      <link>https://arxiv.org/abs/2411.12127</link>
      <description>arXiv:2411.12127v1 Announce Type: cross 
Abstract: We propose a new approach for fine-grained uncertainty quantification (UQ) using a collision matrix. For a classification problem involving $K$ classes, the $K\times K$ collision matrix $S$ measures the inherent (aleatoric) difficulty in distinguishing between each pair of classes. In contrast to existing UQ methods, the collision matrix gives a much more detailed picture of the difficulty of classification. We discuss several possible downstream applications of the collision matrix, establish its fundamental mathematical properties, as well as show its relationship with existing UQ methods, including the Bayes error rate. We also address the new problem of estimating the collision matrix using one-hot labeled data. We propose a series of innovative techniques to estimate $S$. First, we learn a contrastive binary classifier which takes two inputs and determines if they belong to the same class. We then show that this contrastive classifier (which is PAC learnable) can be used to reliably estimate the Gramian matrix of $S$, defined as $G=S^TS$. Finally, we show that under very mild assumptions, $G$ can be used to uniquely recover $S$, a new result on stochastic matrices which could be of independent interest. Experimental results are also presented to validate our methods on several datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12127v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse Friedbaum, Sudarshan Adiga, Ravi Tandon</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Time-Sparsity Trade-Offs for Solving Noisy Linear Equations</title>
      <link>https://arxiv.org/abs/2411.12512</link>
      <description>arXiv:2411.12512v1 Announce Type: cross 
Abstract: We present a polynomial-time reduction from solving noisy linear equations over $\mathbb{Z}/q\mathbb{Z}$ in dimension $\Theta(k\log n/\mathsf{poly}(\log k,\log q,\log\log n))$ with a uniformly random coefficient matrix to noisy linear equations over $\mathbb{Z}/q\mathbb{Z}$ in dimension $n$ where each row of the coefficient matrix has uniformly random support of size $k$. This allows us to deduce the hardness of sparse problems from their dense counterparts. In particular, we derive hardness results in the following canonical settings. 1) Assuming the $\ell$-dimensional (dense) LWE over a polynomial-size field takes time $2^{\Omega(\ell)}$, $k$-sparse LWE in dimension $n$ takes time $n^{\Omega({k}/{(\log k \cdot (\log k + \log \log n))})}.$ 2) Assuming the $\ell$-dimensional (dense) LPN over $\mathbb{F}_2$ takes time $2^{\Omega(\ell/\log \ell)}$, $k$-sparse LPN in dimension $n$ takes time $n^{\Omega(k/(\log k \cdot (\log k + \log \log n)^2))}~.$ These running time lower bounds are nearly tight as both sparse problems can be solved in time $n^{O(k)},$ given sufficiently many samples. We further give a reduction from $k$-sparse LWE to noisy tensor completion. Concretely, composing the two reductions implies that order-$k$ rank-$2^{k-1}$ noisy tensor completion in $\mathbb{R}^{n^{\otimes k}}$ takes time $n^{\Omega(k/ \log k \cdot (\log k + \log \log n))}$, assuming the exponential hardness of standard worst-case lattice problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12512v1</guid>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <category>cs.DM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kiril Bangachev, Guy Bresler, Stefan Tiegel, Vinod Vaikuntanathan</dc:creator>
    </item>
    <item>
      <title>Robust Inference for High-dimensional Linear Models with Heavy-tailed Errors via Partial Gini Covariance</title>
      <link>https://arxiv.org/abs/2411.12578</link>
      <description>arXiv:2411.12578v2 Announce Type: cross 
Abstract: This paper introduces the partial Gini covariance, a novel dependence measure that addresses the challenges of high-dimensional inference with heavy-tailed errors, often encountered in fields like finance, insurance, climate, and biology. Conventional high-dimensional regression inference methods suffer from inaccurate type I errors and reduced power in heavy-tailed contexts, limiting their effectiveness. Our proposed approach leverages the partial Gini covariance to construct a robust statistical inference framework that requires minimal tuning and does not impose restrictive moment conditions on error distributions. Unlike traditional methods, it circumvents the need for estimating the density of random errors and enhances the computational feasibility and robustness. Extensive simulations demonstrate the proposed method's superior power and robustness over standard high-dimensional inference approaches, such as those based on the debiased Lasso. The asymptotic relative efficiency analysis provides additional theoretical insight on the improved efficiency of the new approach in the heavy-tailed setting. Additionally, the partial Gini covariance extends to the multivariate setting, enabling chi-square testing for a group of coefficients. We illustrate the method's practical application with a real-world data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12578v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yilin Zhang, Songshan Yang, Yunan Wu, Lan Wang</dc:creator>
    </item>
    <item>
      <title>Random signed measures</title>
      <link>https://arxiv.org/abs/2411.12623</link>
      <description>arXiv:2411.12623v1 Announce Type: cross 
Abstract: Point processes and, more generally, random measures are ubiquitous in modern statistics. However, they can only take positive values, which is a severe limitation in many situations. In this work, we introduce and study random signed measures, also known as real-valued random measures, and apply them to constrcut various Bayesian non-parametric models. In particular, we provide an existence result for random signed measures, allowing us to obtain a canonical definition for them and solve a 70-year-old open problem. Further, we provide a representation of completely random signed measures (CRSMs), which extends the celebrated Kingman's representation for completely random measures (CRMs) to the real-valued case. We then introduce specific classes of random signed measures, including the Skellam point process, which plays the role of the Poisson point process in the real-valued case, and the Gaussian random measure. We use the theoretical results to develop two Bayesian nonparametric models -- one for topic modeling and the other for random graphs -- and to investigate mean function estimation in Bayesian nonparametric regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12623v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Riccardo Passeggeri</dc:creator>
    </item>
    <item>
      <title>Multiple Testing under High-dimensional Dynamic Factor Model</title>
      <link>https://arxiv.org/abs/2303.07631</link>
      <description>arXiv:2303.07631v2 Announce Type: replace 
Abstract: Large-scale multiple testing under static factor models is commonly used to select skilled funds in financial market. However, static factor models are arguably too stringent as it ignores the serial correlation, which severely distorts error rate control in large-scale inference. In this manuscript, we propose a new multiple testing procedure under dynamic factor models that is robust to the nonlinear serial dependence (e.g., GARCH). The idea is to integrate a new sample-splitting strategy based on chronological order and a two-pass Fama-Macbeth regression to form a series of statistics with marginal symmetry properties and then to utilize the symmetry properties to obtain a data-driven threshold. We show that our procedure is able to control the false discovery rate (FDR) asymptotically under high-dimensional dynamic factor models. As a byproduct that is of independent interest, we establish a new exponential-type deviation inequality for the sum of random variables on a variety of functionals of linear and non-linear processes. Numerical results including a case study on hedge fund selection demonstrate the advantage of the proposed method over several state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07631v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinxin Yang, Lilun Du</dc:creator>
    </item>
    <item>
      <title>Aggregating Dependent Signals with Heavy-Tailed Combination Tests</title>
      <link>https://arxiv.org/abs/2310.20460</link>
      <description>arXiv:2310.20460v2 Announce Type: replace-cross 
Abstract: Combining dependent p-values to evaluate the global null hypothesis presents a longstanding challenge in statistical inference, particularly when aggregating results from diverse methods to boost signal detection. P-value combination tests using heavy-tailed distribution based transformations, such as the Cauchy combination test and the harmonic mean p-value, have recently garnered significant interest for their potential to efficiently handle arbitrary p-value dependencies. Despite their growing popularity in practical applications, there is a gap in comprehensive theoretical and empirical evaluations of these methods. This paper conducts an extensive investigation, revealing that, theoretically, while these combination tests are asymptotically valid for pairwise quasi-asymptotically independent test statistics, such as bivariate normal variables, they are also asymptotically equivalent to the Bonferroni test under the same conditions. However, extensive simulations unveil their practical utility, especially in scenarios where stringent type-I error control is not necessary and signals are dense. Both the heaviness of the distribution and its support substantially impact the tests' non-asymptotic validity and power, and we recommend using a truncated Cauchy distribution in practice. Moreover, we show that under the violation of quasi-asymptotic independence among test statistics, these tests remain valid and, in fact, can be considerably less conservative than the Bonferroni test. We also present two case studies in genetics and genomics, showcasing the potential of the combination tests to significantly enhance statistical power while effectively controlling type-I errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.20460v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Gui, Yuchao Jiang, Jingshu Wang</dc:creator>
    </item>
    <item>
      <title>Robust Learning for Optimal Dynamic Treatment Regimes with Observational Data</title>
      <link>https://arxiv.org/abs/2404.00221</link>
      <description>arXiv:2404.00221v4 Announce Type: replace-cross 
Abstract: Public policies and medical interventions often involve dynamics in their treatment assignments, where individuals receive a series of interventions over multiple stages. We study the statistical learning of optimal dynamic treatment regimes (DTRs) that guide the optimal treatment assignment for each individual at each stage based on the individual's evolving history. We propose a doubly robust, classification-based approach to learning the optimal DTR using observational data under the assumption of sequential ignorability. This approach learns the optimal DTR through backward induction. At each step, it constructs an augmented inverse probability weighting (AIPW) estimator of the policy value function and maximizes it to learn the optimal policy for the corresponding stage. We show that the resulting DTR can achieve an optimal convergence rate of $n^{-1/2}$ for welfare regret under mild convergence conditions on estimators of the nuisance components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00221v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shosei Sakaguchi</dc:creator>
    </item>
    <item>
      <title>Robust Estimation of Polychoric Correlation</title>
      <link>https://arxiv.org/abs/2407.18835</link>
      <description>arXiv:2407.18835v3 Announce Type: replace-cross 
Abstract: Polychoric correlation is often an important building block in the analysis of rating data, particularly for structural equation models. However, the commonly employed maximum likelihood (ML) estimator is highly susceptible to misspecification of the polychoric correlation model, for instance through violations of latent normality assumptions. We propose a novel estimator that is designed to be robust to partial misspecification of the polychoric model, that is, the model is only misspecified for an unknown fraction of observations, for instance (but not limited to) careless respondents. In contrast to existing literature, our estimator makes no assumption on the type or degree of model misspecification. It furthermore generalizes ML estimation, is consistent as well as asymptotically normally distributed, and comes at no additional computational cost. We demonstrate the robustness and practical usefulness of our estimator in simulation studies and an empirical application on a Big Five administration. In the latter, the polychoric correlation estimates of our estimator and ML differ substantially, which, after further inspection, is likely due to the presence of careless respondents that the estimator helps identify.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18835v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Welz, Patrick Mair, Andreas Alfons</dc:creator>
    </item>
    <item>
      <title>Estimating quantum Markov chains using coherent absorber post-processing and pattern counting estimator</title>
      <link>https://arxiv.org/abs/2408.00626</link>
      <description>arXiv:2408.00626v2 Announce Type: replace-cross 
Abstract: We propose a two step strategy for estimating one-dimensional dynamical parameters of a quantum Markov chain, which involves quantum post-processing the output using a coherent quantum absorber and a "pattern counting'' estimator computed as a simple additive functional of the outcomes trajectory produced by sequential, identical measurements on the output units. We provide strong theoretical and numerical evidence that the estimator achieves the quantum Cram\'{e}-Rao bound in the limit of large output size. Our estimation method is underpinned by an asymptotic theory of translationally invariant modes (TIMs) built as averages of shifted tensor products of output operators, labelled by binary patterns. For large times, the TIMs form a bosonic algebra and the output state approaches a joint coherent state of the TIMs whose amplitude depends linearly on the mismatch between system and absorber parameters. Moreover, in the asymptotic regime the TIMs capture the full quantum Fisher information of the output state. While directly probing the TIMs' quadratures seems impractical, we show that the standard sequential measurement is an effective joint measurement of all the TIMs number operators; indeed, we show that counts of different binary patterns extracted from the measurement trajectory have the expected joint Poisson distribution. Together with the displaced-null methodology of J. Phys. A: Math. Theor. 57 245304 2024 this provides a computationally efficient estimator which only depends on the total number of patterns. This opens the way for similar estimation strategies in continuous-time dynamics, expanding the results of Phys. Rev. X 13, 031012 2023.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00626v2</guid>
      <category>quant-ph</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federico Girotti, Alfred Godley, M\u{a}d\u{a}lin Gu\c{t}\u{a}</dc:creator>
    </item>
    <item>
      <title>Dense ReLU Neural Networks for Temporal-spatial Model</title>
      <link>https://arxiv.org/abs/2411.09961</link>
      <description>arXiv:2411.09961v2 Announce Type: replace-cross 
Abstract: In this paper, we focus on fully connected deep neural networks utilizing the Rectified Linear Unit (ReLU) activation function for nonparametric estimation. We derive non-asymptotic bounds that lead to convergence rates, addressing both temporal and spatial dependence in the observed measurements. By accounting for dependencies across time and space, our models better reflect the complexities of real-world data, enhancing both predictive performance and theoretical robustness. We also tackle the curse of dimensionality by modeling the data on a manifold, exploring the intrinsic dimensionality of high-dimensional data. We broaden existing theoretical findings of temporal-spatial analysis by applying them to neural networks in more general contexts and demonstrate that our proof techniques are effective for models with short-range dependence. Our empirical simulations across various synthetic response functions underscore the superior performance of our method, outperforming established approaches in the existing literature. These findings provide valuable insights into the strong capabilities of dense neural networks for temporal-spatial modeling across a broad range of function classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09961v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhi Zhang, Carlos Misael Madrid Padilla, Xiaokai Luo, Oscar Hernan Madrid Padilla, Daren Wang</dc:creator>
    </item>
    <item>
      <title>Variational Bayesian Bow tie Neural Networks with Shrinkage</title>
      <link>https://arxiv.org/abs/2411.11132</link>
      <description>arXiv:2411.11132v2 Announce Type: replace-cross 
Abstract: Despite the dominant role of deep models in machine learning, limitations persist, including overconfident predictions, susceptibility to adversarial attacks, and underestimation of variability in predictions. The Bayesian paradigm provides a natural framework to overcome such issues and has become the gold standard for uncertainty estimation with deep models, also providing improved accuracy and a framework for tuning critical hyperparameters. However, exact Bayesian inference is challenging, typically involving variational algorithms that impose strong independence and distributional assumptions. Moreover, existing methods are sensitive to the architectural choice of the network. We address these issues by constructing a relaxed version of the standard feed-forward rectified neural network, and employing Polya-Gamma data augmentation tricks to render a conditionally linear and Gaussian model. Additionally, we use sparsity-promoting priors on the weights of the neural network for data-driven architectural design. To approximate the posterior, we derive a variational inference algorithm that avoids distributional assumptions and independence across layers and is a faster alternative to the usual Markov Chain Monte Carlo schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11132v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alisa Sheinkman, Sara Wade</dc:creator>
    </item>
    <item>
      <title>Debiased Regression for Root-N-Consistent Conditional Mean Estimation</title>
      <link>https://arxiv.org/abs/2411.11748</link>
      <description>arXiv:2411.11748v2 Announce Type: replace-cross 
Abstract: This study introduces a debiasing method for regression estimators, including high-dimensional and nonparametric regression estimators. For example, nonparametric regression methods allow for the estimation of regression functions in a data-driven manner with minimal assumptions; however, these methods typically fail to achieve $\sqrt{n}$-consistency in their convergence rates, and many, including those in machine learning, lack guarantees that their estimators asymptotically follow a normal distribution. To address these challenges, we propose a debiasing technique for nonparametric estimators by adding a bias-correction term to the original estimators, extending the conventional one-step estimator used in semiparametric analysis. Specifically, for each data point, we estimate the conditional expected residual of the original nonparametric estimator, which can, for instance, be computed using kernel (Nadaraya-Watson) regression, and incorporate it as a bias-reduction term. Our theoretical analysis demonstrates that the proposed estimator achieves $\sqrt{n}$-consistency and asymptotic normality under a mild convergence rate condition for both the original nonparametric estimator and the conditional expected residual estimator. Notably, this approach remains model-free as long as the original estimator and the conditional expected residual estimator satisfy the convergence rate condition. The proposed method offers several advantages, including improved estimation accuracy and simplified construction of confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11748v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
  </channel>
</rss>
