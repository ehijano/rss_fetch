<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 14 Apr 2025 04:00:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Do LLMs trust AI regulation? Emerging behaviour of game-theoretic LLM agents</title>
      <link>https://arxiv.org/abs/2504.08640</link>
      <description>arXiv:2504.08640v1 Announce Type: cross 
Abstract: There is general agreement that fostering trust and cooperation within the AI development ecosystem is essential to promote the adoption of trustworthy AI systems. By embedding Large Language Model (LLM) agents within an evolutionary game-theoretic framework, this paper investigates the complex interplay between AI developers, regulators and users, modelling their strategic choices under different regulatory scenarios. Evolutionary game theory (EGT) is used to quantitatively model the dilemmas faced by each actor, and LLMs provide additional degrees of complexity and nuances and enable repeated games and incorporation of personality traits. Our research identifies emerging behaviours of strategic AI agents, which tend to adopt more "pessimistic" (not trusting and defective) stances than pure game-theoretic agents. We observe that, in case of full trust by users, incentives are effective to promote effective regulation; however, conditional trust may deteriorate the "social pact". Establishing a virtuous feedback between users' trust and regulators' reputation thus appears to be key to nudge developers towards creating safe AI. However, the level at which this trust emerges may depend on the specific LLM used for testing. Our results thus provide guidance for AI regulation systems, and help predict the outcome of strategic LLM agents, should they be used to aid regulation itself.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08640v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <category>nlin.CD</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessio Buscemi, Daniele Proverbio, Paolo Bova, Nataliya Balabanova, Adeela Bashir, Theodor Cimpeanu, Henrique Correia da Fonseca, Manh Hong Duong, Elias Fernandez Domingos, Antonio M. Fernandes, Marcus Krellner, Ndidi Bianca Ogbo, Simon T. Powers, Fernando P. Santos, Zia Ush Shamszaman, Zhao Song, Alessandro Di Stefano, The Anh Han</dc:creator>
    </item>
    <item>
      <title>Polytime Algorithms for One-to-Many Matching Games</title>
      <link>https://arxiv.org/abs/2107.07440</link>
      <description>arXiv:2107.07440v3 Announce Type: replace 
Abstract: Matching games is a novel matching model introduced by Garrido-Lucero and Laraki, in which agents' utilities are endogenously determined as the outcome of a strategic game they play simultaneously with the matching process. Matching games encompass most one-to-one matching market models and reinforce the classical notion of pairwise stability by analyzing their robustness to unilateral deviations within games. In this article, we extend the model to the one-to-many setting, where hospitals can be matched to multiple doctors, and their utility is given by the sum of their game outcomes. We adapt the deferred acceptance with competitions algorithm and the renegotiation process to this new framework and prove that both are polynomial whenever couples play bi-matrix games in mixed strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.07440v3</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felipe Garrido-Lucero, Rida Laraki</dc:creator>
    </item>
    <item>
      <title>Incentivizing Forecasters to Learn: Summarized vs. Unrestricted Advice</title>
      <link>https://arxiv.org/abs/2310.19147</link>
      <description>arXiv:2310.19147v3 Announce Type: replace-cross 
Abstract: How should forecasters be incentivized to acquire the most information when learning takes place over time? We address this question in the context of a novel dynamic mechanism design problem where a designer can incentivize learning by conditioning a reward on an event's outcome and expert reports. Eliciting summarized advice at a terminal date maximizes information acquisition if an informative signal fully reveals the outcome or has predictable content. Otherwise, richer reporting capabilities may be required. Our findings shed light on incentive design for consultation and forecasting by illustrating how learning dynamics shape qualitative properties of effort-maximizing contracts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19147v3</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingkai Li, Jonathan Libgober</dc:creator>
    </item>
  </channel>
</rss>
