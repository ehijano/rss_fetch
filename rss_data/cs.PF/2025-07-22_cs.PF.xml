<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Jul 2025 04:05:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Towards a Proactive Autoscaling Framework for Data Stream Processing at the Edge using GRU and Transfer Learning</title>
      <link>https://arxiv.org/abs/2507.14597</link>
      <description>arXiv:2507.14597v1 Announce Type: cross 
Abstract: Processing data at high speeds is becoming increasingly critical as digital economies generate enormous data. The current paradigms for timely data processing are edge computing and data stream processing (DSP). Edge computing places resources closer to where data is generated, while stream processing analyzes the unbounded high-speed data in motion. However, edge stream processing faces rapid workload fluctuations, complicating resource provisioning. Inadequate resource allocation leads to bottlenecks, whereas excess allocation results in wastage. Existing reactive methods, such as threshold-based policies and queuing theory scale only after performance degrades, potentially violating SLAs. Although reinforcement learning (RL) offers a proactive approach through agents that learn optimal runtime adaptation policies, it requires extensive simulation. Furthermore, predictive machine learning models face online distribution and concept drift that minimize their accuracy. We propose a three-step solution to the proactive edge stream processing autoscaling problem. Firstly, a GRU neural network forecasts the upstream load using real-world and synthetic DSP datasets. Secondly, a transfer learning framework integrates the predictive model into an online stream processing system using the DTW algorithm and joint distribution adaptation to handle the disparities between offline and online domains. Finally, a horizontal autoscaling module dynamically adjusts the degree of operator parallelism, based on predicted load while considering edge resource constraints. The lightweight GRU model for load predictions recorded up to 1.3\% SMAPE value on a real-world data set. It outperformed CNN, ARIMA, and Prophet on the SMAPE and RMSE evaluation metrics, with lower training time than the computationally intensive RL models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14597v1</guid>
      <category>cs.DC</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eugene Armah, Linda Amoako Bannning</dc:creator>
    </item>
    <item>
      <title>Mayura: Exploiting Similarities in Motifs for Temporal Co-Mining</title>
      <link>https://arxiv.org/abs/2507.14813</link>
      <description>arXiv:2507.14813v1 Announce Type: cross 
Abstract: Temporal graphs serve as a critical foundation for modeling evolving interactions in domains ranging from financial networks to social media. Mining temporal motifs is essential for applications such as fraud detection, cybersecurity, and dynamic network analysis. However, conventional motif mining approaches treat each query independently, incurring significant redundant computations when similar substructures exist across multiple motifs. In this paper, we propose Mayura, a novel framework that unifies the mining of multiple temporal motifs by exploiting their inherent structural and temporal commonalities. Central to our approach is the Motif-Group Tree (MG-Tree), a hierarchical data structure that organizes related motifs and enables the reuse of common search paths, thereby reducing redundant computation. We propose a co-mining algorithm that leverages the MG-Tree and develop a flexible runtime capable of exploiting both CPU and GPU architectures for scalable performance. Empirical evaluations on diverse real-world datasets demonstrate that Mayura achieves substantial improvements over the state-of-the-art techniques that mine each motif individually, with an average speed-up of 2.4x on the CPU and 1.7x on the GPU, while maintaining the exactness required for high-stakes applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14813v1</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanjay Sri Vallabh Singapuram, Ronald Dreslinski, Nishil Talati</dc:creator>
    </item>
    <item>
      <title>Polymorph: Energy-Efficient Multi-Label Classification for Video Streams on Embedded Devices</title>
      <link>https://arxiv.org/abs/2507.14959</link>
      <description>arXiv:2507.14959v1 Announce Type: cross 
Abstract: Real-time multi-label video classification on embedded devices is constrained by limited compute and energy budgets. Yet, video streams exhibit structural properties such as label sparsity, temporal continuity, and label co-occurrence that can be leveraged for more efficient inference. We introduce Polymorph, a context-aware framework that activates a minimal set of lightweight Low Rank Adapters (LoRA) per frame. Each adapter specializes in a subset of classes derived from co-occurrence patterns and is implemented as a LoRA weight over a shared backbone. At runtime, Polymorph dynamically selects and composes only the adapters needed to cover the active labels, avoiding full-model switching and weight merging. This modular strategy improves scalability while reducing latency and energy overhead. Polymorph achieves 40% lower energy consumption and improves mAP by 9 points over strong baselines on the TAO dataset. Polymorph is open source at https://github.com/inference-serving/polymorph/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14959v1</guid>
      <category>cs.CV</category>
      <category>cs.PF</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saeid Ghafouri, Mohsen Fayyaz, Xiangchen Li, Deepu John, Bo Ji, Dimitrios Nikolopoulos, Hans Vandierendonck</dc:creator>
    </item>
    <item>
      <title>Enhancements to P4TG: Histogram-Based RTT Monitoring in the Data Plane</title>
      <link>https://arxiv.org/abs/2507.15382</link>
      <description>arXiv:2507.15382v1 Announce Type: cross 
Abstract: Modern traffic generators are essential tools for evaluating the performance of network environments. P4TG is a P4-based traffic generator implemented for Intel Tofino switches that offers high-speed packet generation with fine-grained measurement capabilities. However, P4TG samples time-based metrics such as the round-trip time (RTT) in the data plane and collects them at the controller. This leads to a reduced accuracy. In this paper, we introduce a histogram-based RTT measurement feature for P4TG. It enables accurate analysis at line rate without sampling. Generally, histogram bins are modeled as ranges, and values are matched to a bin. Efficient packet matching in hardware is typically achieved using ternary content addressable memory (TCAM). However, representing range matching rules in TCAM poses a challenge. Therefore, we implemented a range-to-prefix conversion algorithm that models range matching with multiple ternary entries. This paper describes the data plane implementation and runtime configuration of RTT histograms in P4TG. Further, we discuss the efficiency of the ternary decomposition. Our evaluation demonstrates the applicability of the histogram-based RTT analysis by comparing the measured values with a configured theoretical distribution of RTTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15382v1</guid>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabian Ihle, Etienne Zink, Michael Menth</dc:creator>
    </item>
    <item>
      <title>Photonic Fabric Platform for AI Accelerators</title>
      <link>https://arxiv.org/abs/2507.14000</link>
      <description>arXiv:2507.14000v2 Announce Type: replace 
Abstract: This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM (PFA), a photonic-enabled switch and memory subsystem that delivers low latency, high bandwidth, and low per-bit energy. By integrating high-bandwidth HBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D electro-optical system-in-package, the PFA offers up to 32 TB of shared memory alongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM enables distributed AI training and inference to execute parallelism strategies more efficiently. The Photonic Fabric removes the silicon beachfront constraint that limits the fixed memory-to-compute ratio observed in virtually all current XPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet that connects to the Photonic Fabric increases its memory capacity and correspondingly its memory bandwidth by offering a flexible path to scaling well beyond the limitations of on-package HBM alone. We introduce CelestiSim, a lightweight analytical simulator validated on NVIDIA H100 and H200 systems. It is used to evaluate the performance of LLM reference and energy savings on PFA, without any significant change to the GPU core design. With the PFA, the simulation results show that up to 3.66x throughput and 1.40x latency improvements in LLM inference at 405B parameters, up to 7.04x throughput and 1.41x latency improvements at 1T parameters, and 60-90% energy savings in data movement for heavy collective operations in all LLM training scenarios. While these results are shown for NVIDIA GPUs, they can be applied similarly to other AI accelerator designs (XPUs) that share the same fundamental limitation of fixed memory to compute.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14000v2</guid>
      <category>cs.PF</category>
      <category>cs.AI</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jing Ding, Trung Diep</dc:creator>
    </item>
    <item>
      <title>How Cars Move: Analyzing Driving Dynamics for Safer Urban Traffic</title>
      <link>https://arxiv.org/abs/2412.04020</link>
      <description>arXiv:2412.04020v3 Announce Type: replace-cross 
Abstract: Understanding the spatial dynamics of cars within urban systems is essential for optimizing infrastructure management and resource allocation. Recent empirical approaches for analyzing traffic patterns have gained traction due to their applicability to city-scale policy development. However, conventional methodologies often rely on fragmented grid-based techniques, which may overlook critical interdependencies among spatial elements and temporal continuity. These limitations can compromise analytical effectiveness in complex urban environments. To address these challenges, we propose PriorMotion, a data integration framework designed to systematically uncover movement patterns through driving dynamics analysis. Our approach combines multi-scale empirical observations with customized analytical tools to capture evolving spatial-temporal trends in urban traffic. Comprehensive evaluations demonstrate that PriorMotion significantly enhances analytical outcomes, including increased accuracy in traffic pattern analysis, improved adaptability to heterogeneous data environments, and reduced long-term projection errors. Validation confirms its effectiveness for urban infrastructure management applications requiring precise characterization of complex spatial-temporal interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04020v3</guid>
      <category>cs.CV</category>
      <category>cs.PF</category>
      <category>cs.RO</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kangan Qian, Jinyu Miao, Xinyu Jiao, Ziang Luo, Zheng Fu, Yining Shi, Yunlong Wang, Kun Jiang, Diange Yang</dc:creator>
    </item>
  </channel>
</rss>
