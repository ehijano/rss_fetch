<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.PM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.PM</link>
    <description>q-fin.PM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.PM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 May 2025 04:00:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Loss-Versus-Rebalancing under Deterministic and Generalized block-times</title>
      <link>https://arxiv.org/abs/2505.05113</link>
      <description>arXiv:2505.05113v1 Announce Type: cross 
Abstract: Although modern blockchains almost universally produce blocks at fixed intervals, existing models still lack an analytical formula for the loss-versus-rebalancing (LVR) incurred by Automated Market Makers (AMMs) liquidity providers in this setting. Leveraging tools from random walk theory, we derive the following closed-form approximation for the per block per unit of liquidity expected LVR under constant block time:
  \[ \overline{\mathrm{ARB}}= \frac{\,\sigma_b^{2}} {\,2+\sqrt{2\pi}\,\gamma/(|\zeta(1/2)|\,\sigma_b)\,}+O\!\bigl(e^{-\mathrm{const}\tfrac{\gamma}{\sigma_b}}\bigr)\;\approx\; \frac{\sigma_b^{2}}{\,2 + 1.7164\,\gamma/\sigma_b}, \] where $\sigma_b$ is the intra-block asset volatility, $\gamma$ the AMM spread and $\zeta$ the Riemann Zeta function. Our large Monte Carlo simulations show that this formula is in fact quasi-exact across practical parameter ranges.
  Extending our analysis to arbitrary block-time distributions as well, we demonstrate both that--under every admissible inter-block law--the probability that a block carries an arbitrage trade converges to a universal limit, and that only constant block spacing attains the asymptotically minimal LVR. This shows that constant block intervals provide the best possible protection against arbitrage for liquidity providers. \end{abstract}</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05113v1</guid>
      <category>q-fin.MF</category>
      <category>math.PR</category>
      <category>q-fin.PM</category>
      <category>q-fin.PR</category>
      <category>q-fin.TR</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Nezlobin, Martin Tassy</dc:creator>
    </item>
    <item>
      <title>Data-Driven Merton's Strategies via Policy Randomization</title>
      <link>https://arxiv.org/abs/2312.11797</link>
      <description>arXiv:2312.11797v2 Announce Type: replace 
Abstract: We study Merton's expected utility maximization problem in an incomplete market, characterized by a factor process in addition to the stock price process, where all the model primitives are unknown. The agent under consideration is a price taker who has access only to the stock and factor value processes and the instantaneous volatility. We propose an auxiliary problem in which the agent can invoke policy randomization according to a specific class of Gaussian distributions, and prove that the mean of its optimal Gaussian policy solves the original Merton problem. With randomized policies, we are in the realm of continuous-time reinforcement learning (RL) recently developed in Wang et al. (2020) and Jia and Zhou (2022a, 2022b, 2023), enabling us to solve the auxiliary problem in a data-driven way without having to estimate the model primitives. Specifically, we establish a policy improvement theorem based on which we design both online and offline actor-critic RL algorithms for learning Merton's strategies. A key insight from this study is that RL in general and policy randomization in particular are useful beyond the purpose for exploration -- they can be employed as a technical tool to solve a problem that cannot be otherwise solved by mere deterministic policies. At last, we carry out both simulation and empirical studies in a stochastic volatility environment to demonstrate the decisive outperformance of the devised RL algorithms in comparison to the conventional model-based, plug-in method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11797v2</guid>
      <category>q-fin.PM</category>
      <category>cs.LG</category>
      <category>q-fin.CP</category>
      <pubDate>Fri, 09 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Min Dai, Yuchao Dong, Yanwei Jia, Xun Yu Zhou</dc:creator>
    </item>
  </channel>
</rss>
