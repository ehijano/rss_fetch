<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Nov 2025 05:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Scalable NorthPole System with End-to-End Vertical Integration for Low-Latency and Energy-Efficient LLM Inference</title>
      <link>https://arxiv.org/abs/2511.15950</link>
      <description>arXiv:2511.15950v1 Announce Type: new 
Abstract: A vertically integrated, end-to-end, research prototype system combines 288 NorthPole neural inference accelerator cards, offline training algorithms, a high-performance runtime stack, and a containerized inference pipeline to deliver a scalable and efficient cloud inference service. The system delivers 115 peta-ops at 4-bit integer precision and 3.7 PB/s of memory bandwidth across 18 2U servers, while consuming only 30 kW of power and weighing 730 kg in a 0.67 m^2 42U rack footprint. The system can run 3 simultaneous instances of the 8-billion-parameter open-source IBM Granite-3.3-8b-instruct model at 2,048 context length with 28 simultaneous users and a per-user inter-token latency of 2.8 ms. The system is scalable, modular, and reconfigurable, supporting various model sizes and context lengths, and is ideal for deploying agentic workflows for enterprise AI applications in existing data center (cloud, on-prem) environments. For example, the system can support 18 instances of a 3-billion-parameter model or a single instance of a 70-billion-parameter model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15950v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael V. DeBole, Rathinakumar Appuswamy, Neil McGlohon, Brian Taba, Steven K. Esser, Filipp Akopyan, John V. Arthur, Arnon Amir, Alexander Andreopoulos, Peter J. Carlson, Andrew S. Cassidy, Pallab Datta, Myron D. Flickner, Rajamohan Gandhasri, Guillaume J. Garreau, Megumi Ito, Jennifer L. Klamo, Jeffrey A. Kusnitz, Nathaniel J. McClatchey, Jeffrey L. McKinstry, Tapan K. Nayak, Carlos Ortega Otero, Hartmut Penner, William P. Risk, Jun Sawada, Jay Sivagnaname, Daniel F. Smith, Rafael Sousa, Ignacio Terrizzano, Takanori Ueda, Trent Gray-Donald, David Cox, Dharmendra S. Modha</dc:creator>
    </item>
    <item>
      <title>Optimizing Communication in Byzantine Agreement Protocols with Slim-HBBFT</title>
      <link>https://arxiv.org/abs/2511.15957</link>
      <description>arXiv:2511.15957v1 Announce Type: new 
Abstract: Byzantine agreement protocols in asynchronous networks have received renewed interest because they do not rely on network behavior to achieve termination. Conventional asynchronous Byzantine agreement protocols require every party to broadcast its requests (e.g., transactions), and at the end of the protocol, parties agree on one party's request. If parties agree on one party's requests while exchanging every party's request, the protocol becomes expensive. These protocols are used to design an atomic broadcast (ABC) protocol where parties agree on $\langle n-f \rangle$ parties' requests (assuming $n=3f+1$, where $n$ is the total number of parties, and $f$ is the number of Byzantine parties). Although the parties agree on a subset of requests in the ABC protocol, if the requests do not vary (are duplicated), investing in a costly protocol is not justified. We propose Slim-HBBFT, an atomic broadcast protocol that considers requests from a fraction of $n$ parties and improves communication complexity by a factor of $O(n)$. At the core of our design is a prioritized provable-broadcast (P-PB) protocol that generates proof of broadcast only for selected parties. We use the P-PB protocol to design the Slim-HBBFT atomic broadcast protocol. Additionally, we conduct a comprehensive security analysis to demonstrate that Slim-HBBFT satisfies the properties of the Asynchronous Common Subset protocol, ensuring robust security and reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15957v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nasit S Sony, Xianzhong Ding</dc:creator>
    </item>
    <item>
      <title>Efficient Chromosome Parallelization for Precision Medicine Genomic Workflows</title>
      <link>https://arxiv.org/abs/2511.15977</link>
      <description>arXiv:2511.15977v1 Announce Type: new 
Abstract: Large-scale genomic workflows used in precision medicine can process datasets spanning tens to hundreds of gigabytes per sample, leading to high memory spikes, intensive disk I/O, and task failures due to out-of-memory errors. Simple static resource allocation methods struggle to handle the variability in per-chromosome RAM demands, resulting in poor resource utilization and long runtimes. In this work, we propose multiple mechanisms for adaptive, RAM-efficient parallelization of chromosome-level bioinformatics workflows. First, we develop a symbolic regression model that estimates per-chromosome memory consumption for a given task and introduces an interpolating bias to conservatively minimize over-allocation. Second, we present a dynamic scheduler that adaptively predicts RAM usage with a polynomial regression model, treating task packing as a Knapsack problem to optimally batch jobs based on predicted memory requirements. Additionally, we present a static scheduler that optimizes chromosome processing order to minimize peak memory while preserving throughput. Our proposed methods, evaluated on simulations and real-world genomic pipelines, provide new mechanisms to reduce memory overruns and balance load across threads. We thereby achieve faster end-to-end execution, showcasing the potential to optimize large-scale genomic workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15977v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <category>q-bio.GN</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Mas Montserrat, Ray Verma, M\'iriam Barrab\'es, Francisco M. de la Vega, Carlos D. Bustamante, Alexander G. Ioannidis</dc:creator>
    </item>
    <item>
      <title>Can Asymmetric Tile Buffering Be Beneficial?</title>
      <link>https://arxiv.org/abs/2511.16041</link>
      <description>arXiv:2511.16041v1 Announce Type: new 
Abstract: General matrix multiplication (GEMM) is the computational backbone of modern AI workloads, and its efficiency is critically dependent on effective tiling strategies. Conventional approaches employ symmetric tile buffering, where the buffered tile size of the input $A$ along the dimension $M$ matches the output tile size of $C$.
  In this paper, we introduce asymmetric tile buffering (ATB), a simple but powerful technique that decouples the buffered tile dimensions of the input and output operands. We show, for the first time, that ATB is both practical and highly beneficial. To explain this effect, we develop a performance model that incorporates both the benefits of ATB (higher arithmetic intensity) and its overheads (higher kernel switching costs), providing insight into how to select effective ATB tiling factors. As a case study, we apply ATB to AMD's latest XDNA2 AI Engine (AIE), achieving up to a 4.54x speedup, from 4.8 to 24.6 TFLOPS on mixed-precision BFP16--BF16 GEMM, establishing a new performance record for XDNA2 AIE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16041v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chengyue Wang, Wesley Pang, Xinrui Wu, Gregory Jun, Luis Romero, Endri Taka, Diana Marculescu, Tony Nowatzki, Pranathi Vasireddy, Joseph Melber, Deming Chen, Jason Cong</dc:creator>
    </item>
    <item>
      <title>Mitigating Shared Storage Congestion Using Control Theory</title>
      <link>https://arxiv.org/abs/2511.16177</link>
      <description>arXiv:2511.16177v1 Announce Type: new 
Abstract: Efficient data access in High-Performance Computing (HPC) systems is essential to the performance of intensive computing tasks. Traditional optimizations of the I/O stack aim to improve peak performance but are often workload specific and require deep expertise, making them difficult to generalize or re-use. In shared HPC environments, resource congestion can lead to unpredictable performance, causing slowdowns and timeouts. To address these challenges, we propose a self-adaptive approach based on Control Theory to dynamically regulate client-side I/O rates. Our approach leverages a small set of runtime system load metrics to reduce congestion and enhance performance stability. We implement a controller in a multi-node cluster and evaluate it on a real testbed under a representative workload. Experimental results demonstrate that our method effectively mitigates I/O congestion, reducing total runtime by up to 20% and lowering tail latency, while maintaining stable performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16177v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3773274.3774277</arxiv:DOI>
      <dc:creator>Thomas Collignon (SPIRALS - Self-adaptation for distributed services and large software systems), Kouds Halitim (CTRL-A - Control for Autonomic computing systems), Rapha\"el Bleuse (CTRL-A - Control for Autonomic computing systems), Sophie Cerf (CTRL-A - Control for Autonomic computing systems), Bogdan Robu (GIPSA-MODUS - GIPSA - Modelling and Optimal Decision for Uncertain Systems), \'Eric Rutten (CTRL-A - Control for Autonomic computing systems), Lionel Seinturier (Universit\'e de Lille), Alexandre van Kempen (Qarnot Computing)</dc:creator>
    </item>
    <item>
      <title>Fast LLM Post-training via Decoupled and Best-of-N Speculation</title>
      <link>https://arxiv.org/abs/2511.16193</link>
      <description>arXiv:2511.16193v1 Announce Type: new 
Abstract: Rollout dominates the training time in large language model (LLM) post-training, where the trained model is used to generate tokens given a batch of prompts. SpecActor achieves fast rollout with speculative decoding that deploys a fast path (e.g., a smaller model) to accelerate the unparallelizable generation, while the correctness is guaranteed by fast parallel verification of the outputs with the original model. SpecActor addresses two foundational challenges in speculative rollout by (1) a \emph{dynamic decoupled speculation} execution method that maximizes the GPU computational efficiency to realize speedup for large-batch execution -- a configuration common in training but unfriendly to speculative execution and (2) a \emph{dynamic Best-of-N speculation} method that selects and combines different drafting methods according to the rollout progress. It substantially improves the speculation accuracy even when the best drafting method is unknown a priori, meanwhile without requiring adding extra computation resources. {\sys} is {1.3--1.7}\,$\times$ faster than common post-training baselines, and is {1.3--1.5}\,$\times$ faster compared to naively adopting speculative decoding for rollout.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16193v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rongxin Cheng, Kai Zhou, Xingda Wei, Siyuan Liu, Mingcong Han, Mingjing Ai, Yeju Zhou, Baoquan Zhong, Wencong Xiao, Xin Liu, Rong Chen, Haibo Chen</dc:creator>
    </item>
    <item>
      <title>Optimizing Federated Learning in the Era of LLMs: Message Quantization and Streaming</title>
      <link>https://arxiv.org/abs/2511.16450</link>
      <description>arXiv:2511.16450v1 Announce Type: new 
Abstract: Federated Learning (FL) offers a promising solution for training machine learning models across distributed data sources while preserving data privacy. However, FL faces critical challenges related to communication overhead and local resource constraints, especially in the era of Large Language Models (LLMs) with billions of parameters. The sheer size of these models exacerbates both memory and communication constraints, making efficient transmission and processing essential for practical deployment. NVIDIA FLARE, an open-source SDK for federated learning, addresses these challenges by introducing advanced communication capabilities. Building upon existing solutions for large object streaming, we enhance FL workflows for LLMs through two key techniques: message quantization and container/file streaming. Quantization reduces message size, while streaming enables efficient memory management, improving scalability and integration with existing workflows. These advancements significantly enhance the robustness and efficiency of FL with LLMs, ensuring better performance in real-world federated learning scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16450v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyue Xu, Zhihong Zhang, Holger R. Roth, Chester Chen, Yan Cheng, Andrew Feng</dc:creator>
    </item>
    <item>
      <title>Distributed MIS Algorithms for Rational Agents using Games</title>
      <link>https://arxiv.org/abs/2511.16533</link>
      <description>arXiv:2511.16533v1 Announce Type: new 
Abstract: We study the problem of computing a Maximal Independent Set (MIS) in distributed networks where each node is a rational agent whose payoff depends on whether it joins the MIS. Classical distributed algorithms assume that nodes follow the prescribed protocol, but this assumption fails when nodes are strategic and may deviate if doing so increases their expected utility.
  Standard MIS algorithms rely on honest randomness or unique identifiers to break symmetry. In rational settings, however, agents may manipulate randomness, and relying solely on identifiers can create unfairness, giving some nodes zero probability of joining the MIS and thus no incentive to participate. To address these issues, we propose two algorithms based on a utility model in which agents seek locally correct solutions while also having preferences over which solution is chosen. Randomness in our algorithms is generated through pairwise interactions between neighboring nodes, viewed as simple games in which no single node can unilaterally affect the outcome. This allows symmetry breaking while remaining compatible with rational behavior.
  For both algorithms, we prove that at every stage of the execution, given any history, no agent can increase its expected utility through a unilateral deviation, assuming others follow the algorithm. This gives a stronger guarantee than Trembling-Hand Perfect Equilibrium. When all nodes follow the protocol, every node has a positive probability of joining the MIS, and the final output is a correct MIS. Under mild additional assumptions, both algorithms terminate in $O(\log n)$ rounds with high probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16533v1</guid>
      <category>cs.DC</category>
      <category>cs.GT</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nithin Salevemula, Shreyas Pai</dc:creator>
    </item>
    <item>
      <title>Secure Autonomous Agent Payments: Verifying Authenticity and Intent in a Trustless Environment</title>
      <link>https://arxiv.org/abs/2511.15712</link>
      <description>arXiv:2511.15712v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) agents are increasingly capable of initiating financial transactions on behalf of users or other agents. This evolution introduces a fundamental challenge: verifying both the authenticity of an autonomous agent and the true intent behind its transactions in a decentralized, trustless environment. Traditional payment systems assume human authorization, but autonomous, agent-led payments remove that safeguard. This paper presents a blockchain-based framework that cryptographically authenticates and verifies the intent of every AI-initiated transaction. The proposed system leverages decentralized identity (DID) standards and verifiable credentials to establish agent identities, on-chain intent proofs to record user authorization, and zero-knowledge proofs (ZKPs) to preserve privacy while ensuring policy compliance. Additionally, secure execution environments (TEE-based attestations) guarantee the integrity of agent reasoning and execution. The hybrid on-chain/off-chain architecture provides an immutable audit trail linking user intent to payment outcome. Through qualitative analysis, the framework demonstrates strong resistance to impersonation, unauthorized transactions, and misalignment of intent. This work lays the foundation for secure, auditable, and intent-aware autonomous economic agents, enabling a future of verifiable trust and accountability in AI-driven financial ecosystems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15712v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Vivek Acharya</dc:creator>
    </item>
    <item>
      <title>Digital Agriculture Sandbox for Collaborative Research</title>
      <link>https://arxiv.org/abs/2511.15990</link>
      <description>arXiv:2511.15990v1 Announce Type: cross 
Abstract: Digital agriculture is transforming the way we grow food by utilizing technology to make farming more efficient, sustainable, and productive. This modern approach to agriculture generates a wealth of valuable data that could help address global food challenges, but farmers are hesitant to share it due to privacy concerns. This limits the extent to which researchers can learn from this data to inform improvements in farming. This paper presents the Digital Agriculture Sandbox, a secure online platform that solves this problem. The platform enables farmers (with limited technical resources) and researchers to collaborate on analyzing farm data without exposing private information. We employ specialized techniques such as federated learning, differential privacy, and data analysis methods to safeguard the data while maintaining its utility for research purposes. The system enables farmers to identify similar farmers in a simplified manner without needing extensive technical knowledge or access to computational resources. Similarly, it enables researchers to learn from the data and build helpful tools without the sensitive information ever leaving the farmer's system. This creates a safe space where farmers feel comfortable sharing data, allowing researchers to make important discoveries. Our platform helps bridge the gap between maintaining farm data privacy and utilizing that data to address critical food and farming challenges worldwide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15990v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Osama Zafar, Rosemarie Santa Gonz\'alez, Alfonso Morales, Erman Ayday</dc:creator>
    </item>
    <item>
      <title>Pipelined Dense Symmetric Eigenvalue Decomposition on Multi-GPU Architectures</title>
      <link>https://arxiv.org/abs/2511.16174</link>
      <description>arXiv:2511.16174v1 Announce Type: cross 
Abstract: Large symmetric eigenvalue problems are commonly observed in many disciplines such as Chemistry and Physics, and several libraries including cuSOLVERMp, MAGMA and ELPA support computing large eigenvalue decomposition on multi-GPU or multi-CPU-GPU hybrid architectures. However, these libraries do not provide satisfied performance that all of the libraries only utilize around 1.5\% of the peak multi-GPU performance. In this paper, we propose a pipelined two-stage eigenvalue decomposition algorithm instead of conventional subsequent algorithm with substantial optimizations. On an 8$\times$A100 platform, our implementation surpasses state-of-the-art cuSOLVERMp and MAGMA baselines, delivering mean speedups of 5.74$\times$ and 6.59$\times$, with better strong and weak scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16174v1</guid>
      <category>cs.MS</category>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hansheng Wang, Ruiyi Zhan, Dajun Huang, Xingchen Liu, Qiao Li, Hancong Duan, Dingwen Tao, Guangming Tan, Shaoshuai Zhang</dc:creator>
    </item>
    <item>
      <title>A Fast Relax-and-Round Approach to Unit Commitment for Data Center Own Generation</title>
      <link>https://arxiv.org/abs/2511.16420</link>
      <description>arXiv:2511.16420v1 Announce Type: cross 
Abstract: The rapid growth of data centers increasingly requires data center operators to "bring own generation" to complement the available utility power plants to supply all or part of data center load. This practice sharply increases the number of generators on the bulk power system and shifts operational focus toward fuel costs rather than traditional startup and runtime constraints. Conventional mixed-integer unit commitment formulations are not well suited for systems with thousands of flexible, fast-cycling units. We propose a unit commitment formulation that relaxes binary commitment decisions by allowing generators to be fractionally on, enabling the use of algorithms for continuous solvers. We then use a rounding approach to get a feasible unit commitment. For a 276-unit system, solution time decreases from 10 hours to less than a second, with minor accuracy degradation. Our approach scales with no issues to tens of thousands of generators, which allows solving problems on the scale of the major North America interconnections. The bulk of computation is parallel and GPU compatible, enabling further acceleration in future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16420v1</guid>
      <category>math.OC</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaked Regev, Eve Tsybina, Slaven Peles</dc:creator>
    </item>
    <item>
      <title>Taming the Long-Tail: Efficient Reasoning RL Training with Adaptive Drafter</title>
      <link>https://arxiv.org/abs/2511.16665</link>
      <description>arXiv:2511.16665v1 Announce Type: cross 
Abstract: The emergence of Large Language Models (LLMs) with strong reasoning capabilities marks a significant milestone, unlocking new frontiers in complex problem-solving. However, training these reasoning models, typically using Reinforcement Learning (RL), encounters critical efficiency bottlenecks: response generation during RL training exhibits a persistent long-tail distribution, where a few very long responses dominate execution time, wasting resources and inflating costs. To address this, we propose TLT, a system that accelerates reasoning RL training losslessly by integrating adaptive speculative decoding. Applying speculative decoding in RL is challenging due to the dynamic workloads, evolving target model, and draft model training overhead. TLT overcomes these obstacles with two synergistic components: (1) Adaptive Drafter, a lightweight draft model trained continuously on idle GPUs during long-tail generation to maintain alignment with the target model at no extra cost; and (2) Adaptive Rollout Engine, which maintains a memory-efficient pool of pre-captured CUDAGraphs and adaptively select suitable SD strategies for each input batch. Evaluations demonstrate that TLT achieves over 1.7x end-to-end RL training speedup over state-of-the-art systems, preserves the model accuracy, and yields a high-quality draft model as a free byproduct suitable for efficient deployment. Code is released at https://github.com/mit-han-lab/fastrl.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16665v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qinghao Hu, Shang Yang, Junxian Guo, Xiaozhe Yao, Yujun Lin, Yuxian Gu, Han Cai, Chuang Gan, Ana Klimovic, Song Han</dc:creator>
    </item>
    <item>
      <title>Prioritized-MVBA: A New Approach to Design an Optimal Asynchronous Byzantine Agreement Protocol</title>
      <link>https://arxiv.org/abs/2406.03739</link>
      <description>arXiv:2406.03739v2 Announce Type: replace 
Abstract: The multi-valued byzantine agreement protocol (MVBA) in the authenticated setting has been widely used as a core to design atomic broadcast and fault-tolerant state machine replication protocols in asynchronous networks. Originating from the seminal work of Cachin et al. \cite{CACHIN01}, subsequent research endeavors have sought to optimize protocol efficiency in terms of communication complexity. Notable advancements following Cachin's contributions include: i) VABA \cite{BYZ17}, requiring multiple protocol instances to achieve agreement on a party's request, and ii) Dumbo-MVBA \cite{LU20}, employing a cryptographic asynchronous dispersal and recovery methods to manage communication complexity alongside additional computational and communication rounds overheads.
  Our objective is to devise an MVBA protocol that achieves agreement in each instance without extra computation and communication rounds while maintaining the optimal metrics. Central to our design approach is the introduction of the committee in the classic MVBA protocol, wherein a randomly selected subset of ($f+1$, where $n=3f+1$) parties get selected and simultaneously broadcast their requests (transactions) to gather verifiable proofs. Successive distributions of these proofs afford us the necessary properties to employ the asynchronous binary Byzantine agreement (ABBA) protocol for reaching an agreement on a selected party's requests. By integrating the committee and ABBA protocols, we devise the optimal MVBA protocol, termed pMVBA (Prioritized-MVBA). This protocol exhibits resilience to tolerate up to $\lfloor \frac{n}{3}\rfloor$ Byzantine failures, with an expected runtime of $O(1)$, optimal message complexity of $O(n^2)$, and optimal communication complexity $O((l+\lambda)n^2)$ .</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03739v2</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nasit S Sony, Xianzhong Ding</dc:creator>
    </item>
    <item>
      <title>Renaming in distributed certification</title>
      <link>https://arxiv.org/abs/2409.15404</link>
      <description>arXiv:2409.15404v3 Announce Type: replace 
Abstract: Local certification is the area of distributed network computing asking the following question: How to certify to the nodes of a network that a global property holds, if they are limited to a local verification?
  In this area, it is often essential to have identifiers, that is, unique integers assigned to the nodes. In this short paper, we show how to reduce the range of the identifiers, in three different settings. More precisely, we show how to rename identifiers in the classical local certification setting, when we can (resp.\ cannot) choose the new identifiers, and we show how a global certificate can help to encode very compactly a new identifier assignment that is not injective in general, but still useful in applications.
  We conclude with a number of applications of these results: For every $\ell$, there are local certification schemes for the properties of having clique number at most $\ell$, having diameter at most $\ell$, and having independence number at most~2, with certificates of size $O(n)$. We also show that there is a global certification scheme for bipartiteness with a certificate of size $O(n)$. All these results are optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15404v3</guid>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Bousquet, Louis Esperet, Laurent Feuilloley, S\'ebastien Zeitoun</dc:creator>
    </item>
    <item>
      <title>MonadBFT: Fast, Responsive, Fork-Resistant Streamlined Consensus</title>
      <link>https://arxiv.org/abs/2502.20692</link>
      <description>arXiv:2502.20692v2 Announce Type: replace 
Abstract: This paper introduces MonadBFT, a novel Byzantine Fault Tolerant (BFT) consensus protocol that advances both performance and robustness. MonadBFT is implemented as the consensus protocol in the Monad blockchain. As a HotStuff-family protocol, MonadBFT has linear message complexity in the common case and is optimistically responsive, operating as quickly as the network allows. A central feature of MonadBFT is its tail-forking resistance. In pipelined BFT protocols, when a leader goes offline, the previous proposal is abandoned. Malicious leaders can exploit this tail-forking behavior as a form of Maximal Extractable Value (MEV) attack by deliberately discarding their predecessor's block, depriving that proposer of rewards and enabling transaction reordering, censorship or theft. MonadBFT prevents such tail-forking attacks, preserving both fairness and integrity in transaction execution. Another related feature of MonadBFT is its notion of speculative finality, which enables parties to execute ordered transactions after a single round (i.e., a single view), with reverts occurring only in the rare case of provable leader equivocation. This mechanism reduces user-perceived latency. Additionally, we introduce the leader fault isolation property, which ensures that the protocol can quickly recover from a failure. To our knowledge, no prior pipelined, leader-based BFT consensus protocol combines all of these properties in a single design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20692v2</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Mussadiq Jalalzai, Kushal Babel, Jovan Komatovic, Tobias Klenze, Sourav Das, Fatima Elsheimy, Mike Setrin, John Bergschneider, Babak Gilkalaye</dc:creator>
    </item>
    <item>
      <title>Distributed Computing From First Principles</title>
      <link>https://arxiv.org/abs/2506.12959</link>
      <description>arXiv:2506.12959v2 Announce Type: replace 
Abstract: This book on Distributed Computing aims to benefit a diverse audience, ranging from aspiring engineers, and seasoned researchers, to a wide range of professionals. Driven by my passion for making the core concepts of distributed computing accessible, this work is a significant undertaking designed to empower individuals from all backgrounds to gain valuable insight. Have you ever wondered how a typical distributed system works under the hood? Are you looking for a pedagogical guide with complete implementations? In this work, we have implemented several foundational algorithms in Distributed Computing. Whether your expertise lies in the theoretical foundations or the practical applications of the principles of Distributed Systems, this book is for you.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12959v2</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>cs.SE</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenneth Odoh</dc:creator>
    </item>
    <item>
      <title>Minimmit: Fast Finality with Even Faster Blocks</title>
      <link>https://arxiv.org/abs/2508.10862</link>
      <description>arXiv:2508.10862v4 Announce Type: replace 
Abstract: Achieving low-latency consensus in geographically distributed systems remains a key challenge for blockchain and distributed database applications. To this end, there has been significant recent interest in State-Machine-Replication (SMR) protocols that achieve 2-round finality under the assumption that $5f+1\leq n$, where $n$ is the number of processors and $f$ bounds the number of processors that may exhibit Byzantine faults. In these protocols, instructions are organised into views, each led by a different designated leader, and 2-round finality means that a leader's proposal can be finalised after just a single round of voting, meaning two rounds overall (one round for the proposal and one for voting).
  We introduce Minimmit, a Byzantine-fault-tolerant SMR protocol with lower latency than previous 2-round finality approaches. Our key insight is that view progression and transaction finality can operate on different quorum thresholds without compromising safety or liveness. Experiments simulating a globally distributed network of 50 processors, uniformly assigned across ten virtual regions, show that the approach leads to a 23.1% reduction in view latency and a 10.7% reduction in transaction latency compared to the state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10862v4</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brendan Kobayashi Chou, Andrew Lewis-Pye, Patrick O'Grady</dc:creator>
    </item>
    <item>
      <title>New Hardness Results for the LOCAL Model via a Simple Self-Reduction</title>
      <link>https://arxiv.org/abs/2510.19972</link>
      <description>arXiv:2510.19972v2 Announce Type: replace 
Abstract: Very recently, Khoury and Schild [FOCS 2025] showed that any randomized LOCAL algorithm that solves maximal matching requires $\Omega(\min\{\log \Delta, \log_\Delta n\})$ rounds, where $n$ is the number of nodes in the graph and $\Delta$ is the maximum degree. This result is shown through a new technique, called round elimination via self-reduction. The lower bound proof is beautiful and presents very nice ideas. However, it spans more than 25 pages of technical details, and hence it is hard to digest and generalize to other problems. Historically, the simplification of proofs and techniques has marked an important turning point in our understanding of the complexity of graph problems. Our paper makes a step forward towards this direction, and provides the following contributions.
  1. We present a short and simplified version of the round elimination via self-reduction technique. The simplification of this technique enables us to obtain the following two hardness results.
  2. We show that any randomized LOCAL algorithm that solves the maximal $b$-matching problem requires $\Omega(\min\{\log_{1+b}\Delta, \log_\Delta n\})$ and $\Omega(\sqrt{\log_{1+b} n})$ rounds. We recall that the $b$-matching problem is a generalization of the matching problem where each vertex can have up to $b$ incident edges in the matching. As a corollary, for $b=1$, we obtain a short proof for the maximal matching lower bound shown by Khoury and Schild.
  3. Finally, we show that any randomized LOCAL algorithm that properly colors the edges of a graph with $\Delta + k$ colors requires $\Omega(\min\{\log \Delta, \log_\Delta n\})$ and $\Omega(\sqrt{\log n})$ rounds, for any $k\le \Delta^{1-\varepsilon}$ and any constant $\varepsilon &gt; 0$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19972v2</guid>
      <category>cs.DC</category>
      <category>cs.CC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alkida Balliu, Filippo Casagrande, Francesco d'Amore, Dennis Olivetti</dc:creator>
    </item>
    <item>
      <title>MoFa: A Unified Performance Modeling Framework for LLM Pretraining</title>
      <link>https://arxiv.org/abs/2511.09837</link>
      <description>arXiv:2511.09837v2 Announce Type: replace 
Abstract: The exponential growth in LLM scales, with parameters soaring from billions to trillions, has necessitated distributed pretraining across large clusters comprising thousands to tens of thousands of devices. While hybrid parallelization strategies enable such pretraining, the vast combinatorial strategy space introduces significant optimization challenges. Traditional manual tuning methods incur prohibitive trial-and-error costs, and existing performance modeling approaches exhibit critical limitations: they fail to comprehensively account for prevalent optimization features and ignore the substantial overhead imposed by essential fault tolerance mechanisms like checkpoint recovery in long-duration pretraining. To address these gaps, we propose MoFa, a novel pretraining performance modeling framework that unifies multi-dimensional optimization features and fault tolerance. MoFa incorporates an enhanced cost model to accurately capture the effects of key optimizations and integrates a fault tolerance model based on historical cluster reliability data. Besides, a MoFa-based tuning system is developed to explore optimal pretraining performance and potential bottlenecks in various scenarios. Extensive modeling evaluations demonstrate that MoFa can achieve high prediction accuracy across various scenarios. In addition, through comprehensive tuning experiments, our framework systematically reveals the key factors influencing pretraining performance under different configurations, which provides solid a priori guidance for LLM pretraining system design and deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09837v2</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lu Zhao, Rong Shi, Shaoqing Zhang, Shangchao Su, Ziqing Yin, Zhiyan Cui, Hongfeng Sun, Baoguo He, Yueqiang Chen, Liang Dong, Xiyuan Li, Lingbin Wang, Lijun Ma, Qiang Huang, Ting Liu, Chong Wang, Can Wei</dc:creator>
    </item>
    <item>
      <title>Improved Bounds for Coin Flipping, Leader Election, and Random Selection</title>
      <link>https://arxiv.org/abs/2504.01856</link>
      <description>arXiv:2504.01856v2 Announce Type: replace-cross 
Abstract: Random selection, leader election, and collective coin flipping are fundamental tasks in fault-tolerant distributed computing. We study these problems in the full-information model where despite decades of study, key gaps remain in our understanding of the trade-offs between round complexity, communication per player in each round, and adversarial resilience. We make progress by proving improved bounds for these problems.
  We first show that any $k$-round coin flipping protocol over $\ell$ players, each player sending one bit per round, can be biased by $O(\ell/\log^{(k)}(\ell))$ bad players. We obtain a similar lower bound for leader election. This strengthens prior best bounds [RSZ, SICOMP 2002] of $O(\ell/\log^{(2k-1)}(\ell))$ for coin flipping protocols and $O(\ell/\log^{(2k+1)}(\ell))$ for leader election protocols. Our result implies that any (1-bit per player) protocol tolerating linear fraction of bad players requires at least $\log^* \ell$ rounds, showing existing protocols [RZ, JCSS 2001; F, FOCS 1999] are near-optimal.
  We next initiate the study of one-round, (1-bit per player) random selection. We construct a protocol resilient to $\ell / (\log \ell)^2$ bad players that outputs $(\log \ell)^2 / (\log \log \ell)^2$ uniform random bits. This implies a one-round leader election protocol resilient to $\ell / (\log \ell)^2$ bad players, improving the prior best protocol [RZ, JCSS 2001] which was resilient to $\ell / (\log \ell)^3$ bad players. Our resilience matches that of the best one-round coin flipping protocol by Ajtai &amp; Linial. We also obtain an almost matching lower bound: any protocol outputting $(\log \ell)^2 / (\log \log \ell)^2$ bits can be corrupted by $\ell (\log \log \ell)^2 / (\log \ell)^2$ bad players. To obtain our lower bound, we introduce multi-output influence, an extension of influence of boolean functions to the multi-output setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01856v2</guid>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eshan Chattopadhyay, Mohit Gurumukhani, Noam Ringach, Rocco Servedio</dc:creator>
    </item>
  </channel>
</rss>
