<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Jul 2025 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Faster Multi-Source Reachability and Approximate Distances via Shortcuts, Hopsets and Matrix Multiplication</title>
      <link>https://arxiv.org/abs/2507.13470</link>
      <description>arXiv:2507.13470v1 Announce Type: new 
Abstract: Given an $n$-vertex $m$-edge digraph $G = (V,E)$ and a subset $S \subseteq V$ of $|S| = n^{\sigma}$ (for some $0 \le \sigma \le 1$) designated sources, the $S \times V$ reachability problem is to compute the sets $\mathcal V_s$ of vertices reachable from $s$, for every $s \in S$. Naive centralized algorithms run BFS/DFS from each source in $O(m \cdot n^{\sigma})$ time or compute $G$'s transitive closure in $\hat O(n^{\omega})$ time, where $\omega \le 2.371552\ldots$ is the matrix multiplication exponent. Thus, the best known bound is $\hat O(n^{\min \{ 2 + \sigma, \omega\}})$. Leveraging shortcut constructions by Kogan and Parter [SODA 2022, ICALP 2022], we develop a centralized algorithm with running time $\hat O(n^{1 + \frac{2}{3} \omega(\sigma)})$, where $\omega(\sigma)$ is the rectangular matrix multiplication exponent. Using current estimates on $\omega(\sigma)$, our exponent improves upon $\min \{2 + \sigma, \omega \}$ for $\tilde \sigma \leq \sigma \leq 0.53$, where $1/3 &lt; \tilde \sigma &lt; 0.3336$ is a universal constant.
  In a classical result, Cohen [Journal of Algorithms, 1996] devised parallel algorithms for $S \times V$ reachability on graphs admitting balanced recursive separators of size $n^{\rho}$ for $\rho &lt; 1$, requiring polylogarithmic time and work $n^{\max \{\omega \rho, 2\rho + \sigma \} + o(1)}$. We significantly improve, extend, and generalize Cohen's result. First, our parallel algorithm for graphs with small recursive separators has lower work complexity than Cohen's in boraod paramater ranges. Second, we generalize our algorithm to graphs of treewidth at most $n^{\rho}$ ($\rho &lt; 1$) and provide a centralized algorithm that outperforms existing bounds for $S \times V$ reachability on such graphs. We also do this for some other graph familes with small separators. Finally, we extend these results to $(1 + \epsilon)$-approximate distance computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13470v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Elkin, Chhaya Trehan</dc:creator>
    </item>
    <item>
      <title>Strassen $2\times2$ Matrix Multiplication from a 3-dimensional Volume Form</title>
      <link>https://arxiv.org/abs/2507.13510</link>
      <description>arXiv:2507.13510v1 Announce Type: new 
Abstract: The Strassen $2\times2$ matrix multiplication algorithm arises from the volume form on the 3-dimensional quotient space of the $2\times 2$ matrices by the multiples of identity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13510v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benoit Jacob (AMD)</dc:creator>
    </item>
    <item>
      <title>Combinatorics of Palindromes</title>
      <link>https://arxiv.org/abs/2507.13671</link>
      <description>arXiv:2507.13671v1 Announce Type: new 
Abstract: We investigate the structure and reconstruction complexity of Manacher arrays. First, we establish a combinatorial lower bound, proving that the number of rooted tandem repeat trees with $n+1$ genes exceeds the number of distinct Manacher arrays of length $n$. Second, we introduce a graph-theoretic framework that associates a graph to each Manacher array, where every proper vertex coloring yields a string consistent with the array. Finally, we analyze a reconstruction algorithm by I et al. (SPIRE 2010), showing that it simultaneously achieves a globally minimal alphabet size, uses at most $\log_2(n{-}1) + 2$ distinct symbols, and can be adapted to produce reconstructions over arbitrary alphabets when possible. Our results also resolve an open problem posed by the original authors. Together, these findings advance the combinatorial understanding of Manacher arrays and open new directions for string reconstruction under structural constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13671v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Itzhaki</dc:creator>
    </item>
    <item>
      <title>Tight Bounds for Answering Adaptively Chosen Concentrated Queries</title>
      <link>https://arxiv.org/abs/2507.13700</link>
      <description>arXiv:2507.13700v1 Announce Type: new 
Abstract: Most work on adaptive data analysis assumes that samples in the dataset are independent. When correlations are allowed, even the non-adaptive setting can become intractable, unless some structural constraints are imposed. To address this, Bassily and Freund [2016] introduced the elegant framework of concentrated queries, which requires the analyst to restrict itself to queries that are concentrated around their expected value. While this assumption makes the problem trivial in the non-adaptive setting, in the adaptive setting it remains quite challenging. In fact, all known algorithms in this framework support significantly fewer queries than in the independent case: At most $O(n)$ queries for a sample of size $n$, compared to $O(n^2)$ in the independent setting.
  In this work, we prove that this utility gap is inherent under the current formulation of the concentrated queries framework, assuming some natural conditions on the algorithm. Additionally, we present a simplified version of the best-known algorithms that match our impossibility result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13700v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emma Rapoport, Edith Cohen, Uri Stemmer</dc:creator>
    </item>
    <item>
      <title>Improved girth approximation in weighted undirected graphs</title>
      <link>https://arxiv.org/abs/2507.13869</link>
      <description>arXiv:2507.13869v1 Announce Type: new 
Abstract: Let $G = (V,E,\ell)$ be a $n$-node $m$-edge weighted undirected graph, where $\ell: E \rightarrow (0,\infty)$ is a real \emph{length} function defined on its edges, and let $g$ denote the girth of $G$, i.e., the length of its shortest cycle. We present an algorithm that, for any input, integer $k \geq 1$, in $O(kn^{1+1/k}\log{n} + m(k+\log{n}))$ expected time finds a cycle of length at most $\frac{4k}{3}g$. This algorithm nearly matches a $O(n^{1+1/k}\log{n})$-time algorithm of \cite{KadriaRSWZ22} which applied to unweighted graphs of girth $3$. For weighted graphs, this result also improves upon the previous state-of-the-art algorithm that in $O((n^{1+1/k}\log n+m)\log (nM))$ time, where $\ell: E \rightarrow [1, M]$ is an integral length function, finds a cycle of length at most $2kg$~\cite{KadriaRSWZ22}. For $k=1$ this result improves upon the result of Roditty and Tov~\cite{RodittyT13}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13869v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Avi Kadria, Liam Roditty, Aaron Sidford, Virginia Vassilevska Williams, Uri Zwick</dc:creator>
    </item>
    <item>
      <title>Quantum Pattern Matching with Wildcards</title>
      <link>https://arxiv.org/abs/2507.13885</link>
      <description>arXiv:2507.13885v1 Announce Type: new 
Abstract: Pattern matching is one of the fundamental problems in Computer Science. Both the classic version of the problem as well as the more sophisticated version where wildcards can also appear in the input can be solved in almost linear time $\tilde O(n)$ using the KMP algorithm and Fast Fourier Transform, respectively. In 2000, Ramesh and Vinay~\cite{ramesh2003string} give a quantum algorithm that solves classic pattern matching in sublinear time and asked whether the wildcard problem can also be solved in sublinear time? In this work, we give a quantum algorithm for pattern matching with wildcards that runs in time $\tilde O(\sqrt{n}\sqrt{k})$ when the number of wildcards is bounded by $k$ for $k \geq \sqrt{n}$. This leads to an algorithm that runs in sublinear time as long as the number of wildcards is sublinear.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13885v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masoud Seddighin, Saeed Seddighin</dc:creator>
    </item>
    <item>
      <title>Optimal antimatroid sorting</title>
      <link>https://arxiv.org/abs/2507.13994</link>
      <description>arXiv:2507.13994v1 Announce Type: new 
Abstract: The classical comparison-based sorting problem asks us to find the underlying total order of a given set of elements, where we can only access the elements via comparisons. In this paper, we study a restricted version, where, as a hint, a set $T$ of possible total orders is given, usually in some compressed form.
  Recently, an algorithm called topological heapsort with optimal running time was found for the case where $T$ is the set of topological orderings of a given directed acyclic graph, or, equivalently, $T$ is the set of linear extensions of a given partial order [Haeupler et al. 2024]. We show that a simple generalization of topological heapsort is applicable to a much broader class of restricted sorting problems, where $T$ corresponds to a given antimatroid.
  As a consequence, we obtain optimal algorithms for the following restricted sorting problems, where the allowed total orders are restricted by: a given set of monotone precedence formulas; the perfect elimination orders of a given chordal graph; or the possible vertex search orders of a given connected rooted graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13994v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Aram Berendsohn</dc:creator>
    </item>
    <item>
      <title>Sparse Navigable Graphs for Nearest Neighbor Search: Algorithms and Hardness</title>
      <link>https://arxiv.org/abs/2507.14060</link>
      <description>arXiv:2507.14060v1 Announce Type: new 
Abstract: We initiate the study of approximation algorithms and computational barriers for constructing sparse $\alpha$-navigable graphs [IX23, DGM+24], a core primitive underlying recent advances in graph-based nearest neighbor search. Given an $n$-point dataset $P$ with an associated metric $\mathsf{d}$ and a parameter $\alpha \geq 1$, the goal is to efficiently build the sparsest graph $G=(P, E)$ that is $\alpha$-navigable: for every distinct $s, t \in P$, there exists an edge $(s, u) \in E$ with $\mathsf{d}(u, t) &lt; \mathsf{d}(s, t)/\alpha$. We consider two natural sparsity objectives: minimizing the maximum out-degree and minimizing the total size.
  We first show a strong negative result: the slow-preprocessing version of DiskANN (analyzed in [IX23] for low-doubling metrics) can yield solutions whose sparsity is $\widetilde{\Omega}(n)$ times larger than optimal, even on Euclidean instances. We then show a tight approximation-preserving equivalence between the Sparsest Navigable Graph problem and the classic Set Cover problem, obtaining an $O(n^3)$-time $(\ln n + 1)$-approximation algorithm, as well as establishing NP-hardness of achieving an $o(\ln n)$-approximation. Building on this equivalence, we develop faster $O(\ln n)$-approximation algorithms. The first runs in $\widetilde{O}(n \cdot \mathrm{OPT})$ time and is thus much faster when the optimal solution is sparse. The second, based on fast matrix multiplication, is a bicriteria algorithm that computes an $O(\ln n)$-approximation to the sparsest $2\alpha$-navigable graph, running in $\widetilde{O}(n^{\omega})$ time.
  Finally, we complement our upper bounds with a query complexity lower bound, showing that any $o(n)$-approximation requires examining $\Omega(n^2)$ distances. This result shows that in the regime where $\mathrm{OPT} = \widetilde{O}(n)$, our $\widetilde{O}(n \cdot \mathrm{OPT})$-time algorithm is essentially best possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14060v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanjeev Khanna, Ashwin Padaki, Erik Waingarten</dc:creator>
    </item>
    <item>
      <title>An Efficient Massively Parallel Constant-Factor Approximation Algorithm for the $k$-Means Problem</title>
      <link>https://arxiv.org/abs/2507.14089</link>
      <description>arXiv:2507.14089v1 Announce Type: new 
Abstract: In this paper, we present an efficient massively parallel approximation algorithm for the $k$-means problem. Specifically, we provide an MPC algorithm that computes a constant-factor approximation to an arbitrary $k$-means instance in $O(\log\log n \cdot \log\log\log n)$ rounds. The algorithm uses $O(n^\sigma)$ bits of memory per machine, where $\sigma &gt; 0$ is a constant that can be made arbitrarily small. The global memory usage is $O(n^{1+\varepsilon})$ bits for an arbitrarily small constant $\varepsilon &gt; 0$, and is thus only slightly superlinear. Recently, Czumaj, Gao, Jiang, Krauthgamer, and Vesel\'{y} showed that a constant-factor bicriteria approximation can be computed in $O(1)$ rounds in the MPC model. However, our algorithm is the first constant-factor approximation for the general $k$-means problem that runs in $o(\log n)$ rounds in the MPC model.
  Our approach builds upon the foundational framework of Jain and Vazirani. The core component of our algorithm is a constant-factor approximation for the related facility location problem. While such an approximation was already achieved in constant time in the work of Czumaj et al.\ mentioned above, our version additionally satisfies the so-called Lagrangian Multiplier Preserving (LMP) property. This property enables the transformation of a facility location approximation into a comparably good $k$-means approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14089v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Cohen-Addad, Fabian Kuhn, Zahra Parsaeian</dc:creator>
    </item>
    <item>
      <title>Weighted Matching in a Poly-Streaming Model</title>
      <link>https://arxiv.org/abs/2507.14114</link>
      <description>arXiv:2507.14114v1 Announce Type: new 
Abstract: We introduce the poly-streaming model, a generalization of streaming models of computation in which $k$ processors process $k$ data streams containing a total of $N$ items. The algorithm is allowed $O\left(f(k)\cdot M_1\right)$ space, where $M_1$ is either $o\left(N\right)$ or the space bound for a sequential streaming algorithm. Processors may communicate as needed. Algorithms are assessed by the number of passes, per-item processing time, total runtime, space usage, communication cost, and solution quality.
  We design a single-pass algorithm in this model for approximating the maximum weight matching (MWM) problem. Given $k$ edge streams and a parameter $\varepsilon &gt; 0$, the algorithm computes a $\left(2+\epsilon\right)$-approximate MWM. We analyze its performance in a shared-memory parallel setting: for any constant $\varepsilon &gt; 0$, it runs in time $\widetilde{O}\left(L_{\max}+n\right)$, where $n$ is the number of vertices and $L_{\max}$ is the maximum stream length. It supports $O\left(1\right)$ per-edge processing time using $\widetilde{O}\left(k\cdot n\right)$ space. We further generalize the design to hierarchical architectures, in which $k$ processors are partitioned into $r$ groups, each with its own shared local memory. The total intergroup communication is $\widetilde{O}\left(r \cdot n\right)$ bits, while all other performance guarantees are preserved.
  We evaluate the algorithm on a shared-memory system using graphs with trillions of edges. It achieves substantial speedups as $k$ increases and produces matchings with weights significantly exceeding the theoretical guarantee. On our largest test graph, it reduces runtime by nearly two orders of magnitude and memory usage by five orders of magnitude compared to an offline algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14114v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahammed Ullah, S. M. Ferdous, Alex Pothen</dc:creator>
    </item>
    <item>
      <title>Treedepth Inapproximability and Exponential ETH Lower Bound</title>
      <link>https://arxiv.org/abs/2507.13818</link>
      <description>arXiv:2507.13818v1 Announce Type: cross 
Abstract: Treedepth is a central parameter to algorithmic graph theory. The current state-of-the-art in computing and approximating treedepth consists of a $2^{O(k^2)} n$-time exact algorithm and a polynomial-time $O(\text{OPT} \log^{3/2} \text{OPT})$-approximation algorithm, where the former algorithm returns an elimination forest of height $k$ (witnessing that treedepth is at most $k$) for the $n$-vertex input graph $G$, or correctly reports that $G$ has treedepth larger than $k$, and $\text{OPT}$ is the actual value of the treedepth. On the complexity side, exactly computing treedepth is NP-complete, but the known reductions do not rule out a polynomial-time approximation scheme (PTAS), and under the Exponential Time Hypothesis (ETH) only exclude a running time of $2^{o(\sqrt n)}$ for exact algorithms.
  We show that 1.0003-approximating treedepth is NP-hard, and that exactly computing the treedepth of an $n$-vertex graph requires time $2^{\Omega(n)}$, unless the ETH fails. We further derive that there exist absolute constants $\delta, c &gt; 0$ such that any $(1+\delta)$-approximation algorithm requires time $2^{\Omega(n / \log^c n)}$. We do so via a simple direct reduction from Satisfiability to Treedepth, inspired by a reduction recently designed for Treewidth [STOC '25].</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13818v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>\'Edouard Bonnet, Daniel Neuen, Marek Soko{\l}owski</dc:creator>
    </item>
    <item>
      <title>Efficient Online Random Sampling via Randomness Recycling</title>
      <link>https://arxiv.org/abs/2505.18879</link>
      <description>arXiv:2505.18879v2 Announce Type: replace 
Abstract: ``Randomness recycling'' is a powerful algorithmic technique for reusing a fraction of the random information consumed by a probabilistic algorithm to reduce its entropy requirements. This article presents a family of randomness recycling algorithms for efficiently sampling a sequence $X_1, X_2, X_3, \dots$ of discrete random variables whose joint distribution follows an arbitrary stochastic process. We develop randomness recycling techniques to reduce the entropy cost of a variety of prominent sampling algorithms, which include uniform sampling, inverse transform sampling, lookup-table sampling, alias sampling, and discrete distribution generating (DDG) tree sampling. Our method achieves an expected amortized entropy cost of $H(X_1,\dots,X_k)/k + \varepsilon$ input bits per output sample using $O(\log(1/\varepsilon))$ space as $k\to\infty$, which is arbitrarily close to the optimal Shannon entropy rate of $H(X_1,\dots,X_k)/k$ bits per sample. The combination of space, time, and entropy properties of our method improves upon the Knuth and Yao entropy-optimal algorithm and Han and Hoshi interval algorithm for sampling a discrete random sequence.
  On the empirical side, we show that randomness recycling enables state-of-the-art runtime performance on the Fisher-Yates shuffle when using a cryptographically secure pseudorandom number generator; and it can also speed up discrete Gaussian samplers. Accompanying the manuscript is a performant software library in the C programming language that uses randomness recycling to accelerate several existing algorithms for random sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18879v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas L. Draper, Feras A. Saad</dc:creator>
    </item>
    <item>
      <title>Permutation patterns in streams</title>
      <link>https://arxiv.org/abs/2507.11291</link>
      <description>arXiv:2507.11291v2 Announce Type: replace 
Abstract: Permutation patterns and pattern avoidance are central, well-studied concepts in combinatorics and computer science. Given two permutations $\tau$ and $\pi$, the pattern matching problem (PPM) asks whether $\tau$ contains $\pi$. This problem arises in various contexts in computer science and statistics and has been studied extensively in exact-, parameterized-, approximate-, property-testing- and other formulations.
  In this paper, we study pattern matching in a streaming setting, when the input $\tau$ is revealed sequentially, one element at a time. There is extensive work on the space complexity of various statistics in streams of integers. The novelty of our setting is that the input stream is a permutation, which allows inferring some information about future inputs. Our algorithms crucially take advantage of this fact, while existing lower bound techniques become difficult to apply.
  We show that the complexity of the problem changes dramatically depending on the pattern $\pi$. The space requirement is: $\Theta(k\log{n})$ for the monotone patterns $\pi = 12\dots k$, or $\pi = k\dots21$, $O(\sqrt{n\log{n}})$ for $\pi \in \{312,132\}$, $O(\sqrt{n} \log n)$ for $\pi \in \{231,213\}$, and $\widetilde{\Theta}_{\pi}(n)$ for all other $\pi$. If $\tau$ is an arbitrary sequence of integers (not necessary a permutation), we show that the complexity is $\widetilde{\Theta}_{\pi}(n)$ in all except the first (monotone) cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11291v2</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Aram Berendsohn</dc:creator>
    </item>
    <item>
      <title>More Efforts Towards Fixed-Parameter Approximability of Multiwinner Rules</title>
      <link>https://arxiv.org/abs/2505.12699</link>
      <description>arXiv:2505.12699v2 Announce Type: replace-cross 
Abstract: Multiwinner Elections have emerged as a prominent area of research with numerous practical applications. We contribute to this area by designing parameterized approximation algorithms and also resolving an open question by Yang and Wang [AAMAS'18]. More formally, given a set of candidates, \mathcal{C}, a set of voters,\mathcal{V}, approving a subset of candidates (called approval set of a voter), and an integer $k$, we consider the problem of selecting a ``good'' committee using Thiele rules. This problem is computationally challenging for most Thiele rules with monotone submodular satisfaction functions, as there is no (1-\frac{1}{e}-\epsilon)\footnote{Here, $e$ denotes the base of the natural logarithm.}-approximation algorithm in f(k)(|\mathcal{C}| + |\mathcal{V}|)^{o(k)} time for any fixed $\epsilon &gt; 0$ and any computable function $f$, and no {\sf PTAS} even when the length of approval set is two. Skowron [WINE'16] designed an approximation scheme running in FPT time parameterized by the combined parameter, size of the approval set and $k$. In this paper, we consider a parameter $d+k$ (no $d$ voters approve the same set of $d$ candidates), where $d$ is upper bounded by the size of the approval set (thus, can be much smaller).
  With respect to this parameter, we design parameterized approximation schemes, a lossy polynomial-time preprocessing method, and show that an extra committee member suffices to achieve the desired score (i.e., $1$-additive approximation). Additionally, we resolve an open question by Yang and Wang~[AAMAS'18] regarding the fixed-parameter tractability of the problem under the PAV rule with the total score as the parameter, demonstrating that it admits an FPT algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12699v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sushmita Gupta, Pallavi Jain, Souvik Saha, Saket Saurabh, Anannya Upasana</dc:creator>
    </item>
  </channel>
</rss>
