<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Sep 2025 04:01:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Meshing Framework for Digital Twins for Extrusion based Additive Manufacturing</title>
      <link>https://arxiv.org/abs/2509.12436</link>
      <description>arXiv:2509.12436v1 Announce Type: new 
Abstract: Additive manufacturing (AM) allows for manufacturing of complex three-dimensional geometries not typically realizable with standard subtractive manufacturing practices. The internal microstructure of a 3D printed component can have a significant impact on its mechanical, vibrational, and shock properties and allows for a richer design space when this is controllable. Due to the complex interactions of the internal geometry of an extrusion-based AM component, it is common practice to assume a homogeneous behavior or to perform characterization testing on the specific toolpath configurations. To avoid unnecessary testing or material waste, it is necessary to develop an accurate and consistent numerical simulation framework with relevant boundary value problems that can handle the complicated geometry of internal material microstructure present in AM components. Herein, a framework is proposed to directly create computational meshes suitable for finite element analysis (FEA) of the fine-scale features generated from extrusion-based AM tool paths to maintain a strong process-structure-property-performance linkage. This mesh can be manually or automatically analyzed using standard FEA simulations such as quasi-static preloading, modal analysis, or thermal analysis. The framework allows an in-silico assessment of a target AM geometry where fine-scale features may greatly impact quantities of design interest such as in soft elastomeric lattices where toolpath infill can greatly influence the self contact of a structure in compression, which we will use as a motivating exemplar. This approach greatly reduces the waste of both time and resources consumed through traditional build and test design cycles for non-intuitive design spaces. It also further allows for the exploration of toolpath infill to optimize component properties beyond simple linear properties such as density and stiffness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12436v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Gallup, Kevin N. Long, Devin J. Roach, William D. Reinholtz, Adam Cook, Craig M. Hamel</dc:creator>
    </item>
    <item>
      <title>Context-Aware Language Models for Forecasting Market Impact from Sequences of Financial News</title>
      <link>https://arxiv.org/abs/2509.12519</link>
      <description>arXiv:2509.12519v1 Announce Type: new 
Abstract: Financial news plays a critical role in the information diffusion process in financial markets and is a known driver of stock prices. However, the information in each news article is not necessarily self-contained, often requiring a broader understanding of the historical news coverage for accurate interpretation. Further, identifying and incorporating the most relevant contextual information presents significant challenges. In this work, we explore the value of historical context in the ability of large language models to understand the market impact of financial news. We find that historical context provides a consistent and significant improvement in performance across methods and time horizons. To this end, we propose an efficient and effective contextualization method that uses a large LM to process the main article, while a small LM encodes the historical context into concise summary embeddings that are then aligned with the large model's representation space. We explore the behavior of the model through multiple qualitative and quantitative interpretability tests and reveal insights into the value of contextualization. Finally, we demonstrate that the value of historical context in model predictions has real-world applications, translating to substantial improvements in simulated investment performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12519v1</guid>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>q-fin.CP</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ross Koval, Nicholas Andrews, Xifeng Yan</dc:creator>
    </item>
    <item>
      <title>Impact of Geometric Uncertainty on the Computation of Abdominal Aortic Aneurysm Wall Strain</title>
      <link>https://arxiv.org/abs/2509.12550</link>
      <description>arXiv:2509.12550v1 Announce Type: new 
Abstract: Abdominal aortic aneurysm (AAA) is a life-threatening condition characterized by permanent enlargement of the aorta, often detected incidentally during imaging for unrelated conditions. Current management relies primarily on aneurysm diameter and growth rate, which may not reliably predict patient-specific rupture risk. Computation of AAA wall stress and strain has the potential to improve individualized risk assessment, but these analyses depend on image-derived geometry, which is subject to segmentation uncertainty and lacks a definitive ground truth for the wall boundary. While the effect of geometric uncertainty on wall stress has been studied, its influence on wall strain remains unclear. In this study, we assessed the impact of geometric uncertainty on AAA wall strain computed using deformable image registration of time-resolved 3D computed tomography angiography (4D-CTA). Controlled perturbations were applied to the wall geometry along the surface normal, parameterized by the standard deviation for random variation and the mean for systematic inward or outward bias, both scaled relative to wall thickness. Results show that uncertainties in AAA wall geometry reduce the accuracy of computed strain, with inward bias (toward the blood lumen and intraluminal thrombus) consistently causing greater deviations than outward bias (toward regions external to the aortic wall). Peak strain is more sensitive but less robust, whereas the 99th percentile strain remains more stable under perturbations. We concluded that, for sufficiently accurate strain estimation, geometric uncertainty should remain within one wall thickness (typically 1.5 mm).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12550v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saeideh Sekhavat, Mostafa Jamshidian, Adam Wittek, Karol Miller</dc:creator>
    </item>
    <item>
      <title>FinSentLLM: Multi-LLM and Structured Semantic Signals for Enhanced Financial Sentiment Forecasting</title>
      <link>https://arxiv.org/abs/2509.12638</link>
      <description>arXiv:2509.12638v1 Announce Type: new 
Abstract: Financial sentiment analysis (FSA) has attracted significant attention, and recent studies increasingly explore large language models (LLMs) for this field. Yet most work evaluates only classification metrics, leaving unclear whether sentiment signals align with market behavior. We propose FinSentLLM, a lightweight multi-LLM framework that integrates an expert panel of sentiment forecasting LLMs, and structured semantic financial signals via a compact meta-classifier. This design captures expert complementarity, semantic reasoning signal, and agreement/divergence patterns without costly retraining, yielding consistent 3-6% gains over strong baselines in accuracy and F1-score on the Financial PhraseBank dataset. In addition, we also provide econometric evidence that financial sentiment and stock markets exhibit statistically significant long-run comovement, applying Dynamic Conditional Correlation GARCH (DCC-GARCH) and the Johansen cointegration test to daily sentiment scores computed from the FNSPID dataset and major stock indices. Together, these results demonstrate that FinSentLLM delivers superior forecasting accuracy for financial sentiment and further establish that sentiment signals are robustly linked to long-run equity market dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12638v1</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zijian Zhang, Rong Fu, Yangfan He, Xinze Shen, Yanlong Wang, Xiaojing Du, Haochen You, Jiazhao Shi, Simon Fong</dc:creator>
    </item>
    <item>
      <title>A Cost-Optimization Model for EV Charging Stations Utilizing Solar Energy and Variable Pricing</title>
      <link>https://arxiv.org/abs/2509.12214</link>
      <description>arXiv:2509.12214v1 Announce Type: cross 
Abstract: This paper presents a cost optimization framework for electric vehicle (EV) charging stations that leverages on-site photovoltaic (PV) generation and explicitly accounts for electricity price uncertainty through a Bertsimas--Sim robust formulation. The model is formulated as a linear program that satisfies vehicle energy demands, respects charging and grid capacity constraints, and minimizes procurement cost. Evaluations on real charging data from the Caltech ACN dataset show average savings of about 12\% compared to a first-come--first-served baseline, with peak monthly reductions up to 19.2\%. A lightweight sensitivity analysis indicates that a modest $\sim$5\% increase in nominal cost can reduce worst-case exposure by 14\%. Computational tests confirm real-time feasibility, with instances of up to 50 concurrent EVs solved in under 5 seconds on a standard laptop. The proposed method provides a practical, grid-friendly, and scalable solution for future EV charging operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12214v1</guid>
      <category>eess.SY</category>
      <category>cs.CE</category>
      <category>cs.SY</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>An Nguyen, Hung Pham, Cuong Do</dc:creator>
    </item>
    <item>
      <title>An End to End Edge to Cloud Data and Analytics Strategy</title>
      <link>https://arxiv.org/abs/2509.12296</link>
      <description>arXiv:2509.12296v1 Announce Type: cross 
Abstract: There is an exponential growth of connected Internet of Things (IoT) devices. These have given rise to applications that rely on real time data to make critical decisions quickly. Enterprises today are adopting cloud at a rapid pace. There is a critical need to develop secure and efficient strategy and architectures to best leverage capabilities of cloud and edge assets. This paper provides an end to end secure edge to cloud data and analytics strategy. To enable real life implementation, the paper provides reference architectures for device layer, edge layer and cloud layer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12296v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICCCNT54827.2022.9984604</arxiv:DOI>
      <arxiv:journal_reference>2022 13th International Conference on Computing Communication and Networking Technologies (ICCCNT). IEEE, 2022</arxiv:journal_reference>
      <dc:creator>Vijay Kumar Butte, Sujata Butte</dc:creator>
    </item>
    <item>
      <title>A comparison of pipelines for the translation of a low resource language based on transformers</title>
      <link>https://arxiv.org/abs/2509.12514</link>
      <description>arXiv:2509.12514v1 Announce Type: cross 
Abstract: This work compares three pipelines for training transformer-based neural networks to produce machine translators for Bambara, a Mand\`e language spoken in Africa by about 14,188,850 people. The first pipeline trains a simple transformer to translate sentences from French into Bambara. The second fine-tunes LLaMA3 (3B-8B) instructor models using decoder-only architectures for French-to-Bambara translation. Models from the first two pipelines were trained with different hyperparameter combinations to improve BLEU and chrF scores, evaluated on both test sentences and official Bambara benchmarks. The third pipeline uses language distillation with a student-teacher dual neural network to integrate Bambara into a pre-trained LaBSE model, which provides language-agnostic embeddings. A BERT extension is then applied to LaBSE to generate translations. All pipelines were tested on Dokotoro (medical) and Bayelemagaba (mixed domains). Results show that the first pipeline, although simpler, achieves the best translation accuracy (10% BLEU, 21% chrF on Bayelemagaba), consistent with low-resource translation results. On the Yiri dataset, created for this work, it achieves 33.81% BLEU and 41% chrF. Instructor-based models perform better on single datasets than on aggregated collections, suggesting they capture dataset-specific patterns more effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12514v1</guid>
      <category>cs.CL</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chiara Bonfanti, Michele Colombino, Giulia Coucourde, Faeze Memari, Stefano Pinardi, Rosa Meo</dc:creator>
    </item>
    <item>
      <title>A Computational Pipeline for Patient-Specific Modeling of Thoracic Aortic Aneurysm: From Medical Image to Finite Element Analysis</title>
      <link>https://arxiv.org/abs/2509.12596</link>
      <description>arXiv:2509.12596v1 Announce Type: cross 
Abstract: The aorta is the body's largest arterial vessel, serving as the primary pathway for oxygenated blood within the systemic circulation. Aortic aneurysms consistently rank among the top twenty causes of mortality in the United States. Thoracic aortic aneurysm (TAA) arises from abnormal dilation of the thoracic aorta and remains a clinically significant disease, ranking as one of the leading causes of death in adults. A thoracic aortic aneurysm ruptures when the integrity of all aortic wall layers is compromised due to elevated blood pressure. Currently, three-dimensional computed tomography (3D CT) is considered the gold standard for diagnosing TAA. The geometric characteristics of the aorta, which can be quantified from medical imaging, and stresses on the aortic wall, which can be obtained by finite element analysis (FEA), are critical in evaluating the risk of rupture and dissection. Deep learning based image segmentation has emerged as a reliable method for extracting anatomical regions of interest from medical images. Voxel based segmentation masks of anatomical structures are typically converted into structured mesh representation to enable accurate simulation. Hexahedral meshes are commonly used in finite element simulations of the aorta due to their computational efficiency and superior simulation accuracy. Due to anatomical variability, patient specific modeling enables detailed assessment of individual anatomical and biomechanics behaviors, supporting precise simulations, accurate diagnoses, and personalized treatment strategies. Finite element (FE) simulations provide valuable insights into the biomechanical behaviors of tissues and organs in clinical studies. Developing accurate FE models represents a crucial initial step in establishing a patient-specific, biomechanically based framework for predicting the risk of TAA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12596v1</guid>
      <category>eess.IV</category>
      <category>cs.CE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiasong Chen, Linchen Qian, Ruonan Gong, Christina Sun, Tongran Qin, Thuy Pham, Caitlin Martin, Mohammad Zafar, John Elefteriades, Wei Sun, Liang Liang</dc:creator>
    </item>
    <item>
      <title>Large Language Model Scaling Laws for Neural Quantum States in Quantum Chemistry</title>
      <link>https://arxiv.org/abs/2509.12679</link>
      <description>arXiv:2509.12679v1 Announce Type: cross 
Abstract: Scaling laws have been used to describe how large language model (LLM) performance scales with model size, training data size, or amount of computational resources. Motivated by the fact that neural quantum states (NQS) has increasingly adopted LLM-based components, we seek to understand NQS scaling laws, thereby shedding light on the scalability and optimal performance--resource trade-offs of NQS ansatze. In particular, we identify scaling laws that predict the performance, as measured by absolute error and V-score, for transformer-based NQS as a function of problem size in second-quantized quantum chemistry applications. By performing analogous compute-constrained optimization of the obtained parametric curves, we find that the relationship between model size and training time is highly dependent on loss metric and ansatz, and does not follow the approximately linear relationship found for language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12679v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>quant-ph</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oliver Knitter, Dan Zhao, Stefan Leichenauer, Shravan Veerapaneni</dc:creator>
    </item>
    <item>
      <title>A Graph Machine Learning Approach for Detecting Topological Patterns in Transactional Graphs</title>
      <link>https://arxiv.org/abs/2509.12730</link>
      <description>arXiv:2509.12730v1 Announce Type: cross 
Abstract: The rise of digital ecosystems has exposed the financial sector to evolving abuse and criminal tactics that share operational knowledge and techniques both within and across different environments (fiat-based, crypto-assets, etc.). Traditional rule-based systems lack the adaptability needed to detect sophisticated or coordinated criminal behaviors (patterns), highlighting the need for strategies that analyze actors' interactions to uncover suspicious activities and extract their modus operandi. For this reason, in this work, we propose an approach that integrates graph machine learning and network analysis to improve the detection of well-known topological patterns within transactional graphs. However, a key challenge lies in the limitations of traditional financial datasets, which often provide sparse, unlabeled information that is difficult to use for graph-based pattern analysis. Therefore, we firstly propose a four-step preprocessing framework that involves (i) extracting graph structures, (ii) considering data temporality to manage large node sets, (iii) detecting communities within, and (iv) applying automatic labeling strategies to generate weak ground-truth labels. Then, once the data is processed, Graph Autoencoders are implemented to distinguish among the well-known topological patterns. Specifically, three different GAE variants are implemented and compared in this analysis. Preliminary results show that this pattern-focused, topology-driven method is effective for detecting complex financial crime schemes, offering a promising alternative to conventional rule-based detection systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12730v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Francesco Zola, Jon Ander Medina, Andrea Venturi, Amaia Gil, Raul Orduna</dc:creator>
    </item>
    <item>
      <title>A batch production scheduling problem in a reconfigurable hybrid manufacturing-remanufacturing system</title>
      <link>https://arxiv.org/abs/2504.00605</link>
      <description>arXiv:2504.00605v3 Announce Type: replace 
Abstract: In recent years, remanufacturing of End-of-Life (EOL) products has been adopted by manufacturing sectors as a competent practice to enhance their sustainability and market share. Due to the mass customization of products and high volatility of market, processing of new products and remanufacturing of EOLs in the same shared facility, namely Hybrid Manufacturing-Remanufacturing System (HMRS), is a mean to keep such production efficient. Accordingly, customized production capabilities are required to increase flexibility, which can be effectively provided under the Reconfigurable Manufacturing System (RMS) paradigm. Despite the advantages of utilizing RMS technologies in HMRSs, production management of such systems suffers excessive complexity. Hence, this study concentrates on the production scheduling of an HMRS consisting of non-identical parallel reconfigurable machines where the orders can be grouped into batches. In this regard, Mixed-Integer Linear Programming (MILP) and Constraint Programming (CP) models are devised to formulate the problem. Furthermore, a computationally efficient solution method is developed based on a Logic-based Benders Decomposition (LBBD) approach. The warm start technique is also implemented by providing a decent initial solution to the MILP model. Computational experiments attest to the LBBD method's superiority over the MILP, CP, and warm-started MILP models by obtaining an average gap of about 2%, besides it yields actionable managerial insights for scheduling in HMRSs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00605v3</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cie.2025.111099</arxiv:DOI>
      <dc:creator>Behdin Vahedi-Nouri, Mohammad Rohaninejad, Zden\v{e}k Hanz\'alek, Mehdi Foumani</dc:creator>
    </item>
    <item>
      <title>Image memorability predicts social media virality and externally-associated commenting</title>
      <link>https://arxiv.org/abs/2409.14659</link>
      <description>arXiv:2409.14659v3 Announce Type: replace-cross 
Abstract: Visual content on social media plays a key role in entertainment and information sharing, yet some images gain more engagement than others. We propose that image memorability - the ability to be remembered - may predict viral potential. Using 1,247 Reddit image posts across three timepoints, we assessed memorability with neural network ResMem and correlated the predicted memorability scores with virality metrics. Memorable images were consistently associated with more comments, even after controlling for image categories with ResNet-152. Semantic analysis revealed that memorable images relate to more neutral-affect comments, suggesting a distinct pathway to virality from emotional content. Additionally, visual consistency analysis showed that memorable posts inspired diverse, externally-associated comments. By analyzing ResMem's layers, we found semantic distinctiveness was key to both memorability and virality. This study highlights memorability as a unique correlate of social media virality, offering insights into how visual features and human cognitive behavioral interactions are associated with online engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14659v3</guid>
      <category>cs.HC</category>
      <category>cs.CE</category>
      <category>cs.SI</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shikang Peng, Wilma A. Bainbridge</dc:creator>
    </item>
    <item>
      <title>EB-gMCR: Energy-Based Generative Modeling for Signal Unmixing and Multivariate Curve Resolution</title>
      <link>https://arxiv.org/abs/2507.23600</link>
      <description>arXiv:2507.23600v3 Announce Type: replace-cross 
Abstract: Signal unmixing analysis decomposes data into basic patterns and is widely applied in chemical and biological research. Multivariate curve resolution (MCR), a branch of signal unmixing, separates mixed signals into components (base patterns) and their concentrations (intensity), playing a key role in understanding composition. Classical MCR is typically framed as matrix factorization (MF) and requires a user-specified number of components, usually unknown in real data. Once data or component number increases, the scalability of these MCR approaches face significant challenges. This study reformulates MCR as a data generative process (gMCR), and introduces an Energy-Based solver, EB-gMCR, that automatically discovers the smallest component set and their concentrations for reconstructing the mixed signals faithfully. On synthetic benchmarks with up to 256 components, EB-gMCR attains high reconstruction fidelity and recovers the component count within 5% at 20dB noise and near-exact at 30dB. On two public spectral datasets, it identifies the correct component count and improves component separation over MF-based MCR approaches (NMF variants, ICA, MCR-ALS). EB-gMCR is a general solver for fixed-pattern signal unmixing (components remain invariant across mixtures). Domain priors (non-negativity, nonlinear mixing) enter as plug-in modules, enabling adaptation to new instruments or domains without altering the core selection learning step. The source code is available at https://github.com/b05611038/ebgmcr_solver.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23600v3</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu-Tang Chang, Shih-Fang Chen</dc:creator>
    </item>
    <item>
      <title>TinyDef-DETR: A DETR-based Framework for Defect Detection in Transmission Lines from UAV Imagery</title>
      <link>https://arxiv.org/abs/2509.06035</link>
      <description>arXiv:2509.06035v3 Announce Type: replace-cross 
Abstract: Automated defect detection from UAV imagery of transmission lines is a challenging task due to the small size, ambiguity, and complex backgrounds of defects. This paper proposes TinyDef-DETR, a DETR-based framework designed to achieve accurate and efficient detection of transmission line defects from UAV-acquired images. The model integrates four major components: an edge-enhanced ResNet backbone to strengthen boundary-sensitive representations, a stride-free space-to-depth module to enable detail-preserving downsampling, a cross-stage dual-domain multi-scale attention mechanism to jointly model global context and local cues, and a Focaler-Wise-SIoU regression loss to improve the localization of small and difficult targets. Together, these designs effectively mitigate the limitations of conventional detectors. Extensive experiments on both public and real-world datasets demonstrate that TinyDef-DETR achieves superior detection performance and strong generalization capability, while maintaining modest computational overhead. The accuracy and efficiency of TinyDef-DETR make it a suitable method for UAV-based transmission line defect detection, particularly in scenarios involving small and ambiguous targets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06035v3</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaming Cui, Shuai Zhou, Feng Shen</dc:creator>
    </item>
  </channel>
</rss>
