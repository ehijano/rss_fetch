<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.comp-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.comp-ph</link>
    <description>physics.comp-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.comp-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 Jan 2026 05:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Knowledge Distillation of a Protein Language Model Yields a Foundational Implicit Solvent Model</title>
      <link>https://arxiv.org/abs/2601.05388</link>
      <description>arXiv:2601.05388v1 Announce Type: cross 
Abstract: Implicit solvent models (ISMs) promise to deliver the accuracy of explicit solvent simulations at a fraction of the computational cost. However, despite decades of development, their accuracy has remained insufficient for many critical applications, particularly for simulating protein folding and the behavior of intrinsically disordered proteins. Developing a transferable, data-driven ISM that overcomes the limitations of traditional analytical formulas remains a central challenge in computational chemistry. Here we address this challenge by introducing a novel strategy that distills the evolutionary information learned by a protein language model, ESM3, into a computationally efficient graph neural network (GNN). We show that this GNN potential, trained on effective energies from ESM3, is robust enough to drive stable, long-timescale molecular dynamics simulations. When combined with a standard electrostatics term, our hybrid model accurately reproduces protein folding free-energy landscapes and predicts the structural ensembles of intrinsically disordered proteins. This approach yields a single, unified model that is transferable across both folded and disordered protein states, resolving a long-standing limitation of conventional ISMs. By successfully distilling evolutionary knowledge into a physical potential, our work delivers a foundational implicit solvent model poised to accelerate the development of predictive, large-scale simulation tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05388v1</guid>
      <category>physics.bio-ph</category>
      <category>physics.chem-ph</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin Airas, Bin Zhang</dc:creator>
    </item>
    <item>
      <title>Explainable AI: Learning from the Learners</title>
      <link>https://arxiv.org/abs/2601.05525</link>
      <description>arXiv:2601.05525v1 Announce Type: cross 
Abstract: Artificial intelligence now outperforms humans in several scientific and engineering tasks, yet its internal representations often remain opaque. In this Perspective, we argue that explainable artificial intelligence (XAI), combined with causal reasoning, enables {\it learning from the learners}. Focusing on discovery, optimization and certification, we show how the combination of foundation models and explainability methods allows the extraction of causal mechanisms, guides robust design and control, and supports trust and accountability in high-stakes applications. We discuss challenges in faithfulness, generalization and usability of explanations, and propose XAI as a unifying framework for human-AI collaboration in science and engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05525v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ricardo Vinuesa, Steven L. Brunton, Gianmarco Mengaldo</dc:creator>
    </item>
    <item>
      <title>Autonomous Discovery of the Ising Model's Critical Parameters with Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2601.05577</link>
      <description>arXiv:2601.05577v1 Announce Type: cross 
Abstract: Traditional methods for determining critical parameters are often influenced by human factors. This research introduces a physics-inspired adaptive reinforcement learning framework that enables agents to autonomously interact with physical environments, simultaneously identifying both the critical temperature and various types of critical exponents in the Ising model with precision. Interestingly, our algorithm exhibits search behavior reminiscent of phase transitions, efficiently converging to target parameters regardless of initial conditions. Experimental results demonstrate that this method significantly outperforms traditional approaches, particularly in environments with strong perturbations. This study not only incorporates physical concepts into machine learning to enhance algorithm interpretability but also establishes a new paradigm for scientific exploration, transitioning from manual analysis to autonomous AI discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05577v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1088/1742-5468/ae22ea</arxiv:DOI>
      <arxiv:journal_reference>J. Stat. Mech. (2025)</arxiv:journal_reference>
      <dc:creator>Hai Man, Chaobo Wang, Jia-Rui Li, Yuping Tian, Shu-Gang Chen</dc:creator>
    </item>
    <item>
      <title>GlueNN: gluing patchwise analytic solutions with neural networks</title>
      <link>https://arxiv.org/abs/2601.05889</link>
      <description>arXiv:2601.05889v1 Announce Type: cross 
Abstract: In many problems in physics and engineering, one encounters complicated differential equations with strongly scale-dependent terms for which exact analytical or numerical solutions are not available. A common strategy is to divide the domain into several regions (patches) and simplify the equation in each region. When approximate analytic solutions can be obtained in each patch, they are then matched at the interfaces to construct a global solution. However, this patching procedure can fail to reproduce the correct solution, since the approximate forms may break down near the matching boundaries. In this work, we propose a learning framework in which the integration constants of asymptotic analytic solutions are promoted to scale-dependent functions. By constraining these coefficient functions with the original differential equation over the domain, the network learns a globally valid solution that smoothly interpolates between asymptotic regimes, eliminating the need for arbitrary boundary matching. We demonstrate the effectiveness of this framework in representative problems from chemical kinetics and cosmology, where it accurately reproduces global solutions and outperforms conventional matching procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05889v1</guid>
      <category>cs.LG</category>
      <category>astro-ph.CO</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Doyoung Kim, Donghee Lee, Hye-Sung Lee, Jiheon Lee, Jaeok Yi</dc:creator>
    </item>
    <item>
      <title>Computer-Assisted Proofs for Geometric Optimization: From Crystallization to Carbon Nanotubes</title>
      <link>https://arxiv.org/abs/2506.22614</link>
      <description>arXiv:2506.22614v2 Announce Type: replace 
Abstract: We present a framework based on computer-assisted proofs that turns standard geometry optimization simulations for atomistic structures into mathematical proofs. Starting from a numerically computed approximation of a local minimizer or saddle point, we use validated numerical computations to prove the existence of a critical point of the potential energy close to this approximation. We demonstrate this framework in two settings. In the first, we study capped carbon nanotubes modeled as minimizers of carbon interatomic potentials (harmonic, Tersoff, and a Huber potential) and obtain proven bounds on tube diameter, bond lengths, and bond angles. In particular, we show that caps induce diameter oscillations along the tube. As a second application, we consider a finite Lennard-Jones crystal in a face-centered cubic (fcc) lattice and provide computer-proofs of a local minimizer representing the perfect crystal, a local minimizer with a single vacancy defect, and a saddle point that connects two single-vacancy configurations on the energy landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22614v2</guid>
      <category>physics.comp-ph</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miguel Ayala, Rustum Choksi, Benedikt Wirth</dc:creator>
    </item>
    <item>
      <title>Adaptive Online Emulation for Accelerating Complex Physical Simulations</title>
      <link>https://arxiv.org/abs/2508.08012</link>
      <description>arXiv:2508.08012v2 Announce Type: replace 
Abstract: Complex physical simulations often require trade-offs between model fidelity and computational feasibility. We introduce Adaptive Online Emulation (AOE), which dynamically learns neural network surrogates during simulation execution to accelerate expensive components. Unlike existing methods requiring extensive offline training, AOE uses Online Sequential Extreme Learning Machines (OS-ELMs) to continuously adapt emulators along the actual simulation trajectory. We employ a numerically stable variant of the OS-ELM using cumulative sufficient statistics to avoid matrix inversion instabilities. AOE integrates with time-stepping frameworks through a three-phase strategy balancing data collection, updates, and surrogate usage, while requiring orders of magnitude less training data than conventional surrogate approaches. Demonstrated on a 1D atmospheric model of exoplanet GJ1214b, AOE achieves 11.1 times speedup (91% time reduction) across 200,000 timesteps while maintaining accuracy, potentially making previously intractable high-fidelity time-stepping simulations computationally feasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08012v2</guid>
      <category>physics.comp-ph</category>
      <category>astro-ph.IM</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tara P. A. Tahseen, Nikolaos Nikolaou, Lu\'is F. Sim\~oes, Kai Hou Yip, Jo\~ao M. Mendon\c{c}a, Ingo P. Waldmann</dc:creator>
    </item>
    <item>
      <title>Materials Informatics: Emergence To Autonomous Discovery In The Age Of AI</title>
      <link>https://arxiv.org/abs/2601.00742</link>
      <description>arXiv:2601.00742v2 Announce Type: replace 
Abstract: This perspective explores the evolution of materials informatics, from its foundational roots in physics and information theory to its maturation through artificial intelligence (AI). We trace the field's trajectory from early milestones to the transformative impact of the Materials Genome Initiative and the recent advent of large language models (LLMs). Rather than a mere toolkit, we present materials informatics as an evolving ecosystem, reviewing key methodologies such as Bayesian Optimization, Reinforcement Learning, and Transformers that drive inverse design and autonomous self-driving laboratories. We specifically address the practical challenges of LLM integration, comparing specialist versus generalist models and discussing solutions for uncertainty quantification. Looking forward, we assess the transition of AI from a predictive tool to a collaborative research partner. By leveraging active learning and retrieval-augmented generation (RAG), the field is moving toward a new era of autonomous materials science, increasingly characterized by "human-out-of-the-loop" discovery processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00742v2</guid>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/adma.202515941</arxiv:DOI>
      <dc:creator>Turab Lookman, YuJie Liu, Zhibin Gao</dc:creator>
    </item>
    <item>
      <title>Learning electromagnetic fields based on finite element basis functions</title>
      <link>https://arxiv.org/abs/2507.19255</link>
      <description>arXiv:2507.19255v2 Announce Type: replace-cross 
Abstract: Parametric surrogate models of electric machines are widely used for efficient design optimization and operational monitoring. Addressing geometry variations, spline-based computer-aided design representations play a pivotal role. In this study, we propose a novel approach that combines isogeometric analysis, proper orthogonal decomposition and deep learning to enable rapid and physically consistent predictions by directly learning spline basis coefficients. The effectiveness of this method is demonstrated using a parametric nonlinear magnetostatic model of a permanent magnet synchronous machine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19255v2</guid>
      <category>cs.CE</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TMAG.2025.3629546</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Magnetics, 2025</arxiv:journal_reference>
      <dc:creator>Merle Backmeyer, Michael Wiesheu, Sebastian Sch\"ops</dc:creator>
    </item>
    <item>
      <title>Low-dimensional semi-supervised latent Bayesian optimization for designing antimicrobial peptides</title>
      <link>https://arxiv.org/abs/2510.17569</link>
      <description>arXiv:2510.17569v2 Announce Type: replace-cross 
Abstract: Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat bacterial infections. Discovering and designing such peptides is difficult because of the vast number of possible sequences of amino acids. Deep generative models, such as variational autoencoders, have shown value in peptide design due to their ability to model sequence space with a continuous-valued latent space. Although such models have already been used to great effect in biomolecular design, they still suffer from a lack of interpretability and rigorous quantification of latent space quality as a search space. We investigate (1) whether searching through a dimensionally-reduced variant of the latent design space may facilitate optimization, (2) how organizing latent spaces with physicochemical properties may improve the efficiency of optimizing antimicrobial activity, and (3) the interpretability of the spaces. We find that employing a dimensionally-reduced version of the latent space is more interpretable and can be advantageous, while we can organize the latent space with different physicochemical properties even at different percentages of available labels. This work lays crucial groundwork for biophysically-motivated peptide design procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17569v2</guid>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jyler Menard, R. A. Mansbach</dc:creator>
    </item>
    <item>
      <title>Differential syntactic and semantic encoding in LLMs</title>
      <link>https://arxiv.org/abs/2601.04765</link>
      <description>arXiv:2601.04765v2 Announce Type: replace-cross 
Abstract: We study how syntactic and semantic information is encoded in inner layer representations of Large Language Models (LLMs), focusing on the very large DeepSeek-V3. We find that, by averaging hidden-representation vectors of sentences sharing syntactic structure or meaning, we obtain vectors that capture a significant proportion of the syntactic and semantic information contained in the representations. In particular, subtracting these syntactic and semantic ``centroids'' from sentence vectors strongly affects their similarity with syntactically and semantically matched sentences, respectively, suggesting that syntax and semantics are, at least partially, linearly encoded. We also find that the cross-layer encoding profiles of syntax and semantics are different, and that the two signals can to some extent be decoupled, suggesting differential encoding of these two types of linguistic information in LLM representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04765v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Santiago Acevedo, Alessandro Laio, Marco Baroni</dc:creator>
    </item>
  </channel>
</rss>
