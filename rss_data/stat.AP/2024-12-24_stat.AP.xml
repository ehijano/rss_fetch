<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Dec 2024 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>From pixels to parcels: flexible, practical small-area uncertainty estimation for spatial averages obtained from aboveground biomass maps</title>
      <link>https://arxiv.org/abs/2412.16403</link>
      <description>arXiv:2412.16403v1 Announce Type: new 
Abstract: Fine-resolution maps of forest aboveground biomass (AGB) effectively represent spatial patterns and can be flexibly aggregated to map subregions by computing spatial averages or totals of pixel-level predictions. However, generalized model-based uncertainty estimation for spatial aggregates requires computationally expensive processes like iterative bootstrapping and computing pixel covariances. Uncertainty estimation for map subregions is critical for enhancing practicality and eventual adoption of model-based data products, as this capability would empower users to produce estimates at scales most germane to management: individual forest stands and ownership parcels. In this study we produced estimates of standard error (SE) associated with spatial averages of AGB predictions for ownership parcels in New York State (NYS). This represents the first model-based uncertainty estimation study to include all four types of uncertainty (reference data, sample variability, residual variability, and auxiliary data), incorporate spatial autocorrelation of model residuals, and use methods compatible with algorithmic modeling. We found that uncertainty attributed to residual variance, largely resulting from spatial correlation of residuals, dominated all other sources for most parcels in the study. These results suggest that improvements to model accuracy will yield the greatest reductions to total uncertainty in regions like the northeastern and midwestern United States where forests are divided into smaller spatial units. Further, we demonstrated that log-log regression relating parcel characteristics (area, perimeter, AGB density, forest cover) to parcel-level SE can accurately estimate uncertainty for map subregions, thus providing a convenient means to empower map users. These findings support transparency in future regional-scale model-based forest carbon accounting and monitoring efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16403v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas K. Johnson, Grant M Domke, Stephen V Stehman, Michael J Mahoney, Colin M Beier</dc:creator>
    </item>
    <item>
      <title>Profile least squares estimation in networks with covariates</title>
      <link>https://arxiv.org/abs/2412.16298</link>
      <description>arXiv:2412.16298v1 Announce Type: cross 
Abstract: Many real world networks exhibit edge heterogeneity with different pairs of nodes interacting with different intensities. Further, nodes with similar attributes tend to interact more with each other. Thus, in the presence of observed node attributes (covariates), it is of interest to understand the extent to which these covariates explain interactions between pairs of nodes and to suitably estimate the remaining structure due to unobserved factors. For example, in the study of international relations, the extent to which country-pair specific attributes such as the number of material/verbal conflicts and volume of trade explain military alliances between different countries can lead to valuable insights. We study the model where pairwise edge probabilities are given by the sum of a linear edge covariate term and a residual term to model the remaining heterogeneity from unobserved factors. We approach estimation of the model via profile least squares and show how it leads to a simple algorithm to estimate the linear covariate term and the residual structure that is truly latent in the presence of observed covariates. Our framework lends itself naturally to a bootstrap procedure which is used to draw inference on model parameters, such as to determine significance of the homophily parameter or covariates in explaining the underlying network structure. Application to four real network datasets and comparisons using simulated data illustrate the usefulness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16298v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swati Chandna, Benjamin Bagozzi, Snigdhansu Chatterjee</dc:creator>
    </item>
    <item>
      <title>Generalizing causal effect estimates to larger populations while accounting for (uncertainty in) effect modifiers using a scaled Bayesian bootstrap with application to estimating the effect of family planning on employment in Nigeria</title>
      <link>https://arxiv.org/abs/2412.16320</link>
      <description>arXiv:2412.16320v1 Announce Type: cross 
Abstract: Strategies are needed to generalize causal effects from a sample that may differ systematically from the population of interest. In a motivating case study, interest lies in the causal effect of family planning on empowerment-related outcomes among urban Nigerian women, while estimates of this effect and its variation by covariates are available only from a sample of women in six Nigerian cities. Data on covariates in target populations are available from a complex sampling design survey. Our approach, analogous to the plug-in g-formula, takes the expectation of conditional average treatment effects from the source study over the covariate distribution in the target population. This method leverages generalizability literature from randomized trials, applied to a source study using principal stratification for identification. The approach uses a scaled Bayesian bootstrap to account for the complex sampling design. We also introduce checks for sensitivity to plausible departures of assumptions. In our case study, the average effect in the target population is higher than in the source sample based on point estimates and sensitivity analysis shows that a strong omitted effect modifier must be present in at least 40% of the target population for the 95% credible interval to include the null effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16320v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Godoy Garraza, Ilene Speizer, Leontine Alkema</dc:creator>
    </item>
    <item>
      <title>Optimizing Fintech Marketing: A Comparative Study of Logistic Regression and XGBoost</title>
      <link>https://arxiv.org/abs/2412.16333</link>
      <description>arXiv:2412.16333v1 Announce Type: cross 
Abstract: As several studies have shown, predicting credit risk is still a major concern for the financial services industry and is receiving a lot of scholarly interest. This area of study is crucial because it aids financial organizations in determining the probability that borrowers would default, which has a direct bearing on lending choices and risk management tactics. Despite the progress made in this domain, there is still a substantial knowledge gap concerning consumer actions that take place prior to the filing of credit card applications. The objective of this study is to predict customer responses to mail campaigns and assess the likelihood of default among those who engage. This research employs advanced machine learning techniques, specifically logistic regression and XGBoost, to analyze consumer behavior and predict responses to direct mail campaigns. By integrating different data preprocessing strategies, including imputation and binning, we enhance the robustness and accuracy of our predictive models. The results indicate that XGBoost consistently outperforms logistic regression across various metrics, particularly in scenarios using categorical binning and custom imputation. These findings suggest that XGBoost is particularly effective in handling complex data structures and provides a strong predictive capability in assessing credit risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16333v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sahar Yarmohammadtoosky Dinesh Chowdary Attota</dc:creator>
    </item>
    <item>
      <title>Learning Disease Progression Models That Capture Health Disparities</title>
      <link>https://arxiv.org/abs/2412.16406</link>
      <description>arXiv:2412.16406v1 Announce Type: cross 
Abstract: Disease progression models are widely used to inform the diagnosis and treatment of many progressive diseases. However, a significant limitation of existing models is that they do not account for health disparities that can bias the observed data. To address this, we develop an interpretable Bayesian disease progression model that captures three key health disparities: certain patient populations may (1) start receiving care only when their disease is more severe, (2) experience faster disease progression even while receiving care, or (3) receive follow-up care less frequently conditional on disease severity. We show theoretically and empirically that failing to account for disparities produces biased estimates of severity (underestimating severity for disadvantaged groups, for example). On a dataset of heart failure patients, we show that our model can identify groups that face each type of health disparity, and that accounting for these disparities meaningfully shifts which patients are considered high-risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16406v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erica Chiang, Divya Shanmugam, Ashley N. Beecy, Gabriel Sayer, Nir Uriel, Deborah Estrin, Nikhil Garg, Emma Pierson</dc:creator>
    </item>
    <item>
      <title>Visualizing Linear Prediction</title>
      <link>https://arxiv.org/abs/2412.16980</link>
      <description>arXiv:2412.16980v1 Announce Type: cross 
Abstract: Many statistics courses cover multiple linear regression, and present students with the formula of a prediction using the regressors, slopes, and an intercept. But is it really easy to see which terms have the largest effect, or to explain why the prediction of a specific case is unusually high or low? To assist with this the so-called grill plot is proposed. Its simplicity makes it easy to interpret, and it combines much information. Its main benefit is that it helps explainability of the linear formula as it is, without depending on how the formula was derived. The regressors can be numerical, categorical, or interaction terms, and the model can be linear or generalized linear. Another display is proposed to visualize correlations between predictors, in a way that is tailored for this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16980v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Peter J. Rousseeuw</dc:creator>
    </item>
    <item>
      <title>Three mechanistically different variability and noise sources in the trial-to-trial fluctuations of responses to brain stimulation</title>
      <link>https://arxiv.org/abs/2412.16997</link>
      <description>arXiv:2412.16997v1 Announce Type: cross 
Abstract: Motor-evoked potentials (MEPs) are among the few directly observable responses to external brain stimulation and serve a variety of applications, often in the form of input-output (IO) curves. Previous statistical models with two variability sources inherently consider the small MEPs at the low-side plateau as part of the neural recruitment properties. However, recent studies demonstrated that small MEP responses under resting conditions are contaminated and over-shadowed by background noise of mostly technical quality, e.g., caused by the amplifier, and suggested that the neural recruitment curve should continue below this noise level. This work intends to separate physiological variability from background noise and improve the description of recruitment behaviour. We developed a triple-variability-source model around a logarithmic logistic function without a lower plateau and incorporated an additional source for background noise. Compared to models with two or fewer variability sources, our approach better described IO characteristics, evidenced by lower Bayesian Information Criterion scores across all subjects and pulse shapes. The model independently extracted hidden variability information across the stimulated neural system and isolated it from background noise, which led to an accurate estimation of the IO curve parameters. This new model offers a robust tool to analyse brain stimulation IO curves in clinical and experimental neuroscience and reduces the risk of spurious results from inappropriate statistical methods. The presented model together with the corresponding calibration method provides a more accurate representation of MEP responses and variability sources, advances our understanding of cortical excitability, and may improve the assessment of neuromodulation effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16997v1</guid>
      <category>q-bio.NC</category>
      <category>eess.SP</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ke Ma, Siwei Liu, Mengjie Qin, Stefan Goetz</dc:creator>
    </item>
    <item>
      <title>Biomarker combination based on the Youden index with and without gold standard</title>
      <link>https://arxiv.org/abs/2412.17471</link>
      <description>arXiv:2412.17471v1 Announce Type: cross 
Abstract: In clinical practice, multiple biomarkers are often measured on the same subject for disease diagnosis, and combining them can improve diagnostic accuracy. Existing studies typically combine multiple biomarkers by maximizing the Area Under the ROC Curve (AUC), assuming a gold standard exists or that biomarkers follow a multivariate normal distribution. However, practical diagnostic settings require both optimal combination coefficients and an effective cutoff value, and the reference test may be imperfect. In this paper, we propose a two-stage method for identifying the optimal linear combination and cutoff value based on the Youden index. First, it maximizes an approximation of the empirical AUC to estimate the optimal linear coefficients for combining multiple biomarkers. Then, it maximizes the empirical Youden index to determine the optimal cutoff point for disease classification. Under the semiparametric single index model and regularity conditions, the estimators for the linear coefficients, cutoff point, and Youden index are consistent. This method is also applicable when the reference standard is imperfect. We demonstrate the performance of our method through simulations and apply it to construct a diagnostic scale for Chinese medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17471v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ao Sun, Yanting Li, Xiao-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>Bayesian Multilevel Bivariate Spatial Modelling of Italian School Data</title>
      <link>https://arxiv.org/abs/2412.17710</link>
      <description>arXiv:2412.17710v1 Announce Type: cross 
Abstract: This paper studies the relationship between the student's abilities in the second year of high school and the infrastructural endowment in all Italian municipalities, using spatial Bayesian modelling. Municipal student scores are obtained by averaging standardized and spatially homogeneous indicators of student outcomes provided by the Invalsi Institute for two subjects, Italian and Mathematics. Given the nature of the data, we employ a multilevel regression model assuming a bivariate Intrinsic Conditionally Autoregressive (ICAR) latent effect to explain the spatial variability and account for the correlation between the two subjects. Bayesian model estimation is obtained by the Integrated Nested Laplace Approximation (INLA), implemented in the \texttt{R-INLA} package. We find that alongside a significant association with the current state of school infrastructure and facilities, spatially structured latent effects are still necessary to explain the different student outcomes across municipalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17710v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonardo Cefalo, Alessio Pollice, Virgilio G\'omez-Rubio</dc:creator>
    </item>
    <item>
      <title>Minimax Optimal Simple Regret in Two-Armed Best-Arm Identification</title>
      <link>https://arxiv.org/abs/2412.17753</link>
      <description>arXiv:2412.17753v1 Announce Type: cross 
Abstract: This study investigates an asymptotically minimax optimal algorithm in the two-armed fixed-budget best-arm identification (BAI) problem. Given two treatment arms, the objective is to identify the arm with the highest expected outcome through an adaptive experiment. We focus on the Neyman allocation, where treatment arms are allocated following the ratio of their outcome standard deviations. Our primary contribution is to prove the minimax optimality of the Neyman allocation for the simple regret, defined as the difference between the expected outcomes of the true best arm and the estimated best arm. Specifically, we first derive a minimax lower bound for the expected simple regret, which characterizes the worst-case performance achievable under the location-shift distributions, including Gaussian distributions. We then show that the simple regret of the Neyman allocation asymptotically matches this lower bound, including the constant term, not just the rate in terms of the sample size, under the worst-case distribution. Notably, our optimality result holds without imposing locality restrictions on the distribution, such as the local asymptotic normality. Furthermore, we demonstrate that the Neyman allocation reduces to the uniform allocation, i.e., the standard randomized controlled trial, under Bernoulli distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17753v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>On the Temporal-spatial Analysis of Estimating Urban Traffic Patterns Via GPS Trace Data of Car-hailing Vehicles</title>
      <link>https://arxiv.org/abs/2306.07456</link>
      <description>arXiv:2306.07456v2 Announce Type: replace 
Abstract: Car-hailing services have become a prominent data source for urban traffic studies. Extracting useful information from car-hailing trace data is essential for effective traffic management, while discrepancies between car-hailing vehicles and urban traffic should be considered. This paper proposes a generic framework for estimating and analyzing urban traffic patterns using car-hailing trace data. The framework consists of three layers: the data layer, the interactive software layer, and the processing method layer. By pre-processing car-hailing GPS trace data with operations such as data cutting, map matching, and trace correction, the framework generates tensor matrices that estimate traffic patterns for car-hailing vehicle flow and average road speed. An analysis block based on these matrices examines the relationships and differences between car-hailing vehicles and urban traffic patterns, which have been overlooked in previous research. Experimental results demonstrate the effectiveness of the proposed framework in examining temporal-spatial patterns of car-hailing vehicles and urban traffic. For temporal analysis, urban road traffic displays a bimodal characteristic while car-hailing flow exhibits a 'multi-peak' pattern, fluctuating significantly during holidays and thus generating a hierarchical structure. For spatial analysis, the heat maps generated from the matrices exhibit certain discrepancies, but the spatial distribution of hotspots and vehicle aggregation areas remains similar.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.07456v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiannan Mao, Lan Liu, Hao Huang, Weike Lu, Kaiyu Yang, Tianli Tang, Haotian Shi</dc:creator>
    </item>
    <item>
      <title>Monitoring road infrastructures from satellite images in Greater Maputo</title>
      <link>https://arxiv.org/abs/2409.06406</link>
      <description>arXiv:2409.06406v2 Announce Type: replace 
Abstract: The information about pavement surface type is rarely available in road network databases of developing countries although it represents a cornerstone of the design of efficient mobility systems. This research develops an automatic classification pipeline for road pavement which makes use of satellite images to recognize road segments as paved or unpaved. The proposed methodology is based on an object-oriented approach, so that each road is classified by looking at the distribution of its pixels in the RGB space. The proposed approach is proven to be accurate, inexpensive, and readily replicable in other cities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06406v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s10260-024-00772-y</arxiv:DOI>
      <dc:creator>Arianna Burzacchi, Matteo Landr\`o, Simone Vantini</dc:creator>
    </item>
    <item>
      <title>Scaling Methods To Estimate Macroscopic Fundamental Diagrams in Urban Networks with Sparse Sensor Coverage</title>
      <link>https://arxiv.org/abs/2411.19721</link>
      <description>arXiv:2411.19721v2 Announce Type: replace 
Abstract: Accurately estimating traffic variables across unequipped portions of a network remains a significant challenge due to the limited coverage of sensor-equipped links, such as loop detectors and probe vehicles. A common approach is to apply uniform scaling, treating unequipped links as equivalent to equipped ones. This study introduces a novel framework to improve traffic variable estimation by integrating statistical scaling methods with geospatial imputation techniques. Two main approaches are proposed: (1) Statistical Scaling, which includes hierarchical and non-hierarchical network approaches, and (2) Geospatial Imputation, based on variogram modeling. The hierarchical scaling method categorizes the network into several levels according to spatial and functional characteristics, applying tailored scaling factors to each category. In contrast, the non-hierarchical method uses a uniform scaling factor across all links, ignoring network heterogeneity. The variogram-based geospatial imputation leverages spatial correlations to estimate traffic variables for unequipped links, capturing spatial dependencies in urban road networks. Validation results indicate that the hierarchical scaling approach provides the most accurate estimates, achieving reliable performance even with as low as 5% uniform detector coverage. Although the variogram-based method yields strong results, it is slightly less effective than the hierarchical scaling approach but outperforms the non-hierarchical method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19721v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nandan Maiti, Manon Seppecher, Ludovic Leclercq</dc:creator>
    </item>
    <item>
      <title>Geographic distribution of the global agricultural workforce every decade for the years 2000-2100</title>
      <link>https://arxiv.org/abs/2412.15841</link>
      <description>arXiv:2412.15841v2 Announce Type: replace 
Abstract: Agricultural workers play a vital role in the global economy and food security by cultivating, transporting, and processing food for populations worldwide. Despite their importance, detailed spatial data on the global agricultural workforce have remained scarce. Here, we present a new gridded dataset that maps the global distribution of agricultural workers for every decade over the years 2000-2100, distributed at 0.083$\times$0.083 degrees resolution, roughly $\sim$10km$\times$10km at the Equator. The dataset is developed using an empirical modeling framework relying on generalized additive mixed models (GAMMs) that integrate socioeconomic variables, including gross domestic product per capita, total population, rural population size, and agricultural land use. The predictions are consistent with Shared Socio-economic Pathways and we distribute full time series data for all SSPs 1 to 5. This dataset opens new avenues for future research on labour force health, productivity and risk, and could be very useful for developing informed, forward-looking strategies that address the challenges of climate resilience in agriculture. The dataset and code for reproducing it are available for the user community [publicly available on publication at DOI: 10.5281/zenodo.14443333].</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15841v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Naia Ormaza-Zulueta, Steve Miller, Zia Mehrabi</dc:creator>
    </item>
    <item>
      <title>Nested Dirichlet models for unsupervised attack pattern detection in honeypot data</title>
      <link>https://arxiv.org/abs/2301.02505</link>
      <description>arXiv:2301.02505v4 Announce Type: replace-cross 
Abstract: Cyber-systems are under near-constant threat from intrusion attempts. Attacks types vary, but each attempt typically has a specific underlying intent, and the perpetrators are typically groups of individuals with similar objectives. Clustering attacks appearing to share a common intent is very valuable to threat-hunting experts. This article explores Dirichlet distribution topic models for clustering terminal session commands collected from honeypots, which are special network hosts designed to entice malicious attackers. The main practical implications of clustering the sessions are two-fold: finding similar groups of attacks, and identifying outliers. A range of statistical models are considered, adapted to the structures of command-line syntax. In particular, concepts of primary and secondary topics, and then session-level and command-level topics, are introduced into the models to improve interpretability. The proposed methods are further extended in a Bayesian nonparametric fashion to allow unboundedness in the vocabulary size and the number of latent intents. The methods are shown to discover an unusual MIRAI variant which attempts to take over existing cryptocurrency coin-mining infrastructure, not detected by traditional topic-modelling approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.02505v4</guid>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Sanna Passino, Anastasia Mantziou, Daniyar Ghani, Philip Thiede, Ross Bevington, Nicholas A. Heard</dc:creator>
    </item>
    <item>
      <title>Quantile balancing inverse probability weighting for non-probability samples</title>
      <link>https://arxiv.org/abs/2403.09726</link>
      <description>arXiv:2403.09726v3 Announce Type: replace-cross 
Abstract: The use of non-probability data sources for statistical purposes has become increasingly popular in recent years, also in official statistics. However, statistical inference based on non-probability samples is made more difficult by nature of them being biased and not representative of the target population. In this paper we propose quantile balancing inverse probability weighting estimator (QBIPW) for non-probability samples. We use the idea of Harms and Duchesne (2006) which allows to include quantile information in the estimation process so known totals and distribution for auxiliary variables are being reproduced. We discuss the estimation of the QBIPW probabilities and its variance. Our simulation study has demonstrated that the proposed estimators are robust against model mis-specification and, as a result, help to reduce bias and mean squared error. Finally, we applied the proposed methods to estimate the share of vacancies aimed at Ukrainian workers in Poland using an integrated set of administrative and survey data about job vacancies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09726v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maciej Ber\k{e}sewicz, Marcin Szymkowiak, Piotr Chlebicki</dc:creator>
    </item>
    <item>
      <title>Improving probabilistic forecasts of extreme wind speeds by training statistical post-processing models with weighted scoring rules</title>
      <link>https://arxiv.org/abs/2407.15900</link>
      <description>arXiv:2407.15900v3 Announce Type: replace-cross 
Abstract: Accurate forecasts of extreme wind speeds are of high importance for many applications. Such forecasts are usually generated by ensembles of numerical weather prediction (NWP) models, which however can be biased and have errors in dispersion, thus necessitating the application of statistical post-processing techniques. In this work we aim to improve statistical post-processing models for probabilistic predictions of extreme wind speeds. We do this by adjusting the training procedure used to fit ensemble model output statistics (EMOS) models - a commonly applied post-processing technique - and propose estimating parameters using the so-called threshold-weighted continuous ranked probability score (twCRPS), a proper scoring rule that places special emphasis on predictions over a threshold. We show that training using the twCRPS leads to improved extreme event performance of post-processing models for a variety of thresholds. We find a distribution body-tail trade-off where improved performance for probabilistic predictions of extreme events comes with worse performance for predictions of the distribution body. However, we introduce strategies to mitigate this trade-off based on weighted training and linear pooling. Finally, we consider some synthetic experiments to explain the training impact of the twCRPS and derive closed-form expressions of the twCRPS for a number of distributions, giving the first such collection in the literature. The results will enable researchers and practitioners alike to improve the performance of probabilistic forecasting models for extremes and other events of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15900v3</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jakob Benjamin Wessel, Christopher A. T. Ferro, Gavin R. Evans, Frank Kwasniok</dc:creator>
    </item>
    <item>
      <title>Improved order selection method for hidden Markov models: a case study with movement data</title>
      <link>https://arxiv.org/abs/2411.18826</link>
      <description>arXiv:2411.18826v2 Announce Type: replace-cross 
Abstract: Hidden Markov models (HMMs) are a versatile statistical framework commonly used in ecology to characterize behavioural patterns from animal movement data. In HMMs, the observed data depend on a finite number of underlying hidden states, generally interpreted as the animal's unobserved behaviour. The number of states is a crucial parameter, controlling the trade-off between ecological interpretability of behaviours (fewer states) and the goodness of fit of the model (more states). Selecting the number of states, commonly referred to as order selection, is notoriously challenging. Common model selection metrics, such as AIC and BIC, often perform poorly in determining the number of states, particularly when models are misspecified. Building on existing methods for HMMs and mixture models, we propose a double penalized likelihood maximum estimate (DPMLE) for the simultaneous estimation of the number of states and parameters of non-stationary HMMs. The DPMLE differs from traditional information criteria by using two penalty functions on the stationary probabilities and state-dependent parameters. For non-stationary HMMs, forward and backward probabilities are used to approximate stationary probabilities. Using a simulation study that includes scenarios with additional complexity in the data, we compare the performance of our method with that of AIC and BIC. We also illustrate how the DPMLE differs from AIC and BIC using narwhal (Monodon monoceros) movement data. The proposed method outperformed AIC and BIC in identifying the correct number of states under model misspecification. Furthermore, its capacity to handle non-stationary dynamics allowed for more realistic modeling of complex movement data, offering deeper insights into narwhal behaviour. Our method is a powerful tool for order selection in non-stationary HMMs, with potential applications extending beyond the field of ecology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18826v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fanny Dupont, Marianne Marcoux, Nigel Hussey, Marie Auger-M\'eth\'e</dc:creator>
    </item>
  </channel>
</rss>
