<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Dec 2025 05:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An O(1) Space Algorithm for N-Dimensional Tensor Rotation: A Generalization of the Reversal Method</title>
      <link>https://arxiv.org/abs/2512.00111</link>
      <description>arXiv:2512.00111v1 Announce Type: new 
Abstract: The rotation of multi-dimensional arrays, or tensors, is a fundamental operation in computer science with applications ranging from data processing to scientific computing. While various methods exist, achieving this rotation in-place (i.e., with O(1) auxiliary space) presents a significant algorithmic challenge. The elegant three-reversal algorithm provides a well-known O(1) space solution for one-dimensional arrays. This paper introduces a generalization of this method to N dimensions, resulting in the "$2^n+1$ reversal algorithm". This algorithm achieves in-place tensor rotation with O(1) auxiliary space and a time complexity linear in the number of elements. We provide a formal definition for N-dimensional tensor reversal, present the algorithm with detailed pseudocode, and offer a rigorous proof of its correctness, demonstrating that the pattern observed in one dimension ($2^1+1=3$ reversals) and two dimensions ($2^2+1=5$ reversals) holds for any arbitrary number of dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00111v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dexin Chen</dc:creator>
    </item>
    <item>
      <title>Maximum-Flow and Minimum-Cut Sensitivity Oracles for Directed Graphs</title>
      <link>https://arxiv.org/abs/2512.00153</link>
      <description>arXiv:2512.00153v1 Announce Type: new 
Abstract: Given a digraph $G = (V, E)$ with a designated source $s$, sink $t$, and an $(s,t)$-max-flow of value $\lambda$, we present constructions for max-flow and min-cut sensitivity oracles, and introduce the concept of a fault-tolerant flow family, which may be of independent interest. Our main contributions are as follows.
  1. Fault-Tolerant Flow Family: For any graph $G$ with $(s,t)$-max-flow value $\lambda$, we construct a family $B$ of $2\lambda+1$ $(s,t)$-flows such that for every edge $e$, $B$ contains an $(s,t)$-max-flow of $G-e$.
  2. Max-Flow Sensitivity Oracle: We construct a single as well as dual-edge sensitivity oracle for $(s,t)$-max-flow that requires only $O(\lambda n)$ space. Given any set $F$ of up to two failing edges, the oracle reports the updated max-flow value in $G-F$ in $O(n)$ time. Additionally, for the single-failure case, the oracle can determine in constant time whether the flow through an edge $x$ changes when another edge $e$ fails.
  3. Min-Cut Sensitivity Oracle for Dual Failures: Recently, Baswana et al. (ICALP'22) designed an $O(n^2)$-sized oracle for answering $(s,t)$-min-cut size queries under dual edge failures in constant time. We extend this by focusing on graphs with small min-cut values $\lambda$, and present a more compact oracle of size $O(\lambda n)$ that answers such min-cut size queries in constant time and reports the corresponding $(s,t)$-min-cut partition in $O(n)$ time.
  4. Min-Cut Sensitivity Oracle for Multiple Failures: We extend our results to the general case of $k$ edge failures. For any graph with $(s,t)$-min-cut of size $\lambda$, we construct a $k$-fault-tolerant min-cut oracle with space complexity $O_{\lambda,k}(n \log n)$ that answers min-cut size queries in $O_{\lambda,k}(\log n)$ time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00153v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mridul Ahi, Keerti Choudhary, Shlok Pande,  Pushpraj, Lakshay Saggi</dc:creator>
    </item>
    <item>
      <title>Approximating Directed Connectivity in Almost-Linear Time</title>
      <link>https://arxiv.org/abs/2512.00176</link>
      <description>arXiv:2512.00176v1 Announce Type: new 
Abstract: We present randomized algorithms that compute $(1+\epsilon)$-approximate minimum global edge and vertex cuts in weighted directed graphs in $O(\log^4(n) / \epsilon)$ and $O(\log^5(n)/\epsilon)$ single-commodity flows, respectively. With the almost-linear time flow algorithm of [CKL+22], this gives almost linear time approximation schemes for edge and vertex connectivity. By setting $\epsilon$ appropriately, this also gives faster exact algorithms for small vertex connectivity.
  At the heart of these algorithms is a divide-and-conquer technique called "shrink-wrapping" for a certain well-conditioned rooted Steiner connectivity problem. Loosely speaking, for a root $r$ and a set of terminals, shrink-wrapping uses flow to certify the connectivity from a root $r$ to some of the terminals, and for the remaining uncertified terminals, generates an $r$-cut where the sink component both (a) contains the sink component of the minimum $(r,t)$-cut for each uncertified terminal $t$ and (b) has size proportional to the number of uncertified terminals. This yields a divide-and-conquer scheme over the terminals where we can divide the set of terminals and compute their respective minimum $r$-cuts in smaller, contracted subgraphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00176v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kent Quanrud</dc:creator>
    </item>
    <item>
      <title>Expected Cost Analysis of Online Facility Assignment on Regular Polygons</title>
      <link>https://arxiv.org/abs/2512.00506</link>
      <description>arXiv:2512.00506v1 Announce Type: new 
Abstract: This paper analyzes the online facility assignment problem in a geometric setting where facilities with unit capacity are positioned at the vertices of a regular $n$-gon. Customers arrive sequentially at uniformly random positions along the edges. They must be assigned immediately to the nearest available facility, with ties broken by coin toss. The sequential nature and unknown future arrivals require a probabilistic analysis of the expected assignment cost. Our main contribution is a recursive characterization of the expected cost: for any occupancy state $S$, the expected remaining cost $V(S)$ equals the average over all edge positions of the immediate assignment cost plus the expected future cost after assignment. We prove that this integral equation can calculate a solution and provide the expected value for small $n$ ($n = 3, 4, 5$). For larger values of $n$ and expected cost, we develop efficient numerical methods, including a discretized dynamic programming approach and Monte Carlo simulation. The work establishes a fundamental probabilistic approach for online assignment in polygonal environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00506v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Rawha Siddiqi Riad, Md Manzurul Hasan</dc:creator>
    </item>
    <item>
      <title>Perfect $L_p$ Sampling with Polylogarithmic Update Time</title>
      <link>https://arxiv.org/abs/2512.00632</link>
      <description>arXiv:2512.00632v1 Announce Type: new 
Abstract: Perfect $L_p$ sampling in a stream was introduced by Jayaram and Woodruff (FOCS 2018) as a streaming primitive which, given turnstile updates to a vector $x \in \{-\text{poly}(n), \ldots, \text{poly}(n)\}^n$, outputs an index $i^* \in \{1, 2, \ldots, n\}$ such that the probability of returning index $i$ is exactly \[\Pr[i^* = i] = \frac{|x_i|^p}{\|x\|_p^p} \pm \frac{1}{n^C},\] where $C &gt; 0$ is an arbitrarily large constant. Jayaram and Woodruff achieved the optimal $\tilde{O}(\log^2 n)$ bits of memory for $0 &lt; p &lt; 2$, but their update time is at least $n^C$ per stream update. Thus an important open question is to achieve efficient update time while maintaining optimal space. For $0 &lt; p &lt; 2$, we give the first perfect $L_p$-sampler with the same optimal amount of memory but with only $\text{poly}(\log n)$ update time. Crucial to our result is an efficient simulation of a sum of reciprocals of powers of truncated exponential random variables by approximating its characteristic function, using the Gil-Pelaez inversion formula, and applying variants of the trapezoid formula to quickly approximate it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00632v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Swartworth, David P. Woodruff, Samson Zhou</dc:creator>
    </item>
    <item>
      <title>A Fast Algorithm for Finding Minimum Weight Cycles in Mining Cyclic Graph Topologies</title>
      <link>https://arxiv.org/abs/2512.01049</link>
      <description>arXiv:2512.01049v1 Announce Type: new 
Abstract: Cyclic structures are fundamental topological features in graphs, playing critical roles in network robustness, information flow, community structure, and various dynamic processes. Algorithmic tools that can efficiently probe and analyze these cyclic topologies are increasingly vital for tasks in graph mining, network optimization, bioinformatics, and social network analysis. A core primitive for quantitative analysis of cycles is finding the Minimum Weight Cycle (MWC), representing the shortest cyclic path in a weighted graph. However, computing the MWC efficiently remains a challenge, particularly compared to shortest path computations. This paper introduces a novel deterministic algorithm for finding the MWC in general weighted graphs. Our approach adapts the structure of Dijkstra's algorithm by introducing and minimizing a \textit{composite distance} metric, effectively translating the global cycle search into an iterative node-centric optimization. We provide a rigorous proof of correctness based on loop invariants. We detail two mechanisms for accelerating the search: a provable node discarding technique based on intermediate results, and a highly effective graph pruning heuristic. This heuristic dynamically restricts the search to relevant subgraphs, leveraging the principle of locality often present in complex networks to achieve significant empirical speedups, while periodic resets ensure global optimality is maintained. The efficiency of the proposed MWC algorithm enables its use as a core component in more complex analyses focused on cyclic properties. We illustrate this through a detailed application case study: accelerating the computation of the Loop Modulus, a measure of cycle richness used in advanced network characterization. Our algorithm dramatically reduces the runtime of the iterative constraint-finding bottleneck in this computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01049v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heman Shakeri, Torben Amtoft, Behnaz Moradi-Jamei, Nathan Albin, Pietro Poggi-Corradini</dc:creator>
    </item>
    <item>
      <title>Beware of the Classical Benchmark Instances for the Traveling Salesman Problem with Time Windows</title>
      <link>https://arxiv.org/abs/2512.01064</link>
      <description>arXiv:2512.01064v1 Announce Type: new 
Abstract: We propose a simple and exact informed search method for the Traveling Salesman Problem with Time Windows and Makespan objective (TSPTW-M) that solves all instances of the classical benchmark with 50 or more customers in less than ten seconds each. Applying this algorithm as an off-the-shelf method, we also solve all but one of these instances for the Duration objective. Our main conclusion is that these instances should no longer be employed for evaluating the TSPTW-M and its Duration variant: they can be ``hacked'' to yield results that seem outstanding at first sight. Additionally, caution is advised when designing hard training sets for machine learning algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01064v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francisco J. Soulignac</dc:creator>
    </item>
    <item>
      <title>A practical algorithm for 3-admissibility</title>
      <link>https://arxiv.org/abs/2512.01121</link>
      <description>arXiv:2512.01121v1 Announce Type: new 
Abstract: The $3$-admissibility of a graph is a promising measure to identify real-world networks that have an algorithmically favourable structure.
  We design an algorithm that decides whether the $3$-admissibility of an input graph~$G$ is at most~$p$ in time~\runtime and space~\memory, where $m$ is the number of edges in $G$ and $n$ the number of vertices. To the best of our knowledge, this is the first explicit algorithm to compute the $3$-admissibility.
  The linear dependence on the input size in both time and space complexity, coupled with an `optimistic' design philosophy for the algorithm itself, makes this algorithm practicable, as we demonstrate with an experimental evaluation on a corpus of \corpussize real-world networks.
  Our experimental results show, surprisingly, that the $3$-admissibility of most real-world networks is not much larger than the $2$-admissibility, despite the fact that the former has better algorithmic properties than the latter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01121v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christine Awofeso, Patrick Greaves, Oded Lachish, Felix Reidl</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Sparsifiers for Stochastic Knapsack and Assignment Problems</title>
      <link>https://arxiv.org/abs/2512.01240</link>
      <description>arXiv:2512.01240v1 Announce Type: new 
Abstract: When uncertainty meets costly information gathering, a fundamental question emerges: which data points should we probe to unlock near-optimal solutions? Sparsification of stochastic packing problems addresses this trade-off. The existing notions of sparsification measure the level of sparsity, called degree, as the ratio of queried items to the optimal solution size. While effective for matching and matroid-type problems with uniform structures, this cardinality-based approach fails for knapsack-type constraints where feasible sets exhibit dramatic structural variation. We introduce a polyhedral sparsification framework that measures the degree as the smallest scalar needed to embed the query set within a scaled feasibility polytope, naturally capturing redundancy without relying on cardinality.
  Our main contribution establishes that knapsack, multiple knapsack, and generalized assignment problems admit (1 - epsilon)-approximate sparsifiers with degree polynomial in 1/p and 1/epsilon -- where p denotes the independent activation probability of each element -- remarkably independent of problem dimensions. The key insight involves grouping items with similar weights and deploying a charging argument: when our query set misses an optimal item, we either substitute it with a queried item from the same group or leverage that group's excess contribution to compensate for the loss. This reveals an intriguing complexity-theoretic separation -- while the multiple knapsack problem lacks an FPTAS and generalized assignment is APX-hard, their sparsification counterparts admit efficient (1 - epsilon)-approximation algorithms that identify polynomial-degree query sets. Finally, we raise an open question: can such sparsification extend to general integer linear programs with degree independent of problem dimensions?</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01240v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaddin Dughmi, Yusuf Hakan Kalayci, Xinyu Liu</dc:creator>
    </item>
    <item>
      <title>Separator Theorem for Minor-Free Graphs in Linear Time</title>
      <link>https://arxiv.org/abs/2512.01587</link>
      <description>arXiv:2512.01587v1 Announce Type: new 
Abstract: The planar separator theorem by Lipton and Tarjan [FOCS '77, SIAM Journal on Applied Mathematics '79] states that any planar graph with $n$ vertices has a balanced separator of size $O(\sqrt{n})$ that can be found in linear time. This landmark result kicked off decades of research on designing linear or nearly linear-time algorithms on planar graphs. In an attempt to generalize Lipton-Tarjan's theorem to nonplanar graphs, Alon, Seymour, and Thomas [STOC '90, Journal of the AMS '90] showed that any minor-free graph admits a balanced separator of size $O(\sqrt{n})$ that can be found in $O(n^{3/2})$ time. The superlinear running time in their separator theorem is a key bottleneck for generalizing algorithmic results from planar to minor-free graphs. Despite extensive research for more than two decades, finding a balanced separator of size $O(\sqrt{n})$ in (linear) $O(n)$ time for minor-free graphs remains a major open problem. Known algorithms either give a separator of size much larger than $O(\sqrt{n})$ or have superlinear running time, or both.
  In this paper, we answer the open problem affirmatively. Our algorithm is very simple: it runs a vertex-weighted variant of breadth-first search (BFS) a constant number of times on the input graph. Our key technical contribution is a weighting scheme on the vertices to guide the search for a balanced separator, offering a new connection between the size of a balanced separator and the existence of a clique-minor model. We believe that our weighting scheme may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01587v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\'Edouard Bonnet, Tuukka Korhonen, Hung Le, Jason Li, Tom\'a\v{s} Masa\v{r}\'ik</dc:creator>
    </item>
    <item>
      <title>JFR: An Efficient Jump Frontier Relaxation Strategy for Bellman-Ford</title>
      <link>https://arxiv.org/abs/2512.01802</link>
      <description>arXiv:2512.01802v1 Announce Type: new 
Abstract: We propose JFR, a Bellman-Ford-based optimization framework leveraging frontier contraction and abstract multi-hop jump propagation to accelerate shortest-path computation while strictly preserving correctness. JFR achieves substantial reductions in relaxation operations, ranging from 25 to 99 percent, across sparse, dense, and negative-edge graphs, ensuring robust performance even under adversarial or highly connected topologies. On ultra-large graphs with up to N=20,000 nodes and 295 million edges, JFR maintains strong operational reductions and comparable or improved runtime relative to SPFA-SLF, demonstrating consistent robustness across graph size and density. Lower relaxation counts imply reduced memory-access overheads and computational effort; this normalized work reduction highlights JFR's suitability for scenarios requiring high throughput or energy-conscious operation. Future work focuses on integrating high-performance queue structures, adaptive frontier strategies, and cache-aware techniques to further reduce constant-factor overheads and fully realize JFR's practical runtime potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01802v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Wang, Xi Chen</dc:creator>
    </item>
    <item>
      <title>Tight Bounds for Feedback Vertex Set Parameterized by Clique-width</title>
      <link>https://arxiv.org/abs/2512.01900</link>
      <description>arXiv:2512.01900v1 Announce Type: new 
Abstract: We introduce a new notion of acyclicity representation in labeled graphs, and present three applications thereof. Our main result is an algorithm that, given a graph $G$ and a $k$-clique expression of $G$, in time $O(6^kn^c)$ counts modulo $2$ the number of feedback vertex sets of $G$ of each size. We achieve this through an involved subroutine for merging partial solutions at union nodes in the expression. In the usual way this results in a one-sided error Monte-Carlo algorithm for solving the decision problem in the same time. We complement these by a matching lower bound under the Strong Exponential-Time Hypothesis (SETH). This closes an open question that appeared multiple times in the literature [ESA 23, ICALP 24, IPEC 25].
  We also present an algorithm that, given a graph $G$ and a tree decomposition of width $k$ of $G$, in time $O(3^kn^c)$ counts modulo $2$ the number of feedback vertex sets of $G$ of each size. This matches the known SETH-tight bound for the decision version, which was obtained using the celebrated cut-and-count technique [FOCS 11, TALG 22]. Unlike other applications of cut-and-count, which use the isolation lemma to reduce a decision problem to counting solutions modulo $2$, this bound was obtained via counting other objects, leaving the complexity of counting solutions modulo $2$ open.
  Finally, we present a one-sided error Monte-Carlo algorithm that, given a graph $G$ and a $k$-clique expression of $G$, in time $O(18^kn^c)$ decides the existence of a connected feedback vertex set of size $b$ in $G$. We provide a matching lower bound under SETH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01900v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Narek Bojikian, Stefan Kratsch</dc:creator>
    </item>
    <item>
      <title>Adaptive Matrix Sparsification and Applications to Empirical Risk Minimization</title>
      <link>https://arxiv.org/abs/2512.02003</link>
      <description>arXiv:2512.02003v1 Announce Type: new 
Abstract: Consider the empirical risk minimization (ERM) problem, which is stated as follows. Let $K_1, \dots, K_m$ be compact convex sets with $K_i \subseteq \mathbb{R}^{n_i}$ for $i \in [m]$, $n = \sum_{i=1}^m n_i$, and $n_i\le C_K$ for some absolute constant $C_K$. Also, consider a matrix $A \in \mathbb{R}^{n \times d}$ and vectors $b \in \mathbb{R}^d$ and $c \in \mathbb{R}^n$. Then the ERM problem asks to find \[ \min_{\substack{x \in K_1 \times \dots \times K_m\\ A^\top x = b}}
  c^\top x. \] We give an algorithm to solve this to high accuracy in time $\widetilde{O}(nd + d^6\sqrt{n}) \le \widetilde{O} (nd + d^{11})$, which is nearly-linear time in the input size when $A$ is dense and $n \ge d^{10}$.
  Our result is achieved by implementing an $\widetilde{O}(\sqrt{n})$-iteration interior point method (IPM) efficiently using dynamic data structures. In this direction, our key technical advance is a new algorithm for maintaining leverage score overestimates of matrices undergoing row updates. Formally, given a matrix $A \in \mathbb{R}^{n \times d}$ undergoing $T$ batches of row updates of total size $n$ we give an algorithm which can maintain leverage score overestimates of the rows of $A$ summing to $\widetilde{O}(d)$ in total time $\widetilde{O}(nd + Td^6)$. This data structure is used to sample a spectral sparsifier within a robust IPM framework to establish the main result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02003v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang P. Liu, Richard Peng, Colin Tang, Albert Weng, Junzhao Yang</dc:creator>
    </item>
    <item>
      <title>Efficient Turing Machine Simulation with Transformers</title>
      <link>https://arxiv.org/abs/2512.00003</link>
      <description>arXiv:2512.00003v1 Announce Type: cross 
Abstract: Constant bit-size Transformers are known to be Turing complete, but existing constructions require $\Omega(s(n))$ chain-of-thought (CoT) steps per simulated Turing machine (TM) step, leading to impractical reasoning lengths. In this paper, we significantly reduce this efficiency gap by proving that any $(t(n),s(n))$-bounded multi-tape TM can be simulated by a constant bit-size Transformer with an optimal $O(s(n))$-long context window and only $O(s(n)^c)$ CoT steps per TM step, where $c&gt;0$ can be made arbitrarily small by letting the Transformers' head-layer product sufficiently large. In addition, our construction shows that sparse attention with fixed geometric offsets suffices for efficient universal computation. Our proof leverages multi-queue TMs as a bridge. The main technical novelty is a more efficient simulation of multi-tape TMs by synchronous multi-queue TMs, improving both time and space complexity under stricter model assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00003v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qian Li, Yuyi Wang</dc:creator>
    </item>
    <item>
      <title>The Information Theory of Similarity</title>
      <link>https://arxiv.org/abs/2512.00378</link>
      <description>arXiv:2512.00378v1 Announce Type: cross 
Abstract: We establish a precise mathematical equivalence between witness-based similarity systems (REWA) and Shannon's information theory. We prove that witness overlap is mutual information, that REWA bit complexity bounds arise from channel capacity limitations, and that ranking-preserving encodings obey rate-distortion constraints. This unification reveals that fifty years of similarity search research -- from Bloom filters to locality-sensitive hashing to neural retrieval -- implicitly developed information theory for relational data. We derive fundamental lower bounds showing that REWA's $O(\Delta^{-2} \log N)$ complexity is optimal: no encoding scheme can preserve similarity rankings with fewer bits. The framework establishes that semantic similarity has physical units (bits of mutual information), search is communication (query transmission over a noisy channel), and retrieval systems face fundamental capacity limits analogous to Shannon's channel coding theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00378v1</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikit Phadke</dc:creator>
    </item>
    <item>
      <title>Efficiently Learning Branching Networks for Multitask Algorithmic Reasoning</title>
      <link>https://arxiv.org/abs/2512.01113</link>
      <description>arXiv:2512.01113v1 Announce Type: cross 
Abstract: Algorithmic reasoning -- the ability to perform step-by-step logical inference -- has become a core benchmark for evaluating reasoning in graph neural networks (GNNs) and large language models (LLMs). Ideally, one would like to design a single model capable of performing well on multiple algorithmic reasoning tasks simultaneously. However, this is challenging when the execution steps of algorithms differ from one another, causing negative interference when they are trained together.
  We propose branching neural networks, a principled architecture for multitask algorithmic reasoning. Searching for the optimal $k$-ary tree with $L$ layers over $n$ algorithmic tasks is combinatorial, requiring exploration of up to $k^{nL}$ possible structures. We develop AutoBRANE, an efficient algorithm that reduces this search to $O(nL)$ time by solving a convex relaxation at each layer to approximate an optimal task partition. The method clusters tasks using gradient-based affinity scores and can be used on top of any base model, including GNNs and LLMs.
  We validate AutoBRANE on a broad suite of graph-algorithmic and text-based reasoning benchmarks. We show that gradient features estimate true task performance within 5% error across four GNNs and four LLMs (up to 34B parameters). On the CLRS benchmark, it outperforms the strongest single multitask GNN by 3.7% and the best baseline by 1.2%, while reducing runtime by 48% and memory usage by 26%. The learned branching structures reveal an intuitively reasonable hierarchical clustering of related algorithms. On three text-based graph reasoning benchmarks, AutoBRANE improves over the best non-branching multitask baseline by 3.2%. Finally, on a large graph dataset with 21M edges and 500 tasks, AutoBRANE achieves a 28% accuracy gain over existing multitask and branching architectures, along with a 4.5$\times$ reduction in runtime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01113v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dongyue Li, Zhenshuo Zhang, Minxuan Duan, Edgar Dobriban, Hongyang R. Zhang</dc:creator>
    </item>
    <item>
      <title>Dynamic Algorithm for Explainable k-medians Clustering under lp Norm</title>
      <link>https://arxiv.org/abs/2512.01150</link>
      <description>arXiv:2512.01150v1 Announce Type: cross 
Abstract: We study the problem of explainable k-medians clustering introduced by Dasgupta, Frost, Moshkovitz, and Rashtchian (2020). In this problem, the goal is to construct a threshold decision tree that partitions data into k clusters while minimizing the k-medians objective. These trees are interpretable because each internal node makes a simple decision by thresholding a single feature, allowing users to trace and understand how each point is assigned to a cluster. We present the first algorithm for explainable k-medians under lp norm for every finite p &gt;= 1. Our algorithm achieves an O(p(log k)^{1 + 1/p - 1/p^2}) approximation to the optimal k-medians cost for any p &gt;= 1. Previously, algorithms were known only for p = 1 and p = 2. For p = 2, our algorithm improves upon the existing bound of O(log^{3/2}k), and for p = 1, it matches the tight bound of log k + O(1) up to a multiplicative O(log log k) factor. We show how to implement our algorithm in a dynamic setting. The dynamic algorithm maintains an explainable clustering under a sequence of insertions and deletions, with amortized update time O(d log^3 k) and O(log k) recourse, making it suitable for large-scale and evolving datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01150v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Konstantin Makarychev, Ilias Papanikolaou, Liren Shan</dc:creator>
    </item>
    <item>
      <title>Samplability makes learning easier</title>
      <link>https://arxiv.org/abs/2512.01276</link>
      <description>arXiv:2512.01276v1 Announce Type: cross 
Abstract: The standard definition of PAC learning (Valiant 1984) requires learners to succeed under all distributions -- even ones that are intractable to sample from. This stands in contrast to samplable PAC learning (Blum, Furst, Kearns, and Lipton 1993), where learners only have to succeed under samplable distributions. We study this distinction and show that samplable PAC substantially expands the power of efficient learners.
  We first construct a concept class that requires exponential sample complexity in standard PAC but is learnable with polynomial sample complexity in samplable PAC. We then lift this statistical separation to the computational setting and obtain a separation relative to a random oracle. Our proofs center around a new complexity primitive, explicit evasive sets, that we introduce and study. These are sets for which membership is easy to determine but are extremely hard to sample from.
  Our results extend to the online setting to similarly show how its landscape changes when the adversary is assumed to be efficient instead of computationally unbounded.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01276v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy Blanc, Caleb Koch, Jane Lange, Carmen Strassle, Li-Yang Tan</dc:creator>
    </item>
    <item>
      <title>Accelerating Probabilistic Response-Time Analysis: Revised Critical Instant and Optimized Convolution</title>
      <link>https://arxiv.org/abs/2512.01381</link>
      <description>arXiv:2512.01381v1 Announce Type: cross 
Abstract: Accurate estimation of the Worst-Case Deadline Failure Probability (WCDFP) has attracted growing attention as a means to provide safety assurances in complex systems such as robotic platforms and autonomous vehicles. WCDFP quantifies the likelihood of deadline misses under the most pessimistic operating conditions, and safe estimation is essential for dependable real-time applications. However, achieving high accuracy in WCDFP estimation often incurs significant computational cost. Recent studies have revealed that the classical assumption of the critical instant, the activation pattern traditionally considered to trigger the worst-case behavior, can lead to underestimation of WCDFP in probabilistic settings. This observation motivates the use of a revised critical instant formulation that more faithfully captures the true worst-case scenario. This paper investigates convolution-based methods for WCDFP estimation under this revised setting and proposes an optimization technique that accelerates convolution by improving the merge order. Extensive experiments with diverse execution-time distributions demonstrate that the proposed optimized Aggregate Convolution reduces computation time by up to an order of magnitude compared to Sequential Convolution, while retaining accurate and safe-sided WCDFP estimates. These results highlight the potential of the approach to provide both efficiency and reliability in probabilistic timing analysis for safety-critical real-time applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01381v1</guid>
      <category>cs.OS</category>
      <category>cs.DS</category>
      <category>cs.RO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of 8th Asia Pacific Conference on Robot IoT System Development and Platform (APRIS2025), 2025, pp. 1-8</arxiv:journal_reference>
      <dc:creator>Hiroto Takahashi, Atsushi Yano, Takuya Azumi</dc:creator>
    </item>
    <item>
      <title>Computing Treedepth Obstructions</title>
      <link>https://arxiv.org/abs/2512.01658</link>
      <description>arXiv:2512.01658v1 Announce Type: cross 
Abstract: The graph parameter treedepth is minor-monotone; hence, the class of graphs with treedepth at most $k$ is minor-closed. By the Graph Minor Theorem, such a class is characterized by a finite set of forbidden minors. A conjecture of Dvo\v{r}\'ak, Giannopoulou, and Thilikos states that every such forbidden minor has at most $2^k$ vertices. We present an algorithm that, given $n, k \in \mathbb{N}$, computes the set of forbidden minors, forbidden subgraphs, and forbidden induced subgraphs on at most $n$ vertices, for the class of graphs of treedepth at most $k$. Applying this algorithm to $k = 4$ and $n = 16$, we enumerate 1546 forbidden minors, 1718 forbidden subgraphs, and 12204 forbidden induced subgraphs. Assuming the above conjecture holds, these sets constitute the complete obstruction sets for graphs of treedepth at most 4.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01658v1</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kolja K\"uhn</dc:creator>
    </item>
    <item>
      <title>Excluding a Forest Induced Minor</title>
      <link>https://arxiv.org/abs/2512.01857</link>
      <description>arXiv:2512.01857v1 Announce Type: cross 
Abstract: In the first paper of the Graph Minors series [JCTB '83], Robertson and Seymour proved the Forest Minor theorem: the $H$-minor-free graphs have bounded pathwidth if and only if $H$ is a forest. In recent years, considerable effort has been devoted to understanding the unavoidable induced substructures of graphs with large pathwidth or large treewidth. In this paper, we give an induced counterpart of the Forest Minor theorem: for any $t \geqslant 2$, the $K_{t,t}$-subgraph-free $H$-induced-minor-free graphs have bounded pathwidth if and only if $H$ belongs to a class $\mathcal F$ of forests, which we describe as the induced minors of two (very similar) infinite parameterized families. This constitutes a significant step toward classifying the graphs $H$ for which every weakly sparse $H$-induced-minor-free class has bounded treewidth. Our work builds on the theory of constellations developed in the Induced Subgraphs and Tree Decompositions series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01857v1</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\'Edouard Bonnet, Benjamin Duhamel, Robert Hickingbotham</dc:creator>
    </item>
    <item>
      <title>Range Longest Increasing Subsequence and its Relatives</title>
      <link>https://arxiv.org/abs/2404.04795</link>
      <description>arXiv:2404.04795v2 Announce Type: replace 
Abstract: In this work, we present a plethora of results for the range longest increasing subsequence problem (Range-LIS) and its variants. The input to RLIS is a sequence $S$ of $n$ real numbers and a collection $Q$ of $m$ query ranges, and for each query in $Q$, the goal is to report the LIS of the sequence $S$ restricted to that query. Our two main results are for the following generalizations of the RLIS problem.
  2D range queries: In this variant of the RLIS problem, each query is a pair of ranges, one of indices and the other of values, and we provide a randomized algorithm with running time $\tilde{O}(m n^{1/2} + n^{3/2}) + O(k)$, where $k$ is the cumulative length of the $m$ output subsequences. This improves on the elementary $O(mn)$-time algorithm when $m$ is at least $n^{1/2}$. Previously, the only known result breaking the quadratic barrier was due to Tiskin [SODA'10], which could only handle 1D range queries (i.e., each query was a range of indices) and also just outputted the length of the LIS (instead of reporting the subsequence achieving that length).
  Colored sequences: In this variant of the RLIS problem, each element in $S$ is colored, and for each query in $Q$, the goal is to report a monochromatic LIS contained in the sequence $S$ restricted to that query. For 2D queries, we provide a randomized algorithm for this colored version with running time $\tilde{O}(m n^{2/3} + n^{5/3}) + O(k)$. Moreover, for 1D queries, we provide an improved algorithm with running time $\tilde{O}(m n^{1/2} + n^{3/2}) + O(k)$. Thus, we again improve on the elementary $O(mn)$-time algorithm. Additionally, assuming the well-known Combinatorial Boolean Matrix Multiplication Hypothesis, we prove that the running time for 1D queries is essentially tight for combinatorial algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04795v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karthik C. S., Saladi Rahul</dc:creator>
    </item>
    <item>
      <title>Parameterized Approximation Algorithms for TSP on Non-Metric Graphs</title>
      <link>https://arxiv.org/abs/2503.03642</link>
      <description>arXiv:2503.03642v3 Announce Type: replace 
Abstract: The Traveling Salesman Problem (TSP) is a classic and extensively studied problem with numerous real-world applications in artificial intelligence and operations research. It is well-known that TSP admits a constant approximation ratio on metric graphs but becomes NP-hard to approximate within any computable function $f(n)$ on general graphs. This disparity highlights a significant gap between the results on metric graphs and general graphs. Recent research has introduced some parameters to measure the ``distance'' of general graphs from being metric and explored Fixed-Parameter Tractable (FPT) approximation algorithms parameterized by these parameters. Two commonly studied parameters are $p$, the number of vertices in triangles violating the triangle inequality, and $q$, the minimum number of vertices whose removal results in a metric graph. In this paper, we present improved FPT approximation algorithms with respect to these two parameters. For $p$, we propose an FPT algorithm with a 1.5-approximation ratio, improving upon the previous ratio of 2.5. For $q$, we significantly enhance the approximation ratio from 11 to 3, advancing the state of the art in both cases. In addition, when $p$ (or $q$) is a constant, we obtain a better approximation ratio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03642v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyang Zhao, Zimo Sheng, Mingyu Xiao</dc:creator>
    </item>
    <item>
      <title>Overlapping Biclustering</title>
      <link>https://arxiv.org/abs/2505.05213</link>
      <description>arXiv:2505.05213v2 Announce Type: replace 
Abstract: We study the problem of transforming bipartite graphs into bicluster graphs. Abu-Khzam, Isenmann, and Merchad [IWOCA '25] introduced two variants of this problem. In both problems, the goal is to transform a bipartite graph into a bicluster graph with at most $k$ operations, where the allowed operations are inserting an edge, deleting an edge, and splitting a vertex. Splitting a vertex $v$ refers to replacing $v$ by two new vertices whose combined neighborhood equals the neighborhood of $v$. The latter models overlapping clusters, that is, vertices belonging to multiple clusters, and is motivated by several real-world applications. The versions differ in that one variant allows splitting any vertex, while the second variant only allows vertex splits on one side of the bipartition.
  Regarding computational complexity, they showed APX-hardness for both variants and a polynomial kernel (with $O(k^5)$ vertices) for the one-sided variant. They asked as open problems whether the polynomial kernel can be improved and whether it can also be extended for the other variant. We answer both questions in the affirmative and give kernels with $O(k^2)$ vertices for both variants. We also show that both problems can be solved in $O(k^{11k} + n + m)$ time, where $n$ and $m$ denote the number of vertices and edges in the input graph, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05213v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Bentert, P{\aa}l Gr{\o}n{\aa}s Drange, Erlend Haugen</dc:creator>
    </item>
    <item>
      <title>Smoothed Analysis of Online Metric Matching with a Single Sample: Beyond Metric Distortion</title>
      <link>https://arxiv.org/abs/2510.20288</link>
      <description>arXiv:2510.20288v2 Announce Type: replace 
Abstract: In the online metric matching problem, $n$ servers and $n$ requests lie in a metric space. Servers are available upfront, and requests arrive sequentially. An arriving request must be matched immediately and irrevocably to an available server, incurring a cost equal to their distance. The goal is to minimize the total matching cost.
  We study this problem in the Euclidean metric $[0, 1]^d$, when servers are adversarial and requests are independently drawn from distinct distributions that satisfy a mild smoothness condition. Our main result is an $O(1)$-competitive algorithm for $d \neq 2$ that requires no distributional knowledge, relying only on a single sample from each request distribution. To our knowledge, this is the first algorithm to achieve an $o(\log n)$ competitive ratio for non-trivial metrics beyond the i.i.d. setting. Our approach bypasses the $\Omega(\log n)$ barrier introduced by probabilistic metric embeddings: instead of analyzing the embedding distortion and the algorithm separately, we directly bound the cost of the algorithm on the target metric of a simple deterministic embedding. We then combine this analysis with lower bounds on the offline optimum for Euclidean metrics, derived via majorization arguments, to obtain our guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20288v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingxi Li, Ellen Vitercik, Mingwei Yang</dc:creator>
    </item>
    <item>
      <title>A Combinatorial Characterization of Constant Mixing Time</title>
      <link>https://arxiv.org/abs/2511.21868</link>
      <description>arXiv:2511.21868v2 Announce Type: replace 
Abstract: Classical spectral graph theory characterizes graphs with logarithmic mixing time. In this work, we present a combinatorial characterization of graphs with constant mixing time. The combinatorial characterization is based on the small-set bipartite density condition, which is weaker than having near-optimal spectral radius and is stronger than having near-optimal small-set vertex expansion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21868v2</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lap Chi Lau, Raymond Liu</dc:creator>
    </item>
    <item>
      <title>A closer look at TDFA</title>
      <link>https://arxiv.org/abs/2206.01398</link>
      <description>arXiv:2206.01398v2 Announce Type: replace-cross 
Abstract: We present an algorithm for regular expression parsing and submatch extraction based on tagged deterministic finite automata. The algorithm works with different disambiguation policies. We give detailed pseudocode for the algorithm, covering important practical optimizations. All transformations from a regular expression to an optimized automaton are explained on a step-by-step example. We consider both ahead-of-time and just-in-time determinization and describe variants of the algorithm suited to each setting. We provide benchmarks showing that the algorithm is very fast in practice. Our research is based on two independent implementations: an open-source lexer generator RE2C and an experimental Java library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.01398v2</guid>
      <category>cs.FL</category>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Angelo Borsotti, Ulya Trafimovich</dc:creator>
    </item>
    <item>
      <title>Local Fragments, Global Gains: Subgraph Counting using Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2305.19659</link>
      <description>arXiv:2305.19659v5 Announce Type: replace-cross 
Abstract: Subgraph counting is a fundamental task for analyzing structural patterns in graph-structured data, with important applications in domains such as computational biology and social network analysis, where recurring motifs reveal functional and organizational properties. In this paper, we propose localized versions of the Weisfeiler-Leman (WL) algorithms to improve both expressivity and computational efficiency for this task. We introduce Local $k$-WL, which we prove to be more expressive than $k$-WL and at most as expressive as $(k+1)$-WL, and provide a characterization of patterns whose subgraph and induced subgraph counts are invariant under Local $k$-WL equivalence. To enhance scalability, we present two variants -- Layer $k$-WL and Recursive $k$-WL -- that achieve greater time and space efficiency compared to applying $k$-WL on the entire graph. Additionally, we propose a novel fragmentation technique that decomposes complex subgraphs into simpler subpatterns, enabling the exact count of all induced subgraphs of size at most $4$ using only $1$-WL, with extensions possible for larger patterns when $k&gt;1$. Building on these ideas, we develop a three-stage differentiable learning framework that combines subpattern counts to compute counts of more complex motifs, bridging combinatorial algorithm design with machine learning approaches. We also compare the expressive power of Local $k$-WL with existing GNN hierarchies and demonstrate that, under bounded time complexity, our methods are more expressive than prior approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.19659v5</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shubhajit Roy, Shrutimoy Das, Binita Maity, Anant Kumar, Anirban Dasgupta</dc:creator>
    </item>
    <item>
      <title>Generalized Short Path Algorithms: Towards Super-Quadratic Speedup over Markov Chain Search for Combinatorial Optimization</title>
      <link>https://arxiv.org/abs/2410.23270</link>
      <description>arXiv:2410.23270v2 Announce Type: replace-cross 
Abstract: We analyze generalizations of quantum algorithms based on the short path framework first proposed by Hastings~[\textit{Quantum} 2, 78 (2018)], which has been extended and shown by Dalzell~et~al.~[STOC~'23] to achieve super-Grover speedups for certain binary optimization problems. We demonstrate that, under some commonly satisfied technical conditions, an appropriate generalization can achieve super-quadratic speedups not only over unstructured search but also over a classical optimization algorithm that searches for the optimum by drawing samples from the stationary distribution of a Markov chain. We employ this framework to obtain algorithms for problems including variants of Max Bisection, Max Independent Set, and finding the ground states of the Antiferromagnetic Ising Model and the Sherrington-Kirkpatrick Model, whose runtimes are asymptotically faster than those obtainable with previous short path techniques. In certain cases, our algorithms achieve super-quadratic speedups compared to the best known classical algorithms with rigorously established runtimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23270v2</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shouvanik Chakrabarti, Dylan Herman, Guneykan Ozgul, Shuchen Zhu, Brandon Augustino, Tianyi Hao, Zichang He, Ruslan Shaydulin, Marco Pistoia</dc:creator>
    </item>
    <item>
      <title>Testing Noise Assumptions of Learning Algorithms</title>
      <link>https://arxiv.org/abs/2501.09189</link>
      <description>arXiv:2501.09189v2 Announce Type: replace-cross 
Abstract: We pose a fundamental question in computational learning theory: can we efficiently test whether a training set satisfies the assumptions of a given noise model? This question has remained unaddressed despite decades of research on learning in the presence of noise. In this work, we show that this task is tractable and present the first efficient algorithm to test various noise assumptions on the training data.
  To model this question, we extend the recently proposed testable learning framework of Rubinfeld and Vasilyan (2023) and require a learner to run an associated test that satisfies the following two conditions: (1) whenever the test accepts, the learner outputs a classifier along with a certificate of optimality, and (2) the test must pass for any dataset drawn according to a specified modeling assumption on both the marginal distribution and the noise model. We then consider the problem of learning halfspaces over Gaussian marginals with Massart noise (where each label can be flipped with probability less than $1/2$ depending on the input features), and give a fully-polynomial time testable learning algorithm.
  We also show a separation between the classical setting of learning in the presence of structured noise and testable learning. In fact, for the simple case of random classification noise (where each label is flipped with fixed probability $\eta = 1/2$), we show that testable learning requires super-polynomial time while classical learning is trivial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09189v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Surbhi Goel, Adam R. Klivans, Konstantinos Stavropoulos, Arsen Vasilyan</dc:creator>
    </item>
    <item>
      <title>Approximately Dominating Sets in Elections</title>
      <link>https://arxiv.org/abs/2504.20372</link>
      <description>arXiv:2504.20372v3 Announce Type: replace-cross 
Abstract: Condorcet's paradox is a fundamental result in social choice theory which states that there exist elections in which, no matter which candidate wins, a majority of voters prefer a different candidate. In fact, even if we can select any $k$ winners, there still may exist another candidate that would beat each of the winners in a majority vote. That is, elections may require arbitrarily large dominating sets.
  We show that approximately dominating sets of constant size always exist. In particular, for every $\varepsilon &gt; 0$, every election (irrespective of the number of voters or candidates) can select $O(\frac{1}{\varepsilon ^2})$ winners such that no other candidate beats each of the winners by a margin of more than $\varepsilon$ fraction of voters.
  Our proof uses a simple probabilistic construction using samples from a maximal lottery, a well-studied distribution over candidates derived from the Nash equilibrium of a two-player game. In stark contrast to general approximate equilibria, which may require support logarithmic in the number of pure strategies, we show that maximal lotteries can be approximated with constant support size. These approximate maximal lotteries may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20372v3</guid>
      <category>cs.GT</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moses Charikar, Prasanna Ramakrishnan, Kangning Wang</dc:creator>
    </item>
    <item>
      <title>Private Continual Counting of Unbounded Streams</title>
      <link>https://arxiv.org/abs/2506.15018</link>
      <description>arXiv:2506.15018v2 Announce Type: replace-cross 
Abstract: We study the problem of differentially private continual counting in the unbounded setting where the input size $n$ is not known in advance. Current state-of-the-art algorithms based on optimal instantiations of the matrix mechanism cannot be directly applied here because their privacy guarantees only hold when key parameters are tuned to $n$. Using the common `doubling trick' avoids knowledge of $n$ but leads to suboptimal and non-smooth error. We solve this problem by introducing novel matrix factorizations based on logarithmic perturbations of the function $\frac{1}{\sqrt{1-z}}$ studied in prior works, which may be of independent interest. The resulting algorithm has smooth error, and for any $\alpha &gt; 0$ and $t\leq n$ it is able to privately estimate the sum of the first $t$ data points with $O(\log^{2+2\alpha}(t))$ variance. It requires $O(t)$ space and amortized $O(\log t)$ time per round, compared to $O(\log(n)\log(t))$ variance, $O(n)$ space and $O(n \log n)$ pre-processing time for the nearly-optimal bounded-input algorithm of Henzinger et al. (SODA 2023). Empirically, we find that our algorithm's performance is also comparable to theirs in absolute terms: our variance is less than $1.5\times$ theirs for $t$ as large as $2^{24}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15018v2</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Jacobsen, Kassem Fawaz</dc:creator>
    </item>
    <item>
      <title>Faster Algorithms for Structured Matrix Multiplication via Flip Graph Search</title>
      <link>https://arxiv.org/abs/2511.10786</link>
      <description>arXiv:2511.10786v2 Announce Type: replace-cross 
Abstract: We give explicit low-rank bilinear non-commutative schemes for multiplying structured $n \times n$ matrices with $2 \leq n \leq 5$, which serve as building blocks for recursive algorithms with improved multiplicative factors in asymptotic complexity. Our schemes are discovered over $\mathbb{F}_2$ or $\mathbb{F}_3$ and lifted to $\mathbb{Z}$ or $\mathbb{Q}$. Using a flip graph search over tensor decompositions, we derive schemes for general, upper-triangular, lower-triangular, symmetric, and skew-symmetric inputs, as well as products of a structured matrix with its transpose. These schemes improve asymptotic constants for 13 of 15 structured formats. In particular, we obtain $4 \times 4$ rank-34 schemes for both multiplying a general matrix by its transpose and an upper-triangular matrix by a general matrix, improving the asymptotic factor from 8/13 (0.615) to 22/37 (0.595). Additionally, using $\mathbb{F}_3$ flip graphs, we discover schemes over $\mathbb{Q}$ that fundamentally require the inverse of 2, including a $2 \times 2$ symmetric-symmetric multiplication of rank 5 and a $3 \times 3$ skew-symmetric-general multiplication of rank 14 (improving upon AlphaTensor's 15).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10786v2</guid>
      <category>cs.SC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kirill Khoruzhii, Patrick Gel{\ss}, Sebastian Pokutta</dc:creator>
    </item>
    <item>
      <title>Fast Matrix Multiplication via Ternary Meta Flip Graphs</title>
      <link>https://arxiv.org/abs/2511.20317</link>
      <description>arXiv:2511.20317v2 Announce Type: replace-cross 
Abstract: Matrix multiplication optimization remains a fundamental challenge in computational mathematics. This work introduces a novel approach that discovers matrix multiplication schemes whose coefficients are restricted to the set $\{-1, 0, 1\}$ (denoted $Z_T$), minimizing naive additive complexity for efficient hardware implementation. The core of the method is a GPU-accelerated meta flip graph algorithm that maintains ternary safety through specialized arithmetic operations and sign symmetry breaking. Key results include new best ranks for the formats $4 \times 5 \times 12$, $5 \times 6 \times 10$, and $6 \times 7 \times 9$, the independent discovery of 32 schemes in $Z_T$ that match known optimal ranks (including 8 previously known only with rational coefficients), and 30 rank improvements in the binary field. The analysis of 164 known schemes shows that 92 admit a ternary-coefficient implementation, while 72 could not be found under this constraint, defining the current boundaries of the approach. All software, results, and discovered schemes are provided as open-source.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20317v2</guid>
      <category>cs.SC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. I. Perminov</dc:creator>
    </item>
    <item>
      <title>The communication complexity of distributed estimation</title>
      <link>https://arxiv.org/abs/2511.21015</link>
      <description>arXiv:2511.21015v2 Announce Type: replace-cross 
Abstract: We study an extension of the standard two-party communication model in which Alice and Bob hold probability distributions $p$ and $q$ over domains $X$ and $Y$, respectively. Their goal is to estimate \[ \mathbb{E}_{x \sim p,\, y \sim q}[f(x, y)] \] to within additive error $\varepsilon$ for a bounded function $f$, known to both parties. We refer to this as the distributed estimation problem. Special cases of this problem arise in a variety of areas including sketching, databases and learning. Our goal is to understand how the required communication scales with the communication complexity of $f$ and the error parameter $\varepsilon$.
  The random sampling approach -- estimating the mean by averaging $f$ over $O(1/\varepsilon^2)$ random samples -- requires $O(R(f)/\varepsilon^2)$ total communication, where $R(f)$ is the randomized communication complexity of $f$. We design a new debiasing protocol which improves the dependence on $1/\varepsilon$ to be linear instead of quadratic. Additionally we show better upper bounds for several special classes of functions, including the Equality and Greater-than functions. We introduce lower bound techniques based on spectral methods and discrepancy, and show the optimality of many of our protocols: the debiasing protocol is tight for general functions, and that our protocols for the equality and greater-than functions are also optimal. Furthermore, we show that among full-rank Boolean functions, Equality is essentially the easiest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21015v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 02 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Parikshit Gopalan, Raghu Meka, Prasad Raghavendra, Mihir Singhal, Avi Wigderson</dc:creator>
    </item>
  </channel>
</rss>
