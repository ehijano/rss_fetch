<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 May 2024 17:18:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 29 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Towards the use of multiple ROIs for radiomics-based survival modelling: finding a strategy of aggregating lesions</title>
      <link>https://arxiv.org/abs/2405.17668</link>
      <description>arXiv:2405.17668v1 Announce Type: new 
Abstract: The main objective of this work is to explore the possibility of incorporating radiomic information from multiple lesions into survival models. We hypothesise that when more lesions are present, their inclusion can improve model performance, and we aim to find an optimal strategy for using multiple distinct regions in modelling.
  The idea of using multiple regions of interest (ROIs) to extract radiomic features for predictive models has been implemented in many recent works. However, in almost all studies, analogous regions were segmented according to particular criteria for all patients -- for example, the primary tumour and peritumoral area, or subregions of the primary tumour. They can be included in a model in a straightforward way as additional features. A more interesting scenario occurs when multiple distinct ROIs are present, such as multiple lesions in a regionally disseminated cancer. Since the number of such regions may differ between patients, their inclusion in a model is non-trivial and requires additional processing steps.
  We proposed several methods of handling multiple ROIs representing either ROI or risk aggregation strategy, compared them to a published one, and evaluated their performance in different classes of survival models in a Monte Carlo Cross-Validation scheme. We demonstrated the effectiveness of the methods using a cohort of 115 non-small cell lung cancer patients, for whom we predicted the metastasis risk based on features extracted from PET images in original resolution or interpolated to CT image resolution. For both feature sets, incorporating all available lesions, as opposed to a singular ROI representing the primary tumour, allowed for considerable improvement of predictive ability regardless of the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17668v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Agata Ma{\l}gorzata Wilk, Andrzej Swierniak, Andrea d'Amico, Rafa{\l} Suwi\'nski, Krzysztof Fujarewicz, Damian Borys</dc:creator>
    </item>
    <item>
      <title>The association between environmental variables and short-term mortality: evidence from Europe</title>
      <link>https://arxiv.org/abs/2405.18020</link>
      <description>arXiv:2405.18020v1 Announce Type: new 
Abstract: Using fine-grained, publicly available data, this paper studies the association between environmental factors, i.e., variables capturing weather and air pollution characteristics, and weekly mortality rates in small geographical regions in Europe. Hereto, we develop a mortality modelling framework where a baseline captures a region-specific, seasonal historical trend observed within the weekly mortality rates. Using a machine learning algorithm, we then explain deviations from this baseline using anomalies and extreme indices constructed from the environmental data. We illustrate our proposed modelling framework through a case study on more than 550 NUTS 3 regions (Nomenclature of Territorial Units for Statistics, level 3) located in 20 different European countries. Through interpretation tools, we unravel insights into which environmental features are most important when estimating excess or deficit mortality with respect to the baseline and explore how these features interact. Moreover, we investigate harvesting effects of the environmental features through our constructed weekly mortality modelling framework. Our findings show that temperature-related features exert the most significant influence in explaining deviations in mortality from the baseline. Furthermore, we find that environmental features prove particularly beneficial in southern regions for explaining elevated levels of mortality over short time periods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18020v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jens Robben, Katrien Antonio, Torsten Kleinow</dc:creator>
    </item>
    <item>
      <title>Predicting Progression Events in Multiple Myeloma from Routine Blood Work</title>
      <link>https://arxiv.org/abs/2405.18051</link>
      <description>arXiv:2405.18051v1 Announce Type: new 
Abstract: The ability to accurately predict disease progression is paramount for optimizing multiple myeloma patient care. This study introduces a hybrid neural network architecture, combining Long Short-Term Memory networks with a Conditional Restricted Boltzmann Machine, to predict future blood work of affected patients from a series of historical laboratory results. We demonstrate that our model can replicate the statistical moments of the time series ($0.95~\pm~0.01~\geq~R^2~\geq~0.83~\pm~0.03$) and forecast future blood work features with high correlation to actual patient data ($0.92\pm0.02~\geq~r~\geq~0.52~\pm~0.09$). Subsequently, a second Long Short-Term Memory network is employed to detect and annotate disease progression events within the forecasted blood work time series. We show that these annotations enable the prediction of progression events with significant reliability (AUROC$~=~0.88~\pm~0.01$), up to 12 months in advance (AUROC($t+12~$mos)$~=0.65~\pm~0.01$). Our system is designed in a modular fashion, featuring separate entities for forecasting and progression event annotation. This structure not only enhances interpretability but also facilitates the integration of additional modules to perform subsequent operations on the generated outputs. Our approach utilizes a minimal set of routine blood work measurements, which avoids the need for expensive or resource-intensive tests and ensures accessibility of the system in clinical routine. This capability allows for individualized risk assessment and making informed treatment decisions tailored to a patient's unique disease kinetics. The represented approach contributes to the development of a scalable and cost-effective virtual human twin system for optimized healthcare resource utilization and improved patient outcomes in multiple myeloma care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18051v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maximilian Ferle, Nora Grieb, Markus Kreuz, Uwe Platzbecker, Thomas Neumuth, Kristin Reiche, Alexander Oeser, Maximilian Merz</dc:creator>
    </item>
    <item>
      <title>Unraveling Factors Influencing Shooting Incidents: Preliminary Analysis and Insights</title>
      <link>https://arxiv.org/abs/2405.18271</link>
      <description>arXiv:2405.18271v1 Announce Type: new 
Abstract: The following is a write up of the progress of modeling data from the K12 organization \cite{Riedman_2023}. Data was characterized and investigated for statistically significant factors. The incident data was spilt into three sets: the entire set of incidents, incidents from 1966 - 2017, and incidents from 2018 - 2023. This was done in an attempt to discern key factors for the acceleration of incidents over the last several years. The data set was cleaned and processed primarily through RStudio. The individual factors were studied and subjected to statistical analysis where appropriate. As it turns out, there are differences between media portrayals of shooters and actual shooters. Then, multiple regression techniques were performed then followed by ANOVA of the models to determine statistically significant independent variables and their influence on casualties. Thus far, linear regression and negative binomial regression have been attempted. Further refining of the methods will be necessary for Poisson regression and logistic regression to be viably attempted. At this point in time a common theme among each of the models is the presence of targeted attacks affecting casualties. Further study can lead to improved safe guarding strategies to eliminate or minimize casualties. Further, increased understanding of shooter demographics can also lead to outreach and prevention programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18271v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Hernandez, Linn Carothers</dc:creator>
    </item>
    <item>
      <title>Identifiability, Observability, Uncertainty and Bayesian System Identification of Epidemiological Models</title>
      <link>https://arxiv.org/abs/2405.18279</link>
      <description>arXiv:2405.18279v1 Announce Type: new 
Abstract: In this project, identifiability, observability and uncertainty properties of the deterministic and Chain Binomial stochastic SIR, SEIR and SEIAR epidemiological models are studied. Techniques for modeling overdispersion are investigated and used to compare simulated trajectories for moderately sized, homogenous populations. With the chosen model parameters overdispersion was found to have small impact, but larger impact on smaller populations and simulations closer to the initial outbreak of an epidemic. Using a software tool for model identifiability and observability (DAISY[Bellu et al. 2007]), the deterministic SIR and SEIR models was found to be structurally identifiable and observable under mild conditions, while SEIAR in general remains structurally unidentifiable and unobservable. Sequential Monte Carlo and Markov Chain Monte Carlo methods were implemented in a custom C++ library and applied to stochastic SIR, SEIR and SEIAR models in order to generate parameter distributions. With the chosen model parameters overdispersion was found to have a small impact on parameter distributions for SIR and SEIR models. For SEIAR, the algorithm did not converge around the true parameters of the deterministic model. The custom C++ library was found to be computationally efficient, and is very likely to be used in future projects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18279v1</guid>
      <category>stat.AP</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Hjulstad</dc:creator>
    </item>
    <item>
      <title>Augmented Risk Prediction for the Onset of Alzheimer's Disease from Electronic Health Records with Large Language Models</title>
      <link>https://arxiv.org/abs/2405.16413</link>
      <description>arXiv:2405.16413v1 Announce Type: cross 
Abstract: Alzheimer's disease (AD) is the fifth-leading cause of death among Americans aged 65 and older. Screening and early detection of AD and related dementias (ADRD) are critical for timely intervention and for identifying clinical trial participants. The widespread adoption of electronic health records (EHRs) offers an important resource for developing ADRD screening tools such as machine learning based predictive models. Recent advancements in large language models (LLMs) demonstrate their unprecedented capability of encoding knowledge and performing reasoning, which offers them strong potential for enhancing risk prediction. This paper proposes a novel pipeline that augments risk prediction by leveraging the few-shot inference power of LLMs to make predictions on cases where traditional supervised learning methods (SLs) may not excel. Specifically, we develop a collaborative pipeline that combines SLs and LLMs via a confidence-driven decision-making mechanism, leveraging the strengths of SLs in clear-cut cases and LLMs in more complex scenarios. We evaluate this pipeline using a real-world EHR data warehouse from Oregon Health \&amp; Science University (OHSU) Hospital, encompassing EHRs from over 2.5 million patients and more than 20 million patient encounters. Our results show that our proposed approach effectively combines the power of SLs and LLMs, offering significant improvements in predictive performance. This advancement holds promise for revolutionizing ADRD screening and early detection practices, with potential implications for better strategies of patient management and thus improving healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16413v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jiankun Wang, Sumyeong Ahn, Taykhoom Dalal, Xiaodan Zhang, Weishen Pan, Qiannan Zhang, Bin Chen, Hiroko H. Dodge, Fei Wang, Jiayu Zhou</dc:creator>
    </item>
    <item>
      <title>ZIKQ: An innovative centile chart method for utilizing natural history data in rare disease clinical development</title>
      <link>https://arxiv.org/abs/2405.17684</link>
      <description>arXiv:2405.17684v1 Announce Type: cross 
Abstract: Utilizing natural history data as external control plays an important role in the clinical development of rare diseases, since placebo groups in double-blind randomization trials may not be available due to ethical reasons and low disease prevalence. This article proposed an innovative approach for utilizing natural history data to support rare disease clinical development by constructing reference centile charts. Due to the deterioration nature of certain rare diseases, the distributions of clinical endpoints can be age-dependent and have an absorbing state of zero, which can result in censored natural history data. Existing methods of reference centile charts can not be directly used in the censored natural history data. Therefore, we propose a new calibrated zero-inflated kernel quantile (ZIKQ) estimation to construct reference centile charts from censored natural history data. Using the application to Duchenne Muscular Dystrophy drug development, we demonstrate that the reference centile charts using the ZIKQ method can be implemented to evaluate treatment efficacy and facilitate a more targeted patient enrollment in rare disease clinical development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17684v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5705/ss.202023.0107</arxiv:DOI>
      <dc:creator>Tianying Wang, Wenfei Zhang, Ying Wei</dc:creator>
    </item>
    <item>
      <title>The Multiplex $p_2$ Model: Mixed-Effects Modeling for Multiplex Social Networks</title>
      <link>https://arxiv.org/abs/2405.17707</link>
      <description>arXiv:2405.17707v1 Announce Type: cross 
Abstract: Social actors are often embedded in multiple social networks, and there is a growing interest in studying social systems from a multiplex network perspective. In this paper, we propose a mixed-effects model for cross-sectional multiplex network data that assumes dyads to be conditionally independent. Building on the uniplex $p_2$ model, we incorporate dependencies between different network layers via cross-layer dyadic effects and actor random effects. These cross-layer effects model the tendencies for ties between two actors and the ties to and from the same actor to be dependent across different relational dimensions. The model can also study the effect of actor and dyad covariates. As simulation-based goodness-of-fit analyses are common practice in applied network studies, we here propose goodness-of-fit measures for multiplex network analyses. We evaluate our choice of priors and the computational faithfulness and inferential properties of the proposed method through simulation. We illustrate the utility of the multiplex $p_2$ model in a replication study of a toxic chemical policy network. An original study that reflects on gossip as perceived by gossip senders and gossip targets, and their differences in perspectives, based on data from 34 Hungarian elementary school classes, highlights the applicability of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17707v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anni Hong, Nynke M. D. Niezink</dc:creator>
    </item>
    <item>
      <title>Towards Efficient Disaster Response via Cost-effective Unbiased Class Rate Estimation through Neyman Allocation Stratified Sampling Active Learning</title>
      <link>https://arxiv.org/abs/2405.17734</link>
      <description>arXiv:2405.17734v1 Announce Type: cross 
Abstract: With the rapid development of earth observation technology, we have entered an era of massively available satellite remote-sensing data. However, a large amount of satellite remote sensing data lacks a label or the label cost is too high to hinder the potential of AI technology mining satellite data. Especially in such an emergency response scenario that uses satellite data to evaluate the degree of disaster damage. Disaster damage assessment encountered bottlenecks due to excessive focus on the damage of a certain building in a specific geographical space or a certain area on a larger scale. In fact, in the early days of disaster emergency response, government departments were more concerned about the overall damage rate of the disaster area instead of single-building damage, because this helps the government decide the level of emergency response. We present an innovative algorithm that constructs Neyman stratified random sampling trees for binary classification and extends this approach to multiclass problems. Through extensive experimentation on various datasets and model structures, our findings demonstrate that our method surpasses both passive and conventional active learning techniques in terms of class rate estimation and model enhancement with only 30\%-60\% of the annotation cost of simple sampling. It effectively addresses the 'sampling bias' challenge in traditional active learning strategies and mitigates the 'cold start' dilemma. The efficacy of our approach is further substantiated through application to disaster evaluation tasks using Xview2 Satellite imagery, showcasing its practical utility in real-world contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17734v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanbing Bai, Xinyi Wu, Lai Xu, Jihan Pei, Erick Mas, Shunichi Koshimura</dc:creator>
    </item>
    <item>
      <title>Simulation of Single-Phase Natural Circulation within the BEPU Framework: Sketching Scaling Uncertainty Principle by Multi-Scale CFD Approaches</title>
      <link>https://arxiv.org/abs/2405.18108</link>
      <description>arXiv:2405.18108v1 Announce Type: cross 
Abstract: In order to enhance safety, nuclear reactors in the design phase consider natural circulation as a mean to remove residual power. The simulation of this passive mechanism must be qualified between the validation range and the scope of utilization (reactor case), introducing potential physical and numerical distortion effects. In this study, we simulate the flow of liquid sodium using the TrioCFD code, employing both higher-fidelity (HF) LES and lower-fidelity (LF) URANS models. We tackle respectively numerical uncertainties through the Grid Convergence Index method, and physical modelling uncertainties through the Polynomial Chaos Expansion method available on the URANIE platform. HF simulations are shown to exhibit a strong resilience to physical distortion effects, with numerical uncertainties being intricately correlated. Conversely, the LF approach, the only one applicable at the reactor scale, is likely to present a reduced predictability. If so, the HF approach should be effective in pinpointing the LF weaknesses: the concept of scaling uncertainty is inline introduced as the growth of the LF simulation uncertainty associated with distortion effects. Thus, the paper outlines that a specific methodology within the BEPU framework - leveraging both HF and LF approaches - could pragmatically enable correlating distortion effects with scaling uncertainty, thereby providing a metric principle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18108v1</guid>
      <category>physics.class-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Best Estimate Plus Uncertainty International Conference (BEPU 2024), May 2024, Lucca, Italy</arxiv:journal_reference>
      <dc:creator>Haifu Huang (IUSTI), Jorge Perez (IUSTI), Nicolas Alpy (IUSTI), Marc Medale (IUSTI)</dc:creator>
    </item>
    <item>
      <title>MC-GTA: Metric-Constrained Model-Based Clustering using Goodness-of-fit Tests with Autocorrelations</title>
      <link>https://arxiv.org/abs/2405.18395</link>
      <description>arXiv:2405.18395v1 Announce Type: cross 
Abstract: A wide range of (multivariate) temporal (1D) and spatial (2D) data analysis tasks, such as grouping vehicle sensor trajectories, can be formulated as clustering with given metric constraints. Existing metric-constrained clustering algorithms overlook the rich correlation between feature similarity and metric distance, i.e., metric autocorrelation. The model-based variations of these clustering algorithms (e.g. TICC and STICC) achieve SOTA performance, yet suffer from computational instability and complexity by using a metric-constrained Expectation-Maximization procedure. In order to address these two problems, we propose a novel clustering algorithm, MC-GTA (Model-based Clustering via Goodness-of-fit Tests with Autocorrelations). Its objective is only composed of pairwise weighted sums of feature similarity terms (square Wasserstein-2 distance) and metric autocorrelation terms (a novel multivariate generalization of classic semivariogram). We show that MC-GTA is effectively minimizing the total hinge loss for intra-cluster observation pairs not passing goodness-of-fit tests, i.e., statistically not originating from the same distribution. Experiments on 1D/2D synthetic and real-world datasets demonstrate that MC-GTA successfully incorporates metric autocorrelation. It outperforms strong baselines by large margins (up to 14.3% in ARI and 32.1% in NMI) with faster and stabler optimization (&gt;10x speedup).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18395v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhangyu Wang, Gengchen Mai, Krzysztof Janowicz, Ni Lao</dc:creator>
    </item>
    <item>
      <title>Measurement That Matches Theory: Theory-Driven Identification in IRT Models</title>
      <link>https://arxiv.org/abs/2111.11979</link>
      <description>arXiv:2111.11979v3 Announce Type: replace 
Abstract: Measurement bridges theory and empirics. Without measures that appropriately capture theoretical concepts, description will fail to represent reality and true causal inference will be impossible. Yet, the social sciences traffic in complex concepts and their measurement is difficult. Item Response Theory (IRT) models reduce variation in multiple variables to continuous variation along one or more latent dimensions intended to capture key theoretical concepts. Unfortunately, those latent dimensions have no intrinsic conceptual meaning. Partial solutions to that problem include limiting the number of dimensions to one or assigning meaning post-analysis, but either can lead to potential bias and a lack of reliability across data sources. We propose, detail, and validate a semi-supervised approach employing Bayesian Item Response Theory on multiple latent dimensions and binary data. Our approach, which we validate on simulated and real data, yields conceptually meaningful latent dimensions that are reliable across different data sources without additional exogenous assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.11979v3</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marco Morucci, Margaret Foster, Kaitlyn Webster, So Jin Lee, David Siegel</dc:creator>
    </item>
    <item>
      <title>Bayesian material flow analysis for systems with multiple levels of disaggregation and high dimensional data</title>
      <link>https://arxiv.org/abs/2211.06178</link>
      <description>arXiv:2211.06178v2 Announce Type: replace 
Abstract: Material Flow Analysis (MFA) is used to quantify and understand the life cycles of materials from production to end of use, which enables environmental, social and economic impacts and interventions. MFA is challenging as available data is often limited and uncertain, leading to an underdetermined system with an infinite number of possible stocks and flows values. Bayesian statistics is an effective way to address these challenges by principally incorporating domain knowledge, and quantifying uncertainty in the data and providing probabilities associated with model solutions.
  This paper presents a novel MFA methodology under the Bayesian framework. By relaxing the mass balance constraints, we improve the computational scalability and reliability of the posterior samples compared to existing Bayesian MFA methods. We propose a mass based, child and parent process framework to model systems with disaggregated processes and flows. We show posterior predictive checks can be used to identify inconsistencies in the data and aid noise and hyperparameter selection. The proposed approach is demonstrated on case studies, including a global aluminium cycle with significant disaggregation, under weakly informative priors and significant data gaps to investigate the feasibility of Bayesian MFA. We illustrate just a weakly informative prior can greatly improve the performance of Bayesian methods, for both estimation accuracy and uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.06178v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Junyang Wang, Kolyan Ray, Pablo Brito-Parada, Yves Plancherel, Tom Bide, Joseph Mankelow, John Morley, Julia Stegemann, Rupert Myers</dc:creator>
    </item>
    <item>
      <title>Aligning the Western Balkans power sectors with the European Green Deal</title>
      <link>https://arxiv.org/abs/2305.07433</link>
      <description>arXiv:2305.07433v2 Announce Type: replace 
Abstract: Located in Southern Europe, the Drina River Basin is shared between Bosnia and Herzegovina, Montenegro, and Serbia. The power sectors of the three countries have an exceptionally high dependence on coal for power generation. In this paper, we analyse different development pathways for achieving climate neutrality in these countries and explore the potential of variable renewable energy (VRE) and its role in power sector decarbonization. We investigate whether hydro and non-hydro renewables can enable a net-zero transition by 2050 and how VRE might affect the hydropower cascade shared by the three countries. The Open-Source Energy Modelling System (OSeMOSYS) was used to develop a model representation of the countries' power sectors. Findings show that the renewable potential of the countries is a significant 94.4 GW. This potential is 68% higher than previous assessments have shown. Under an Emission Limit scenario assuming net zero by 2050, 17% of this VRE potential is utilized to support the decarbonization of the power sectors. Additional findings show a limited impact of VRE technologies on total power generation output from the hydropower cascade. However, increased solar deployment shifts the operation of the cascade to increased short-term balancing, moving from baseload to more responsive power generation patterns. Prolonged use of thermal power plants is observed under scenarios assuming high wholesale electricity prices, leading to increased emissions. Results from scenarios with low cost of electricity trade suggest power sector developments that lead to decreased energy security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.07433v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emir Fejzi\'c, Taco Niet, Cameron Wade, Will Usher</dc:creator>
    </item>
    <item>
      <title>SB-ETAS: using simulation based inference for scalable, likelihood-free inference for the ETAS model of earthquake occurrences</title>
      <link>https://arxiv.org/abs/2404.16590</link>
      <description>arXiv:2404.16590v2 Announce Type: replace 
Abstract: Performing Bayesian inference for the Epidemic-Type Aftershock Sequence (ETAS) model of earthquakes typically requires MCMC sampling using the likelihood function or estimating the latent branching structure. These tasks have computational complexity $O(n^2)$ with the number of earthquakes and therefore do not scale well with new enhanced catalogs, which can now contain an order of $10^6$ events. On the other hand, simulation from the ETAS model can be done more quickly $O(n \log n )$. We present SB-ETAS: simulation-based inference for the ETAS model. This is an approximate Bayesian method which uses Sequential Neural Posterior Estimation (SNPE), a machine learning based algorithm for learning posterior distributions from simulations. SB-ETAS can successfully approximate ETAS posterior distributions on shorter catalogues where it is computationally feasible to compare with MCMC sampling. Furthermore, the scaling of SB-ETAS makes it feasible to fit to very large earthquake catalogs, such as one for Southern California dating back to 1932. SB-ETAS can find Bayesian estimates of ETAS parameters for this catalog in less than 10 hours on a standard laptop, which would have taken over 2 weeks using MCMC. Looking beyond the standard ETAS model, this simulation based framework would allow earthquake modellers to define and infer parameters for much more complex models that have intractable likelihood functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16590v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Stockman, Daniel J. Lawson, Maximilian J. Werner</dc:creator>
    </item>
    <item>
      <title>Instrumental Variable Estimation for Compositional Treatments</title>
      <link>https://arxiv.org/abs/2106.11234</link>
      <description>arXiv:2106.11234v3 Announce Type: replace-cross 
Abstract: Many scientific datasets are compositional in nature. Important biological examples include species abundances in ecology, cell-type compositions derived from single-cell sequencing data, and amplicon abundance data in microbiome research. Here, we provide a causal view on compositional data in an instrumental variable setting where the composition acts as the cause. First, we crisply articulate potential pitfalls for practitioners regarding the interpretation of compositional causes from the viewpoint of interventions and warn against attributing causal meaning to common summary statistics such as diversity indices in microbiome data analysis. We then advocate for and develop multivariate methods using statistical data transformations and regression techniques that take the special structure of the compositional sample space into account while still yielding scientifically interpretable results. In a comparative analysis on synthetic and real microbiome data we show the advantages and limitations of our proposal. We posit that our analysis provides a useful framework and guidance for valid and informative cause-effect estimation in the context of compositional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.11234v3</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elisabeth Ailer, Christian L. M\"uller, Niki Kilbertus</dc:creator>
    </item>
    <item>
      <title>On Robust Inference in Time Series Regression</title>
      <link>https://arxiv.org/abs/2203.04080</link>
      <description>arXiv:2203.04080v3 Announce Type: replace-cross 
Abstract: Least squares regression with heteroskedasticity consistent standard errors ("OLS-HC regression") has proved very useful in cross section environments. However, several major difficulties, which are generally overlooked, must be confronted when transferring the HC technology to time series environments via heteroskedasticity and autocorrelation consistent standard errors ("OLS-HAC regression"). First, in plausible time-series environments, OLS parameter estimates can be inconsistent, so that OLS-HAC inference fails even asymptotically. Second, most economic time series have autocorrelation, which renders OLS parameter estimates inefficient. Third, autocorrelation similarly renders conditional predictions based on OLS parameter estimates inefficient. Finally, the structure of popular HAC covariance matrix estimators is ill-suited for capturing the autoregressive autocorrelation typically present in economic time series, which produces large size distortions and reduced power in HAC-based hypothesis testing, in all but the largest samples. We show that all four problems are largely avoided by the use of a simple and easily-implemented dynamic regression procedure, which we call DURBIN. We demonstrate the advantages of DURBIN with detailed simulations covering a range of practical issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.04080v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Richard T. Baillie, Francis X. Diebold, George Kapetanios, Kun Ho Kim, Aaron Mora</dc:creator>
    </item>
    <item>
      <title>Finding Pareto Efficient Redistricting Plans with Short Bursts</title>
      <link>https://arxiv.org/abs/2304.00427</link>
      <description>arXiv:2304.00427v2 Announce Type: replace-cross 
Abstract: Redistricting practitioners must balance many competing constraints and criteria when drawing district boundaries. To aid in this process, researchers have developed many methods for optimizing districting plans according to one or more criteria. This research note extends a recently-proposed single-criterion optimization method, short bursts (Cannon et al., 2023), to handle the multi-criterion case, and in doing so approximate the Pareto frontier for any set of constraints. We study the empirical performance of the method in a realistic setting and find it behaves as expected and is not very sensitive to algorithmic parameters. The proposed approach, which is implemented in open-source software, should allow researchers and practitioners to better understand the tradeoffs inherent to the redistricting process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.00427v2</guid>
      <category>cs.CY</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cory McCartan</dc:creator>
    </item>
    <item>
      <title>Parameter Inference for Degenerate Diffusion Processes</title>
      <link>https://arxiv.org/abs/2307.16485</link>
      <description>arXiv:2307.16485v3 Announce Type: replace-cross 
Abstract: We study parametric inference for ergodic diffusion processes with a degenerate diffusion matrix. Existing research focuses on a particular class of hypo-elliptic SDEs, with components split into `rough'/`smooth' and noise from rough components propagating directly onto smooth ones, but some critical model classes arising in applications have yet to be explored. We aim to cover this gap, thus analyse the highly degenerate class of SDEs, where components split into further sub-groups. Such models include e.g. the notable case of generalised Langevin equations. We propose a tailored time-discretisation scheme and provide asymptotic results supporting our scheme in the context of high-frequency, full observations. The proposed discretisation scheme is applicable in much more general data regimes and is shown to overcome biases via simulation studies also in the practical case when only a smooth component is observed. Joint consideration of our study for highly degenerate SDEs and existing research provides a general `recipe' for the development of time-discretisation schemes to be used within statistical methods for general classes of hypo-elliptic SDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.16485v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spa.2024.104384</arxiv:DOI>
      <dc:creator>Yuga Iguchi, Alexandros Beskos, Matthew Graham</dc:creator>
    </item>
    <item>
      <title>TimeGPT-1</title>
      <link>https://arxiv.org/abs/2310.03589</link>
      <description>arXiv:2310.03589v3 Announce Type: replace-cross 
Abstract: In this paper, we introduce TimeGPT, the first foundation model for time series, capable of generating accurate predictions for diverse datasets not seen during training. We evaluate our pre-trained model against established statistical, machine learning, and deep learning methods, demonstrating that TimeGPT zero-shot inference excels in performance, efficiency, and simplicity. Our study provides compelling evidence that insights from other domains of artificial intelligence can be effectively applied to time series analysis. We conclude that large-scale time series models offer an exciting opportunity to democratize access to precise predictions and reduce uncertainty by leveraging the capabilities of contemporary advancements in deep learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.03589v3</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Azul Garza, Cristian Challu, Max Mergenthaler-Canseco</dc:creator>
    </item>
    <item>
      <title>Bayesian sample size determination using robust commensurate priors with interpretable discrepancy weights</title>
      <link>https://arxiv.org/abs/2401.10592</link>
      <description>arXiv:2401.10592v2 Announce Type: replace-cross 
Abstract: Randomized controlled clinical trials provide the gold standard for evidence generation in relation to the efficacy of a new treatment in medical research. Relevant information from previous studies may be desirable to incorporate in the design and analysis of a new trial, with the Bayesian paradigm providing a coherent framework to formally incorporate prior knowledge. Many established methods involve the use of a discounting factor, sometimes related to a measure of `similarity' between historical and the new trials. However, it is often the case that the sample size is highly nonlinear in those discounting factors. This hinders communication with subject-matter experts to elicit sensible values for borrowing strength at the trial design stage. Focusing on a commensurate predictive prior method that can incorporate historical data from multiple sources, we highlight a particular issue of nonmonotonicity and explain why this causes issues with interpretability of the discounting factors (hereafter referred to as `weights'). We propose a solution for this, from which an analytical sample size formula is derived. We then propose a linearization technique such that the sample size changes uniformly over the weights. Our approach leads to interpretable weights that represent the probability that historical data are (ir)relevant to the new trial, and could therefore facilitate easier elicitation of expert opinion on their values.
  Keywords: Bayesian sample size determination; Commensurate priors; Historical borrowing; Prior aggregation; Uniform shrinkage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10592v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lou E. Whitehead, James M. S. Wason, Oliver Sailer, Haiyan Zheng</dc:creator>
    </item>
    <item>
      <title>A Bayesian Approach to Online Learning for Contextual Restless Bandits with Applications to Public Health</title>
      <link>https://arxiv.org/abs/2402.04933</link>
      <description>arXiv:2402.04933v2 Announce Type: replace-cross 
Abstract: Public health programs often provide interventions to encourage beneficiary adherence,and effectively allocating interventions is vital for producing the greatest overall health outcomes. Such resource allocation problems are often modeled as restless multi-armed bandits (RMABs) with unknown underlying transition dynamics, hence requiring online reinforcement learning (RL). We present Bayesian Learning for Contextual RMABs (BCoR), an online RL approach for RMABs that novelly combines techniques in Bayesian modeling with Thompson sampling to flexibly model the complex RMAB settings present in public health program adherence problems, such as context and non-stationarity. BCoR's key strength is the ability to leverage shared information within and between arms to learn the unknown RMAB transition dynamics quickly in intervention-scarce settings with relatively short time horizons, which is common in public health applications. Empirically, BCoR achieves substantially higher finite-sample performance over a range of experimental settings, including an example based on real-world adherence data that was developed in collaboration with ARMMAN, an NGO in India which runs a large-scale maternal health program, showcasing BCoR practical utility and potential for real-world deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04933v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Biyonka Liang, Lily Xu, Aparna Taneja, Milind Tambe, Lucas Janson</dc:creator>
    </item>
    <item>
      <title>Improving prediction models by incorporating external data with weights based on similarity</title>
      <link>https://arxiv.org/abs/2405.07631</link>
      <description>arXiv:2405.07631v2 Announce Type: replace-cross 
Abstract: In clinical settings, we often face the challenge of building prediction models based on small observational data sets. For example, such a data set might be from a medical center in a multi-center study. Differences between centers might be large, thus requiring specific models based on the data set from the target center. Still, we want to borrow information from the external centers, to deal with small sample sizes. There are approaches that either assign weights to each external data set or each external observation. To incorporate information on differences between data sets and observations, we propose an approach that combines both into weights that can be incorporated into a likelihood for fitting regression models. Specifically, we suggest weights at the data set level that incorporate information on how well the models that provide the observation weights distinguish between data sets. Technically, this takes the form of inverse probability weighting. We explore different scenarios where covariates and outcomes differ among data sets, informing our simulation design for method evaluation. The concept of effective sample size is used for understanding the effectiveness of our subgroup modeling approach. We demonstrate our approach through a clinical application, predicting applied radiotherapy doses for cancer patients. Generally, the proposed approach provides improved prediction performance when external data sets are similar. We thus provide a method for quantifying similarity of external data sets to the target data set and use this similarity to include external observations for improving performance in a target data set prediction modeling task with small data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07631v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Behrens, Maryam Farhadizadeh, Angelika Rohde, Alexander R\"uhle, Nils H. Nicolay, Harald Binder, Daniela Z\"oller</dc:creator>
    </item>
    <item>
      <title>Acquiring Better Load Estimates by Combining Anomaly and Change-point Detection in Power Grid Time-series Measurements</title>
      <link>https://arxiv.org/abs/2405.16164</link>
      <description>arXiv:2405.16164v2 Announce Type: replace-cross 
Abstract: In this paper we present novel methodology for automatic anomaly and switch event filtering to improve load estimation in power grid systems. By leveraging unsupervised methods with supervised optimization, our approach prioritizes interpretability while ensuring robust and generalizable performance on unseen data. Through experimentation, a combination of binary segmentation for change point detection and statistical process control for anomaly detection emerges as the most effective strategy, specifically when ensembled in a novel sequential manner. Results indicate the clear wasted potential when filtering is not applied. The automatic load estimation is also fairly accurate, with approximately 90% of estimates falling within a 10% error margin, with only a single significant failure in both the minimum and maximum load estimates across 60 measurements in the test set. Our methodology's interpretability makes it particularly suitable for critical infrastructure planning, thereby enhancing decision-making processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16164v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roel Bouman, Linda Schmeitz, Luco Buise, Jacco Heres, Yuliya Shapovalova, Tom Heskes</dc:creator>
    </item>
  </channel>
</rss>
