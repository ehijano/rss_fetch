<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Jun 2024 04:00:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 07 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Estimation of Out-of-Sample Sharpe Ratio for High Dimensional Portfolio Optimization</title>
      <link>https://arxiv.org/abs/2406.03954</link>
      <description>arXiv:2406.03954v1 Announce Type: new 
Abstract: Portfolio optimization aims at constructing a realistic portfolio with significant out-of-sample performance, which is typically measured by the out-of-sample Sharpe ratio. However, due to in-sample optimism, it is inappropriate to use the in-sample estimated covariance to evaluate the out-of-sample Sharpe, especially in the high dimensional settings. In this paper, we propose a novel method to estimate the out-of-sample Sharpe ratio using only in-sample data, based on random matrix theory. Furthermore, portfolio managers can use the estimated out-of-sample Sharpe as a criterion to decide the best tuning for constructing their portfolios. Specifically, we consider the classical framework of Markowits mean-variance portfolio optimization with known mean vector and the high dimensional regime of $p/n \to c \in (0,\infty)$, where $p$ is the portfolio dimension and $n$ is the number of samples or time points. We propose to correct the sample covariance by a regularization matrix and provide a consistent estimator of its Sharpe ratio. The new estimator works well under either of three conditions: (1) bounded covariance spectrum, (2) arbitrary number of diverging spikes when $c &lt; 1$, and (3) fixed number of diverging spikes when $c \ge 1$. We can also extend the results to construct global minimum variance portfolio and correct out-of-sample efficient frontier. We demonstrate the effectiveness of our approach through comprehensive simulations and real data experiments. Our results highlight the potential of this methodology as a powerful tool for portfolio optimization in high dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03954v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xuran Meng, Yuan Cao, Weichen Wang</dc:creator>
    </item>
    <item>
      <title>Strong Approximations for Empirical Processes Indexed by Lipschitz Functions</title>
      <link>https://arxiv.org/abs/2406.04191</link>
      <description>arXiv:2406.04191v1 Announce Type: new 
Abstract: This paper presents new uniform Gaussian strong approximations for empirical processes indexed by classes of functions based on $d$-variate random vectors ($d\geq1$). First, a uniform Gaussian strong approximation is established for general empirical processes indexed by Lipschitz functions, encompassing and improving on all previous results in the literature. When specialized to the setting considered by Rio (1994), and certain constraints on the function class hold, our result improves the approximation rate $n^{-1/(2d)}$ to $n^{-1/\max\{d,2\}}$, up to the same $\operatorname{polylog} n$ term, where $n$ denotes the sample size. Remarkably, we establish a valid uniform Gaussian strong approximation at the optimal rate $n^{-1/2}\log n$ for $d=2$, which was previously known to be valid only for univariate ($d=1$) empirical processes via the celebrated Hungarian construction (Koml\'os et al., 1975). Second, a uniform Gaussian strong approximation is established for a class of multiplicative separable empirical processes indexed by Lipschitz functions, which address some outstanding problems in the literature (Chernozhukov et al., 2014, Section 3). In addition, two other uniform Gaussian strong approximation results are presented for settings where the function class takes the form of a sequence of Haar basis based on generalized quasi-uniform partitions. We demonstrate the improvements and usefulness of our new strong approximation results with several statistical applications to nonparametric density and regression estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04191v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo (Rae),  Ruiqi (Rae),  Yu</dc:creator>
    </item>
    <item>
      <title>Equivalence Set Restricted Latent Class Models (ESRLCM)</title>
      <link>https://arxiv.org/abs/2406.03653</link>
      <description>arXiv:2406.03653v1 Announce Type: cross 
Abstract: Latent Class Models (LCMs) are used to cluster multivariate categorical data, commonly used to interpret survey responses. We propose a novel Bayesian model called the Equivalence Set Restricted Latent Class Model (ESRLCM). This model identifies clusters who have common item response probabilities, and does so more generically than traditional restricted latent attribute models. We verify the identifiability of ESRLCMs, and demonstrate the effectiveness in both simulations and real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03653v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jesse Bowers, Steve Culpepper</dc:creator>
    </item>
    <item>
      <title>Multiscale Tests for Point Processes and Longitudinal Networks</title>
      <link>https://arxiv.org/abs/2406.03681</link>
      <description>arXiv:2406.03681v1 Announce Type: cross 
Abstract: We propose a new testing framework applicable to both the two-sample problem on point processes and the community detection problem on rectangular arrays of point processes, which we refer to as longitudinal networks; the latter problem is useful in situations where we observe interactions among a group of individuals over time. Our framework is based on a multiscale discretization scheme that consider not just the global null but also a collection of nulls local to small regions in the domain; in the two-sample problem, the local rejections tell us where the intensity functions differ and in the longitudinal network problem, the local rejections tell us when the community structure is most salient. We provide theoretical analysis for the two-sample problem and show that our method has minimax optimal power under a Holder continuity condition. We provide extensive simulation and real data analysis demonstrating the practicality of our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03681v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Youmeng Jiang, Min Xu</dc:creator>
    </item>
    <item>
      <title>CLT for Linear Spectral Statistics in High-Dimensional Random Effects Models</title>
      <link>https://arxiv.org/abs/2406.03719</link>
      <description>arXiv:2406.03719v1 Announce Type: cross 
Abstract: We study sample covariance matrices arising from multi-level components of variance. Thus, let $ B_n=\frac{1}{N}\sum_{j=1}^NT_{j}^{1/2}x_jx_j^TT_{j}^{1/2}$, where $x_j\in R^n$ are i.i.d. standard Gaussian, and $T_{j}=\sum_{r=1}^kl_{jr}^2\Sigma_{r}$ are $n\times n$ real symmetric matrices with bounded spectral norm, corresponding to $k$ levels of variation. As the matrix dimensions $n$ and $N$ increase proportionally, we show that the linear spectral statistics (LSS) of $B_n$ have Gaussian limits. The CLT is expressed as the convergence of a set of LSS to a standard multivariate Gaussian after centering by a mean vector $\Gamma_n$ and a covariance matrix $\Lambda_n$ which depend on $n$ and $N$ and may be evaluated numerically. Our work is motivated by the estimation of high-dimensional covariance matrices between phenotypic traits in quantitative genetics, particularly within nested linear random-effects models with up to $k$ levels of randomness. Our proof builds on the Bai-Silverstein \cite{baisilverstein2004} martingale method with some innovation to handle the multi-level setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03719v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ran Xie, Iain Johnstone</dc:creator>
    </item>
    <item>
      <title>Reassessing How to Compare and Improve the Calibration of Machine Learning Models</title>
      <link>https://arxiv.org/abs/2406.04068</link>
      <description>arXiv:2406.04068v1 Announce Type: cross 
Abstract: A machine learning model is calibrated if its predicted probability for an outcome matches the observed frequency for that outcome conditional on the model prediction. This property has become increasingly important as the impact of machine learning models has continued to spread to various domains. As a result, there are now a dizzying number of recent papers on measuring and improving the calibration of (specifically deep learning) models. In this work, we reassess the reporting of calibration metrics in the recent literature. We show that there exist trivial recalibration approaches that can appear seemingly state-of-the-art unless calibration and prediction metrics (i.e. test accuracy) are accompanied by additional generalization metrics such as negative log-likelihood. We then derive a calibration-based decomposition of Bregman divergences that can be used to both motivate a choice of calibration metric based on a generalization metric, and to detect trivial calibration. Finally, we apply these ideas to develop a new extension to reliability diagrams that can be used to jointly visualize calibration as well as the estimated generalization error of a model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04068v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muthu Chidambaram, Rong Ge</dc:creator>
    </item>
    <item>
      <title>Dynamic angular synchronization under smoothness constraints</title>
      <link>https://arxiv.org/abs/2406.04071</link>
      <description>arXiv:2406.04071v1 Announce Type: cross 
Abstract: Given an undirected measurement graph $\mathcal{H} = ([n], \mathcal{E})$, the classical angular synchronization problem consists of recovering unknown angles $\theta_1^*,\dots,\theta_n^*$ from a collection of noisy pairwise measurements of the form $(\theta_i^* - \theta_j^*) \mod 2\pi$, for all $\{i,j\} \in \mathcal{E}$. This problem arises in a variety of applications, including computer vision, time synchronization of distributed networks, and ranking from pairwise comparisons. In this paper, we consider a dynamic version of this problem where the angles, and also the measurement graphs evolve over $T$ time points. Assuming a smoothness condition on the evolution of the latent angles, we derive three algorithms for joint estimation of the angles over all time points. Moreover, for one of the algorithms, we establish non-asymptotic recovery guarantees for the mean-squared error (MSE) under different statistical models. In particular, we show that the MSE converges to zero as $T$ increases under milder conditions than in the static setting. This includes the setting where the measurement graphs are highly sparse and disconnected, and also when the measurement noise is large and can potentially increase with $T$. We complement our theoretical results with experiments on synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04071v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ernesto Araya, Mihai Cucuringu, Hemant Tyagi</dc:creator>
    </item>
    <item>
      <title>Copula-based models for correlated circular data</title>
      <link>https://arxiv.org/abs/2406.04085</link>
      <description>arXiv:2406.04085v1 Announce Type: cross 
Abstract: We exploit Gaussian copulas to specify a class of multivariate circular distributions and obtain parametric models for the analysis of correlated circular data. This approach provides a straightforward extension of traditional multivariate normal models to the circular setting, without imposing restrictions on the marginal data distribution nor requiring overwhelming routines for parameter estimation. The proposal is illustrated on two case studies of animal orientation and sea currents, where we propose an autoregressive model for circular time series and a geostatistical model for circular spatial series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04085v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Lagona, Marco Mingione</dc:creator>
    </item>
    <item>
      <title>Optimal Batched Linear Bandits</title>
      <link>https://arxiv.org/abs/2406.04137</link>
      <description>arXiv:2406.04137v1 Announce Type: cross 
Abstract: We introduce the E$^4$ algorithm for the batched linear bandit problem, incorporating an Explore-Estimate-Eliminate-Exploit framework. With a proper choice of exploration rate, we prove E$^4$ achieves the finite-time minimax optimal regret with only $O(\log\log T)$ batches, and the asymptotically optimal regret with only $3$ batches as $T\rightarrow\infty$, where $T$ is the time horizon. We further prove a lower bound on the batch complexity of linear contextual bandits showing that any asymptotically optimal algorithm must require at least $3$ batches in expectation as $T\rightarrow\infty$, which indicates E$^4$ achieves the asymptotic optimality in regret and batch complexity simultaneously. To the best of our knowledge, E$^4$ is the first algorithm for linear bandits that simultaneously achieves the minimax and asymptotic optimality in regret with the corresponding optimal batch complexities. In addition, we show that with another choice of exploration rate E$^4$ achieves an instance-dependent regret bound requiring at most $O(\log T)$ batches, and maintains the minimax optimality and asymptotic optimality. We conduct thorough experiments to evaluate our algorithm on randomly generated instances and the challenging \textit{End of Optimism} instances \citep{lattimore2017end} which were shown to be hard to learn for optimism based algorithms. Empirical results show that E$^4$ consistently outperforms baseline algorithms with respect to regret minimization, batch complexity, and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04137v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuanfei Ren, Tianyuan Jin, Pan Xu</dc:creator>
    </item>
    <item>
      <title>Normal approximations for the multivariate inverse Gaussian distribution and asymmetric kernel smoothing on $d$-dimensional half-spaces</title>
      <link>https://arxiv.org/abs/2209.04757</link>
      <description>arXiv:2209.04757v3 Announce Type: replace 
Abstract: This paper introduces a novel density estimator supported on $d$-dimensional half-spaces. It stands out as the first asymmetric kernel smoother for half-spaces in the literature. Using the multivariate inverse Gaussian (MIG) density from Minami (2003) as the kernel and incorporating locally adaptive parameters, the estimator achieves desirable boundary properties. To analyze its mean integrated squared error (MISE) and asymptotic normality, a local limit theorem and probability metric bounds are established between the MIG and the corresponding multivariate Gaussian distribution with the same mean vector and covariance matrix, which may also be of independent interest. Additionally, a new algorithm for generating MIG random vectors is developed, proving to be faster and more accurate than Minami's algorithm based on a Brownian hitting location representation. This algorithm is then used to discuss and compare optimal MISE and likelihood cross-validation bandwidths for the estimator in a simulation study under various target distributions. As an illustration, the MIG asymmetric kernel is used to smooth the posterior distribution of a generalized Pareto model fitted to large electromagnetic storms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.04757v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'eo R. Belzile, Alain Desgagn\'e, Christian Genest, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Tight Bounds on the Laplace Approximation Accuracy in High Dimensions</title>
      <link>https://arxiv.org/abs/2305.17604</link>
      <description>arXiv:2305.17604v4 Announce Type: replace 
Abstract: In Bayesian inference, a widespread technique to compute integrals against a high-dimensional posterior is to use a Gaussian proxy to the posterior known as the Laplace approximation. We address the question of accuracy of the approximation in terms of TV distance, in the regime in which dimension $d$ grows with sample size $n$. Multiple prior works have shown the requirement $d^3\ll n$ is sufficient for accuracy of the approximation. But in a recent breakthrough, Kasprzak et al, 2022 derived an upper bound scaling as $d/\sqrt n$. In this work, we further refine our understanding of the Laplace approximation error by decomposing the TV error into an $O(d/\sqrt n)$ leading order term, and an $O(d^2/n)$ remainder. This decomposition has far reaching implications: first, we use it to prove that the requirement $d^2\ll n$ cannot in general be improved by showing TV$\gtrsim d/\sqrt n$ for a posterior stemming from logistic regression with Gaussian design. Second, the decomposition provides tighter and more easily computable upper bounds on the TV error. Our result also opens the door to proving the BvM in the $d^2\ll n$ regime, and correcting the Laplace approximation to account for skew; this is pursued in two follow-up works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.17604v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anya Katsevich</dc:creator>
    </item>
    <item>
      <title>Degree Heterogeneity in Higher-Order Networks: Inference in the Hypergraph $\boldsymbol{\beta}$-Model</title>
      <link>https://arxiv.org/abs/2307.02818</link>
      <description>arXiv:2307.02818v4 Announce Type: replace 
Abstract: The $\boldsymbol{\beta}$-model for random graphs is commonly used for representing pairwise interactions in a network with degree heterogeneity. Going beyond pairwise interactions, Stasi et al. (2014) introduced the hypergraph $\boldsymbol{\beta}$-model for capturing degree heterogeneity in networks with higher-order (multi-way) interactions. In this paper we initiate the rigorous study of the hypergraph $\boldsymbol{\beta}$-model with multiple layers, which allows for hyperedges of different sizes across the layers. To begin with, we derive the rates of convergence of the maximum likelihood (ML) estimate and establish their minimax rate optimality. We also derive the limiting distribution of the ML estimate and construct asymptotically valid confidence intervals for the model parameters. Next, we consider the goodness-of-fit problem in the hypergraph $\boldsymbol{\beta}$-model. Specifically, we establish the asymptotic normality of the likelihood ratio (LR) test under the null hypothesis, derive its detection threshold, and also its limiting power at the threshold. Interestingly, the detection threshold of the LR test turns out to be minimax optimal, that is, all tests are asymptotically powerless below this threshold. The theoretical results are further validated in numerical experiments. In addition to developing the theoretical framework for estimation and inference for hypergraph $\boldsymbol{\beta}$-models, the above results fill a number of gaps in the graph $\boldsymbol{\beta}$-model literature, such as the minimax optimality of the ML estimates and the non-null properties of the LR test, which, to the best of our knowledge, have not been studied before.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.02818v4</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.SI</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sagnik Nandy, Bhaswar B. Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Statistically Optimal Generative Modeling with Maximum Deviation from the Empirical Distribution</title>
      <link>https://arxiv.org/abs/2307.16422</link>
      <description>arXiv:2307.16422v2 Announce Type: replace 
Abstract: This paper explores the problem of generative modeling, aiming to simulate diverse examples from an unknown distribution based on observed examples. While recent studies have focused on quantifying the statistical precision of popular algorithms, there is a lack of mathematical evaluation regarding the non-replication of observed examples and the creativity of the generative model. We present theoretical insights into this aspect, demonstrating that the Wasserstein GAN, constrained to left-invertible push-forward maps, generates distributions that avoid replication and significantly deviate from the empirical distribution. Importantly, we show that left-invertibility achieves this without compromising the statistical optimality of the resulting generator. Our most important contribution provides a finite-sample lower bound on the Wasserstein-1 distance between the generative distribution and the empirical one. We also establish a finite-sample upper bound on the distance between the generative distribution and the true data-generating one. Both bounds are explicit and show the impact of key parameters such as sample size, dimensions of the ambient and latent spaces, noise level, and smoothness measured by the Lipschitz constant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.16422v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elen Vardanyan, Sona Hunanyan, Tigran Galstyan, Arshak Minasyan, Arnak Dalalyan</dc:creator>
    </item>
    <item>
      <title>Mixtures of Discrete Decomposable Graphical Models</title>
      <link>https://arxiv.org/abs/2401.15950</link>
      <description>arXiv:2401.15950v2 Announce Type: replace 
Abstract: We study mixtures of decomposable graphical models, focusing on their ideals and dimensions. For mixtures of clique stars, we characterize the ideals in terms of ideals of mixtures of independence models. We also give a recursive formula for their ML degrees. Finally, we prove that second secant varieties of all other decomposable graphical models have the expected dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15950v2</guid>
      <category>math.ST</category>
      <category>math.AG</category>
      <category>math.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yulia Alexandr, Jane Ivy Coons, Nils Sturma</dc:creator>
    </item>
    <item>
      <title>On Wilks' joint moment formulas for embedded principal minors of Wishart random matrices</title>
      <link>https://arxiv.org/abs/2403.06330</link>
      <description>arXiv:2403.06330v2 Announce Type: replace 
Abstract: In 1934, the American statistician Samuel S. Wilks derived remarkable formulas for the joint moments of embedded principal minors of sample covariance matrices in multivariate Gaussian populations, and he used them to compute the moments of sample statistics in various applications related to multivariate linear regression. These important but little-known moment results were extended in 1963 by the Australian statistician A. Graham Constantine using Bartlett's decomposition. In this note, a new proof of Wilks' results is derived using the concept of iterated Schur complements, thereby bypassing Bartlett's decomposition. Furthermore, Wilks' open problem of evaluating joint moments of disjoint principal minors of Wishart random matrices is related to the Gaussian product inequality conjecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06330v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Stat (2024)</arxiv:journal_reference>
      <dc:creator>Christian Genest, Fr\'ed\'eric Ouimet, Donald Richards</dc:creator>
    </item>
    <item>
      <title>Products, Abstractions and Inclusions of Causal Spaces</title>
      <link>https://arxiv.org/abs/2406.00388</link>
      <description>arXiv:2406.00388v2 Announce Type: replace 
Abstract: Causal spaces have recently been introduced as a measure-theoretic framework to encode the notion of causality. While it has some advantages over established frameworks, such as structural causal models, the theory is so far only developed for single causal spaces. In many mathematical theories, not least the theory of probability spaces of which causal spaces are a direct extension, combinations of objects and maps between objects form a central part. In this paper, taking inspiration from such objects in probability theory, we propose the definitions of products of causal spaces, as well as (stochastic) transformations between causal spaces. In the context of causality, these quantities can be given direct semantic interpretations as causally independent components, abstractions and extensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00388v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Buchholz, Junhyung Park, Bernhard Sch\"olkopf</dc:creator>
    </item>
    <item>
      <title>Learning Coefficients in Semi-Regular Models</title>
      <link>https://arxiv.org/abs/2406.02646</link>
      <description>arXiv:2406.02646v2 Announce Type: replace 
Abstract: Statistical learning models such as multilayer neural networks and mixed distributions are widely used, and understanding the accuracy of these models is crucial for their use. Recent advances have clarified theoretical learning accuracy in Bayesian inference, where metrics such as generalization loss and free energy are used to measure the accuracy of predictive distributions. It has become clear that the asymptotic behavior of these metrics is determined by a rational number specific to each statistical model, known as the learning coefficient (real log canonical threshold).
  The problem of determining the learning coefficient is known to be reducible to the problem of finding the normal crossing of Kullback-Leibler divergence in relation to algebraic geometry. In this context, it is crucial to perform appropriate coordinate transformations and blow-ups. This paper attempts to derive appropriate variable transformations and blow-ups from the properties of the log-likelihood ratio function. That is, instead of dealing with the Kullback-Leibler information itself, it uses the properties of the log-likelihood ratio function before taking the expectation to calculate the real log canonical threshold. This approach has not been considered in previous research.
  Using these variable transformations and blow-ups, this paper provides the exact values of the learning coefficients and their calculation methods for statistical models that meet simple conditions next to the regular conditions (referred to as semi-regular models), and as specific examples, provides the learning coefficients for semi-regular models with two parameters and for those models where the random variables take a finite number of values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02646v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuki Kurumadani</dc:creator>
    </item>
    <item>
      <title>A Measure-Theoretic Axiomatisation of Causality</title>
      <link>https://arxiv.org/abs/2305.17139</link>
      <description>arXiv:2305.17139v3 Announce Type: replace-cross 
Abstract: Causality is a central concept in a wide range of research areas, yet there is still no universally agreed axiomatisation of causality. We view causality both as an extension of probability theory and as a study of \textit{what happens when one intervenes on a system}, and argue in favour of taking Kolmogorov's measure-theoretic axiomatisation of probability as the starting point towards an axiomatisation of causality. To that end, we propose the notion of a \textit{causal space}, consisting of a probability space along with a collection of transition probability kernels, called \textit{causal kernels}, that encode the causal information of the space. Our proposed framework is not only rigorously grounded in measure theory, but it also sheds light on long-standing limitations of existing frameworks including, for example, cycles, latent variables and stochastic processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.17139v3</guid>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhyung Park, Simon Buchholz, Bernhard Sch\"olkopf, Krikamol Muandet</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction for Deep Classifier via Label Ranking</title>
      <link>https://arxiv.org/abs/2310.06430</link>
      <description>arXiv:2310.06430v2 Announce Type: replace-cross 
Abstract: Conformal prediction is a statistical framework that generates prediction sets containing ground-truth labels with a desired coverage guarantee. The predicted probabilities produced by machine learning models are generally miscalibrated, leading to large prediction sets in conformal prediction. To address this issue, we propose a novel algorithm named $\textit{Sorted Adaptive Prediction Sets}$ (SAPS), which discards all the probability values except for the maximum softmax probability. The key idea behind SAPS is to minimize the dependence of the non-conformity score on the probability values while retaining the uncertainty information. In this manner, SAPS can produce compact prediction sets and communicate instance-wise uncertainty. Extensive experiments validate that SAPS not only lessens the prediction sets but also broadly enhances the conditional coverage rate of prediction sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06430v2</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianguo Huang, Huajun Xi, Linjun Zhang, Huaxiu Yao, Yue Qiu, Hongxin Wei</dc:creator>
    </item>
    <item>
      <title>An operator learning perspective on parameter-to-observable maps</title>
      <link>https://arxiv.org/abs/2402.06031</link>
      <description>arXiv:2402.06031v2 Announce Type: replace-cross 
Abstract: Computationally efficient surrogates for parametrized physical models play a crucial role in science and engineering. Operator learning provides data-driven surrogates that map between function spaces. However, instead of full-field measurements, often the available data are only finite-dimensional parametrizations of model inputs or finite observables of model outputs. Building on Fourier Neural Operators, this paper introduces the Fourier Neural Mappings (FNMs) framework that is able to accommodate such finite-dimensional vector inputs or outputs. The paper develops universal approximation theorems for the method. Moreover, in many applications the underlying parameter-to-observable (PtO) map is defined implicitly through an infinite-dimensional operator, such as the solution operator of a partial differential equation. A natural question is whether it is more data-efficient to learn the PtO map end-to-end or first learn the solution operator and subsequently compute the observable from the full-field solution. A theoretical analysis of Bayesian nonparametric regression of linear functionals, which is of independent interest, suggests that the end-to-end approach can actually have worse sample complexity. Extending beyond the theory, numerical results for the FNM approximation of three nonlinear PtO maps demonstrate the benefits of the operator learning perspective that this paper adopts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06031v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Zhengyu Huang, Nicholas H. Nelsen, Margaret Trautner</dc:creator>
    </item>
  </channel>
</rss>
