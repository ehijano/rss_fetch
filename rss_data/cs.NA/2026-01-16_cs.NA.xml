<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Jan 2026 05:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Learning Ecological and Epidemic Processes using Neural ODEs, Kolmogorov-Arnold Network ODEs and SINDy</title>
      <link>https://arxiv.org/abs/2601.09811</link>
      <description>arXiv:2601.09811v1 Announce Type: new 
Abstract: We consider epidemic and ecological models to investigate their coupled dynamics. Starting with the classical Susceptible-Infected-Recovered (SIR) model for basic epidemic behavior and the predator-prey (Lotka-Volterra, LV) system for ecological interactions, we then combine these frameworks into a coupled Lotka-Volterra-Susceptible-Infected-Susceptible (LVSIS) model. The resulting system consists of four differential equations describing the evolution of susceptible and infected prey and predator populations, incorporating ecological interactions, disease transmission, and spatial dispersal. To learn the underlying dynamics directly from data, we employ several data-driven modeling frameworks: Neural Ordinary Differential Equations (Neural ODEs), Kolmogorov-Arnold Network Ordinary Differential Equations (KANODEs), and Sparse Identification of Nonlinear Dynamics (SINDy). Numerical experiments based on synthetic data are conducted to investigate the learning ability of these models in capturing the epidemic and ecological behavior. We further extend our approach to spatio-temporal models, aiming to uncover hidden local couplings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09811v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Vasilyeva, Zheng Wei, Kelum Gajamannage, Hyangim Ji, Aleksei Krasnikov, Alexey Sadovski</dc:creator>
    </item>
    <item>
      <title>An efficient probabilistic scheme for the exit time probability of $\alpha$-stable L\'evy process</title>
      <link>https://arxiv.org/abs/2601.09882</link>
      <description>arXiv:2601.09882v1 Announce Type: new 
Abstract: The {\alpha}-stable L\'evy process, commonly used to describe L\'evy flight, is characterized by discontinuous jumps and is widely used to model anomalous transport phenomena. In this study, we investigate the associated exit problem and propose a method to compute the exit time probability, which quantifies the likelihood that a trajectory starting from an initial condition exits a bounded region in phase space within a given time. This estimation plays a key role in understanding anomalous diffusion behavior. The proposed method approximates the {\alpha}-stable process by combining a Brownian motion with a compound Poisson process. The exit time probability is then modeled using a framework based on partial integro-differential equations (PIDEs). The Feynman-Kac formula provides a probabilistic representation of the solution, involving conditional expectations over stochastic differential equations. These expectations are computed via tailored quadrature rules and interpolation techniques. The proposed method achieves first-order convergence in time and offers significant computational advantages over standard Monte Carlo and deterministic approaches. In particular, it avoids assembling and solving large dense linear systems, resulting in improved efficiency. We demonstrate the method's accuracy and performance through two numerical examples, highlighting its applicability to physical transport problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09882v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minglei Yang, Diego del-Castillo-Negrete, Guannan Zhang</dc:creator>
    </item>
    <item>
      <title>Nonlinear numerical schemes using specular differentiation for initial value problems of first-order ordinary differential equations</title>
      <link>https://arxiv.org/abs/2601.09900</link>
      <description>arXiv:2601.09900v1 Announce Type: new 
Abstract: This paper proposes specular differentiation in one-dimensional Euclidean space and provides its fundamental analysis, including quasi-Fermat's theorem and the quasi-Mean Value Theorem. As an application, this paper develops several numerical schemes for solving initial value problems for first-order ordinary differential equations. Based on numerical simulations, we select one scheme and prove its first-order consistency and second-order local convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09900v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kiyuob Jung</dc:creator>
    </item>
    <item>
      <title>An Efficient Constant-Coefficient MSAV Scheme for Computing Vesicle Growth and Shrinkage</title>
      <link>https://arxiv.org/abs/2601.10057</link>
      <description>arXiv:2601.10057v1 Announce Type: new 
Abstract: We present a fast, unconditionally energy-stable numerical scheme for simulating vesicle deformation under osmotic pressure using a phase-field approach. The model couples an Allen-Cahn equation for the biomembrane interface with a variable-mobility Cahn-Hilliard equation governing mass exchange across the membrane. Classical approaches, including nonlinear multigrid and Multiple Scalar Auxiliary Variable (MSAV) methods, require iterative solution of variable-coefficient systems at each time step, resulting in substantial computational cost. We introduce a constant-coefficient MSAV (CC-MSAV) scheme that incorporates stabilization directly into the Cahn-Hilliard evolution equation rather than the chemical potential. This reformulation yields fully decoupled constant-coefficient elliptic problems solvable via fast discrete cosine transform (DCT), eliminating iterative solvers entirely. The method achieves O(N^2 log N) complexity per time step while preserving unconditional energy stability and discrete mass conservation. Numerical experiments verify second-order temporal and spatial accuracy, mass conservation to relative errors below 5 x 10^-11, and close agreement with nonlinear multigrid benchmarks. On grids with N &gt;= 2048, CC-MSAV achieves 6-15x overall speedup compared to classical MSAV with optimized preconditioning, while the dominant Cahn-Hilliard subsystem is accelerated by up to two orders of magnitude. These efficiency gains, achieved without sacrificing accuracy, make CC-MSAV particularly well suited for large-scale simulations of vesicle dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10057v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiwei Zhang, Shuwang Li, John Lowengrub, Steven M. Wise</dc:creator>
    </item>
    <item>
      <title>New Second-order Convergent Schemes for Solving decoupled FBSDEs</title>
      <link>https://arxiv.org/abs/2601.10149</link>
      <description>arXiv:2601.10149v1 Announce Type: new 
Abstract: This paper proposes a new second-order symmetric algorithm for solving decoupled forward-backward stochastic differential equations. Inspired by the alternating direction implicit splitting method for partial differential equations, we split the generator into the sum of two functions. In the computation of the value process Y, explicit and implicit schemes are alternately applied to these two generators, while the algorithms from \citep{ZhaoLi2014} are used for the control process Z. We rigorously prove that the two new schemes have second-order convergence rate. The proposed splitting methods show clear advantages for equations whose generator consists of a linear part plus a nonlinear part, as they reduce the number of iterations required for solving implicit schemes, thereby decreasing computational cost while maintaining second-order convergence. Two numerical examples are provided, including the backward stochastic Riccati equation arising in mean-variance hedging. The numerical results verify the theoretical error analysis and demonstrate the advantage of reduced computational cost compared to the algorithm in \citep{ZhaoLi2014}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10149v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenbo Wang, Guangyan Jia</dc:creator>
    </item>
    <item>
      <title>Introduction to optimization methods for training SciML models</title>
      <link>https://arxiv.org/abs/2601.10222</link>
      <description>arXiv:2601.10222v1 Announce Type: new 
Abstract: Optimization is central to both modern machine learning (ML) and scientific machine learning (SciML), yet the structure of the underlying optimization problems differs substantially across these domains. Classical ML typically relies on stochastic, sample-separable objectives that favor first-order and adaptive gradient methods. In contrast, SciML often involves physics-informed or operator-constrained formulations in which differential operators induce global coupling, stiffness, and strong anisotropy in the loss landscape. As a result, optimization behavior in SciML is governed by the spectral properties of the underlying physical models rather than by data statistics, frequently limiting the effectiveness of standard stochastic methods and motivating deterministic or curvature-aware approaches. This document provides a unified introduction to optimization methods in ML and SciML, emphasizing how problem structure shapes algorithmic choices. We review first- and second-order optimization techniques in both deterministic and stochastic settings, discuss their adaptation to physics-constrained and data-driven SciML models, and illustrate practical strategies through tutorial examples, while highlighting open research directions at the interface of scientific computing and scientific machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10222v1</guid>
      <category>math.NA</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alena Kopani\v{c}\'akov\'a, Elisa Riccietti</dc:creator>
    </item>
    <item>
      <title>Restoring similarity in randomized Krylov methods with applications to eigenvalue problems and matrix functions</title>
      <link>https://arxiv.org/abs/2601.10248</link>
      <description>arXiv:2601.10248v1 Announce Type: new 
Abstract: The randomized Arnoldi process has been used in large-scale scientific computing because it produces a well-conditioned basis for the Krylov subspace more quickly than the standard Arnoldi process. However, the resulting Hessenberg matrix is generally not similar to the one produced by the standard Arnoldi process, which can lead to delays or spike-like irregularities in convergence. In this paper, we introduce a modification of the randomized Arnoldi process that restores similarity with the Hessenberg matrix generated by the standard Arnoldi process. This is accomplished by enforcing orthogonality between the last Arnoldi vector and the previously generated subspace, which requires solving only one additional least-squares problem. When applied to eigenvalue problems and matrix function evaluations, the modified randomized Arnoldi process produces approximations that are identical to those obtained with the standard Arnoldi process. Numerical experiments demonstrate that our approach is as fast as the randomized Arnoldi process and as robust as the standard Arnoldi process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10248v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Grigori, Daniel Kressner, Nian Shao, Igor Simunec</dc:creator>
    </item>
    <item>
      <title>Conjugate Gradient Methods are Not Efficient: Experimental Study of the Locality Limitation</title>
      <link>https://arxiv.org/abs/2601.10322</link>
      <description>arXiv:2601.10322v1 Announce Type: new 
Abstract: The convergence of the Conjugate Gradient method is subject to a locality limitation which imposes a lower bound on the number of iterations required before a qualitatively accurate approximation can be obtained. This limitation originates from the restricted transport of information in the graph induced by the sparsity pattern of the system matrix. In each iteration, information from the right-hand side can propagate only across directly connected graph nodes. The diameter of this graph therefore determines a minimum number of iterations that is necessary to achieve an acceptable level of accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10322v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ulrich R\"ude</dc:creator>
    </item>
    <item>
      <title>Regularization of linear inverse problems by rational Krylov methods</title>
      <link>https://arxiv.org/abs/2601.10389</link>
      <description>arXiv:2601.10389v1 Announce Type: new 
Abstract: For approximately solving linear ill-posed problems in Hilbert spaces, we investigate the regularization properties of the aggregation method and the RatCG method. These recent algorithms use previously calculated solutions of Tikhonov regularization (respectively, Landweber iterations) to set up a new search space on which the least-squares functional is minimized. We outline how these methods can be understood as rational Krylov space methods, i.e., based on the space of rational functions of the forward operator. The main result is that these methods form an optimal-order regularization schemes when combined with the discrepancy principle as stopping rule and when the underlying regularization parameters are sufficiently large.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10389v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Kindermann</dc:creator>
    </item>
    <item>
      <title>A Geometric Multigrid Preconditioner for Shifted Boundary Method</title>
      <link>https://arxiv.org/abs/2601.10399</link>
      <description>arXiv:2601.10399v1 Announce Type: new 
Abstract: The Shifted Boundary Method (SBM) trades some part of the burden of body-fitted meshing for increased algebraic complexity. While the resulting linear systems retain the standard $\mathcal{O}(h^{-2})$ conditioning of second-order operators, the non-symmetry and non-local boundary coupling render them resistant to standard Algebraic Multigrid (AMG) and simple smoothers for high-order discretisations. We present a geometric multigrid preconditioner that effectively tames these systems. At its core lies the \emph{Full-Residual Shy Patch} smoother: a subspace correction strategy that filters out some patches while capturing the full physics of the shifted boundary. Unlike previous cell-wise approaches that falter at high polynomial degrees, our method delivers convergence with low mesh dependence. We demonstrate performance for Continuous Galerkin approximations, maintaining low and stable iteration counts up to polynomial degree $p=3$ in 3D, proving that SBM can be both geometrically flexible and algebraically efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10399v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Micha{\l} Wichrowski, Ajay Ajith</dc:creator>
    </item>
    <item>
      <title>Optimal error estimates for a discontinuous Galerkin method on curved boundaries with polygonal meshes</title>
      <link>https://arxiv.org/abs/2601.10474</link>
      <description>arXiv:2601.10474v1 Announce Type: new 
Abstract: We consider a discontinuous Galerkin method for the numerical solution of boundary value problems in two-dimensional domains with curved boundaries. A key challenge in this setting is the potential loss of convergence order due to approximating the physical domain by a polygonal mesh. Unless boundary conditions can be accurately transferred from the true boundary to the computational one, such geometric approximation errors generally lead to suboptimal convergence. To overcome this limitation, a higher-order strategy based on polynomial reconstruction of boundary data was introduced for classical finite element methods in [28, 29] and in the finite volume context in [7, 11]. More recently, this approach was extended to discontinuous Galerkin methods in [32], leading to the DG-ROD method, which restores optimal convergence rates on polygonal approximations of domains with curved boundaries. In this work, we provide a rigorous theoretical analysis of the DG-ROD method, establishing existence and uniqueness of the discrete solution and deriving error estimates for a two-dimensional linear advection-diffusion-reaction problem with homogeneous Dirichlet boundary conditions on both convex and non-convex domains. Following and extending techniques from classical finite element methods [29], we prove that, under suitable regularity assumptions on the exact solution, the DG-ROD method achieves optimal convergence despite polygonal approximations. Finally, we illustrate and confirm the theoretical results with a numerical benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10474v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ad\'erito Ara\'ujo, Milene Santos</dc:creator>
    </item>
    <item>
      <title>Chebyshev Accelerated Subspsace Eigensolver for Pseudo-hermitian Hamiltonians</title>
      <link>https://arxiv.org/abs/2601.10557</link>
      <description>arXiv:2601.10557v1 Announce Type: new 
Abstract: Studying the optoelectronic structure of materials can require the computation of up to several thousands of the smallest eigenpairs of a pseudo-hermitian Hamiltonian. Iterative eigensolvers may be preferred over direct methods for this task since their complexity is a function of the desired fraction of the spectrum. In addition, they generally rely on highly optimized and scalable kernels such as matrix-vector multiplications that leverage the massive parallelism and the computational power of modern exascale systems. \textit{Chebyshev Accelerated Subspace iteration Eigensolver} (ChASE) is able to compute several thousands of the most extreme eigenpairs of dense hermitian matrices with proven scalability over massive parallel accelerated clusters. This work presents an extension of ChASE to solve for a portion of the spectrum of pseudo-hermitian Hamiltonians as they appear in the treatment of excitonic materials. The new pseudo-hermitian solver achieves similar convergence and performance as the hermitian one. By exploiting the numerical structure and spectral properties of the Hamiltonian matrix, we propose an oblique variant of Rayleigh-Ritz projection featuring quadratic convergence of the Ritz-values with no explicit construction of the dual basis set. Additionally, we introduce a parallel implementation of the recursive matrix-product operation appearing in the Chebyshev filter with limited amount of global communications. Our development is supported by a full numerical analysis and experimental tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10557v1</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edoardo Di Napoli (J\"ulich Supercomputing Centre, Forschungszentrum J\"ulich, Germany), Cl\'ement Richefort (J\"ulich Supercomputing Centre, Forschungszentrum J\"ulich, Germany), Xinzhe Wu (J\"ulich Supercomputing Centre, Forschungszentrum J\"ulich, Germany)</dc:creator>
    </item>
    <item>
      <title>Stable evaluation of derivatives for barycentric and continued fraction representations of rational functions</title>
      <link>https://arxiv.org/abs/2601.10667</link>
      <description>arXiv:2601.10667v1 Announce Type: new 
Abstract: Fast algorithms for approximation by rational functions exist for both barycentric and Thiele continued fraction (TCF) representations. We present the first numerically stable methods for derivative evaluation in the barycentric representation, including an $O(n)$ algorithm for all derivatives. We also extend an earlier $O(n)$ algorithm for evaluation of the TCF first derivative to higher orders. Numerical experiments confirm the robustness and efficiency of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10667v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobin A. Driscoll, Yuxing Zhou</dc:creator>
    </item>
    <item>
      <title>Shallow-KAN Based Solution of Moving Boundary PDEs</title>
      <link>https://arxiv.org/abs/2601.09818</link>
      <description>arXiv:2601.09818v1 Announce Type: cross 
Abstract: Kolmogorov-Arnold Networks (KANs) require significantly smaller architectures compared to multilayer perceptron (MLP)-based approaches, while retaining expressive power through spline-based activations. We propose a shallow KAN framework that directly approximates the temperature distribution T(x,t) and the moving interface $\Gamma(t)$, enforcing the governing PDEs, phase equilibrium, and Stefan condition through physics-informed residuals. To enhance accuracy, we employ interface-focused collocation resampling. Numerical experiments in one and two dimensions show that the framework achieves accurate reconstructions of both temperature fields and interface dynamics, highlighting the potential of KANs as a compact and efficient alternative for moving boundary PDEs. First, we validate the model with semi-infinite analytical solutions. Subsequently, the model is extended to 2D using a level-set based formulation for interface propagation, which is solved within the KAN framework. This work demonstrates that KANs are capable of solving complex moving boundary problems without the need for measurement data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09818v1</guid>
      <category>math-ph</category>
      <category>cs.NA</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tarus Pande, V M S K Minnikanti, Shyamprasad Karagadde</dc:creator>
    </item>
    <item>
      <title>A Level Set Method on Particle Flow Maps</title>
      <link>https://arxiv.org/abs/2601.09939</link>
      <description>arXiv:2601.09939v1 Announce Type: cross 
Abstract: This paper introduces a Particle Flow Map Level Set (PFM-LS) method for high-fidelity interface tracking. We store level-set values, gradients, and Hessians on particles concentrated in a narrow band around the interface, advecting them via bidirectional flow maps while using a conventional grid-based representation elsewhere. By interpreting the level set value as a 3-form and its gradient as a 1-form, PFM-LS achieves exceptional geometric fidelity during complex deformations and preserves sub-grid features that traditional methods cannot capture. Our dual-timescale approach utilizes long-range maps for values and gradients, with frequent reinitialization of short-range maps for the distortion-sensitive Hessian, alongside adaptive particle control that maintains sufficient density within the narrow band. We also develop a hybrid particle-grid quasi-Newton redistancing scheme that preserves fine-scale features while enforcing the signed-distance property. Benchmark comparisons in 2D and 3D demonstrate that PFM-LS achieves state-of-the-art volume preservation and shape fidelity against a broad range of existing level-set methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09939v1</guid>
      <category>physics.comp-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinjin He, Taiyuan Zhang, Zhiqi Li, Junwei Zhou, Duowen Chen, Bo Zhu</dc:creator>
    </item>
    <item>
      <title>In-Context Operator Learning on the Space of Probability Measures</title>
      <link>https://arxiv.org/abs/2601.09979</link>
      <description>arXiv:2601.09979v1 Announce Type: cross 
Abstract: We introduce \emph{in-context operator learning on probability measure spaces} for optimal transport (OT). The goal is to learn a single solution operator that maps a pair of distributions to the OT map, using only few-shot samples from each distribution as a prompt and \emph{without} gradient updates at inference. We parameterize the solution operator and develop scaling-law theory in two regimes. In the \emph{nonparametric} setting, when tasks concentrate on a low-intrinsic-dimension manifold of source--target pairs, we establish generalization bounds that quantify how in-context accuracy scales with prompt size, intrinsic task dimension, and model capacity. In the \emph{parametric} setting (e.g., Gaussian families), we give an explicit architecture that recovers the exact OT map in context and provide finite-sample excess-risk bounds. Our numerical experiments on synthetic transports and generative-modeling benchmarks validate the framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09979v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frank Cole, Dixi Wang, Yineng Chen, Yulong Lu, Rongjie Lai</dc:creator>
    </item>
    <item>
      <title>A volume penalization method for solving conjugate scalar transport with interfacial jump conditions</title>
      <link>https://arxiv.org/abs/2601.10134</link>
      <description>arXiv:2601.10134v1 Announce Type: cross 
Abstract: Conjugate scalar transport with interfacial jump conditions on complex interfacial geometries is common in thermal and chemical processes, while its accurate and efficient simulations are still quite challenging. In the present study, a novel treatment of a two-phase interface in the volume penalization method, a kind of immersed boundary method, for solving conjugate scalar transport with general interfacial boundary conditions is developed. We first propose an interfacial treatment for solving an advection-diffusion equation with a Neumann boundary condition, and then extend it to general conjugate scalar transport with both interfacial flux and scalar jumps. A one-dimensional diffusion problem is solved to verify the present scheme and demonstrate the advantage of the present scheme in improving accuracy and unifying the governing equations in the two phases with an additional source term representing the local jump condition of the interfacial scalar flux. Then, the present scheme is further applied to fluid-solid coupled scalar diffusion and advection-diffusion problems with the scalar and its flux jumps across the interface. The simulation results of the present scheme generally show good agreement with reference results obtained by body-fitted mesh simulations with average relative deviations less than 3.0%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10134v1</guid>
      <category>physics.comp-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ming Liu, Yosuke Hasegawa</dc:creator>
    </item>
    <item>
      <title>Discrete versus continuous -- lattice models and their exact continuous counterparts</title>
      <link>https://arxiv.org/abs/2601.10184</link>
      <description>arXiv:2601.10184v1 Announce Type: cross 
Abstract: We review and study the correspondence between discrete lattice/chain models of interacting particles and their continuous counterparts represented by partial differential equations. We study the correspondence problem for nearest neighbour interaction lattice models as well as for multiple-neighbour interaction lattice models, and we gradually proceed from infinite lattices to periodic lattices and finally to finite lattices with fixed ends/zero Dirichlet boundary conditions. The whole study is framed as systematic specialisation of Fourier analysis tools from the continuous to the discrete setting and vice versa, and the correspondence between the discrete and continuous models is examined primarily with regard to the dispersion relation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10184v1</guid>
      <category>physics.class-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Fusi, Oliver K\v{r}enek, V\'it Pr\r{u}\v{s}a, Casey Rodriguez, Rebecca Tozzi, Martin Vejvoda</dc:creator>
    </item>
    <item>
      <title>On gradient stability in nonlinear PDE models and inference in interacting particle systems</title>
      <link>https://arxiv.org/abs/2601.10326</link>
      <description>arXiv:2601.10326v1 Announce Type: cross 
Abstract: We consider general parameter to solution maps $\theta \mapsto \mathcal G(\theta)$ of non-linear partial differential equations and describe an approach based on a Banach space version of the implicit function theorem to verify the gradient stability condition of Nickl&amp;Wang (JEMS 2024) for the underlying non-linear inverse problem, providing also injectivity estimates and corresponding statistical identifiability results. We illustrate our methods in two examples involving a non-linear reaction diffusion system as well as a McKean--Vlasov interacting particle model, both with periodic boundary conditions. We apply our results to prove the polynomial time convergence of a Langevin-type algorithm sampling the posterior measure of the interaction potential arising from a discrete aggregate measurement of the interacting particle system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10326v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aur\'elien Castre, Richard Nickl</dc:creator>
    </item>
    <item>
      <title>Jordan-Segmentable Masks: A Topology-Aware definition for characterizing Binary Image Segmentation</title>
      <link>https://arxiv.org/abs/2601.10577</link>
      <description>arXiv:2601.10577v1 Announce Type: cross 
Abstract: Image segmentation plays a central role in computer vision. However, widely used evaluation metrics, whether pixel-wise, region-based, or boundary-focused, often struggle to capture the structural and topological coherence of a segmentation. In many practical scenarios, such as medical imaging or object delineation, small inaccuracies in boundary, holes, or fragmented predictions can result in high metric scores, despite the fact that the resulting masks fail to preserve the object global shape or connectivity. This highlights a limitation of conventional metrics: they are unable to assess whether a predicted segmentation partitions the image into meaningful interior and exterior regions.
  In this work, we introduce a topology-aware notion of segmentation based on the Jordan Curve Theorem, and adapted for use in digital planes. We define the concept of a \emph{Jordan-segmentatable mask}, which is a binary segmentation whose structure ensures a topological separation of the image domain into two connected components. We analyze segmentation masks through the lens of digital topology and homology theory, extracting a $4$-curve candidate from the mask, verifying its topological validity using Betti numbers. A mask is considered Jordan-segmentatable when this candidate forms a digital 4-curve with $\beta_0 = \beta_1 = 1$, or equivalently when its complement splits into exactly two $8$-connected components.
  This framework provides a mathematically rigorous, unsupervised criterion with which to assess the structural coherence of segmentation masks. By combining digital Jordan theory and homological invariants, our approach provides a valuable alternative to standard evaluation metrics, especially in applications where topological correctness must be preserved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.10577v1</guid>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.AT</category>
      <category>math.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Serena Grazia De Benedictis, Amedeo Altavilla, Nicoletta Del Buono</dc:creator>
    </item>
    <item>
      <title>Uniform Approximation of Eigenproblems of a Large-Scale Parameter-Dependent Hermitian Matrix</title>
      <link>https://arxiv.org/abs/2409.05791</link>
      <description>arXiv:2409.05791v4 Announce Type: replace 
Abstract: We consider the uniform approximation of the smallest eigenvalue of a large parameter-dependent Hermitian matrix by that of a smaller counterpart obtained through projections. The projection subspaces are constructed iteratively by means of a greedy strategy; at each iteration the parameter where a surrogate error is maximal is computed and the eigenvectors associated with the smallest eigenvalues at the maximizing parameter value are added to the subspace. Unlike the classical approaches, such as the successive constraint method, that maximize such surrogate errors over a discrete and finite set, we maximize the surrogate error over the continuum of all permissible parameter values globally. We formally prove that the projected eigenvalue function converges to the actual eigenvalue function uniformly. In the second part, we focus on the uniform approximation of the smallest singular value of a large parameter-dependent matrix, in case it is non-Hermitian. The proposed frameworks on numerical examples, including those arising from discretizations of parametric PDEs, reduce the size of the large matrix-valued function drastically, while retaining a high accuracy over all permissible parameter values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05791v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mattia Manucci, Emre Mengi, Nicola Guglielmi</dc:creator>
    </item>
    <item>
      <title>Finite Element Approximations of Stochastic Linear Schr\"{o}dinger equation driven by additive Wiener noise</title>
      <link>https://arxiv.org/abs/2410.06006</link>
      <description>arXiv:2410.06006v2 Announce Type: replace 
Abstract: In this article, we have analyzed semi-discrete finite element approximations of the Stochastic linear Schr\"{o}dinger equation in a bounded convex polygonal domain driven by additive Wiener noise. We use the finite element method for spatial discretization and derive an error estimate with respect to the discretization parameter of the finite element approximation. Numerical experiments have also been performed to support theoretical bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06006v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.AP</category>
      <category>math.MP</category>
      <category>math.PR</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suprio Bhar, Mrinmay Biswas, Mangala Prasad</dc:creator>
    </item>
    <item>
      <title>Keep the beat going: Automatic drum transcription with momentum</title>
      <link>https://arxiv.org/abs/2507.12596</link>
      <description>arXiv:2507.12596v2 Announce Type: replace 
Abstract: How can we process a piece of recorded music to detect and visualize the onset of each instrument? A simple, interpretable approach is based on partially fixed nonnegative matrix factorization (NMF). Yet despite the method's simplicity, partially fixed NMF is challenging to apply because the associated optimization problem is high-dimensional and non-convex. This paper explores two optimization approaches that preserve the nonnegative structure, including a multiplicative update rule and projected gradient descent with momentum. These techniques are derived from the previous literature, but they have not been fully developed for partially fixed NMF before now. Results indicate that projected gradient descent with momentum leads to the higher accuracy among the two methods, and it satisfies stronger local convergence guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12596v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alisha L. Foster, Robert J. Webber</dc:creator>
    </item>
    <item>
      <title>1/2 order convergence rate of Euler-type methods for time-changed stochastic differential equations with super-linearly growing drift and diffusion coefficients</title>
      <link>https://arxiv.org/abs/2507.14562</link>
      <description>arXiv:2507.14562v4 Announce Type: replace 
Abstract: This paper investigates the strong convergence properties of two Euler-type methods for a class of time-changed stochastic differential equations (TCSDEs) with super-linearly growing drift and diffusion coefficients. Building upon existing research, we propose a backward Euler method (BEM) and introduce its explicit counterpart -- the projected Euler method (PEM). We prove that both methods converge strongly in the $L_2$-sense at the optimal rate of 1/2. This result extends the applicability of both the BEM and the PEM to a broader class of TCSDEs. Moreover, the two methods offer complementary strengths: while BEM possesses wide applicability, PEM is computationally more efficient. Numerical simulations confirm our theoretical findings and illustrate practical performance of both schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14562v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuai Wang, Yuanling Niu, Ying Zhang</dc:creator>
    </item>
    <item>
      <title>Some new properties of the PamPa scheme</title>
      <link>https://arxiv.org/abs/2508.17147</link>
      <description>arXiv:2508.17147v2 Announce Type: replace 
Abstract: In this paper, we provide a few new properties of Active Flux (AF)/Point-Average-Moment PolynomiAl-interpreted (\pampa) schemes. First, we show, in full generality, that the AF/pampa schemes can be interpreted in such a way that the discontinuous Galerkin (dG) scheme is one of their building blocks. Secondly we provide intrinsic bound preserving properties of the current variant of pampa. This is also illustrated numerically. Last, we show, at least in one dimension, that the pampa scheme has the summation by part (SBP) property.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17147v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>R\'emi Abgrall, Philipp \"Offner, Yongle Liu</dc:creator>
    </item>
    <item>
      <title>Data selection: at the interface of PDE-based inverse problem and randomized linear algebra</title>
      <link>https://arxiv.org/abs/2510.01567</link>
      <description>arXiv:2510.01567v2 Announce Type: replace 
Abstract: All inverse problems rely on data to recover unknown parameters, yet not all data are equally informative. This raises the central question of data selection. A distinctive challenge in PDE-based inverse problems is their inherently infinite-dimensional nature: both the parameter space and the design space are infinite, which greatly complicates the selection process. Somewhat unexpectedly, randomized numerical linear algebra (RNLA), originally developed in very different contexts, has provided powerful tools for addressing this challenge. These methods are inherently probabilistic, with guarantees typically stating that information is preserved with probability at least 1-p when using N randomly selected, weighted samples. Here, the notion of "information" can take different mathematical forms depending on the setting. In this review, we survey the problem of data selection in PDE-based inverse problems, emphasize its unique infinite-dimensional aspects, and highlight how RNLA strategies have been adapted and applied in this context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01567v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kathrin Hellmuth, Ruhui Jin, Qin Li, Stephen J. Wright</dc:creator>
    </item>
    <item>
      <title>L^1 data fitting for Inverse Problems yields optimal rates of convergence in case of discretized white Gaussian noise</title>
      <link>https://arxiv.org/abs/2511.11321</link>
      <description>arXiv:2511.11321v2 Announce Type: replace 
Abstract: It is well-known in practice, that L^1 data fitting leads to improved robustness compared to standard L^2 data fitting. However, it is unclear whether resulting algorithms will perform as well in case of regular data without outliers. In this paper, we therefore analyze generalized Tikhonov regularization with L^1 data fidelity for Inverse Problems F(u) = g in a general setting, including general measurement errors and errors in the forward operator. The derived results are then applied to the situation of discretized Gaussian white noise, and we show that the resulting error bounds allow for order-optimal rates of convergence. These findings are also investigated in numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11321v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kristina B\"atz, Frank Werner</dc:creator>
    </item>
    <item>
      <title>Milstein-type Schemes for Hyperbolic SPDEs</title>
      <link>https://arxiv.org/abs/2512.19647</link>
      <description>arXiv:2512.19647v2 Announce Type: replace 
Abstract: This article studies the temporal approximation of hyperbolic semilinear stochastic evolution equations with multiplicative Gaussian noise by Milstein-type schemes. We take the term hyperbolic to mean that the leading operator generates a contractive, not necessarily analytic $C_0$-semigroup. Optimal convergence rates are derived for the pathwise uniform strong error \[
  E_h^\infty := \Big(\mathbb{E}\Big[\max_{1\le j \le M}\|U_{t_j}-u_j\|_X^p\Big]\Big)^{1/p} \] on a Hilbert space $X$ for $p\in [2,\infty)$. Here, $U$ is the mild solution and $u_j$ its Milstein approximation at time $t_j=jh$ with step size $h&gt;0$ and final time $T=Mh&gt;0$. For sufficiently regular nonlinearity and noise, we establish strong convergence of order one, with the error satisfying $E_h^\infty\lesssim h\sqrt{\log(T/h)}$ for rational Milstein schemes and $E_h^\infty \lesssim h$ for exponential Milstein schemes. This extends previous results from parabolic to hyperbolic SPDEs and from exponential to rational Milstein schemes. Moreover, root-mean-square error estimates are strengthened to pathwise uniform estimates. Numerical experiments validate the convergence rates for the stochastic Schr\"odinger equation. Further applications to Maxwell's and transport equations are included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19647v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.FA</category>
      <category>math.PR</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Kastner, Katharina Klioba</dc:creator>
    </item>
    <item>
      <title>Physics-Constrained Learning of Energy-Preserving Stencils for Maxwell's Equations</title>
      <link>https://arxiv.org/abs/2601.01902</link>
      <description>arXiv:2601.01902v3 Announce Type: replace 
Abstract: We study data-driven construction of spatial discretizations for the one-dimensional Maxwell system. Using high-fidelity training data from a spectral discretization, we learn a \emph{linear convolution stencil} that approximates the spatial derivative operator in Maxwell's equations. We formulate a convex quadratic program for the stencil coefficients with linear constraints that enforce skew-adjointness of the discrete derivative; these constraints guarantee a semi-discrete electromagnetic energy identity and yield a CFL condition expressed directly in terms of the stencil's Fourier symbol. We compare several convex solvers for the resulting quadratic program -- projected gradient, Nesterov-accelerated gradient, ADMM, and an interior-point reference implemented in CVXPY -- and evaluate the learned operators in time-dependent Maxwell simulations using a Crank--Nicolson (CN) discretization. Numerical experiments, including cases with nonstandard target operators and noisy training data, show that (i) energy-constrained learned stencils achieve accuracy comparable to standard central differences while exactly preserving the discrete electromagnetic energy under CN time-stepping, and (ii) ADMM and interior-point solvers produce nearly identical operators, with ADMM offering a favorable tradeoff between accuracy, constraint satisfaction, and runtime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01902v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victory Obieke</dc:creator>
    </item>
    <item>
      <title>Constrained dynamics for searching saddle points on general Riemannian manifolds</title>
      <link>https://arxiv.org/abs/2601.03931</link>
      <description>arXiv:2601.03931v2 Announce Type: replace 
Abstract: Finding constrained saddle points on Riemannian manifolds is significant for analyzing energy landscapes arising in physics and chemistry. Existing works have been limited to special manifolds that admit global regular level-set representations, excluding applications such as electronic excited-state calculations. In this paper, we develop a constrained saddle dynamics applicable to smooth functions on general Riemannian manifolds. Our dynamics is formulated compactly over the Grassmann bundle of the tangent bundle. By analyzing the Grassmann bundle geometry, we achieve universality via incorporating the second fundamental form, which captures variations of tangent spaces along the trajectory. We rigorously establish the local linear stability of the dynamics and the local linear convergence of the resulting algorithms. Remarkably, our analysis provides the first convergence guarantees for discretized saddle-search algorithms in manifold settings. Moreover, by respecting the intrinsic quotient structure, we remove unnecessary nondegeneracy assumptions on the eigenvalues of the Riemannian Hessian that are present in existing works. We also point out that locating saddle points can be more ill-conditioning than finding local minimizers, and requires using nonredundant parametrizations. Finally, numerical experiments on linear eigenvalue problems and electronic excited-state calculations showcase the effectiveness of the proposed algorithms and corroborate the established local theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.03931v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>physics.chem-ph</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yukuan Hu, Laura Grazioli</dc:creator>
    </item>
    <item>
      <title>VICON: Vision In-Context Operator Networks for Multi-Physics Fluid Dynamics Prediction</title>
      <link>https://arxiv.org/abs/2411.16063</link>
      <description>arXiv:2411.16063v5 Announce Type: replace-cross 
Abstract: In-Context Operator Networks (ICONs) have demonstrated the ability to learn operators across diverse partial differential equations using few-shot, in-context learning. However, existing ICONs process each spatial point as an individual token, severely limiting computational efficiency when handling dense data in higher spatial dimensions. We propose Vision In-Context Operator Networks (VICON), which integrates vision transformer architectures to efficiently process 2D data through patch-wise operations while preserving ICON's adaptability to multiphysics systems and varying timesteps. Evaluated across three fluid dynamics benchmarks, VICON significantly outperforms state-of-the-art baselines: DPOT and MPP, reducing the averaged last-step rollout error by 37.9% compared to DPOT and 44.7% compared to MPP, while requiring only 72.5% and 34.8% of their respective inference times. VICON naturally supports flexible rollout strategies with varying timestep strides, enabling immediate deployment in imperfect measurement systems where sampling frequencies may differ or frames might be dropped - common challenges in real-world settings - without requiring retraining or interpolation. In these realistic scenarios, VICON exhibits remarkable robustness, experiencing only 24.41% relative performance degradation compared to 71.37%-74.49% degradation in baseline methods, demonstrating its versatility for deploying in realistic applications. Our scripts for processing datasets and code are publicly available at https://github.com/Eydcao/VICON.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16063v5</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.flu-dyn</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yadi Cao, Yuxuan Liu, Liu Yang, Rose Yu, Hayden Schaeffer, Stanley Osher</dc:creator>
    </item>
    <item>
      <title>Exploring specialization and sensitivity of convolutional neural networks in the context of simultaneous image augmentations</title>
      <link>https://arxiv.org/abs/2503.03283</link>
      <description>arXiv:2503.03283v2 Announce Type: replace-cross 
Abstract: Drawing parallels with the way biological networks are studied, we adapt the treatment--control paradigm to explainable artificial intelligence research and enrich it through multi-parametric input alterations. In this study, we propose a framework for investigating the internal inference impacted by input data augmentations. The internal changes in network operation are reflected in activation changes measured by variance, which can be decomposed into components related to each augmentation, employing Sobol indices and Shapley values. These quantities enable one to visualize sensitivity to different variables and use them for guided masking of activations. In addition, we introduce a way of single-class sensitivity analysis where the candidates are filtered according to their matching to prediction bias generated by targeted damaging of the activations. Relying on the observed parallels, we assume that the developed framework can potentially be transferred to studying biological neural networks in complex environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03283v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pavel Kharyuk, Sergey Matveev, Ivan Oseledets</dc:creator>
    </item>
    <item>
      <title>COALA: Numerically Stable and Efficient Framework for Context-Aware Low-Rank Approximation</title>
      <link>https://arxiv.org/abs/2507.07580</link>
      <description>arXiv:2507.07580v2 Announce Type: replace-cross 
Abstract: Recent studies suggest that context-aware low-rank approximation is a useful tool for compression and fine-tuning of modern large-scale neural networks. In this type of approximation, a norm is weighted by a matrix of input activations, significantly improving metrics over the unweighted case. Nevertheless, existing methods for neural networks suffer from numerical instabilities due to their reliance on classical formulas involving explicit Gram matrix computation and their subsequent inversion. We demonstrate that this can degrade the approximation quality or cause numerically singular matrices.
  To address these limitations, we propose a novel inversion-free regularized framework that is based entirely on stable decompositions and overcomes the numerical pitfalls of prior art. Our method can handle possible challenging scenarios: (1) when calibration matrices exceed GPU memory capacity, (2) when input activation matrices are nearly singular, and even (3) when insufficient data prevents unique approximation. For the latter, we prove that our solution converges to a desired approximation and derive explicit error bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07580v2</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Uliana Parkina, Maxim Rakhuba</dc:creator>
    </item>
    <item>
      <title>Learning Regularization Functionals for Inverse Problems: A Comparative Study</title>
      <link>https://arxiv.org/abs/2510.01755</link>
      <description>arXiv:2510.01755v2 Announce Type: replace-cross 
Abstract: In recent years, a variety of learned regularization frameworks for solving inverse problems in imaging have emerged. These offer flexible modeling together with mathematical insights. The proposed methods differ in their architectural design and training strategies, making direct comparison challenging due to non-modular implementations. We address this gap by collecting and unifying the available code into a common framework. This unified view allows us to systematically compare the approaches and highlight their strengths and limitations, providing valuable insights into their future potential. We also provide concise descriptions of each method, complemented by practical guidelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01755v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Hertrich, Hok Shing Wong, Alexander Denker, Stanislas Ducotterd, Zhenghan Fang, Markus Haltmeier, \v{Z}eljko Kereta, Erich Kobler, Oscar Leong, Mohammad Sadegh Salehi, Carola-Bibiane Sch\"onlieb, Johannes Schwab, Zakhar Shumaylov, Jeremias Sulam, German Sh\^ama Wache, Martin Zach, Yasi Zhang, Matthias J. Ehrhardt, Sebastian Neumayer</dc:creator>
    </item>
    <item>
      <title>Discrete Solution Operator Learning for Geometry-Dependent PDEs</title>
      <link>https://arxiv.org/abs/2601.09143</link>
      <description>arXiv:2601.09143v2 Announce Type: replace-cross 
Abstract: Neural operator learning accelerates PDE solution by approximating operators as mappings between continuous function spaces. Yet in many engineering settings, varying geometry induces discrete structural changes, including topological changes, abrupt changes in boundary conditions or boundary types, and changes in the computational domain, which break the smooth-variation premise. Here we introduce Discrete Solution Operator Learning (DiSOL), a complementary paradigm that learns discrete solution procedures rather than continuous function-space operators. DiSOL factorizes the solver into learnable stages that mirror classical discretizations: local contribution encoding, multiscale assembly, and implicit solution reconstruction on an embedded grid, thereby preserving procedure-level consistency while adapting to geometry-dependent discrete structures. Across geometry-dependent Poisson, advection-diffusion, linear elasticity, as well as spatiotemporal heat conduction problems, DiSOL produces stable and accurate predictions under both in-distribution and strongly out-of-distribution geometries, including discontinuous boundaries and topological changes. These results highlight the need for procedural operator representations in geometry-dominated problems and position discrete solution operator learning as a distinct, complementary direction in scientific machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09143v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Fri, 16 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinshuai Bai, Haolin Li, Zahra Sharif Khodaei, M. H. Aliabadi, YuanTong Gu, Xi-Qiao Feng</dc:creator>
    </item>
  </channel>
</rss>
