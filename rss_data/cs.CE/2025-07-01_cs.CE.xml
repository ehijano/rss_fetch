<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Jul 2025 04:04:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Ensemble Kalman Filter for Data Assimilation coupled with low-resolution computations techniques applied in Fluid Dynamics</title>
      <link>https://arxiv.org/abs/2507.00539</link>
      <description>arXiv:2507.00539v1 Announce Type: new 
Abstract: This paper presents an innovative Reduced-Order Model (ROM) for merging experimental and simulation data using Data Assimilation (DA) to estimate the "True" state of a fluid dynamics system, leading to more accurate predictions. Our methodology introduces a novel approach implementing the Ensemble Kalman Filter (EnKF) within a reduced-dimensional framework, grounded in a robust theoretical foundation and applied to fluid dynamics. To address the substantial computational demands of DA, the proposed ROM employs low-resolution (LR) techniques to drastically reduce computational costs. This approach involves downsampling datasets for DA computations, followed by an advanced reconstruction technique based on low-cost Singular Value Decomposition (lcSVD). The lcSVD method, a key innovation in this paper, has never been applied to DA before and offers a highly efficient way to enhance resolution with minimal computational resources. Our results demonstrate significant reductions in both computation time and RAM usage through the LR techniques without compromising the accuracy of the estimations. For instance, in a turbulent test case, the LR approach with a compression rate of 15.9 can achieve a speed-up of 13.7 and a RAM compression of 90.9% while maintaining a low Relative Root Mean Square Error (RRMSE) of 2.6%, compared to 0.8% in the high-resolution (HR) reference. Furthermore, we highlight the effectiveness of the EnKF in estimating and predicting the state of fluid flow systems based on limited observations and low-fidelity numerical data. This paper highlights the potential of the proposed DA method in fluid dynamics applications, particularly for improving computational efficiency in CFD and related fields. Its ability to balance accuracy with low computational and memory costs makes it suitable for large-scale and real-time applications, such as environmental monitoring or aerospace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00539v1</guid>
      <category>cs.CE</category>
      <category>physics.flu-dyn</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Paul Jeanney, Ashton Hetherington, Shady E. Ahmed, David Lanceta, Susana Saiz, Jos\'e Miguel Perez, Soledad Le CLainche</dc:creator>
    </item>
    <item>
      <title>Data Collection with Non-Uniform Axial Power for Phase II of the OECD/NEA AI/ML Critical Heat Flux Benchmark</title>
      <link>https://arxiv.org/abs/2507.00034</link>
      <description>arXiv:2507.00034v1 Announce Type: cross 
Abstract: Critical heat flux (CHF) marks the onset of boiling crisis in light-water reactors, defining safe thermal-hydraulic operating limits. To support Phase II of the OECD/NEA AI/ML CHF benchmark, which introduces spatially varying power profiles, this work compiles and digitizes a broad CHF dataset covering both uniform and non-uniform axial heating conditions. Heating profiles were extracted from technical reports, interpolated onto a consistent axial mesh, validated via energy-balance checks, and encoded in machine-readable formats for benchmark compatibility.
  Classical CHF correlations exhibit substantial errors under uniform heating and degrade markedly when applied to non-uniform profiles, while modern tabular methods offer improved but still imperfect predictions. A neural network trained solely on uniform data performs well in that regime but fails to generalize to spatially varying scenarios, underscoring the need for models that explicitly incorporate axial power distributions. By providing these curated datasets and baseline modeling results, this study lays the groundwork for advanced transfer-learning strategies, rigorous uncertainty quantification, and design-optimization efforts in the next phase of the CHF benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00034v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Reece Bourisaw, Reid McCants, Jean-Marie Le Corre, Anna Iskhakova, Arsen S. Iskhakov</dc:creator>
    </item>
    <item>
      <title>Evolutionary computing-based image segmentation method to detect defects and features in Additive Friction Stir Deposition Process</title>
      <link>https://arxiv.org/abs/2507.00046</link>
      <description>arXiv:2507.00046v1 Announce Type: cross 
Abstract: This work proposes an evolutionary computing-based image segmentation approach for analyzing soundness in Additive Friction Stir Deposition (AFSD) processes. Particle Swarm Optimization (PSO) was employed to determine optimal segmentation thresholds for detecting defects and features in multilayer AFSD builds. The methodology integrates gradient magnitude analysis with distance transforms to create novel attention-weighted visualizations that highlight critical interface regions. Five AFSD samples processed under different conditions were analyzed using multiple visualization techniques i.e. self-attention maps, and multi-channel visualization. These complementary approaches reveal subtle material transition zones and potential defect regions which were not readily observable through conventional imaging. The PSO algorithm automatically identified optimal threshold values (ranging from 156-173) for each sample, enabling precise segmentation of material interfaces. The multi-channel visualization technique effectively combines boundary information (red channel), spatial relationships (green channel), and material density data (blue channel) into cohesive representations that quantify interface quality. The results demonstrate that attention-based analysis successfully identifies regions of incomplete bonding and inhomogeneities in AFSD joints, providing quantitative metrics for process optimization and quality assessment of additively manufactured components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00046v1</guid>
      <category>cs.CV</category>
      <category>cs.CE</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Akshansh Mishra, Eyob Mesele Sefene, Shivraman Thapliyal</dc:creator>
    </item>
    <item>
      <title>A collaborative digital twin built on FAIR data and compute infrastructure</title>
      <link>https://arxiv.org/abs/2507.00048</link>
      <description>arXiv:2507.00048v1 Announce Type: cross 
Abstract: The integration of machine learning with automated experimentation in self-driving laboratories (SDL) offers a powerful approach to accelerate discovery and optimization tasks in science and engineering applications. When supported by findable, accessible, interoperable, and reusable (FAIR) data infrastructure, SDLs with overlapping interests can collaborate more effectively. This work presents a distributed SDL implementation built on nanoHUB services for online simulation and FAIR data management. In this framework, geographically dispersed collaborators conducting independent optimization tasks contribute raw experimental data to a shared central database. These researchers can then benefit from analysis tools and machine learning models that automatically update as additional data become available. New data points are submitted through a simple web interface and automatically processed using a nanoHUB Sim2L, which extracts derived quantities and indexes all inputs and outputs in a FAIR data repository called ResultsDB. A separate nanoHUB workflow enables sequential optimization using active learning, where researchers define the optimization objective, and machine learning models are trained on-the-fly with all existing data, guiding the selection of future experiments. Inspired by the concept of ``frugal twin", the optimization task seeks to find the optimal recipe to combine food dyes to achieve the desired target color. With easily accessible and inexpensive materials, researchers and students can set up their own experiments, share data with collaborators, and explore the combination of FAIR data, predictive ML models, and sequential optimization. The tools introduced are generally applicable and can easily be extended to other optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00048v1</guid>
      <category>cs.AI</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas M. Deucher, Juan C. Verduzco, Michael Titus, Alejandro Strachan</dc:creator>
    </item>
    <item>
      <title>The gradual transformation of inland countries -- human plowing, horse plowing and equity incentives</title>
      <link>https://arxiv.org/abs/2507.00067</link>
      <description>arXiv:2507.00067v1 Announce Type: cross 
Abstract: Many modern countries have not learned their lessons and often hope for the wisdom of later generations, resulting in them only possessing modern technology and difficult to iterate ancient civilizations. At present, there is no way to tell how we should learn from history and promote the gradual upgrading of civilization. Therefore, we must tell the history of civilization's progress and the means of governance, learn from experience to improve the comprehensive strength and survival ability of civilization, and achieve an optimal solution for the tempering brought by conflicts and the reduction of internal conflicts. Firstly, we must follow the footsteps of history and explore the reasons for the long-term stability of each country in conflict, including providing economic benefits to the people and means of suppressing them; then, use mathematical methods to demonstrate how we can achieve the optimal solution at the current stage. After analysis, we can conclude that the civilization transformed from human plowing to horse plowing can easily suppress the resistance of the people and provide them with the ability to resist; The selection of rulers should consider multiple institutional aspects, such as exams, elections, and drawing lots; Economic development follows a lognormal distribution and can be adjusted by expected value and variance. Using a lognormal distribution with the maximum value to divide equity can adjust the wealth gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00067v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CE</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongfa Zi, Zhen Liu</dc:creator>
    </item>
    <item>
      <title>A Deep Learning-Aided Approach for Estimating Field Permeability Map by Fusing Well Logs, Well Tests, and Seismic Data</title>
      <link>https://arxiv.org/abs/2505.02093</link>
      <description>arXiv:2505.02093v3 Announce Type: replace 
Abstract: Obtaining reliable permeability maps of oil reservoirs is crucial for building a robust and accurate reservoir simulation model and, therefore, designing effective recovery strategies. This problem, however, remains challenging, as it requires the integration of various data sources by experts from different disciplines. Moreover, there are no sources to provide direct information about the inter-well space. In this work, a new method based on the data-fusion approach is proposed for predicting two-dimensional permeability maps on the whole reservoir area. This method utilizes non-parametric regression with a custom kernel shape accounting for different data sources: well logs, well tests, and seismics. A convolutional neural network is developed to process seismic data and then incorporate it with other sources. A multi-stage data fusion procedure helps to artificially increase the training dataset for the seismic interpretation model and finally to construct the adequate permeability map. The proposed methodology of permeability map construction from different sources was tested on a real oil reservoir located in Western Siberia. The results demonstrate that the developed map perfectly corresponds to the permeability estimations in the wells, and the inter-well space permeability predictions are considerably improved through the incorporation of the seismic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02093v3</guid>
      <category>cs.CE</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Grigoriy Shutov, Viktor Duplyakov, Shadfar Davoodi, Anton Morozov, Dmitriy Popkov, Kirill Pavlenko, Albert Vainshtein, Viktor Kotezhekov, Sergey Kaygorodov, Boris Belozerov, Mars M Khasanov, Vladimir Vanovskiy, Andrei Osiptsov, Evgeny Burnaev</dc:creator>
    </item>
    <item>
      <title>SPGD: Steepest Perturbed Gradient Descent Optimization</title>
      <link>https://arxiv.org/abs/2411.04946</link>
      <description>arXiv:2411.04946v2 Announce Type: replace-cross 
Abstract: Optimization algorithms are pivotal in advancing various scientific and industrial fields but often encounter obstacles such as trapping in local minima, saddle points, and plateaus (flat regions), which makes the convergence to reasonable or near-optimal solutions particularly challenging. This paper presents the Steepest Perturbed Gradient Descent (SPGD), a novel algorithm that innovatively combines the principles of the gradient descent method with periodic uniform perturbation sampling to effectively circumvent these impediments and lead to better solutions whenever possible. SPGD is distinctively designed to generate a set of candidate solutions and select the one exhibiting the steepest loss difference relative to the current solution. It enhances the traditional gradient descent approach by integrating a strategic exploration mechanism that significantly increases the likelihood of escaping sub-optimal local minima and navigating complex optimization landscapes effectively. Our approach not only retains the directed efficiency of gradient descent but also leverages the exploratory benefits of stochastic perturbations, thus enabling a more comprehensive search for global optima across diverse problem spaces. We demonstrate the efficacy of SPGD in solving the 3D component packing problem, an NP-hard challenge. Preliminary results show a substantial improvement over four established methods, particularly on response surfaces with complex topographies and in multidimensional non-convex continuous optimization problems. Comparative analyses with established 2D benchmark functions highlight SPGD's superior performance, showcasing its ability to navigate complex optimization landscapes. These results emphasize SPGD's potential as a versatile tool for a wide range of optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04946v2</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amir M. Vahedi, Horea T. Ilies</dc:creator>
    </item>
    <item>
      <title>STONet: A neural operator for modeling solute transport in micro-cracked reservoirs</title>
      <link>https://arxiv.org/abs/2412.05576</link>
      <description>arXiv:2412.05576v2 Announce Type: replace-cross 
Abstract: In this work, we introduce a novel neural operator, the Solute Transport Operator Network (STONet), to efficiently model contaminant transport in micro-cracked porous media. STONet's model architecture is specifically designed for this problem and uniquely integrates an enriched DeepONet structure with a transformer-based multi-head attention mechanism, enhancing performance without incurring additional computational overhead compared to existing neural operators. The model combines different networks to encode heterogeneous properties effectively and predict the rate of change of the concentration field to accurately model the transport process. The training data is obtained using finite element (FEM) simulations by random sampling of micro-fracture distributions and applied pressure boundary conditions, which capture diverse scenarios of fracture densities, orientations, apertures, lengths, and balance of pressure-driven to density-driven flow. Our numerical experiments demonstrate that, once trained, STONet achieves accurate predictions, with relative errors typically below 1% compared with FEM simulations while reducing runtime by approximately two orders of magnitude. This type of computational efficiency facilitates building digital twins for rapid assessment of subsurface contamination risks and optimization of environmental remediation strategies. The data and code for the paper will be published at https://github.com/ehsanhaghighat/STONet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05576v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.NE</category>
      <category>physics.flu-dyn</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ehsan Haghighat, Mohammad Hesan Adeli, S Mohammad Mousavi, Ruben Juanes</dc:creator>
    </item>
    <item>
      <title>Towards Efficient Parametric State Estimation in Circulating Fuel Reactors with Shallow Recurrent Decoder Networks</title>
      <link>https://arxiv.org/abs/2503.08904</link>
      <description>arXiv:2503.08904v2 Announce Type: replace-cross 
Abstract: The recent developments in data-driven methods have paved the way to new methodologies to provide accurate state reconstruction of engineering systems; nuclear reactors represent particularly challenging applications for this task due to the complexity of the strongly coupled physics involved and the extremely harsh and hostile environments, especially for new technologies such as Generation-IV reactors. Data-driven techniques can combine different sources of information, including computational proxy models and local noisy measurements on the system, to robustly estimate the state. This work leverages the novel Shallow Recurrent Decoder architecture to infer the entire state vector (including neutron fluxes, precursors concentrations, temperature, pressure and velocity) of a reactor from three out-of-core time-series neutron flux measurements alone. In particular, this work extends the standard architecture to treat parametric time-series data, ensuring the possibility of investigating different accidental scenarios and showing the capabilities of this approach to provide an accurate state estimation in various operating conditions. This paper considers as a test case the Molten Salt Fast Reactor (MSFR), a Generation-IV reactor concept, characterised by strong coupling between the neutronics and the thermal hydraulics due to the liquid nature of the fuel. The promising results of this work are further strengthened by the possibility of quantifying the uncertainty associated with the state estimation, due to the considerably low training cost. The accurate reconstruction of every characteristic field in real-time makes this approach suitable for monitoring and control purposes in the framework of a reactor digital twin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08904v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Stefano Riva, Carolina Introini, J. Nathan Kutz, Antonio Cammi</dc:creator>
    </item>
    <item>
      <title>ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition</title>
      <link>https://arxiv.org/abs/2503.21248</link>
      <description>arXiv:2503.21248v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have demonstrated potential in assisting scientific research, yet their ability to discover high-quality research hypotheses remains unexamined due to the lack of a dedicated benchmark. To address this gap, we introduce the first large-scale benchmark for evaluating LLMs with a near-sufficient set of sub-tasks of scientific discovery: inspiration retrieval, hypothesis composition, and hypothesis ranking. We develop an automated framework that extracts critical components - research questions, background surveys, inspirations, and hypotheses - from scientific papers across 12 disciplines, with expert validation confirming its accuracy. To prevent data contamination, we focus exclusively on papers published in 2024, ensuring minimal overlap with LLM pretraining data. Our evaluation reveals that LLMs perform well in retrieving inspirations, an out-of-distribution task, suggesting their ability to surface novel knowledge associations. This positions LLMs as "research hypothesis mines", capable of facilitating automated scientific discovery by generating innovative hypotheses at scale with minimal human intervention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21248v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou</dc:creator>
    </item>
    <item>
      <title>Analogical Learning for Cross-Scenario Generalization: Framework and Application to Intelligent Localization</title>
      <link>https://arxiv.org/abs/2504.08811</link>
      <description>arXiv:2504.08811v2 Announce Type: replace-cross 
Abstract: Existing learning models often exhibit poor generalization when deployed across diverse scenarios. It is primarily due to that the underlying reference frame of the data varies with the deployment environment and settings. However, despite that data of each scenario has a distinct reference frame, its generation generally follows common underlying physical rules. Based on this understanding, this article proposes a deep learning framework named analogical learning (AL), which implicitly retrieves the reference frame information associated with a scenario and then to make accurate prediction by relative analogy with other scenarios. Specifically, we design a bipartite neural network called Mateformer. Its first part captures the relativity within multiple latent feature spaces between the input data and a small amount of embedded data from the studied scenario, while its second part uses this relativity to guide the nonlinear analogy. We apply AL to the typical multi-scenario learning problem of intelligent wireless localization in cellular networks. Extensive experiments validate AL's superiority across three key dimensions. First, it achieves state-of-the-art accuracy in single-scenario benchmarks. Second, it demonstrates stable transferability between different scenarios, avoiding catastrophic forgetting. Finally, and most importantly, it robustly adapts to new, unseen scenarios--including dynamic weather and traffic conditions--without any tuning. All data and code are available at https://github.com/ziruichen-research/ALLoc.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08811v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>eess.SP</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zirui Chen, Zhaoyang Zhang, Ziqing Xing, Ridong Li, Zhaohui Yang, Richeng Jin, Chongwen Huang, Yuzhi Yang, M\'erouane Debbah</dc:creator>
    </item>
    <item>
      <title>Collaborative Charging Scheduling via Balanced Bounding Box Methods</title>
      <link>https://arxiv.org/abs/2506.14461</link>
      <description>arXiv:2506.14461v2 Announce Type: replace-cross 
Abstract: Electric mobility faces several challenges, most notably the high cost of infrastructure development and the underutilization of charging stations. The concept of shared charging offers a promising solution. The paper explores sustainable urban logistics through horizontal collaboration between two fleet operators and addresses a scheduling problem for the shared use of charging stations. To tackle this, the study formulates a collaborative scheduling problem as a bi-objective nonlinear integer programming model, in which each company aims to minimize its own costs, creating inherent conflicts that require trade-offs. The Balanced Bounding Box Methods (B3Ms) are introduced in order to efficiently derive the efficient frontier, identifying a reduced set of representative solutions. These methods enhance computational efficiency by selectively disregarding closely positioned and competing solutions, preserving the diversity and representativeness of the solutions over the efficient frontier. To determine the final solution and ensure balanced collaboration, cooperative bargaining methods are applied. Numerical case studies demonstrate the viability and scalability of the developed methods, showing that the B3Ms can significantly reduce computational time while maintaining the integrity of the frontier. These methods, along with cooperative bargaining, provide an effective framework for solving various bi-objective optimization problems, extending beyond the collaborative scheduling problem presented here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14461v2</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <pubDate>Wed, 02 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fangting Zhou, Balazs Kulcsar, Jiaming Wu</dc:creator>
    </item>
  </channel>
</rss>
