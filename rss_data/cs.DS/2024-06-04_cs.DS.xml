<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Jun 2024 01:54:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Submodular Maximization in Exactly $n$ Queries</title>
      <link>https://arxiv.org/abs/2406.00148</link>
      <description>arXiv:2406.00148v1 Announce Type: new 
Abstract: In this work, we study the classical problem of maximizing a submodular function subject to a matroid constraint. We develop deterministic algorithms that are very parsimonious with respect to querying the submodular function, for both the case when the submodular function is monotone and the general submodular case. In particular, we present a 1/4 approximation algorithm for the monotone case that uses exactly one query per element, which gives the same total number of queries n as the number of queries required to compute the maximum singleton. For the general case, we present a constant factor approximation algorithm that requires 2 queries per element, which is the first algorithm for this problem with linear query complexity in the size of the ground set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00148v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Balkanski, Steven DiSilvio, Alan Kuhnle</dc:creator>
    </item>
    <item>
      <title>Individual Fairness in Graph Decomposition</title>
      <link>https://arxiv.org/abs/2406.00213</link>
      <description>arXiv:2406.00213v1 Announce Type: new 
Abstract: In this paper, we consider classic randomized low diameter decomposition procedures for planar graphs that obtain connected clusters which are cohesive in that close-by pairs of nodes are assigned to the same cluster with high probability. We require the additional aspect of individual fairness - pairs of nodes at comparable distances should be separated with comparable probability. We show that classic decomposition procedures do not satisfy this property. We present novel algorithms that achieve various trade-offs between this property and additional desiderata of connectivity of the clusters and optimality in the number of clusters. We show that our individual fairness bounds may be difficult to improve by tying the improvement to resolving a major open question in metric embeddings. We finally show the efficacy of our algorithms on real planar networks modeling congressional redistricting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00213v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kamesh Munagala, Govind S. Sankar</dc:creator>
    </item>
    <item>
      <title>Counting on General Run-Length Grammars</title>
      <link>https://arxiv.org/abs/2406.00221</link>
      <description>arXiv:2406.00221v1 Announce Type: new 
Abstract: We introduce a data structure for counting pattern occurrences in texts compressed with any run-length context-free grammar. Our structure uses space proportional to the grammar size and counts the occurrences of a pattern of length $m$ in a text of length $n$ in time (O(m\log^{2+\epsilon} n)), for any constant (\epsilon &gt; 0). This closes an open problem posed by Christiansen et al.~[ACM TALG 2020] and enhances our abilities for computation over compressed data; we give an example application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00221v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gonzalo Navarro, Alejandro Pacheco</dc:creator>
    </item>
    <item>
      <title>Optimal bounds for $\ell_p$ sensitivity sampling via $\ell_2$ augmentation</title>
      <link>https://arxiv.org/abs/2406.00328</link>
      <description>arXiv:2406.00328v1 Announce Type: new 
Abstract: Data subsampling is one of the most natural methods to approximate a massively large data set by a small representative proxy. In particular, sensitivity sampling received a lot of attention, which samples points proportional to an individual importance measure called sensitivity. This framework reduces in very general settings the size of data to roughly the VC dimension $d$ times the total sensitivity $\mathfrak S$ while providing strong $(1\pm\varepsilon)$ guarantees on the quality of approximation. The recent work of Woodruff &amp; Yasuda (2023c) improved substantially over the general $\tilde O(\varepsilon^{-2}\mathfrak Sd)$ bound for the important problem of $\ell_p$ subspace embeddings to $\tilde O(\varepsilon^{-2}\mathfrak S^{2/p})$ for $p\in[1,2]$. Their result was subsumed by an earlier $\tilde O(\varepsilon^{-2}\mathfrak Sd^{1-p/2})$ bound which was implicitly given in the work of Chen &amp; Derezinski (2021). We show that their result is tight when sampling according to plain $\ell_p$ sensitivities. We observe that by augmenting the $\ell_p$ sensitivities by $\ell_2$ sensitivities, we obtain better bounds improving over the aforementioned results to optimal linear $\tilde O(\varepsilon^{-2}(\mathfrak S+d)) = \tilde O(\varepsilon^{-2}d)$ sampling complexity for all $p \in [1,2]$. In particular, this resolves an open question of Woodruff &amp; Yasuda (2023c) in the affirmative for $p \in [1,2]$ and brings sensitivity subsampling into the regime that was previously only known to be possible using Lewis weights (Cohen &amp; Peng, 2015). As an application of our main result, we also obtain an $\tilde O(\varepsilon^{-2}\mu d)$ sensitivity sampling bound for logistic regression, where $\mu$ is a natural complexity measure for this problem. This improves over the previous $\tilde O(\varepsilon^{-2}\mu^2 d)$ bound of Mai et al. (2021) which was based on Lewis weights subsampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00328v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Munteanu, Simon Omlor</dc:creator>
    </item>
    <item>
      <title>Turnstile $\ell_p$ leverage score sampling with applications</title>
      <link>https://arxiv.org/abs/2406.00339</link>
      <description>arXiv:2406.00339v1 Announce Type: new 
Abstract: The turnstile data stream model offers the most flexible framework where data can be manipulated dynamically, i.e., rows, columns, and even single entries of an input matrix can be added, deleted, or updated multiple times in a data stream. We develop a novel algorithm for sampling rows $a_i$ of a matrix $A\in\mathbb{R}^{n\times d}$, proportional to their $\ell_p$ norm, when $A$ is presented in a turnstile data stream. Our algorithm not only returns the set of sampled row indexes, it also returns slightly perturbed rows $\tilde{a}_i \approx a_i$, and approximates their sampling probabilities up to $\varepsilon$ relative error. When combined with preconditioning techniques, our algorithm extends to $\ell_p$ leverage score sampling over turnstile data streams. With these properties in place, it allows us to simulate subsampling constructions of coresets for important regression problems to operate over turnstile data streams with very little overhead compared to their respective off-line subsampling algorithms. For logistic regression, our framework yields the first algorithm that achieves a $(1+\varepsilon)$ approximation and works in a turnstile data stream using polynomial sketch/subsample size, improving over $O(1)$ approximations, or $\exp(1/\varepsilon)$ sketch size of previous work. We compare experimentally to plain oblivious sketching and plain leverage score sampling algorithms for $\ell_p$ and logistic regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00339v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Munteanu, Simon Omlor</dc:creator>
    </item>
    <item>
      <title>Better coloring of 3-colorable graphs</title>
      <link>https://arxiv.org/abs/2406.00357</link>
      <description>arXiv:2406.00357v1 Announce Type: new 
Abstract: We consider the problem of coloring a 3-colorable graph in polynomial time using as few colors as possible. This is one of the most challenging problems in graph algorithms.
  In this paper using Blum's notion of ``progress'', we develop a new combinatorial algorithm for the following: Given any 3-colorable graph with minimum degree $\ds&gt;\sqrt n$, we can, in polynomial time, make progress towards a $k$-coloring for some $k=\sqrt{n/\ds}\cdot n^{o(1)}$.
  We balance our main result with the best-known semi-definite(SDP) approach which we use for degrees below $n^{0.605073}$. As a result, we show that $\tO(n^{0.19747})$ colors suffice for coloring 3-colorable graphs. This improves on the previous best bound of $\tO(n^{0.19996})$ by Kawarabayashi and Thorup in 2017.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00357v1</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ken-ichi Kawarabayashi, Mikkel Thorup, Hirotaka Yoneda</dc:creator>
    </item>
    <item>
      <title>Approaching 100% Confidence in Stream Summary through ReliableSketch</title>
      <link>https://arxiv.org/abs/2406.00376</link>
      <description>arXiv:2406.00376v1 Announce Type: new 
Abstract: To approximate sums of values in key-value data streams, sketches are widely used in databases and networking systems. They offer high-confidence approximations for any given key while ensuring low time and space overhead. While existing sketches are proficient in estimating individual keys, they struggle to maintain this high confidence across all keys collectively, an objective that is critically important in both algorithm theory and its practical applications. We propose ReliableSketch, the first to control the error of all keys to less than $\Lambda$ with a small failure probability $\Delta$, requiring only $O(1 + \Delta\ln\ln(\frac{N}{\Lambda}))$ amortized time and $O(\frac{N}{\Lambda} + \ln(\frac{1}{\Delta}))$ space. Furthermore, its simplicity makes it hardware-friendly, and we implement it on CPU servers, FPGAs, and programmable switches. Our experiments show that under the same small space, ReliableSketch not only keeps all keys' errors below $\Lambda$ but also achieves near-optimal throughput, outperforming competitors with thousands of uncontrolled estimations. We have made our source code publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00376v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhan Wu, Hanbo Wu, Xilai Liu, Yikai Zhao, Tong Yang, Kaicheng Yang, Sha Wang, Lihua Miao, Gaogang Xie</dc:creator>
    </item>
    <item>
      <title>Structural and algorithmic results for stable cycles and partitions in the Roommates problem</title>
      <link>https://arxiv.org/abs/2406.00437</link>
      <description>arXiv:2406.00437v1 Announce Type: new 
Abstract: An instance of the Stable Roommates problem involves a set of agents, each with ordinal preferences over the others. We seek a stable matching, in which no two agents have an incentive to deviate from their assignment. It is well known that a stable matching is unlikely to exist for instances with a large number of agents. However, stable partitions always exist and provide a succinct certificate for the unsolvability of an instance, although their significance extends beyond this. They are also a useful structural tool to study the problem and correspond to half-matchings in which the agents are in a stable equilibrium.
  In this paper, we investigate the stable partition structure further and show how to efficiently enumerate all stable partitions and the cycles included in such structures. Furthermore, we adapt known fairness and optimality criteria from stable matchings to stable partitions. As there can be an exponential number of stable partitions, we investigate the complexity of computing different "fair" and "optimal" stable partitions directly. While a minimum-regret stable partition always exists and can be computed in linear time, we prove the NP-hardness of finding five other kinds of stable partitions that are "optimal" regarding their profile (measuring the number of first, second, third, etc., choices assigned). Furthermore, we give 2-approximation algorithms for two of the optimal stable partition problems and show the inapproximability within any constant factor for another.
  Through this research, we contribute to a deeper understanding of stable partitions from a combinatorial and complexity point of view, closing the gap between integral and fractional stable matchings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00437v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frederik Glitzner, David Manlove</dc:creator>
    </item>
    <item>
      <title>Knapsack with Vertex Cover, Set Cover, and Hitting Set</title>
      <link>https://arxiv.org/abs/2406.01057</link>
      <description>arXiv:2406.01057v1 Announce Type: new 
Abstract: Given an undirected graph $\GG=(\VV,\EE)$, with vertex weights $(w(u))_{u\in\VV}$, vertex values $(\alpha(u))_{u\in\VV}$, a knapsack size $s$, and a target value $d$, the \vcknapsack problem is to determine if there exists a subset $\UU\subseteq\VV$ of vertices such that \UU forms a vertex cover, $w(\UU)=\sum_{u\in\UU} w(u) \le s$, and $\alpha(\UU)=\sum_{u\in\UU} \alpha(u) \ge d$. In this paper, we closely study the \vcknapsack problem and its variations, such as \vcknapsackbudget, \minimalvcknapsack, and \minimumvcknapsack, for both general graphs and trees. We first prove that the \vcknapsack problem belongs to the complexity class \NPC and then study the complexity of the other variations. We generalize the problem to \setc and \hs versions and design polynomial time $H_g$-factor approximation algorithm for the \setckp problem and d-factor approximation algorithm for \hstp using primal dual method. We further show that \setcks and \hsmb are hard to approximate in polynomial time. Additionally, we develop a fixed parameter tractable algorithm running in time $8^{\OO(\tw)}\cdot n\cdot {\sf min}\{s,d\}$ where $\tw,s,d,n$ are respectively treewidth of the graph, the size of the knapsack, the target value of the knapsack, and the number of items for the \minimalvcknapsack problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01057v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Palash Dey, Ashlesha Hota, Sudeshna Kolay, Sipra Singh</dc:creator>
    </item>
    <item>
      <title>Linear Index for Logarithmic Search-Time for any String under any Internal Node in Suffix Trees</title>
      <link>https://arxiv.org/abs/2406.01174</link>
      <description>arXiv:2406.01174v1 Announce Type: new 
Abstract: Suffix trees are key and efficient data structure for solving string problems. A suffix tree is a compressed trie containing all the suffixes of a given text of length $n$ with a linear construction cost. In this work, we introduce an algorithm to build a linear index that allows finding a pattern of any length under any internal node in a suffix tree in O(logn) time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01174v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anas Al-okaily</dc:creator>
    </item>
    <item>
      <title>Robust Fair Clustering with Group Membership Uncertainty Sets</title>
      <link>https://arxiv.org/abs/2406.00599</link>
      <description>arXiv:2406.00599v1 Announce Type: cross 
Abstract: We study the canonical fair clustering problem where each cluster is constrained to have close to population-level representation of each group. Despite significant attention, the salient issue of having incomplete knowledge about the group membership of each point has been superficially addressed. In this paper, we consider a setting where errors exist in the assigned group memberships. We introduce a simple and interpretable family of error models that require a small number of parameters to be given by the decision maker. We then present an algorithm for fair clustering with provable robustness guarantees. Our framework enables the decision maker to trade off between the robustness and the clustering quality. Unlike previous work, our algorithms are backed by worst-case theoretical guarantees. Finally, we empirically verify the performance of our algorithm on real world datasets and show its superior performance over existing baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00599v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sharmila Duppala, Juan Luque, John P. Dickerson, Seyed A. Esmaeili</dc:creator>
    </item>
    <item>
      <title>Sample Complexity of Posted Pricing for a Single Item</title>
      <link>https://arxiv.org/abs/2406.00819</link>
      <description>arXiv:2406.00819v1 Announce Type: cross 
Abstract: Selling a single item to $n$ self-interested buyers is a fundamental problem in economics, where the two objectives typically considered are welfare maximization and revenue maximization. Since the optimal mechanisms are often impractical and do not work for sequential buyers, posted pricing mechanisms, where fixed prices are set for the item for different buyers, have emerged as a practical and effective alternative. This paper investigates how many samples are needed from buyers' value distributions to find near-optimal posted prices, considering both independent and correlated buyer distributions, and welfare versus revenue maximization. We obtain matching upper and lower bounds (up to logarithmic factors) on the sample complexity for all these settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00819v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Billy Jin, Thomas Kesselheim, Will Ma, Sahil Singla</dc:creator>
    </item>
    <item>
      <title>Faster Diffusion-based Sampling with Randomized Midpoints: Sequential and Parallel</title>
      <link>https://arxiv.org/abs/2406.00924</link>
      <description>arXiv:2406.00924v1 Announce Type: cross 
Abstract: In recent years, there has been a surge of interest in proving discretization bounds for diffusion models. These works show that for essentially any data distribution, one can approximately sample in polynomial time given a sufficiently accurate estimate of its score functions at different noise levels. In this work, we propose a new discretization scheme for diffusion models inspired by Shen and Lee's randomized midpoint method for log-concave sampling~\cite{ShenL19}. We prove that this approach achieves the best known dimension dependence for sampling from arbitrary smooth distributions in total variation distance ($\widetilde O(d^{5/12})$ compared to $\widetilde O(\sqrt{d})$ from prior work). We also show that our algorithm can be parallelized to run in only $\widetilde O(\log^2 d)$ parallel rounds, constituting the first provable guarantees for parallel sampling with diffusion models.
  As a byproduct of our methods, for the well-studied problem of log-concave sampling in total variation distance, we give an algorithm and simple analysis achieving dimension dependence $\widetilde O(d^{5/12})$ compared to $\widetilde O(\sqrt{d})$ from prior work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00924v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shivam Gupta, Linda Cai, Sitan Chen</dc:creator>
    </item>
    <item>
      <title>Profile Reconstruction from Private Sketches</title>
      <link>https://arxiv.org/abs/2406.01158</link>
      <description>arXiv:2406.01158v1 Announce Type: cross 
Abstract: Given a multiset of $n$ items from $\mathcal{D}$, the \emph{profile reconstruction} problem is to estimate, for $t = 0, 1, \dots, n$, the fraction $\vec{f}[t]$ of items in $\mathcal{D}$ that appear exactly $t$ times. We consider differentially private profile estimation in a distributed, space-constrained setting where we wish to maintain an updatable, private sketch of the multiset that allows us to compute an approximation of $\vec{f} = (\vec{f}[0], \dots, \vec{f}[n])$. Using a histogram privatized using discrete Laplace noise, we show how to ``reverse'' the noise, using an approach of Dwork et al.~(ITCS '10). We show how to speed up their LP-based technique from polynomial time to $O(d + n \log n)$, where $d = |\mathcal{D}|$, and analyze the achievable error in the $\ell_1$, $\ell_2$ and $\ell_\infty$ norms. In all cases the dependency of the error on $d$ is $O( 1 / \sqrt{d})$ -- we give an information-theoretic lower bound showing that this dependence on $d$ is asymptotically optimal among all private, updatable sketches for the profile reconstruction problem with a high-probability error guarantee.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01158v1</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Wu, Rasmus Pagh</dc:creator>
    </item>
    <item>
      <title>Polynomial Bounds of CFLOBDDs against BDDs</title>
      <link>https://arxiv.org/abs/2406.01525</link>
      <description>arXiv:2406.01525v1 Announce Type: cross 
Abstract: Binary Decision Diagrams (BDDs) are widely used for the representation of Boolean functions. Context-Free-Language Ordered Decision Diagrams (CFLOBDDs) are a plug-compatible replacement for BDDs -- roughly, they are BDDs augmented with a certain form of procedure call. A natural question to ask is, ``For a given Boolean function $f$, what is the relationship between the size of a BDD for $f$ and the size of a CFLOBDD for $f$?'' Sistla et al. established that, in the best case, the CFLOBDD for a function $f$ can be exponentially smaller than any BDD for $f$ (regardless of what variable ordering is used in the BDD); however, they did not give a worst-case bound -- i.e., they left open the question, ``Is there a family of functions $\{ f_i \}$ for which the size of a CFLOBDD for $f_i$ must be substantially larger than a BDD for $f_i$?'' For instance, it could be that there is a family of functions for which the BDDs are exponentially more succinct than any corresponding CFLOBDDs.
  This paper studies such questions, and answers the second question posed above in the negative. In particular, we show that by using the same variable ordering in the CFLOBDD that is used in the BDD, the size of a CFLOBDD for any function $f$ cannot be far worse than the size of the BDD for $f$. The bound that relates their sizes is polynomial: If BDD $B$ for function $f$ is of size $|B|$ and uses variable ordering $\textit{Ord}$, then the size of the CFLOBDD $C$ for $f$ that also uses $\textit{Ord}$ is bounded by $O(|B|^3)$.
  The paper also shows that the bound is tight: there is a family of functions for which $|C|$ grows as $\Omega(|B|^3)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01525v1</guid>
      <category>cs.SC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.FL</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xusheng Zhi (University of Wisconsin-Madison,Peking University), Thomas Reps (University of Wisconsin-Madison)</dc:creator>
    </item>
    <item>
      <title>Approximating optimization problems in graphs with locational uncertainty</title>
      <link>https://arxiv.org/abs/2206.08187</link>
      <description>arXiv:2206.08187v2 Announce Type: replace 
Abstract: Many combinatorial optimization problems can be formulated as the search for a subgraph that satisfies certain properties and minimizes the total weight. We assume here that the vertices correspond to points in a metric space and can take any position in given uncertainty sets. Then, the cost function to be minimized is the sum of the distances for the worst positions of the vertices in their uncertainty sets. We propose two types of polynomial-time approximation algorithms. The first one relies on solving a deterministic counterpart of the problem where the uncertain distances are replaced with maximum pairwise distances. We study in details the resulting approximation ratio, which depends on the structure of the feasible subgraphs and whether the metric space is Ptolemaic or not. The second algorithm is a fully-polynomial time approximation scheme for the special case of $s-t$ paths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.08187v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marin Bougeret, J\'er\'emy Omer, Michael Poss</dc:creator>
    </item>
    <item>
      <title>Decomposable Submodular Maximization in Federated Setting</title>
      <link>https://arxiv.org/abs/2402.00138</link>
      <description>arXiv:2402.00138v2 Announce Type: replace 
Abstract: Submodular functions, as well as the sub-class of decomposable submodular functions, and their optimization appear in a wide range of applications in machine learning, recommendation systems, and welfare maximization. However, optimization of decomposable submodular functions with millions of component functions is computationally prohibitive. Furthermore, the component functions may be private (they might represent user preference function, for example) and cannot be widely shared. To address these issues, we propose a {\em federated optimization} setting for decomposable submodular optimization. In this setting, clients have their own preference functions, and a weighted sum of these preferences needs to be maximized. We implement the popular {\em continuous greedy} algorithm in this setting where clients take parallel small local steps towards the local solution and then the local changes are aggregated at a central server. To address the large number of clients, the aggregation is performed only on a subsampled set. Further, the aggregation is performed only intermittently between stretches of parallel local steps, which reduces communication cost significantly. We show that our federated algorithm is guaranteed to provide a good approximate solution, even in the presence of above cost-cutting measures. Finally, we show how the federated setting can be incorporated in solving fundamental discrete submodular optimization problems such as Maximum Coverage and Facility Location.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00138v2</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akbar Rafiey</dc:creator>
    </item>
    <item>
      <title>Expanderizing Higher Order Random Walks</title>
      <link>https://arxiv.org/abs/2405.08927</link>
      <description>arXiv:2405.08927v3 Announce Type: replace 
Abstract: We study a variant of the down-up and up-down walks over an $n$-partite simplicial complex, which we call expanderized higher order random walks -- where the sequence of updated coordinates correspond to the sequence of vertices visited by a random walk over an auxiliary expander graph $H$. When $H$ is the clique, this random walk reduces to the usual down-up walk and when $H$ is the directed cycle, this random walk reduces to the well-known systematic scan Glauber dynamics. We show that whenever the usual higher order random walks satisfy a log-Sobolev inequality or a Poincar\'e inequality, the expanderized walks satisfy the same inequalities with a loss of quality related to the two-sided expansion of the auxillary graph $H$. Our construction can be thought as a higher order random walk generalization of the derandomized squaring algorithm of Rozenman and Vadhan. We show that when initiated with an expander graph our expanderized random walks have mixing time $O(n \log n)$ for sampling a uniformly random list colorings of a graph $G$ of maximum degree $\Delta = O(1)$ where each vertex has at least $(11/6 - \epsilon) \Delta$ and at most $O(\Delta)$ colors and $O\left( \frac{n \log n}{(1 - \| J\|)^2}\right)$ for sampling the Ising model with a PSD interaction matrix $J \in R^{n \times n}$ satisfying $\| J \| \le 1$ and the external field $h \in R^n$-- here the $O(\bullet)$ notation hides a constant that depends linearly on the largest entry of $h$. As expander graphs can be very sparse, this decreases the amount of randomness required to simulate the down-up walks by a logarithmic factor. We also prove some simple results which enable us to argue about log-Sobolev constants of higher order random walks and provide a simple and self-contained analysis of local-to-global $\Phi$-entropy contraction in simplicial complexes -- giving simpler proofs for many pre-existing results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08927v3</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vedat Levi Alev, Shravas Rao</dc:creator>
    </item>
    <item>
      <title>Automated Expected Amortised Cost Analysis of Probabilistic Data Structures</title>
      <link>https://arxiv.org/abs/2206.03537</link>
      <description>arXiv:2206.03537v2 Announce Type: replace-cross 
Abstract: In this paper, we present the first fully-automated expected amortised cost analysis of self-adjusting data structures, that is, of randomised splay trees, randomised splay heaps and randomised meldable heaps, which so far have only (semi-) manually been analysed in the literature. Our analysis is stated as a type-and-effect system for a first-order functional programming language with support for sampling over discrete distributions, non-deterministic choice and a ticking operator. The latter allows for the specification of fine-grained cost models. We state two soundness theorems based on two different -- but strongly related -- typing rules of ticking, which account differently for the cost of non-terminating computations. Finally we provide a prototype implementation able to fully automatically analyse the aforementioned case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.03537v2</guid>
      <category>cs.LO</category>
      <category>cs.DS</category>
      <category>cs.PL</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-13188-2_4</arxiv:DOI>
      <dc:creator>Lorenz Leutgeb, Georg Moser, Florian Zuleger</dc:creator>
    </item>
    <item>
      <title>Schr\"odinger as a Quantum Programmer: Estimating Entanglement via Steering</title>
      <link>https://arxiv.org/abs/2303.07911</link>
      <description>arXiv:2303.07911v4 Announce Type: replace-cross 
Abstract: Quantifying entanglement is an important task by which the resourcefulness of a quantum state can be measured. Here, we develop a quantum algorithm that tests for and quantifies the separability of a general bipartite state by using the quantum steering effect, the latter initially discovered by Schr\"odinger. Our separability test consists of a distributed quantum computation involving two parties: a computationally limited client, who prepares a purification of the state of interest, and a computationally unbounded server, who tries to steer the reduced systems to a probabilistic ensemble of pure product states. To design a practical algorithm, we replace the role of the server with a combination of parameterized unitary circuits and classical optimization techniques to perform the necessary computation. The result is a variational quantum steering algorithm (VQSA), a modified separability test that is implementable on quantum computers that are available today. We then simulate our VQSA on noisy quantum simulators and find favorable convergence properties on the examples tested. We also develop semidefinite programs, executable on classical computers, that benchmark the results obtained from our VQSA. Thus, our findings provide a meaningful connection between steering, entanglement, quantum algorithms, and quantum computational complexity theory. They also demonstrate the value of a parameterized mid-circuit measurement in a VQSA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07911v4</guid>
      <category>quant-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>hep-th</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aby Philip, Soorya Rethinasamy, Vincent Russo, Mark M. Wilde</dc:creator>
    </item>
  </channel>
</rss>
