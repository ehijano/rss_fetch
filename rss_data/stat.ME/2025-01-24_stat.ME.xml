<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Jan 2025 05:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Normalizing Flows for Gaussian Process Regression under Hierarchical Shrinkage Priors</title>
      <link>https://arxiv.org/abs/2501.13173</link>
      <description>arXiv:2501.13173v1 Announce Type: new 
Abstract: Gaussian Process Regression (GPR) is a powerful tool for nonparametric regression, but its fully Bayesian application in high-dimensional settings is hindered by two primary challenges: the computational burden (exacerbated by fully Bayesian inference) and the difficulty of variable selection. This paper introduces a novel methodology that combines hierarchical global-local shrinkage priors with normalizing flows to address these challenges. The hierarchical triple gamma prior offers a principled framework for inducing sparsity in high-dimensional GPR, effectively excluding irrelevant covariates while preserving interpretability and flexibility in model size. Normalizing flows are employed within a variational inference framework to approximate the posterior distribution of hyperparameters, capturing complex dependencies while ensuring computational scalability. Simulation studies demonstrate the efficacy of the proposed approach, outperforming traditional maximum likelihood estimation and mean-field variational methods, particularly in high-sparsity and high-dimensional settings. The results highlight the robustness and flexibility of hierarchical shrinkage priors and the computational efficiency of normalizing flows for Bayesian GPR. This work provides a scalable and interpretable solution for high-dimensional regression, with implications for sparse modeling and posterior approximation in broader Bayesian contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13173v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Knaus</dc:creator>
    </item>
    <item>
      <title>Design of Bayesian Clinical Trials with Clustered Data and Multiple Endpoints</title>
      <link>https://arxiv.org/abs/2501.13218</link>
      <description>arXiv:2501.13218v1 Announce Type: new 
Abstract: In the design of clinical trials, it is essential to assess the design operating characteristics (i.e., the probabilities of making correct decisions). Common practice for the evaluation of operating characteristics in Bayesian clinical trials relies on estimating the sampling distribution of posterior summaries via Monte Carlo simulation. It is computationally intensive to repeat this estimation process for each design configuration considered, particularly for clustered data that are analyzed using complex, high-dimensional models. In this paper, we propose an efficient method to assess operating characteristics and determine sample sizes for Bayesian trials with clustered data and multiple endpoints. We prove theoretical results that enable posterior probabilities to be modelled as a function of the sample size. Using these functions, we assess operating characteristics at a range of sample sizes given simulations conducted at only two sample sizes. These theoretical results are also leveraged to quantify the impact of simulation variability on our sample size recommendations. The applicability of our methodology is illustrated using a current clinical trial with clustered data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13218v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luke Hagar, Shirin Golchi</dc:creator>
    </item>
    <item>
      <title>Peak Inference for Gaussian Random Fields on a Lattice</title>
      <link>https://arxiv.org/abs/2501.13239</link>
      <description>arXiv:2501.13239v1 Announce Type: new 
Abstract: In this work we develop a Monte Carlo method to compute the height distribution of local maxima of a stationary Gaussian or Gaussian-related random field that is observed on a regular lattice. We show that our method can be used to provide valid peak based inference in datasets with low levels of smoothness, where existing formulae derived for continuous domains are not accurate. We also extend the methods in Worsley (2005) and Taylor et al. (2007) to compute the peak height distribution and compare them with our approach. Lastly, we apply our method to a task fMRI dataset to show how it can be used in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13239v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tuo Lin, Armin Schwartzman, Samuel Davenport</dc:creator>
    </item>
    <item>
      <title>A Bayesian Record Linkage Approach to Applications in Tree Demography Using Overlapping LiDAR Scans</title>
      <link>https://arxiv.org/abs/2501.13285</link>
      <description>arXiv:2501.13285v1 Announce Type: new 
Abstract: In the information age, it has become increasingly common for data containing records about overlapping individuals to be distributed across multiple sources, making it necessary to identify which records refer to the same individual. The goal of record linkage is to estimate this unknown structure in the absence of a unique identifiable attribute. We introduce a Bayesian hierarchical record linkage model for spatial location data motivated by the estimation of individual specific growth-size curves for conifer species using data derived from overlapping LiDAR scans. Annual tree growth may be estimated dependent upon correctly identifying unique individuals across scans in the presence of noise. We formalize a two-stage modeling framework, connecting the record linkage model and a flexible downstream individual tree growth model, that provides robust uncertainty quantification and propagation through both stages of the modeling pipeline via an extension of the linkage-averaging approach of Sadinle (2018). In this paper, we discuss the two-stage model formulation, outline the computational strategies required to achieve scalability, assess the model performance on simulated data, and fit the model to a bi-temporal dataset derived from LiDAR scans of the Upper Gunnison Watershed provided by the Rocky Mountain Biological Laboratory to assess the impact of key topographic covariates on the growth behavior of conifer species in the Southern Rocky Mountains (USA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13285v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>L. Drew, A. Kaplan, I. Breckheimer</dc:creator>
    </item>
    <item>
      <title>Enterprise Experimentation with Hierarchical Entities</title>
      <link>https://arxiv.org/abs/2501.13293</link>
      <description>arXiv:2501.13293v1 Announce Type: new 
Abstract: In this paper, we address the challenges in running enterprise experimentation with hierarchical entities and present the methodologies behind the implementation of the Enterprise Experimentation Platform (EEP) at LinkedIn, which plays a pivotal role in delivering an intelligent, scalable, and reliable experimentation experience to optimize performance across all LinkedIn's enterprise products. We start with an introduction to the hierarchical entity relationships of the enterprise products and how such complex entity structure poses challenges to experimentation. We then delve into the details of our solutions for EEP including taxonomy based design setup with multiple entities, analysis methodologies in the presence of hierarchical entities, and advanced variance reduction techniques, etc. Recognizing the hierarchical ramping patterns inherent in enterprise experiments, we also propose a two-level Sample Size Ratio Mismatch (SSRM) detection methodology. This approach addresses SSRM at both the randomization unit and analysis unit levels, bolstering the internal validity and trustworthiness of analysis results within EEP. In the end, we discuss implementations and examine the business impacts of EEP through practical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13293v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shan Ba, Shilpa Garg, Jitendra Agarwal, Hanyue Zhao</dc:creator>
    </item>
    <item>
      <title>Model selection for vine copulas under nested hypotheses</title>
      <link>https://arxiv.org/abs/2501.13304</link>
      <description>arXiv:2501.13304v1 Announce Type: new 
Abstract: Vine copulas, constructed using bivariate copulas as building blocks, provide a flexible framework for modeling high-dimensional dependencies. However, this flexibility is accompanied by rapidly increasing complexity as dimensionality grows, necessitating appropriate truncation to manage this challenge. While use of Vuong's model selection test has been proposed as a method to determine the optimal truncation level, its application to vine copulas has been heuristic, assuming only strictly non-nested hypotheses. This assumption conflicts with the inherent topological nesting within truncated vine copula structures. In this paper, we systematically apply Vuong's model selection tests to distinguish competing models of truncated vine copulas under both nested and strictly non-nested hypotheses. By clarifying the differences between the results and exploring their practical implications, we enhance the precision of truncation selection and contribute to the development of more effective methodologies in vine copula modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13304v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ichiro Nishi, Yoshinori Kawasaki</dc:creator>
    </item>
    <item>
      <title>Capturing heterogeneous time-variation in covariate effects in non-proportional hazard regression models</title>
      <link>https://arxiv.org/abs/2501.13525</link>
      <description>arXiv:2501.13525v1 Announce Type: new 
Abstract: A central focus in survival analysis is examining how covariates influence survival time. These covariate effects are often found to be either time-varying, heterogeneous - such as being specific to patients, treatments, or subgroups - or exhibit both characteristics simultaneously. While the standard model, the Cox proportional hazards model, allows neither time-varying nor heterogeneous effects, several extensions to the Cox model as well as alternative modeling frameworks have been introduced. However, no unified framework for incorporating heterogeneously time-varying effects of covariates has been proposed yet. Such effects occur when a covariate influences survival not only in a heterogeneous and time-varying manner, but when the time-variation is also heterogeneous. We propose to model such effects by introducing heterogeneously time-varying coefficients to piecewise exponential additive mixed models. We deploy functional random effects, also known as factor smooths, to model such coefficients as the interaction effect of heterogeneity and time-variation. Our approach allows for non-linear time-effects due to being based on penalized splines and uses an efficient random effects basis to model the heterogeneity. Using a penalized basis prevents overfitting in case of absence of such effects. In addition, the penalization mostly solves the problem of choosing the number of intervals which is usually present in unregularized piecewise exponential approaches. We demonstrate the superiority of our approach in comparison to competitors by means of a simulation study. Finally, the practical application and relevance are outlined by presenting a brain tumor case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13525v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niklas Hagemann, Thomas Kneib, Kathrin M\"ollenhoff</dc:creator>
    </item>
    <item>
      <title>Determining The Number of Factors in Two-Way Factor Model of High-Dimensional Matrix-Variate Time Series: A White-Noise based Method for Serial Correlation Models</title>
      <link>https://arxiv.org/abs/2501.13614</link>
      <description>arXiv:2501.13614v1 Announce Type: new 
Abstract: In this paper, we study a new two-way factor model for high-dimensional matrix-variate time series. To estimate the number of factors in this two-way factor model, we decompose the series into two parts: one being a non-weakly correlated series and the other being a weakly correlated noise. By comparing the difference between two series, we can construct white-noise based signal statistics to determine the number of factors in row loading matrix (column loading matrix). Furthermore, to mitigate the negative impact on the accuracy of the estimation, which is caused by the interaction between the row loading matrix and the column loading matrix, we propose a transformation so that the transformed model only contains the row loading matrix (column loading matrix). We define sequences of ratios of two test statistics as signal statistics to determine the number of factors and derive the consistence of the estimation. We implement the numerical studies to examine the performance of the new methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13614v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiang Xia</dc:creator>
    </item>
    <item>
      <title>Reconciling Binary Replicates: Beyond the Average</title>
      <link>https://arxiv.org/abs/2501.13745</link>
      <description>arXiv:2501.13745v1 Announce Type: new 
Abstract: Binary observations are often repeated to improve data quality, creating technical replicates. Several scoring methods are commonly used to infer the actual individual state and obtain a probability for each state. The common practice of averaging replicates has limitations, and alternative methods for scoring and classifying individuals are proposed. Additionally, an indecisive response might be wiser than classifying all individuals based on their replicates in the medical context, where 1 indicates a particular health condition. Building on the inherent limitations of the averaging approach, three alternative methods are examined: the median, maximum penalized likelihood estimation, and a Bayesian algorithm. The theoretical analysis suggests that the proposed alternatives outperform the averaging approach, especially the Bayesian method, which incorporates uncertainty and provides credible intervals. Simulations and real-world medical datasets are used to demonstrate the practical implications of these methods for improving diagnostic accuracy and disease prevalence estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13745v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manuela Royer-Carenzi, Hadrien Lorenzo, Pierre Pudlo</dc:creator>
    </item>
    <item>
      <title>Inference for generalized additive mixed models via penalized marginal likelihood</title>
      <link>https://arxiv.org/abs/2501.13797</link>
      <description>arXiv:2501.13797v1 Announce Type: new 
Abstract: Existing methods for fitting generalized additive mixed models to longitudinal repeated measures data rely on Laplace-approximate marginal likelihood for estimation of variance components and smoothing penalty parameters. This is thought to be appropriate due to the Laplace approximation being established as an appropriate tool for smoothing penalty parameter estimation in spline models and the well-known connection between penalized regression and random effects. This paper argues that the Laplace approximation is sometimes not sufficiently accurate for smoothing parameter estimation in generalized additive mixed models leading to estimates that exhibit increasing bias and decreasing confidence interval coverage as more groups are sampled. A novel estimation strategy based on penalizing an adaptive quadrature approximate marginal likelihood is proposed that solves this problem and leads to estimates exhibiting the correct statistical properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13797v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Stringer</dc:creator>
    </item>
    <item>
      <title>Detecting Sparse Cointegration</title>
      <link>https://arxiv.org/abs/2501.13839</link>
      <description>arXiv:2501.13839v1 Announce Type: new 
Abstract: We propose a two-step procedure to detect cointegration in high-dimensional settings, focusing on sparse relationships. First, we use the adaptive LASSO to identify the small subset of integrated covariates driving the equilibrium relationship with a target series, ensuring model-selection consistency. Second, we adopt an information-theoretic model choice criterion to distinguish between stationarity and nonstationarity in the resulting residuals, avoiding dependence on asymptotic distributional assumptions. Monte Carlo experiments confirm robust finite-sample performance, even under endogeneity and serial correlation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13839v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesus Gonzalo, Jean-Yves Pitarakis</dc:creator>
    </item>
    <item>
      <title>Efficient Active Learning Strategies for Computer Experiments</title>
      <link>https://arxiv.org/abs/2501.13841</link>
      <description>arXiv:2501.13841v1 Announce Type: new 
Abstract: Active learning in computer experiments aims at allocating resources in an intelligent manner based on the already observed data to satisfy certain objectives such as emulating or optimizing a computationally expensive function. There are two main ingredients for active learning: an initial experimental design, which helps to approximately learn the function, and a surrogate modeling technique, which provides a prediction of the output along with its uncertainty estimates. Space-filling designs are commonly used as initial design and Gaussian processes for surrogate modeling. This article aims at improving the active learning procedure by proposing a new type of initial design and a new correlation function for the Gaussian process. The ideas behind them are known in other fields such as in sensitivity analysis or in kernel theory, but they never seem to have been used for active learning in computer experiments. We show that they provide substantial improvement to the state-of-the-art methods for both emulation and optimization. We support our findings through theory and simulations, and a real experiment on the vapor-phase infiltration process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13841v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Difan Song, V. Roshan Joseph</dc:creator>
    </item>
    <item>
      <title>Finite mixture representations of zero-&amp;-$N$-inflated distributions for count-compositional data</title>
      <link>https://arxiv.org/abs/2501.13879</link>
      <description>arXiv:2501.13879v1 Announce Type: new 
Abstract: We provide novel probabilistic portrayals of two multivariate models designed to handle zero-inflation in count-compositional data. We develop a new unifying framework that represents both as finite mixture distributions. One of these distributions, based on Dirichlet-multinomial components, has been studied before, but has not yet been properly characterised as a sampling distribution of the counts. The other, based on multinomial components, is a new contribution. Using our finite mixture representations enables us to derive key statistical properties, including moments, marginal distributions, and special cases for both distributions. We develop enhanced Bayesian inference schemes with efficient Gibbs sampling updates, wherever possible, for parameters and auxiliary variables, demonstrating improvements over existing methods in the literature. We conduct simulation studies to evaluate the efficiency of the Bayesian inference procedures and to illustrate the practical utility of the proposed distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13879v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'e F. B. Menezes, Andrew C. Parnell, Keefe Murphy</dc:creator>
    </item>
    <item>
      <title>Distributed Multiple Testing with False Discovery Rate Control in the Presence of Byzantines</title>
      <link>https://arxiv.org/abs/2501.13242</link>
      <description>arXiv:2501.13242v1 Announce Type: cross 
Abstract: This work studies distributed multiple testing with false discovery rate (FDR) control in the presence of Byzantine attacks, where an adversary captures a fraction of the nodes and corrupts their reported p-values. We focus on two baseline attack models: an oracle model with the full knowledge of which hypotheses are true nulls, and a practical attack model that leverages the Benjamini-Hochberg (BH) procedure locally to classify which p-values follow the true null hypotheses. We provide a thorough characterization of how both attack models affect the global FDR, which in turn motivates counter-attack strategies and stronger attack models. Our extensive simulation studies confirm the theoretical results, highlight key design trade-offs under attacks and countermeasures, and provide insights into more sophisticated attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13242v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daofu Zhang, Mehrdad Pournaderi, Yu Xiang, Pramod Varshney</dc:creator>
    </item>
    <item>
      <title>Generalizability with ignorance in mind: learning what we do (not) know for archetypes discovery</title>
      <link>https://arxiv.org/abs/2501.13355</link>
      <description>arXiv:2501.13355v1 Announce Type: cross 
Abstract: When studying policy interventions, researchers are often interested in two related goals: i) learning for which types of individuals the program has the largest effects (heterogeneity) and ii) understanding whether those patterns of treatment effects have predictive power across environments (generalizability). To that end, we develop a framework to learn from the data how to partition observations into groups of individual and environmental characteristics whose effects are generalizable for others - a set of generalizable archetypes. Our view is that implicit in the task of archetypal discovery is detecting those contexts where effects do not generalize and where researchers should collect more evidence before drawing inference on treatment effects. We introduce a method that jointly estimates when and how a prediction can be formed and when, instead, researchers should admit ignorance and elicit further evidence before making predictions. We provide both a decision-theoretic and Bayesian foundation of our procedure. We derive finite-sample (frequentist) regret guarantees, asymptotic theory for inference, and discuss computational properties. We illustrate the benefits of our procedure over existing alternatives that would fail to admit ignorance and force pooling across all units by re-analyzing a multifaceted program targeted towards the poor across six different countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13355v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Emily Breza, Arun G. Chandrasekhar, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>LITE: Efficiently Estimating Gaussian Probability of Maximality</title>
      <link>https://arxiv.org/abs/2501.13535</link>
      <description>arXiv:2501.13535v1 Announce Type: cross 
Abstract: We consider the problem of computing the probability of maximality (PoM) of a Gaussian random vector, i.e., the probability for each dimension to be maximal. This is a key challenge in applications ranging from Bayesian optimization to reinforcement learning, where the PoM not only helps with finding an optimal action, but yields a fine-grained analysis of the action domain, crucial in tasks such as drug discovery. Existing techniques are costly, scaling polynomially in computation and memory with the vector size. We introduce LITE, the first approach for estimating Gaussian PoM with almost-linear time and memory complexity. LITE achieves SOTA accuracy on a number of tasks, while being in practice several orders of magnitude faster than the baselines. This also translates to a better performance on downstream tasks such as entropy estimation and optimal control of bandits. Theoretically, we cast LITE as entropy-regularized UCB and connect it to prior PoM estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13535v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Menet (ETH Z\"urich), Jonas H\"ubotter (ETH Z\"urich), Parnian Kassraie (ETH Z\"urich), Andreas Krause (ETH Z\"urich)</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian graphical modeling using $\gamma$-divergence</title>
      <link>https://arxiv.org/abs/2312.07262</link>
      <description>arXiv:2312.07262v2 Announce Type: replace 
Abstract: Gaussian graphical model is one of the powerful tools to analyze conditional independence between two variables for multivariate Gaussian-distributed observations. When the dimension of data is moderate or high, penalized likelihood methods such as the graphical lasso are useful to detect significant conditional independence structures. However, the estimates are affected by outliers due to the Gaussian assumption. This paper proposes a novel robust posterior distribution for inference of Gaussian graphical models using the $\gamma$-divergence which is one of the robust divergences. In particular, we focus on the Bayesian graphical lasso by assuming the Laplace-type prior for elements of the inverse covariance matrix. The proposed posterior distribution matches its maximum a posteriori estimate with the minimum $\gamma$-divergence estimate provided by the frequentist penalized method. We show that the proposed method satisfies the posterior robustness which is a kind of measure of robustness in Bayesian analysis. The property means that the information of outliers is automatically ignored in the posterior distribution as long as the outliers are extremely large. A sufficient condition for the posterior propriety of the proposed posterior distribution is also derived. Furthermore, an efficient posterior computation algorithm via the weighted Bayesian bootstrap method is proposed. The performance of the proposed method is illustrated through simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07262v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takahiro Onizuka, Shintaro Hashimoto</dc:creator>
    </item>
    <item>
      <title>Fourier analysis of spatial point processes</title>
      <link>https://arxiv.org/abs/2401.06403</link>
      <description>arXiv:2401.06403v3 Announce Type: replace 
Abstract: In this article, we develop comprehensive frequency domain methods for estimating and inferring the second-order structure of spatial point processes. The main element here is on utilizing the discrete Fourier transform (DFT) of the point pattern and its tapered counterpart. Under second-order stationarity, we show that both the DFTs and the tapered DFTs are asymptotically jointly independent Gaussian even when the DFTs share the same limiting frequencies. Based on these results, we establish an $\alpha$-mixing central limit theorem for a statistic formulated as a quadratic form of the tapered DFT. As applications, we derive the asymptotic distribution of the kernel spectral density estimator and establish a frequency domain inferential method for parametric stationary point processes. For the latter, the resulting model parameter estimator is computationally tractable and yields meaningful interpretations even in the case of model misspecification. We investigate the finite sample performance of our estimator through simulations, considering scenarios of both correctly specified and misspecified models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06403v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junho Yang, Yongtao Guan</dc:creator>
    </item>
    <item>
      <title>Enhancing Dose Selection in Phase I Cancer Trials: Extending the Bayesian Logistic Regression Model with Non-DLT Adverse Events Integration</title>
      <link>https://arxiv.org/abs/2405.13767</link>
      <description>arXiv:2405.13767v2 Announce Type: replace 
Abstract: This paper presents the Burdened Bayesian Logistic Regression Model (BBLRM), an enhancement to the Bayesian Logistic Regression Model (BLRM) for dose-finding in phase I oncology trials. Traditionally, the BLRM determines the maximum tolerated dose (MTD) based on dose-limiting toxicities (DLTs). However, clinicians often perceive model-based designs like BLRM as complex and less conservative than rule-based designs, such as the widely used 3+3 method. To address these concerns, the BBLRM incorporates non-DLT adverse events (nDLTAEs) into the model. These events, although not severe enough to qualify as DLTs, provide additional information suggesting that higher doses might result in DLTs. In the BBLRM, an additional parameter $\delta$ is introduced to account for nDLTAEs. This parameter adjusts the toxicity probability estimates, making the model more conservative in dose escalation without compromising the accuracy in allocating the true MTD. The $\delta$ parameter is derived from the proportion of patients experiencing nDLTAEs and is tuned based on the design characteristics to balance the conservatism of the model. This approach aims to reduce the likelihood of assigning toxic doses as MTD while involving clinicians more directly in the decision-making process identifying the nDLTAEs along the study conduction. The paper includes a simulation study comparing BBLRM with more traditional versions of BLRM and a two stage Continual Reassessment Method (CRM) that incorporates nDLTAEs across various scenarios. The simulations demonstrate that BBLRM significantly reduces the selection of toxic doses as MTD without compromising the accuracy of MTD identification. These results suggest that integrating nDLTAEs into the dose-finding process can enhance the safety and acceptance of model-based designs in phase I oncology trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13767v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Nizzardo, Luca Genetti, Marco Pergher</dc:creator>
    </item>
    <item>
      <title>Bayesian Deep Generative Models for Multiplex Networks with Multiscale Overlapping Clusters</title>
      <link>https://arxiv.org/abs/2405.20936</link>
      <description>arXiv:2405.20936v4 Announce Type: replace 
Abstract: Our interest is in multiplex network data with multiple network samples observed across the same set of nodes. Examples originate from a variety of fields, including brain connectivity, international trade networks, and social networks, among others. Our goal is to infer a hierarchical structure of the nodes at a population level, while performing multi-resolution clustering of the individual replicates. To accomplish this, we propose a Bayesian hierarchical model, provide theoretical support in terms of identifiability and posterior consistency, and design efficient methods for posterior computation. We provide novel technical tools for proving model identifiability, which are of independent interest. Our proposed methodology is demonstrated through numerical simulation and an application to brain connectome data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20936v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuren Zhou, Yuqi Gu, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Structural adaptation via directional regularity: rate accelerated estimation in multivariate functional data</title>
      <link>https://arxiv.org/abs/2409.00817</link>
      <description>arXiv:2409.00817v4 Announce Type: replace 
Abstract: We introduce directional regularity, a new definition of anisotropy for multivariate functional data. Instead of taking the conventional view which determines anisotropy as a notion of smoothness along a dimension, directional regularity additionally views anisotropy through the lens of directions. We show that faster rates of convergence can be obtained through a change-of-basis by adapting to the directional regularity of a multivariate process. An algorithm for the estimation and identification of the change-of-basis matrix is constructed, made possible due to the replication structure of functional data. Non-asymptotic bounds are provided for our algorithm, supplemented by numerical evidence from an extensive simulation study. Possible applications of the directional regularity approach are discussed, and we advocate its consideration as a standard pre-processing step in multivariate functional data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00817v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Omar Kassi, Sunny G. W. Wang</dc:creator>
    </item>
    <item>
      <title>From Estimands to Robust Inference of Treatment Effects in Platform Trials</title>
      <link>https://arxiv.org/abs/2411.12944</link>
      <description>arXiv:2411.12944v3 Announce Type: replace 
Abstract: A platform trial is an innovative clinical trial design that uses a master protocol (i.e., one overarching protocol) to evaluate multiple treatments in an ongoing manner and can accelerate the evaluation of new treatments. However, its flexibility introduces inferential challenges, with two fundamental ones being the precise definition of treatment effects and robust, efficient inference on these effects. Central to these challenges is defining an appropriate target population for the estimand, as the populations represented by some commonly used analysis approaches can arbitrarily depend on the randomization ratio or trial type. For the first time, this article presents a clear framework for constructing a clinically meaningful estimand with precise specificity regarding the population of interest. The proposed entire concurrently eligible (ECE) population not only preserves the integrity of randomized comparisons but also remains invariant to both the randomization ratio and trial type. This lays the groundwork for future design, analysis, and research of platform trials. Then, we develop weighting and post-stratification methods for estimation of treatment effects with minimal assumptions. To fully leverage the efficiency potential of platform trials, we also consider a model-assisted approach for baseline covariate adjustment to gain efficiency while maintaining robustness against model misspecification. We derive and compare asymptotic distributions of proposed estimators in theory and propose robust variance estimators. The proposed estimators are empirically evaluated in a simulation study and applied to the SIMPLIFY trial, using the R package RobinCID.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12944v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhan Qian, Yifan Yi, Jun Shao, Yanyao Yi, Gregory Levin, Nicole Mayer-Hamblett, Patrick J. Heagerty, Ting Ye</dc:creator>
    </item>
    <item>
      <title>Multiple change point detection based on Hodrick-Prescott and $l_1$ filtering method for random walk time series data</title>
      <link>https://arxiv.org/abs/2501.11805</link>
      <description>arXiv:2501.11805v2 Announce Type: replace 
Abstract: We propose new methods for detecting multiple change points in time series, specifically designed for random walk processes, where stationarity and variance changes present challenges. Our approach combines two trend estimation methods: the Hodrick Prescott (HP) filter and the l1 filter. A major challenge in these methods is selecting the tuning parameter lambda, which we address by introducing two selection techniques. For the HP based change point detection, we propose a probability-based threshold to select lambda under the assumption of an exponential distribution. For the l1 based method, we suggest a selection strategy assuming normality. Additionally, we introduce a technique to estimate the maximum number of change points in time segments using the l1 based method. We validate our methods by comparing them to similar techniques, such as PELT, using simulated data. We also demonstrate the practical application of our approach to real-world SNP stock data, showcasing its effectiveness in detecting change points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11805v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiyuan Liu</dc:creator>
    </item>
    <item>
      <title>The Dynamic Triple Gamma Prior as a Shrinkage Process Prior for Time-Varying Parameter Models</title>
      <link>https://arxiv.org/abs/2312.10487</link>
      <description>arXiv:2312.10487v2 Announce Type: replace-cross 
Abstract: Many existing shrinkage approaches for time-varying parameter (TVP) models assume constant innovation variances across time points, inducing sparsity by shrinking these variances toward zero. However, this assumption falls short when states exhibit large jumps or structural changes, as often seen in empirical time series analysis. To address this, we propose the dynamic triple gamma prior -- a stochastic process that induces time-dependent shrinkage by modeling dependence among innovations while retaining a well-known triple gamma marginal distribution. This framework encompasses various special and limiting cases, including the horseshoe shrinkage prior, making it highly flexible. We derive key properties of the dynamic triple gamma that highlight its dynamic shrinkage behavior and develop an efficient Markov chain Monte Carlo algorithm for posterior sampling. The proposed approach is evaluated through sparse covariance modeling and forecasting of the returns of the EURO STOXX 50 index, demonstrating favorable forecasting performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10487v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Knaus, Sylvia Fr\"uhwirth-Schnatter</dc:creator>
    </item>
    <item>
      <title>High-dimensional analysis of ridge regression for non-identically distributed data with a variance profile</title>
      <link>https://arxiv.org/abs/2403.20200</link>
      <description>arXiv:2403.20200v3 Announce Type: replace-cross 
Abstract: High-dimensional linear regression has been thoroughly studied in the context of independent and identically distributed data. We propose to investigate high-dimensional regression models for independent but non-identically distributed data. To this end, we suppose that the set of observed predictors (or features) is a random matrix with a variance profile and with dimensions growing at a proportional rate. Assuming a random effect model, we study the predictive risk of the ridge estimator for linear regression with such a variance profile. In this setting, we provide deterministic equivalents of this risk and of the degree of freedom of the ridge estimator. For certain class of variance profile, our work highlights the emergence of the well-known double descent phenomenon in high-dimensional regression for the minimum norm least-squares estimator when the ridge regularization parameter goes to zero. We also exhibit variance profiles for which the shape of this predictive risk differs from double descent. The proofs of our results are based on tools from random matrix theory in the presence of a variance profile that have not been considered so far to study regression models. Numerical experiments are provided to show the accuracy of the aforementioned deterministic equivalents on the computation of the predictive risk of ridge regression. We also investigate the similarities and differences that exist with the standard setting of independent and identically distributed data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20200v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J\'er\'emie Bigot, Issa-Mbenard Dabo, Camille Male</dc:creator>
    </item>
    <item>
      <title>Data-driven upper bounds and event attribution for unprecedented heatwaves</title>
      <link>https://arxiv.org/abs/2408.10251</link>
      <description>arXiv:2408.10251v2 Announce Type: replace-cross 
Abstract: The last decade has seen numerous record-shattering heatwaves in all corners of the globe. In the aftermath of these devastating events, there is interest in identifying worst-case thresholds or upper bounds that quantify just how hot temperatures can become. Generalized Extreme Value theory provides a data-driven estimate of extreme thresholds; however, upper bounds may be exceeded by future events, which undermines attribution and planning for heatwave impacts. Here, we show how the occurrence and relative probability of observed events that exceed a priori upper bound estimates, so-called "impossible" temperatures, has changed over time. We find that many unprecedented events are actually within data-driven upper bounds, but only when using modern spatial statistical methods. Furthermore, there are clear connections between anthropogenic forcing and the "impossibility" of the most extreme temperatures. Robust understanding of heatwave thresholds provides critical information about future record-breaking events and how their extremity relates to historical measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10251v2</guid>
      <category>physics.data-an</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mark D. Risser, Likun Zhang, Michael F. Wehner</dc:creator>
    </item>
    <item>
      <title>Estimation of conditional inequality measures</title>
      <link>https://arxiv.org/abs/2412.20228</link>
      <description>arXiv:2412.20228v2 Announce Type: replace-cross 
Abstract: Classical inequality measures such as the Gini index are often used to describe the sparsity of the distribution of a certain feature in a population. It is sometimes also used to compare the inequalities between some subpopulations, conditioned on certain values of the covariates. The concept of measuring inequality in subpopulation was described in the literature and it is strongly related to the decomposition of the Gini index. In this paper, the idea of conditional inequality measures is extended to the case where covariates are continuous. Curves of conditional inequality measures are introduced, especially, the curves of the conditional quantile versions of the Zenga and $D$ indices are considered. Various methods of their estimation based on quantile regression are presented. An approach using isotonic regression is used to prevent quantile crossing in quantile regression. The accuracy of the estimators considered is compared in simulation studies. Furthermore, an analysis of the growth in salary inequalities with respect to employee age is included to demonstrate the potential of conditional inequality measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20228v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alicja Jokiel-Rokita, Sylwester Pi\k{a}tek, Rafa{\l} Topolnicki</dc:creator>
    </item>
  </channel>
</rss>
