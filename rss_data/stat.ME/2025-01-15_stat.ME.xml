<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Jan 2025 02:28:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>ML-assisted Randomization Tests for Detecting Treatment Effects in A/B Experiments</title>
      <link>https://arxiv.org/abs/2501.07722</link>
      <description>arXiv:2501.07722v1 Announce Type: new 
Abstract: Experimentation is widely utilized for causal inference and data-driven decision-making across disciplines. In an A/B experiment, for example, an online business randomizes two different treatments (e.g., website designs) to their customers and then aims to infer which treatment is better. In this paper, we construct randomization tests for complex treatment effects, including heterogeneity and interference. A key feature of our approach is the use of flexible machine learning (ML) models, where the test statistic is defined as the difference between the cross-validation errors from two ML models, one including the treatment variable and the other without it. This approach combines the predictive power of modern ML tools with the finite-sample validity of randomization procedures, enabling a robust and efficient way to detect complex treatment effects in experimental settings. We demonstrate this combined benefit both theoretically and empirically through applied examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07722v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenxuan Guo, JungHo Lee, Panos Toulis</dc:creator>
    </item>
    <item>
      <title>Sampling from Density power divergence-based Generalized posterior distribution via Stochastic optimization</title>
      <link>https://arxiv.org/abs/2501.07790</link>
      <description>arXiv:2501.07790v1 Announce Type: new 
Abstract: Robust Bayesian inference using density power divergence (DPD) has emerged as a promising approach for handling outliers in statistical estimation. While the DPD-based posterior offers theoretical guarantees for robustness, its practical implementation faces significant computational challenges, particularly for general parametric models with intractable integral terms. These challenges become especially pronounced in high-dimensional settings where traditional numerical integration methods prove inadequate and computationally expensive. We propose a novel sampling methodology that addresses these limitations by integrating the loss-likelihood bootstrap with a stochastic gradient descent algorithm specifically designed for DPD-based estimation. Our approach enables efficient and scalable sampling from DPD-based posteriors for a broad class of parametric models, including those with intractable integrals, and we further extend it to accommodate generalized linear models. Through comprehensive simulation studies, we demonstrate that our method efficiently samples from DPD-based posteriors, offering superior computational scalability compared to conventional methods, particularly in high-dimensional settings. The results also highlight its ability to handle complex parametric models with intractable integral terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07790v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naruki Sonobe, Tomotaka Momozaki, Tomoyuki Nakagawa</dc:creator>
    </item>
    <item>
      <title>Prediction Inference Using Generalized Functional Mixed Effects Models</title>
      <link>https://arxiv.org/abs/2501.07842</link>
      <description>arXiv:2501.07842v1 Announce Type: new 
Abstract: We introduce inferential methods for prediction based on functional random effects in generalized functional mixed effects models. This is similar to the inference for random effects in generalized linear mixed effects models (GLMMs), but for functional instead of scalar outcomes. The method combines: (1) local GLMMs to extract initial estimators of the functional random components on the linear predictor scale; (2) structural functional principal components analysis (SFPCA) for dimension reduction; and (3) global Bayesian multilevel model conditional on the eigenfunctions for inference on the functional random effects. Extensive simulations demonstrate excellent coverage properties of credible intervals for the functional random effects in a variety of scenarios and for different data sizes. To our knowledge, this is the first time such simulations are conducted and reported, likely because prediction inference was not viewed as a priority and existing methods are too slow to calculate coverage. Methods are implemented in a reproducible R package and demonstrated using the NHANES 2011-2014 accelerometry data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07842v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xinkai Zhou, Erjia Cui, Joseph Sartini, Ciprian Crainiceanu</dc:creator>
    </item>
    <item>
      <title>One cut-point phase-type distributions in Reliability. An application to Resistive Random Access Memories</title>
      <link>https://arxiv.org/abs/2501.07949</link>
      <description>arXiv:2501.07949v1 Announce Type: new 
Abstract: A new probability distribution to study lifetime data in reliability is introduced in this paper. This one is a first approach to a non-homogeneous phase-type distribution. It is built by considering one cut-point in the non-negative semi-line of a phase-type distribution. The density function is defined and the main measures associated, such as the reliability function, hazard rate, cumulative hazard rate and the characteristic function are also worked out. This new class of distributions enables to decrease the number of parameter in the estimate when inference is considered. Besides, the likelihood distribution is built to estimate the model parameters by maximum likelihood. Several applications by considering Resistive Random Access Memories compare the adjustment when phase type distributions and one cut-point phase-type distributions are considered. The developed methodology has been computationally implemented in R-cran.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07949v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3390/math9212734</arxiv:DOI>
      <arxiv:journal_reference>Mathematics 2021, 9(21), 2734</arxiv:journal_reference>
      <dc:creator>Christian Acal, Juan Eloy Ruiz-Castro, David Maldonado, Juan B. Rold\'an</dc:creator>
    </item>
    <item>
      <title>MMAPs to model complex multi-state systems with vacation policies in the repair facility</title>
      <link>https://arxiv.org/abs/2501.07995</link>
      <description>arXiv:2501.07995v1 Announce Type: new 
Abstract: Two complex multi-state systems subject to multiple events are built in an algorithmic and computational way by considering phase-type distributions and Markovian arrival processes with marked arrivals. The internal performance of the system is composed of different degradation levels and internal repairable and non-repairable failures can occur. Also, the system is subject to external shocks that may provoke repairable or non-repairable failure. A multiple vacation policy is introduced in the system for the repairperson. Preventive maintenance is included in the system to improve the behaviour. Two types of task may be performed by the repairperson; corrective repair and preventive maintenance. The systems are modelled, the transient and stationary distributions are built and different performance measures are calculated in a matrix-algorithmic form. Cost and rewards are included in the model in a vector matrix way. Several economic measures are worked out and the net reward per unit of time is used to optimize the system. A numerical example shows that the system can be optimized according to the existence of preventive maintenance and the distribution of vacation time. The results have been implemented computationally with Matlab and R (packages: expm, optim).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07995v1</guid>
      <category>stat.ME</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.13052/jrss0974-8024.1524</arxiv:DOI>
      <arxiv:journal_reference>Journal of Reliability and Statistical Studies, 2022, 15, 2, 473-504</arxiv:journal_reference>
      <dc:creator>Juan Eloy Ruiz-Castro, Christian Acal</dc:creator>
    </item>
    <item>
      <title>Semiparametric Skew-Elliptical Distributions For High-Dimensional Graphical Models</title>
      <link>https://arxiv.org/abs/2501.08033</link>
      <description>arXiv:2501.08033v1 Announce Type: new 
Abstract: We propose a semiparametric approach called elliptical skew-(S)KEPTIC for efficiently and robustly estimating non-Gaussian graphical models. Relaxing the assumption of semiparametric elliptical distributions to the family of \textit{meta skew-elliptical} that accommodates a skewness component, we derive a new estimator which is an extension of the SKEPTIC estimator in Liu et al. (2012), based on semiparametric Gaussian copula graphical models, to the case of skew-elliptical copula graphical models. Theoretically, we demonstrate that the elliptical skew-(S)KEPTIC estimator achieves robust parametric convergence rates in both graph recovery and parameters estimation. We conduct numerical simulations to prove the reliable graph recovery performance of the elliptical skew-(S)KEPTIC estimator. Finally, the new method is applied to the daily log-returns of the stocks of the S\&amp;P500 index and shows better interpretability compared to the Gaussian copula graphical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08033v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele Di Luzio, Giacomo Morelli</dc:creator>
    </item>
    <item>
      <title>A note on local parameter orthogonality for multivariate data and the Whittle algorithm for multivariate autoregressive models</title>
      <link>https://arxiv.org/abs/2501.08093</link>
      <description>arXiv:2501.08093v1 Announce Type: new 
Abstract: This article extends the Cox-Reid local parameter orthogonality to a multivariate setting and shows that the extension can lead to the Whittle algorithm for multivariate autoregressive models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08093v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changle Shen, Dong Li, Howell Tong</dc:creator>
    </item>
    <item>
      <title>Estimation of survival functions for events based on a continuous outcome: a distributional approach</title>
      <link>https://arxiv.org/abs/2501.08228</link>
      <description>arXiv:2501.08228v1 Announce Type: new 
Abstract: The limitations resulting from the dichtomisation of continuous outcomes have been extensively described. But the need to present results based on binary outcomes in particular in health science remains. Alternatives based on the distribution of the continuous outcome have been proposed. Here we explore the possibilities of using a distributional approach in the context of time-to-event analysis when the event is defined by values of a continuous outcome above or below a threshold. For this we propose in a first step a distributional version of the Kaplan-Meier estimate of the survival function based on repeated truncation of the normal distribution. The method is evaluated with a simulation study and illustrated with a case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08228v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Odile Sauzet</dc:creator>
    </item>
    <item>
      <title>Bayesian Shrinkage Priors for Penalized Synthetic Control Estimators in the Presence of Spillovers</title>
      <link>https://arxiv.org/abs/2501.08231</link>
      <description>arXiv:2501.08231v1 Announce Type: new 
Abstract: Synthetic control (SC) methods are widely used to evaluate the impact of policy interventions, particularly those targeting specific geographic areas or regions, commonly referred to as units. These methods construct an artificial (synthetic) unit from untreated (control) units, intended to mirror the characteristics of the treated region had the intervention not occurred. While neighboring areas are often chosen as controls due to their assumed similarities with the treated, their proximity can introduce spillovers, where the intervention indirectly affects these controls, biasing the estimates. To address this challenge, we propose a Bayesian SC method with distance-based shrinkage priors, designed to estimate causal effects while accounting for spillovers. Modifying traditional penalization techniques, our approach incorporates a weighted distance function that considers both covariate information and spatial proximity to the treated. Rather than simply excluding nearby controls, this framework data-adaptively selects those less likely to be impacted by spillovers, providing a balance between bias and variance reduction. Through simulation studies, we demonstrate the finite-sample properties of our method under varying levels of spillover. We then apply this approach to evaluate the impact of Philadelphia's beverage tax on the sales of sugar-sweetened and artificially sweetened beverages in mass merchandise stores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08231v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Esteban Fern\'andez-Morales, Arman Oganisian, Youjin Lee</dc:creator>
    </item>
    <item>
      <title>Constructing optimal dynamic monitoring and treatment regimes: An application to hypertension care</title>
      <link>https://arxiv.org/abs/2501.08274</link>
      <description>arXiv:2501.08274v1 Announce Type: new 
Abstract: Hypertension is a leading cause of cardiovascular diseases and morbidity, with antihypertensive drugs and blood pressure management strategies having heterogeneous effects on patients. Previous authors exploited this heterogeneity to construct optimal dynamic treatment regimes for hypertension that input patient characteristics and output the best drug or blood pressure management strategy to prescribe. There is, however, a lack of research on optimizing monitoring schedules for these patients. It is unclear whether different monitoring patterns and drug add-on strategies could lower blood pressure differently across patients. We propose a new consistent methodology to develop optimal dynamic monitoring and add-on regimes that is doubly-robust and relies on the theory of Robins' g-methods and dynamic weighted ordinary least squares. We discuss the treatment of longitudinal missing data for that inference. The approach is evaluated in large simulation studies and applied to data from the SPRINT trial in the United States to derive a new optimal rule. This type of rule could be used by patients or physicians to personalize the timing of visit and by physicians to decide whether prescribing an antihypertensive drug is beneficial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08274v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Janie Coulombe, Dany El-Riachi, Fanxing Du, Tianze Jiao</dc:creator>
    </item>
    <item>
      <title>Bridging Root-$n$ and Non-standard Asymptotics: Dimension-agnostic Adaptive Inference in M-Estimation</title>
      <link>https://arxiv.org/abs/2501.07772</link>
      <description>arXiv:2501.07772v1 Announce Type: cross 
Abstract: This manuscript studies a general approach to construct confidence sets for the solution of population-level optimization, commonly referred to as M-estimation. Statistical inference for M-estimation poses significant challenges due to the non-standard limiting behaviors of the corresponding estimator, which arise in settings with increasing dimension of parameters, non-smooth objectives, or constraints. We propose a simple and unified method that guarantees validity in both regular and irregular cases. Moreover, we provide a comprehensive width analysis of the proposed confidence set, showing that the convergence rate of the diameter is adaptive to the unknown degree of instance-specific regularity. We apply the proposed method to several high-dimensional and irregular statistical problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07772v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenta Takatsu, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Methods to Calculate the Upper Bound of Gini Coefficient Based on Grouped Data and the Result for China</title>
      <link>https://arxiv.org/abs/1305.4896</link>
      <description>arXiv:1305.4896v3 Announce Type: replace 
Abstract: Determining an upper bound, particularly the optimal upper bound of the Gini coefficient when dealing with grouped data without specified income brackets, remains an important and open question. In this paper, we introduce an efficient algorithm to calculate the exact optimal upper bound of the Gini coefficient with provable guarantees. To exemplify these methods, we also offer computed results for the Gini coefficients of urban and rural China spanning the years 2003 to 2008.</description>
      <guid isPermaLink="false">oai:arXiv.org:1305.4896v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pixu Shi, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis for Binary Outcome Misclassification in Randomization Tests via Integer Programming</title>
      <link>https://arxiv.org/abs/2201.03111</link>
      <description>arXiv:2201.03111v3 Announce Type: replace 
Abstract: Conducting a randomization test is a common method for testing causal null hypotheses in randomized experiments. The popularity of randomization tests is largely because their statistical validity only depends on the randomization design, and no distributional or modeling assumption on the outcome variable is needed. However, randomization tests may still suffer from other sources of bias, among which outcome misclassification is a significant one. We propose a model-free and finite-population sensitivity analysis approach for binary outcome misclassification in randomization tests. A central quantity in our framework is ``warning accuracy," defined as the threshold such that a randomization test result based on the measured outcomes may differ from that based on the true outcomes if the outcome measurement accuracy did not surpass that threshold. We show how learning the warning accuracy and related concepts can amplify analyses of randomization tests subject to outcome misclassification without adding additional assumptions. We show that the warning accuracy can be computed efficiently for large data sets by adaptively reformulating a large-scale integer program with respect to the randomization design. We apply the proposed approach to the Prostate Cancer Prevention Trial (PCPT). We also developed an open-source R package for implementation of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.03111v3</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyu Heng, Pamela A. Shaw</dc:creator>
    </item>
    <item>
      <title>An Efficient Sequential Approach for k-Parametric Dynamic Generalised Linear Models</title>
      <link>https://arxiv.org/abs/2201.05387</link>
      <description>arXiv:2201.05387v4 Announce Type: replace 
Abstract: A novel sequential inferential method for Bayesian dynamic generalised linear models is presented, addressing both univariate and multivariate $k$-parametric exponential families. It efficiently handles diverse responses, including multinomial, gamma, normal, and Poisson distributed outcomes, by leveraging the conjugate and predictive structure of the exponential family. The approach integrates information geometry concepts, such as the projection theorem and Kullback-Leibler divergence, and aligns with recent advances in variational inference. Applications to both synthetic and real datasets highlight its computational efficiency and scalability, surpassing alternative methods. The approach supports the strategic integration of new information, facilitating monitoring, intervention, and the application of discount factors, which are typical in sequential analyses. The R package kDGLM is available for direct use by applied researchers, facilitating the implementation of the method for specific k-parametric dynamic generalised models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.05387v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mariane Branco Alves, Helio S. Migon, Silvaneo V. Santos Jr, Ra\'ira Marotta</dc:creator>
    </item>
    <item>
      <title>Multinomial Link Models</title>
      <link>https://arxiv.org/abs/2312.16260</link>
      <description>arXiv:2312.16260v3 Announce Type: replace 
Abstract: We propose a new class of regression models for analyzing categorical responses, called multinomial link models. It consists of four subclasses, including mixed-link models that generalize existing multinomial logistic models and their extensions, two-group models that can incorporate the observations with NA or unknown responses, multinomial conditional link models that handle longitudinal categorical responses, and po-npo mixture models that extend partial proportional odds models. We provide explicit formulae and detailed algorithms for finding the maximum likelihood estimates of the model parameters and computing the Fisher information matrix. Our algorithms solve the infeasibility issue of existing statistical software when estimating parameters of cumulative link models. The applications to real datasets show that the new models can fit the data significantly better, correct misleading conclusions due to missing responses, and make more informative inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16260v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianmeng Wang, Liping Tong, Jie Yang</dc:creator>
    </item>
    <item>
      <title>Degree-heterogeneous Latent Class Analysis for High-dimensional Discrete Data</title>
      <link>https://arxiv.org/abs/2402.18745</link>
      <description>arXiv:2402.18745v4 Announce Type: replace 
Abstract: The latent class model is a widely used mixture model for multivariate discrete data. Besides the existence of qualitatively heterogeneous latent classes, real data often exhibit additional quantitative heterogeneity nested within each latent class. The modern latent class analysis also faces extra challenges, including the high-dimensionality, sparsity, and heteroskedastic noise inherent in discrete data. Motivated by these phenomena, we introduce the Degree-heterogeneous Latent Class Model and propose an easy-to-implement HeteroClustering algorithm for it. HeteroClustering uses heteroskedastic PCA with $\ell_2$ normalization to remove degree effects and perform clustering in the top singular subspace of the data matrix. We establish the result of exact clustering under minimal signal-to-noise conditions. We further investigate the estimation and inference of the high-dimensional continuous item parameters in the model, which are crucial to interpreting and finding useful markers for latent classes. We provide comprehensive procedures for global testing and multiple testing of these parameters with valid error controls. The superior performance of our methods is demonstrated through extensive simulations and applications to three diverse real-world datasets from political voting records, genetic variations, and single-cell sequencing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18745v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongyuan Lyu, Ling Chen, Yuqi Gu</dc:creator>
    </item>
    <item>
      <title>Continuous-time mediation analysis for repeatedly measured mediators and outcomes</title>
      <link>https://arxiv.org/abs/2403.11017</link>
      <description>arXiv:2403.11017v3 Announce Type: replace 
Abstract: Mediation analysis aims to decipher the underlying causal mechanisms between an exposure, an outcome, and intermediate variables called mediators. Initially developed for fixed-time mediator and outcome, it has been extended to the framework of longitudinal data by discretizing the assessment times of mediator and outcome. Yet, processes in play in longitudinal studies are usually defined in continuous time and measured at irregular and subject-specific visits. This is the case in dementia research when cerebral and cognitive changes measured at planned visits in cohorts are of interest. We thus propose a methodology to estimate the causal mechanisms between a time-fixed exposure ($X$), a mediator process ($\mathcal{M}_t$) and an outcome process ($\mathcal{Y}_t$) both measured repeatedly over time in the presence of a time-dependent confounding process ($\mathcal{L}_t$). We consider three types of causal estimands, the natural effects, path-specific effects and randomized interventional analogues to natural effects, and provide identifiability assumptions. We employ a dynamic multivariate model based on differential equations for their estimation. The performance of the methods are explored in simulations, and we illustrate the method in two real-world examples motivated by the 3C cerebral aging study to assess: (1) the effect of educational level on functional dependency through depressive symptomatology and cognitive functioning, and (2) the effect of a genetic factor on cognitive functioning potentially mediated by vascular brain lesions and confounded by neurodegeneration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11017v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>K. Le Bourdonnec, L. Valeri, C. Proust-Lima</dc:creator>
    </item>
    <item>
      <title>Bind Recovery of Sparse Factor Structures by Signal Cancellation</title>
      <link>https://arxiv.org/abs/2404.03781</link>
      <description>arXiv:2404.03781v2 Announce Type: replace 
Abstract: Blind factor recovery follows from the principle that the signal of variables exclusive to a factor can be combined in a contrast (weighted sum) that cancels their factor contributions, leaving only a compound of the variables unique variances. Successful contrasts, uncorrelated with any remaining variable, become the signature of factors with at least two unique indicator variables. Pairwise signal cancellation, usually incomplete for variables affected by different factors, nevertheless succeeds for variables with proportional loadings on two factors, which places three cancelling clusters in the plane of two factors. This is recognized by successful cancellation among variable triplets representing the three clusters. The Signal Cancellation Recovery of Factors (SCRoF) algorithm implements these principles, only requiring that each factor has at least two unique indicators, not even requiring having pre-estimated the number of factors. Alternate sparse factor solutions are obtained through a two significance-threshold strategy. The individually estimated factor loadings and factor correlations of each potential solution are globally optimized for maximum likelihood, yielding a chi-square indication of compatibility with observed data. SCRoF is illustrated with synthetic data from a complex six-factor structure. Actual data then document that SCRoF can even benefit confirmatory factor analysis when the initial model appears inadequate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03781v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'e Achim</dc:creator>
    </item>
    <item>
      <title>Statistical inference of convex order by Wasserstein projection</title>
      <link>https://arxiv.org/abs/2406.02840</link>
      <description>arXiv:2406.02840v3 Announce Type: replace 
Abstract: Ranking distributions according to a stochastic order has wide applications in diverse areas. Although stochastic dominance has received much attention, convex order, particularly in general dimensions, has yet to be investigated from a statistical point of view. This article addresses this gap by introducing a simple statistical test for convex order based on the Wasserstein projection distance. This projection distance not only encodes whether two distributions are indeed in convex order, but also quantifies the deviation from the desired convex order and produces an optimal convex order approximation. Lipschitz stability of the backward and forward Wasserstein projection distance is proved, which leads to elegant consistency and concentration results of the estimator we employ as our test statistic. Combining these with state of the art results regarding the convergence rate of empirical distributions, we also derive upper bounds for the $p$-value and type I error of our test statistic, as well as upper bounds on the type II error for an appropriate class of strict alternatives. With proper choices of families of distributions, we further attain that the power of the proposed test increases to one as the number of samples grows to infinity. Lastly, we provide an efficient numerical scheme for our test statistic, by way of an entropic Frank-Wolfe algorithm. Experiments based on synthetic data sets illuminate the success of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02840v3</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakwang Kim, Young-Heon Kim, Yuanlong Ruan, Andrew Warren</dc:creator>
    </item>
    <item>
      <title>Factorial Difference-in-Differences</title>
      <link>https://arxiv.org/abs/2407.11937</link>
      <description>arXiv:2407.11937v3 Announce Type: replace 
Abstract: In many panel data settings, researchers apply the difference-in-differences (DID) estimator, exploiting cross-sectional variation in a baseline factor and temporal variation in exposure to an event affecting all units. However, the exact estimand is often unspecified and the justification for this method remains unclear. This paper formalizes this empirical approach, which we term factorial DID (FDID), as a research design including its data structure, estimands, and identifying assumptions. We frame it as a factorial design with two factors - the baseline factor G and exposure level Z - and define effect modification and causal moderation as the associative and causal effects of G on the effect of Z, respectively. We show that under standard assumptions, including no anticipation and parallel trends, the DID estimator identifies effect modification but not causal moderation. To identify the latter, we propose an additional factorial parallel trends assumption. Moreover, we reconcile canonical DID as a special case of FDID with an additional exclusion restriction and link causal moderation to G's conditional effect with another exclusion restriction. We extend our framework to conditionally valid assumptions, clarify regression-based approaches, and illustrate our findings with an empirical example. We offer practical recommendations for FDID applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11937v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yiqing Xu, Anqi Zhao, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Hierarchical Latent Class Models for Mortality Surveillance Using Partially Verified Verbal Autopsies</title>
      <link>https://arxiv.org/abs/2410.09274</link>
      <description>arXiv:2410.09274v2 Announce Type: replace 
Abstract: Monitoring data on causes of death is an important part of understanding the burden of diseases and the effects of public health interventions. Verbal autopsy (VA) is a well-established method for gathering information about deaths outside of hospitals by conducting an interview to family members or caregivers of a deceased person, and is usually the only tool for cause-of-death surveillance in low-resource settings. A critical limitation with the current practice of VA analysis is that all algorithms require either highly informative domain knowledge about symptom-cause relationships or large labeled datasets for model training. Therefore, they cannot be quickly adopted during public health emergencies when new diseases emerge with rapidly evolving epidemiological patterns. In this paper, we consider the task of estimating the fraction of deaths due to an emerging disease using continuously collected VAs where causes of death are only partially verified. We develop a novel Bayesian framework using a hierarchical latent class model to account for the informative verification process. Our model flexibly captures the joint distribution of symptoms and how they change over time in different sub-populations. We also propose structured priors to further improve the precision of prevalence estimation for small sub-populations. Our model is motivated by mortality surveillance of COVID-19 related deaths in low-resource settings. We apply our method to a dataset that includes suspected COVID-19 related deaths in Brazil in 2021. We show that standard modeling approaches can be severely biased under selective verification and our model leads to more robust and accurate quantification of disease prevalence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09274v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Zhu, Zehang Richard Li</dc:creator>
    </item>
    <item>
      <title>Multiple Imputation for Nonresponse in Complex Surveys Using Design Weights and Auxiliary Margins</title>
      <link>https://arxiv.org/abs/2412.10988</link>
      <description>arXiv:2412.10988v3 Announce Type: replace 
Abstract: Survey data typically have missing values due to unit and item nonresponse. Sometimes, survey organizations know the marginal distributions of certain categorical variables in the survey. As shown in previous work, survey organizations can leverage these distributions in multiple imputation for nonignorable unit nonresponse, generating imputations that result in plausible completed-data estimates for the variables with known margins. However, this prior work does not use the design weights for unit nonrespondents; rather, it relies on a set of fabricated weights for these units. We extend this previous work to utilize the design weights for all sampled units. We illustrate the approach using simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10988v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kewei Xu, Jerome P. Reiter</dc:creator>
    </item>
    <item>
      <title>Optimal Sampling for Generalized Linear Model under Measurement Constraint with Surrogate Variables</title>
      <link>https://arxiv.org/abs/2501.00972</link>
      <description>arXiv:2501.00972v2 Announce Type: replace 
Abstract: Measurement-constrained datasets, often encountered in semi-supervised learning, arise when data labeling is costly, time-intensive, or hindered by confidentiality or ethical concerns, resulting in a scarcity of labeled data. In certain cases, surrogate variables are accessible across the entire dataset and can serve as approximations to the true response variable; however, these surrogates often contain measurement errors and thus cannot be directly used for accurate prediction. We propose an optimal sampling strategy that effectively harnesses the available information from surrogate variables. This approach provides consistent estimators under the assumption of a generalized linear model, achieving theoretically lower asymptotic variance than existing optimal sampling algorithms that do not use surrogate data information. By employing the A-optimality criterion from optimal experimental design, our strategy maximizes statistical efficiency. Numerical studies demonstrate that our approach surpasses existing optimal sampling methods, exhibiting reduced empirical mean squared error and enhanced robustness in algorithmic performance. These findings highlight the practical advantages of our strategy in scenarios where measurement constraints exist and surrogates are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00972v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yixin Shen, Yang Ning</dc:creator>
    </item>
    <item>
      <title>The ladder of abstraction in statistical graphics</title>
      <link>https://arxiv.org/abs/2501.06920</link>
      <description>arXiv:2501.06920v2 Announce Type: replace 
Abstract: Graphical forms such as scatterplots, line plots, and histograms are so familiar that it can be easy to forget how abstract they are. As a result, we often produce graphs that are difficult to follow. We propose a strategy for graphical communication by climbing a ladder of abstraction (a term from linguistics that we borrow from Hayakawa, 1939), starting with simple plots of special cases and then at each step embedding a graph into a more general framework. We demonstrate with two examples, first graphing a set of equations related to a modeled trajectory and then graphing data from an analysis of income and voting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06920v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Gelman</dc:creator>
    </item>
    <item>
      <title>Monotone Curve Estimation via Convex Duality</title>
      <link>https://arxiv.org/abs/2501.06975</link>
      <description>arXiv:2501.06975v2 Announce Type: replace 
Abstract: A principal curve serves as a powerful tool for uncovering underlying structures of data through 1-dimensional smooth and continuous representations. On the basis of optimal transport theories, this paper introduces a novel principal curve framework constrained by monotonicity with rigorous theoretical justifications. We establish statistical guarantees for our monotone curve estimate, including expected empirical and generalized mean squared errors, while proving the existence of such estimates. These statistical foundations justify adopting the popular early stopping procedure in machine learning to implement our numeric algorithm with neural networks. Comprehensive simulation studies reveal that the proposed monotone curve estimate outperforms competing methods in terms of accuracy when the data exhibits a monotonic structure. Moreover, through two real-world applications on future prices of copper, gold, and silver, and avocado prices and sales volume, we underline the robustness of our curve estimate against variable transformation, further confirming its effective applicability for noisy and complex data sets. We believe that this monotone curve-fitting framework offers significant potential for numerous applications where monotonic relationships are intrinsic or need to be imposed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06975v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tongseok Lim, Kyeongsik Nam, Jinwon Sohn</dc:creator>
    </item>
    <item>
      <title>Optimal Scaling for the Proximal Langevin Algorithm in High Dimensions</title>
      <link>https://arxiv.org/abs/2204.10793</link>
      <description>arXiv:2204.10793v2 Announce Type: replace-cross 
Abstract: The Metropolis-adjusted Langevin (MALA) algorithm is a sampling algorithm that incorporates the gradient of the logarithm of the target density in its proposal distribution. In an earlier joint work \citet{pill:stu:12}, the author had extended the seminal work of \cite{Robe:Rose:98} and showed that in stationarity, MALA applied to an $N-$dimensional approximation of the target will take ${\cal O}(N^{\frac13})$ steps to explore its target measure. It was also shown that the MALA algorithm is optimized at an average acceptance probability of $0.574$. In \citet{pere:16}, the author introduced the proximal MALA algorithm where the gradient of the log target density is replaced by the proximal function. In this paper, we show that for a wide class of twice differentiable target densities, the proximal MALA enjoys the same optimal scaling as that of MALA in high dimensions and also has an average optimal acceptance probability of $0.574$. The results of this paper thus give the following practically useful guideline: for smooth target densities where it is expensive to compute the gradient while implementing MALA, users may replace the gradient with the corresponding proximal function (that can be often computed relatively cheaply via convex optimization) \emph{without} losing any efficiency gains from optimal scaling. This confirms some of the empirical observations made in \cite{pere:16}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10793v2</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research, 2025</arxiv:journal_reference>
      <dc:creator>Natesh S. Pillai</dc:creator>
    </item>
    <item>
      <title>Simple Estimation of Semiparametric Models with Measurement Errors</title>
      <link>https://arxiv.org/abs/2306.14311</link>
      <description>arXiv:2306.14311v3 Announce Type: replace-cross 
Abstract: We develop a practical way of addressing the Errors-In-Variables (EIV) problem in the Generalized Method of Moments (GMM) framework. We focus on the settings in which the variability of the EIV is a fraction of that of the mismeasured variables, which is typical for empirical applications. For any initial set of moment conditions our approach provides a "corrected" set of moment conditions that are robust to the EIV. We show that the GMM estimator based on these moments is root-n-consistent, with the standard tests and confidence intervals providing valid inference. This is true even when the EIV are so large that naive estimators (that ignore the EIV problem) are heavily biased with their confidence intervals having 0% coverage. Our approach involves no nonparametric estimation, which is especially important for applications with many covariates, and settings with multivariate or non-classical EIV. In particular, the approach makes it easy to use instrumental variables to address EIV in nonlinear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14311v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kirill S. Evdokimov, Andrei Zeleneev</dc:creator>
    </item>
    <item>
      <title>Consistency Theory of General Nonparametric Classification Methods in Cognitive Diagnosis</title>
      <link>https://arxiv.org/abs/2312.11437</link>
      <description>arXiv:2312.11437v3 Announce Type: replace-cross 
Abstract: Cognitive diagnosis models have been popularly used in fields such as education, psychology, and social sciences. While parametric likelihood estimation is a prevailing method for fitting cognitive diagnosis models, nonparametric methodologies are attracting increasing attention due to their ease of implementation and robustness, particularly when sample sizes are relatively small. However, existing clustering consistency results of the nonparametric estimation methods often rely on certain restrictive conditions, which may not be easily satisfied in practice. In this article, the clustering consistency of the general nonparametric classification method is reestablished under weaker and more practical conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11437v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengyu Cui, Yanlong Liu, Gongjun Xu</dc:creator>
    </item>
    <item>
      <title>A Closer Look at AUROC and AUPRC under Class Imbalance</title>
      <link>https://arxiv.org/abs/2401.06091</link>
      <description>arXiv:2401.06091v4 Announce Type: replace-cross 
Abstract: In machine learning (ML), a widespread claim is that the area under the precision-recall curve (AUPRC) is a superior metric for model comparison to the area under the receiver operating characteristic (AUROC) for tasks with class imbalance. This paper refutes this notion on two fronts. First, we theoretically characterize the behavior of AUROC and AUPRC in the presence of model mistakes, establishing clearly that AUPRC is not generally superior in cases of class imbalance. We further show that AUPRC can be a harmful metric as it can unduly favor model improvements in subpopulations with more frequent positive labels, heightening algorithmic disparities. Next, we empirically support our theory using experiments on both semi-synthetic and real-world fairness datasets. Prompted by these insights, we conduct a review of over 1.5 million scientific papers to understand the origin of this invalid claim, finding that it is often made without citation, misattributed to papers that do not argue this point, and aggressively over-generalized from source arguments. Our findings represent a dual contribution: a significant technical advancement in understanding the relationship between AUROC and AUPRC and a stark warning about unchecked assumptions in the ML community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06091v4</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew B. A. McDermott (Harvard Medical School), Haoran Zhang (Massachusetts Institute of Technology), Lasse Hyldig Hansen (Aarhus University), Giovanni Angelotti (IRCCS Humanitas Research Hospital), Jack Gallifant (Massachusetts Institute of Technology)</dc:creator>
    </item>
    <item>
      <title>Optimal estimation of the null distribution in large-scale inference</title>
      <link>https://arxiv.org/abs/2401.06350</link>
      <description>arXiv:2401.06350v2 Announce Type: replace-cross 
Abstract: The advent of large-scale inference has spurred reexamination of conventional statistical thinking. In a Gaussian model for $n$ many $z$-scores with at most $k &lt; \frac{n}{2}$ nonnulls, Efron suggests estimating the location and scale parameters of the null distribution. Placing no assumptions on the nonnull effects, the statistical task can be viewed as a robust estimation problem. However, the best known robust estimators fail to be consistent in the regime $k \asymp n$ which is especially relevant in large-scale inference. The failure of estimators which are minimax rate-optimal with respect to other formulations of robustness (e.g. Huber's contamination model) might suggest the impossibility of consistent estimation in this regime and, consequently, a major weakness of Efron's suggestion. A sound evaluation of Efron's model thus requires a complete understanding of consistency. We sharply characterize the regime of $k$ for which consistent estimation is possible and further establish the minimax estimation rates. It is shown consistent estimation of the location parameter is possible if and only if $\frac{n}{2} - k = \omega(\sqrt{n})$, and consistent estimation of the scale parameter is possible in the entire regime $k &lt; \frac{n}{2}$. Faster rates than those in Huber's contamination model are achievable by exploiting the Gaussian character of the data. The minimax upper bound is obtained by considering estimators based on the empirical characteristic function. The minimax lower bound involves constructing two marginal distributions whose characteristic functions match on a wide interval containing zero. The construction notably differs from those in the literature by sharply capturing a scaling of $n-2k$ in the minimax estimation rate of the location.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06350v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhodh Kotekal, Chao Gao</dc:creator>
    </item>
    <item>
      <title>Regression Adjustment for Estimating Distributional Treatment Effects in Randomized Controlled Trials</title>
      <link>https://arxiv.org/abs/2407.14074</link>
      <description>arXiv:2407.14074v2 Announce Type: replace-cross 
Abstract: In this paper, we address the issue of estimating and inferring distributional treatment effects in randomized experiments. The distributional treatment effect provides a more comprehensive understanding of treatment heterogeneity compared to average treatment effects. We propose a regression adjustment method that utilizes distributional regression and pre-treatment information, establishing theoretical efficiency gains without imposing restrictive distributional assumptions. We develop a practical inferential framework and demonstrate its advantages through extensive simulations. Analyzing water conservation policies, our method reveals that behavioral nudges systematically shift consumption from high to moderate levels. Examining health insurance coverage, we show the treatment reduces the probability of zero doctor visits by 6.6 percentage points while increasing the likelihood of 3-6 visits. In both applications, our regression adjustment method substantially improves precision and identifies treatment effects that were statistically insignificant under conventional approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14074v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tatsushi Oka, Shota Yasui, Yuta Hayakawa, Undral Byambadalai</dc:creator>
    </item>
    <item>
      <title>Distribution-free uncertainty quantification for inverse problems: application to weak lensing mass mapping</title>
      <link>https://arxiv.org/abs/2410.08831</link>
      <description>arXiv:2410.08831v2 Announce Type: replace-cross 
Abstract: In inverse problems, distribution-free uncertainty quantification (UQ) aims to obtain error bars with coverage guarantees that are independent of any prior assumptions about the data distribution. In the context of mass mapping, uncertainties could lead to errors that affects our understanding of the underlying mass distribution, or could propagate to cosmological parameter estimation, thereby impacting the precision and reliability of cosmological models. Current surveys, such as Euclid or Rubin, will provide new weak lensing datasets of very high quality. Accurately quantifying uncertainties in mass maps is therefore critical to perform reliable cosmological parameter inference. In this paper, we extend the conformalized quantile regression (CQR) algorithm, initially proposed for scalar regression, to inverse problems. We compare our approach with another distribution-free approach based on risk-controlling prediction sets (RCPS). Both methods are based on a calibration dataset, and offer finite-sample coverage guarantees that are independent of the data distribution. Furthermore, they are applicable to any mass mapping method, including blackbox predictors. In our experiments, we apply UQ on three mass-mapping method: the Kaiser-Squires inversion, iterative Wiener filtering, and the MCALens algorithm. Our experiments reveal that RCPS tends to produce overconservative confidence bounds with small calibration sets, whereas CQR is designed to avoid this issue. Although the expected miscoverage rate is guaranteed to stay below a user-prescribed threshold regardless of the mass mapping method, selecting an appropriate reconstruction algorithm remains crucial for obtaining accurate estimates, especially around peak-like structures, which are particularly important for inferring cosmological parameters. Additionally, the choice of mass mapping method influences the size of the error bars.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08831v2</guid>
      <category>astro-ph.CO</category>
      <category>astro-ph.IM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hubert Leterme, Jalal Fadili, Jean-Luc Starck</dc:creator>
    </item>
    <item>
      <title>Multiple testing in multi-stream sequential change detection</title>
      <link>https://arxiv.org/abs/2501.04130</link>
      <description>arXiv:2501.04130v2 Announce Type: replace-cross 
Abstract: Multi-stream sequential change detection involves simultaneously monitoring many streams of data and trying to detect when their distributions change, if at all. Here, we theoretically study multiple testing issues that arise from detecting changes in many streams. We point out that any algorithm with finite average run length (ARL) must have a trivial worst-case false detection rate (FDR), family-wise error rate (FWER), and per-family error rate (PFER); thus, any attempt to control these Type I error metrics is fundamentally in conflict with the desire for a finite ARL (which is typically necessary in order to have a small detection delay). One of our contributions is to define a new class of metrics which can be controlled, called error over patience (EOP). We propose algorithms that combine the recent e-detector framework (which generalizes the Shiryaev-Roberts and CUSUM methods) with the recent e-Benjamini-Hochberg procedure and e-Bonferroni procedures. We prove that these algorithms control the EOP at any desired level under very general dependence structures on the data within and across the streams. In fact, we prove a more general error control that holds uniformly over all stopping times and provides a smooth trade-off between the conflicting metrics. Additionally, if finiteness of the ARL is forfeited, we show that our algorithms control the Type I error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04130v2</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Counterfactually Fair Reinforcement Learning via Sequential Data Preprocessing</title>
      <link>https://arxiv.org/abs/2501.06366</link>
      <description>arXiv:2501.06366v2 Announce Type: replace-cross 
Abstract: When applied in healthcare, reinforcement learning (RL) seeks to dynamically match the right interventions to subjects to maximize population benefit. However, the learned policy may disproportionately allocate efficacious actions to one subpopulation, creating or exacerbating disparities in other socioeconomically-disadvantaged subgroups. These biases tend to occur in multi-stage decision making and can be self-perpetuating, which if unaccounted for could cause serious unintended consequences that limit access to care or treatment benefit. Counterfactual fairness (CF) offers a promising statistical tool grounded in causal inference to formulate and study fairness. In this paper, we propose a general framework for fair sequential decision making. We theoretically characterize the optimal CF policy and prove its stationarity, which greatly simplifies the search for optimal CF policies by leveraging existing RL algorithms. The theory also motivates a sequential data preprocessing algorithm to achieve CF decision making under an additive noise assumption. We prove and then validate our policy learning approach in controlling unfairness and attaining optimal value through simulations. Analysis of a digital health dataset designed to reduce opioid misuse shows that our proposal greatly enhances fair access to counseling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06366v2</guid>
      <category>stat.ML</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 15 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jitao Wang, Chengchun Shi, John D. Piette, Joshua R. Loftus, Donglin Zeng, Zhenke Wu</dc:creator>
    </item>
  </channel>
</rss>
