<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Apr 2025 01:49:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bayesian analysis of regression discontinuity designs with heterogeneous treatment effects</title>
      <link>https://arxiv.org/abs/2504.10652</link>
      <description>arXiv:2504.10652v1 Announce Type: new 
Abstract: Regression Discontinuity Design (RDD) is a popular framework for estimating a causal effect in settings where treatment is assigned if an observed covariate exceeds a fixed threshold. We consider estimation and inference in the common setting where the sample consists of multiple known sub-populations with potentially heterogeneous treatment effects. In the applied literature, it is common to account for heterogeneity by either fitting a parametric model or considering each sub-population separately. In contrast, we develop a Bayesian hierarchical model using Gaussian process regression which allows for non-parametric regression while borrowing information across sub-populations. We derive the posterior distribution, prove posterior consistency, and develop a Metropolis-Hastings within Gibbs sampling algorithm. In extensive simulations, we show that the proposed procedure outperforms existing methods in both estimation and inferential tasks. Finally, we apply our procedure to U.S. Senate election data and discover an incumbent party advantage which is heterogeneous over different time periods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10652v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kevin Tao, Y. Samuel Wang, David Ruppert</dc:creator>
    </item>
    <item>
      <title>On the Contractivity of Stochastic Interpolation Flow</title>
      <link>https://arxiv.org/abs/2504.10653</link>
      <description>arXiv:2504.10653v1 Announce Type: new 
Abstract: We investigate stochastic interpolation, a recently introduced framework for high dimensional sampling which bears many similarities to diffusion modeling. Stochastic interpolation generates a data sample by first randomly initializing a particle drawn from a simple base distribution, then simulating deterministic or stochastic dynamics such that in finite time the particle's distribution converges to the target. We show that for a Gaussian base distribution and a strongly log-concave target distribution, the stochastic interpolation flow map is Lipschitz with a sharp constant which matches that of Caffarelli's theorem for optimal transport maps. We are further able to construct Lipschitz transport maps between non-Gaussian distributions, generalizing some recent constructions in the literature on transport methods for establishing functional inequalities. We discuss the practical implications of our theorem for the sampling and estimation problems required by stochastic interpolation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10653v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Max Daniels</dc:creator>
    </item>
    <item>
      <title>Beyond Coordinates: Meta-Equivariance in Statistical Inference</title>
      <link>https://arxiv.org/abs/2504.10667</link>
      <description>arXiv:2504.10667v1 Announce Type: new 
Abstract: Optimal statistical decisions should transcend the language used to describe them. Yet, how do we guarantee that the choice of coordinates - the parameterisation of an optimisation problem - does not subtly dictate the solution? This paper reveals a fundamental geometric invariance principle. We first analyse the optimal combination of two asymptotically normal estimators under a strictly convex trace-AMSE risk. While methods for finding optimal weights are known, we prove that the resulting optimal estimator is invariant under direct affine reparameterisations of the weighting scheme. This exemplifies a broader principle we term meta-equivariance: the unique minimiser of any strictly convex, differentiable scalar objective over a matrix space transforms covariantly under any invertible affine reparameterisation of that space. Distinct from classical statistical equivariance tied to data symmetries, meta-equivariance arises from the immutable geometry of convex optimisation itself. It guarantees that optimality, in these settings, is not an artefact of representation but an intrinsic, coordinate-free truth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10667v1</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Cook</dc:creator>
    </item>
    <item>
      <title>Power properties of the two sample test based on nearest neighbors graphs</title>
      <link>https://arxiv.org/abs/2504.10719</link>
      <description>arXiv:2504.10719v1 Announce Type: new 
Abstract: In this paper, we study the problem of testing the equality of two multivariate distributions. One class of tests used for this purpose utilizes geometric graphs constructed using inter-point distances. The asymptotic theory of these tests so far applies only to graphs which fall under the stabilizing graphs framework of Penrose and Yukich. We study the case of the $K$-nearest neighbors graph where $K=k_N$ increases with the sample size, which does not fall under the stabilizing graphs framework. Our main result gives detection thresholds for this test in parametrized families when $k_N = o(N^{1/4})$, thus extending the family of graphs where the theoretical behavior is known. We propose a 2-sided version of the test which removes an exponent gap that plagues the 1-sided test. Our result also shows that using a greater number of nearest neighbors boosts the power of the test. This provides theoretical justification for using denser graphs in testing equality of two distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10719v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahul Raphael Kanekar</dc:creator>
    </item>
    <item>
      <title>Gaussian Approximation for High-Dimensional $U$-statistics with Size-Dependent Kernels</title>
      <link>https://arxiv.org/abs/2504.10866</link>
      <description>arXiv:2504.10866v1 Announce Type: new 
Abstract: Motivated by small bandwidth asymptotics for kernel-based semiparametric estimators in econometrics, this paper establishes Gaussian approximation results for high-dimensional fixed-order $U$-statistics whose kernels depend on the sample size. Our results allow for a situation where the dominant component of the Hoeffding decomposition is absent or unknown, including cases with known degrees of degeneracy as special forms. The obtained error bounds for Gaussian approximations are sharp enough to almost recover the weakest bandwidth condition of small bandwidth asymptotics in the fixed-dimensional setting when applied to a canonical semiparametric estimation problem. We also present an application to an adaptive goodness-of-fit testing, along with discussions about several potential applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10866v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunsuke Imai, Yuta Koike</dc:creator>
    </item>
    <item>
      <title>Optimal inference for the mean of random functions</title>
      <link>https://arxiv.org/abs/2504.11025</link>
      <description>arXiv:2504.11025v1 Announce Type: new 
Abstract: We study estimation and inference for the mean of real-valued random functions defined on a hypercube. The independent random functions are observed on a discrete, random subset of design points, possibly with heteroscedastic noise. We propose a novel optimal-rate estimator based on Fourier series expansions and establish a sharp non-asymptotic error bound in $L^2-$norm. Additionally, we derive a non-asymptotic Gaussian approximation bound for our estimated Fourier coefficients. Pointwise and uniform confidence sets are constructed. Our approach is made adaptive by a plug-in estimator for the H\"older regularity of the mean function, for which we derive non-asymptotic concentration bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11025v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omar Kassi, Valentin Patilea</dc:creator>
    </item>
    <item>
      <title>On relative universality, regression operator, and conditional independence</title>
      <link>https://arxiv.org/abs/2504.11044</link>
      <description>arXiv:2504.11044v1 Announce Type: new 
Abstract: The notion of relative universality with respect to a {\sigma}-field was introduced to establish the unbiasedness and Fisher consistency of an estimator in nonlinear sufficient dimension reduction. However, there is a gap in the proof of this result in the existing literature. The existing definition of relative universality seems to be too strong for the proof to be valid. In this note we modify the definition of relative universality using the concept of \k{o}-measurability, and rigorously establish the mentioned unbiasedness and Fisher consistency. The significance of this result is beyond its original context of sufficient dimension reduction, because relative universality allows us to use the regression operator to fully characterize conditional independence, a crucially important statistical relation that sits at the core of many areas and methodologies in statistics and machine learning, such as dimension reduction, graphical models, probability embedding, causal inference, and Bayesian estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11044v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bing Li, Ben Jones, Andreas Artemiou</dc:creator>
    </item>
    <item>
      <title>Is model selection possible for the $\ell_p$-loss? PCO estimation for regression models</title>
      <link>https://arxiv.org/abs/2504.11217</link>
      <description>arXiv:2504.11217v1 Announce Type: new 
Abstract: This paper addresses the problem of model selection in the sequence model $Y=\theta+\varepsilon\xi$, when $\xi$ is sub-Gaussian, for non-euclidian loss-functions. In this model, the Penalized Comparison to Overfitting procedure is studied for the weighted $\ell_p$-loss, $p\geq 1.$ Several oracle inequalities are derived from concentration inequalities for sub-Weibull variables. Using judicious collections of models and penalty terms, minimax rates of convergence are stated for Besov bodies $\mathcal{B}_{r,\infty}^s$. These results are applied to the functional model of nonparametric regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11217v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claire Lacour, Pascal Massart, Vincent Rivoirard</dc:creator>
    </item>
    <item>
      <title>Minimax asymptotics</title>
      <link>https://arxiv.org/abs/2504.11269</link>
      <description>arXiv:2504.11269v1 Announce Type: new 
Abstract: In this paper, we consider asymptotics of the optimal value and the optimal solutions of parametric minimax estimation problems. Specifically, we consider estimators of the optimal value and the optimal solutions in a sample minimax problem that approximates the true population problem and study the limiting distributions of these estimators as the sample size tends to infinity. The main technical tool we employ in our analysis is the theory of sensitivity analysis of parameterized mathematical optimization problems. Our results go well beyond the existing literature and show that these limiting distributions are highly non-Gaussian in general and normal in simple specific cases. These results open up the way for the development of statistical inference methods in parametric minimax problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11269v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mika Meitz, Alexander Shapiro</dc:creator>
    </item>
    <item>
      <title>Posterior Consistency in Parametric Models via a Tighter Notion of Identifiability</title>
      <link>https://arxiv.org/abs/2504.11360</link>
      <description>arXiv:2504.11360v1 Announce Type: new 
Abstract: We investigate Bayesian posterior consistency in the context of parametric models with proper priors. While typical state-of-the-art approaches rely on regularity conditions that are difficult to verify and often misaligned with the actual mechanisms driving posterior consistency, we propose an alternative framework centered on a simple yet general condition we call ''sequential identifiability''. This concept strengthens the usual identifiability assumption by requiring that any sequence of parameters whose induced distributions converge to the true data-generating distribution must itself converge to the true parameter value. We demonstrate that sequential identifiability, combined with a standard Kullback--Leibler prior support condition, is sufficient to ensure posterior consistency. Moreover, we show that failure of this condition necessarily entails a specific and pathological form of oscillations of the model around the true density, which cannot exist without intentional design. This leads to the important insight that posterior inconsistency may be safely ruled out, except in the unrealistic scenario where the modeler possesses precise knowledge of the data-generating distribution and deliberately incorporates oscillatory pathologies into the model targeting the corresponding density. Taken together, these results provide a unified perspective on both consistency and inconsistency in parametric settings, significantly expanding the class of models for which posterior consistency can be rigorously established. To illustrate the strength and versatility of our framework, we construct a one-dimensional model that violates standard regularity conditions and fails to admit a consistent maximum likelihood estimator, yet supports a complete posterior consistency analysis via our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11360v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicola Bariletto, Bernardo Flores, Stephen G. Walker</dc:creator>
    </item>
    <item>
      <title>Trimmed sample means for robust uniform mean estimation and regression</title>
      <link>https://arxiv.org/abs/2302.06710</link>
      <description>arXiv:2302.06710v3 Announce Type: replace 
Abstract: It is well-known that trimmed sample means are robust against heavy tails and data contamination. This paper analyzes the performance of trimmed means and related methods in two novel contexts. The first one consists of estimating expectations of functions in a given family, with uniform error bounds; this is closely related to the problem of estimating the mean of a random vector under a general norm. The second problem considered is that of regression with quadratic loss. In both cases, trimmed-mean-based estimators are the first to obtain optimal dependence on the (adversarial) contamination level. Moreover, they also match or improve upon the state of the art in terms of heavy tails. Experiments with synthetic data show that a natural ``trimmed mean linear regression'' method often performs better than both ordinary least squares and alternative methods based on median-of-means.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.06710v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto I. Oliveira, Lucas Resende</dc:creator>
    </item>
    <item>
      <title>On the Optimality of Functional Sliced Inverse Regression</title>
      <link>https://arxiv.org/abs/2307.02777</link>
      <description>arXiv:2307.02777v2 Announce Type: replace 
Abstract: In this paper, we prove that functional sliced inverse regression (FSIR) achieves the optimal (minimax) rate for estimating the central space in functional sufficient dimension reduction problems. First, we provide a concentration inequality for the FSIR estimator of the covariance of the conditional mean. Based on this inequality, we establish the root-$n$ consistency of the FSIR estimator of the image of covariance of the conditional mean. Second, we apply the most widely used truncated scheme to estimate the inverse of the covariance operator and identify the truncation parameter that ensures
  that FSIR can achieve the optimal minimax convergence rate for estimating the central space. Finally, we conduct simulations to demonstrate the optimal choice of truncation parameter and the estimation efficiency of FSIR. To the best of our knowledge, this is the first paper to rigorously prove the minimax optimality of FSIR in estimating the central space for multiple-index models and general $Y$ (not necessarily discrete).</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.02777v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Chen, Songtao Tian, Dongming Huang, Qian Lin, Jun S. Liu</dc:creator>
    </item>
    <item>
      <title>Improved Convergence Rate of Nested Simulation with LSE on Sieve</title>
      <link>https://arxiv.org/abs/2310.11756</link>
      <description>arXiv:2310.11756v2 Announce Type: replace 
Abstract: Nested simulation encompasses the estimation of functionals linked to conditional expectations through simulation techniques. In this paper, we treat conditional expectation as a function of the multidimensional conditioning variable and provide asymptotic analyses of general Least Squared Estimators on sieve, without imposing specific assumptions on the function's form. Our study explores scenarios in which the convergence rate surpasses that of the standard Monte Carlo method and the one recently proposed based on kernel ridge regression. We also delve into the conditions that allow for achieving the best possible square root convergence rate among all methods. Numerical experiments are conducted to support our statements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.11756v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruoxue Liu, Liang Ding, Wenjia Wang, Lu Zou</dc:creator>
    </item>
    <item>
      <title>Bridging Root-$n$ and Non-standard Asymptotics: Adaptive Inference in M-Estimation</title>
      <link>https://arxiv.org/abs/2501.07772</link>
      <description>arXiv:2501.07772v3 Announce Type: replace 
Abstract: This manuscript studies a general approach to construct confidence sets for the solution of population-level optimization, commonly referred to as M-estimation. Statistical inference for M-estimation poses significant challenges due to the non-standard limiting behaviors of the corresponding estimator, which arise in settings with increasing dimension of parameters, non-smooth objectives, or constraints. We propose a simple and unified method that guarantees validity in both regular and irregular cases. Moreover, we provide a comprehensive width analysis of the proposed confidence set, showing that the convergence rate of the diameter is adaptive to the unknown degree of instance-specific regularity. We apply the proposed method to several high-dimensional and irregular statistical problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07772v3</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenta Takatsu, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Safety of particle filters: Some results on the time evolution of particle filter estimates</title>
      <link>https://arxiv.org/abs/2503.21334</link>
      <description>arXiv:2503.21334v3 Announce Type: replace 
Abstract: Particle filters (PFs) is a class of Monte Carlo algorithms that propagate over time a set of $N\in\mathbb{N}$ particles which can be used to estimate, in an online fashion, the sequence of filtering distributions $(\hat{\eta}_t)_{t\geq 1}$ defined by a state-space model. Despite the popularity of PFs, the study of the time evolution of their estimates has received barely attention in the literature. Denoting by $(\hat{\eta}_t^N)_{t\geq 1}$ the PF estimate of $(\hat{\eta}_t)_{t\geq 1}$ and letting $\kappa\in (0,1)$, in this work we first show that for any number of particles $N$ it holds that, with probability one, we have $\|\hat{\eta}_t^N- \hat{\eta}_t\|\geq \kappa$ for infinitely many $t\geq 1$, with $\|\cdot\|$ a measure of distance between probability distributions. Considering a simple filtering problem we then provide reassuring results concerning the ability of PFs to estimate jointly a finite set $\{\hat{\eta}_t\}_{t=1}^T$ of filtering distributions by studying $\mathbb{P}(\sup_{t\in\{1,\dots,T\}}\|\hat{\eta}_t^{N}-\hat{\eta}_t\|\geq \kappa)$. Finally, on the same toy filtering problem, we prove that sequential quasi-Monte Carlo, a randomized quasi-Monte Carlo version of PF algorithms, offers greater safety guarantees than PFs in the sense that, for this algorithm, it holds that $\lim_{N\rightarrow\infty}\sup_{t\geq 1}\|\hat{\eta}_t^N-\hat{\eta}_t\|=0$ with probability one.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21334v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mathieu Gerber</dc:creator>
    </item>
    <item>
      <title>High-dimensional statistical inference for linkage disequilibrium score regression and its cross-ancestry extensions</title>
      <link>https://arxiv.org/abs/2306.15779</link>
      <description>arXiv:2306.15779v2 Announce Type: replace-cross 
Abstract: Linkage disequilibrium score regression (LDSC) has emerged as an essential tool for genetic and genomic analyses of complex traits, utilizing high-dimensional data derived from genome-wide association studies (GWAS). LDSC computes the linkage disequilibrium (LD) scores using an external reference panel, and integrates the LD scores with only summary data from the original GWAS. In this paper, we investigate LDSC within a fixed-effect data integration framework, underscoring its ability to merge multi-source GWAS data and reference panels. In particular, we take account of the genome-wide dependence among the high-dimensional GWAS summary statistics, along with the block-diagonal dependence pattern in estimated LD scores. Our analysis uncovers several key factors of both the original GWAS and reference panel datasets that determine the performance of LDSC. We show that it is relatively feasible for LDSC-based estimators to achieve asymptotic normality when applied to genome-wide genetic variants (e.g., in genetic variance and covariance estimation), whereas it becomes considerably challenging when we focus on a much smaller subset of genetic variants (e.g., in partitioned heritability analysis). Moreover, by modeling the disparities in LD patterns across different populations, we unveil that LDSC can be expanded to conduct cross-ancestry analyses using data from distinct global populations (such as European and Asian). We validate our theoretical findings through extensive numerical evaluations using real genetic data from the UK Biobank study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15779v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fei Xue, Bingxin Zhao</dc:creator>
    </item>
    <item>
      <title>Posterior and variational inference for deep neural networks with heavy-tailed weights</title>
      <link>https://arxiv.org/abs/2406.03369</link>
      <description>arXiv:2406.03369v2 Announce Type: replace-cross 
Abstract: We consider deep neural networks in a Bayesian framework with a prior distribution sampling the network weights at random. Following a recent idea of Agapiou and Castillo (2023), who show that heavy-tailed prior distributions achieve automatic adaptation to smoothness, we introduce a simple Bayesian deep learning prior based on heavy-tailed weights and ReLU activation. We show that the corresponding posterior distribution achieves near-optimal minimax contraction rates, simultaneously adaptive to both intrinsic dimension and smoothness of the underlying function, in a variety of contexts including nonparametric regression, geometric data and Besov spaces. While most works so far need a form of model selection built-in within the prior distribution, a key aspect of our approach is that it does not require to sample hyperparameters to learn the architecture of the network. We also provide variational Bayes counterparts of the results, that show that mean-field variational approximations still benefit from near-optimal theoretical support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03369v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isma\"el Castillo, Paul Egels</dc:creator>
    </item>
    <item>
      <title>Gaussian Differentially Private Human Faces Under a Face Radial Curve Representation</title>
      <link>https://arxiv.org/abs/2409.08301</link>
      <description>arXiv:2409.08301v2 Announce Type: replace-cross 
Abstract: In this paper we consider the problem of releasing a Gaussian Differentially Private (GDP) 3D human face. The human face is a complex structure with many features and inherently tied to one's identity. Protecting this data, in a formally private way, is important yet challenging given the dimensionality of the problem. We extend approximate DP techniques for functional data to the GDP framework. We further propose a novel representation, face radial curves, of a 3D face as a set of functions and then utilize our proposed GDP functional data mechanism. To preserve the shape of the face while injecting noise we rely on tools from shape analysis for our novel representation of the face. We show that our method preserves the shape of the average face and injects less noise than traditional methods for the same privacy budget. Our mechanism consists of two primary components, the first is generally applicable to function value summaries (as are commonly found in nonparametric statistics or functional data analysis) while the second is general to disk-like surfaces and hence more applicable than just to human faces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08301v2</guid>
      <category>cs.CR</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Soto, Matthew Reimherr, Aleksandra Slavkovic, Mark Shriver</dc:creator>
    </item>
    <item>
      <title>Model-free Estimation of Latent Structure via Multiscale Nonparametric Maximum Likelihood</title>
      <link>https://arxiv.org/abs/2410.22248</link>
      <description>arXiv:2410.22248v2 Announce Type: replace-cross 
Abstract: Multivariate distributions often carry latent structures that are difficult to identify and estimate, and which better reflect the data generating mechanism than extrinsic structures exhibited simply by the raw data. In this paper, we propose a model-free approach for estimating such latent structures whenever they are present, without assuming they exist a priori. Given an arbitrary density $p_0$, we construct a multiscale representation of the density and propose data-driven methods for selecting representative models that capture meaningful discrete structure. Our approach uses a nonparametric maximum likelihood estimator to estimate the latent structure at different scales and we further characterize their asymptotic limits. By carrying out such a multiscale analysis, we obtain coarseto-fine structures inherent in the original distribution, which are integrated via a model selection procedure to yield an interpretable discrete representation of it. As an application, we design a clustering algorithm based on the proposed procedure and demonstrate its effectiveness in capturing a wide range of latent structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22248v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bryon Aragam, Ruiyi Yang</dc:creator>
    </item>
    <item>
      <title>A Piecewise Lyapunov Analysis of Sub-quadratic SGD: Applications to Robust and Quantile Regression</title>
      <link>https://arxiv.org/abs/2504.08178</link>
      <description>arXiv:2504.08178v3 Announce Type: replace-cross 
Abstract: Motivated by robust and quantile regression problems, we investigate the stochastic gradient descent (SGD) algorithm for minimizing an objective function $f$ that is locally strongly convex with a sub--quadratic tail. This setting covers many widely used online statistical methods. We introduce a novel piecewise Lyapunov function that enables us to handle functions $f$ with only first-order differentiability, which includes a wide range of popular loss functions such as Huber loss. Leveraging our proposed Lyapunov function, we derive finite-time moment bounds under general diminishing stepsizes, as well as constant stepsizes. We further establish the weak convergence, central limit theorem and bias characterization under constant stepsize, providing the first geometrical convergence result for sub--quadratic SGD. Our results have wide applications, especially in online statistical methods. In particular, we discuss two applications of our results. 1) Online robust regression: We consider a corrupted linear model with sub--exponential covariates and heavy--tailed noise. Our analysis provides convergence rates comparable to those for corrupted models with Gaussian covariates and noise. 2) Online quantile regression: Importantly, our results relax the common assumption in prior work that the conditional density is continuous and provide a more fine-grained analysis for the moment bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08178v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yixuan Zhang, Dongyan Huo, Yudong Chen, Qiaomin Xie</dc:creator>
    </item>
    <item>
      <title>Inferring Outcome Means of Exponential Family Distributions Estimated by Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2504.09347</link>
      <description>arXiv:2504.09347v2 Announce Type: replace-cross 
Abstract: While deep neural networks (DNNs) are widely used for prediction, inference on DNN-estimated subject-specific means for categorical or exponential family outcomes remains underexplored. We address this by proposing a DNN estimator under generalized nonparametric regression models (GNRMs) and developing a rigorous inference framework. Unlike existing approaches that assume independence between prediction errors and inputs to establish the error bound, a condition often violated in GNRMs, we allow for dependence and our theoretical analysis demonstrates the feasibility of drawing inference under GNRMs. To implement inference, we consider an Ensemble Subsampling Method (ESM) that leverages U-statistics and the Hoeffding decomposition to construct reliable confidence intervals for DNN estimates. We show that, under GNRM settings, ESM enables model-free variance estimation and accounts for heterogeneity among individuals in the population. Through simulations under nonparametric logistic, Poisson, and binomial regression models, we demonstrate the effectiveness and efficiency of our method. We further apply the method to the electronic Intensive Care Unit (eICU) dataset, a large-scale collection of anonymized health records from ICU patients, to predict ICU readmission risk and offer patient-centric insights for clinical decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09347v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xuran Meng, Yi Li</dc:creator>
    </item>
  </channel>
</rss>
