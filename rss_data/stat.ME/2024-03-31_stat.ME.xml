<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Apr 2024 04:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 01 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Deep Learning Framework with Uncertainty Quantification for Survey Data: Assessing and Predicting Diabetes Mellitus Risk in the American Population</title>
      <link>https://arxiv.org/abs/2403.19752</link>
      <description>arXiv:2403.19752v1 Announce Type: new 
Abstract: Complex survey designs are commonly employed in many medical cohorts. In such scenarios, developing case-specific predictive risk score models that reflect the unique characteristics of the study design is essential. This approach is key to minimizing potential selective biases in results. The objectives of this paper are: (i) To propose a general predictive framework for regression and classification using neural network (NN) modeling, which incorporates survey weights into the estimation process; (ii) To introduce an uncertainty quantification algorithm for model prediction, tailored for data from complex survey designs; (iii) To apply this method in developing robust risk score models to assess the risk of Diabetes Mellitus in the US population, utilizing data from the NHANES 2011-2014 cohort. The theoretical properties of our estimators are designed to ensure minimal bias and the statistical consistency, thereby ensuring that our models yield reliable predictions and contribute novel scientific insights in diabetes research. While focused on diabetes, this NN predictive framework is adaptable to create clinical models for a diverse range of diseases and medical cohorts. The software and the data used in this paper is publicly available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19752v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcos Matabuena, Juan C. Vidal, Rahul Ghosal, Jukka-Pekka Onnela</dc:creator>
    </item>
    <item>
      <title>Nonparametric conditional risk mapping under heteroscedasticity</title>
      <link>https://arxiv.org/abs/2403.19757</link>
      <description>arXiv:2403.19757v1 Announce Type: new 
Abstract: A nonparametric procedure to estimate the conditional probability that a nonstationary geostatistical process exceeds a certain threshold value is proposed. The method consists of a bootstrap algorithm that combines conditional simulation techniques with nonparametric estimations of the trend and the variability. The nonparametric local linear estimator, considering a bandwidth matrix selected by a method that takes the spatial dependence into account, is used to estimate the trend. The variability is modeled estimating the conditional variance and the variogram from corrected residuals to avoid the biasses. The proposed method allows to obtain estimates of the conditional exceedance risk in non-observed spatial locations. The performance of the approach is analyzed by simulation and illustrated with the application to a real data set of precipitations in the U.S.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19757v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s13253-023-00555-0</arxiv:DOI>
      <arxiv:journal_reference>Journal of Agricultural, Biological, and Environmental Statistics (2024), 29, 56-72</arxiv:journal_reference>
      <dc:creator>Rub\'en Fern\'andez-Casal, Sergio Castillo-P\'aez, Mario Francisco-Fern\'andez</dc:creator>
    </item>
    <item>
      <title>Protocols for Observational Studies: Methods and Open Problems</title>
      <link>https://arxiv.org/abs/2403.19807</link>
      <description>arXiv:2403.19807v1 Announce Type: new 
Abstract: For learning about the causal effect of a treatment, a randomized controlled trial (RCT) is considered the gold standard. However, randomizing treatment is sometimes unethical or infeasible, and instead an observational study may be conducted. While some aspects of a well designed RCT cannot be replicated in an observational study, one aspect that can is to have a protocol with prespecified hypotheses about prespecified outcomes and a prespecified analysis. We illustrate the value of protocols for observational studies in three applications -- the effect of playing high school football on later life mental functioning, the effect of police seizing a gun when arresting a domestic violence suspect on future domestic violence and the effect of mountaintop mining on health. We then discuss methodologies for observational study protocols. We discuss considerations for protocols that are similar between observational studies and RCTs, and considerations that are different. The considerations that are different include (i) whether the protocol should be specified before treatment assignment is known or after; (ii) how multiple outcomes should be incorporated into the planned analysis and (iii) how subgroups should be incorporated into the planned analysis. We conclude with discussion of a few open problems in the methodology of observational study protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19807v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dylan S. Small</dc:creator>
    </item>
    <item>
      <title>Testing for common structures in high-dimensional factor models</title>
      <link>https://arxiv.org/abs/2403.19818</link>
      <description>arXiv:2403.19818v1 Announce Type: new 
Abstract: This work proposes a novel procedure to test for common structures across two high-dimensional factor models. The introduced test allows to uncover whether two factor models are driven by the same loading matrix up to some linear transformation. The test can be used to discover inter individual relationships between two data sets. In addition, it can be applied to test for structural changes over time in the loading matrix of an individual factor model. The test aims to reduce the set of possible alternatives in a classical change-point setting. The theoretical results establish the asymptotic behavior of the introduced test statistic. The theory is supported by a simulation study showing promising results in empirical test size and power. A data application investigates changes in the loadings when modeling the celebrated US macroeconomic data set of Stock and Watson.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19818v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marie-Christine D\"uker, Vladas Pipiras</dc:creator>
    </item>
    <item>
      <title>Constrained least squares simplicial-simplicial regression</title>
      <link>https://arxiv.org/abs/2403.19835</link>
      <description>arXiv:2403.19835v1 Announce Type: new 
Abstract: Simplicial-simplicial regression refers to the regression setting where both the responses and predictor variables lie within the simplex space, i.e. they are compositional. For this setting, constrained least squares, where the regression coefficients themselves lie within the simplex, is proposed. The model is transformation-free but the adoption of a power transformation is straightforward, it can treat more than one compositional datasets as predictors and offers the possibility of weights among the simplicial predictors. Among the model's advantages are its ability to treat zeros in a natural way and a highly computationally efficient algorithm to estimate its coefficients. Resampling based hypothesis testing procedures are employed regarding inference, such as linear independence, and equality of the regression coefficients to some pre-specified values. The performance of the proposed technique and its comparison to an existing methodology that is of the same spirit takes place using simulation studies and real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19835v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Tsagris</dc:creator>
    </item>
    <item>
      <title>Optimal regimes with limited resources</title>
      <link>https://arxiv.org/abs/2403.19842</link>
      <description>arXiv:2403.19842v1 Announce Type: new 
Abstract: Policy-makers are often faced with the task of distributing a limited supply of resources. To support decision-making in these settings, statisticians are confronted with two challenges: estimands are defined by allocation strategies that are functions of features of all individuals in a cluster; and relatedly the observed data are neither independent nor identically distributed when individuals compete for resources. Existing statistical approaches are inadequate because they ignore at least one of these core features. As a solution, we develop theory for a general policy class of dynamic regimes for clustered data, covering existing results in classical and interference settings as special cases. We cover policy-relevant estimands and articulate realistic conditions compatible with resource-limited observed data. We derive identification and inference results for settings with a finite number of individuals in a cluster, where the observed dataset is viewed as a single draw from a super-population of clusters. We also consider asymptotic estimands when the number of individuals in a cluster is allowed to grow; under explicit conditions, we recover previous results, thereby clarifying when the use of existing methods is permitted. Our general results lay the foundation for future research on dynamic regimes for clustered data, including the longitudinal cluster setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19842v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron L. Sarvet, Julien D. Laurendeau, Mats J. Stensrud</dc:creator>
    </item>
    <item>
      <title>Doubly robust estimation and inference for a log-concave counterfactual density</title>
      <link>https://arxiv.org/abs/2403.19917</link>
      <description>arXiv:2403.19917v1 Announce Type: new 
Abstract: We consider the problem of causal inference based on observational data (or the related missing data problem) with a binary or discrete treatment variable. In that context we study counterfactual density estimation, which provides more nuanced information than counterfactual mean estimation (i.e., the average treatment effect). We impose the shape-constraint of log-concavity (a unimodality constraint) on the counterfactual densities, and then develop doubly robust estimators of the log-concave counterfactual density (based on an augmented inverse-probability weighted pseudo-outcome), and show the consistency in various global metrics of that estimator. Based on that estimator we also develop asymptotically valid pointwise confidence intervals for the counterfactual density.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19917v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daeyoung Ham, Ted Westling, Charles R. Doss</dc:creator>
    </item>
    <item>
      <title>Supervised Bayesian joint graphical model for simultaneous network estimation and subgroup identification</title>
      <link>https://arxiv.org/abs/2403.19994</link>
      <description>arXiv:2403.19994v1 Announce Type: new 
Abstract: Heterogeneity is a fundamental characteristic of cancer. To accommodate heterogeneity, subgroup identification has been extensively studied and broadly categorized into unsupervised and supervised analysis. Compared to unsupervised analysis, supervised approaches potentially hold greater clinical implications. Under the unsupervised analysis framework, several methods focusing on network-based subgroup identification have been developed, offering more comprehensive insights than those restricted to mean, variance, and other simplistic distributions by incorporating the interconnections among variables. However, research on supervised network-based subgroup identification remains limited. In this study, we develop a novel supervised Bayesian graphical model for jointly identifying multiple heterogeneous networks and subgroups. In the proposed model, heterogeneity is not only reflected in molecular data but also associated with a clinical outcome, and a novel similarity prior is introduced to effectively accommodate similarities among the networks of different subgroups, significantly facilitating clinically meaningful biological network construction and subgroup identification. The consistency properties of the estimates are rigorously established, and an efficient algorithm is developed. Extensive simulation studies and a real-world application to TCGA data are conducted, which demonstrate the advantages of the proposed approach in terms of both subgroup and network identification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19994v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xing Qin, Xu Liu, Shuangge Ma, Mengyun Wu</dc:creator>
    </item>
    <item>
      <title>Best Subset Solution Path for Linear Dimension Reduction Models using Continuous Optimization</title>
      <link>https://arxiv.org/abs/2403.20007</link>
      <description>arXiv:2403.20007v1 Announce Type: new 
Abstract: The selection of best variables is a challenging problem in supervised and unsupervised learning, especially in high dimensional contexts where the number of variables is usually much larger than the number of observations. In this paper, we focus on two multivariate statistical methods: principal components analysis and partial least squares. Both approaches are popular linear dimension-reduction methods with numerous applications in several fields including in genomics, biology, environmental science, and engineering. In particular, these approaches build principal components, new variables that are combinations of all the original variables. A main drawback of principal components is the difficulty to interpret them when the number of variables is large. To define principal components from the most relevant variables, we propose to cast the best subset solution path method into principal component analysis and partial least square frameworks. We offer a new alternative by exploiting a continuous optimization algorithm for best subset solution path. Empirical studies show the efficacy of our approach for providing the best subset solution path. The usage of our algorithm is further exposed through the analysis of two real datasets. The first dataset is analyzed using the principle component analysis while the analysis of the second dataset is based on partial least square framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20007v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benoit Liquet, Sarat Moka, Samuel Muller</dc:creator>
    </item>
    <item>
      <title>Quantifying Uncertainty: All We Need is the Bootstrap?</title>
      <link>https://arxiv.org/abs/2403.20182</link>
      <description>arXiv:2403.20182v1 Announce Type: new 
Abstract: Standard errors, confidence intervals, hypothesis tests, and other quantifications of uncertainty are essential to statistical practice. However, they feature a plethora of different methods, mathematical formulas, and concepts. Could we not just replace them all with the general and relatively easy-to-understand non-parametric bootstrap? We contribute to answering this question with a review of related work and a simulation study of one- and two-sided confidence intervals over several different sample sizes, confidence levels, data generating processes, and functionals. Results show that double bootstrap is the overall best method and a viable alternative to typically used approaches in all but the smallest sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20182v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ur\v{s}a Zrim\v{s}ek, Erik \v{S}trumbelj</dc:creator>
    </item>
    <item>
      <title>Hypergraph adjusted plus-minus</title>
      <link>https://arxiv.org/abs/2403.20214</link>
      <description>arXiv:2403.20214v1 Announce Type: new 
Abstract: In team sports, traditional ranking statistics do not allow for the simultaneous evaluation of both individuals and combinations of players. Metrics for individual player rankings often fail to consider the interaction effects between groups of players, while methods for assessing full lineups cannot be used to identify the value of lower-order combinations of players (pairs, trios, etc.). Given that player and lineup rankings are inherently dependent on each other, these limitations may affect the accuracy of performance evaluations. To address this, we propose a novel adjusted box score plus-minus (APM) approach that allows for the simultaneous ranking of individual players, lower-order combinations of players, and full lineups. The method adjusts for the complete dependency structure and is motivated by the connection between APM and the hypergraph representation of a team. We discuss the similarities of our approach to other advanced metrics, demonstrate it using NBA data from 2012-2022, and suggest potential directions for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20214v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Josephs, Elizabeth Upton</dc:creator>
    </item>
    <item>
      <title>Towards a turnkey approach to unbiased Monte Carlo estimation of smooth functions of expectations</title>
      <link>https://arxiv.org/abs/2403.20313</link>
      <description>arXiv:2403.20313v1 Announce Type: new 
Abstract: Given a smooth function $f$, we develop a general approach to turn Monte
  Carlo samples with expectation $m$ into an unbiased estimate of $f(m)$.
  Specifically, we develop estimators that are based on randomly truncating
  the Taylor series expansion of $f$ and estimating the coefficients of the
  truncated series. We derive their properties and propose a strategy to set
  their tuning parameters -- which depend on $m$ -- automatically, with a
  view to make the whole approach simple to use. We develop our methods for
  the specific functions $f(x)=\log x$ and $f(x)=1/x$, as they arise in
  several statistical applications such as maximum likelihood estimation of
  latent variable models and Bayesian inference for un-normalised models.
  Detailed numerical studies are performed for a range of applications to
  determine how competitive and reliable the proposed approach is.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20313v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Chopin, Francesca R. Crucinio, Sumeetpal S. Singh</dc:creator>
    </item>
    <item>
      <title>High-dimensional analysis of ridge regression for non-identically distributed data with a variance profile</title>
      <link>https://arxiv.org/abs/2403.20200</link>
      <description>arXiv:2403.20200v1 Announce Type: cross 
Abstract: High-dimensional linear regression has been thoroughly studied in the context of independent and identically distributed data. We propose to investigate high-dimensional regression models for independent but non-identically distributed data. To this end, we suppose that the set of observed predictors (or features) is a random matrix with a variance profile and with dimensions growing at a proportional rate. Assuming a random effect model, we study the predictive risk of the ridge estimator for linear regression with such a variance profile. In this setting, we provide deterministic equivalents of this risk and of the degree of freedom of the ridge estimator. For certain class of variance profile, our work highlights the emergence of the well-known double descent phenomenon in high-dimensional regression for the minimum norm least-squares estimator when the ridge regularization parameter goes to zero. We also exhibit variance profiles for which the shape of this predictive risk differs from double descent. The proofs of our results are based on tools from random matrix theory in the presence of a variance profile that have not been considered so far to study regression models. Numerical experiments are provided to show the accuracy of the aforementioned deterministic equivalents on the computation of the predictive risk of ridge regression. We also investigate the similarities and differences that exist with the standard setting of independent and identically distributed data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20200v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J\'er\'emie Bigot, Issa-Mbenard Dabo, Camille Male</dc:creator>
    </item>
    <item>
      <title>Flexible Basis Representations for Modeling Large Non-Gaussian Spatial Data</title>
      <link>https://arxiv.org/abs/2211.06808</link>
      <description>arXiv:2211.06808v3 Announce Type: replace 
Abstract: Nonstationary and non-Gaussian spatial data are common in various fields, including ecology (e.g., counts of animal species), epidemiology (e.g., disease incidence counts in susceptible regions), and environmental science (e.g., remotely-sensed satellite imagery). Due to modern data collection methods, the size of these datasets have grown considerably. Spatial generalized linear mixed models (SGLMMs) are a flexible class of models used to model nonstationary and non-Gaussian datasets. Despite their utility, SGLMMs can be computationally prohibitive for even moderately large datasets (e.g., 5,000 to 100,000 observed locations). To circumvent this issue, past studies have embedded nested radial basis functions into the SGLMM. However, two crucial specifications (knot placement and bandwidth parameters), which directly affect model performance, are typically fixed prior to model-fitting. We propose a novel approach to model large nonstationary and non-Gaussian spatial datasets using adaptive radial basis functions. Our approach: (1) partitions the spatial domain into subregions; (2) employs reversible-jump Markov chain Monte Carlo (RJMCMC) to infer the number and location of the knots within each partition; and (3) models the latent spatial surface using partition-varying and adaptive basis functions. Through an extensive simulation study, we show that our approach provides more accurate predictions than competing methods while preserving computational efficiency. We demonstrate our approach on two environmental datasets - incidences of plant species and counts of bird species in the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.06808v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Remy MacDonald, Benjamin Seiyon Lee</dc:creator>
    </item>
    <item>
      <title>Sensitivity analysis for principal ignorability violation in estimating complier and noncomplier average causal effects</title>
      <link>https://arxiv.org/abs/2303.05032</link>
      <description>arXiv:2303.05032v4 Announce Type: replace 
Abstract: An important strategy for identifying principal causal effects, which are often used in settings with noncompliance, is to invoke the principal ignorability (PI) assumption. As PI is untestable, it is important to gauge how sensitive effect estimates are to its violation. We focus on this task for the common one-sided noncompliance setting where there are two principal strata, compliers and noncompliers. Under PI, compliers and noncompliers share the same outcome-mean-given-covariates function under the control condition. For sensitivity analysis, we allow this function to differ between compliers and noncompliers in several ways, indexed by an odds ratio, a generalized odds ratio, a mean ratio, or a standardized mean difference sensitivity parameter. We tailor sensitivity analysis techniques (with any sensitivity parameter choice) to several types of PI-based main analysis methods, including outcome regression, influence function (IF) based and weighting methods. We illustrate the proposed sensitivity analyses using several outcome types from the JOBS II study. This application estimates nuisance functions parametrically -- for simplicity and accessibility. In addition, we establish rate conditions on nonparametric nuisance estimation for IF-based estimators to be asymptotically normal -- with a view to inform nonparametric inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.05032v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Trang Quynh Nguyen, Elizabeth A. Stuart, Daniel O. Scharfstein, Elizabeth L. Ogburn</dc:creator>
    </item>
    <item>
      <title>Flexible sensitivity analysis for causal inference in observational studies subject to unmeasured confounding</title>
      <link>https://arxiv.org/abs/2305.17643</link>
      <description>arXiv:2305.17643v2 Announce Type: replace 
Abstract: Causal inference with observational studies often suffers from unmeasured confounding, yielding biased estimators based on the unconfoundedness assumption. Sensitivity analysis assesses how the causal conclusions change with respect to different degrees of unmeasured confounding. Most existing sensitivity analysis methods work well for specific types of statistical estimation or testing strategies. We propose a flexible sensitivity analysis framework that can deal with commonly used inverse probability weighting, outcome regression, and doubly robust estimators simultaneously. It is based on the well-known parametrization of the selection bias as comparisons of the observed and counterfactual outcomes conditional on observed covariates. It is attractive for practical use because it only requires simple modifications of the standard estimators. Moreover, it naturally extends to many other causal inference settings, including the causal risk ratio or odds ratio, the average causal effect on the treated units, and studies with survival outcomes. We also develop an R package saci to implement our sensitivity analysis estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.17643v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sizhu Lu, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Incorporating Auxiliary Variables to Improve the Efficiency of Time-Varying Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2306.17260</link>
      <description>arXiv:2306.17260v2 Announce Type: replace 
Abstract: The use of smart devices (e.g., smartphones, smartwatches) and other wearables for context sensing and delivery of digital interventions to improve health outcomes has grown significantly in behavioral and psychiatric studies. Micro-randomized trials (MRTs) are a common experimental design for obtaining data-driven evidence on mobile health (mHealth) intervention effectiveness where each individual is repeatedly randomized to receive treatments over numerous time points. Individual characteristics and the contexts around randomizations are also collected throughout the study, some may be pre-specified as moderators when assessing time-varying causal effect moderation. Moreover, we have access to abundant measurements beyond just the moderators. Our study aims to leverage this auxiliary information to improve causal estimation and better understand the intervention effect. Similar problems have been raised in randomized control trials (RCTs), where extensive literature demonstrates that baseline covariate information can be incorporated to alleviate chance imbalances and increase asymptotic efficiency. However, covariate adjustment in the context of time-varying treatments and repeated measurements, as seen in MRTs, has not been studied. Recognizing the connection to Neyman Orthogonality, we address this gap by introducing an intuitive approach to incorporate auxiliary variables to improve the efficiency of moderated causal excursion effect estimation. The efficiency gain of our approach is proved theoretically and demonstrated through simulation studies and an analysis of data from the Intern Health Study (NeCamp et al., 2020).</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.17260v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jieru Shi, Zhenke Wu, Walter Dempsey</dc:creator>
    </item>
    <item>
      <title>Algorithms for Non-Negative Matrix Factorization on Noisy Data With Negative Values</title>
      <link>https://arxiv.org/abs/2311.04855</link>
      <description>arXiv:2311.04855v2 Announce Type: replace-cross 
Abstract: Non-negative matrix factorization (NMF) is a dimensionality reduction technique that has shown promise for analyzing noisy data, especially astronomical data. For these datasets, the observed data may contain negative values due to noise even when the true underlying physical signal is strictly positive. Prior NMF work has not treated negative data in a statistically consistent manner, which becomes problematic for low signal-to-noise data with many negative values. In this paper we present two algorithms, Shift-NMF and Nearly-NMF, that can handle both the noisiness of the input data and also any introduced negativity. Both of these algorithms use the negative data space without clipping, and correctly recover non-negative signals without any introduced positive offset that occurs when clipping negative data. We demonstrate this numerically on both simple and more realistic examples, and prove that both algorithms have monotonically decreasing update rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04855v2</guid>
      <category>astro-ph.IM</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dylan Green, Stephen Bailey</dc:creator>
    </item>
    <item>
      <title>Non-robustness of diffusion estimates on networks with measurement error</title>
      <link>https://arxiv.org/abs/2403.05704</link>
      <description>arXiv:2403.05704v2 Announce Type: replace-cross 
Abstract: Network diffusion models are used to study things like disease transmission, information spread, and technology adoption. However, small amounts of mismeasurement are extremely likely in the networks constructed to operationalize these models. We show that estimates of diffusions are highly non-robust to this measurement error. First, we show that even when measurement error is vanishingly small, such that the share of missed links is close to zero, forecasts about the extent of diffusion will greatly underestimate the truth. Second, a small mismeasurement in the identity of the initial seed generates a large shift in the locations of expected diffusion path. We show that both of these results still hold when the vanishing measurement error is only local in nature. Such non-robustness in forecasting exists even under conditions where the basic reproductive number is consistently estimable. Possible solutions, such as estimating the measurement error or implementing widespread detection efforts, still face difficulties because the number of missed links are so small. Finally, we conduct Monte Carlo simulations on simulated networks, and real networks from three settings: travel data from the COVID-19 pandemic in the western US, a mobile phone marketing campaign in rural India, and in an insurance experiment in China.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05704v2</guid>
      <category>econ.EM</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arun G. Chandrasekhar, Paul Goldsmith-Pinkham, Tyler H. McCormick, Samuel Thau, Jerry Wei</dc:creator>
    </item>
  </channel>
</rss>
