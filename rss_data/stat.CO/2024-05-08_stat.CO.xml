<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 09 May 2024 04:00:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 09 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>gasmodel: An R Package for Generalized Autoregressive Score Models</title>
      <link>https://arxiv.org/abs/2405.05073</link>
      <description>arXiv:2405.05073v1 Announce Type: new 
Abstract: Generalized autoregressive score (GAS) models are a class of observation-driven time series models that employ the score to dynamically update time-varying parameters of the underlying probability distribution. GAS models have been extensively studied and numerous variants have been proposed in the literature to accommodate diverse data types and probability distributions. This paper introduces the gasmodel package, which has been designed to facilitate the estimation, forecasting, and simulation of a wide range of GAS models. The package provides a rich selection of distributions, offers flexible options for specifying dynamics, and allows to incorporate exogenous variables. Model estimation utilizes the maximum likelihood method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05073v1</guid>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladim\'ir Hol\'y</dc:creator>
    </item>
    <item>
      <title>Fast Exact/Conservative Monte Carlo Confidence Intervals</title>
      <link>https://arxiv.org/abs/2405.05238</link>
      <description>arXiv:2405.05238v1 Announce Type: new 
Abstract: Monte Carlo tests about parameters can be "inverted" to form confidence sets: the confidence set comprises all hypothesized values of the parameter that are not rejected at level $\alpha$. When the tests are exact or conservative -- as some families of such tests are -- so are the confidence sets. Because the validity of confidence sets depends only on the significance level of the test of the true null, every null can be tested using the same Monte Carlo sample, substantially reducing the computational burden of constructing confidence sets: the computation count is $O(n)$, where $n$ is the number of data. The Monte Carlo sample can be arbitrarily small, although the highest nontrivial attainable confidence level generally increases as the number of Monte Carlo replicates increases. When the parameter is real-valued and the $P$-value is quasiconcave in that parameter, it is straightforward to find the endpoints of the confidence interval using bisection in a conservative way. For some test statistics, values for different simulations and parameter values have a simple relationship that make more savings possible. An open-source Python implementation of the approach for the one-sample and two-sample problems is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05238v1</guid>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amanda K. Glazer, Philip B. Stark</dc:creator>
    </item>
    <item>
      <title>Weighted Particle-Based Optimization for Efficient Generalized Posterior Calibration</title>
      <link>https://arxiv.org/abs/2405.04845</link>
      <description>arXiv:2405.04845v1 Announce Type: cross 
Abstract: In the realm of statistical learning, the increasing volume of accessible data and increasing model complexity necessitate robust methodologies. This paper explores two branches of robust Bayesian methods in response to this trend. The first is generalized Bayesian inference, which introduces a learning rate parameter to enhance robustness against model misspecifications. The second is Gibbs posterior inference, which formulates inferential problems using generic loss functions rather than probabilistic models. In such approaches, it is necessary to calibrate the spread of the posterior distribution by selecting a learning rate parameter. The study aims to enhance the generalized posterior calibration (GPC) algorithm proposed by Syring and Martin (2019) [Biometrika, Volume 106, Issue 2, pp. 479-486]. Their algorithm chooses the learning rate to achieve the nominal frequentist coverage probability, but it is computationally intensive because it requires repeated posterior simulations for bootstrap samples. We propose a more efficient version of the GPC inspired by sequential Monte Carlo (SMC) samplers. A target distribution with a different learning rate is evaluated without posterior simulation as in the reweighting step in SMC sampling. Thus, the proposed algorithm can reach the desired value within a few iterations. This improvement substantially reduces the computational cost of the GPC. Its efficacy is demonstrated through synthetic and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04845v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masahiro Tanaka</dc:creator>
    </item>
    <item>
      <title>Fast Computation of Leave-One-Out Cross-Validation for $k$-NN Regression</title>
      <link>https://arxiv.org/abs/2405.04919</link>
      <description>arXiv:2405.04919v1 Announce Type: cross 
Abstract: We describe a fast computation method for leave-one-out cross-validation (LOOCV) for $k$-nearest neighbours ($k$-NN) regression. We show that, under a tie-breaking condition for nearest neighbours, the LOOCV estimate of the mean square error for $k$-NN regression is identical to the mean square error of $(k+1)$-NN regression evaluated on the training data, multiplied by the scaling factor $(k+1)^2/k^2$. Therefore, to compute the LOOCV score, one only needs to fit $(k+1)$-NN regression only once, and does not need to repeat training-validation of $k$-NN regression for the number of training data. Numerical experiments confirm the validity of the fast computation method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04919v1</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Motonobu Kanagawa</dc:creator>
    </item>
    <item>
      <title>Bayesian Multilevel Compositional Data Analysis: Introduction, Evaluation, and Application</title>
      <link>https://arxiv.org/abs/2405.03985</link>
      <description>arXiv:2405.03985v2 Announce Type: replace-cross 
Abstract: Multilevel compositional data commonly occur in various fields, particularly in intensive, longitudinal studies using ecological momentary assessments. Examples include data repeatedly measured over time that are non-negative and sum to a constant value, such as sleep-wake movement behaviours in a 24-hour day. This article presents a novel methodology for analysing multilevel compositional data using a Bayesian inference approach. This method can be used to investigate how reallocation of time between sleep-wake movement behaviours may be associated with other phenomena (e.g., emotions, cognitions) at a daily level. We explain the theoretical details of the data and the models, and outline the steps necessary to implement this method. We introduce the R package multilevelcoda to facilitate the application of this method and illustrate using a real data example. An extensive parameter recovery simulation study verified the robust performance of the method. Across all simulation conditions investigated in the simulation study, the model had minimal convergence issues (convergence rate &gt; 99%) and achieved excellent quality of parameter estimates and inference, with an average bias of 0.00 (range -0.09, 0.05) and coverage of 0.95 (range 0.93, 0.97). We conclude the article with recommendations on the use of the Bayesian compositional multilevel modelling approach, and hope to promote wider application of this method to answer robust questions using the increasingly available data from intensive, longitudinal studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03985v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Flora Le, Tyman E. Stanford, Dorothea Dumuid, Joshua F. Wiley</dc:creator>
    </item>
  </channel>
</rss>
