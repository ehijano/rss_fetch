<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 Jan 2026 05:00:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Wildfire Evacuation Analysis Using Facebook Data: Evidence from Palisades and Eaton Fires</title>
      <link>https://arxiv.org/abs/2601.01052</link>
      <description>arXiv:2601.01052v1 Announce Type: new 
Abstract: The growing frequency and intensity of wildfires pose serious threats to communities in wildland-urban interface regions. Understanding evacuation behavior is critical for effective emergency planning. This study analyzes evacuation during the 2025 Palisades and Eaton Fires using high-resolution Facebook data. We propose a comprehensive framework to derive wildfire evacuation-related metrics, including compliance rate, departure timing, delay, origin-destination flows, travel distance, and destination types. A new metric, Damage-Evacuation Disparity Index (DEDI), identifies areas with severe structural damage but low evacuation compliance. Results reveal spatiotemporal heterogeneity: residents closer to the fire evacuated earlier, whereas late or nighttime orders led to lower compliance and longer delays. Contrasting patterns between East and West Altadena further illustrate this disparity. DEDI-identified communities exhibited higher social vulnerability and fire risk. Most evacuations concluded in residential areas, while longer trips concentrated in hotels and public facilities. These findings showcase the Facebook data's potential for data-driven wildfire evacuation planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01052v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shangkun Jiang, Ruggiero Lovreglio, Thomas J. Cova, Sangung Park, Susu Xu, Xilei Zhao</dc:creator>
    </item>
    <item>
      <title>Discussion Network Formation and Evolution in an Online Professional Development Class: Evidence from a MOOC for K-12 Educators</title>
      <link>https://arxiv.org/abs/2601.01117</link>
      <description>arXiv:2601.01117v1 Announce Type: new 
Abstract: Understanding how educators interact and form peer networks in online professional development contexts has become increasingly important as MOOCs for educators (MOOC-Eds) proliferate. This study examines peer discussion network formation and evolution in 'The Digital Learning Transition in K-12 Schools', a MOOC-Ed offered to U.S. and international educators in Spring 2013. Using cross-sectional and temporal exponential random graph models (ERGMs and TERGMs), the study analyzes two network subsamples: the largest connected component (N = 363) and active participants with three or more interactions (N = 227). Results reveal strong reciprocity and transitive closure effects across both networks, with participants six to nine times more likely to reciprocate interactions and over twice as likely to form ties with peers sharing common discussion partners. Assigned discussion group homophily emerged as the strongest predictor of tie formation, while regional homophily and willingness to connect also significantly influenced network structure. Temporal analysis showed discussion activity peaked mid-course before declining sharply, with network structure evolving from broadly distributed participation to concentrated interaction among a tightly connected core. These findings illuminate the mechanisms driving peer-supported learning in online professional development contexts and suggest design implications for fostering sustained educator engagement in MOOC-based learning environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01117v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuhan Ai</dc:creator>
    </item>
    <item>
      <title>Order-Constrained Spectral Causality in Multivariate Time Series</title>
      <link>https://arxiv.org/abs/2601.01216</link>
      <description>arXiv:2601.01216v1 Announce Type: new 
Abstract: We introduce an operator-theoretic framework for causal analysis in multivariate time series based on order-constrained spectral non-invariance. Directional influence is defined as sensitivity of second-order dependence operators to admissible, order-preserving temporal deformations of a designated source component, yielding an intrinsically multivariate causal notion summarized through orthogonally invariant spectral functionals. Under linear Gaussian assumptions, the criterion coincides with linear Granger causality, while beyond this regime it captures collective and nonlinear directional dependence not reflected in pairwise predictability. We establish existence, uniform consistency, and valid inference for the resulting non-smooth supremum--infimum statistics using shift-based randomization that exploits order-induced group invariance, yielding finite-sample exactness under exact invariance and asymptotic validity under weak dependence without parametric assumptions. Simulations demonstrate correct size and strong power against distributed and bulk-dominated alternatives, including nonlinear dependence missed by linear Granger tests with appropriate feature embeddings. An empirical application to a high-dimensional panel of daily financial return series spanning major asset classes illustrates system-level causal monitoring in practice. Directional organization is episodic and stress-dependent, causal propagation strengthens while remaining multi-channel, dominant causal hubs reallocate rapidly, and statistically robust transmission channels are sparse and horizon-heterogeneous even when aggregate lead--lag asymmetry is weak. The framework provides a scalable and interpretable complement to correlation-, factor-, and pairwise Granger-style analyses for complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01216v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alejandro Rodriguez Dominguez</dc:creator>
    </item>
    <item>
      <title>Model-Assisted Causal Inference for the Treatment Effect on Recurrent Events in the Presence of Terminal Events</title>
      <link>https://arxiv.org/abs/2601.01245</link>
      <description>arXiv:2601.01245v1 Announce Type: new 
Abstract: This paper is motivated by evaluating the benefits of patients receiving mechanical circulatory support (MCS) devices in end-stage heart failure management inference, in which hypothesis testing for a treatment effect on the risk of recurrent events is challenged in the presence of terminal events. Existing methods based on cumulative frequency unreasonably disadvantage longer survivors as they tend to experience more recurrent events. The While-Alive-based (WA) test has provided a solution to address this survival-length-bias problem, and it performs well when the recurrent event rate holds constant over time. However, if such a constant-rate assumption is violated, the WA test can exhibit an inflated type I error and inaccurate estimation of treatment effects. To fill this methodological gap, we propose a Proportional Rate Marginal Structural Model-assisted Test (PR-MSMaT) in the causal inference framework of separable treatment effects for recurrent and terminal events. Using the simulation study, we demonstrate that our PR-MSMaT can properly control type I error while gaining power comparable to the WA test under time-varying recurrent event rates. We employ PR-MSMaT to compare different MCS devices with the postoperative risk of gastrointestinal bleeding among patients enrolled in the Interagency Registry of Mechanically Assisted Circulatory Support program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01245v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <category>q-bio.TO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiyuan Huang, Ling Zhou, Min Zhang, Peter X. K. Song</dc:creator>
    </item>
    <item>
      <title>Errors-in-variables regression for dependent data with estimated error covariance matrix: To prewhiten or not?</title>
      <link>https://arxiv.org/abs/2601.01351</link>
      <description>arXiv:2601.01351v1 Announce Type: new 
Abstract: We consider statistical inference for errors-in-variables regression models with dependent observations under the high dimensionality of the error covariance matrix. It is tempting to prewhiten the model and data that had led to efficient weighted least squares estimation in the presence of the measurement errors, as being practised in the optimal fingerprinting approach in climate change studies. However, it is unclear to what extent the prewhitened estimator can improve the estimation efficiency of the unprewhitened estimator for errors-in-variables regression. We compare the prewhitening and unprewhitening estimators in terms of their estimation efficiency and computational cost. It shows that while the prewhitening operation does not necessarily improve the estimation efficiency of its unprewhitening counterpart, it demands more on the ensemble size needed in the error-covariance matrix estimation to ensure the asymptotic normality, and hence it would requires much more computationally resource.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01351v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingkun Qiu, Hanyue Chen, Song Xi Chen</dc:creator>
    </item>
    <item>
      <title>Cyclists Cardiac Conundrum</title>
      <link>https://arxiv.org/abs/2601.02011</link>
      <description>arXiv:2601.02011v1 Announce Type: new 
Abstract: Arrhythmia is an abnormality of the heart's rhythm, caused by problems in the conductive system and resulting in irregular heartbeats. There is increasing evidence that undertaking frequent endurance sports training elevates one's risk of arrhythmia. Arrhythmia is diagnosed using an electrocardiogram (ECG) but this is not typically available to athletes while exercising. Previous research by Crickles investigates the usefulness of commonly available heart rate data in detecting signs of arrhythmia. It is hypothesised that a feature termed 'gappiness', defined by jumps in the heart rate while the athlete is under exertion, may be a characteristic of arrhythmia. A correlation was found between the proportion of 'gappy' activities and survey responses about heart rhythm problems. We develop on this measure by exploring various methods to detect spikes in heart rate data, allowing us to describe the extent of irregularity in an activity via the rate of spikes. We first compare the performance of these methods on simulated data, where we find that smoothing using a moving average and setting a constant threshold on the residuals is most effective. This method was then implemented on real data provided by Crickles from 168 athletes, where no significant correlation was found between the spike rates and survey responses. However, when considering only those spikes that occur above a heart rate of 160 beats per minute (bpm) a significant correlation was found. This supports the hypothesis that jumps at only high heart rates are informative of arrhythmia and indicates the need for further research into better measures to characterise features of heart rate data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02011v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Nugent, Yi Ting Loo, Jack Buckingham</dc:creator>
    </item>
    <item>
      <title>Initial data analysis of the national German transplantation registry with a focus on kidney transplantation</title>
      <link>https://arxiv.org/abs/2601.02226</link>
      <description>arXiv:2601.02226v1 Announce Type: new 
Abstract: This study presents an Initial Data Analysis (IDA) of the German Transplantation Registry (TxReg) data for a better data understanding and to inform future data analyses. The IDA is focusing on data on first-time kidney-only transplantations in adult recipients from deceased donors between 2006 and 2016 and refers to data from 14,954 recipients and 9,964 donors across 25 tables. Investigated aspects include missing data patterns and structure, data consistency, and availability of event time data. Results show that missing data proportions vary widely, with some tables nearly complete while others have over 50% missing values. Missing data patterns are identified using a decision tree approach. An influx and outflux analysis demonstrates that some variables have high potential for imputing missing data, while others were less suitable for imputation. We identified 168 multi-sourced variables that are reported by multiple data providers in parallel leading to discrepancies for some variables but also providing opportunities for missing data imputation. Our findings on event time data demonstrate the importance of carefully selecting the variables used for event time analyses as results will strongly depend on this selection. In summary, our findings highlight the challenges when utilizing the TxReg data for research and provide recommendations for data preprocessing and analysis in future analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02226v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Klein (Martin Luther University Halle-Wittenberg, Halle, Germany, Darmstadt University of Applied Sciences, Darmstadt, Germany), Gunter Grieser (Darmstadt University of Applied Sciences, Darmstadt, Germany), Carl-Ludwig Fischer-Fr\"ohlich (Deutsche Stiftung Organtransplantation, Frankfurt am Main, Germany), Axel Rahmel (Deutsche Stiftung Organtransplantation, Frankfurt am Main, Germany), Henrik Stahl (Darmstadt University of Applied Sciences, Darmstadt, Germany), Andreas Wienke (Martin Luther University Halle-Wittenberg, Halle, Germany), Antje Jahn-Eimermacher (Darmstadt University of Applied Sciences, Darmstadt, Germany)</dc:creator>
    </item>
    <item>
      <title>Dynamic Disruption Resilience in Intermodal Transport Networks: Integrating Flow Weighting and Centrality Measures</title>
      <link>https://arxiv.org/abs/2601.00906</link>
      <description>arXiv:2601.00906v1 Announce Type: cross 
Abstract: Resilient intermodal freight networks are vital for sustaining supply chains amid increasing threats from natural hazards and cyberattacks. While transportation resilience has been widely studied, understanding how random and targeted disruptions affect both structural connectivity and functional performance remains a key challenge. To address this, our study evaluates the robustness of the U.S. intermodal freight network, comprising rail and water modes, using a simulation-based framework that integrates graph-theoretic metrics with flow-weighted centrality measures. We examine disruption scenarios including random failures as well as targeted node and edge removals based on static and dynamically updated degree and betweenness centrality. To reflect more realistic conditions, we also consider flow-weighted degree centralities and partial node degradation. Two resilience indicators are used: the size of the giant connected component (GCC) to measure structural connectivity, and flow-weighted network efficiency (NE) to assess freight mobility under disruption. Results show that progressively degrading nodes ranked by Weighted Degree Centrality to 60% of their original functionality causes a sharper decline in normalized NE, for up to approximately 45 affected nodes, than complete failure (100% loss of functionality) applied to nodes targeted by weighted betweenness centrality or selected at random. This highlights how partial degradation of high-tonnage hubs can produce disproportionately large functional losses. The findings emphasize the need for resilience strategies that go beyond network topology to incorporate freight flow dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00906v1</guid>
      <category>physics.soc-ph</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aliza Sharmin, Bharat Sharma, Mustafa Can Camur, Olufemi A. Omitaomu, Xueping Li</dc:creator>
    </item>
    <item>
      <title>Learned Hemodynamic Coupling Inference in Resting-State Functional MRI</title>
      <link>https://arxiv.org/abs/2601.00973</link>
      <description>arXiv:2601.00973v1 Announce Type: cross 
Abstract: Functional magnetic resonance imaging (fMRI) provides an indirect measurement of neuronal activity via hemodynamic responses that vary across brain regions and individuals. Ignoring this hemodynamic variability can bias downstream connectivity estimates. Furthermore, the hemodynamic parameters themselves may serve as important imaging biomarkers. Estimating spatially varying hemodynamics from resting-state fMRI (rsfMRI) is therefore an important but challenging blind inverse problem, since both the latent neural activity and the hemodynamic coupling are unknown. In this work, we propose a methodology for inferring hemodynamic coupling on the cortical surface from rsfMRI. Our approach avoids the highly unstable joint recovery of neural activity and hemodynamics by marginalizing out the latent neural signal and basing inference on the resulting marginal likelihood. To enable scalable, high-resolution estimation, we employ a deep neural network combined with conditional normalizing flows to accurately approximate this intractable marginal likelihood, while enforcing spatial coherence through priors defined on the cortical surface that admit sparse representations. The proposed approach is extensively validated using synthetic data and real fMRI datasets, demonstrating clear improvements over current methods for hemodynamic estimation and downstream connectivity analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00973v1</guid>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Consagra, Eardi Lila</dc:creator>
    </item>
    <item>
      <title>Tessellation Localized Transfer learning for nonparametric regression</title>
      <link>https://arxiv.org/abs/2601.00987</link>
      <description>arXiv:2601.00987v1 Announce Type: cross 
Abstract: Transfer learning aims to improve performance on a target task by leveraging information from related source tasks. We propose a nonparametric regression transfer learning framework that explicitly models heterogeneity in the source-target relationship. Our approach relies on a local transfer assumption: the covariate space is partitioned into finitely many cells such that, within each cell, the target regression function can be expressed as a low-complexity transformation of the source regression function. This localized structure enables effective transfer where similarity is present while limiting negative transfer elsewhere. We introduce estimators that jointly learn the local transfer functions and the target regression, together with fully data-driven procedures that adapt to unknown partition structure and transfer strength. We establish sharp minimax rates for target regression estimation, showing that local transfer can mitigate the curse of dimensionality by exploiting reduced functional complexity. Our theoretical guarantees take the form of oracle inequalities that decompose excess risk into estimation and approximation terms, ensuring robustness to model misspecification. Numerical experiments illustrate the benefits of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00987v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H\'el\`ene Halconruy, Benjamin Bobbia, Paul Lejamtel</dc:creator>
    </item>
    <item>
      <title>Modeling Information Blackouts in Missing Not-At-Random Time Series Data</title>
      <link>https://arxiv.org/abs/2601.01480</link>
      <description>arXiv:2601.01480v1 Announce Type: cross 
Abstract: Large-scale traffic forecasting relies on fixed sensor networks that often exhibit blackouts: contiguous intervals of missing measurements caused by detector or communication failures. These outages are typically handled under a Missing At Random (MAR) assumption, even though blackout events may correlate with unobserved traffic conditions (e.g., congestion or anomalous flow), motivating a Missing Not At Random (MNAR) treatment. We propose a latent state-space framework that jointly models (i) traffic dynamics via a linear dynamical system and (ii) sensor dropout via a Bernoulli observation channel whose probability depends on the latent traffic state. Inference uses an Extended Kalman Filter with Rauch-Tung-Striebel smoothing, and parameters are learned via an approximate EM procedure with a dedicated update for detector-specific missingness parameters. On the Seattle inductive loop detector data, introducing latent dynamics yields large gains over naive baselines, reducing blackout imputation RMSE from 7.02 (LOCF) and 5.02 (linear interpolation + seasonal naive) to 4.23 (MAR LDS), corresponding to about a 64% reduction in MSE relative to LOCF. Explicit MNAR modeling provides a consistent but smaller additional improvement on real data (imputation RMSE 4.20; 0.8% RMSE reduction relative to MAR), with similar modest gains for short-horizon post-blackout forecasts (evaluated at 1, 3, and 6 steps). In controlled synthetic experiments, the MNAR advantage increases as the true missingness dependence on latent state strengthens. Overall, temporal dynamics dominate performance, while MNAR modeling offers a principled refinement that becomes most valuable when missingness is genuinely informative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01480v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aman Sunesh (New York University), Allan Ma (New York University), Siddarth Nilol (New York University)</dc:creator>
    </item>
    <item>
      <title>Cubic lower record-based transmuted family of distributions: Theory, Estimation, Applications</title>
      <link>https://arxiv.org/abs/2601.01583</link>
      <description>arXiv:2601.01583v1 Announce Type: cross 
Abstract: In this study, a family of distributions called cubic lower record-based transmuted is provided. A special case of this family is proposed as an alternative exponential distribution. Several statistical properties are explored. We utilize nine different methods to estimate the parameters of the suggested distribution. In order to compare the performances of these methods, we consider a comprehensive Monte-Carlo simulation study. As a result of simulation study, we conclude that minimum absolute distance estimator is a valuable alternative to maximum likelihood estimator. Then, we carried out two real-world data examples to evaluate the fits of introduced distribution as well as its potential competitor ones. The findings of real-world data analysis show that the best-fitting distribution for both datasets is our model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01583v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caner Tan{\i}\c{s}</dc:creator>
    </item>
    <item>
      <title>On regional treatment effect assessment using robust MAP priors</title>
      <link>https://arxiv.org/abs/2601.01811</link>
      <description>arXiv:2601.01811v1 Announce Type: cross 
Abstract: Bayesian dynamic borrowing has become an increasingly important tool for evaluating the consistency of regional treatment effects which is a key requirement for local regulatory approval of a new drug. It helps increase the precision of regional treatment effect estimate when regional and global data are similar, while guarding against potential bias when they differ. In practice, the two-component mixture prior, of which one mixture component utilizes the power prior to incorporate external data, is widely used. It allows convenient prior specification, analytical posterior computation, and fast evaluation of operating characteristics. Though the robust meta-analytical-predictive (MAP) prior is broadly used with multiple external data sources, it remains underutilized for regional treatment effect assessment (typically only one external data source is available) due to its inherit complexity in prior specification and posterior computation. In this article, we illustrate the applicability of the robust MAP prior in the regional treatment effect assessment by developing a closed-form approximation for its posterior distribution while leveraging its relationship with the power prior. The proposed methodology substantially reduces the computational burden of identifying prior parameters for desired operating characteristics. Moreover, we have demonstrated that the MAP prior is an attractive choice to construct the informative component of the mixture prior compared to the power prior. The advantage can be explained through a Bayesian hypothesis testing perspective. Using a real-world example, we illustrate how our proposed method enables efficient and transparent development of a Bayesian dynamic borrowing design to show regional consistency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01811v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Zhang, Hui Zhang, Satrajit Roychoudhury</dc:creator>
    </item>
    <item>
      <title>Causal Network Recovery in Perturb-seq Experiments Using Proxy and Instrumental Variables</title>
      <link>https://arxiv.org/abs/2601.01830</link>
      <description>arXiv:2601.01830v1 Announce Type: cross 
Abstract: Emerging single-cell technologies that integrate CRISPR-based genetic perturbations with single-cell RNA sequencing, such as Perturb-seq, have substantially advanced our understanding of gene regulation and causal influence of genes. While Perturb-seq data provide valuable causal insights into gene-gene interactions, statistical concerns remain regarding unobserved confounders that may bias inference. These latent factors may arise not only from intrinsic molecular features of regulatory elements encoded in Perturb-seq experiments, but also from unobserved genes arising from cost-constrained experimental designs. Although methods for analyzing largescale Perturb-seq data are rapidly maturing, approaches that explicitly account for such unobserved confounders in learning the causal gene networks are still lacking. Here, we propose a novel method to recover causal gene networks from Perturb-seq experiments with robustness to arbitrarily omitted confounders. Our framework leverages proxy and instrumental variable strategies to exploit the rich information embedded in perturbations, enabling unbiased estimation of the underlying directed acyclic graph (DAG) of gene expressions. Simulation studies and analyses of CRISPR interference experiments of K562 cells demonstrate that our method outperforms baseline approaches that ignore unmeasured confounding, yielding more accurate and biologically relevant recovery of the true gene causal DAGs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01830v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kwangmoon Park, Hongzhe Li</dc:creator>
    </item>
    <item>
      <title>A Multilayered Approach to Classifying Customer Responsiveness and Credit Risk</title>
      <link>https://arxiv.org/abs/2601.01970</link>
      <description>arXiv:2601.01970v1 Announce Type: cross 
Abstract: This study evaluates the performance of various classifiers in three distinct models: response, risk, and response-risk, concerning credit card mail campaigns and default prediction. In the response model, the Extra Trees classifier demonstrates the highest recall level (79.1%), emphasizing its effectiveness in identifying potential responders to targeted credit card offers. Conversely, in the risk model, the Random Forest classifier exhibits remarkable specificity of 84.1%, crucial for identifying customers least likely to default. Furthermore, in the multi-class response-risk model, the Random Forest classifier achieves the highest accuracy (83.2%), indicating its efficacy in discerning both potential responders to credit card mail campaign and low-risk credit card users. In this study, we optimized various performance metrics to solve a specific credit risk and mail responsiveness business problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01970v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayomide Afolabi, Ebere Ogburu, Symon Kimitei</dc:creator>
    </item>
    <item>
      <title>Machine Learning Guided Cooling Optimization for Data Centers</title>
      <link>https://arxiv.org/abs/2601.02275</link>
      <description>arXiv:2601.02275v1 Announce Type: cross 
Abstract: Effective data center cooling is crucial for reliable operation; however, cooling systems often exhibit inefficiencies that result in excessive energy consumption. This paper presents a three-stage, physics-guided machine learning framework for identifying and reducing cooling energy waste in high-performance computing facilities. Using one year of 10-minute resolution operational data from the Frontier exascale supercomputer, we first train a monotonicity-constrained gradient boosting surrogate that predicts facility accessory power from coolant flow rates, temperatures, and server power. The surrogate achieves a mean absolute error of 0.026 MW and predicts power usage effectiveness within 0.01 of measured values for 98.7% of test samples. In the second stage, the surrogate serves as a physics-consistent baseline to quantify excess cooling energy, revealing approximately 85 MWh of annual inefficiency concentrated in specific months, hours, and operating regimes. The third stage evaluates guardrail-constrained counterfactual adjustments to supply temperature and subloop flows, demonstrating that up to 96% of identified excess can be recovered through small, safe setpoint changes while respecting thermal limits and operational constraints. The framework yields interpretable recommendations, supports counterfactual analyses such as flow reduction during low-load periods and redistribution of thermal duty across cooling loops, and provides a practical pathway toward quantifiable reductions in accessory power. The developed framework is readily compatible with model predictive control and can be extended to other liquid-cooled data centers with different configurations and cooling requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02275v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shrenik Jadhav, Zheng Liu</dc:creator>
    </item>
    <item>
      <title>A neighbour selection approach for identifying differential networks in conditional functional graphical models</title>
      <link>https://arxiv.org/abs/2601.02292</link>
      <description>arXiv:2601.02292v1 Announce Type: cross 
Abstract: Estimation of brain functional connectivity from EEG data is of great importance both for medical research and diagnosis. It involves quantifying the conditional dependencies among the activity of different brain areas from the time-varying electric field recorded by sensors placed outside the scalp. These dependencies may vary within and across individuals and be influenced by covariates such as age, mental status, or disease severity. Motivated by this problem, we propose a novel neighbour selection approach based on functional-on-functional regression for the characterization of conditional Gaussian functional graphical models. We provide a fully automated, data-driven procedure for inferring conditional dependence structures among observed functional variables. In particular, pairwise interactions are directly identified and allowed to vary as a function of covariates, enabling covariate-specific modulation of connectivity patterns. Our proposed method accommodates an arbitrary number of continuous and discrete covariates. Moreover, unlike existing methods for direct estimation of differential graphical models, the proposed approach yields directly interpretable coefficients, allowing discrimination between covariate-induced increases and decreases in interaction strength. The methodology is evaluated through extensive simulation studies and an application to experimental EEG data. The results demonstrate clear advantages over existing approaches, including higher estimation accuracy and substantially reduced computational cost, especially in high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02292v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessia Mapelli, Laura Carini, Francesca Ieva, Sara Sommariva</dc:creator>
    </item>
    <item>
      <title>Multilevel Regression and Poststratification Interface: An Application to Track Community-level COVID-19 Viral Transmission</title>
      <link>https://arxiv.org/abs/2405.05909</link>
      <description>arXiv:2405.05909v3 Announce Type: replace 
Abstract: We present a novel Bayesian workflow for multilevel regression and poststratification (MRP), introducing extensions to time-varying data and granular geography and publicly available open-source computation tools, facilitating broad research adoption and reproducibility. In the absence of comprehensive or random testing throughout the COVID-19 pandemic, we have developed a proxy method for synthetic random sampling to estimate community-level viral incidence, based on viral RNA testing of asymptomatic patients who present for elective procedures within a hospital system. The approach collects routine testing data on SARS-CoV-2 exposure among outpatients and performs statistical adjustments of sample representation using MRP, a procedure that adjusts for nonrepresentativeness of the sample and yields stable small group estimates. We illustrate the MRP interface with an application to track community-level COVID-19 viral transmission in the state of Michigan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05909v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yajuan Si, Toan Tran, Jonah Gabry, Mitzi Morris, Andrew Gelman</dc:creator>
    </item>
    <item>
      <title>Is Stephen Curry really a guard? A new perspective on player typologies using functional data analysis</title>
      <link>https://arxiv.org/abs/2504.21761</link>
      <description>arXiv:2504.21761v2 Announce Type: replace 
Abstract: We present a novel representation of NBA players' shooting patterns based on Functional Data Analysis (FDA). Each player's charts of made and missed shots are treated as smooth functional data defined over a two-dimensional domain corresponding to the offensive half-court. This continuous representation enables a parsimonious multivariate functional principal components analysis (MFPCA) decomposition, producing a set of common principal component functions that capture the primary modes of variability in shooting patterns, along with player-specific scores that quantify individual deviations from the average behavior. We first interpret the principal component functions to characterize the main sources of variation in shooting tendencies. We then apply $k$-medoids clustering to the principal component scores to construct a data-driven taxonomy of players. Comparing our empirical clusters to conventional NBA position labels reveals low agreement, suggesting that our shooting-pattern representation might capture aspects of playing style not fully reflected in official designations. The proposed methodology provides a flexible, interpretable, and continuous framework for analyzing player tendencies, with potential applications in coaching, scouting, and historical player or match comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21761v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven Golovkine, Edward Gunning</dc:creator>
    </item>
    <item>
      <title>Tractable Algorithms for Changepoint Detection in Player Performance Metrics</title>
      <link>https://arxiv.org/abs/2510.25961</link>
      <description>arXiv:2510.25961v2 Announce Type: replace 
Abstract: We present a tractable framework for detecting changes in performance metrics and apply these methods to Major League Baseball (MLB) batting and pitching data from the 2023 and 2024 seasons. We propose a changepoint detection algorithm that combines a likelihood-based approach with split-sample inference to better control false positives, using either nonparametric tests or tests appropriate to the underlying data distribution. These tests incorporate a shift parameter, allowing users to specify the minimum magnitude of change to detect. We demonstrate the utility of this approach across simulation studies and several baseball applications: detecting changes in batter plate discipline metrics (e.g., chase and whiff rate), identifying velocity changes in pitcher fastballs, and validating velocity changepoints against a curated quasi-ground-truth dataset of pitchers who transitioned from relief to starting roles. Our method flags meaningful changes in 91% of these "ground-truth" cases and reveals that, for some metrics, more than 60% of detected changes occur in-season. While developed for baseball, the proposed framework is broadly applicable to any setting involving monitoring of individual performance over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25961v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amanda Glazer</dc:creator>
    </item>
    <item>
      <title>Climatic Extremes in Brazil: a parallel analysis of Historical Trends and Socioeconomical Impacts</title>
      <link>https://arxiv.org/abs/2409.16309</link>
      <description>arXiv:2409.16309v2 Announce Type: replace-cross 
Abstract: An important consequence of human induced climate change is the increase in extreme weather events. This study contributes to the understanding of Brazil's climate change by examining historical temperature and precipitation patterns. Extreme events of temperature and precipitation are identified using data from the Brazilian Institute of Meteorology, which includes records from 634 meteorological stations operating intermittently since 1961. Using the first 30 years (1961 to 1990) as the reference period, our results show a significant increase in warm days and a corresponding decrease in cold days over the last 30 years (1991 to 2020), in agreement with previous works. In terms of precipitation, it indicates a trend toward drier conditions in the Northeast region of Brazil, whereas the South is experiencing wetter conditions, with an increase in the number of heavy precipitation days in South and in the extremely dry periods in the Northeast. These results have been verified for consistency with several extreme climate indices measured in this study. Additionally, data from S2iD is analyzed, an official database that records natural disasters in Brazil, to estimate their impact in terms of human losses and financial costs over the past decade. Our findings indicate that drought events are the most economically costly, with multiple instances causing damages exceeding a billion USD, whereas storms have the greatest impact on people. Although it is not possible to directly attribute the natural disasters recorded in the S2iD database to the extreme weather events identified through meteorological data, discussion is done on potential implications of these events in the frequency and location of the disasters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16309v2</guid>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Davi Lazzari, Am\'alia Garcez, Nicole Poltozi, Gianluca Pozzi, Carolina Brito</dc:creator>
    </item>
    <item>
      <title>The ECME Algorithm Using Factor Analysis for DOA Estimation in Nonuniform Noise</title>
      <link>https://arxiv.org/abs/2508.02223</link>
      <description>arXiv:2508.02223v3 Announce Type: replace-cross 
Abstract: Factor analysis (FA) plays a critical role in psychometrics, econometrics, and statistics. Recently, maximum likelihood FA (MLFA) has been applied to direction of arrival (DOA) estimation in unknown nonuniform noise and a variety of iterative approaches have been developed. In particular, the Factor Analysis for Anisotropic Noise (FAAN) method proposed by Stoica and Babu has excellent convergence properties. In this article, the Expectation/Conditional Maximization Either (ECME) algorithm, an extension of the expectation-maximization algorithm, is designed again for MLFA by introducing new complete data, which can thus use two explicit formulas to sequentially update the estimates of parameters at each iteration and have excellent convergence properties. Theoretical analysis shows that the ECME algorithm has almost the same computational complexity at each iteration as the FAAN method. However, numerical results show that the ECME algorithm yields faster stable convergence and the convergence to the global optimum is easier. Importantly, MLFA is not the best choice for the subspace based DOA estimation in unknown nonuniform noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02223v3</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyan Gong</dc:creator>
    </item>
    <item>
      <title>Digital Twins as Funhouse Mirrors: Five Key Distortions</title>
      <link>https://arxiv.org/abs/2509.19088</link>
      <description>arXiv:2509.19088v4 Announce Type: replace-cross 
Abstract: Scientists and practitioners are aggressively moving to deploy digital twins - LLM-based models of real individuals - across social science and policy research. We conducted 19 pre-registered studies with 164 diverse outcomes (e.g., attitudes towards hiring algorithms, intention to share misinformation) and compared human responses to those of their digital twins (trained on each person's previous answers to over 500 questions). We find that digital twins' answers are only modestly more accurate than those from the homogeneous base LLM and correlate weakly with human responses (average r = 0.20). We document five ways in which digital twins distort human behavior: (i) stereotyping, (ii) insufficient individuation, (iii) representation bias, (iv) ideological biases, and (v) hyper-rationality. Together, our results caution against the premature deployment of digital twins, which may systematically misrepresent human cognition and undermine both scientific understanding and practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19088v4</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyi Peng, George Gui, Melanie Brucks, Daniel J. Merlau, Grace Jiarui Fan, Malek Ben Sliman, Eric J. Johnson, Abdullah Althenayyan, Silvia Bellezza, Dante Donati, Hortense Fong, Elizabeth Friedman, Ariana Guevara, Mohamed Hussein, Kinshuk Jerath, Bruce Kogut, Akshit Kumar, Kristen Lane, Hannah Li, Vicki Morwitz, Oded Netzer, Patryk Perkowski, Olivier Toubia</dc:creator>
    </item>
    <item>
      <title>Do Test Scores Help Teachers Give Better Track Advice to Students? A Principal Stratification Analysis</title>
      <link>https://arxiv.org/abs/2511.05128</link>
      <description>arXiv:2511.05128v2 Announce Type: replace-cross 
Abstract: We study whether access to standardized test scores improves the quality of teachers' secondary school track recommendations, using Dutch data and a metric based on Principal Stratification in a quasi-randomized setting. Allowing teachers to revise their recommendations when test results exceed expectations increases the share of students successfully placed in more demanding tracks by at least 6%, but misplaces 7% of weaker students. However, only implausibly high weights on the short-term losses of students who must change track because of misplacement would justify prohibiting test-score-based upgrades. Access to test scores also induces fairer recommendations for immigrant and low-SES students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05128v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Ichino, Fabrizia Mealli, Javier Viviens</dc:creator>
    </item>
    <item>
      <title>How to Correctly Report LLM-as-a-Judge Evaluations</title>
      <link>https://arxiv.org/abs/2511.21140</link>
      <description>arXiv:2511.21140v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are widely used as scalable evaluators of model responses in lieu of human annotators. However, imperfect sensitivity and specificity of LLM judgments induce bias in naive evaluation scores. We propose a simple plug-in framework that corrects this bias and constructs confidence intervals accounting for uncertainty from both the test dataset and a human-evaluated calibration dataset, enabling statistically sound and practical LLM-based evaluation. Building on this framework, we introduce an adaptive calibration strategy for constructing the calibration dataset to reduce uncertainty in the estimated score. Notably, we characterize the regimes in which LLM-based evaluation within our framework produces more reliable estimates than fully human evaluation. Moreover, our framework is more robust to distribution shift between the test and calibration datasets than existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21140v2</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Chungpa Lee, Thomas Zeng, Jongwon Jeong, Jy-yong Sohn, Kangwook Lee</dc:creator>
    </item>
    <item>
      <title>Assessing the Effects of Macroeconomic Variables on Child Mortality in D-8 Countries Using Panel Data Analysis</title>
      <link>https://arxiv.org/abs/2512.23110</link>
      <description>arXiv:2512.23110v2 Announce Type: replace-cross 
Abstract: This research analyses the axiomatic link among health expenditures, inflation rate, and gross national income (GNI) per capita concerning the child mortality (CMU5) rate in D-8 nations, employing panel data analysis from 1995 to 2014. Utilising conventional panel unit root tests and linear regression models, we establish that education expenditures, in conjunction with health expenditures, inflation rate, and GNI per capita, display stationarity at level. Additionally, we examine fixed effects and random effects estimators for the pertinent variables, utilising metrics such as the Hausman Test (HT) and comparisons with CCMR correlations. Our data demonstrate that the CMU5 rate in D-8 nations has steadily decreased, according to a somewhat negative linear regression model, therefore slightly undermining the fourth Millennium Development Goal (MDG4) of the World Health Organisation (WHO).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23110v2</guid>
      <category>econ.TH</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Waseem Akram, Binita Shahi, M. Javed Akram</dc:creator>
    </item>
  </channel>
</rss>
