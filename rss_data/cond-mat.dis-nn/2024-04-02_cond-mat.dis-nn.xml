<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Apr 2024 19:07:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 02 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Real-space renormalisation approach to the Chalker-Coddington model revisited: improved statistics</title>
      <link>https://arxiv.org/abs/2404.00660</link>
      <description>arXiv:2404.00660v1 Announce Type: new 
Abstract: The real-space renormalisation group method can be applied to the Chalker-Coddington model of the quantum Hall transition to provide a convenient numerical estimation of the localisation critical exponent, $\nu$. Previous such studies found $\nu\sim 2.39$ which falls considerably short of the current best estimates by transfer matrix ($\nu\approx 2.593$) and exact-diagonalisation studies ($\nu=2.58(3)$). By increasing the amount of data $500$ fold we can now measure closer to the critical point and find an improved estimate $\nu\approx 2.51$. This deviates only $\sim 3\%$ from the previous two values and is already better than the $\sim 7\%$ accuracy of the classical small-cell renormalisation approach from which our method is adapted. We also study a previously proposed mixing of the Chalker-Coddington model with a classical scattering model which is meant to provide a route to understanding why experimental estimates give a lower $\nu\sim 2.3$. Upon implementing this mixing into our RG unit, we find only further increases to the value of $\nu$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00660v1</guid>
      <category>cond-mat.dis-nn</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Syl Shaw, Rudolf A. R\"omer</dc:creator>
    </item>
    <item>
      <title>Dual-Unitary Classical Shadow Tomography</title>
      <link>https://arxiv.org/abs/2404.01068</link>
      <description>arXiv:2404.01068v1 Announce Type: cross 
Abstract: We study operator spreading in random dual-unitary circuits within the context of classical shadow tomography. Primarily, we analyze the dynamics of the Pauli weight in one-dimensional qubit systems evolved by random two-local dual-unitary gates arranged in a brick-wall structure, ending with a final measurement layer. We do this by deriving general constraints on the Pauli weight transfer matrix and specializing to the case of dual-unitarity. We first show that dual-unitaries must have a minimal amount of entropy production. Remarkably, we find that operator spreading in these circuits has a rich structure resembling that of relativistic quantum field theories, with massless chiral excitations that can decay or fuse into each other, which we call left- or right-movers. We develop a mean-field description of the Pauli weight in terms of $\rho(x,t)$, which represents the probability of having nontrivial support at site $x$ and depth $t$ starting from a fixed weight distribution. We develop an equation of state for $\rho(x,t)$, and simulate it numerically using Monte Carlo simulations. Lastly, we demonstrate that the fast-thermalizing properties of dual-unitary circuits make them better at predicting large operators than shallow brick-wall Clifford circuits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01068v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmed A. Akhtar, Namit Anand, Jeffrey Marshall, Yi-Zhuang You</dc:creator>
    </item>
    <item>
      <title>Parisi's hypercube, Fock-space frustration and near-AdS$_2$/near-CFT$_1$ holography</title>
      <link>https://arxiv.org/abs/2303.18182</link>
      <description>arXiv:2303.18182v2 Announce Type: replace-cross 
Abstract: We consider a model of Parisi where a single particle hops on an infinite-dimensional hypercube, under the influence of a uniform but disordered magnetic flux. We reinterpret the hypercube as the Fock-space graph of a many-body Hamiltonian and the flux as a frustration of the return amplitudes in Fock space. We will identify the set of observables that have the same correlation functions as the double-scaled Sachdev-Ye-Kitaev (DS-SYK) model, and hence the hypercube model is an equally good quantum model for near-AdS$_2$/near-CFT$_{1}$ (NAdS$_2$/NCFT$_1$) holography. Unlike the SYK model, the hypercube Hamiltonian is not $p$ local. Instead, the SYK model can be understood as a Fock-space model with similar frustrations. Hence we propose this type of Fock-space frustration as the broader characterization for NAdS$_2$/NCFT$_1$ microscopics, which encompasses the hypercube and the DS-SYK models as two specific examples. We then speculate on the possible origin of such frustrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.18182v2</guid>
      <category>hep-th</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.str-el</category>
      <category>nlin.CD</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevLett.132.081601</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. Lett. 132 (2024), 081601</arxiv:journal_reference>
      <dc:creator>Micha Berkooz, Yiyang Jia, Navot Silberstein</dc:creator>
    </item>
    <item>
      <title>iMixer: hierarchical Hopfield network implies an invertible, implicit and iterative MLP-Mixer</title>
      <link>https://arxiv.org/abs/2304.13061</link>
      <description>arXiv:2304.13061v2 Announce Type: replace-cross 
Abstract: In the last few years, the success of Transformers in computer vision has stimulated the discovery of many alternative models that compete with Transformers, such as the MLP-Mixer. Despite their weak inductive bias, these models have achieved performance comparable to well-studied convolutional neural networks. Recent studies on modern Hopfield networks suggest the correspondence between certain energy-based associative memory models and Transformers or MLP-Mixer, and shed some light on the theoretical background of the Transformer-type architectures design. In this paper, we generalize the correspondence to the recently introduced hierarchical Hopfield network, and find iMixer, a novel generalization of MLP-Mixer model. Unlike ordinary feedforward neural networks, iMixer involves MLP layers that propagate forward from the output side to the input side. We characterize the module as an example of invertible, implicit, and iterative mixing module. We evaluate the model performance with various datasets on image classification tasks, and find that iMixer, despite its unique architecture, exhibits stable learning capabilities and achieves performance comparable to or better than the baseline vanilla MLP-Mixer. The results imply that the correspondence between the Hopfield networks and the Mixer models serves as a principle for understanding a broader class of Transformer-like architecture designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.13061v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toshihiro Ota, Masato Taki</dc:creator>
    </item>
    <item>
      <title>The twin peaks of learning neural networks</title>
      <link>https://arxiv.org/abs/2401.12610</link>
      <description>arXiv:2401.12610v2 Announce Type: replace-cross 
Abstract: Recent works demonstrated the existence of a double-descent phenomenon for the generalization error of neural networks, where highly overparameterized models escape overfitting and achieve good test performance, at odds with the standard bias-variance trade-off described by statistical learning theory. In the present work, we explore a link between this phenomenon and the increase of complexity and sensitivity of the function represented by neural networks. In particular, we study the Boolean mean dimension (BMD), a metric developed in the context of Boolean function analysis. Focusing on a simple teacher-student setting for the random feature model, we derive a theoretical analysis based on the replica method that yields an interpretable expression for the BMD, in the high dimensional regime where the number of data points, the number of features, and the input size grow to infinity. We find that, as the degree of overparameterization of the network is increased, the BMD reaches an evident peak at the interpolation threshold, in correspondence with the generalization error peak, and then slowly approaches a low asymptotic value. The same phenomenology is then traced in numerical experiments with different model classes and training setups. Moreover, we find empirically that adversarially initialized models tend to show higher BMD values, and that models that are more robust to adversarial attacks exhibit a lower BMD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12610v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elizaveta Demyanenko, Christoph Feinauer, Enrico M. Malatesta, Luca Saglietti</dc:creator>
    </item>
    <item>
      <title>X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Molecular Design</title>
      <link>https://arxiv.org/abs/2402.07148</link>
      <description>arXiv:2402.07148v2 Announce Type: replace-cross 
Abstract: We report a mixture of expert strategy to create fine-tuned large language models using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, our gating strategy uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations to solve tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing large language model (LLM) without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced reasoning capability, focused on biomaterial analysis, protein mechanics and design. The impact of this work include access to readily expandable and adaptable models with strong domain knowledge and the capability to integrate across areas of knowledge. Featuring experts in biology, mathematics, reasoning, bio-inspired materials, mechanics and materials, chemistry, protein biophysics, mechanics and quantum-mechanics based molecular properties, we conduct a series of physics-focused case studies. We examine knowledge recall, protein mechanics forward/inverse tasks, protein design, adversarial agentic modeling including ontological knowledge graph construction, as well as molecular design. The model is capable not only of making quantitative predictions of nanomechanical properties of proteins or quantum mechanical molecular properties, but also reasons over the results and correctly predicts likely mechanisms that explain distinct molecular behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07148v2</guid>
      <category>cond-mat.soft</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Eric L. Buehler, Markus J. Buehler</dc:creator>
    </item>
  </channel>
</rss>
