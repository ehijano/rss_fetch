<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Oct 2024 02:05:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Low-degree spanning trees of $2$-edge-connected graphs in linear time</title>
      <link>https://arxiv.org/abs/2410.20137</link>
      <description>arXiv:2410.20137v1 Announce Type: new 
Abstract: We present a simple linear-time algorithm that finds a spanning tree $T$ of a given $2$-edge-connected graph $G$ such that each vertex $v$ of $T$ has degree at most $\lceil \frac{\deg_G(v)}{2}\rceil + 1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20137v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dariusz Dereniowski, Janusz Dybizba\'nski, Przemys{\l}aw Karpi\'nski, Micha{\l} Zakrzewski, Pawe{\l} \.Zyli\'nski</dc:creator>
    </item>
    <item>
      <title>On the I/O Complexity of the CYK Algorithm and of a Family of Related DP Algorithms</title>
      <link>https://arxiv.org/abs/2410.20337</link>
      <description>arXiv:2410.20337v1 Announce Type: new 
Abstract: Asymptotically tight lower bounds are derived for the Input/Output (I/O) complexity of a class of dynamic programming algorithms including matrix chain multiplication, optimal polygon triangulation, and the construction of optimal binary search trees. Assuming no recomputation of intermediate values, we establish an $\Omega\left(\frac{n^3}{\sqrt{M}B}\right)$ I/O lower bound, where $n$ denotes the size of the input and $M$ denotes the size of the available fast memory (cache). When recomputation is allowed, we show the same bound holds for $M &lt; cn$, where $c$ is a positive constant. In the case where $M \ge 2n$, we show an $\Omega\left(n/B\right)$ I/O lower bound. We also discuss algorithms for which the number of executed I/O operations matches asymptotically each of the presented lower bounds, which are thus asymptotically tight.
  Additionally, we refine our general method to obtain a lower bound for the I/O complexity of the Cocke-Younger-Kasami algorithm, where the size of the grammar impacts the I/O complexity. An upper bound with asymptotically matching performance in many cases is also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20337v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo De Stefani, Vedant Gupta</dc:creator>
    </item>
    <item>
      <title>Improved Online Reachability Preservers</title>
      <link>https://arxiv.org/abs/2410.20471</link>
      <description>arXiv:2410.20471v1 Announce Type: new 
Abstract: A reachability preserver is a basic kind of graph sparsifier, which preserves the reachability relation of an $n$-node directed input graph $G$ among a set of given demand pairs $P$ of size $|P|=p$. We give constructions of sparse reachability preservers in the online setting, where $G$ is given on input, the demand pairs $(s, t) \in P$ arrive one at a time, and we must irrevocably add edges to a preserver $H$ to ensure reachability for the pair $(s, t)$ before we can see the next demand pair. Our main results are:
  -- There is a construction that guarantees a maximum preserver size of $$|E(H)| \le O\left( n^{0.72}p^{0.56} + n^{0.6}p^{0.7} + n\right).$$ This improves polynomially on the previous online upper bound of $O( \min\{np^{0.5}, n^{0.5}p\}) + n$, implicit in the work of Coppersmith and Elkin [SODA '05].
  -- Given a promise that the demand pairs will satisfy $P \subseteq S \times V$ for some vertex set $S$ of size $|S|=:\sigma$, there is a construction that guarantees a maximum preserver size of $$|E(H)| \le O\left( (np\sigma)^{1/2} + n\right).$$ A slightly different construction gives the same result for the setting $P \subseteq V \times S$. This improves polynomially on the previous online upper bound of $O( \sigma n)$ (folklore).
  All of these constructions are polynomial time, deterministic, and they do not require knowledge of the values of $p, \sigma$, or $S$. Our techniques also give a small polynomial improvement in the current upper bounds for offline reachability preservers, and they extend to a stronger model in which we must commit to a path for all possible reachable pairs in $G$ before any demand pairs have been received. As an application, we improve the competitive ratio for Online Unweighted Directed Steiner Forest to $O(n^{3/5 + \varepsilon})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20471v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Greg Bodwin, Tuong Le</dc:creator>
    </item>
    <item>
      <title>On Sparsest Cut and Conductance in Directed Polymatroidal Networks</title>
      <link>https://arxiv.org/abs/2410.20525</link>
      <description>arXiv:2410.20525v1 Announce Type: new 
Abstract: We consider algorithms and spectral bounds for sparsest cut and conductance in directed polymatrodal networks. This is motivated by recent work on submodular hypergraphs \cite{Yoshida19,LiM18,ChenOT23,Veldt23} and previous work on multicommodity flows and cuts in polymatrodial networks \cite{ChekuriKRV15}. We obtain three results. First, we obtain an $O(\sqrt{\log n})$-approximation for sparsest cut and point out how this generalizes the result in \cite{ChenOT23}. Second, we consider the symmetric version of conductance and obtain an $O(\sqrt{OPT \log r})$ approximation where $r$ is the maximum degree and we point out how this generalizes previous work on vertex expansion in graphs. Third, we prove a non-constructive Cheeger like inequality that generalizes previous work on hypergraphs. We provide a unified treatment via line-embeddings which were shown to be effective for submodular cuts in \cite{ChekuriKRV15}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20525v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chandra Chekuri, Anand Louis</dc:creator>
    </item>
    <item>
      <title>A New Method for Inserting Train Paths into a Timetable</title>
      <link>https://arxiv.org/abs/2410.20561</link>
      <description>arXiv:2410.20561v1 Announce Type: new 
Abstract: A seemingly simple, yet widely applicable subroutine in automated train scheduling is the insertion of a new train path to a timetable in a railway network. We believe it to be the first step towards a new train-rerouting framework in case of large disturbances or maintenance works. Other applications include handling ad-hoc requests and modifying train paths upon request from railway undertakings. We propose a fast and scalable path-insertion algorithm based on dynamic programming that is able to output multiple suitable paths. Our algorithm uses macroscopic data and can run on railway networks with any number of tracks. We apply the algorithm on the line from G\"oteborg S\"aven\"as to the Norwegian border at Kornsj\"o. For a time window of seven hours, we obtain eight suitable paths for a freight train within 0.3 seconds after preprocessing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20561v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Dekker, Carl Henrik H\"all, Anders Peterson, Christiane Schmidt</dc:creator>
    </item>
    <item>
      <title>New Applications of 3SUM-Counting in Fine-Grained Complexity and Pattern Matching</title>
      <link>https://arxiv.org/abs/2410.20764</link>
      <description>arXiv:2410.20764v1 Announce Type: new 
Abstract: The 3SUM problem is one of the cornerstones of fine-grained complexity. Its study has led to countless lower bounds, but as has been sporadically observed before -- and as we will demonstrate again -- insights on 3SUM can also lead to algorithmic applications.
  The starting point of our work is that we spend a lot of technical effort to develop new algorithms for 3SUM-type problems such as approximate 3SUM-counting, small-doubling 3SUM-counting, and a deterministic subquadratic-time algorithm for the celebrated Balog-Szemer\'edi-Gowers theorem from additive combinatorics. As consequences of these tools, we derive diverse new results in fine-grained complexity and pattern matching algorithms, answering open questions from many unrelated research areas. Specifically:
  - A recent line of research on the "short cycle removal" technique culminated in tight 3SUM-based lower bounds for various graph problems via randomized fine-grained reductions [Abboud, Bringmann, Fischer; STOC '23] [Jin, Xu; STOC '23]. In this paper we derandomize the reduction to the important 4-Cycle Listing problem.
  - We establish that \#3SUM and 3SUM are fine-grained equivalent under deterministic reductions.
  - We give a deterministic algorithm for the $(1+\epsilon)$-approximate Text-to-Pattern Hamming Distances problem in time $n^{1+o(1)} \cdot \epsilon^{-1}$.
  - In the $k$-Mismatch Constellation problem the input consists of two integer sets $A, B \subseteq [N]$, and the goal is to test whether there is a shift $c$ such that $|(c + B) \setminus A| \leq k$ (i.e., whether $B$ shifted by $c$ matches $A$ up to $k$ mismatches). For moderately small $k$ the previously best running time was $\tilde O(|A| \cdot k)$ [Cardoze, Schulman; FOCS '98] [Fischer; SODA '24]. We give a faster $|A| \cdot k^{2/3} \cdot N^{o(1)}$-time algorithm in the regime where $|B| = \Theta(|A|)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20764v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick Fischer, Ce Jin, Yinzhan Xu</dc:creator>
    </item>
    <item>
      <title>Parameterized Approximation for Capacitated $d$-Hitting Set with Hard Capacities</title>
      <link>https://arxiv.org/abs/2410.20900</link>
      <description>arXiv:2410.20900v1 Announce Type: new 
Abstract: The \textsc{Capacitated $d$-Hitting Set} problem involves a universe $U$ with a capacity function $\mathsf{cap}: U \rightarrow \mathbb{N}$ and a collection $\mathcal{A}$ of subsets of $U$, each of size at most $d$. The goal is to find a minimum subset $S \subseteq U$ and an assignment $\phi : \mathcal{A} \rightarrow S$ such that for every $A \in \mathcal{A}$, $\phi(A) \in A$, and for each $x \in U$, $|\phi^{-1}(x)| \leq \mathsf{cap}(x)$. For $d=2$, this is known as \textsc{Capacitated Vertex Cover}. In the weighted variant, each element of $U$ has a positive integer weight, with the objective of finding a minimum-weight capacitated hitting set.
  Chuzhoy and Naor [SICOMP 2006] provided a factor-3 approximation for \textsc{Capacitated Vertex Cover} and showed that the weighted case lacks an $o(\log n)$-approximation unless $P=NP$. Kao and Wong [SODA 2017] later independently achieved a $d$-approximation for \textsc{Capacitated $d$-Hitting Set}, with no $d - \epsilon$ improvements possible under the Unique Games Conjecture. Our main result is a parameterized approximation algorithm with runtime $\left(\frac{k}{\epsilon}\right)^k 2^{k^{O(kd)}}(|U|+|\mathcal{A}|)^{O(1)}$ that either concludes no solution of size $\leq k$ exists or finds $S$ of size $\leq 4/3 \cdot k$ and weight at most $2+\epsilon$ times the minimum weight for solutions of size $\leq k$. We further show that no FPT-approximation with factor $c &gt; 1$ exists for unweighted \textsc{Capacitated $d$-Hitting Set} with $d \geq 3$, nor with factor $2 - \epsilon$ for the weighted version, assuming the Exponential Time Hypothesis. These results extend to \textsc{Capacitated Vertex Cover} in multigraphs. Additionally, a variant of multi-dimensional \textsc{Knapsack} is shown hard to FPT-approximate within $2 - \epsilon$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20900v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Lokshtanov, Abhishek Sahu, Saket Saurabh, Vaishali Surianarayanan, Jie Xue</dc:creator>
    </item>
    <item>
      <title>Popping Bubbles in Pangenome Graphs</title>
      <link>https://arxiv.org/abs/2410.20932</link>
      <description>arXiv:2410.20932v1 Announce Type: new 
Abstract: In this paper, we introduce flubbles, a new definition of "bubbles" corresponding to variants in a (pan)genome graph $G$. We then show a characterization for flubbles in terms of equivalence classes regarding cycles in an intermediate data structure we built from the spanning tree of the $G$, which leads us to a linear time and space solution for finding all flubbles. Furthermore, we show how a related characterization also allows us to efficiently detect what we define as hairpin inversions: a cycle preceded and followed by the same path in the graph; being the latter necessarily traversed both ways, this structure corresponds to inversions. Finally, Inspired by the concept of Program Structure Tree introduced fifty years ago to represent the hierarchy of the control structure of a program, we define a tree representing the structure of G in terms of flubbles, the flubble tree, which we also find in linear time. The hierarchy of variants introduced by the flubble tree paves the way for new investigations of (pan)genomic structures and their decomposition for practical analyses. We have implemented our methods into a prototype tool named povu which we tested on human and yeast data. We show that povu can find flubbles and also output the flubble tree while being as fast (or faster than) well established tools that find bubbles, such as vg and BubbleGun. Moreover, we show how, within the same time, povu can find hairpin inversions that, to the best of our knowledge, no other tool is able to find. Our tool is freely available at https://github.com/urbanslug/povu/ under the MIT License.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20932v1</guid>
      <category>cs.DS</category>
      <category>q-bio.GN</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Njagi Mwaniki, Erik Garrison, Nadia Pisanti</dc:creator>
    </item>
    <item>
      <title>A Simple Parallel Algorithm with Near-Linear Work for Negative-Weight Single-Source Shortest Paths</title>
      <link>https://arxiv.org/abs/2410.20959</link>
      <description>arXiv:2410.20959v1 Announce Type: new 
Abstract: We give the first parallel algorithm with optimal $\tilde{O}(m)$ work for the classical problem of computing Single-Source Shortest Paths in general graphs with negative-weight edges.
  In graphs without negative edges, Dijkstra's algorithm solves the Single-Source Shortest Paths (SSSP) problem with optimal $\tilde O(m)$ work, but is inherently sequential. A recent breakthrough by Bernstein, Nanongkai, Wulff-Nilsen; FOCS '22 achieves the same for general graphs. Parallel shortest path algorithms are more difficult and have been intensely studied for decades. Only very recently, multiple lines of research culminated in parallel algorithms with optimal work $\tilde O(m)$ for various restricted settings, such as approximate or exact algorithms for directed or undirected graphs without negative edges. For general graphs, the best known algorithm by [shvinkumar, Bernstein, Cao, Grunau, Haeupler, Jiang, Nanongkai, Su; ESA '24 still requires $m^{1+o(1)}$ work.
  This paper presents a randomized parallel algorithm for SSSP in general graphs with near-linear work $\tilde O(m)$ and state-of-the-art span $n^{1/2 + o(1)}$. We follow a novel bottom-up approach leading to a particularly clean and simple algorithm. Our algorithm can be seen as a \emph{near-optimal parallel black-box reduction} from SSSP in general graphs to graphs without negative edges. In contrast to prior works, the reduction in this paper is both parallel and essentially without overhead, only affecting work and span by polylogarithmic factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20959v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nick Fischer, Bernhard Haeupler, Rustam Latypov, Antti Roeyskoe, Aurelio L. Sulser</dc:creator>
    </item>
    <item>
      <title>Matrix-by-matrix multiplication algorithm with $O(N^2log_2N)$ computational complexity for variable precision arithmetic</title>
      <link>https://arxiv.org/abs/2410.21050</link>
      <description>arXiv:2410.21050v1 Announce Type: new 
Abstract: We show that assuming the availability of the processor with variable precision arithmetic, we can compute matrix-by-matrix multiplications in $O(N^2log_2N)$ computational complexity. We replace the standard matrix-by-matrix multiplications algorithm $\begin{bmatrix}A_{11}&amp;A_{12}\\A_{21}&amp;A_{22}\end{bmatrix}\begin{bmatrix}B_{11}&amp;B_{12}\\B_{21}&amp;B_{22}\end{bmatrix}=\begin{bmatrix}A_{11}B_{11}+A_{12}B_{21}&amp;A_{11}B_{12}+A_{12}B_{22}\\A_{21}B_{11}+A_{22}B_{21}&amp;A_{21}B_{12}+A_{22}B_{22}\end{bmatrix}$ by $\begin{bmatrix}A_{11}&amp;A_{12}\\A_{21}&amp;A_{22}\end{bmatrix}\begin{bmatrix}B_{11}&amp;B_{12}\\B_{21}&amp;B_{22}\end{bmatrix}=\Bigl\lfloor\begin{bmatrix} (A_{11}+\epsilon A_{12})(B_{11}+1/{\epsilon}B_{21})&amp;(A_{11}+\epsilon A_{12})(B_{12}+1/{\epsilon}B_{22})\\(A_{21}+\epsilon A_{22})(B_{11}+1/{\epsilon}B_{21})&amp;(A_{21}+\epsilon A_{22})(B_{12}+1/{\epsilon}B_{22})\end{bmatrix}\Bigr\rfloor \mod \frac{1}{\epsilon}$. The resulting computational complexity for $N\times N$ matrices can be estimated from recursive equation $T(N)=4(N/2)^2$ (multiplication of a matrix by number)+$4(N/2)^2$ (additions of matrices)+$2N^2$ (floor and modulo)+$4T(N/2)$ (recursive calls) as $O(N^2log_2N)$. The novelty of the method lies in the observation, somehow ignored by other matrix-by-matrix multiplication algorithms, that we can multiply matrix entries by non-integer numbers to improve computational complexity. In other words, while having a processor that can compute multiplications, additions, modulo and floor operations with variable precision arithmetic in $O(1)$, we can obtain a matrix-by-matrix multiplication algorithm with $O(N^2log_2N)$ computational complexity. We also present a MATLAB code using VPA variable precision arithmetic emulator that can multiply matrices of size $N\times N$ using $(4log_2N+1)N^2$ variable precision arithmetic operations. This emulator uses $O(N)$ digits to run our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21050v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.MS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maciej Paszy\'nski</dc:creator>
    </item>
    <item>
      <title>SoS Certifiability of Subgaussian Distributions and its Algorithmic Applications</title>
      <link>https://arxiv.org/abs/2410.21194</link>
      <description>arXiv:2410.21194v1 Announce Type: new 
Abstract: We prove that there is a universal constant $C&gt;0$ so that for every $d \in \mathbb N$, every centered subgaussian distribution $\mathcal D$ on $\mathbb R^d$, and every even $p \in \mathbb N$, the $d$-variate polynomial $(Cp)^{p/2} \cdot \|v\|_{2}^p - \mathbb E_{X \sim \mathcal D} \langle v,X\rangle^p$ is a sum of square polynomials. This establishes that every subgaussian distribution is \emph{SoS-certifiably subgaussian} -- a condition that yields efficient learning algorithms for a wide variety of high-dimensional statistical tasks. As a direct corollary, we obtain computationally efficient algorithms with near-optimal guarantees for the following tasks, when given samples from an arbitrary subgaussian distribution: robust mean estimation, list-decodable mean estimation, clustering mean-separated mixture models, robust covariance-aware mean estimation, robust covariance estimation, and robust linear regression. Our proof makes essential use of Talagrand's generic chaining/majorizing measures theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21194v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Diakonikolas, Samuel B. Hopkins, Ankit Pensia, Stefan Tiegel</dc:creator>
    </item>
    <item>
      <title>Deep Learning and Machine Learning -- Python Data Structures and Mathematics Fundamental: From Theory to Practice</title>
      <link>https://arxiv.org/abs/2410.19849</link>
      <description>arXiv:2410.19849v1 Announce Type: cross 
Abstract: This book provides a comprehensive introduction to the foundational concepts of machine learning (ML) and deep learning (DL). It bridges the gap between theoretical mathematics and practical application, focusing on Python as the primary programming language for implementing key algorithms and data structures. The book covers a wide range of topics, including basic and advanced Python programming, fundamental mathematical operations, matrix operations, linear algebra, and optimization techniques crucial for training ML and DL models. Advanced subjects like neural networks, optimization algorithms, and frequency domain methods are also explored, along with real-world applications of large language models (LLMs) and artificial intelligence (AI) in big data management. Designed for both beginners and advanced learners, the book emphasizes the critical role of mathematical principles in developing scalable AI solutions. Practical examples and Python code are provided throughout, ensuring readers gain hands-on experience in applying theoretical knowledge to solve complex problems in ML, DL, and big data analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19849v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>cs.PL</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Silin Chen, Ziqian Bi, Junyu Liu, Benji Peng, Sen Zhang, Xuanhe Pan, Jiawei Xu, Jinlang Wang, Keyu Chen, Caitlyn Heqi Yin, Pohsun Feng, Yizhu Wen, Tianyang Wang, Ming Li, Jintao Ren, Qian Niu, Ming Liu</dc:creator>
    </item>
    <item>
      <title>Solving Polynomial Equations Over Finite Fields</title>
      <link>https://arxiv.org/abs/2410.20162</link>
      <description>arXiv:2410.20162v1 Announce Type: cross 
Abstract: We present a randomized algorithm for solving low-degree polynomial equation systems over finite fields faster than exhaustive search. In order to do so, we follow a line of work by Lokshtanov, Paturi, Tamaki, Williams, and Yu (SODA 2017), Bj\"orklund, Kaski, and Williams (ICALP 2019), and Dinur (SODA 2021). In particular, we generalize Dinur's algorithm for $\mathbb{F}_2$ to all finite fields, in particular the "symbolic interpolation" of Bj\"orklund, Kaski, and Williams, and we use an efficient trimmed multipoint evaluation and interpolation procedure for multivariate polynomials over finite fields by Van der Hoeven and Schost (AAECC 2013). The running time of our algorithm matches that of Dinur's algorithm for $\mathbb{F}_2$ and is significantly faster than the one of Lokshtanov et al. for $q&gt;2$.
  We complement our results with tight conditional lower bounds that, surprisingly, we were not able to find in the literature. In particular, under the strong exponential time hypothesis, we prove that it is impossible to solve $n$-variate low-degree polynomial equation systems over $\mathbb{F}_q$ in time $O((q-\varepsilon)^{n})$. As a bonus, we show that under the counting version of the strong exponential time hypothesis, it is impossible to compute the number of roots of a single $n$-variate low-degree polynomial over $\mathbb{F}_q$ in time ${O((q-\varepsilon)^{n})}$; this generalizes a result of Williams (SOSA 2018) from $\mathbb{F}_2$ to all finite fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20162v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Holger Dell, Anselm Haak, Melvin Kallmayer, Leo Wennmann</dc:creator>
    </item>
    <item>
      <title>Parameterized Saga of First-Fit and Last-Fit Coloring</title>
      <link>https://arxiv.org/abs/2410.20629</link>
      <description>arXiv:2410.20629v1 Announce Type: cross 
Abstract: The classic greedy coloring (first-fit) algorithm considers the vertices of an input graph $G$ in a given order and assigns the first available color to each vertex $v$ in $G$. In the {\sc Grundy Coloring} problem, the task is to find an ordering of the vertices that will force the greedy algorithm to use as many colors as possible. In the {\sc Partial Grundy Coloring}, the task is also to color the graph using as many colors as possible. This time, however, we may select both the ordering in which the vertices are considered and which color to assign the vertex. The only constraint is that the color assigned to a vertex $v$ is a color previously used for another vertex if such a color is available.
  Whether {\sc Grundy Coloring} and {\sc Partial Grundy Coloring} admit fixed-parameter tractable (FPT) algorithms, algorithms with running time $f(k)n^{\OO(1)}$, where $k$ is the number of colors, was posed as an open problem by Zaker and by Effantin et al., respectively. Recently, Aboulker et al. (STACS 2020 and Algorithmica 2022) resolved the question for \Grundycol\ in the negative by showing that the problem is W[1]-hard. For {\sc Partial Grundy Coloring}, they obtain an FPT algorithm on graphs that do not contain $K_{i,j}$ as a subgraph (a.k.a. $K_{i,j}$-free graphs). Aboulker et al.~re-iterate the question of whether there exists an FPT algorithm for {\sc Partial Grundy Coloring} on general graphs and also asks whether {\sc Grundy Coloring} admits an FPT algorithm on $K_{i,j}$-free graphs. We give FPT algorithms for {\sc Partial Grundy Coloring} on general graphs and for {\sc Grundy Coloring} on $K_{i,j}$-free graphs, resolving both the questions in the affirmative. We believe that our new structural theorems for partial Grundy coloring and ``representative-family'' like sets for $K_{i,j}$-free graphs that we use in obtaining our results may have wider algorithmic applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20629v1</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akanksha Agrawal, Daniel Lokshtanov, Fahad Panolan, Saket Saurabh, Shaily Verma</dc:creator>
    </item>
    <item>
      <title>Fully-Distributed Byzantine Agreement in Sparse Networks</title>
      <link>https://arxiv.org/abs/2410.20865</link>
      <description>arXiv:2410.20865v1 Announce Type: cross 
Abstract: Byzantine agreement is a fundamental problem in fault-tolerant distributed networks that has been studied intensively for the last four decades. Most of these works designed protocols for complete networks. A key goal in Byzantine protocols is to tolerate as many Byzantine nodes as possible.
  The work of Dwork, Peleg, Pippenger, and Upfal [STOC 1986, SICOMP 1988] was the first to address the Byzantine agreement problem in sparse, bounded degree networks and presented a protocol that achieved almost-everywhere agreement among honest nodes. In such networks, all known Byzantine agreement protocols (e.g., Dwork, Peleg, Pippenger, and Upfal, STOC 1986; Upfal, PODC 1992; King, Saia, Sanwalani, and Vee, FOCS 2006) that tolerated a large number of Byzantine nodes had a major drawback that they were not fully-distributed -- in those protocols, nodes are required to have initial knowledge of the entire network topology. This drawback makes such protocols inapplicable to real-world communication networks such as peer-to-peer (P2P) networks, which are typically sparse and bounded degree and where nodes initially have only local knowledge of themselves and their neighbors. Indeed, a fundamental open question raised by the above works is whether one can design Byzantine protocols that tolerate a large number of Byzantine nodes in sparse networks that work with only local knowledge, i.e., fully-distributed protocols. The work of Augustine, Pandurangan, and Robinson [PODC 2013] presented the first fully-distributed Byzantine agreement protocol that works in sparse networks, but it tolerated only up to $O(\sqrt{n}/ polylog(n))$ Byzantine nodes (where $n$ is the total network size).
  We answer the earlier open question by presenting fully-distributed Byzantine agreement protocols for sparse, bounded degree networks that tolerate significantly more Byzantine nodes -- up to $O(n/ polylog(n))$ of them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20865v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John Augustine, Fabien Dufoulon, Gopal Pandurangan</dc:creator>
    </item>
    <item>
      <title>Sample-Optimal Quantum Estimators for Pure-State Trace Distance and Fidelity via Samplizer</title>
      <link>https://arxiv.org/abs/2410.21201</link>
      <description>arXiv:2410.21201v1 Announce Type: cross 
Abstract: Trace distance and infidelity (induced by square root fidelity), as basic measures of the closeness of quantum states, are commonly used in quantum state discrimination, certification, and tomography. However, the sample complexity for their estimation still remains open. In this paper, we solve this problem for pure states. We present a quantum algorithm that estimates the trace distance and square root fidelity between pure states to within additive error $\varepsilon$, given sample access to their identical copies. Our algorithm achieves the optimal sample complexity $\Theta(1/\varepsilon^2)$, improving the long-standing folklore $O(1/\varepsilon^4)$. Our algorithm is composed of a samplized phase estimation of the product of two Householder reflections. Notably, an improved (multi-)samplizer for pure states is used as an algorithmic tool in our construction, through which any quantum query algorithm using $Q$ queries to the reflection operator about a pure state $|\psi\rangle$ can be converted to a $\delta$-close (in the diamond norm) quantum sample algorithm using $\Theta(Q^2/\delta)$ samples of $|\psi\rangle$. This samplizer for pure states is shown to be optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21201v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qisheng Wang, Zhicheng Zhang</dc:creator>
    </item>
    <item>
      <title>Analysis of Different Algorithmic Design Techniques for Seam Carving</title>
      <link>https://arxiv.org/abs/2410.21207</link>
      <description>arXiv:2410.21207v1 Announce Type: cross 
Abstract: Seam carving, a content-aware image resizing technique, has garnered significant attention for its ability to resize images while preserving important content. In this paper, we conduct a comprehensive analysis of four algorithmic design techniques for seam carving: brute-force, greedy, dynamic programming, and GPU-based parallel algorithms. We begin by presenting a theoretical overview of each technique, discussing their underlying principles and computational complexities. Subsequently, we delve into empirical evaluations, comparing the performance of these algorithms in terms of runtime efficiency. Our experimental results provide insights into the theoretical complexities of the design techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21207v1</guid>
      <category>math.NA</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Owais Aijaz, Syed Muhammad Ali, Yousuf Uyghur</dc:creator>
    </item>
    <item>
      <title>Online Weighted Paging with Unknown Weights</title>
      <link>https://arxiv.org/abs/2410.21266</link>
      <description>arXiv:2410.21266v1 Announce Type: cross 
Abstract: Online paging is a fundamental problem in the field of online algorithms, in which one maintains a cache of $k$ slots as requests for fetching pages arrive online. In the weighted variant of this problem, each page has its own fetching cost; a substantial line of work on this problem culminated in an (optimal) $O(\log k)$-competitive randomized algorithm, due to Bansal, Buchbinder and Naor (FOCS'07).
  Existing work for weighted paging assumes that page weights are known in advance, which is not always the case in practice. For example, in multi-level caching architectures, the expected cost of fetching a memory block is a function of its probability of being in a mid-level cache rather than the main memory. This complex property cannot be predicted in advance; over time, however, one may glean information about page weights through sampling their fetching cost multiple times.
  We present the first algorithm for online weighted paging that does not know page weights in advance, but rather learns from weight samples. In terms of techniques, this requires providing (integral) samples to a fractional solver, requiring a delicate interface between this solver and the randomized rounding scheme; we believe that our work can inspire online algorithms to other problems that involve cost sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21266v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Orin Levy, Noam Touitou, Aviv Rosenberg</dc:creator>
    </item>
    <item>
      <title>Noisy searching: simple, fast and correct</title>
      <link>https://arxiv.org/abs/2107.05753</link>
      <description>arXiv:2107.05753v3 Announce Type: replace 
Abstract: This work considers the problem of the noisy binary search in a sorted array. The noise is modeled by a parameter $p$ that dictates that a comparison can be incorrect with probability $p$, independently of other queries. We state two types of upper bounds on the number of queries: the worst-case and expected query complexity scenarios. The bounds improve the ones known to date, i.e., our algorithms require fewer queries. Additionally, they have simpler statements, and work for the full range of parameters. All query complexities for the expected query scenarios are tight up to lower order terms. For the problem where the target prior is uniform over all possible inputs, we provide an algorithm with expected complexity upperbounded by $(\log_2 n + \log_2 \delta^{-1} + 3)/I(p)$, where $n$ is the domain size, $0\le p &lt; 1/2$ is the noise ratio, and $\delta&gt;0$ is the failure probability, and $I(p)$ is the information gain function. As a side-effect, we close some correctness issues regarding previous work. Also, en route, we obtain new and improved query complexities for the search generalized to arbitrary graphs. This paper continues and improves the lines of research of Burnashev--Zigangirov [Prob. Per. Informatsii, 1974], Ben-Or and Hassidim [FOCS 2008], Gu and Xu [STOC 2023], and Emamjomeh-Zadeh et al. [STOC 2016], Dereniowski et al. [SOSA@SODA 2019].</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.05753v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dariusz Dereniowski, Aleksander {\L}ukasiewicz, Przemys{\l}aw Uzna\'nski</dc:creator>
    </item>
    <item>
      <title>A Cut-Matching Game for Constant-Hop Expanders</title>
      <link>https://arxiv.org/abs/2211.11726</link>
      <description>arXiv:2211.11726v2 Announce Type: replace 
Abstract: This paper extends and generalizes the well-known cut-matching game framework and provides a novel cut-strategy that produces constant-hop expanders.
  Constant-hop expanders are a significant strengthening of regular expanders with the additional guarantee that any demand can be (obliviously) routed along constant-hop flow-paths - in contrast to the $\Omega(\log n)$-hop paths in expanders.
  Cut-matching games for expanders are key tools for obtaining linear-time approximation algorithms for many hard problems, including finding (balanced or approximately-largest) sparse cuts, certifying the expansion of a graph by embedding an (explicit) expander, as well as computing expander decompositions, hierarchical cut decompositions, oblivious routings, multi-cuts, and multi-commodity flows.
  The cut-matching game of this paper is crucial in extending this versatile and powerful machinery to constant-hop and length-constrained expanders and has been already been extensively used. For example, as a key ingredient in several recent breakthroughs, including, computing constant-approximate $k$-commodity (min-cost) flows in $(m+k)^{1+\epsilon}$ time as well as the optimal constant-approximate deterministic worst-case fully-dynamic APSP-distance oracle - in all applications the constant-approximation factor directly traces to and crucially relies on the expanders from a cut-matching game guaranteeing constant-hop routing paths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.11726v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <category>math.CO</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bernhard Haeupler (r), Jonas Huebotter (r), Mohsen Ghaffari</dc:creator>
    </item>
    <item>
      <title>Online Dependent Rounding Schemes for Bipartite Matchings, with Applications</title>
      <link>https://arxiv.org/abs/2301.08680</link>
      <description>arXiv:2301.08680v3 Announce Type: replace 
Abstract: We introduce the abstract problem of rounding an unknown fractional bipartite $b$-matching $\bf{x}$ revealed online (e.g., output by an online fractional algorithm), exposed node-by-node on~one~side. The objective is to maximize the \emph{rounding ratio} of the output matching $M$, which is the minimum over all fractional $b$-matchings $\bf{x}$, and edges $e$, of the ratio $\Pr[e\in M]/x_e$. In analogy with the highly influential offline dependent rounding schemes of Gandhi et al.~(FOCS'02, JACM'06), we refer to such algorithms as \emph{online dependent rounding schemes} (ODRSes). This problem, with additional restrictions on the possible inputs $\bf{x}$, has played a key role in recent developments in online computing.
  We provide the first generic $b$-matching ODRSes that impose no restrictions on $\bf{x}$. Specifically, we provide ODRSes with rounding ratios of $0.646$ and $0.652$ for $b$-matchings and simple matchings, respectively. This breaks the natural barrier of $1-1/e$, prevalent for online matching problems, and numerous online problems more broadly. Using our ODRSes, we provide a number of algorithms with similar better-than-$(1-1/e)$ ratios for several problems in online edge coloring, stochastic optimization, and more.
  Our techniques, which have already found applications in several follow-up works (Patel and Wajc SODA'24, Blikstad et al.~SODA'25, Braverman et al.~SODA'25, and Aouad et al.~2024), include periodic use of \emph{offline} contention resolution schemes (in online algorithm design), grouping nodes, and a new scaling method which we call \emph{group discount and individual markup}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.08680v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> Joseph (Seffi),  Naor, Aravind Srinivasan, David Wajc</dc:creator>
    </item>
    <item>
      <title>Nearly Optimal Dynamic Set Cover: Breaking the Quadratic-in-$f$ Time Barrier</title>
      <link>https://arxiv.org/abs/2308.00793</link>
      <description>arXiv:2308.00793v2 Announce Type: replace 
Abstract: The dynamic set cover problem has been subject to extensive research since the pioneering works of [Bhattacharya et al, 2015] and [Gupta et al, 2017]. The input is a set system $(U, S)$ on a fixed collection $S$ of sets and a dynamic universe of elements, where each element appears in a most $f$ sets and the cost of each set lies in the range $[1/C, 1]$, and the goal is to efficiently maintain an approximately-minimum set cover under insertions and deletions of elements.
  Most previous work considers the low-frequency regime, namely $f = O(\log n)$, and this line of work has culminated with a deterministic $(1+\epsilon)f$-approximation algorithm with amortized update time $O(\frac{f^2}{\epsilon^3} + \frac{f}{\epsilon^2}\log C)$ [Bhattacharya et al, 2021]. In the high-frequency regime of $f = \Omega(\log n)$, an $O(\log n)$-approximation algorithm with amortized update time $O(f\log n)$ was given by [Gupta et al, 2017].
  Interestingly, at the intersection of the two regimes, i.e., $f = \Theta(\log n)$, the state-of-the-art results coincide: approximation $\Theta(f) = \Theta(\log n)$ with amortized update time $O(f^2) = O(f \log n) = O(\log^2 n)$. Up to this date, no previous work achieved update time of $o(f^2)$.
  In this paper we break the $\Omega(f^2)$ update time barrier via the following results: (1) $(1+\epsilon)f$-approximation can be maintained in $O\left(\frac{f}{\epsilon^3}\log^*f + \frac{f}{\epsilon^3}\log C\right) = O_{\epsilon,C}(f \log^* f)$ expected amortized update time; our algorithm works against an adaptive adversary. (2) $(1+\epsilon)f$-approximation can be maintained deterministically in $O\left(\frac{1}{\epsilon}f\log f + \frac{f}{\epsilon^3} + \frac{f\log C}{\epsilon^2}\right) = O_{\epsilon,C}(f \log f)$ amortized update time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.00793v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anton Bukov, Shay Solomon, Tianyi Zhang</dc:creator>
    </item>
    <item>
      <title>Universal Optimality of Dijkstra via Beyond-Worst-Case Heaps</title>
      <link>https://arxiv.org/abs/2311.11793</link>
      <description>arXiv:2311.11793v3 Announce Type: replace 
Abstract: This paper proves that Dijkstra's shortest-path algorithm is universally optimal in both its running time and number of comparisons when combined with a sufficiently efficient heap data structure.
  Universal optimality is a powerful beyond-worst-case performance guarantee for graph algorithms that informally states that a single algorithm performs as well as possible for every single graph topology. We give the first application of this notion to any sequential algorithm.
  We design a new heap data structure with a working-set property guaranteeing that the heap takes advantage of locality in heap operations. Our heap matches the optimal (worst-case) bounds of Fibonacci heaps but also provides the beyond-worst-case guarantee that the cost of extracting the minimum element is merely logarithmic in the number of elements inserted after it instead of logarithmic in the number of all elements in the heap. This makes the extraction of recently added elements cheaper.
  We prove that this working-set property guarantees universal optimality for the problem of ordering vertices by their distance from the source vertex: The sequence of heap operations generated by any run of Dijkstra's algorithm on a fixed graph possesses enough locality that one can couple the number of comparisons performed by any heap with our working-set bound to the minimum number of comparisons required to solve the distance ordering problem on this graph for a worst-case choice of arc lengths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11793v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bernhard Haeupler, Richard Hlad\'ik, V\'aclav Rozho\v{n}, Robert Tarjan, Jakub T\v{e}tek</dc:creator>
    </item>
    <item>
      <title>Reconfiguration of Multisets with Applications to Bin Packing</title>
      <link>https://arxiv.org/abs/2405.05535</link>
      <description>arXiv:2405.05535v2 Announce Type: replace 
Abstract: We use the reconfiguration framework to analyze problems that involve the rearrangement of items among groups. In various applications, a group of items could correspond to the files or jobs assigned to a particular machine, and the goal of rearrangement could be improving efficiency or increasing locality.
  To cover problems arising in a wide range of application areas, we define the general Repacking problem as the rearrangement of multisets of multisets. We present hardness results for the general case and algorithms for various classes of instances that arise in real-life scenarios. By limiting the total size of items in each multiset, our results can be viewed as an offline approach to Bin Packing, in which each bin is represented as a multiset.
  In addition to providing the first results on reconfiguration of multisets, our contributions open up several research avenues: the interplay between reconfiguration and online algorithms and parallel algorithms; the use of the tools of linear programming in reconfiguration; and, in the longer term, a focus on extra resources in reconfiguration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05535v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey Kam, Shahin Kamali, Avery Miller, Naomi Nishimura</dc:creator>
    </item>
    <item>
      <title>Efficient Certificates of Anti-Concentration Beyond Gaussians</title>
      <link>https://arxiv.org/abs/2405.15084</link>
      <description>arXiv:2405.15084v2 Announce Type: replace 
Abstract: A set of high dimensional points $X=\{x_1, x_2,\ldots, x_n\} \subset R^d$ in isotropic position is said to be $\delta$-anti concentrated if for every direction $v$, the fraction of points in $X$ satisfying $|\langle x_i,v \rangle |\leq \delta$ is at most $O(\delta)$. Motivated by applications to list-decodable learning and clustering, recent works have considered the problem of constructing efficient certificates of anti-concentration in the average case, when the set of points $X$ corresponds to samples from a Gaussian distribution. Their certificates played a crucial role in several subsequent works in algorithmic robust statistics on list-decodable learning and settling the robust learnability of arbitrary Gaussian mixtures, yet remain limited to rotationally invariant distributions.
  This work presents a new (and arguably the most natural) formulation for anti-concentration. Using this formulation, we give quasi-polynomial time verifiable sum-of-squares certificates of anti-concentration that hold for a wide class of non-Gaussian distributions including anti-concentrated bounded product distributions and uniform distributions over $L_p$ balls (and their affine transformations). Consequently, our method upgrades and extends results in algorithmic robust statistics e.g., list-decodable learning and clustering, to such distributions. Our approach constructs a canonical integer program for anti-concentration and analysis a sum-of-squares relaxation of it, independent of the intended application. We rely on duality and analyze a pseudo-expectation on large subsets of the input points that take a small value in some direction. Our analysis uses the method of polynomial reweightings to reduce the problem to analyzing only analytically dense or sparse directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15084v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ainesh Bakshi, Pravesh Kothari, Goutham Rajendran, Madhur Tulsiani, Aravindan Vijayaraghavan</dc:creator>
    </item>
    <item>
      <title>Congestion-Approximators from the Bottom Up</title>
      <link>https://arxiv.org/abs/2407.04976</link>
      <description>arXiv:2407.04976v2 Announce Type: replace 
Abstract: We develop a novel algorithm to construct a congestion-approximator with polylogarithmic quality on a capacitated, undirected graph in nearly-linear time. Our approach is the first *bottom-up* hierarchical construction, in contrast to previous *top-down* approaches including that of Racke, Shah, and Taubig (SODA 2014), the only other construction achieving polylogarithmic quality that is implementable in nearly-linear time (Peng, SODA 2016). Similar to Racke, Shah, and Taubig, our construction at each hierarchical level requires calls to an approximate max-flow/min-cut subroutine. However, the main advantage to our bottom-up approach is that these max-flow calls can be implemented directly *without recursion*. More precisely, the previously computed levels of the hierarchy can be converted into a *pseudo-congestion-approximator*, which then translates to a max-flow algorithm that is sufficient for the particular max-flow calls used in the construction of the next hierarchical level. As a result, we obtain the first non-recursive algorithms for congestion-approximator and approximate max-flow that run in nearly-linear time, a conceptual improvement to the aforementioned algorithms that recursively alternate between the two problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04976v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason Li, Satish Rao, Di Wang</dc:creator>
    </item>
    <item>
      <title>Memento Filter: A Fast, Dynamic, and Robust Range Filter</title>
      <link>https://arxiv.org/abs/2408.05625</link>
      <description>arXiv:2408.05625v3 Announce Type: replace 
Abstract: Range filters are probabilistic data structures that answer approximate range emptiness queries. They aid in avoiding processing empty range queries and have use cases in many application domains such as key-value stores and social web analytics. However, current range filter designs do not support dynamically changing and growing datasets. Moreover, several of these designs also exhibit impractically high false positive rates under correlated workloads, which are common in practice. These impediments restrict the applicability of range filters across a wide range of use cases. We introduce Memento filter, the first range filter to offer dynamicity, fast operations, and a robust false positive rate guarantee for any workload. Memento filter partitions the key universe and clusters its keys according to this partitioning. For each cluster, it stores a fingerprint and a list of key suffixes contiguously. The encoding of these lists makes them amenable to existing dynamic filter structures. Due to the well-defined one-to-one mapping from keys to suffixes, Memento filter supports inserts and deletes and can even expand to accommodate a growing dataset. We implement Memento filter on top of a Rank-and-Select Quotient filter and InfiniFilter and demonstrate that it achieves competitive false positive rates and performance with the state-of-the-art while also providing dynamicity. Due to its dynamicity, Memento filter is the first range filter applicable to B-Trees. We showcase this by integrating Memento filter into WiredTiger, a B-Tree-based key-value store. Memento filter doubles WiredTiger's range query throughput when 50% of the queries are empty while keeping all other cost metrics unharmed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05625v3</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3698820</arxiv:DOI>
      <dc:creator>Navid Eslami, Niv Dayan</dc:creator>
    </item>
    <item>
      <title>Parallel Cluster-BFS and Applications to Shortest Paths</title>
      <link>https://arxiv.org/abs/2410.17226</link>
      <description>arXiv:2410.17226v2 Announce Type: replace 
Abstract: Breadth-first Search (BFS) is one of the most important graph processing subroutines, especially for computing the unweighted distance. Many applications may require running BFS from multiple sources. Sequentially, when running BFS on a cluster of nearby vertices, a known optimization is using bit-parallelism. Given a subset of vertices with size $k$ and the distance between any pair of them is no more than $d$, BFS can be applied to all of them in total work $O(dm(k/w+1))$, where $w$ is the length of a word in bits and $m$ is the number of edges. We will refer to this approach as cluster-BFS (C-BFS). Such an approach has been studied and shown effective both in theory and in practice in the sequential setting. However, it remains unknown how this can be combined with thread-level parallelism.
  In this paper, we focus on designing efficient parallel C-BFS based on BFS to answer unweighted distance queries. Our solution combines the strengths of bit-level parallelism and thread-level parallelism, and achieves significant speedup over the plain sequential solution. We also apply our algorithm to real-world applications. In particular, we identified another application (landmark-labeling for the approximate distance oracle) that can take advantage of parallel C-BFS. Under the same memory budget, our new solution improves accuracy and/or time on all the 18 tested graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17226v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Letong Wang, Guy Blelloch, Yan Gu, Yihan Sun</dc:creator>
    </item>
    <item>
      <title>Stronger adversaries grow cheaper forests: online node-weighted Steiner problems</title>
      <link>https://arxiv.org/abs/2410.18542</link>
      <description>arXiv:2410.18542v2 Announce Type: replace 
Abstract: We propose a $O(\log k \log n)$-competitive randomized algorithm for online node-weighted Steiner forest. This is essentially optimal and significantly improves over the previous bound of $O(\log^2 k \log n)$ by Hajiaghayi et al. [2017]. In fact, our result extends to the more general prize-collecting setting, improving over previous works by a poly-logarithmic factor. Our key technical contribution is a randomized online algorithm for set cover and non-metric facility location in a new adversarial model which we call semi-adaptive adversaries. As a by-product of our techniques, we obtain the first deterministic $O(\log |C| \log |F|)$-competitive algorithm for non-metric facility location.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18542v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sander Borst, Marek Eli\'a\v{s}, Moritz Venzin</dc:creator>
    </item>
    <item>
      <title>Recognizing Sumsets is NP-Complete</title>
      <link>https://arxiv.org/abs/2410.18661</link>
      <description>arXiv:2410.18661v2 Announce Type: replace 
Abstract: Sumsets are central objects in additive combinatorics. In 2007, Granville asked whether one can efficiently recognize whether a given set $S$ is a sumset, i.e. whether there is a set $A$ such that $A+A=S$. Granville suggested an algorithm that takes exponential time in the size of the given set, but can we do polynomial or even linear time? This basic computational question is indirectly asking a fundamental structural question: do the special characteristics of sumsets allow them to be efficiently recognizable? In this paper, we answer this question negatively by proving that the problem is NP-complete. Specifically, our results hold for integer sets and over any finite field. Assuming the Exponential Time Hypothesis, our lower bound becomes $2^{\Omega(n^{1/4})}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18661v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Abboud, Nick Fischer, Ron Safier, Nathan Wallheimer</dc:creator>
    </item>
    <item>
      <title>On Differentially Private Subspace Estimation in a Distribution-Free Setting</title>
      <link>https://arxiv.org/abs/2402.06465</link>
      <description>arXiv:2402.06465v3 Announce Type: replace-cross 
Abstract: Private data analysis faces a significant challenge known as the curse of dimensionality, leading to increased costs. However, many datasets possess an inherent low-dimensional structure. For instance, during optimization via gradient descent, the gradients frequently reside near a low-dimensional subspace. If the low-dimensional structure could be privately identified using a small amount of points, we could avoid paying for the high ambient dimension.
  On the negative side, Dwork, Talwar, Thakurta, and Zhang (STOC 2014) proved that privately estimating subspaces, in general, requires an amount of points that has a polynomial dependency on the dimension. However, their bounds do not rule out the possibility to reduce the number of points for "easy" instances. Yet, providing a measure that captures how much a given dataset is "easy" for this task turns out to be challenging, and was not properly addressed in prior works.
  Inspired by the work of Singhal and Steinke (NeurIPS 2021), we provide the first measures that quantify "easiness" as a function of multiplicative singular-value gaps in the input dataset, and support them with new upper and lower bounds. In particular, our results determine the first types of gaps that are sufficient and necessary for estimating a subspace with an amount of points that is independent of the dimension. Furthermore, we realize our upper bounds using a practical algorithm and demonstrate its advantage in high-dimensional regimes compared to prior approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06465v3</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eliad Tsfadia</dc:creator>
    </item>
    <item>
      <title>The ESPRIT algorithm under high noise: Optimal error scaling and noisy super-resolution</title>
      <link>https://arxiv.org/abs/2404.03885</link>
      <description>arXiv:2404.03885v3 Announce Type: replace-cross 
Abstract: Subspace-based signal processing techniques, such as the Estimation of Signal Parameters via Rotational Invariant Techniques (ESPRIT) algorithm, are popular methods for spectral estimation. These algorithms can achieve the so-called super-resolution scaling under low noise conditions, surpassing the well-known Nyquist limit. However, the performance of these algorithms under high-noise conditions is not as well understood. Existing state-of-the-art analysis indicates that ESPRIT and related algorithms can be resilient even for signals where each observation is corrupted by statistically independent, mean-zero noise of size $\mathcal{O}(1)$, but these analyses only show that the error $\epsilon$ decays at a slow rate $\epsilon=\mathcal{\tilde{O}}(n^{-1/2})$ with respect to the cutoff frequency $n$ (i.e., the maximum frequency of the measurements). In this work, we prove that under certain assumptions, the ESPRIT algorithm can attain a significantly improved error scaling $\epsilon = \mathcal{\tilde{O}}(n^{-3/2})$, exhibiting noisy super-resolution scaling beyond the Nyquist limit $\epsilon = \mathcal{O}(n^{-1})$ given by the Nyquist-Shannon sampling theorem. We further establish a theoretical lower bound and show that this scaling is optimal. Our analysis introduces novel matrix perturbation results, which could be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03885v3</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyan Ding, Ethan N. Epperly, Lin Lin, Ruizhe Zhang</dc:creator>
    </item>
    <item>
      <title>EFX Allocations and Orientations on Bipartite Multi-graphs: A Complete Picture</title>
      <link>https://arxiv.org/abs/2410.17002</link>
      <description>arXiv:2410.17002v2 Announce Type: replace-cross 
Abstract: We consider the fundamental problem of fairly allocating a set of indivisible items among agents having valuations that are represented by a multi-graph -- here, agents appear as the vertices and items as the edges between them and each vertex (agent) only values the set of its incident edges (items). The goal is to find a fair, i.e., envy-free up to any item (EFX) allocation. This model has recently been introduced by Christodoulou et al. (EC'23) where they show that EFX allocations always exist on simple graphs for monotone valuations, i.e., where any two agents can share at most one edge (item). A natural question arises as to what happens when we go beyond simple graphs and study various classes of multi-graphs?
  We answer the above question affirmatively for the valuation class of bipartite multi-graphs and multi-cycles. Our main positive result is that EFX allocations on bipartite multi-graphs (and multi-cycles) always exist and can be computed in polynomial time for additive valuations. We, therefore, push the frontiers of our understanding of EFX allocations and expand the scenarios where they are known to exist for an arbitrary number of agents. Next, we study EFX orientations (i.e., allocations where every item is allocated to one of its two endpoint agents) and give a complete picture of when they exist for bipartite multi-graphs dependent on two parameters -- the number of edges shared between any two agents and the diameter of the graph. Finally, we prove that it is NP-complete to determine whether a given fair division instance on a bipartite multi-graph admits an EFX orientation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17002v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahyar Afshinmehr, Alireza Danaei, Mehrafarin Kazemi, Kurt Mehlhorn, Nidhi Rathi</dc:creator>
    </item>
  </channel>
</rss>
