<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Feb 2025 04:17:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Adjoint of Least Squares Shadowing: Existence, Uniqueness and Coarse Domain Discretization</title>
      <link>https://arxiv.org/abs/2502.09737</link>
      <description>arXiv:2502.09737v1 Announce Type: new 
Abstract: Chaotic dynamical systems are characterized by the sensitive dependence of trajectories on initial conditions. Conventional sensitivity analysis of time-averaged functionals yields unbounded sensitivities when the simulation is chaotic. The least squares shadowing (LSS) is a popular approach to computing bounded sensitivities in the presence of chaotic dynamical systems. The current paper proves the existence, uniqueness, and boundedness of the adjoint of the LSS equations. In particular, the analysis yields a sharper bound on the condition number of the LSS equations than currently demonstrated in existing literature and shows that the condition number is bounded for large integration times. The derived bound on condition number also shows a relation between the conditioning of the LSS and the time dilation factor which is consistent with the trend numerically observed in the previous LSS literature. Furthermore, using the boundedness of the condition number for large integration times, we provide an alternate proof to (Chater et al., 2017) of the convergence of the LSS sensitivity to the true sensitivity at the rate of $\mathcal{O}\left(\frac{1}{\sqrt{T}}\right)$ regardless of the boundary conditions imposed on the adjoint, as long as the adjoint boundary conditions are bounded. Existence and uniqueness of the solution to the continuous-in-time adjoint LSS equation ensure that the LSS equation can be discretized independently of the primal equation and that the true LSS adjoint solution is recovered as the time step is refined. This allows for the adjoint LSS equation to be discretized on a coarser time domain than that of the primal governing equation to reduce the cost of solving the linear space-time system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09737v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pranshul Thakur, Siva Nadarajah</dc:creator>
    </item>
    <item>
      <title>Robust Adaptive Meshing, Mesh Density Functions, and Nonlocal Observations for Ensemble Based Data Assimilation</title>
      <link>https://arxiv.org/abs/2502.09754</link>
      <description>arXiv:2502.09754v1 Announce Type: new 
Abstract: Adaptive spatial meshing has proven invaluable for the accurate, efficient computation of solutions of time dependent partial differential equations. In a DA context the use of adaptive spatial meshes addresses several factors that place increased demands on meshing; these include the location and relative importance of observations and the use of ensemble solutions. To increase the efficiency of adaptive meshes for data assimilation, robust look ahead meshes are developed that fix the same adaptive mesh for all ensemble members for the entire time interval of the forecasts and that incorporates the observations at the next analysis time. This allows for increased vectorization of the ensemble forecasts while minimizing interpolation of solutions between different meshes. The techniques to determine these robust meshes are based upon combining metric tensors or mesh density functions to define nonuniform meshes. We illustrate the robust ensemble look ahead meshes using traveling wave solutions of a bistable reaction-diffusion equation. Observation operators based on convolution type integrals and their associated metric tensors are derived. These further the goals of making efficient use of adaptive meshes in ensemble based DA techniques, developing and employing robust meshes that are effective for a range of similar behaviors in both the ensembles and the observations, and the integration with advanced numerical PDE techniques (a quasi-Lagrangian moving mesh DG technique employing embedded pairs for time stepping). Numerical experiments with different observation scenarios are presented for a 2D inviscid Burgers' equation, a multi-component system, a 2D Shallow Water model, and for a coupled system of two 1D Kuramoto-Sivashinsky equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09754v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremiah Buenger, Weizhang Huang, Erik Van Vleck</dc:creator>
    </item>
    <item>
      <title>Compression Properties for large Toeplitz-like matrices</title>
      <link>https://arxiv.org/abs/2502.09823</link>
      <description>arXiv:2502.09823v1 Announce Type: new 
Abstract: Toeplitz matrices are abundant in computational mathematics, and there is a rich literature on the development of fast and superfast algorithms for solving linear systems involving such matrices. Any Toeplitz matrix can be transformed into a matrix with off-diagonal blocks that are of low numerical rank.Surprisingly little is known about the compressibility of these matrices in a theoretically rigorous sense, even though this compressibility is relied upon in practice in a number of superfast Toeplitz solvers. In this paper, we show that the compression properties of these matrices can be thoroughly explained using their displacement structure. We provide explicit bounds on the numerical ranks of important submatrices that arise when applying HSS, HODLR and other approximations with hierarchical low-rank structure to transformed Toeplitz and Toeplitz-like matrices. Our results lead to very efficient displacement-based compression strategies that can be used to formulate adaptive superfast rank-structured solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09823v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernhard Beckermann, Daniel Kressner, Heather Wilber</dc:creator>
    </item>
    <item>
      <title>Efficient, Accurate, and Robust Penalty-Projection Algorithm for Parameterized Stochastic Navier-Stokes Flow Problems</title>
      <link>https://arxiv.org/abs/2502.09842</link>
      <description>arXiv:2502.09842v1 Announce Type: new 
Abstract: This paper presents and analyzes a fast, robust, efficient, and optimally accurate fully discrete splitting algorithm for the Uncertainty Quantification (UQ) of parameterized Stochastic Navier-Stokes Equations (SNSEs) flow problems those occur in the convection-dominated regimes. The time-stepping algorithm is an implicit backward-Euler linearized method, grad-div and Ensemble Eddy Viscosity (EEV) regularized, and split using discrete Hodge decomposition. Additionally, the scheme's sub-problems are all designed to have different Right-Hand-Side (RHS) vectors but the same system matrix for all realizations at each time-step. The stability of the algorithm is rigorously proven, and it has been shown that appropriately large grad-div stabilization parameters vanish the splitting error. The proposed UQ algorithm is then combined with the Stochastic Collocation Methods (SCMs). Several numerical experiments are given to verify this superior scheme's predicted convergence rates and performance on benchmark problems for high expected Reynolds numbers ($Re$).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09842v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Neethu Suma Raveendran, Md. Abdul Aziz, Sivaguru S. Ravindran, Muhammad Mohebujjaman</dc:creator>
    </item>
    <item>
      <title>Verified error bounds for the singular values of structured matrices with applications to computer-assisted proofs for differential equations</title>
      <link>https://arxiv.org/abs/2502.09984</link>
      <description>arXiv:2502.09984v1 Announce Type: new 
Abstract: This paper introduces two methods for verifying the singular values of the structured matrix denoted by $R^{-H}AR^{-1}$, where $R$ is a nonsingular matrix and $A$ is a general nonsingular square matrix. The first of the two methods uses the computed factors from a singular value decomposition (SVD) to verify all singular values; the second estimates a lower bound of the minimum singular value without performing the SVD. The proposed approach for verifying all singular values efficiently computes tight error bounds. The method for estimating a lower bound of the minimum singular value is particularly effective for sparse matrices. These methods have proven to be efficient in verifying solutions to differential equation problems, that were previously challenging due to the extensive computational time and memory requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09984v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takeshi Terao, Yoshitaka Watanabe, Katsuhisa Ozaki</dc:creator>
    </item>
    <item>
      <title>Phi-FEM-FNO: a new approach to train a Neural Operator as a fast PDE solver for variable geometries</title>
      <link>https://arxiv.org/abs/2502.10033</link>
      <description>arXiv:2502.10033v1 Announce Type: new 
Abstract: In this paper, we propose a way to solve partial differential equations (PDEs) by combining machine learning techniques and the finite element method called Phi-FEM. For that, we use the Fourier Neural Operator (FNO), a learning mapping operator. The purpose of this paper is to provide numerical evidence to show the effectiveness of this technique. We will focus here on the resolution of two equations: the Poisson-Dirichlet equation and the non-linear elasticity equations. The key idea of our method is to address the challenging scenario of varying domains, where each problem is solved on a different geometry. The considered domains are defined by level-set functions due to the use of the Phi-FEM approach. We will first recall the idea of $\varphi$-FEM and of the Fourier Neural Operator. Then, we will explain how to combine these two methods. We will finally illustrate the efficiency of this combination with some numerical results on three test cases. In addition, in the last test case, we propose a new numerical scheme for hyperelastic materials following the Phi-FEM paradigm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10033v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Michel Duprez, Vanessa Lleras, Alexei Lozinski, Vincent Vigon, Killian Vuillemot</dc:creator>
    </item>
    <item>
      <title>Bound preserving {P}oint-{A}verage-{M}oment {P}olynomi{A}l-interpreted ({PAMPA}) on polygonal meshes</title>
      <link>https://arxiv.org/abs/2502.10069</link>
      <description>arXiv:2502.10069v1 Announce Type: new 
Abstract: We present a novel discretisation strategy, strongly inspired from Roe's Active Flux scheme. It can use polygonal meshes and is provably bound preserving for scalar problems and the Euler equations. Several cases demonstrates the quality of the method, and improvements with respect to previous work of the authors. This paper is a summary of \cite{BPPampa}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10069v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>R\'emi Abgrall, Yongle Liu, Walter Boscheri</dc:creator>
    </item>
    <item>
      <title>Statistical data analysis for Tourism in Poland in R Programming Environment</title>
      <link>https://arxiv.org/abs/2502.10100</link>
      <description>arXiv:2502.10100v1 Announce Type: new 
Abstract: This study utilises the R programming language for statistical data analysis to understand Tourism dynamics in Poland. It focuses on methods for data visualisation, multivariate statistics, and hypothesis testing. To investigate the expenditure behavior of tourist, spending patterns, correlations, and associations among variables were analysed in the dataset. The results revealed a significant relationship between accommodation type and the purpose of trip, showing that the purpose of a trip impacts the selection of accommodation. A strong correlation was observed between organizer expenditure and private expenditure, indicating that individual spending are more when the spending on organizing the trip are higher. However, no significant difference was observed in total expenditure across different accommodation types and purpose of the trip revealing that travelers tend to spend similar amounts regardless of their reason for travel or choice of accommodation. Although significant relationships were observed among certain variables, ANOVA could not be applied because the dataset was not able to hold on the normality assumption. In future, the dataset can be explored further to find more meaningful insights. The developed code is available on GitHub: https://github.com/SaadAhmedJamal/DataAnalysis RProgEnv.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10100v1</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <category>cs.NA</category>
      <category>cs.PL</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saad Ahmed Jamal</dc:creator>
    </item>
    <item>
      <title>On the unconventional Hug integrator</title>
      <link>https://arxiv.org/abs/2502.10199</link>
      <description>arXiv:2502.10199v1 Announce Type: new 
Abstract: Hug is a recently proposed iterative mapping used to design efficient updates in Markov chain Monte Carlo (MCMC) methods when sampling along manifolds is of interest. In this paper we show that Hug may be interpreted as a consistent discretization of a system of differential equations with a rather complicated structure. The proof of convergence of this discretization includes a number of unusual features we explore fully. We uncover an unexpected and, yet, undocumented property of the solutions of the underlying dynamical system that manifest itself by the existence of Hug trajectories that fail to cover the manifold of interest. This suggests caution when using the Hug update.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10199v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Andrieu, J. M. Sanz-Serna</dc:creator>
    </item>
    <item>
      <title>Investigation of the Estimation Accuracy of 5 Different Numerical ODE Solvers on 3 Case Studies</title>
      <link>https://arxiv.org/abs/2502.10289</link>
      <description>arXiv:2502.10289v1 Announce Type: new 
Abstract: Numerical ordinary differential equation (ODE) solvers are indispensable tools in various engineering domains, enabling the simulation and analysis of dynamic systems. In this work, we utilize 5 different numerical ODE solvers namely: Euler's method, Heun's method, Midpoint Method, Runge-kutta 4th order and ODE45 method in order to discover the answer of three wellknown case studies and compare their results by calculation of relative errors. To check for the validity of the estimations, the experimental data of previous literature have been compared with the data in this paper which shows a good accordance. We observe that for each of the case studies based on the behavior of the model, the estimation accuracy of the solvers is different. For the logistic population change as the first case study, the results of all solvers are so close to each other that only their solution cost can be considered for their superiority. For temperature change of a building as the second case study we see that in some especial areas the accuracy of the solvers is different and in general Midpoint ODE solver shows better results. As the last case study, market equilibrium price shows that none of the numerical ODE solvers can estimate its behavior which is due to its sudden changing nature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10289v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AG</category>
      <category>math.DG</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hamidreza Moradi, Erfan Kefayat, Hamideh Hossei</dc:creator>
    </item>
    <item>
      <title>Fast Inexact Bilevel Optimization for Analytical Deep Image Priors</title>
      <link>https://arxiv.org/abs/2502.09758</link>
      <description>arXiv:2502.09758v1 Announce Type: cross 
Abstract: The analytical deep image prior (ADP) introduced by Dittmer et al. (2020) establishes a link between deep image priors and classical regularization theory via bilevel optimization. While this is an elegant construction, it involves expensive computations if the lower-level problem is to be solved accurately. To overcome this issue, we propose to use adaptive inexact bilevel optimization to solve ADP problems. We discuss an extension of a recent inexact bilevel method called the method of adaptive inexact descent of Salehi et al.(2024) to an infinite-dimensional setting required by the ADP framework. In our numerical experiments we demonstrate that the computational speed-up achieved by adaptive inexact bilevel optimization allows one to use ADP on larger-scale problems than in the previous literature, e.g. in deblurring of 2D color images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09758v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>eess.IV</category>
      <category>math.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Sadegh Salehi, Tatiana A. Bubba, Yury Korolev</dc:creator>
    </item>
    <item>
      <title>Self-consistent bounds method for dissipative PDEs</title>
      <link>https://arxiv.org/abs/2502.09760</link>
      <description>arXiv:2502.09760v1 Announce Type: cross 
Abstract: We discuss the method of self-consistent bounds for dissipative PDEs with periodic boundary conditions. We prove convergence theorems for a class of dissipative PDEs, which constitute a theoretical basis of a general framework for construction of an algorithm that computes bounds for the solutions of the underlying PDE and its dependence on initial conditions.
  We also show, that the classical examples of parabolic PDEs including Kuramoto-Sivashinsky equation and the Navier-Stokes on the torus fit into this framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09760v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Wilczak, Piotr Zgliczy\'nski</dc:creator>
    </item>
    <item>
      <title>Optimal lower Lipschitz bounds for ReLU layers, saturation, and phase retrieval</title>
      <link>https://arxiv.org/abs/2502.09898</link>
      <description>arXiv:2502.09898v1 Announce Type: cross 
Abstract: The injectivity of ReLU layers in neural networks, the recovery of vectors from clipped or saturated measurements, and (real) phase retrieval in $\mathbb{R}^n$ allow for a similar problem formulation and characterization using frame theory. In this paper, we revisit all three problems with a unified perspective and derive lower Lipschitz bounds for ReLU layers and clipping which are analogous to the previously known result for phase retrieval and are optimal up to a constant factor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09898v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <category>math.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Freeman, Daniel Haider</dc:creator>
    </item>
    <item>
      <title>Discovering Polynomial and Quadratic Structure in Nonlinear Ordinary Differential Equations</title>
      <link>https://arxiv.org/abs/2502.10005</link>
      <description>arXiv:2502.10005v1 Announce Type: cross 
Abstract: Dynamical systems with quadratic or polynomial drift exhibit complex dynamics, yet compared to nonlinear systems in general form, are often easier to analyze, simulate, control, and learn. Results going back over a century have shown that the majority of nonpolynomial nonlinear systems can be recast in polynomial form, and their degree can be reduced further to quadratic. This process of polynomialization/quadratization reveals new variables (in most cases, additional variables have to be added to achieve this) in which the system dynamics adhere to that specific form, which leads us to discover new structures of a model. This chapter summarizes the state of the art for the discovery of polynomial and quadratic representations of finite-dimensional dynamical systems. We review known existence results, discuss the two prevalent algorithms for automating the discovery process, and give examples in form of a single-layer neural network and a phenomenological model of cell signaling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10005v1</guid>
      <category>cs.SC</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <category>q-bio.MN</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boris Kramer, Gleb Pogudin</dc:creator>
    </item>
    <item>
      <title>An adaptive importance sampling algorithm for risk-averse optimization</title>
      <link>https://arxiv.org/abs/2502.10084</link>
      <description>arXiv:2502.10084v1 Announce Type: cross 
Abstract: Adaptive sampling algorithms are modern and efficient methods that dynamically adjust the sample size throughout the optimization process. However, they may encounter difficulties in risk-averse settings, particularly due to the challenge of accurately sampling from the tails of the underlying distribution of random inputs. This often leads to a much faster growth of the sample size compared to risk-neutral problems. In this work, we propose a novel adaptive sampling algorithm that adapts both the sample size and the sampling distribution at each iteration. The biasing distributions are constructed on the fly, leveraging a reduced-order model of the objective function to be minimized, and are designed to oversample a so-called risk region. As a result, a reduction of the variance of the gradients is achieved, which permits to use fewer samples per iteration compared to a standard algorithm, while still preserving the asymptotic convergence rate. Our focus is on the minimization of the Conditional Value-at-Risk (CVaR), and we establish the convergence of the proposed computational framework. Numerical experiments confirm the substantial computational savings achieved by our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10084v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sandra Pieraccini, Tommaso Vanzan</dc:creator>
    </item>
    <item>
      <title>Characterization of Logarithmic Fekete Critical Configurations of at Most Six Points in All Dimensions</title>
      <link>https://arxiv.org/abs/2502.10152</link>
      <description>arXiv:2502.10152v1 Announce Type: cross 
Abstract: We consider the logarithmic Fekete problem, which consists of placing a fixed number of points on the unit sphere in $\mathbb{R}^d$, in such a way that the product of all pairs of mutual Euclidean distances is maximized or, equivalently, so that their logarithmic energy is minimized. Using tools from Computational Algebraic Geometry, we find and classify all critical configurations for this problem when considering at most six points in every dimension $d$. Our results discover some previously unknown optimal configurations and give the first reported case of a spurious local minimum for the Fekete problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10152v1</guid>
      <category>math.AC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diego Armentano, Leandro Bentancur, Federico Carrasco, Marcelo Fiori, Mat\'ias Vald\'es, Mauricio Velasco</dc:creator>
    </item>
    <item>
      <title>Global Solver based on the Sperner-Lemma and Mazurkewicz-Knaster-Kuratowski-Lemma based proof of the Brouwer Fixed-Point Theorem</title>
      <link>https://arxiv.org/abs/2407.18816</link>
      <description>arXiv:2407.18816v5 Announce Type: replace 
Abstract: In this paper a fixed-point solver for mappings from a Simplex into itself that is gradient-free, global and requires $d$ function evaluations for halvening the error is presented, where $d$ is the dimension. It is based on topological arguments and uses the constructive proof of the Mazurkewicz-Knaster-Kuratowski lemma as used as part of the proof for Brouwers Fixed-Point theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18816v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Thilo Moshagen</dc:creator>
    </item>
    <item>
      <title>High-order finite element methods for three-dimensional multicomponent convection-diffusion</title>
      <link>https://arxiv.org/abs/2408.17390</link>
      <description>arXiv:2408.17390v2 Announce Type: replace 
Abstract: We derive and analyze a broad class of finite element methods for numerically simulating the stationary, low Reynolds number flow of concentrated mixtures of several distinct chemical species in a common thermodynamic phase. The underlying partial differential equations that we discretize are the Stokes$\unicode{x2013}$Onsager$\unicode{x2013}$Stefan$\unicode{x2013}$Maxwell (SOSM) equations, which model bulk momentum transport and multicomponent diffusion within ideal and non-ideal mixtures. Unlike previous approaches, the methods are straightforward to implement in two and three spatial dimensions, and allow for high-order finite element spaces to be employed. The key idea in deriving the discretization is to suitably reformulate the SOSM equations in terms of the species mass fluxes and chemical potentials, and discretize these unknown fields using stable $H(\textrm{div}) \unicode{x2013} L^2$ finite element pairs. We prove that the methods are convergent and yield a symmetric linear system for a Picard linearization of the SOSM equations, which staggers the updates for concentrations and chemical potentials. We also discuss how the proposed approach can be extended to the Newton linearization of the SOSM equations, which requires the simultaneous solution of mole fractions, chemical potentials, and other variables. Our theoretical results are supported by numerical experiments and we present an example of a physical application involving the microfluidic non-ideal mixing of hydrocarbons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17390v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Baier-Reinio, Patrick E. Farrell</dc:creator>
    </item>
    <item>
      <title>Stabilizing and Solving Inverse Problems using Data and Machine Learning</title>
      <link>https://arxiv.org/abs/2412.04409</link>
      <description>arXiv:2412.04409v2 Announce Type: replace 
Abstract: We consider an inverse problem involving the reconstruction of the solution to a nonlinear partial differential equation (PDE) with unknown boundary conditions. Instead of direct boundary data, we are provided with a large dataset of boundary observations for typical solutions (collective data) and a bulk measurement of a specific realization. To leverage this collective data, we first compress the boundary data using proper orthogonal decomposition (POD) in a linear expansion. Next, we identify a possible nonlinear low-dimensional structure in the expansion coefficients using an autoencoder, which provides a parametrization of the dataset in a lower-dimensional latent space. We then train an operator network to map the expansion coefficients representing the boundary data to the finite element solution of the PDE. Finally, we connect the autoencoder's decoder to the operator network which enables us to solve the inverse problem by optimizing a data-fitting term over the latent space. We analyze the underlying stabilized finite element method in the linear setting and establish an optimal error estimate in the $H^1$-norm. The nonlinear problem is then studied numerically, demonstrating the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04409v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erik Burman, Mats G. Larson, Karl Larsson, Carl Lundholm</dc:creator>
    </item>
    <item>
      <title>Parameter optimization for restarted mixed precision iterative sparse solver</title>
      <link>https://arxiv.org/abs/2412.08059</link>
      <description>arXiv:2412.08059v3 Announce Type: replace 
Abstract: We consider the problem of optimizing the parameter of a two-stage algorithm for approximate solution of a system of linear algebraic equations with a sparse $n\times n$-matrix, i.e., with one in which the number of nonzero elements is $m\!=\!O(n)$. The two-stage algorithm uses conjugate gradient method at its stages. At the 1st stage, an approximate solution with accuracy $\varepsilon_1$ is found for zero initial vector. All numerical values used at this stage are represented as single-precision numbers. The obtained solution is used as initial approximation for an approximate solution with a given accuracy $\varepsilon_2$ that we obtain at the 2nd stage, where double-precision numbers are used. Based on the values of some matrix parameters, computed in a time not exceeding $O(m)$, we need to determine the value $\varepsilon_1$ which minimizes the total computation time at two stages.
  Using single-precision numbers for computations at the 1st stage is advantageous, since the execution time of one iteration will be approximately half that of one iteration at the 2nd stage. At the same time, using machine numbers with half the mantissa length accelerates the growth of the rounding error per iteration of the conjugate gradient method at the 1st stage, which entails an increase in the number of iterations performed at 2nd stage.
  As parameters that allow us to determine $\varepsilon_1$ for the input matrix, we use $n$, $m$, an estimate of the diameter of the graph associated with the matrix, an estimate of the spread of the matrix' eigenvalues, and estimates of its maximum eigenvalue. The optimal or close to the optimal value of $\varepsilon_1$ can be determined for matrix with such a vector of parameters using the nearest neighbor regression or some other type of regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08059v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alexander V. Prolubnikov</dc:creator>
    </item>
    <item>
      <title>An hp Multigrid Approach for Tensor-Product Space-Time Finite Element Discretizations of the Stokes Equations</title>
      <link>https://arxiv.org/abs/2502.09159</link>
      <description>arXiv:2502.09159v2 Announce Type: replace 
Abstract: We present a monolithic $hp$ space-time multigrid method for tensor-product space-time finite element discretizations of the Stokes equations. Geometric and polynomial coarsening of the space-time mesh is performed, and the entire algorithm is expressed through rigorous mathematical mappings. For the discretization, we use inf-sup stable pairs $\mathbb Q_{r+1}/\mathbb P_{r}^{\text{disc}}$ of elements in space and a discontinuous Galerkin (DG$(k)$) discretization in time with piecewise polynomials of order $k$. The key novelty of this work is the application of $hp$ multigrid techniques in space and time, facilitated and accelerated by the matrix-free capabilities of the deal.II library. While multigrid methods are well-established for stationary problems, their application in space-time formulations encounter unique challenges, particularly in constructing suitable smoothers. To overcome these challenges, we employ a space-time cell-wise Vanka smoother. Extensive tests on high-performance computing platforms demonstrate the efficiency of our $hp$ multigrid approach on problem sizes exceeding a trillion degrees of freedom (dofs), sustaining throughputs of hundreds of millions of dofs per second.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09159v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nils Margenberg, Peter Munch, Markus Bause</dc:creator>
    </item>
    <item>
      <title>Analysis of harmonic average method for interface problems with discontinuous solutions and fluxes</title>
      <link>https://arxiv.org/abs/2502.09413</link>
      <description>arXiv:2502.09413v2 Announce Type: replace 
Abstract: Harmonic average method has been widely utilized to deal with heterogeneous coefficients in solving differential equations. One remarkable advantage of the harmonic averaging method is that no derivative of the coefficient is needed. Furthermore, the coefficient matrix of the finite difference equations is an M-matrix which guarantees the stability of the algorithm. It has been numerically observed but not theoretically proved that the method produces second order pointwise accuracy when the solution and flux are continuous even if the coefficient has finite discontinuities for which the method is inconsistent ($O(1)$ in the local truncation errors). It has been believed that there are some fortunate error cancellations. The harmonic average method does not converge when the solution or the flux has finite discontinuities. In this paper, not only we rigorously prove the second order convergence of the harmonic averaging method for one-dimensional interface problem when the coefficient has a finite discontinuities and the solution and the flux are continuous, but also proposed an {\em improved harmonic average method} that is also second order accurate (in the $L^{\infty}$ norm), which allows discontinuous solutions and fluxes along with the discontinuous coefficients. The key in the convergence proof is the construction of the Green's function. The proof shows how the error cancellations occur in a subtle way. Numerical experiments in both 1D and 2D confirmed the theoretical proof of the improved harmonic average method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09413v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Kejia Pan, Hengrui Xu, Zhilin Li</dc:creator>
    </item>
    <item>
      <title>Computing the spectrum and pseudospectrum of infinite-volume operators from local patches</title>
      <link>https://arxiv.org/abs/2403.19055</link>
      <description>arXiv:2403.19055v2 Announce Type: replace-cross 
Abstract: We show how the spectrum of normal discrete short-range infinite-volume operators can be approximated with two-sided error control using only data from finite-sized local patches. As a corollary, we prove the computability of the spectrum of such infinite-volume operators with the additional property of finite local complexity and provide an explicit algorithm. Such operators appear in many applications, e.g. as discretizations of differential operators on unbounded domains or as so-called tight-binding Hamiltonians in solid state physics. For a large class of such operators, our result allows for the first time to establish computationally also the absence of spectrum, i.e. the existence and the size of spectral gaps. We extend our results to the $\varepsilon$-pseudospectrum of non-normal operators, proving that also the pseudospectrum of such operators is computable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19055v2</guid>
      <category>math.SP</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Hege, Massimo Moscolari, Stefan Teufel</dc:creator>
    </item>
    <item>
      <title>Computing MHD equilibria of stellarators with a flexible coordinate frame</title>
      <link>https://arxiv.org/abs/2410.17595</link>
      <description>arXiv:2410.17595v3 Announce Type: replace-cross 
Abstract: For the representation of axi-symmetric plasma configurations, it is natural to use cyl. coordinates (R,Z,$\phi$), where $\phi$ is an independent coordinate. The same cyl. coordinates have also been widely used for representing 3D MHD equilibria of non-axisymmetric configurations (stellarators), with cross-sections, defined in RZ-planes, that vary over $\phi$. Stellarator equilibria have been found, however, for which cyl. coordinates are not at all a natural choice, for instance certain stellarators obtained using the near-axis expansion (NAE), defined by a magn. axis curve and its Frenet frame.
  In this contribution, we propose an alternative approach for representing the boundary in a fixed-boundary 3D MHD equil. solver, moving away from cyl. coordinates. Instead, we use planar cross-sections whose orientation is determined by a general coordinate frame (G-Frame). This frame is similar to the conventional Frenet frame, but more flexible. As an additional part of the boundary representation, it becomes an input to the equil. solve, along with the geometry of the cross-sections. We see two advantages: 1) the capability to easily represent configurations where the magn. axis is highly non-planar or even knotted 2) a reduction in the degrees of freedom needed for the boundary surface, and thus the equil. solver, enabling progress in optimization of these configurations.
  We discuss the properties of the G-Frame, starting from the conventional Frenet frame. Then we show two exemplary ways of constructing it, first from a NAE solution and also from a given boundary surface. We present the details of the implementation of the new frame in the 3D MHD equil. solver GVEC. Furthermore, we demonstrate for a highly shaped QI-optimized stellarator that far fewer degrees of freedom are necessary to find a high quality equil. solution, compared to the solution computed in cyl. coordinates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17595v3</guid>
      <category>physics.plasm-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian J. Hindenlang, Gabriel G. Plunk, Omar Maj</dc:creator>
    </item>
    <item>
      <title>Reduced Order Models and Conditional Expectation -- Analysing Parametric Low-Order Approximations</title>
      <link>https://arxiv.org/abs/2412.19836</link>
      <description>arXiv:2412.19836v2 Announce Type: replace-cross 
Abstract: Systems may depend on parameters which one may control, or which serve to optimise the system, or are imposed externally, or they could be uncertain. This last case is taken as the ``Leitmotiv'' for the following. A reduced order model is produced from the full order model by some kind of projection onto a relatively low-dimensional manifold or subspace. The parameter dependent reduction process produces a function of the parameters into the manifold. One now wants to examine the relation between the full and the reduced state for all possible parameter values of interest. Similarly, in the field of machine learning, also a function of the parameter set into the image space of the machine learning model is learned on a training set of samples, typically minimising the mean-square error. This set may be seen as a sample from some probability distribution, and thus the training is an approximate computation of the expectation, giving an approximation to the conditional expectation, a special case of an Bayesian updating where the Bayesian loss function is the mean-square error. This offers the possibility of having a combined look at these methods, and also of introducing more general loss functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19836v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hermann G. Matthies</dc:creator>
    </item>
    <item>
      <title>Gaussian Processes Regression for Uncertainty Quantification: An Introductory Tutorial</title>
      <link>https://arxiv.org/abs/2502.03090</link>
      <description>arXiv:2502.03090v2 Announce Type: replace-cross 
Abstract: Gaussian Process Regression (GPR) is a powerful nonparametric regression method that is widely used in Uncertainty Quantification (UQ) for constructing surrogate models. This tutorial serves as an introductory guide for beginners, aiming to offer a structured and accessible overview of GPR's applications in UQ. We begin with an introduction to UQ and outline its key tasks, including uncertainty propagation, risk estimation, optimization under uncertainty, parameter estimation, and sensitivity analysis. We then introduce Gaussian Processes (GPs) as a surrogate modeling technique, detailing their formulation, choice of covariance kernels, hyperparameter estimation, and active learning strategies for efficient data acquisition. The tutorial further explores how GPR can be applied to different UQ tasks, including Bayesian quadrature for uncertainty propagation, active learning-based risk estimation, Bayesian optimization for optimization under uncertainty, and surrogate-based sensitivity analysis. Throughout, we emphasize how to leverage the unique formulation of GP for these UQ tasks, rather than simply using it as a standard surrogate model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03090v2</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinglai Li, Hongqiao Wang</dc:creator>
    </item>
  </channel>
</rss>
