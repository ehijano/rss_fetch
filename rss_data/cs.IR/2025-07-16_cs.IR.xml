<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.IR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.IR</link>
    <description>cs.IR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.IR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Jul 2025 04:00:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Extracting Document Relations from Search Corpus by Marginalizing over User Queries</title>
      <link>https://arxiv.org/abs/2507.10726</link>
      <description>arXiv:2507.10726v1 Announce Type: new 
Abstract: Understanding relationships between documents in large-scale corpora is essential for knowledge discovery and information organization. However, existing approaches rely heavily on manual annotation or predefined relationship taxonomies. We propose EDR-MQ (Extracting Document Relations by Marginalizing over User Queries), a novel framework that discovers document relationships through query marginalization. EDR-MQ is based on the insight that strongly related documents often co-occur in results across diverse user queries, enabling us to estimate joint probabilities between document pairs by marginalizing over a collection of queries. To enable this query marginalization approach, we develop Multiply Conditioned Retrieval-Augmented Generation (MC-RAG), which employs conditional retrieval where subsequent document retrievals depend on previously retrieved content. By observing co-occurrence patterns across diverse queries, EDR-MQ estimates joint probabilities between document pairs without requiring labeled training data or predefined taxonomies. Experimental results show that our query marginalization approach successfully identifies meaningful document relationships, revealing topical clusters, evidence chains, and cross-domain connections that are not apparent through traditional similarity-based methods. Our query-driven framework offers a practical approach to document organization that adapts to different user perspectives and information needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10726v1</guid>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuki Iwamoto, Kaoru Tsunoda, Ken Kaneiwa</dc:creator>
    </item>
    <item>
      <title>Overview of the TREC 2022 deep learning track</title>
      <link>https://arxiv.org/abs/2507.10865</link>
      <description>arXiv:2507.10865v1 Announce Type: new 
Abstract: This is the fourth year of the TREC Deep Learning track. As in previous years, we leverage the MS MARCO datasets that made hundreds of thousands of human annotated training labels available for both passage and document ranking tasks. In addition, this year we also leverage both the refreshed passage and document collections that were released last year leading to a nearly $16$ times increase in the size of the passage collection and nearly four times increase in the document collection size. Unlike previous years, in 2022 we mainly focused on constructing a more complete test collection for the passage retrieval task, which has been the primary focus of the track. The document ranking task was kept as a secondary task, where document-level labels were inferred from the passage-level labels. Our analysis shows that similar to previous years, deep neural ranking models that employ large scale pretraining continued to outperform traditional retrieval methods. Due to the focusing our judging resources on passage judging, we are more confident in the quality of this year's queries and judgments, with respect to our ability to distinguish between runs and reuse the dataset in future. We also see some surprises in overall outcomes. Some top-performing runs did not do dense retrieval. Runs that did single-stage dense retrieval were not as competitive this year as they were last year.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10865v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Jimmy Lin, Ellen M. Voorhees, Ian Soboroff</dc:creator>
    </item>
    <item>
      <title>LLM-Driven Dual-Level Multi-Interest Modeling for Recommendation</title>
      <link>https://arxiv.org/abs/2507.10917</link>
      <description>arXiv:2507.10917v1 Announce Type: new 
Abstract: Recently, much effort has been devoted to modeling users' multi-interests based on their behaviors or auxiliary signals. However, existing methods often rely on heuristic assumptions, e.g., co-occurring items indicate the same interest of users, failing to capture user multi-interests aligning with real-world scenarios. While large language models (LLMs) show significant potential for multi-interest analysis due to their extensive knowledge and powerful reasoning capabilities, two key challenges remain. First, the granularity of LLM-driven multi-interests is agnostic, possibly leading to overly fine or coarse interest grouping. Second, individual user analysis provides limited insights due to the data sparsity issue. In this paper, we propose an LLM-driven dual-level multi-interest modeling framework for more effective recommendation. At the user-individual level, we exploit LLMs to flexibly allocate items engaged by users into different semantic clusters, indicating their diverse and distinct interests. To alleviate the agnostic generation of LLMs, we adaptively assign these semantic clusters to users' collaborative multi-interests learned from global user-item interactions, allowing the granularity to be automatically adjusted according to the user's behaviors using an alignment module. To alleviate the limited insights derived from individual users' behaviors, at the user-crowd level, we propose aggregating user cliques into synthesized users with rich behaviors for more comprehensive LLM-driven multi-interest analysis. We formulate a max covering problem to ensure the compactness and representativeness of synthesized users' behaviors, and then conduct contrastive learning based on their LLM-driven multi-interests to disentangle item representations among different interests. Experiments on real-world datasets show the superiority of our approach against state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10917v1</guid>
      <category>cs.IR</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyan Wang, Yingpeng Du, Zhu Sun, Jieyi Bi, Haoyan Chua, Tianjun Wei, Jie Zhang</dc:creator>
    </item>
    <item>
      <title>Unraveling the Biomarker Prospects of High-Altitude Diseases: Insights from Biomolecular Event Network Constructed using Text Mining</title>
      <link>https://arxiv.org/abs/2507.10953</link>
      <description>arXiv:2507.10953v1 Announce Type: new 
Abstract: High-altitude diseases (HAD), encompassing acute mountain sickness (AMS), high-altitude cerebral edema (HACE), and high-altitude pulmonary edema (HAPE), are triggered by hypobaric hypoxia at elevations above 2,500 meters. These conditions pose significant health risks, yet the molecular mechanisms remain insufficiently understood. In this study, we developed a biomolecular event extraction pipeline integrating supervised machine learning with feature-based and multiscale Laplacian graph kernels to analyze 7,847 curated HAD-related abstracts from PubMed. We extracted over 150 unique biomolecular events including gene expression, regulation, binding, and localization and constructed a weighted, undirected biomolecular event network comprising 97 nodes and 153 edges. Using the PageRank algorithm, we prioritized key biomolecules based on their centrality within the event network. The top-ranked proteins included Erythropoietin (EPO) (0.0163), Vascular endothelial growth factor (VEGF) (0.0148), Hypoxia-inducible factor 1 (HIF-1) alpha (0.0136), Endothelial PAS Domain Protein 1 (EPAS1) and Angiotensin-Converting Enzyme (ACE) (0.0119), Egl nine homolog 1 (EGLN1), Endothelin 1 (ET-1), and 70 kilodalton heat shock protein (Hsp70)(0.0118), all of which play crucial roles in oxygen sensing, vascular remodeling, erythropoiesis, and blood pressure regulation. Subnetwork analysis revealed three major functional clusters centered on hypoxia response, inflammation, and stress adaptation pathways. Our integrative approach demonstrates the utility of large-scale text mining and graph-based analysis to uncover mechanistic insights and prioritize potential biomarkers for high-altitude disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10953v1</guid>
      <category>cs.IR</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Balu Bhasuran, Sabenabanu Abdulkadhar, Jeyakumar Natarajan</dc:creator>
    </item>
    <item>
      <title>Aligned Query Expansion: Efficient Query Expansion for Information Retrieval through LLM Alignment</title>
      <link>https://arxiv.org/abs/2507.11042</link>
      <description>arXiv:2507.11042v1 Announce Type: new 
Abstract: With the breakthroughs in large language models (LLMs), query generation techniques that expand documents and queries with related terms are becoming increasingly popular in the information retrieval field. Such techniques have been shown to improve the effectiveness of traditional lexical retrieval methods by dealing with the vocabulary mismatch problem. Recent work has found that generating queries with a greedy decoding strategy can produce sub-optimal queries, including hallucinations, and proposed to filter out queries before expansion. This `generate-then-filter' approach is costly, as it requires generating multiple queries and applying a relevance model to all of them and does not teach the LLM which of the generated queries is more effective for expansion. To overcome such limitations, we propose Aligned Query Expansion (AQE), a novel approach to enhance query expansion for passage retrieval in open-domain question answering. AQE leverages recent techniques in LLM alignment to fine-tune models for generating query expansions that directly optimize the effectiveness of the retrieval task, eliminating the need for additional filtering steps. This alignment ensures that queries are more relevant, reducing computational costs while improving retrieval effectiveness. Empirical evaluations show that AQE outperforms baseline models for query expansion in both in-domain and out-of-domain settings, demonstrating significant improvements in retrieval effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11042v1</guid>
      <category>cs.IR</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Adam Yang, Gustavo Penha, Enrico Palumbo, Hugues Bouchard</dc:creator>
    </item>
    <item>
      <title>From Chaos to Automation: Enabling the Use of Unstructured Data for Robotic Process Automation</title>
      <link>https://arxiv.org/abs/2507.11364</link>
      <description>arXiv:2507.11364v1 Announce Type: new 
Abstract: The growing volume of unstructured data within organizations poses significant challenges for data analysis and process automation. Unstructured data, which lacks a predefined format, encompasses various forms such as emails, reports, and scans. It is estimated to constitute approximately 80% of enterprise data. Despite the valuable insights it can offer, extracting meaningful information from unstructured data is more complex compared to structured data. Robotic Process Automation (RPA) has gained popularity for automating repetitive tasks, improving efficiency, and reducing errors. However, RPA is traditionally reliant on structured data, limiting its application to processes involving unstructured documents. This study addresses this limitation by developing the UNstructured Document REtrieval SyStem (UNDRESS), a system that uses fuzzy regular expressions, techniques for natural language processing, and large language models to enable RPA platforms to effectively retrieve information from unstructured documents. The research involved the design and development of a prototype system, and its subsequent evaluation based on text extraction and information retrieval performance. The results demonstrate the effectiveness of UNDRESS in enhancing RPA capabilities for unstructured data, providing a significant advancement in the field. The findings suggest that this system could facilitate broader RPA adoption across processes traditionally hindered by unstructured data, thereby improving overall business process efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11364v1</guid>
      <category>cs.IR</category>
      <category>cs.SE</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kelly Kurowski, Xixi Lu, Hajo A. Reijers</dc:creator>
    </item>
    <item>
      <title>Repairing Language Model Pipelines by Meta Self-Refining Competing Constraints at Runtime</title>
      <link>https://arxiv.org/abs/2507.10590</link>
      <description>arXiv:2507.10590v1 Announce Type: cross 
Abstract: Language Model (LM) pipelines can dynamically refine their outputs against programmatic constraints. However, their effectiveness collapses when faced with competing soft constraints, leading to inefficient backtracking loops where satisfying one constraint violates another. We introduce Meta Self-Refining, a framework that equips LM pipelines with a meta-corrective layer to repair these competitions at runtime/inference-time. Our approach monitors the pipeline's execution history to detect oscillatory failures. Upon detection, it invokes a meta-repairer LM that analyzes the holistic state of the backtracking attempts and synthesizes a strategic instruction to balance the competing requirements. This self-repair instruction guides the original LM out of a failing refining loop towards a successful output. Our results show Meta Self-Refining can successfully repair these loops, leading to more efficient LM programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10590v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mojtaba Eshghie</dc:creator>
    </item>
    <item>
      <title>Access Control for Information-Theoretically Secure Key-Document Stores</title>
      <link>https://arxiv.org/abs/2507.10730</link>
      <description>arXiv:2507.10730v1 Announce Type: cross 
Abstract: This paper presents a novel key-based access control technique for secure outsourcing key-value stores where values correspond to documents that are indexed and accessed using keys. The proposed approach adopts Shamir's secret-sharing that offers unconditional or information-theoretic security. It supports keyword-based document retrieval while preventing leakage of the data, access rights of users, or the size (\textit{i}.\textit{e}., volume of the output that satisfies a query). The proposed approach allows servers to detect (and abort) malicious clients from gaining unauthorized access to data, and prevents malicious servers from altering data undetected while ensuring efficient access -- it takes 231.5ms over 5,000 keywords across 500,000 files.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10730v1</guid>
      <category>cs.CR</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yin Li, Sharad Mehrota, Shantanu Sharma, Komal Kumari</dc:creator>
    </item>
    <item>
      <title>Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs</title>
      <link>https://arxiv.org/abs/2507.10772</link>
      <description>arXiv:2507.10772v1 Announce Type: cross 
Abstract: Labeled property graphs often contain rich textual attributes that can enhance analytical tasks when properly leveraged. This work explores the use of pretrained text embedding models to enable efficient semantic analysis in such graphs. By embedding textual node and edge properties, we support downstream tasks including node classification and relation prediction with improved contextual understanding. Our approach integrates language model embeddings into the graph pipeline without altering its structure, demonstrating that textual semantics can significantly enhance the accuracy and interpretability of property graph analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10772v1</guid>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michal Podstawski</dc:creator>
    </item>
    <item>
      <title>Automated Thematic Analyses Using LLMs: Xylazine Wound Management Social Media Chatter Use Case</title>
      <link>https://arxiv.org/abs/2507.10803</link>
      <description>arXiv:2507.10803v1 Announce Type: cross 
Abstract: Background Large language models (LLMs) face challenges in inductive thematic analysis, a task requiring deep interpretive and domain-specific expertise. We evaluated the feasibility of using LLMs to replicate expert-driven thematic analysis of social media data. Methods Using two temporally non-intersecting Reddit datasets on xylazine (n=286 and n=686, for model optimization and validation, respectively) with twelve expert-derived themes, we evaluated five LLMs against expert coding. We modeled the task as a series of binary classifications, rather than a single, multi-label classification, employing zero-, single-, and few-shot prompting strategies and measuring performance via accuracy, precision, recall, and F1-score. Results On the validation set, GPT-4o with two-shot prompting performed best (accuracy: 90.9%; F1-score: 0.71). For high-prevalence themes, model-derived thematic distributions closely mirrored expert classifications (e.g., xylazine use: 13.6% vs. 17.8%; MOUD use: 16.5% vs. 17.8%). Conclusions Our findings suggest that few-shot LLM-based approaches can automate thematic analyses, offering a scalable supplement for qualitative research. Keywords: thematic analysis, large language models, natural language processing, qualitative analysis, social media, prompt engineering, public health</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10803v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.ET</category>
      <category>cs.IR</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>JaMor Hairston, Ritvik Ranjan, Sahithi Lakamana, Anthony Spadaro, Selen Bozkurt, Jeanmarie Perrone, Abeed Sarker</dc:creator>
    </item>
    <item>
      <title>Seq vs Seq: An Open Suite of Paired Encoders and Decoders</title>
      <link>https://arxiv.org/abs/2507.11412</link>
      <description>arXiv:2507.11412v1 Announce Type: cross 
Abstract: The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets. We introduce the SOTA open-data Ettin suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens. Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders. Like previous work, we find that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks. However, we show that adapting a decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa for generative tasks). We open-source all artifacts of this study including training data, training order segmented by checkpoint, and 200+ checkpoints to allow future work to analyze or extend all aspects of training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11412v1</guid>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Orion Weller, Kathryn Ricci, Marc Marone, Antoine Chaffin, Dawn Lawrie, Benjamin Van Durme</dc:creator>
    </item>
    <item>
      <title>Repeated Padding+: Simple yet Effective Data Augmentation Plugin for Sequential Recommendation</title>
      <link>https://arxiv.org/abs/2403.06372</link>
      <description>arXiv:2403.06372v3 Announce Type: replace 
Abstract: Sequential recommendation aims to provide users with personalized suggestions based on their historical interactions. When training sequential models, padding is a widely adopted technique for two main reasons: 1) The vast majority of models can only handle fixed-length sequences; 2) Batching-based training needs to ensure that the sequences in each batch have the same length. The special value \emph{0} is usually used as the padding content, which does not contain the actual information and is ignored in the model calculations. This common-sense padding strategy leads us to a problem that has never been explored before: Can we fully utilize this idle input space by padding other content to further improve model performance and training efficiency?
  In this work, we propose a simple yet effective padding method called Repeated Padding+ (RepPad+). Specifically, we use the original interaction sequences as the padding content and fill it to the padding positions during model training. This operation can be performed a finite number of times or repeated until the input sequences' length reaches the maximum limit. For those sequences that can not pad full original data, we draw inspiration from the Sliding Windows strategy and intercept consecutive subsequences to fill in the idle space. Our RepPad+ can be viewed as a sequence-level data augmentation strategy. Unlike most existing works, our method contains no trainable parameters or hyperparameters and is a plug-and-play data augmentation operation. Extensive experiments on various categories of sequential models and seven real-world datasets demonstrate the effectiveness and efficiency of our approach. The average recommendation performance improvement is up to 84.11% on GRU4Rec and 35.34% on SASRec. We also provide in-depth analysis and explanation of what makes RepPad+ effective from multiple perspectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06372v3</guid>
      <category>cs.IR</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yizhou Dang, Yuting Liu, Enneng Yang, Guibing Guo, Linying Jiang, Jianzhe Zhao, Xingwei Wang</dc:creator>
    </item>
    <item>
      <title>CART: A Generative Cross-Modal Retrieval Framework with Coarse-To-Fine Semantic Modeling</title>
      <link>https://arxiv.org/abs/2406.17507</link>
      <description>arXiv:2406.17507v2 Announce Type: replace 
Abstract: Cross-modal retrieval aims to search for instances, which are semantically related to the query through the interaction of different modal data. Traditional solutions utilize a single-tower or dual-tower framework to explicitly compute the score between queries and candidates, which is challenged by training cost and inference latency with large-scale data. Inspired by the remarkable performance and efficiency of generative models, we propose a generative cross-modal retrieval framework (CART) based on coarse-to-fine semantic modeling, which assigns identifiers to each candidate and treats the generating identifier as the retrieval target. Specifically, we explore an effective coarse-to-fine scheme, combining K-Means and RQ-VAE to discretize multimodal data into token sequences that support autoregressive generation. Further, considering the lack of explicit interaction between queries and candidates, we propose a feature fusion strategy to align their semantics. Extensive experiments demonstrate the effectiveness of the strategies in the CART, achieving excellent results in both retrieval performance and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17507v2</guid>
      <category>cs.IR</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minghui Fang, Shengpeng Ji, Jialong Zuo, Hai Huang, Yan Xia, Jieming Zhu, Xize Cheng, Xiaoda Yang, Wenrui Liu, Gang Wang, Zhenhua Dong, Zhou Zhao</dc:creator>
    </item>
    <item>
      <title>Big data searching using words</title>
      <link>https://arxiv.org/abs/2409.15346</link>
      <description>arXiv:2409.15346v3 Announce Type: replace 
Abstract: Big data analytics is one of the most promising areas of new research and development in computer science, enterprises, e-commerce, and defense. For many organizations, big data is considered one of their most important strategic assets. This explosive growth has made it necessary to develop effective techniques for examining and analyzing big data from mathematical perspectives. Among various methods of analyzing big data, topological data analysis (TDA) is now considered one of the useful tools. However, there is no fundamental concept related to the topological structure in big data. In this paper, we present fundamental concepts related to the neighborhood structures of words in big data search, laying the groundwork for developing topological frameworks for big data in the future. We also introduce the notion of big data primal within the context of big data search and explore how neighborhood structures, combined with the Jaccard similarity coefficient, can be utilized to detect anomalies in search behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15346v3</guid>
      <category>cs.IR</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>New Mathematics and Natural Computation, 2025</arxiv:journal_reference>
      <dc:creator>Santanu Acharjee, Ripunjoy Choudhury</dc:creator>
    </item>
    <item>
      <title>Are AI Agents interacting with Online Ads?</title>
      <link>https://arxiv.org/abs/2504.07112</link>
      <description>arXiv:2504.07112v4 Announce Type: replace 
Abstract: As AI-driven agents become increasingly integrated into the digital ecosystem, they reshape how online advertising is perceived and processed. Particularly in the travel and hotel booking sector, these autonomous systems influence the effectiveness of traditional advertising formats. While visual cues and emotional appeals sway human users, AI agents prioritize structured data such as price, availability, and specifications. This study examines how different AI agents interact with online advertising, whether they incorporate ads into their decision-making processes, and which ad formats prove most effective. We analyze interaction patterns, click behavior, and decision-making strategies through experiments with multimodal language models such as OpenAI GPT-4o, Anthropic Claude 3.7 Sonnet, and Google Gemini 2.0 Flash. Our findings reveal that AI agents neither ignore nor systematically avoid advertisements but instead favor certain features-particularly keywords and structured data. These insights have significant implications for the future design of advertising strategies in AI-dominated digital environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07112v4</guid>
      <category>cs.IR</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas St\"ockl, Joel Nitu</dc:creator>
    </item>
    <item>
      <title>Alleviating User-Sensitive bias with Fair Generative Sequential Recommendation Model</title>
      <link>https://arxiv.org/abs/2506.19777</link>
      <description>arXiv:2506.19777v2 Announce Type: replace 
Abstract: Recommendation fairness has recently attracted much attention. In the real world, recommendation systems are driven by user behavior, and since users with the same sensitive feature (e.g., gender and age) tend to have the same patterns, recommendation models can easily capture the strong correlation preference of sensitive features and thus cause recommendation unfairness. Diffusion model (DM) as a new generative model paradigm has achieved great success in recommendation systems. DM's ability to model uncertainty and represent diversity, and its modeling mechanism has a high degree of adaptability with the real-world recommendation process with bias. Therefore, we use DM to effectively model the fairness of recommendation and enhance the diversity. This paper proposes a FairGENerative sequential Recommendation model based on DM, FairGENRec. In the training phase, we inject random noise into the original distribution under the guidance of the sensitive feature recognition model, and a sequential denoise model is designed for the reverse reconstruction of items. Simultaneously, recommendation fairness modeling is completed by injecting multi-interests representational information that eliminates the bias of sensitive user features into the generated results. In the inference phase, the model obtains the noise in the form of noise addition by using the history interactions which is followed by reverse iteration to reconstruct the target item representation. Finally, our extensive experiments on three datasets demonstrate the dual enhancement effect of FairGENRec on accuracy and fairness, while the statistical analysis of the cases visualizes the degree of improvement on the fairness of the recommendation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19777v2</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Liu, Feng Wu, Xuefang Zhu</dc:creator>
    </item>
    <item>
      <title>A quantum semantic framework for natural language processing</title>
      <link>https://arxiv.org/abs/2506.10077</link>
      <description>arXiv:2506.10077v2 Announce Type: replace-cross 
Abstract: Semantic degeneracy represents a fundamental property of natural language that extends beyond simple polysemy to encompass the combinatorial explosion of potential interpretations that emerges as semantic expressions increase in complexity. In this work, we argue this property imposes fundamental limitations on Large Language Models (LLMs) and other modern NLP systems, precisely because they operate within natural language itself. Using Kolmogorov complexity, we demonstrate that as an expression's complexity grows, the amount of contextual information required to reliably resolve its ambiguity explodes combinatorially. The computational intractability of recovering a single intended meaning for complex or ambiguous text therefore suggests that the classical view that linguistic forms possess intrinsic meaning in and of themselves is conceptually inadequate. We argue instead that meaning is dynamically actualized through an observer-dependent interpretive act, a process whose non-deterministic nature is most appropriately described by a non-classical, quantum-like logic. To test this hypothesis, we conducted a semantic Bell inequality test using diverse LLM agents. Our experiments yielded average CHSH expectation values from 1.2 to 2.8, with several runs producing values (e.g., 2.3-2.4) in significant violation of the classical boundary ($|S|\leq2$), demonstrating that linguistic interpretation under ambiguity can exhibit non-classical contextuality, consistent with results from human cognition experiments. These results inherently imply that classical frequentist-based analytical approaches for natural language are necessarily lossy. Instead, we propose that Bayesian-style repeated sampling approaches can provide more practically useful and appropriate characterizations of linguistic meaning in context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10077v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher J. Agostino, Quan Le Thien, Molly Apsel, Denizhan Pak, Elina Lesyk, Ashabari Majumdar</dc:creator>
    </item>
    <item>
      <title>Evaluating Multimodal Large Language Models on Educational Textbook Question Answering</title>
      <link>https://arxiv.org/abs/2506.21596</link>
      <description>arXiv:2506.21596v2 Announce Type: replace-cross 
Abstract: Multimodal large language models (MLLMs) have shown success in vision-language tasks, but their ability to reason over complex educational materials remains largely untested. This work presents the first evaluation of state-of-the-art MLLMs, including LLaVA-1.5 and LLaMA 3.2-Vision, on the textbook question answering (TQA) task using the CK12-QA dataset. We introduce a multimodal retrieval-augmented generation (RAG) pipeline to simulate real-world learning by providing relevant lesson paragraphs and diagrams as context. Our zero-shot experiments reveal a critical trade-off: while retrieved context improves LLaVA's performance on text-based questions, it significantly degrades the accuracy of the more powerful LLaMA 3.2-Vision on diagram-based tasks, dropping its validation accuracy from 74.07% to 25.93%. We term this statistically significant phenomenon "catastrophic context interference." Furthermore, fine-tuning highlights architectural differences: LLaMA 3.2-Vision's performance improves to 71.16% on the test set, demonstrating its capacity to learn multimodal integration, whereas LLaVA's performance declines, indicating challenges with generalization. Our results underscore the challenges MLLMs face in modality prioritization and context integration, providing a benchmark and pointing to key directions for developing more robust AI-driven educational tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21596v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hessa A. Alawwad, Anas Zafar, Areej Alhothali, Usman Naseem, Ali Alkhathlan, Amani Jamal</dc:creator>
    </item>
    <item>
      <title>RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism</title>
      <link>https://arxiv.org/abs/2507.02962</link>
      <description>arXiv:2507.02962v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, while they remain prone to generating hallucinated or outdated responses due to their static internal knowledge. Recent advancements in Retrieval-Augmented Generation (RAG) methods have explored enhancing models' search and reasoning capabilities through reinforcement learning (RL). Although these methods demonstrate promising results, they face challenges in training stability and encounter issues such as substantial inference time and restricted capabilities due to the single-query mode. In this paper, we propose RAG-R1, a novel training framework designed to enable LLMs to adaptively leverage internal and external knowledge during the reasoning process. We further expand the generation and retrieval processes within the framework from single-query mode to multi-query parallelism, aimed at reducing inference time and enhancing the model's capabilities. Extensive experiments on seven question-answering benchmarks demonstrate that our method outperforms the strongest baseline by up to 13.2% and decreases inference time by 11.1%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02962v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiwen Tan, Jiaming Huang, Qintong Wu, Hongxuan Zhang, Chenyi Zhuang, Jinjie Gu</dc:creator>
    </item>
  </channel>
</rss>
