<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Feb 2025 02:56:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Predicting Long-term Urban Overheating and Their Mitigations from Nature Based Solutions Using Machine Learning and Field Measurements</title>
      <link>https://arxiv.org/abs/2502.18647</link>
      <description>arXiv:2502.18647v1 Announce Type: new 
Abstract: Urban overheating, exacerbated by climate change, threatens public health and urban sustainability. Traditional approaches, such as numerical simulations and field measurements, face challenges due to uncertainties in input data. This study integrates field measurements with machine learning models to predict the duration and severity of future urban overheating events, focusing on the role of urban greening under different global warming (GW) scenarios. Field measurements were conducted in summer 2024 at an office campus in Ottawa, a cold-climate city. Microclimate data were collected from four locations with varying levels of greenery: a large lawn without trees (Lawn), a parking lot without greenery (Parking), an area with sparsely distributed trees (Tree), and a fully covered forested area (Forest). Machine learning models, including Artificial Neural Networks (ANN), Recurrent Neural Networks (RNN), and Long Short-Term Memory (LSTM) networks, were trained on local microclimate data, with LSTM achieving the best predictions. Four GW scenarios were analyzed, corresponding to different Shared Socioeconomic Pathways (SSP) for 2050 and 2090. Results show that the Universal Thermal Climate Index (UTCI) at the "Parking" location rises from about 27,\textdegree C under GW1.0 to 31,\textdegree C under GW3.5. Moreover, low health risk conditions (UTCI &gt; 26,\textdegree C) increase across all locations due to climate change, regardless of greenery levels. However, tree-covered areas such as "Tree" and "Forest" effectively prevent extreme heat conditions (UTCI &gt; 38.9,\textdegree C). These findings highlight the crucial role of urban greening in mitigating severe thermal stress and enhancing thermal comfort under future climate scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18647v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiwei Zou (Leon), Lin Wang (Leon), Senwen Yang (Leon), Michael Lacasse (Leon),  Liangzhu (Leon),  Wang</dc:creator>
    </item>
    <item>
      <title>Forecasting intermittent time series with Gaussian Processes and Tweedie likelihood</title>
      <link>https://arxiv.org/abs/2502.19086</link>
      <description>arXiv:2502.19086v2 Announce Type: cross 
Abstract: We introduce the use of Gaussian Processes (GPs) for the probabilistic forecasting of intermittent time series. The model is trained in a Bayesian framework that accounts for the uncertainty about the latent function and marginalizes it out when making predictions. We couple the latent GP variable with two types of forecast distributions: the negative binomial (NegBinGP) and the Tweedie distribution (TweedieGP). While the negative binomial has already been used in forecasting intermittent time series, this is the first time in which a fully parameterized Tweedie density is used for intermittent time series. We properly evaluate the Tweedie density, which is both zero-inflated and heavy tailed, avoiding simplifying assumptions made in existing models. We test our models on thousands of intermittent count time series. Results show that our models provide consistently better probabilistic forecasts than the competitors. In particular, TweedieGP obtains the best estimates of the highest quantiles, thus showing that it is more flexible than NegBinGP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19086v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefano Damato, Dario Azzimonti, Giorgio Corani</dc:creator>
    </item>
    <item>
      <title>Age Group Sensitivity Analysis of Epidemic Models: Investigating the Impact of Contact Matrix Structure</title>
      <link>https://arxiv.org/abs/2502.19206</link>
      <description>arXiv:2502.19206v1 Announce Type: cross 
Abstract: Understanding the role of different age groups in disease transmission is crucial for designing effective intervention strategies. A key parameter in age-structured epidemic models is the contact matrix, which defines the interaction structure between age groups. However, accurately estimating contact matrices is challenging, as different age groups respond differently to surveys and are accessible through different channels. This variability introduces significant epistemic uncertainty in epidemic models.
  In this study, we introduce the Age Group Sensitivity Analysis (AGSA) method, a novel framework for assessing the impact of age-structured contact patterns on epidemic outcomes. Our approach integrates age-stratified epidemic models with Latin Hypercube Sampling (LHS) and the Partial Rank Correlation Coefficient (PRCC) method, enabling a systematic sensitivity analysis of age-specific interactions. Additionally, we propose a new sensitivity aggregation technique that quantifies the contribution of each age group to key epidemic parameters.
  By identifying the age groups to which the model is most sensitive, AGSA helps pinpoint those that introduce the greatest epistemic uncertainty. This allows for targeted data collection efforts, focusing surveys and empirical studies on the most influential age groups to improve model accuracy. As a result, AGSA can enhance epidemic forecasting and inform the design of more effective and efficient public health interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19206v1</guid>
      <category>q-bio.QM</category>
      <category>math.DS</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zsolt Vizi, Evans Kiptoo Korir, Norbert Bogya, Csaba Roszt\'oczy, G\'eza Makay, P\'eter Boldog</dc:creator>
    </item>
    <item>
      <title>Arctic teleconnection on climate and ozone pollution in the polar jet stream path of eastern US</title>
      <link>https://arxiv.org/abs/2502.19234</link>
      <description>arXiv:2502.19234v1 Announce Type: cross 
Abstract: Arctic sea ice is in reduction and has been a key significant indicator of climate change. In this paper, we explore Arctic Sea ice extent data to identify teleconnection with weather change in the polar and sub-tropical jet stream intersection in eastern United States (US) and hence the potential influence in ground level ozone pollution. Several statistical methods including Bayesian techniques such as: spatio-temporal modelling and Bayesian network are implemented to identify the teleconnection and also validated based on theories in atmospheric science. We observe that the teleconnection is relatively strong in autumn, winter and spring seasons compared to the summer. Furthermore, the sudden decremental effect of Arctic sea-ice extent in mid-2000s has a shifting influence in ozone pollutions compared to the previous years. A similar downward shift in the Arctic sea-ice extent has been projected in 2030. These findings indicate to initiate further strategic policies for the Arctic influence, ozone concentrations together the seasonal and global changing patterns of climate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19234v1</guid>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>K Shuvo Bakar, Sourish Das, Sudeep Shukla, Anirban Chakraborti</dc:creator>
    </item>
    <item>
      <title>Enhancing Gradient-based Discrete Sampling via Parallel Tempering</title>
      <link>https://arxiv.org/abs/2502.19240</link>
      <description>arXiv:2502.19240v1 Announce Type: cross 
Abstract: While gradient-based discrete samplers are effective in sampling from complex distributions, they are susceptible to getting trapped in local minima, particularly in high-dimensional, multimodal discrete distributions, owing to the discontinuities inherent in these landscapes. To circumvent this issue, we combine parallel tempering, also known as replica exchange, with the discrete Langevin proposal and develop the Parallel Tempering enhanced Discrete Langevin Proposal (PTDLP), which are simulated at a series of temperatures. Significant energy differences prompt sample swaps, which are governed by a Metropolis criterion specifically designed for discrete sampling to ensure detailed balance is maintained. Additionally, we introduce an automatic scheme to determine the optimal temperature schedule and the number of chains, ensuring adaptability across diverse tasks with minimal tuning. Theoretically, we establish that our algorithm converges non-asymptotically to the target energy and exhibits faster mixing compared to a single chain. Empirical results further emphasize the superiority of our method in sampling from complex, multimodal discrete distributions, including synthetic problems, restricted Boltzmann machines, and deep energy-based models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19240v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luxu Liang, Yuhang Jia, Feng Zhou</dc:creator>
    </item>
    <item>
      <title>Prediction intervals for overdispersed binomial endpoints and their application to toxicological historical control data</title>
      <link>https://arxiv.org/abs/2407.13296</link>
      <description>arXiv:2407.13296v3 Announce Type: replace 
Abstract: For toxicology studies, the validation of the concurrent control group by historical control data (HCD) has become requirements. This validation is usually done by historical control limits (HCL), which should cover the observations of the concurrent control with a predefined level of confidence. In many applications, HCL are applied to dichotomous data, e.g. the number of rats with a tumor vs. the number of rats without a tumor (carcinogenicity studies) or the number of cells with a micronucleus out of a total number of cells. Dichotomous HCD may be overdispersed and can be heavily right- (or left-) skewed, which is usually not taken into account in the practical applications of HCL. To overcome this problem, four different prediction intervals (two frequentist, two Bayesian), that can be applied to such data, are proposed. Based on comprehensive Monte-Carlo simulations, the coverage probabilities of the proposed prediction intervals were compared to heuristical HCL typically used in daily toxicological routine (historical range, limits of the np-chart, mean plus minus 2 SD). Our simulations reveal, that frequentist bootstrap calibrated prediction intervals control the type-1-error best, but, also prediction intervals calculated based on Bayesian generalized linear mixed models appear to be practically applicable. Contrary, all heuristics fail to control the type-1-error. The application of HCL is demonstrated based on a real life data set containing historical controls from long-term carcinogenicity studies run on behalf of the U.S. National Toxicology Program. The proposed frequentist prediction intervals are publicly available from the R package predint, whereas R code for the computation of the two Bayesian prediction intervals is provided via GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13296v3</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Max Menssen, Jonathan Rathjens</dc:creator>
    </item>
    <item>
      <title>Statistical enhanced learning for modeling and prediction tennis matches at Grand Slam tournaments</title>
      <link>https://arxiv.org/abs/2502.01613</link>
      <description>arXiv:2502.01613v2 Announce Type: replace 
Abstract: In this manuscript, we concentrate on a specific type of covariates, which we call statistically enhanced, for modeling tennis matches for men at Grand slam tournaments. Our goal is to assess whether these enhanced covariates have the potential to improve statistical learning approaches, in particular, with regard to the predictive performance. For this purpose, various proposed regression and machine learning model classes are compared with and without such features. To achieve this, we considered three slightly enhanced variables, namely elo rating along with two different player age variables. This concept has already been successfully applied in football, where additional team ability parameters, which were obtained from separate statistical models, were able to improve the predictive performance.
  In addition, different interpretable machine learning (IML) tools are employed to gain insights into the factors influencing the outcomes of tennis matches predicted by complex machine learning models, such as the random forest. Specifically, partial dependence plots (PDP) and individual conditional expectation (ICE) plots are employed to provide better interpretability for the most promising ML model from this work. Furthermore, we conduct a comparison of different regression and machine learning approaches in terms of various predictive performance measures such as classification rate, predictive Bernoulli likelihood, and Brier score. This comparison is carried out on external test data using cross-validation, rolling window, and expanding window strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01613v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nourah Buhamra, Andreas Groll</dc:creator>
    </item>
    <item>
      <title>Forecasting macroeconomic data with Bayesian VARs: Sparse or dense? It depends!</title>
      <link>https://arxiv.org/abs/2206.04902</link>
      <description>arXiv:2206.04902v5 Announce Type: replace-cross 
Abstract: Vector autogressions (VARs) are widely applied when it comes to modeling and forecasting macroeconomic variables. In high dimensions, however, they are prone to overfitting. Bayesian methods, more concretely shrinkage priors, have shown to be successful in improving prediction performance. In the present paper, we introduce the semi-global framework, in which we replace the traditional global shrinkage parameter with group-specific shrinkage parameters. We show how this framework can be applied to various shrinkage priors, such as global-local priors and stochastic search variable selection priors. We demonstrate the virtues of the proposed framework in an extensive simulation study and in an empirical application forecasting data of the US economy. Further, we shed more light on the ongoing ``Illusion of Sparsity'' debate, finding that forecasting performances under sparse/dense priors vary across evaluated economic variables and across time frames. Dynamic model averaging, however, can combine the merits of both worlds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.04902v5</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ijforecast.2025.02.001</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Forecasting (2025)</arxiv:journal_reference>
      <dc:creator>Luis Gruber, Gregor Kastner</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference of Reproduction Number from Epidemiological and Genetic Data Using Particle MCMC</title>
      <link>https://arxiv.org/abs/2311.09838</link>
      <description>arXiv:2311.09838v2 Announce Type: replace-cross 
Abstract: Inference of the reproduction number through time is of vital importance during an epidemic outbreak. Typically, epidemiologists tackle this using observed prevalence or incidence data. However, prevalence and incidence data alone is often noisy or partial. Models can also have identifiability issues with determining whether a large amount of a small epidemic or a small amount of a large epidemic has been observed. Sequencing data however is becoming more abundant, so approaches which can incorporate genetic data are an active area of research. We propose using particle MCMC methods to infer the time-varying reproduction number from a combination of prevalence data reported at a set of discrete times and a dated phylogeny reconstructed from sequences. We validate our approach on simulated epidemics with a variety of scenarios. We then apply the method to real data sets of HIV-1 in North Carolina, USA and tuberculosis in Buenos Aires, Argentina. The models and algorithms are implemented in an open source R package called EpiSky which is available at https://github.com/alicia-gill/EpiSky.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09838v2</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alicia Gill, Jere Koskela, Xavier Didelot, Richard G. Everitt</dc:creator>
    </item>
    <item>
      <title>Seemingly unrelated Bayesian additive regression trees for cost-effectiveness analyses in healthcare</title>
      <link>https://arxiv.org/abs/2404.02228</link>
      <description>arXiv:2404.02228v3 Announce Type: replace-cross 
Abstract: In recent years, theoretical results and simulation evidence have shown Bayesian additive regression trees to be a highly-effective method for nonparametric regression. Motivated by cost-effectiveness analyses in health economics, where interest lies in jointly modelling the costs of healthcare treatments and the associated health-related quality of life experienced by a patient, we propose a multivariate extension of BART which is applicable in regression analyses with several dependent outcome variables. Our framework allows for continuous or binary outcomes and overcomes some key limitations of existing multivariate BART models by allowing each individual response to be associated with different ensembles of trees, while still handling dependencies between the outcomes. In the case of continuous outcomes, our model is essentially a nonparametric version of seemingly unrelated regression. Likewise, our proposal for binary outcomes is a nonparametric generalisation of the multivariate probit model. We give suggestions for easily interpretable prior distributions, which allow specification of both informative and uninformative priors. We provide detailed discussions of MCMC sampling methods to conduct posterior inference. Our methods are implemented in the R package "subart". We showcase their performance through extensive simulation experiments and an application to an empirical case study from health economics. By also accommodating propensity scores in a manner befitting a causal analysis, we find substantial evidence for a novel trauma care intervention's cost-effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02228v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Esser, Mateus Maia, Andrew C. Parnell, Judith Bosmans, Hanneke van Dongen, Thomas Klausch, Keefe Murphy</dc:creator>
    </item>
    <item>
      <title>Probabilistic Multi-Layer Perceptrons for Wind Farm Condition Monitoring</title>
      <link>https://arxiv.org/abs/2404.16496</link>
      <description>arXiv:2404.16496v2 Announce Type: replace-cross 
Abstract: We provide a condition monitoring system for wind farms, based on normal behaviour modelling using a probabilistic multi-layer perceptron with transfer learning via fine-tuning. The model predicts the output power of the wind turbine under normal behaviour based on features retrieved from supervisory control and data acquisition (SCADA) systems. Its advantages are that (i) it can be trained with SCADA data of at least a few years, (ii) it can incorporate all SCADA data of all wind turbines in a wind farm as features, (iii) it assumes that the output power follows a normal density with heteroscedastic variance and (iv) it can predict the output of one wind turbine by borrowing strength from the data of all other wind turbines in a farm. Probabilistic guidelines for condition monitoring are given via a cumulative sum (CUSUM) control chart, which is specifically designed based on a real-data classification exercise and, hence, is adapted to the needs of a wind farm. We illustrate the performance of our model in a real SCADA data example which provides evidence that it outperforms other probabilistic prediction models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16496v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filippo Fiocchi, Domna Ladopoulou, Petros Dellaportas</dc:creator>
    </item>
    <item>
      <title>A Public Dataset For the ZKsync Rollup</title>
      <link>https://arxiv.org/abs/2407.18699</link>
      <description>arXiv:2407.18699v2 Announce Type: replace-cross 
Abstract: Despite blockchain data being publicly available, practical challenges and high costs often hinder its effective use by researchers, thus limiting data-driven research and exploration in the blockchain space. This is especially true when it comes to Layer-2 (L2) ecosystems, and ZKsync, in particular. To address these issues, we have curated a dataset from 1 year of activity extracted from a ZKsync Era archive node and made it freely available to external parties. We provide details on this dataset and how it was created, showcase a few example analyses that can be performed with it, and discuss some future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18699v2</guid>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>4th International Workshop on Cryptoasset Analytics (CAAW), co-located with Financial Cryptography and Data Security 2025</arxiv:journal_reference>
      <dc:creator>Maria In\^es Silva, Johnnatan Messias, Benjamin Livshits</dc:creator>
    </item>
    <item>
      <title>The Advancement of Personalized Learning Potentially Accelerated by Generative AI</title>
      <link>https://arxiv.org/abs/2412.00691</link>
      <description>arXiv:2412.00691v2 Announce Type: replace-cross 
Abstract: The rapid development of Generative AI (GAI) has sparked revolutionary changes across various aspects of education. Personalized learning, a focal point and challenge in educational research, has also been influenced by the development of GAI. To explore GAI's extensive impact on personalized learning, this study investigates its potential to enhance various facets of personalized learning through a thorough analysis of existing research. The research comprehensively examines GAI's influence on personalized learning by analyzing its application across different methodologies and contexts, including learning strategies, paths, materials, environments, and specific analyses within the teaching and learning processes. Through this in-depth investigation, we find that GAI demonstrates exceptional capabilities in providing adaptive learning experiences tailored to individual preferences and needs. Utilizing different forms of GAI across various subjects yields superior learning outcomes. The article concludes by summarizing scenarios where GAI is applicable in educational processes and discussing strategies for leveraging GAI to enhance personalized learning, aiming to guide educators and learners in effectively utilizing GAI to achieve superior learning objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00691v2</guid>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuang Wei, Yuan-Hao Jiang, Jiayi Liu, Changyong Qi, Linzhao Jia, Rui Jia</dc:creator>
    </item>
  </channel>
</rss>
