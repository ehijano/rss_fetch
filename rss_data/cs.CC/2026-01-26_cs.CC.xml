<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 26 Jan 2026 05:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 26 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Fine-grained quantum advantage beyond double-logarithmic space</title>
      <link>https://arxiv.org/abs/2601.16695</link>
      <description>arXiv:2601.16695v1 Announce Type: new 
Abstract: Polynomial-time quantum Turing machines are provably superior to their classical counterparts within a common space bound in $o(\log \log n)$. For $\Omega(\log \log n)$ space, the only known quantum advantage result has been the fact $\mathsf{BPTISP}(2^{O(n)},o(\log n))\subsetneq \mathsf{BQTISP}(2^{O(n)},o(\log n))$, proven by exhibiting an exponential-time quantum finite automaton (2QCFA) that recognizes $L_{pal}$, the language of palindromes, which is an impossible task for sublogarithmic-space probabilistic Turing machines. No subexponential-time quantum algorithm can recognize $L_{pal}$ in sublogarithmic space.
  We initiate the study of quantum advantage under simultaneous subexponential time and $\Omega(\log \log n) \cap o(\log n)$ space bounds. We exhibit an infinite family $\mathcal{F}$ of functions in $(\log n)^{\omega(1)}\cap n^{o(1)}$ such that for every $f_i\in\mathcal{F}$, there exists another function $f_{i+1}\in\mathcal{F}$ such that $f_{i+1}(n) \in o(f_{i}(n))$, and each such $f_i$ corresponds to a different quantum advantage statement, i.e. a proper inclusion of the form $\mathsf{BPTISP}(2^{O(f_i(n))},o(\log f_i(n)))\subsetneq \mathsf{BQTISP}(2^{O(f_i(n))},o(\log f_i(n)))$ for a different pair of subexponential time and sublogarithmic space bounds. Our results depend on a technique enabling polynomial-time quantum finite automata to control padding functions with very fine asymptotic granularity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16695v1</guid>
      <category>cs.CC</category>
      <pubDate>Mon, 26 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. C. Cem Say</dc:creator>
    </item>
    <item>
      <title>Modern Hopfield Networks Require Chain-of-Thought to Solve $\mathsf{NC}^1$-Hard Problems</title>
      <link>https://arxiv.org/abs/2412.05562</link>
      <description>arXiv:2412.05562v2 Announce Type: replace 
Abstract: Modern Hopfield Networks (MHNs) have emerged as powerful components in deep learning, serving as effective replacements for pooling layers, LSTMs, and attention mechanisms. While recent advancements have significantly improved their storage capacity and retrieval efficiency, their fundamental theoretical boundaries remain underexplored. In this paper, we rigorously characterize the expressive power of MHNs through the lens of circuit complexity theory. We prove that $\mathrm{poly}(n)$-precision MHNs with constant depth and linear hidden dimension fall within the $\mathsf{DLOGTIME}$-uniform $\mathsf{TC}^0$ complexity class. Consequently, assuming $\mathsf{TC}^0 \neq \mathsf{NC}^1$, we demonstrate that these architectures are incapable of solving $\mathsf{NC}^1$-hard problems, such as undirected graph connectivity and tree isomorphism. We further extend these impossibility results to Kernelized Hopfield Networks. However, we show that these limitations are not absolute: we prove that equipping MHNs with a Chain-of-Thought (CoT) mechanism enables them to transcend the $\mathsf{TC}^0$ barrier, allowing them to solve inherently serial problems like the word problem for the permutation group $S_5$. Collectively, our results delineate a fine-grained boundary between the capabilities of standard MHNs and those augmented with reasoning steps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05562v2</guid>
      <category>cs.CC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Mon, 26 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Cao, Xiaoyu Li, Yuanpeng Li, Yingyu Liang, Zhenmei Shi, Zhao Song</dc:creator>
    </item>
    <item>
      <title>On Fine-Grained I/O Complexity of Attention Backward Passes</title>
      <link>https://arxiv.org/abs/2410.09397</link>
      <description>arXiv:2410.09397v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) exhibit exceptional proficiency in handling extensive context windows in natural language. Nevertheless, the quadratic scaling of attention computation relative to sequence length creates substantial efficiency bottlenecks, necessitating the development of I/O-optimized algorithms. In this work, we conduct a systematic examination of the I/O complexity inherent in attention mechanisms, with a specific emphasis on the backward pass under both small and large cache settings. By leveraging the red-blue pebble game framework, we derive tight bounds for I/O complexity across the full spectrum of cache sizes. We validate that FlashAttention, one of the current industry standards, achieves optimality in the large-cache scenario for both forward and backward passes. Conversely, for small-cache environments, we introduce a novel algorithm that outperforms contemporary methods and successfully attains theoretical tight bounds. Furthermore, we expand our investigation to include sparse attention by establishing granular lower bounds for both forward and backward passes across all cache configurations. Ultimately, our results solidify the theoretical framework regarding I/O complexity in attention mechanisms, providing critical guidance for the development of efficient LLM training and inference systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09397v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>cs.CL</category>
      <pubDate>Mon, 26 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Song Yue, Jiahao Zhang</dc:creator>
    </item>
    <item>
      <title>On Computational Limits of FlowAR Models: Expressivity and Efficiency</title>
      <link>https://arxiv.org/abs/2502.16490</link>
      <description>arXiv:2502.16490v2 Announce Type: replace-cross 
Abstract: The expressive power and computational complexity of deep visual generative models, such as flow-based and autoregressive (AR) models, have gained considerable interest for their wide-ranging applications in generative tasks. However, the theoretical characterization of their expressiveness through the lens of circuit complexity remains underexplored, particularly for the state-of-the-art architecture like FlowAR proposed by [Ren et al., 2024], which integrates flow-based and autoregressive mechanisms. This gap limits our understanding of their inherent computational limits and practical efficiency. In this study, we address this gap by analyzing the circuit complexity of the FlowAR architecture. We demonstrate that when the largest feature map produced by the FlowAR model has dimensions $n \times n \times c$, the FlowAR model is simulable by a family of threshold circuits $\mathsf{TC}^0$, which have constant depth $O(1)$ and polynomial width $\mathrm{poly}(n)$. This is the first study to rigorously highlight the limitations in the expressive power of FlowAR models. Furthermore, we identify the conditions under which the FlowAR model computations can achieve almost quadratic time. To validate our theoretical findings, we present efficient model variant constructions based on low-rank approximations that align with the derived criteria. Our work provides a foundation for future comparisons with other generative paradigms and guides the development of more efficient and expressive implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16490v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>cs.CV</category>
      <pubDate>Mon, 26 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Cao, Chengyue Gong, Yekun Ke, Xiaoyu Li, Yingyu Liang, Zhizhou Sha, Zhenmei Shi, Zhao Song</dc:creator>
    </item>
  </channel>
</rss>
