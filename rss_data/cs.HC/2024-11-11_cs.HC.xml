<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 Nov 2024 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Proposal of a Contact Detection System using Micro-phones for a Chambara-based Augmented Sports</title>
      <link>https://arxiv.org/abs/2411.05098</link>
      <description>arXiv:2411.05098v1 Announce Type: new 
Abstract: This study presents a novel contact detection system for "Parablade," a chambara-based, sword-play augmented sport. Augmented sports combine physical activities with virtual parameters (VPs) to create a balanced and equitable gaming experience, irrespective of players' physical capabilities. The proposed Parablade Microphone Unit (PMU) employs multiple micro-phones and machine learning algorithms to detect and classify hit events through sound recogni-tion. This system aims to ensure real-time updates of VPs, thereby enhancing the gameplay expe-rience. Experimental results indicate that the PMU can accurately recognize the occurrence and location of hit events with a high accuracy rate of 93.33%, with the assistance of 10kHz additional sound generated from the sword.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05098v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yusaku Maeda, Sho Sakurai, Koichi Hirota, Takuya Nojima</dc:creator>
    </item>
    <item>
      <title>Exploring Vibrotactile Intensity Perception with Multiple Waveform Parameters</title>
      <link>https://arxiv.org/abs/2411.05099</link>
      <description>arXiv:2411.05099v1 Announce Type: new 
Abstract: It is known that by longing the duration of a vibrotactile stimuli or applying a damping or an increasing factor to the waveform the perceived intensity is affected in different ways. This paper presents a vibrotactile presentation system assembled with a software waveform generator that enables comparison in the perceived intensity for different waveforms made by multiple parameters. The adjustable parameters are frequency, amplitude, and wave type for the basic part of the stimuli and in addition, it is possible to apply an exponential decay or increasing factor to the waveform by specificizing the duration. By using the presented system, an easy comparison of the influence to the perception of intensity by different parameters of the waveform is possible. We conducted a preliminary experiment on a variety of waveshapes with and without damping by using this system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05099v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takumi Kuhara, Hikari Yukawa, Yoshihiro Tanaka</dc:creator>
    </item>
    <item>
      <title>Fingernail-Based Tangential Force Simulation for Enhanced Dexterous Manipulation in Virtual Reality</title>
      <link>https://arxiv.org/abs/2411.05100</link>
      <description>arXiv:2411.05100v1 Announce Type: new 
Abstract: This study introduces a novel haptic device for enhancing dexterous manipulation in virtual reality. By stimulating mechanoreceptors on both sides of the fingernail, our lightweight system simulates tangential force sensations. We employ mechanical stimulation for more natural tactile feedback. A preliminary "balancing grasp challenge" experiment shows that users make more frequent micro-adjustments with our device, indicating improved precision. This research aims to advance haptic feedback in VR, potentially leading to more immersive and realistic virtual interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05100v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunxiu Xu, Shoichi Hasegawa</dc:creator>
    </item>
    <item>
      <title>MoHeat: A Modular Platform for High-Responsive Non-Contact Thermal Feedback Interactions</title>
      <link>https://arxiv.org/abs/2411.05103</link>
      <description>arXiv:2411.05103v1 Announce Type: new 
Abstract: MoHeat is a modular hardware and software platform designed for rapid prototyping of highly responsive, non-contact thermal feedback interactions. In our previous work, we developed an intensity-adjustable, highly responsive, non-contact thermal feedback system by integrating the vortex effect and thermal radiation. In this study, we further enhanced the system by developing an authoring tool that allows users to freely adjust the intensity of thermal stimuli, the duration of stimuli, the delay time before stimuli, and the interval between alternating hot and cold stimuli. This modular approach enables countless combinations of non-contact thermal feedback experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05103v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiayi Xu, Kazuma Nakamura, Yoshihiro Kuroda, Masahiko Inami</dc:creator>
    </item>
    <item>
      <title>Skill Transfer System that Visualizes and Presents Tactile Information in an AR Environment</title>
      <link>https://arxiv.org/abs/2411.05104</link>
      <description>arXiv:2411.05104v1 Announce Type: new 
Abstract: In recent years, the lack of successors for traditional skills has become an issue. To solve this problem, we propose a skill transfer system that presents tactile information in spatial tasks as a color map on an AR space. We believe that providing the operator with feedback of the force and tactile information during the work is useful for learning skills that require time to master. Furthermore, by following the operator's hand and presenting tactile information, we expect to accelerate the learning of skills by not only presenting tactile information as a physical sensation, but also by making the operator associate tactile information with position.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05104v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takaya Nida, Masamune Waga, Yuta Hamada, Masashi Konyo, Yushi Nakaya, Shubhamkumar Pandey, Satoshi Tadokoro</dc:creator>
    </item>
    <item>
      <title>Presenting the Sense of Effort through Vibration Based on Force Estimated by Inverse Dynamics in Videos</title>
      <link>https://arxiv.org/abs/2411.05105</link>
      <description>arXiv:2411.05105v1 Announce Type: new 
Abstract: We present the sense of effort through vibration to help the video viewer understand how the person in the video moves the body. We suppose sense of effort is related to force, so we generate vibration based on force and present the sense of effort through the vibration. We use perceived intensity to make sense of effort proportional to vibration. In our demonstration, you can experience vibration while watching a video. We can create vibration on the spot, so you can experience vibration made from a video taken on the spot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05105v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryoma Akai, Masashi Konyo, Satoshi Tadokoro</dc:creator>
    </item>
    <item>
      <title>Enhancing Medical Anatomy Education through Virtual Reality (VR): Design, Development, and Evaluation</title>
      <link>https://arxiv.org/abs/2411.05106</link>
      <description>arXiv:2411.05106v1 Announce Type: new 
Abstract: Modern medicine demands innovations in medical education, particularly in the learning of human anatomy, traditionally taught through textbooks, dissections, and lectures. Virtual Reality (VR) has emerged as a promising tool to address the limitations of these conventional methods by emphasising vision-based and active learning. However, current VR educational tools are often inaccessible due to high costs and specialised equipment requirements. This paper details the design and development of an accessible, desktop-based VR system aimed at enhancing anatomy education by leveraging the user's visual perception to promote a meaningful and interactive learning experience. The Virtual Anatomy Lab was designed to enable students to interact with a 3D Skull model to complete tasks virtually via an interactive user interface (UI) with the help of common devices like a mouse and keyboard. As part of the study, a group of medical students from prestigious medical schools throughout Malaysia were invited to evaluate the built system to offer feedback and determine its overall efficiency and usability in fulfilling their learning goals. The results and findings from user evaluations were then analysed to discuss its effectiveness and areas for future improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05106v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Myint Zu Than, Kian Meng Yap</dc:creator>
    </item>
    <item>
      <title>Simultaneous Presentation of Thermal and Mechanical Stimulation Using High-Intensity Airborne Ultrasound</title>
      <link>https://arxiv.org/abs/2411.05108</link>
      <description>arXiv:2411.05108v1 Announce Type: new 
Abstract: In this study, we propose a non-contact thermal presentation method using airborne ultrasound. We generate strong sound field directly on the human skin and present a perceivable temperature rise. The proposed method enables simultaneous presentation of mechanical and thermal stimuli. In preliminary experiments, we confirmed that temperature increase of 5.4 ${}^\circ$C occurs at the palm after 5.0 s.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05108v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sota Iwabuchi, Ryoya Onishi, Shun Suzuki, Takaaki Kamigaki, Yasutoshi Makino, Hiroyuki Shinoda</dc:creator>
    </item>
    <item>
      <title>Haptic Information Feedback Given to Handles in Guide Dog Training</title>
      <link>https://arxiv.org/abs/2411.05109</link>
      <description>arXiv:2411.05109v1 Announce Type: new 
Abstract: In guide dog training, trainers use haptic information transmitted through the handle of the harness worn by the guide dog to understand the dog's state. They then apply appropriate force to the handle to train the dog to make correct judgments. This tactile experience can only be felt between the dog and the trainer, making it challenging to communicate the amount of force applied to others quantitatively. To solve this problem, this study proposes a method for real-time visualization of the force exerted on the handle and quantification of the handle movement through image processing, which can be applied to actual guide dog training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05109v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chonghoon Park, Qirong Zhu, Shinji Tanaka, Yasutoshi Makino, Hiroyuki Shinoda</dc:creator>
    </item>
    <item>
      <title>Haptic Reproduction of Curved Surface and Edge by Controlling the Contact Position between the Disk and the Finger Using Airborne Ultrasound</title>
      <link>https://arxiv.org/abs/2411.05110</link>
      <description>arXiv:2411.05110v1 Announce Type: new 
Abstract: By presenting curved surfaces of various curvatures including edges to the fingertip, it is possible to reproduce the haptic sensation of object shapes that cannot be reproduced by flat surfaces alone, such as spheres and rectangular objects. In this paper, we propose a method of presenting curved surfaces by controlling the inclination of a disk in contact with the finger belly with acoustic radiation pressure of ultrasound. The user only needs to mount a lightweight device on the fingertip to experience a tactile presentation with low physical burden. In the demonstration, the user can experience the sensation of stroking an edge and different curvatures of curved surfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05110v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aoba Sekiguchi, Tao Morisaki, Takaaki Kamigaki, Yasutoshi Makino, Hiroyuki Shinoda</dc:creator>
    </item>
    <item>
      <title>Location-Based Output Adaptation for Enhanced Actuator Performance using Frequency Sweep Analysis</title>
      <link>https://arxiv.org/abs/2411.05111</link>
      <description>arXiv:2411.05111v1 Announce Type: new 
Abstract: This paper presents a methodology for enhancing actuator performance in older devices or retrofitting devices with haptic feedback actuators. The approach is versatile, accommodating various actuator and mounting positions. Through a frequency sweep analysis, the system's characteristics are captured, enabling the creation of location-specific transfer functions to accurately transform input signals into command signals for a precise output at the target location. This method offers fast and simple collection of the system properties and generation of location-specific signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05111v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Fischler, Seungjae Oh, Seokhee Jeon</dc:creator>
    </item>
    <item>
      <title>Co-Located Magnetic Levitation Haptic and Graphic Display using Iron Core Coils under Screen</title>
      <link>https://arxiv.org/abs/2411.05113</link>
      <description>arXiv:2411.05113v1 Announce Type: new 
Abstract: This paper describes a combined haptic and graphical interactive system in which a grasped handle is levitated and controlled so that its dynamic rigid-body motion and the forces and torques generated upon it match those of a tool in a real-time simulated environment, displayed on a thin screen on top of the levitation coils and underneath the levitated handle. In this augmented reality configuration, the haptic sensations delivered to the hand of the user and the displayed simulation graphics are perceived in the same location, and the graphical display of the tool acts as a virtual extension of the grasped handle into the displayed simulated environment. The novelty of the system is that it combines iron core levitation coils with a low-cost position sensing system and co-located display in a portable system. The high closed-loop control bandwidth and precise position sensing of the system enable interactive simulated environments to be presented with a convincing degree of realism. The interactive environments to be demonstrated will include 3D rigid-body dynamics, surface contacts with stiffness and damping, and surface texture and friction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05113v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Berkelman, Steven H W Kang, Sean Trafford, Muneaki Miyasaka</dc:creator>
    </item>
    <item>
      <title>STEM: Soft Tactile Electromagnetic Actuator for Virtual Environment Interactions</title>
      <link>https://arxiv.org/abs/2411.05114</link>
      <description>arXiv:2411.05114v1 Announce Type: new 
Abstract: The research aims to expand tactile feedback beyond vibrations to various modes of stimuli, such as indentation, vibration, among others. By incorporating soft material into the design of a novel tactile actuator, we can achieve multi-modality and enhance the device's wearability, which encompasses compliance, safety, and portability. The proposed tactile device can elevate the presence and immersion in VR by enabling diverse haptic feedback such as, force indentation, vibration and other arbitrary force outputs. This approach enables the rendering of haptic interactions with virtual objects, such as grasping of aa 3D virtual object to feel its stiffness - action that was difficult to achieve using widely adopted vibrotactile motors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05114v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heeju Mun, Seunggyeom Jung, Seung Mo Jeong, David Santiago Diaz Cortes, Ki-Uk Kyung</dc:creator>
    </item>
    <item>
      <title>Bridging Player Intentions: Exploring the Potential of Synchronized Haptic Controllers in Multiplayer Game</title>
      <link>https://arxiv.org/abs/2411.05115</link>
      <description>arXiv:2411.05115v1 Announce Type: new 
Abstract: In multiplayer cooperative video games, players traditionally use individual controllers, inferring others' actions through on-screen visuals and their own movements. This indirect understanding limits truly collaborative gameplay. Research in Joint Action shows that when manipulating a single object, motor performance improves when two people operate together while sensing each other's movements. Building on this, we developed a controller allowing multiple players to operate simultaneously while sharing haptic sensations. We showcased our system at exhibitions, gathering feedback from over 150 participants on how shared sensory input affects their gaming experience. This approach could transform player interaction, enhance cooperation, and redefine multiplayer gaming experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05115v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenta Hashiura, Kazuya Iida, Takeru Hashimoto, Youichi Kamiyama, Keita Watanabe, Kouta Minamizawa, Takuji Narumi</dc:creator>
    </item>
    <item>
      <title>Haptic Color Patterns for Visually Impaired People-Pilot Study for a Learning Color Wheel</title>
      <link>https://arxiv.org/abs/2411.05116</link>
      <description>arXiv:2411.05116v1 Announce Type: new 
Abstract: This study proposes a tactile diagram pattern for visually impaired people to recognize color information. The pattern uses the principle of three primary colors, with different patterns representing red, blue, and yellow. The size of tactile elements on these patterns indicates the proportion of the color mixing. A preliminary experiment showed that even a sighted participant could understand and reconstruct the tactile diagram. Future experiments will target visually impaired people to confirm the effectiveness of this method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05116v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hsin-Yi Chao, Hiroki Ishizuka</dc:creator>
    </item>
    <item>
      <title>Development of fall prevention training device that can provide external disturbance to the ankle with pneumatic gel muscles (PGM) while walking</title>
      <link>https://arxiv.org/abs/2411.05117</link>
      <description>arXiv:2411.05117v1 Announce Type: new 
Abstract: Although the average life expectancy in Japan has been increasing in recent years, the problem of the large gap between healthy life expectancy and average life expectancy is still unresolved. Among the factors that lead to the need for nursing care, injuries due to falls account for a certain percentage of the total. In this paper, we developed boots that can provide external disturbance to the ankle with pneumatic gel muscles (PGM) while walking. We experimented using an angular velocity and acceleration of the heel as evaluation indices to evaluate the effectiveness of fall prevention training using this device, which is smaller and more wearable than conventional devices. In this study, we confirmed that the developed system has enough training intensity to significantly affect the gait waveform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05117v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keigo Isoshima, Mitsunori Tada, Noriaki Maeda, Tsubasa Tashiro, Satoshi Arima, Takumi Nagao, Yuki Tamura, Yuichi Kurita</dc:creator>
    </item>
    <item>
      <title>An emotional expression system with vibrotactile feedback during the robot's speech</title>
      <link>https://arxiv.org/abs/2411.05118</link>
      <description>arXiv:2411.05118v1 Announce Type: new 
Abstract: This study aimed to develop a system that provides vibrotactile feedback corresponding to the emotional content of text when a communication robot speaks. We used OpenAI's "GPT-4o Mini" for emotion estimation, extracting valence and arousal values from the text. The amplitude and frequency of vibrotactile stimulation using sine waves were controlled on the basis of estimated emotional values. We assembled a palm-sized tactile display to present these vibrotactile stimuli. In the experiment, participants listened to the robot's speech while holding the device and then evaluated their psychological state. The results suggested that the communication accompanied by the vibrotactile feedback could influence psychological states and intimacy levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05118v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuki Konishi, Yoshihiro Tanaka</dc:creator>
    </item>
    <item>
      <title>Development and evaluation of a system to express a sense of telekinesis in VR</title>
      <link>https://arxiv.org/abs/2411.05121</link>
      <description>arXiv:2411.05121v1 Announce Type: new 
Abstract: Telekinesis is the ability to manipulate remote objects without direct physical contact. In fictional works, telekinesis users are often depicted as controlling objects with their hands and other body parts as if by will alone. Such depictions suggest that users experience a sense of agency over the object despite not physically touching it. In this study, we developed a VR method to simulate telekinesis and investigated whether it is possible to achieve a sense of physical sensation and agency similar to the experience portrayed in fiction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05121v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shingo Nakaya, Yudai Hirota, Sho Sakurai, Takuya Nojima, Koichi Hirota</dc:creator>
    </item>
    <item>
      <title>Friction tunable electrostatic clutch with low driving voltage for kinesthetic haptic feedback</title>
      <link>https://arxiv.org/abs/2411.05123</link>
      <description>arXiv:2411.05123v1 Announce Type: new 
Abstract: As interest in Virtual Reality (VR) and Augmented Reality (AR) increases, the demand for kinesthetic haptic feedback devices is rapidly rising. Motor based haptic interfaces are heavy and bulky, leading to discomfort for the user. To address this issue, haptic gloves based on electrostatic clutches that offer fast response times and a thin form factor are being researched. However, high operating voltages and variable force control remain challenges to overcome. Electrostatic clutches utilizing functional polymers with charge accumulation properties and dielectric liquid can generate the frictional shear stress over a wide range from 0.35 N/cm$^2$ to 18.9 N/cm$^2$ at low voltages below 100 V. Based on this, the haptic glove generates a high blocking force and is comfortable to wear.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05123v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jongseok Nam, Jihyeong Ma, Nak Hyeong Lee, Ki-Uk Kyung</dc:creator>
    </item>
    <item>
      <title>Mouse Embedded Soft Vibration Actuator for Exploring Surface Textures</title>
      <link>https://arxiv.org/abs/2411.05125</link>
      <description>arXiv:2411.05125v1 Announce Type: new 
Abstract: We aimed to develop a tactile display that allows users to actively explore the virtual texture of a surface. We developed a tactile display embedded in an optical mouse that provides a wide range of frequency vibrations to the user's fingertip in response to its movement. We conducted an experiment to confirm the degree of fineness based on a stripe pattern identification task using a trial system. The results showed that the subject could identify a fine spatial resolution up to 0.16 mm width.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05125v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asahi Kurokawa, Masaharu Shimizu, Mitsuhito Ando, Haruo Noma</dc:creator>
    </item>
    <item>
      <title>A Comparison of Violin Bowing Pressure and Position among Expert Players and Beginners</title>
      <link>https://arxiv.org/abs/2411.05126</link>
      <description>arXiv:2411.05126v1 Announce Type: new 
Abstract: The violin is one of the most popular musical instruments, but mastering it requires a significant amount of practice time. The bowing action (pressure, position, speed) of the right hand is crucial in determining tonal quality, but this is difficult to master. This study compared the bowing movements, specifically bow pressure, bow position, and bow speed, of experienced players with those of beginners. Identifying common bowing characteristics of experienced violin players can aid the evaluation of beginners' skills and the development of a feedback system that supports self-practice and instruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05126v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yurina Mizuho, Yuta Sugiura</dc:creator>
    </item>
    <item>
      <title>Emotional VR handshake by controlling skin deformation distribution</title>
      <link>https://arxiv.org/abs/2411.05127</link>
      <description>arXiv:2411.05127v1 Announce Type: new 
Abstract: Digital communication tools are limited to visual and auditory information and lack non-verbal information such as touch, which is important for communicating intentions and emotions. In order to solve this problem, the use of haptic technology in digital communication is attracting attention. In this study, we constructed a virtual handshake system that can reproduce distributed haptic information using a wearable device that presents skin deformation. Using the system, we experimentally obtained the correspondence between emotions and handshaking behavior, and constructed a demonstration of handshakes that can express differences in emotions based on the experimental results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05127v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shun Watatani, Hikaru Nagano, Yuichi Tazaki, Yasuyoshi Yokokohji</dc:creator>
    </item>
    <item>
      <title>Edge shape sensation presented in a noncontact manner using airborne ultrasound</title>
      <link>https://arxiv.org/abs/2411.05128</link>
      <description>arXiv:2411.05128v1 Announce Type: new 
Abstract: To perceive 3D shapes such as pyramids, the perception of planes and edges as tactile sensations is an essential component. This is difficult to perceive with the conventional vibrotactile sensation used in ultrasound haptics because of its low spatial resolution. Recently, it has become possible to produce a high-resolution pressure sensation using airborne ultrasound. By using this pressure sensation, it is now possible to reproduce a linear, sharp-edged sensation in the area of a fingerpad. In this study, it is demonstrated that this pressure sensation can be used to reproduce the feeling of fine, sharp edges, and its effectiveness is confirmed by comparing it with conventional vibrotactile sensation. In the demonstration, participants can experience the contact sensation of several types of edges with different curvatures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05128v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Koichi Kato, Tao Morisaki, Shun Suzuki, Yasutoshi Makino, Hiroyuki Shinoda</dc:creator>
    </item>
    <item>
      <title>Silicone-made Tactile Actuator Integrated with Hot Thermo-fiber Finger Sleeve</title>
      <link>https://arxiv.org/abs/2411.05129</link>
      <description>arXiv:2411.05129v1 Announce Type: new 
Abstract: Multi-mode haptic feedback is essential to achieve high realism and immersion in virtual environments. This paper proposed a novel silicone fingertip actuator integrated with a hot thermal fabric finger sleeve to render pressure, vibration, and hot thermal feedback simultaneously. The actuator is pneumatically actuated to render a realistic and effective tactile experience in accordance with hot thermal sensation. The silicone actuator, with two air chambers controlled by pneumatic valves connected to compressed air tanks. Simultaneously, a PWM signal from a microcontroller regulates the temperature of the thermal fabric sleeve, enhancing overall system functionality. The lower chamber of the silicone actuator is responsible for pressure feedback, whereas the upper chamber is devoted to vibrotactile feedback. The conductive yarn or thread was utilized to spread the thermal feedback actuation points on the thermal fabric's surface. To demonstrate the actuator's capability, a VR environment consisting of a bowl of liquid and a stove with fire was designed. Based on different functionalities the scenario can simulate the tactile perception of pressure, vibration, and temperature simultaneously or consecutively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05129v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Shadman Hashem, Ahsan Raza, Seokhee Jeon</dc:creator>
    </item>
    <item>
      <title>Independent perceptual process of microscopic texture and surface shapes through lateral resistive force cues</title>
      <link>https://arxiv.org/abs/2411.05130</link>
      <description>arXiv:2411.05130v1 Announce Type: new 
Abstract: Macroscopic surface shapes, such as bumps and dents, as well as microscopic surface features, like texture, can be identified solely through lateral resistive force cues when a stylus moves across them. This perceptual phenomenon has been utilized to advance tactile presentation techniques for surface tactile displays. However, the effects on shape recognition when microscopic textures and macroscopic shapes coexist have not been thoroughly investigated. This study reveals that macroscopic surface shapes can be recognized independently of the presence of microscopic textures. These findings enhance our understanding of human perceptual properties and contribute to the development of tactile displays.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05130v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mirai Azechi, Shogo Okamoto</dc:creator>
    </item>
    <item>
      <title>Innovative Weight Simulation in Virtual Reality Cube Games: A Pseudo-Haptic Approach</title>
      <link>https://arxiv.org/abs/2411.05133</link>
      <description>arXiv:2411.05133v1 Announce Type: new 
Abstract: This paper presents an innovative pseudo-haptic model for weight simulation in virtual reality (VR) environments. By integrating visual feedback with voluntary exerted force through a passive haptic glove, the model creates haptic illusions of weight perception. Two VR cube games were developed to evaluate the model's effectiveness. The first game assesses participants' ability to discriminate relative weights, while the second evaluates their capability to estimate absolute weights. Twelve participants, aged 18 to 59, tested the games. Results suggest that the pseudo-haptic model is effective for relative weight discrimination tasks and holds potential for various VR applications. Further research with a larger participant group and more complex scenarios is recommended to refine and validate the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05133v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Woan Ning Lim, Edric Yi Junn Leong, Yun Li Lee, Kian Meng Yap</dc:creator>
    </item>
    <item>
      <title>A Vibrotactile Belt for Interpersonal Synchronization of Breath</title>
      <link>https://arxiv.org/abs/2411.05135</link>
      <description>arXiv:2411.05135v1 Announce Type: new 
Abstract: This paper introduces a vibrotactile belt for interpersonal synchronization of breath. It can synchronize the breathing tempo of two people by transferring breathing rhythm of one user to vibration signals of another belt, where the depth of breathing is represented by the intensity of vibration. This provides a novel way of emotional connect between people. Meanwhile, this breath-sharing device may also be combined with smart devices in the future to form a one-to-many, many-to-many internet of breath, which has promising application prospects in healthcare, sports breathing guidance and other scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05135v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xilai Tan, Yan Zhang, Bin Zhao, Xiaolu Nan, Yuru Zhang, Dangxiao Wang</dc:creator>
    </item>
    <item>
      <title>Magical Experience with Full-body Action</title>
      <link>https://arxiv.org/abs/2411.05139</link>
      <description>arXiv:2411.05139v1 Announce Type: new 
Abstract: This paper presents a system that generates a magical experience with full-body motion. The system consists of a locomotion interface and a spatial immersive display. A virtual experience system named the Magical Experience Generator was developed, equipped with a Magical Experience Controller. This system provides a physical movement experience along with magical-like interactions in a virtual space. We developed content inspired by the Japanese story "The Man Who Made Flowers Bloom" using Unity as the system's environment. The locomotion interface records the participant's walking trajectory and hand movements, representing their actions in the virtual space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05139v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bowen Ye, Yuki Enzaki, Hiroo Iwata</dc:creator>
    </item>
    <item>
      <title>Hold and Feel! A Multiplayer Video Game System with Interpersonal Vibrotactile Feedback via Bracelet Controllers</title>
      <link>https://arxiv.org/abs/2411.05140</link>
      <description>arXiv:2411.05140v1 Announce Type: new 
Abstract: Recent multiplayer video games (MPVGs) have increasingly incorporated social behaviors, such as gathering in the same place and facing each other, to enhance player interaction. This study aims to develop an MPVG system that facilitates social interaction through interpersonal touch between players. We present an MPVG system where two players wearing bracelet-type game controllers control a player character through touch to catch items and feel the position of the player character through vibrotactile sensations between their hands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05140v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenta Ebina, Taku Hachisu</dc:creator>
    </item>
    <item>
      <title>Relaxed or Tense? Mutual Biosignal Transmission with Heartbeat Vibrations during Online Gameplay</title>
      <link>https://arxiv.org/abs/2411.05142</link>
      <description>arXiv:2411.05142v1 Announce Type: new 
Abstract: Esports offers a platform for players to engage in competitive and cooperative gaming with others remotely via the Internet. Despite these opportunities for social interaction, many players may still experience loneliness while playing online games. This study aims to enhance the social presence of partner players during online gameplay. The demonstration system, designed for 1-on-1 online competitive games, mutually transmits the partner's biosignals, through heartbeat-like vibrotactile stimuli. The system generates vibrotactile signals that represent two-dimensional emotions, arousal and valence, based on biosignals such as heart rate and electrodermal activity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05142v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yoshiki Mori, Sohta Takada, Masayuki Kajiura, Modar Suleiman Hassan, Taku Hachisu</dc:creator>
    </item>
    <item>
      <title>Pneumatically Controlled Tactile Actuating Modules for Enhanced VR Safety Training</title>
      <link>https://arxiv.org/abs/2411.05143</link>
      <description>arXiv:2411.05143v1 Announce Type: new 
Abstract: Our system introduces a modularized pneumatic actuating unit capable of delivering vibration, pressure, and impact feedback. Designed for adaptability, these modular tactile actuating units can be rapidly customized and reconfigured to suit a wide range of virtual reality (VR) scenarios, with a particular emphasis on safety training applications. This flexibility is demonstrated through scenarios such as using construction tools in a virtual environment and simulating safety protocols against falling objects. Innovative mounting solutions securely attach the actuators to various body sites, ensuring both comfort and stability during use. Our approach enables seamless integration into diverse VR safety training programs, enhancing the realism and effectiveness of simulations with precise and reliable haptic feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05143v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahsan Raza, Seokhee Jeon, Mohammad Shadman Hashem</dc:creator>
    </item>
    <item>
      <title>Effects of Coordinative Arm Swing Movements on the Sense of Agency in Walking Sensation Induced by Kinesthetic Illusion</title>
      <link>https://arxiv.org/abs/2411.05144</link>
      <description>arXiv:2411.05144v1 Announce Type: new 
Abstract: Kinesthetic illusion can present a sense of movement without actual physical movement of the body, but it often lacks a sense of agency over the movement. Therefore, we focused on the sensation of walking induced by the kinesthetic illusion and hypothesized that incorporating coordinated arm swing movements as actual actions could enhance the sense of agency over the kinesthetic illusion. In this study, we implemented a system that switches the vibrations of the thighs and ankles back and forth based on arm swing movements and investigated whether the sense of agency over the walking sensation induced by the kinesthetic illusion changes with or without arm swing movements. The results suggest a tendency for the sense of agency to be enhanced when arm swing movements are combined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05144v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eifu Narita, Keigo Ushiyama, Izumi Mizoguchi, Hiroyuki Kajimoto</dc:creator>
    </item>
    <item>
      <title>v-Relax: Virtual Footbath Experiencing by Airflow and Thermal Presentation</title>
      <link>https://arxiv.org/abs/2411.05145</link>
      <description>arXiv:2411.05145v1 Announce Type: new 
Abstract: Relaxation is a critical counterbalance to the demands of modern business life. Footbaths, a simple yet highly effective therapeutic practice, have been used for centuries across various cultures to promote relaxation and overall well-being. This study presents a novel approach to simulating the experience of a public footbath through the use of tactile and thermal stimulation of airflow to the calf and those on the foot soles. Our system aims to offer a realistic and immersive virtual footbath experience without the need for actual water, by controlling the temperature and airflow to mimic the sensation of soaking feet in water or a water wave. Without using actual water, our system can be more compact, highly responsive, and more reproducible. The layer of airflow is made as thin as possible by adjusting air outlet, and the Coanda effect is also considered to generate a water surface more realistic. The system can provide a multi-sensory experience, including visual and audio feedback of water flow, enhancing the relaxation and therapeutic benefits of a footbath.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05145v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vibol Yem, Mattia Quartana, Zi Xin, Kazuhiro Fujitsuka, Tomohiro Amemiya</dc:creator>
    </item>
    <item>
      <title>Break Times: Virtual Reality Art Therapy</title>
      <link>https://arxiv.org/abs/2411.05146</link>
      <description>arXiv:2411.05146v1 Announce Type: new 
Abstract: This paper presents a Virtual Reality (VR) art therapy known as "Break Times" which aims to enhance students' mental well-being and foster creative expression. The proposed "Break Times" application mimics the art therapy sessions in the VR environment design. Pilot user acceptance test with 10 participants showed a notable reduction in stress levels, with 50% reporting normal stress levels post-intervention, compared to 20% pre-intervention. Participants praised the "Break Times" therapy's functionality and engagement features and suggested improvements such as saving creations, incorporating 3D painting, and expanding the artmaking scene variety. The study highlights that VR art therapy has potential as an effective tool for stress management, emphasizing the need for continued refinement to maximize its therapeutic benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05146v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Rou Yap, Yun Li Lee</dc:creator>
    </item>
    <item>
      <title>A Soft Vibrotactile Display Using Sound Speakers</title>
      <link>https://arxiv.org/abs/2411.05147</link>
      <description>arXiv:2411.05147v1 Announce Type: new 
Abstract: This study introduces an innovative vibrotactile display that harnesses audio speakers to convey tactile information to the fingertips while preserving the display's softness and flexibility. Our proposed system integrates a flexible polymer body with silicone rubber tubes connected to audio speakers. By streaming audio through these speakers, we induce air vibrations within the tubes, generating tactile stimuli on the skin. In contrast to conventional tactile displays that often rely on bulky, rigid actuators, our approach employs multiple speakers to deliver high-resolution vibration patterns. This configuration enables the presentation of high-frequency vibrations, potentially enhancing the fidelity of tactile feedback. We present a detailed description of the display's design principles and implementation methodology, highlighting its potential to advance the field of haptic interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05147v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keitaro Ihara, Atsuya Baba, Hiroki Ishizuka, Takefumi Hiraki, Osamu Oshiro</dc:creator>
    </item>
    <item>
      <title>Haptic VR Simulation for Surgery Procedures in Medical Training</title>
      <link>https://arxiv.org/abs/2411.05148</link>
      <description>arXiv:2411.05148v1 Announce Type: new 
Abstract: Traditional medical training faces challenges like ethical concerns, safety risks, and high costs. VR technology offers a promising solution but is limited by low complexity and lack of tactile feedback. This paper presents a cost-effective haptic VR surgery simulation which simulates realistic Kidney Transplant using commercial devices to enhance training authenticity and immersion. Trainees can conduct incision and anastomosis procedures using a haptic stylus device that provides tactile sensations. Results from the test with medical participants showed that haptic feedback positively enhances the VR medical training experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05148v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lim Zheng Jie, Kian Meng Yap</dc:creator>
    </item>
    <item>
      <title>Electrostatic Tactile Display without Insulating Layer</title>
      <link>https://arxiv.org/abs/2411.05149</link>
      <description>arXiv:2411.05149v1 Announce Type: new 
Abstract: This paper explores an approach to eliminating the surface insulating layer in electrostatic (electroadhesion) tactile displays. Electrostatic tactile displays modulate the surface friction by an electrical charge between the skin and the display. Traditionally, the non-conductive dielectric layer has been considered crucial for charge accumulation, as well as for safety to prevent DC current stimulation. However, by utilizing a current control technology for electrotactile displays, we can achieve electrostatic tactile display without the insulating layer. The electrical charge is possibly accumulated in the skin itself or in the air gap between the skin and the electrodes. Safety is maintained by balancing positive and negative current pulses. Furthermore, this system is compatible with existing electrotactile displays. This paper details the system configuration, presentation algorithm, and experimental results. The preliminary trial revealed that five out of eight participants could clearly feel the vibration, confirmed by acceleration recording, while the remaining participants could not experience the sensation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05149v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroyuki Kajimoto</dc:creator>
    </item>
    <item>
      <title>Scanning Tactile Sensor with Spiral Coil Structure Amplifying Detection Performance of Micro-concave</title>
      <link>https://arxiv.org/abs/2411.05150</link>
      <description>arXiv:2411.05150v1 Announce Type: new 
Abstract: Surface inspection is a delicate process aimed at detecting fine defects, irregularities, and foreign substances at the tens of micrometers level, subsequently excluding products that do not meet the quality standards as defective. Currently, this inspection relies on the tactile senses of skilled technicians, leading to variability in the detection accuracy based on the level of proficiency and experience. Consequently, a standardized method for surface inspection has yet to be established. In response to this issue, we have developed a device capable of amplifying tactile information, allowing for the detection of minute distortions without the need for highly skilled technicians. The experimental results on various small distortions suggest the potential for the quantitative evaluation of these distortions. In the future, the application of this device could contribute to the automation of surface inspection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05150v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takeru Kurokawa, Toshinobu Takei, Haruo Noma, Mitsuhito Ando</dc:creator>
    </item>
    <item>
      <title>Presentation of tracing sensation with different roughness by using disk with uneven surface</title>
      <link>https://arxiv.org/abs/2411.05151</link>
      <description>arXiv:2411.05151v1 Announce Type: new 
Abstract: Tracing sensation plays an important role in discriminating friction and texture of objects. We have been studying a method of presenting the tracing sensation by contacting the center of a rotating disk with the fingertip. Although this method can reduce the size and complexity of the presentation device compared to conventional methods, it has the problem of fixed sensation. In this study, we examined a method to dynamically modulate the roughness sensation by devising the surface shape of the disk. We designed two types of disks: slit disks and milling disks. Experiments revealed that the perceived roughness of these disks can be changed by altering the rotation speed and direction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05151v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soma Kato, Izumi Mizoguchi, Hiroyuki Kajimoto</dc:creator>
    </item>
    <item>
      <title>Passive Touch Experience with Virtual Doctor Fish Using Ultrasound Haptics</title>
      <link>https://arxiv.org/abs/2411.05152</link>
      <description>arXiv:2411.05152v1 Announce Type: new 
Abstract: This study implements and evaluates passive interaction using an autostereoscopic display and ultrasound haptics technology, simulating Garra rufa ("doctor fish") nibbling. When the virtual doctor fish touches the user's hand, ultrasound tactile sensations are presented by spatio-temporal modulation (STM) as an ultrasound focal point orbits around the contact points. A user study evaluated parameters affecting realism, including STM frequency, the number of focal points, ultrasound amplitude, and hand moistening. Comparing several combinations of parameters revealed that representing contact with fewer representative points and setting the frequency of the STM to 10 Hz produced the most realistic experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05152v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manato Yo, Atsushi Matsubayashi, Yasutoshi Makino, Hiroyuki Shinoda</dc:creator>
    </item>
    <item>
      <title>Wearable Haptic Device to Render 360-degree Torque Feedback on the Wrist</title>
      <link>https://arxiv.org/abs/2411.05153</link>
      <description>arXiv:2411.05153v1 Announce Type: new 
Abstract: Haptic feedback increases the realism of virtual environments. This paper proposes a wearable haptic device that renders torque feedback to the user's wrist from any angle. The device comprises a control part and a handle part. The control part consists of three DC gear motors and a microcontroller, while the handle part securely holds the Oculus Quest 2 right controller. The control part manages string tension to deliver the sensation of torque feedback during interactions with virtual tools or objects. The three points of the handle part are connected to the three motors of the control part via strings, which pull the handle part to render precise 360-degree (yaw and pitch) torque feedback to the user's wrist. Finally, to show the effectiveness of the proposed device, two VR demos were implemented- Shooting Game and Shielding Experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05153v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seungchae Kim, Mohammad Shadman Hashem, Seokhee Jeon</dc:creator>
    </item>
    <item>
      <title>TelEdge: Haptic Tele-Communication of a Smartphone by Electro-Tactile Stimulation Through the Edges</title>
      <link>https://arxiv.org/abs/2411.05154</link>
      <description>arXiv:2411.05154v1 Announce Type: new 
Abstract: We present TelEdge, a novel method of remote haptic communication using electrical stimulation through the edges of the smartphone. The aim of this study is to explore communications that can be created by adding touch sensing and haptic feedback using the electrical edge display to conventional audio-visual functionality. We conducted monitoring observations and interviews during a video call between two people, presenting interactive haptic feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05154v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taiki Takami, Izumi Mizoguchi, Hiroyuki Kajimoto</dc:creator>
    </item>
    <item>
      <title>DynaPain: Moving Flame Beetle with Dynamic Pain Illusion Adapting Apparent Movement to Thermal Grill Illusion</title>
      <link>https://arxiv.org/abs/2411.05155</link>
      <description>arXiv:2411.05155v1 Announce Type: new 
Abstract: Pain sensation presentation with movable sensory position is important to imitate the pain caused by objects in motion and the pain corresponding to a person's movements. We aimed at proposing a novel dynamic pain sensation experience, called DynaPain. DynaPain was achieved by the non-contact thermal grill illusion and the apparent movement. The demonstration provided the dynamic heat and pain experience through interaction with a flame beetle moving on the arm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05155v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Souta Mizuno, Jiayi Xu, Shoichi Hasegawa, Naoto Ienaga, Yoshihiro Kuroda</dc:creator>
    </item>
    <item>
      <title>HeatFlicker: A Virtual Campfire System Utilizing Flickering Thermal Illusions by Asymmetric Vibrations</title>
      <link>https://arxiv.org/abs/2411.05157</link>
      <description>arXiv:2411.05157v1 Announce Type: new 
Abstract: In recent years, thermal feedback has emerged as a significant sensory modality in virtual reality. However, the concept of conveying the sensation of thermal movement remains largely unexplored. We propose HeatFlicker, a virtual campfire device that recreates the flickering of fire by using a thermal illusion of moving heat identified in preliminary experiments. This device creates the illusion of heat moving from a fixed heat source. In our demonstration, we provide a novel thermal experience by simulating the flickering of a real fire.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05157v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takato Ito, Takeshi Tanabe, Shoichi Hasegawa, Naoto Ienaga, Yoshihiro Kuroda</dc:creator>
    </item>
    <item>
      <title>visionFinGAR: Transmission of Softness and Shape Motion by Vision Based Tactile Sensor and Combination of Mechanical and Electrical Stimulation</title>
      <link>https://arxiv.org/abs/2411.05158</link>
      <description>arXiv:2411.05158v1 Announce Type: new 
Abstract: This paper describes a system for transmitting softness and the motion of shape or contact area sensation using a vision based tactile sensor and a tactile display in which mechanical and electrical stimulation are combined. A unit of tactile sensor consists of a camera and markers, enable to detect a light touch, a pressure or a shape. On the other hand, a unit of tactile display consists of an electrode array and a mechanical arm to provide softness / pressure and shape perception. The display can provide four mode stimulation: anodic, cathodic, mechanical vibration and skin deformation; thus, it can reproduce a large range of tactile sensations. This study mainly aims to transmit a wide range of softness and shape motion perception with a vision based tactile sensor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05158v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hikaru Kukita, Hiroyuki Kajimoto, Yem Vibol</dc:creator>
    </item>
    <item>
      <title>A self-healing tactile sensor using an optical waveguide</title>
      <link>https://arxiv.org/abs/2411.05159</link>
      <description>arXiv:2411.05159v1 Announce Type: new 
Abstract: We propose an optical tactile sensor using self-healing materials. The proposed tactile sensor consists of a structure that includes a diode, a phototransistor, and an optical waveguide made from self-healing materials. This design offers the advantage of being less susceptible to electromagnetic noise compared to traditional tactile sensors based on electrical detection principles. The sensor estimates the applied force by detecting changes in the total internal reflection caused by deformation due to contact force. In this study, we first established a fabrication method for the optical waveguide-based tactile sensor using self-healing materials. Subsequently, we measured the sensor output when a static load was applied to the fabricated tactile sensor and evaluated its characteristics. The results confirmed that the sensor output decreases in response to the applied load.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05159v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seiichi Yamamoto, Hiroki Ishizuka, Sei Ikeda, Osamu Oshiro</dc:creator>
    </item>
    <item>
      <title>Measurement and Interpolation for Data-Driven Pressure Distribution Rendering on a Finger Pad</title>
      <link>https://arxiv.org/abs/2411.05160</link>
      <description>arXiv:2411.05160v1 Announce Type: new 
Abstract: We propose a data-driven pressure distribution rendering method that uses the interpolation of experimentally obtained pressure values. The pressure data were collected using a pressure sensor array. The prediction was performed using linear interpolation, assuming that the pressure distribution is dependent on pushing displacement and contact angle. Leap Motion Controller was used to implement the prediction based on user input. The proposed prediction model was found to be fast and reproduce the measured data well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05160v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kazuya Sase, Rei Onodera, Hikaru Nagano, Masashi Konyo</dc:creator>
    </item>
    <item>
      <title>Investigation of Tactile Texture Simulation on Online Shopping Experience</title>
      <link>https://arxiv.org/abs/2411.05161</link>
      <description>arXiv:2411.05161v1 Announce Type: new 
Abstract: With safety measures towards the current Covid-19 pandemic, many retails clothing stores have restricted on-site fittings and shifted their business online. Inability to touch on product evaluations shows an apparent limitation as compared to retail shopping especially when the object's material information is crucial like clothing. Haptic technologies show potential of bridging the gap between online shops and the shoppers by providing a sense of touch, yet little research has been done especially on the effect of the simulation of tactile texture on the shopping experience. In this study, we modified a mock-up e-commerce website by adding clothing products and enabling a mid-air haptic interface with Ultrahaptics Evaluation Kit (UHEV1). We developed texture sensations using Time Point Streaming (TSP) modulation for clothing products with different texture materials and a user study was carried out to investigate the tactile texture sensation on shoppers' experience in evaluating online products. Our results show that tactile texture sensation using multipoint mid-air haptic feedback improves online shopper's satisfaction on the product browsing experience. This study contributes to the improvement of general lifestyle of the society in terms of e-commerce experience and could expand its application to impact different sectors like education and different communities including the visually impaired.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05161v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pei Hsin Lim, Kian Meng Yap</dc:creator>
    </item>
    <item>
      <title>Automatic Authoring of Physical and Perceptual/Affective Motion Effects for Virtual Reality</title>
      <link>https://arxiv.org/abs/2411.05162</link>
      <description>arXiv:2411.05162v1 Announce Type: new 
Abstract: This demo is about automatic authoring of various motion effects that are provided with audiovisual content to improve user experiences. Traditionally, motion effects have been used for simulators, e.g., flight simulators for pilots and astronauts, to present physically accurate vestibular feedback. At present, we have greatly wider use of motion effects for entertainment purposes, such as 4D rides in amusement parks and even shopping malls, 4D films in theaters, and relative new virtual reality games with head-mounted displays and personal motion platforms. However, the production of motion effects is done solely by manual authoring or coding, and this costly process prevents the faster and wider dissemination of 4D content. It is imperative to facilitate motion effect production by providing automatic synthesis algorithms. This demo video presents nine different automatic synthesis algorithms for motion effects and a recorded demonstration of each.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05162v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiwan Lee, Seungmoon Choi</dc:creator>
    </item>
    <item>
      <title>Soft or Stiff? Stroop Tasks in Visuo-Tactile Tapping Interactions</title>
      <link>https://arxiv.org/abs/2411.05163</link>
      <description>arXiv:2411.05163v1 Announce Type: new 
Abstract: One of the key challenges in the field of haptic research is designing plausible stimuli using haptic interfaces with limited degrees of freedom. Although the plausible approach, which simplifies and/or exaggerates stimuli to enhance information transfer or create an artistic effect, has proven effective, evaluations of such stimuli have traditionally relied on subjective measures. This study aims to establish an objective evaluation method for haptic stimuli designed using the plausible approach. Focusing on stiffness/material perception, we developed a Stroop test within visuo-tactile tapping interactions in a virtual space. The demonstration system presents visual (textures) and tactile (vibration) stimuli at the moment of contact between a stylus and a cube, prompting participants to immediately identify the material they perceive visually. If the tactile stimuli are perceived as plausible, reaction times will be longer when the visual and tactile stimuli represent different materials than when they represent the same material.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05163v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryotaro Ishikawa, Taku Hachisu</dc:creator>
    </item>
    <item>
      <title>Haptic Dial based on Magnetorheological Fluid Having Bumpy Structure</title>
      <link>https://arxiv.org/abs/2411.05165</link>
      <description>arXiv:2411.05165v1 Announce Type: new 
Abstract: We proposed a haptic dial based on magnetorheological fluid (MRF) which enhances performance by increasing the MRF-exposed area through concave shaft and housing structure. We developed a breakout-style game to show that the proposed haptic dial allows users to efficiently interact with virtual objects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05165v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seok Hun Lee, Yong Hae Heo, Seok-Han Lee, Sang-Youn Kim</dc:creator>
    </item>
    <item>
      <title>Out-of-body Localization of Virtual Vibration Sources Using a Limited Numbers of Transducers on the Torso</title>
      <link>https://arxiv.org/abs/2411.05166</link>
      <description>arXiv:2411.05166v1 Announce Type: new 
Abstract: Stereohaptic vibration is an innovative vibrotactile technology that extends the conventional tactile localization to the surrounding space, representing a virtual vibration source in the external environment. Previously, we have developed displays on the forearms and soles. Today, we present a demonstration of a new jacket-type device, which enables localization at any position around the body by arranging multiple vibrators along the torso centered on the midline. In our demonstration, you can experience the footsteps and roars of a inosaur walking around you, and it provides an experience that is as if you are in a fantasy movie.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05166v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gen Ohara, Masashi Konyo, Satoshi Tadokoro</dc:creator>
    </item>
    <item>
      <title>Algorithmic Autonomy in Data-Driven AI</title>
      <link>https://arxiv.org/abs/2411.05210</link>
      <description>arXiv:2411.05210v1 Announce Type: new 
Abstract: In societies increasingly entangled with algorithms, our choices are constantly influenced and shaped by automated systems. This convergence highlights significant concerns for individual autonomy in the age of data-driven AI. It leads to pressing issues such as data-driven segregation, gaps in accountability for algorithmic decisions, and the infringement on essential human rights and values. Through this article, we introduce and explore the concept of algorithmic autonomy, examining what it means for individuals to have autonomy in the face of the pervasive impact of algorithms on our societies. We begin by outlining the data-driven characteristics of AI and its role in diminishing personal autonomy. We then explore the notion of algorithmic autonomy, drawing on existing research. Finally, we address important considerations, highlighting current challenges and directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05210v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ge Wang, Roy Pea</dc:creator>
    </item>
    <item>
      <title>ARLang: An Outdoor Augmented Reality Application for Portuguese Vocabulary Learning</title>
      <link>https://arxiv.org/abs/2411.05211</link>
      <description>arXiv:2411.05211v1 Announce Type: new 
Abstract: With recent computer vision techniques and user-generated content, we can augment the physical world with metadata that describes attributes, such as names, geo-locations, and visual features of physical objects. To assess the benefits of these potentially ubiquitous labels for foreign vocabulary learning, we built a proof-of-concept system that displays bilingual text and sound labels on physical objects outdoors using augmented reality. Established tools for language learning have focused on effective content delivery methods such as books and flashcards. However, recent research and consumer learning tools have begun to focus on how learning can become more mobile, ubiquitous, and desirable. To test whether our system supports vocabulary learning, we conducted a preliminary between-subjects (N=44) study. Our results indicate that participants preferred learning with virtual labels on real-world objects outdoors over learning with flashcards. Our findings motivate further investigation into mobile AR-based learning systems in outdoor settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05211v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3563657.3596090</arxiv:DOI>
      <arxiv:journal_reference>In Proceedings of the 2023 ACM Designing Interactive Systems Conference (pp. 1224-1235)</arxiv:journal_reference>
      <dc:creator>Arthur Caetano, Alyssa Lawson, Yimeng Liu, Misha Sra</dc:creator>
    </item>
    <item>
      <title>ARfy: A Pipeline for Adapting 3D Scenes to Augmented Reality</title>
      <link>https://arxiv.org/abs/2411.05218</link>
      <description>arXiv:2411.05218v1 Announce Type: new 
Abstract: Virtual content placement in physical scenes is a crucial aspect of augmented reality (AR). This task is particularly challenging when the virtual elements must adapt to multiple target physical environments that are unknown during development. AR authors use strategies such as manual placement performed by end-users, automated placement powered by author-defined constraints, and procedural content generation to adapt virtual content to physical spaces. Although effective, these options require human effort or annotated virtual assets. As an alternative, we present ARfy, a pipeline to support the adaptive placement of virtual content from pre-existing 3D scenes in arbitrary physical spaces. ARfy does not require intervention by end-users or asset annotation by AR authors. We demonstrate the pipeline capabilities using simulations on a publicly available indoor space dataset. ARfy automatically makes any generic 3D scene AR-ready and provides evaluation tools to facilitate future research on adaptive virtual content placement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05218v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3526114.3558697</arxiv:DOI>
      <arxiv:journal_reference>In Adjunct Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology (pp. 1-3) (2022, October)</arxiv:journal_reference>
      <dc:creator>Arthur Caetano, Misha Sra</dc:creator>
    </item>
    <item>
      <title>GraV: Grasp Volume Data for the Design of One-Handed XR Interfaces</title>
      <link>https://arxiv.org/abs/2411.05245</link>
      <description>arXiv:2411.05245v1 Announce Type: new 
Abstract: Everyday objects, like remote controls or electric toothbrushes, are crafted with hand-accessible interfaces. Expanding on this design principle, extended reality (XR) interfaces for physical tasks could facilitate interaction without necessitating the release of grasped tools, ensuring seamless workflow integration. While established data, such as hand anthropometric measurements, guide the design of handheld objects, XR currently lacks comparable data, regarding reachability, for single-hand interfaces while grasping objects. To address this, we identify critical design factors and a design space representing grasp-proximate interfaces and introduce a simulation tool for generating reachability and displacement cost data for designing these interfaces. Additionally, using the simulation tool, we generate a dataset based on grasp taxonomy and common household objects. Finally, we share insights from a design workshop that emphasizes the significance of reachability and motion cost data, empowering XR creators to develop bespoke interfaces tailored specifically to grasping hands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05245v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3643834.3661567</arxiv:DOI>
      <arxiv:journal_reference>In Proceedings of the 2024 ACM Designing Interactive Systems Conference (pp. 151-167) (2024, July)</arxiv:journal_reference>
      <dc:creator>Alejandro Aponte, Arthur Caetano, Yunhao Luo, Misha Sra</dc:creator>
    </item>
    <item>
      <title>GroupBeaMR: Analyzing Collaborative Group Behavior in Mixed Reality Through Passive Sensing and Sociometry</title>
      <link>https://arxiv.org/abs/2411.05258</link>
      <description>arXiv:2411.05258v1 Announce Type: new 
Abstract: Understanding group behavior is essential for improving collaboration and productivity. While research on group behavior in virtual reality (VR) is significantly advanced, understanding group dynamics in mixed reality (MR) remains understudied. Understanding MR group dynamics will enable designing systems that optimize collaboration, enhance productivity, and improve user experiences. This work outlines how MR headsets sensory systems can effectively capture group behavior, devise algorithms to process and interpret the data and demonstrate the correlation between group behavior and task-related performance metrics. We propose a framework for group behavior analysis in MR, or GroupBeaMR for short, to capture and analyze group behavior in MR. Using the rich sensory capabilities of MR headsets, GroupBeaMR passively collects data on conversation, shared attention, and proximity. This data is processed using social network analysis techniques to identify patterns of interaction and assess group behavior. Our evaluation, involving 44 participants in 11 groups, demonstrates the effectiveness of GroupBeaMR in capturing and analyzing group behavior in collaborative MR tasks. An example of insight from GroupBeaMR is that balanced participation in different types of interaction leads to higher group cohesion. These findings enable real-time assessments of group behavior in MR that can enhance collaborative experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05258v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Diana Romero, Yasra Chandio, Fatima Anwar, Salma Elmalaki</dc:creator>
    </item>
    <item>
      <title>Tap into Reality: Understanding the Impact of Interactions on Presence and Reaction Time in Mixed Reality</title>
      <link>https://arxiv.org/abs/2411.05272</link>
      <description>arXiv:2411.05272v1 Announce Type: new 
Abstract: Enhancing presence in mixed reality (MR) relies on precise measurement and quantification. While presence has traditionally been measured through subjective questionnaires, recent research links presence with objective metrics like reaction time. Past studies examined this correlation with varying technical factors (object realism and behavior) and human conditioning, but the impact of interaction remains unclear. To answer this question, we conducted a within-subjects study (N=50) to explore the correlation between presence and reaction time across two interaction scenarios (direct and symbolic) with two tasks (selection and manipulation). We found that presence scores and reaction times are correlated (correlation coefficient of $-0.54$), suggesting that the impact of interaction on reaction time correlates with its effect on presence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05272v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yasra Chandio, Victoria Interrante, Fatima Anwar</dc:creator>
    </item>
    <item>
      <title>Reaction Time as a Proxy for Presence in Mixed Reality with Distraction</title>
      <link>https://arxiv.org/abs/2411.05275</link>
      <description>arXiv:2411.05275v1 Announce Type: new 
Abstract: Distractions in mixed reality (MR) environments can significantly influence user experience, affecting key factors such as presence, reaction time, cognitive load, and Break in Presence (BIP). Presence measures immersion, reaction time captures user responsiveness, cognitive load reflects mental effort, and BIP represents moments when attention shifts from the virtual to the real world, breaking immersion. However, the effects of distractions on these elements remain insufficiently explored. To address this gap, we have presented a theoretical model to understand how congruent and incongruent distractions affect all these constructs. We conducted a within-subject study (N=54) where participants performed image-sorting tasks under different distraction conditions. Our findings show that incongruent distractions significantly increase cognitive load, slow reaction times, and elevate BIP frequency, with presence mediating these effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05275v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yasra Chandio, Victoria Interrante, Fatima M. Anwar</dc:creator>
    </item>
    <item>
      <title>Proposition pour une gestion dynamique de l'inter-activit{\'e}s dans le TCAO</title>
      <link>https://arxiv.org/abs/2411.05410</link>
      <description>arXiv:2411.05410v1 Announce Type: new 
Abstract: Using some results coming from human and social sciences, we are working on the still important problem of tailorability inside CSCW systems. Our proposition aims at favouring the dynamic integration of groupware systems in a global and integrated environment that creates a context for their use. This work leads us to define the problem of the inter-activities management. This study helps us to propose a technical solution to this problem and that is realized in the CooLDA platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05410v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/1148613.1148644</arxiv:DOI>
      <arxiv:journal_reference>the 16th conference, Aug 2004, Namur, France. pp.191-194</arxiv:journal_reference>
      <dc:creator>Gr\'egory Bourguin (LISIC)</dc:creator>
    </item>
    <item>
      <title>StoryExplorer: A Visualization Framework for Storyline Generation of Textual Narratives</title>
      <link>https://arxiv.org/abs/2411.05435</link>
      <description>arXiv:2411.05435v1 Announce Type: new 
Abstract: In the context of the exponentially increasing volume of narrative texts such as novels and news, readers struggle to extract and consistently remember storyline from these intricate texts due to the constraints of human working memory and attention span. To tackle this issue, we propose a visualization approach StoryExplorer, which facilitates the process of knowledge externalization of narrative texts and further makes the form of mental models more coherent. Through the formative study and close collaboration with 2 domain experts, we identified key challenges for the extraction of the storyline. Guided by the distilled requirements, we then propose a set of workflow (i.e., insight finding-scripting-storytelling) to enable users to interactively generate fragments of narrative structures. We then propose a visualization system StoryExplorer which combines stroke annotation and GPT-based visual hints to quickly extract story fragments and interactively construct storyline. To evaluate the effectiveness and usefulness of StoryExplorer, we conducted 2 case studies and in-depth user interviews with 16 target users. The result shows that users can better extract the storyline by using StoryExplorer along with the proposed workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05435v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Li Ye, Lei Wang, Shaolun Ruan, Yuwei Meng, Yigang Wang, Wei Chen, Zhiguang Zhou</dc:creator>
    </item>
    <item>
      <title>Digitalization and Virtual Assistive Systems in Tourist Mobility: Evolution, an Experience (with Observed Mistakes), Appropriate Orientations and Recommendations</title>
      <link>https://arxiv.org/abs/2411.05446</link>
      <description>arXiv:2411.05446v1 Announce Type: new 
Abstract: Digitalization and virtualization are extremely active and important approaches in a large scope of activities (marketing, selling, enterprise management, logistics). Tourism management is also highly concerned by this evolution. In this paper we try to present today's situation based on a 7-week trip showing appropriate and shame situations. After this case study, we give a list of appropriate practices and orientations and confirm the fundamental role of User Experience in validating the proposed assistive system and the User Interfaces needed for client/user satisfaction. We also outline the expected role of Metaverse in the future of the evolution of this domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05446v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-35908-8_10</arxiv:DOI>
      <arxiv:journal_reference>HCI International 2023 25th International Conference on Human-Computer Interaction, Jul 2023, Copenhagen, Denmark. pp.125-141</arxiv:journal_reference>
      <dc:creator>Bertrand David (SICAL), Ren\'e Chalon (SICAL)</dc:creator>
    </item>
    <item>
      <title>The Framework of NAVIS: Navigating Virtual Spaces with Immersive Scooters</title>
      <link>https://arxiv.org/abs/2411.05569</link>
      <description>arXiv:2411.05569v1 Announce Type: new 
Abstract: Virtual reality (VR) environments have greatly expanded opportunities for immersive exploration, yet physically navigating these digital spaces remains a significant challenge. In this paper, we present the conceptual framework of NAVIS (Navigating Virtual Spaces with Immersive Scooters), a novel system that utilizes a scooter-based interface to enhance both navigation and interaction within virtual environments. NAVIS combines real-time physical mobility, haptic feedback, and CAVE-like (Cave Automatic Virtual Environment) technology to create a realistic sense of travel and movement, improving both spatial awareness and the overall immersive experience. By offering a more natural and physically engaging method of exploration, NAVIS addresses key limitations found in traditional VR locomotion techniques, such as teleportation or joystick control, which can detract from immersion and realism. This approach highlights the potential of combining physical movement with virtual environments to provide a more intuitive and enjoyable experience for users, opening up new possibilities for applications in gaming, education, and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05569v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3701571.3703381</arxiv:DOI>
      <arxiv:journal_reference>International Conference on Mobile and Ubiquitous Multimedia 2024</arxiv:journal_reference>
      <dc:creator>Zhixun Lin, Wei He, Xinyi Liu, Mingchen Ye, Xiang Li, Ge Lin Kan</dc:creator>
    </item>
    <item>
      <title>LightVA: Lightweight Visual Analytics with LLM Agent-Based Task Planning and Execution</title>
      <link>https://arxiv.org/abs/2411.05651</link>
      <description>arXiv:2411.05651v1 Announce Type: new 
Abstract: Visual analytics (VA) requires analysts to iteratively propose analysis tasks based on observations and execute tasks by creating visualizations and interactive exploration to gain insights. This process demands skills in programming, data processing, and visualization tools, highlighting the need for a more intelligent, streamlined VA approach. Large language models (LLMs) have recently been developed as agents to handle various tasks with dynamic planning and tool-using capabilities, offering the potential to enhance the efficiency and versatility of VA. We propose LightVA, a lightweight VA framework that supports task decomposition, data analysis, and interactive exploration through human-agent collaboration. Our method is designed to help users progressively translate high-level analytical goals into low-level tasks, producing visualizations and deriving insights. Specifically, we introduce an LLM agent-based task planning and execution strategy, employing a recursive process involving a planner, executor, and controller. The planner is responsible for recommending and decomposing tasks, the executor handles task execution, including data analysis, visualization generation and multi-view composition, and the controller coordinates the interaction between the planner and executor. Building on the framework, we develop a system with a hybrid user interface that includes a task flow diagram for monitoring and managing the task planning process, a visualization panel for interactive data exploration, and a chat view for guiding the model through natural language instructions. We examine the effectiveness of our method through a usage scenario and an expert study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05651v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuheng Zhao, Junjie Wang, Linbin Xiang, Xiaowen Zhang, Zifei Guo, Cagatay Turkay, Yu Zhang, Siming Chen</dc:creator>
    </item>
    <item>
      <title>The influence of persona and conversational task on social interactions with a LLM-controlled embodied conversational agent</title>
      <link>https://arxiv.org/abs/2411.05653</link>
      <description>arXiv:2411.05653v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in conversational tasks. Embodying an LLM as a virtual human allows users to engage in face-to-face social interactions in Virtual Reality. However, the influence of person- and task-related factors in social interactions with LLM-controlled agents remains unclear. In this study, forty-six participants interacted with a virtual agent whose persona was manipulated as extravert or introvert in three different conversational tasks (small talk, knowledge test, convincing). Social-evaluation, emotional experience, and realism were assessed using ratings. Interactive engagement was measured by quantifying participants' words and conversational turns. Finally, we measured participants' willingness to ask the agent for help during the knowledge test. Our findings show that the extraverted agent was more positively evaluated, elicited a more pleasant experience and greater engagement, and was assessed as more realistic compared to the introverted agent. Whereas persona did not affect the tendency to ask for help, participants were generally more confident in the answer when they had help of the LLM. Variation of personality traits of LLM-controlled embodied virtual agents, therefore, affects social-emotional processing and behavior in virtual interactions. Embodied virtual agents allow the presentation of naturalistic social encounters in a virtual environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05653v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leon O. H. Kroczek, Alexander May, Selina Hettenkofer, Andreas Ruider, Bernd Ludwig, Andreas M\"uhlberger</dc:creator>
    </item>
    <item>
      <title>Foundations for the psychological safety of human and autonomous vehicles interaction</title>
      <link>https://arxiv.org/abs/2411.05732</link>
      <description>arXiv:2411.05732v1 Announce Type: new 
Abstract: This paper addresses the critical issue of psychological safety in the design and operation of autonomous vehicles, which are increasingly integrated with artificial intelligence technologies. While traditional safety standards focus primarily on physical safety, this paper emphasizes the psychological implications that arise from human interactions with autonomous vehicles, highlighting the importance of trust and perceived risk as significant factors influencing user acceptance. Through a review of existing safety techniques, the paper defines psychological safety in the context of autonomous vehicles, proposes a risk model to identify and assess psychological risks, and adopts a system-theoretic analysis method. The paper illustrates the potential psychological hazards using a scenario involving a family's experience with an autonomous vehicle, aiming to systematically evaluate situations that could lead to psychological harm. By establishing a framework that incorporates psychological safety alongside physical safety, the paper contributes to the broader discourse on the safe deployment of autonomous vehicle and aims to guide future developments in user-cantered design and regulatory practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05732v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yandika Sirgabsou, Benjamin Hardin, Fran\c{c}ois Leblanc, Efi Raili, Pericle Salvini, David Jackson, Marina Jirotka, Lars Kunze</dc:creator>
    </item>
    <item>
      <title>Effects of Distributed Friction Actuation During Sliding Touch</title>
      <link>https://arxiv.org/abs/2411.05769</link>
      <description>arXiv:2411.05769v1 Announce Type: new 
Abstract: Friction modulation allows for a range of different sensations and textures to be simulated on flat touchscreens, yet is largely unable to render fundamental tactile interactions such as path following or shape discrimination due to lack of spatial force distribution across the fingerpad. In order to expand the range of sensations rendered via friction modulation, in this paper we explore the possibility of applying spatial feedback on the fingerpad via differing friction forces on flat touchscreens. To this end, we fabricated six distinct flat surfaces with different spatial distributions of friction and observed deformation of the fingerpad skin in response to motion along these physical samples. In our study, friction changes that occur sequentially along the sliding direction introduced little transitory spatial warping such as compression or stretching to the fingerpad, suggesting limited perceptual differences in comparison to 'classic' friction modulation. Distributing friction across the direction of motion, however, showed pattern-dependent shearing of the fingertip skin, opening avenues for new sensations and illusions heretofore unachievable on flat touchscreen surfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05769v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>MacKenzie Harnett, Paras Kumar, Rebecca F. Friesen</dc:creator>
    </item>
    <item>
      <title>LLMs as Research Tools: A Large Scale Survey of Researchers' Usage and Perceptions</title>
      <link>https://arxiv.org/abs/2411.05025</link>
      <description>arXiv:2411.05025v1 Announce Type: cross 
Abstract: The rise of large language models (LLMs) has led many researchers to consider their usage for scientific work. Some have found benefits using LLMs to augment or automate aspects of their research pipeline, while others have urged caution due to risks and ethical concerns. Yet little work has sought to quantify and characterize how researchers use LLMs and why. We present the first large-scale survey of 816 verified research article authors to understand how the research community leverages and perceives LLMs as research tools. We examine participants' self-reported LLM usage, finding that 81% of researchers have already incorporated LLMs into different aspects of their research workflow. We also find that traditionally disadvantaged groups in academia (non-White, junior, and non-native English speaking researchers) report higher LLM usage and perceived benefits, suggesting potential for improved research equity. However, women, non-binary, and senior researchers have greater ethical concerns, potentially hindering adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05025v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhehui Liao, Maria Antoniak, Inyoung Cheong, Evie Yu-Yen Cheng, Ai-Heng Lee, Kyle Lo, Joseph Chee Chang, Amy X. Zhang</dc:creator>
    </item>
    <item>
      <title>Deep Learning and Machine Learning -- Natural Language Processing: From Theory to Application</title>
      <link>https://arxiv.org/abs/2411.05026</link>
      <description>arXiv:2411.05026v1 Announce Type: cross 
Abstract: With a focus on natural language processing (NLP) and the role of large language models (LLMs), we explore the intersection of machine learning, deep learning, and artificial intelligence. As artificial intelligence continues to revolutionize fields from healthcare to finance, NLP techniques such as tokenization, text classification, and entity recognition are essential for processing and understanding human language. This paper discusses advanced data preprocessing techniques and the use of frameworks like Hugging Face for implementing transformer-based models. Additionally, it highlights challenges such as handling multilingual data, reducing bias, and ensuring model robustness. By addressing key aspects of data processing and model fine-tuning, this work aims to provide insights into deploying effective and ethically sound AI solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05026v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keyu Chen, Cheng Fei, Ziqian Bi, Junyu Liu, Benji Peng, Sen Zhang, Xuanhe Pan, Jiawei Xu, Jinlang Wang, Caitlyn Heqi Yin, Yichao Zhang, Pohsun Feng, Yizhu Wen, Tianyang Wang, Ming Li, Jintao Ren, Qian Niu, Silin Chen, Weiche Hsieh, Lawrence K. Q. Yan, Chia Xin Liang, Han Xu, Hong-Ming Tseng, Xinyuan Song, Ming Liu</dc:creator>
    </item>
    <item>
      <title>EnchantedClothes: Visual and Tactile Feedback with an Abdomen-Attached Robot through Clothes</title>
      <link>https://arxiv.org/abs/2411.05102</link>
      <description>arXiv:2411.05102v1 Announce Type: cross 
Abstract: Wearable robots are designed to be worn on the human body. Taking advantage of their physical form, various applications for wearable robots are being considered. This study proposes a wearable robot worn on the abdomen and a new interaction with it. Our robot enables a variety of applications related to communication between the wearer and surrounding humans through visual and tactile feedback. The contributions of this research will be (1) the proposal of a novel wearable robot worn on the abdomen and (2) a new interaction with it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05102v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takumi Yamamoto, Rin Yoshimura, Yuta Sugiura</dc:creator>
    </item>
    <item>
      <title>Socially Assistive Robots: A Technological Approach to Emotional Support</title>
      <link>https://arxiv.org/abs/2411.05122</link>
      <description>arXiv:2411.05122v1 Announce Type: cross 
Abstract: In today's high-pressure and isolated society, the demand for emotional support has surged, necessitating innovative solutions. Socially Assistive Robots (SARs) offer a technological approach to providing emotional assistance by leveraging advanced robotics, artificial intelligence, and sensor technologies. This study explores the development of an emotional support robot designed to detect and respond to human emotions, particularly sadness, through facial recognition and gesture analysis. Utilising the Lego Mindstorms Robotic Kit, Raspberry Pi 4, and various Python libraries, the robot is capable of delivering empathetic interactions, including comforting hugs and AI-generated conversations. Experimental findings highlight the robot's effective facial recognition accuracy, user interaction, and hug feedback mechanisms. These results demonstrate the feasibility of using SARs for emotional support, showcasing their potential features and functions. This research underscores the promise of SARs in providing innovative emotional assistance and enhancing human-robot interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05122v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leanne Oon Hui Yee, Siew Sui Fun, Thit Sar Zin, Zar Nie Aung, Kian Meng Yap, Jiehan Teoh</dc:creator>
    </item>
    <item>
      <title>Vibrotactile Feedback for a Remote Operated Robot with Noise Subtraction Based on Perceived Intensity</title>
      <link>https://arxiv.org/abs/2411.05138</link>
      <description>arXiv:2411.05138v1 Announce Type: cross 
Abstract: There is a growing demand for teleoperated robots. This paper presents a novel method for reducing vibration noise generated by robot's own motion, which can disrupt the quality of tactile feedback for teleoperated robots. Our approach focuses on perceived intensity, the amount of how humans experience vibration, to create a noise filter that aligns with human perceptual characteristics. This system effectively subtracts ego-noise while preserving the essential tactile signals, ensuring more accurate and reliable haptic feedback for operators. This method offers a refined solution to the challenge of maintaining high-quality tactile feedback in teleoperated systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05138v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryoma Yamawaki, Takeru Shimamura, Noel Alejandro Avila Campos, Masashi Konyo, Shotaro Kojima, Ranulfo Bezerra, Satoshi Tadokoro</dc:creator>
    </item>
    <item>
      <title>Conveying Surroundings Information of a Robot End-Effector by Adjusting Controller Button Stiffness</title>
      <link>https://arxiv.org/abs/2411.05164</link>
      <description>arXiv:2411.05164v1 Announce Type: cross 
Abstract: This study addresses the challenge of low dexterity in teleoperation tasks caused by limited sensory feedback and visual occlusion. We propose a novel approach that integrates haptic feedback into teleoperation using the adaptive triggers of a commercially available DualSense controller. By adjusting button stiffness based on the proximity of objects to the robot's end effector, the system provides intuitive, real-time feedback to the operator. To achieve this, the effective volume of the end effector is virtually expanded, allowing the system to predict interactions by calculating overlap with nearby objects. This predictive capability is independent of the user's intent or the robot's speed, enhancing the operator's situational awareness without requiring complex pre-programmed behaviors. The stiffness of the adaptive triggers is adjusted in proportion to this overlapping volume, effectively conveying spatial proximity and movement cues through an "one degree of freedom" haptic feedback mechanism. Compared to existing solutions, this method reduces hardware requirements and computational complexity by using a geometric simplification approach, enabling efficient operation with minimal processing demands. Simulation results demonstrate that the proposed system reduces collision risk and improves user performance, offering an intuitive, precise, and safe teleoperation experience despite real-world uncertainties and communication delays.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05164v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noel Alejandro Avila Campos, Masashi Konyo, Ranulfo Bezerra, Shotaro Kojima, Satoshi Tadokoro</dc:creator>
    </item>
    <item>
      <title>Toward Cultural Interpretability: A Linguistic Anthropological Framework for Describing and Evaluating Large Language Models (LLMs)</title>
      <link>https://arxiv.org/abs/2411.05200</link>
      <description>arXiv:2411.05200v1 Announce Type: cross 
Abstract: This article proposes a new integration of linguistic anthropology and machine learning (ML) around convergent interests in both the underpinnings of language and making language technologies more socially responsible. While linguistic anthropology focuses on interpreting the cultural basis for human language use, the ML field of interpretability is concerned with uncovering the patterns that Large Language Models (LLMs) learn from human verbal behavior. Through the analysis of a conversation between a human user and an LLM-powered chatbot, we demonstrate the theoretical feasibility of a new, conjoint field of inquiry, cultural interpretability (CI). By focusing attention on the communicative competence involved in the way human users and AI chatbots co-produce meaning in the articulatory interface of human-computer interaction, CI emphasizes how the dynamic relationship between language and culture makes contextually sensitive, open-ended conversation possible. We suggest that, by examining how LLMs internally "represent" relationships between language and culture, CI can: (1) provide insight into long-standing linguistic anthropological questions about the patterning of those relationships; and (2) aid model developers and interface designers in improving value alignment between language models and stylistically diverse speakers and culturally diverse speech communities. Our discussion proposes three critical research axes: relativity, variation, and indexicality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05200v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Graham M. Jones, Shai Satran, Arvind Satyanarayan</dc:creator>
    </item>
    <item>
      <title>Improving Multi-Domain Task-Oriented Dialogue System with Offline Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2411.05340</link>
      <description>arXiv:2411.05340v1 Announce Type: cross 
Abstract: Task-oriented dialogue (TOD) system is designed to accomplish user-defined tasks through dialogues. The TOD system has progressed towards end-to-end modeling by leveraging pre-trained large language models. Fine-tuning the pre-trained language models using only supervised learning leads to the exposure bias and token loss problem and it deviates the models from completing the user's task. To address these issues, we propose a TOD system that leverages a unified pre-trained language model, GPT2, as a base model. It is optimized using supervised learning and reinforcement learning (RL). The issues in the TOD system are mitigated using a non-differentiable reward function. The reward is calculated using the weighted sum of the success rate and BLEU evaluation metrics. The success rate and BLEU metrics in reward calculation guide the language model for user task completion while ensuring a coherent and fluent response. Our model is acquired by fine-tuning a pre-trained model on the dialogue-session level which comprises user utterance, belief state, system act, and system response. Experimental results on MultiWOZ2.1 demonstrate that our model increases the inform rate by 1.60% and the success rate by 3.17% compared to the baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05340v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Dharmendra Prajapat, Durga Toshniwal</dc:creator>
    </item>
    <item>
      <title>Quantitative Assessment of Intersectional Empathetic Bias and Understanding</title>
      <link>https://arxiv.org/abs/2411.05777</link>
      <description>arXiv:2411.05777v1 Announce Type: cross 
Abstract: A growing amount of literature critiques the current operationalizations of empathy based on loose definitions of the construct. Such definitions negatively affect dataset quality, model robustness, and evaluation reliability. We propose an empathy evaluation framework that operationalizes empathy close to its psychological origins. The framework measures the variance in responses of LLMs to prompts using existing metrics for empathy and emotional valence. The variance is introduced through the controlled generation of the prompts by varying social biases affecting context understanding, thus impacting empathetic understanding. The control over generation ensures high theoretical validity of the constructs in the prompt dataset. Also, it makes high-quality translation, especially into languages that currently have little-to-no way of evaluating empathy or bias, such as the Slavonic family, more manageable. Using chosen LLMs and various prompt types, we demonstrate the empathy evaluation with the framework, including multiple-choice answers and free generation. The variance in our initial evaluation sample is small and we were unable to measure convincing differences between the empathetic understanding in contexts given by different social groups. However, the results are promising because the models showed significant alterations their reasoning chains needed to capture the relatively subtle changes in the prompts. This provides the basis for future research into the construction of the evaluation sample and statistical methods for measuring the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05777v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vojtech Formanek, Ondrej Sotolar</dc:creator>
    </item>
    <item>
      <title>ASL STEM Wiki: Dataset and Benchmark for Interpreting STEM Articles</title>
      <link>https://arxiv.org/abs/2411.05783</link>
      <description>arXiv:2411.05783v1 Announce Type: cross 
Abstract: Deaf and hard-of-hearing (DHH) students face significant barriers in accessing science, technology, engineering, and mathematics (STEM) education, notably due to the scarcity of STEM resources in signed languages. To help address this, we introduce ASL STEM Wiki: a parallel corpus of 254 Wikipedia articles on STEM topics in English, interpreted into over 300 hours of American Sign Language (ASL). ASL STEM Wiki is the first continuous signing dataset focused on STEM, facilitating the development of AI resources for STEM education in ASL. We identify several use cases of ASL STEM Wiki with human-centered applications. For example, because this dataset highlights the frequent use of fingerspelling for technical concepts, which inhibits DHH students' ability to learn, we develop models to identify fingerspelled words -- which can later be used to query for appropriate ASL signs to suggest to interpreters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05783v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kayo Yin, Chinmay Singh, Fyodor O. Minakov, Vanessa Milan, Hal Daum\'e III, Cyril Zhang, Alex X. Lu, Danielle Bragg</dc:creator>
    </item>
    <item>
      <title>TexSenseGAN: A User-Guided System for Optimizing Texture-Related Vibrotactile Feedback Using Generative Adversarial Network</title>
      <link>https://arxiv.org/abs/2407.11467</link>
      <description>arXiv:2407.11467v3 Announce Type: replace 
Abstract: Texture rendering has attracted significant attention as a means of creating realistic experiences in human-virtual object interactions. But in practical applications, many limited device conditions do not support the complete reproduction of spatial and temporal tactile stimuli. Different frequency components of designed vibrations can activate texture-related sensations owing to similar receptors. Therefore, we can utilize corresponding vibration signals to provide tactile feedback within the constraints of limited device environments. However, designing specific vibrations for numerous real-world materials is impractical. This study proposes a human-in-the-loop vibration generation model based on user preferences. To enable users to easily control the generation of vibration samples with large parameter spaces, we introduced an optimization model based on Differential Subspace Search (DSS) and Generative Adversarial Network (GAN). With DSS, users can employ a one-dimensional slider to easily modify the high-dimensional latent space to ensure that the GAN can generate desired vibrations. We trained the generative model using an open dataset of tactile vibration data and selected five types of vibrations as target samples for the generation experiment. Extensive user experiments were conducted using the generated and real samples. The results indicated that our system could generate distinguishable samples that matched the target characteristics. Moreover, we established a correlation between subjects' ability to distinguish real samples and their ability to distinguish generated samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11467v3</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingxin Zhang, Shun Terui, Yasutoshi Makino, Hiroyuki Shinoda</dc:creator>
    </item>
    <item>
      <title>Trusting Your AI Agent Emotionally and Cognitively: Development and Validation of a Semantic Differential Scale for AI Trust</title>
      <link>https://arxiv.org/abs/2408.05354</link>
      <description>arXiv:2408.05354v2 Announce Type: replace 
Abstract: Trust is not just a cognitive issue but also an emotional one, yet the research in human-AI interactions has primarily focused on the cognitive route of trust development. Recent work has highlighted the importance of studying affective trust towards AI, especially in the context of emerging human-like LLMs-powered conversational agents. However, there is a lack of validated and generalizable measures for the two-dimensional construct of trust in AI agents. To address this gap, we developed and validated a set of 27-item semantic differential scales for affective and cognitive trust through a scenario-based survey study. We then further validated and applied the scale through an experiment study. Our empirical findings showed how the emotional and cognitive aspects of trust interact with each other and collectively shape a person's overall trust in AI agents. Our study methodology and findings also provide insights into the capability of the state-of-art LLMs to foster trust through different routes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05354v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>AIES '24: Proceedings of the Seventh AAAI/ACM Conference on AI, Ethics, and Society (AIES-24), Pages 1343-1356, 2024</arxiv:journal_reference>
      <dc:creator>Ruoxi Shang, Gary Hsieh, Chirag Shah</dc:creator>
    </item>
    <item>
      <title>Accessible, At-Home Detection of Parkinson's Disease via Multi-task Video Analysis</title>
      <link>https://arxiv.org/abs/2406.14856</link>
      <description>arXiv:2406.14856v3 Announce Type: replace-cross 
Abstract: Limited accessibility to neurological care leads to underdiagnosed Parkinson's Disease (PD), preventing early intervention. Existing AI-based PD detection methods primarily focus on unimodal analysis of motor or speech tasks, overlooking the multifaceted nature of the disease. To address this, we introduce a large-scale, multi-task video dataset consisting of 1102 sessions (each containing videos of finger tapping, facial expression, and speech tasks captured via webcam) from 845 participants (272 with PD). We propose a novel Uncertainty-calibrated Fusion Network (UFNet) that leverages this multimodal data to enhance diagnostic accuracy. UFNet employs independent task-specific networks, trained with Monte Carlo Dropout for uncertainty quantification, followed by self-attended fusion of features, with attention weights dynamically adjusted based on task-specific uncertainties. To ensure patient-centered evaluation, the participants were randomly split into three sets: 60% for training, 20% for model selection, and 20% for final performance evaluation. UFNet significantly outperformed single-task models in terms of accuracy, area under the ROC curve (AUROC), and sensitivity while maintaining non-inferior specificity. Withholding uncertain predictions further boosted the performance, achieving 88.0+-0.3%$ accuracy, 93.0+-0.2% AUROC, 79.3+-0.9% sensitivity, and 92.6+-0.3% specificity, at the expense of not being able to predict for 2.3+-0.3% data (+- denotes 95% confidence interval). Further analysis suggests that the trained model does not exhibit any detectable bias across sex and ethnic subgroups and is most effective for individuals aged between 50 and 80. Requiring only a webcam and microphone, our approach facilitates accessible home-based PD screening, especially in regions with limited healthcare resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14856v3</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Saiful Islam, Tariq Adnan, Jan Freyberg, Sangwu Lee, Abdelrahman Abdelkader, Meghan Pawlik, Cathe Schwartz, Karen Jaffe, Ruth B. Schneider, E Ray Dorsey, Ehsan Hoque</dc:creator>
    </item>
    <item>
      <title>Empowering Agile-Based Generative Software Development through Human-AI Teamwork</title>
      <link>https://arxiv.org/abs/2407.15568</link>
      <description>arXiv:2407.15568v2 Announce Type: replace-cross 
Abstract: In software development, the raw requirements proposed by users are frequently incomplete, which impedes the complete implementation of application functionalities. With the emergence of large language models, recent methods with the top-down waterfall model employ a questioning approach for requirement completion, attempting to explore further user requirements. However, users, constrained by their domain knowledge, lack effective acceptance criteria, which fail to capture the implicit needs of the user. Moreover, the cumulative errors of the waterfall model can lead to discrepancies between the generated code and user requirements. The Agile methodologies reduce cumulative errors through lightweight iteration and collaboration with users, but the challenge lies in ensuring semantic consistency between user requirements and the code generated. We propose AgileGen, an agile-based generative software development through human-AI teamwork. AgileGen attempts for the first time to use testable requirements by Gherkin for semantic consistency between requirements and code. Additionally, we innovate in human-AI teamwork, allowing users to participate in decision-making processes they do well and enhancing the completeness of application functionality. Finally, to improve the reliability of user scenarios, a memory pool mechanism is used to collect user decision-making scenarios and recommend them to new users. AgileGen, as a user-friendly interactive system, significantly outperformed existing best methods by 16.4% and garnered higher user satisfaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15568v2</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sai Zhang, Zhenchang Xing, Ronghui Guo, Fangzhou Xu, Lei Chen, Zhaoyuan Zhang, Xiaowang Zhang, Zhiyong Feng, Zhiqiang Zhuang</dc:creator>
    </item>
    <item>
      <title>Enhancing AAC Software for Dysarthric Speakers in e-Health Settings: An Evaluation Using TORGO</title>
      <link>https://arxiv.org/abs/2411.00980</link>
      <description>arXiv:2411.00980v2 Announce Type: replace-cross 
Abstract: Individuals with cerebral palsy (CP) and amyotrophic lateral sclerosis (ALS) frequently face challenges with articulation, leading to dysarthria and resulting in atypical speech patterns. In healthcare settings, communication breakdowns reduce the quality of care. While building an augmentative and alternative communication (AAC) tool to enable fluid communication we found that state-of-the-art (SOTA) automatic speech recognition (ASR) technology like Whisper and Wav2vec2.0 marginalizes atypical speakers largely due to the lack of training data. Our work looks to leverage SOTA ASR followed by domain specific error-correction. English dysarthric ASR performance is often evaluated on the TORGO dataset. Prompt-overlap is a well-known issue with this dataset where phrases overlap between training and test speakers. Our work proposes an algorithm to break this prompt-overlap. After reducing prompt-overlap, results with SOTA ASR models produce extremely high word error rates for speakers with mild and severe dysarthria. Furthermore, to improve ASR, our work looks at the impact of n-gram language models and large-language model (LLM) based multi-modal generative error-correction algorithms like Whispering-LLaMA for a second pass ASR. Our work highlights how much more needs to be done to improve ASR for atypical speakers to enable equitable healthcare access both in-person and in e-health settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00980v2</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Macarious Hui, Jinda Zhang, Aanchan Mohan</dc:creator>
    </item>
  </channel>
</rss>
