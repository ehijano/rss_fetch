<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Dec 2025 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>ASCHOPLEX encounters Dafne: a federated continuous learning project for the generalizability of the Choroid Plexus automatic segmentation</title>
      <link>https://arxiv.org/abs/2512.20741</link>
      <description>arXiv:2512.20741v1 Announce Type: new 
Abstract: The Choroid Plexus (ChP) is a highly vascularized brain structure that plays a critical role in several physiological processes. ASCHOPLEX, a deep learning-based segmentation toolbox with an integrated fine-tuning stage, provides accurate ChP delineations on non-contrast-enhanced T1-weighted MRI scans; however, its performance is hindered by inter-dataset variability. This study introduces the first federated incremental learning approach for automated ChP segmentation from 3D T1-weighted brain MRI, by integrating an enhanced version of ASCHOPLEX within the Dafne (Deep Anatomical Federated Network) framework. A comparative evaluation is conducted to assess whether federated incremental learning through Dafne improves model generalizability across heterogeneous imaging conditions, relative to the conventional fine-tuning strategy employed by standalone ASCHOPLEX. The experimental cohort comprises 2,284 subjects, including individuals with Multiple Sclerosis as well as healthy controls, collected from five independent MRI datasets. Results indicate that the fine-tuning strategy provides high performance on homogeneous data (e.g., same MRI sequence, same cohort of subjects), but limited generalizability when the data variability is high (e.g., multiple MRI sequences, multiple and new cohorts of subjects). By contrast, the federated incremental learning variant of ASCHOPLEX constitutes a robust alternative consistently achieving higher generalizability and more stable performance across diverse acquisition settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20741v1</guid>
      <category>eess.IV</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valentina Visani, Marco Pinamonti, Valentina Sammassimo, Manuela Moretto, Mattia Veronese, Agnese Tamanti, Francesca Benedetta Pizzini, Massimiliano Calabrese, Marco Castellaro, Francesco Santini</dc:creator>
    </item>
    <item>
      <title>Leveraging Overfitting for Low-Complexity and Modality-Agnostic Joint Source-Channel Coding</title>
      <link>https://arxiv.org/abs/2512.20981</link>
      <description>arXiv:2512.20981v1 Announce Type: new 
Abstract: This paper introduces Implicit-JSCC, a novel overfitted joint source-channel coding paradigm that directly optimizes channel symbols and a lightweight neural decoder for each source. This instance-specific strategy eliminates the need for training datasets or pre-trained models, enabling a storage-free, modality-agnostic solution. As a low-complexity alternative, Implicit-JSCC achieves efficient image transmission with around 1000x lower decoding complexity, using as few as 607 model parameters and 641 multiplications per pixel. This overfitted design inherently addresses source generalizability and achieves state-of-the-art results in the high SNR regimes, underscoring its promise for future communication systems, especially streaming scenarios where one-time offline encoding supports multiple online decoding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20981v1</guid>
      <category>eess.IV</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haotian Wu, Gen Li, Pier Luigi Dragotti, Deniz G\"und\"uz</dc:creator>
    </item>
    <item>
      <title>The Area Signal-to-Noise Ratio: A Robust Alternative to Peak-Based SNR in Spectroscopic Analysis</title>
      <link>https://arxiv.org/abs/2512.20830</link>
      <description>arXiv:2512.20830v1 Announce Type: cross 
Abstract: In spectroscopic analysis, the peak-based signal-to-noise ratio (pSNR) is commonly used but suffers from limitations such as sensitivity to noise spikes and reduced effectiveness for broader peaks. We introduce the area-based signal-to-noise ratio (aSNR) as a robust alternative that integrates the signal over a defined region of interest, reducing noise variance and improving detection for various lineshapes. We used Monte Carlo simulations (n=2,000 trials per condition) to test aSNR on Gaussian, Lorentzian, and Voigt lineshapes. We found that aSNR requires significantly lower amplitudes than pSNR to achieve a 50% detection probability. Receiver operating characteristic (ROC) curves show that aSNR performs better than pSNR at low amplitudes. Our results show that aSNR works especially advantageously for broad peaks and could be extended to volume-based SNR for multidimensional spectra.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20830v1</guid>
      <category>eess.SP</category>
      <category>eess.IV</category>
      <category>stat.AP</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Yu, Huaqing Zhao, Lin Z. Li</dc:creator>
    </item>
    <item>
      <title>NeRV360: Neural Representation for 360-Degree Videos with a Viewport Decoder</title>
      <link>https://arxiv.org/abs/2512.20871</link>
      <description>arXiv:2512.20871v1 Announce Type: cross 
Abstract: Implicit neural representations for videos (NeRV) have shown strong potential for video compression. However, applying NeRV to high-resolution 360-degree videos causes high memory usage and slow decoding, making real-time applications impractical. We propose NeRV360, an end-to-end framework that decodes only the user-selected viewport instead of reconstructing the entire panoramic frame. Unlike conventional pipelines, NeRV360 integrates viewport extraction into decoding and introduces a spatial-temporal affine transform module for conditional decoding based on viewpoint and time. Experiments on 6K-resolution videos show that NeRV360 achieves a 7-fold reduction in memory consumption and a 2.5-fold increase in decoding speed compared to HNeRV, a representative prior work, while delivering better image quality in terms of objective metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20871v1</guid>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <category>eess.IV</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daichi Arai, Kyohei Unno, Yasuko Sugito, Yuichi Kusakabe</dc:creator>
    </item>
    <item>
      <title>AirGS: Real-Time 4D Gaussian Streaming for Free-Viewpoint Video Experiences</title>
      <link>https://arxiv.org/abs/2512.20943</link>
      <description>arXiv:2512.20943v1 Announce Type: cross 
Abstract: Free-viewpoint video (FVV) enables immersive viewing experiences by allowing users to view scenes from arbitrary perspectives. As a prominent reconstruction technique for FVV generation, 4D Gaussian Splatting (4DGS) models dynamic scenes with time-varying 3D Gaussian ellipsoids and achieves high-quality rendering via fast rasterization. However, existing 4DGS approaches suffer from quality degradation over long sequences and impose substantial bandwidth and storage overhead, limiting their applicability in real-time and wide-scale deployments. Therefore, we present AirGS, a streaming-optimized 4DGS framework that rearchitects the training and delivery pipeline to enable high-quality, low-latency FVV experiences. AirGS converts Gaussian video streams into multi-channel 2D formats and intelligently identifies keyframes to enhance frame reconstruction quality. It further combines temporal coherence with inflation loss to reduce training time and representation size. To support communication-efficient transmission, AirGS models 4DGS delivery as an integer linear programming problem and design a lightweight pruning level selection algorithm to adaptively prune the Gaussian updates to be transmitted, balancing reconstruction quality and bandwidth consumption. Extensive experiments demonstrate that AirGS reduces quality deviation in PSNR by more than 20% when scene changes, maintains frame-level PSNR consistently above 30, accelerates training by 6 times, reduces per-frame transmission size by nearly 50% compared to the SOTA 4DGS approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20943v1</guid>
      <category>cs.GR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.MM</category>
      <category>cs.NI</category>
      <category>eess.IV</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhe Wang, Jinghang Li, Yifei Zhu</dc:creator>
    </item>
    <item>
      <title>Equitable non-contact infrared thermography after solar loading using deep learning</title>
      <link>https://arxiv.org/abs/2304.08832</link>
      <description>arXiv:2304.08832v3 Announce Type: replace 
Abstract: Widely deployed for fever detection, infrared thermometers (IRTs) enable rapid non-contact measurement of core body temperature but are inaccurate in unconstrained environments when skin temperature is transient. In this work, we present the first study on the effect of solar loading--solar radiation-induced elevation of skin but not core temperature--on IRT performance. Solar loading causes poor specificity in IRT fever detection, and the standard procedure is to reacclimate subjects for up to 30 minutes before IRT measurement. In contrast, we propose a single-shot deep learning model that removes solar loading transients from thermal facial images, allowing accurate IRT operation in solar loaded conditions. Forehead skin temperature increases by 2.00{\deg}C after solar loading, and our deep learning model, SL-Net, reduces this error by 68\% to 0.64{\deg}C. We show that the solar loading effect depends on skin tone, introducing inequity in IRT performance, while SL-Net is unbiased. We open source a diverse dataset of 100 subjects with co-registered RGB-thermal images, and IRT and skin tone measurements. Our work shows that it is possible to use machine learning to correct complex thermal perturbations to enable robust and equitable human thermography.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.08832v3</guid>
      <category>eess.IV</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ellin Q. Zhao, Alexander Vilesov, Pradyumna Chari, Laleh Jalilian, Achuta Kadambi</dc:creator>
    </item>
    <item>
      <title>A European Multi-Center Breast Cancer MRI Dataset</title>
      <link>https://arxiv.org/abs/2506.00474</link>
      <description>arXiv:2506.00474v2 Announce Type: replace 
Abstract: Early detection of breast cancer is critical for improving patient outcomes. While mammography remains the primary screening modality, magnetic resonance imaging (MRI) is increasingly recommended as a supplemental tool for women with dense breast tissue and those at elevated risk. However, the acquisition and interpretation of multiparametric breast MRI are time-consuming and require specialized expertise, limiting scalability in clinical practice. Artificial intelligence (AI) methods have shown promise in supporting breast MRI interpretation, but their development is hindered by the limited availability of large, diverse, and publicly accessible datasets. To address this gap, we present a publicly available, multi-center breast MRI dataset collected across six clinical institutions in five European countries. The dataset comprises 741 examinations from women undergoing screening or diagnostic breast MRI and includes malignant, benign, and non-lesion cases. Data were acquired using heterogeneous scanners, field strengths, and acquisition protocols, reflecting real-world clinical variability. In addition, we report baseline benchmark experiments using a transformer-based model to illustrate potential use cases of the dataset and to provide reference performance for future methodological comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00474v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gustav M\"uller-Franzes, Lorena Escudero S\'anchez, Nicholas Payne, Alexandra Athanasiou, Michael Kalogeropoulos, Aitor Lopez, Alfredo Miguel Soro Busto, Julia Camps Herrero, Nika Rasoolzadeh, Tianyu Zhang, Ritse Mann, Debora Jutz, Maike Bode, Christiane Kuhl, Yuan Gao, Wouter Veldhuis, Oliver Lester Saldanha, JieFu Zhu, Jakob Nikolas Kather, Daniel Truhn, Fiona J. Gilbert</dc:creator>
    </item>
    <item>
      <title>V-Rex: Real-Time Streaming Video LLM Acceleration via Dynamic KV Cache Retrieval</title>
      <link>https://arxiv.org/abs/2512.12284</link>
      <description>arXiv:2512.12284v3 Announce Type: replace 
Abstract: Streaming video large language models (LLMs) are increasingly used for real-time multimodal tasks such as video captioning, question answering, conversational agents, and augmented reality. However, these models face fundamental memory and computational challenges because their key-value (KV) caches grow substantially with continuous streaming video input. This process requires an iterative prefill stage, which is a unique feature of streaming video LLMs. Due to its iterative prefill stage, it suffers from significant limitations, including extensive computation, substantial data transfer, and degradation in accuracy. Crucially, this issue is exacerbated for edge deployment, which is the primary target for these models.
  In this work, we propose V-Rex, the first software-hardware co-designed accelerator that comprehensively addresses both algorithmic and hardware bottlenecks in streaming video LLM inference. At its core, V-Rex introduces ReSV, a training-free dynamic KV cache retrieval algorithm. ReSV exploits temporal and spatial similarity-based token clustering to reduce excessive KV cache memory across video frames. To fully realize these algorithmic benefits, V-Rex offers a compact, low-latency hardware accelerator with a dynamic KV cache retrieval engine (DRE), featuring bit-level and early-exit based computing units. V-Rex achieves unprecedented real-time of 3.9-8.3 FPS and energy-efficient streaming video LLM inference on edge deployment with negligible accuracy loss. While DRE only accounts for 2.2% power and 2.0% area, the system delivers 1.9-19.7x speedup and 3.1-18.5x energy efficiency improvements over AGX Orin GPU. This work is the first to comprehensively tackle KV cache retrieval across algorithms and hardware, enabling real-time streaming video LLM inference on resource-constrained edge devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12284v3</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Donghyuk Kim, Sejeong Yang, Wonjin Shin, Joo-Young Kim</dc:creator>
    </item>
    <item>
      <title>High contrast holography through dual modulation</title>
      <link>https://arxiv.org/abs/2410.19347</link>
      <description>arXiv:2410.19347v2 Announce Type: replace-cross 
Abstract: Holographic displays are a promising technology for immersive visual experiences, and their potential for compact form factor makes them a strong candidate for head-mounted displays. However, at the short propagation distances needed for a compact, head-mounted architecture, image contrast is low when using a traditional phase-only spatial light modulator (SLM). Although a complex SLM could restore contrast, these modulators require bulky lenses to optically co-locate the amplitude and phase components, making them poorly suited for a compact head-mounted design. In this work, we introduce a novel architecture to improve contrast: by adding a low resolution amplitude SLM a short distance away from the phase modulator, we demonstrate peak signal-to-noise ratio improvement up to 31 dB in simulation and 6.5 dB experimentally compared to phase-only modulation, even when the amplitude modulator is 60$\times$ lower resolution than its phase counterpart. We analyze the relationship between diffraction angle and amplitude modulator pixel size, and validate the concept with a benchtop experimental prototype. By showing that low resolution modulation is sufficient to improve contrast, we open new design spaces for high-contrast holographic displays.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19347v2</guid>
      <category>physics.optics</category>
      <category>cs.GR</category>
      <category>eess.IV</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s41598-025-00459-8</arxiv:DOI>
      <arxiv:journal_reference>Nature Scientific Reports 15, 17615 (2025)</arxiv:journal_reference>
      <dc:creator>Leyla Kabuli, Oliver Cossairt, Florian Schiffers, Nathan Matsuda, Grace Kuo</dc:creator>
    </item>
    <item>
      <title>A Multicore and Edge TPU-Accelerated Multimodal TinyML System for Livestock Behavior Recognition</title>
      <link>https://arxiv.org/abs/2504.11467</link>
      <description>arXiv:2504.11467v3 Announce Type: replace-cross 
Abstract: The advancement of technology has revolutionized the agricultural industry, transitioning it from labor-intensive farming practices to automated, AI-powered management systems. In recent years, more intelligent livestock monitoring solutions have been proposed to enhance farming efficiency and productivity. This work presents a novel approach to animal activity recognition and movement tracking, leveraging tiny machine learning (TinyML) techniques, wireless communication framework, and microcontroller platforms to develop an efficient, cost-effective livestock sensing system. It collects and fuses accelerometer data and vision inputs to build a multimodal network for three tasks: image classification, object detection, and behavior recognition. The system is deployed and evaluated on commercial microcontrollers for real-time inference using embedded applications, demonstrating up to 270$\times$ model size reduction, less than 80ms response latency, and on-par performance comparable to existing methods. The incorporation of the wireless communication technique allows for seamless data transmission between devices, benefiting use cases in remote locations with poor Internet connectivity. This work delivers a robust, scalable IoT-edge livestock monitoring solution adaptable to diverse farming needs, offering flexibility for future extensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11467v3</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Thu, 25 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JIOT.2025.3624811</arxiv:DOI>
      <arxiv:journal_reference>IEEE Internet of Things Journal, vol. 13, no. 1, pp. 666-677, 1 Jan.1, 2026</arxiv:journal_reference>
      <dc:creator>Qianxue Zhang, Eiman Kanjo</dc:creator>
    </item>
  </channel>
</rss>
