<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 May 2024 04:00:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 08 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Randomized iterative methods for generalized absolute value equations: Solvability and error bounds</title>
      <link>https://arxiv.org/abs/2405.04091</link>
      <description>arXiv:2405.04091v1 Announce Type: new 
Abstract: Randomized iterative methods, such as the Kaczmarz method and its variants, have gained growing attention due to their simplicity and efficiency in solving large-scale linear systems. Meanwhile, absolute value equations (AVE) have attracted increasing interest due to their connection with the linear complementarity problem. In this paper, we investigate the application of randomized iterative methods to generalized AVE (GAVE). Our approach differs from most existing works in that we tackle GAVE with non-square coefficient matrices. We establish more comprehensive sufficient and necessary conditions for characterizing the solvability of GAVE and propose precise error bound conditions. Furthermore, we introduce a flexible and efficient randomized iterative algorithmic framework for solving GAVE, which employs sampling matrices drawn from user-specified distributions. This framework is capable of encompassing many well-known methods, including the Picard iteration method and the randomized Kaczmarz method. Leveraging our findings on solvability and error bounds, we establish both almost sure convergence and linear convergence rates for this versatile algorithmic framework. Finally, we present numerical examples to illustrate the advantages of the new algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04091v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaxin Xie, Huoduo Qi, Deren Han</dc:creator>
    </item>
    <item>
      <title>Semi-implicit Lagrangian Voronoi Approximation for the incompressible Navier-Stokes equations</title>
      <link>https://arxiv.org/abs/2405.04116</link>
      <description>arXiv:2405.04116v1 Announce Type: new 
Abstract: We introduce Semi-Implicit Lagrangian Voronoi Approximation (SILVA), a novel numerical method for the solution of the incompressible Euler and Navier-Stokes equations, which combines the efficiency of semi-implicit time marching schemes with the robustness of time-dependent Voronoi tessellations. In SILVA, the numerical solution is stored at particles, which move with the fluid velocity and also play the role of the generators of the computational mesh. The Voronoi mesh is rapidly regenerated at each time step, allowing large deformations with topology changes. As opposed to the reconnection-based Arbitrary-Lagrangian-Eulerian schemes, we need no remapping stage. A semi-implicit scheme is devised in the context of moving Voronoi meshes to project the velocity field onto a divergence-free manifold. We validate SILVA by illustrative benchmarks, including viscous, inviscid, and multi-phase flows. Compared to its closest competitor, the Incompressible Smoothed Particle Hydrodynamics (ISPH) method, SILVA offers a sparser stiffness matrix and facilitates the implementation of no-slip and free-slip boundary conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04116v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ond\v{r}ej Kincl, Ilya Peshkov, Walter Boscheri</dc:creator>
    </item>
    <item>
      <title>Rational methods for abstract linear, non-homogeneous problems without order reduction</title>
      <link>https://arxiv.org/abs/2405.04195</link>
      <description>arXiv:2405.04195v1 Announce Type: new 
Abstract: Starting from an A-stable rational approximation to $\rm{e}^z$ of order $p$, $$r(z)= 1+ z+ \cdots + z^p/ p! + O(z^{p+1}),$$ families of stable methods are proposed to time discretize abstract IVP's of the type $u'(t) = A u(t) + f(t)$. These numerical procedures turn out to be of order $p$, thus overcoming the order reduction phenomenon, and only one evaluation of $f$ per step is required.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04195v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Arranz Sim\'on, Cesar Palencia</dc:creator>
    </item>
    <item>
      <title>On the Gelfand Problem and Viscosity Matrices for Two-Dimensional Hyperbolic Systems of Conservation Laws</title>
      <link>https://arxiv.org/abs/2405.04214</link>
      <description>arXiv:2405.04214v1 Announce Type: new 
Abstract: We present counter-intuitive examples of a viscous regularizations of a two-dimensional strictly hyperbolic system of conservation laws. The regularizations are obtained using two different viscosity matrices. While for both of the constructed ``viscous'' systems waves propagating in either $x$- or $y$-directions are stable, oblique waves may be linearly unstable. Numerical simulations fully corroborate these analytical results. To the best of our knowledge, this is the first nontrivial result related to the multidimensional Gelfand problem. Our conjectures provide direct answer to Gelfand's problem both in one- and multi-dimensional cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04214v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaoshuai Chu, Igor Kliakhandler, Alexander Kurganov</dc:creator>
    </item>
    <item>
      <title>How to reveal the rank of a matrix?</title>
      <link>https://arxiv.org/abs/2405.04330</link>
      <description>arXiv:2405.04330v1 Announce Type: new 
Abstract: We study algorithms called rank-revealers that reveal a matrix's rank structure. Such algorithms form a fundamental component in matrix compression, singular value estimation, and column subset selection problems. While column-pivoted QR has been widely adopted due to its practicality, it is not always a rank-revealer. Conversely, Gaussian elimination (GE) with a pivoting strategy known as global maximum volume pivoting is guaranteed to estimate a matrix's singular values but its exponential algorithmic complexity limits its interest to theory. We show that the concept of local maximum volume pivoting is a crucial and practical pivoting strategy for rank-revealers based on GE and QR, showing that it is both necessary and sufficient. This insight elevates Gu and Eisenstat's rank-revealing QR as an archetypal rank-revealer, and devise a version that is less than $2\times$ more computationally expensive than CPQR. We unify the landscape of rank-revealers by considering GE and QR together and prove that the success of any pivoting strategy can be assessed by benchmarking it against a local maximum volume pivot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04330v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anil Damle, Silke Glas, Alex Townsend, Annan Yu</dc:creator>
    </item>
    <item>
      <title>Development of discontinuous Galerkin methods for hyperbolic systems that preserve a curl or a divergence constraint</title>
      <link>https://arxiv.org/abs/2405.04347</link>
      <description>arXiv:2405.04347v1 Announce Type: new 
Abstract: Some hyperbolic systems are known to include implicit preservation of differential constraints: these are for example the time conservation of the curl or the divergence of a vector that appear as an implicit constraint. In this article, we show that this kind of constraint can be easily conserved at the discrete level with the classical discontinuous Galerkin method, provided the right approximation space is used for the vectorial space, and under some mild assumption on the numerical flux. For this, we develop a discrete differential geometry framework for some well chosen piece-wise polynomial vector approximation space. More precisely, we define the discrete Hodge star operator, the exterior derivative, and their adjoints. The discrete adjoint divergence and curl are proven to be exactly preserved by the discontinuous Galerkin method under a small assumption on the numerical flux. Numerical tests are performed on the wave system, the two dimensional Maxwell system and the induction equation, and confirm that the differential constraints are preserved at machine precision while keeping the high order of accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04347v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Perrier (CAGIRE, LMAP)</dc:creator>
    </item>
    <item>
      <title>Solving ill-conditioned linear algebraic systems using methods that improve conditioning</title>
      <link>https://arxiv.org/abs/2405.04399</link>
      <description>arXiv:2405.04399v1 Announce Type: new 
Abstract: We consider the solution of systems of linear algebraic equations (SLAEs) with an ill- conditioned or degenerate exact matrix and an approximate right-hand side. An approach to solving such a problem is proposed and justified, which makes it possible to improve the conditionality of the SLAE matrix and, as a result, obtain an approximate solution that is stable to perturbations of the right hand side with higher accuracy than using other methods. The approach is implemented by an algorithm that uses so-called minimal pseudoinverse matrices. The results of numerical experiments are presented that confirm the theoretical provisions of the article.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04399v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. S. Leonov</dc:creator>
    </item>
    <item>
      <title>Learning local Dirichlet-to-Neumann maps of nonlinear elliptic PDEs with rough coefficients</title>
      <link>https://arxiv.org/abs/2405.04433</link>
      <description>arXiv:2405.04433v1 Announce Type: new 
Abstract: Partial differential equations (PDEs) involving high contrast and oscillating coefficients are common in scientific and industrial applications. Numerical approximation of these PDEs is a challenging task that can be addressed, for example, by multi-scale finite element analysis. For linear problems, multi-scale finite element method (MsFEM) is well established and some viable extensions to non-linear PDEs are known. However, some features of the method seem to be intrinsically based on linearity-based. In particular, traditional MsFEM rely on the reuse of computations. For example, the stiffness matrix can be calculated just once, while being used for several right-hand sides, or as part of a multi-level iterative algorithm. Roughly speaking, the offline phase of the method amounts to pre-assembling the local linear Dirichlet-to-Neumann (DtN) operators. We present some preliminary results concerning the combination of MsFEM with machine learning tools. The extension of MsFEM to nonlinear problems is achieved by means of learning local nonlinear DtN maps. The resulting learning-based multi-scale method is tested on a set of model nonlinear PDEs involving the $p-$Laplacian and degenerate nonlinear diffusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04433v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miranda Boutilier, Konstantin Brenner, Larissa Miguez</dc:creator>
    </item>
    <item>
      <title>Multiparameter regularization and aggregation in the context of polynomial functional regression</title>
      <link>https://arxiv.org/abs/2405.04147</link>
      <description>arXiv:2405.04147v1 Announce Type: cross 
Abstract: Most of the recent results in polynomial functional regression have been focused on an in-depth exploration of single-parameter regularization schemes. In contrast, in this study we go beyond that framework by introducing an algorithm for multiple parameter regularization and presenting a theoretically grounded method for dealing with the associated parameters. This method facilitates the aggregation of models with varying regularization parameters. The efficacy of the proposed approach is assessed through evaluations on both synthetic and some real-world medical data, revealing promising results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04147v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elke R. Gizewski, Markus Holzleitner, Lukas Mayer-Suess, Sergiy Pereverzyev Jr., Sergei V. Pereverzyev</dc:creator>
    </item>
    <item>
      <title>Preserving Nonlinear Constraints in Variational Flow Filtering Data Assimilation</title>
      <link>https://arxiv.org/abs/2405.04380</link>
      <description>arXiv:2405.04380v1 Announce Type: cross 
Abstract: Data assimilation aims to estimate the states of a dynamical system by optimally combining sparse and noisy observations of the physical system with uncertain forecasts produced by a computational model. The states of many dynamical systems of interest obey nonlinear physical constraints, and the corresponding dynamics is confined to a certain sub-manifold of the state space. Standard data assimilation techniques applied to such systems yield posterior states lying outside the manifold, violating the physical constraints. This work focuses on particle flow filters which use stochastic differential equations to evolve state samples from a prior distribution to samples from an observation-informed posterior distribution. The variational Fokker-Planck (VFP) -- a generic particle flow filtering framework -- is extended to incorporate non-linear, equality state constraints in the analysis. To this end, two algorithmic approaches that modify the VFP stochastic differential equation are discussed: (i) VFPSTAB, to inexactly preserve constraints with the addition of a stabilizing drift term, and (ii) VFPDAE, to exactly preserve constraints by treating the VFP dynamics as a stochastic differential-algebraic equation (SDAE). Additionally, an implicit-explicit time integrator is developed to evolve the VFPDAE dynamics. The strength of the proposed approach for constraint preservation in data assimilation is demonstrated on three test problems: the double pendulum, Korteweg-de-Vries, and the incompressible Navier-Stokes equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04380v1</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amit N. Subrahmanya, Andrey A. Popov, Reid J. Gomillion, Adrian Sandu</dc:creator>
    </item>
    <item>
      <title>Generalized prolate spheroidal functions: algorithms and analysis</title>
      <link>https://arxiv.org/abs/1811.02733</link>
      <description>arXiv:1811.02733v3 Announce Type: replace 
Abstract: Generalized prolate spheroidal functions (GPSFs) arise naturally in the study of bandlimited functions as the eigenfunctions of a certain truncated Fourier transform. In one dimension, the theory of GPSFs (typically referred to as prolate spheroidal wave functions) has a long history and is fairly complete. Furthermore, more recent work has led to the development of numerical algorithms for their computation and use in applications. In this paper we consider the more general problem, extending the one dimensional analysis and algorithms to the case of arbitrary dimension. Specifically, we introduce algorithms for efficient evaluation of GPSFs and their corresponding eigenvalues, quadrature rules for bandlimited functions, formulae for interpolation via GPSF expansion, and various analytical properties of GPSFs. We illustrate the numerical and analytical results with several numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:1811.02733v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philip Greengard</dc:creator>
    </item>
    <item>
      <title>The weak Galerkin finite element method for Stokes interface problems with curved interface</title>
      <link>https://arxiv.org/abs/2211.11926</link>
      <description>arXiv:2211.11926v3 Announce Type: replace 
Abstract: In this paper, we develop a new weak Galerkin finite element scheme for the Stokes interface problem with curved interfaces. We take a unique vector-valued function at the interface and reflect the interface condition in the variational problem. Theoretical analysis and numerical experiments show that the errors can reach the optimal convergence order under the energy norm and $L^2$ norm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.11926v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lin Yang, Qilong Zhai, Ran Zhang</dc:creator>
    </item>
    <item>
      <title>Convergence of the Fourier-Galerkin spectral method for the Boltzmann equation with uncertainties</title>
      <link>https://arxiv.org/abs/2212.04083</link>
      <description>arXiv:2212.04083v4 Announce Type: replace 
Abstract: It is well-known that the Fourier-Galerkin spectral method has been a popular approach for the numerical approximation of the deterministic Boltzmann equation with spectral accuracy rigorously proved. In this paper, we will show that such a spectral convergence of the Fourier-Galerkin spectral method also holds for the Boltzmann equation with uncertainties arising from both collision kernel and initial condition. Our proof is based on newly-established spaces and norms that are carefully designed and take the velocity variable and random variables with their high regularities into account altogether. For future studies, this theoretical result will provide a solid foundation for further showing the convergence of the full-discretized system where both the velocity and random variables are discretized simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.04083v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liu Liu, Kunlun Qi</dc:creator>
    </item>
    <item>
      <title>Double-loop randomized quasi-Monte Carlo estimator for nested integration</title>
      <link>https://arxiv.org/abs/2302.14119</link>
      <description>arXiv:2302.14119v4 Announce Type: replace 
Abstract: Characterized by an outer integral connected to an inner integral through a nonlinear function, nested integration is a challenging problem in various fields, such as engineering and mathematical finance. The available numerical methods for nested integration based on Monte Carlo (MC) methods can be prohibitively expensive owing to the error propagating from the inner to the outer integral. Attempts to enhance the efficiency of these approximations using the quasi-MC (QMC) or randomized QMC (rQMC) method have focused on either the inner or outer integral approximation. This work introduces a novel nested rQMC method that simultaneously addresses the approximation of the inner and outer integrals. This method leverages the unique nested integral structure to offer a more efficient approximation mechanism. By incorporating Owen's scrambling techniques, we address integrands exhibiting infinite variation in the Hardy--Krause sense, enabling theoretically sound error estimates. As the primary contribution, we derive asymptotic error bounds for the bias and variance of our estimator, along with the regularity conditions under which these bounds can be attained. In addition, we provide nearly optimal sample sizes for the rQMC approximations underlying the numerical implementation of the proposed method. Moreover, we indicate how to combine this method with importance sampling to remedy the measure concentration arising in the inner integral. We verify the estimator quality through numerical experiments in the context of expected information gain estimation. We compare the computational efficiency of the nested rQMC method against standard nested MC integration for two case studies: one in thermomechanics and the other in pharmacokinetics. These examples highlight the computational savings and enhanced applicability of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.14119v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arved Bartuska, Andr\'e Gustavo Carlon, Luis Espath, Sebastian Krumscheid, Ra\'ul Tempone</dc:creator>
    </item>
    <item>
      <title>Fast and high-order approximation of parabolic equations using hierarchical direct solvers and implicit Runge-Kutta methods</title>
      <link>https://arxiv.org/abs/2306.02526</link>
      <description>arXiv:2306.02526v2 Announce Type: replace 
Abstract: An additive Runge-Kutta method is used for the time stepping, which integrates the linear stiff terms by an explicit singly diagonally implicit Runge-Kutta (ESDIRK) method and the nonlinear terms by an explicit Runge-Kutta (ERK) method. In each time step, the implicit solve is performed by the recently developed Hierarchical Poincar\'e-Steklov (HPS) method. This is a fast direct solver for elliptic equations that decomposes the space domain into a hierarchical tree of subdomains and builds spectral collocation solvers locally on the subdomains. These ideas are naturally combined in the presented method since the singly diagonal coefficient in ESDIRK and a fixed time-step ensures that the coefficient matrix in the implicit solve of HPS remains the same for all time stages. This means that the precomputed inverse can be efficiently reused, leading to a scheme with complexity (in two dimensions) $\mathcal{O}(N^{1.5})$ for the precomputation where the solution operator to the elliptic problems is built, and then $\mathcal{O}(N \log N)$ for the solve in each time step. The stability of the method is proved for first order in time and any order in space, and numerical evidence substantiates a claim of stability for a much broader class of time discretization methods. Numerical experiments supporting the accuracy of efficiency of the method in one and two dimensions are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.02526v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ke Chen, Daniel Appel\"o, Tracy Babb, Per-Gunnar Martinsson</dc:creator>
    </item>
    <item>
      <title>The effect of approximate coarsest-level solves on the convergence of multigrid V-cycle methods</title>
      <link>https://arxiv.org/abs/2306.06182</link>
      <description>arXiv:2306.06182v3 Announce Type: replace 
Abstract: The multigrid V-cycle method is a popular method for solving systems of linear equations. It computes an approximate solution by using smoothing on fine levels and solving a system of linear equations on the coarsest level. Solving on the coarsest level depends on the size and difficulty of the problem. If the size permits, it is typical to use a direct method based on LU or Cholesky decomposition. In settings with large coarsest-level problems, approximate solvers such as iterative Krylov subspace methods, or direct methods based on low-rank approximation, are often used. The accuracy of the coarsest-level solver is typically determined based on the experience of the users with the concrete problems and methods.
  In this paper we present an approach to analyzing the effects of approximate coarsest-level solves on the convergence of the V-cycle method for symmetric positive definite problems. Using these results, we derive coarsest-level stopping criterion through which we may control the difference between the approximation computed by a V-cycle method with approximate coarsest-level solver and the approximation which would be computed if the coarsest-level problems were solved exactly. The coarsest-level stopping criterion may thus be set up such that the V-cycle method converges to a chosen finest-level accuracy in (nearly) the same number of V-cycle iterations as the V-cycle method with exact coarsest-level solver. We also utilize the theoretical results to discuss how the convergence of the V-cycle method may be affected by the choice of a tolerance in a coarsest-level stopping criterion based on the relative residual norm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06182v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Petr Vacek, Erin Carson, Kirk M. Soodhalter</dc:creator>
    </item>
    <item>
      <title>Match-based solution of general parametric eigenvalue problems</title>
      <link>https://arxiv.org/abs/2308.05335</link>
      <description>arXiv:2308.05335v3 Announce Type: replace 
Abstract: We describe a novel algorithm for solving general parametric (nonlinear) eigenvalue problems. Our method has two steps: first, high-accuracy solutions of non-parametric versions of the problem are gathered at some values of the parameters; these are then combined to obtain global approximations of the parametric eigenvalues. To gather the non-parametric data, we use non-intrusive contour-integration-based methods, which, however, cannot track eigenvalues that migrate into/out of the contour as the parameter changes. Special strategies are described for performing the combination-over-parameter step despite having only partial information on such migrating eigenvalues. Moreover, we dedicate a special focus to the approximation of eigenvalues that undergo bifurcations. Finally, we propose an adaptive strategy that allows one to effectively apply our method even without any a priori information on the behavior of the sought-after eigenvalues. Numerical tests are performed, showing that our algorithm can achieve remarkably high approximation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05335v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Davide Pradovera, Alessandro Borghi</dc:creator>
    </item>
    <item>
      <title>On the accuracy of interpolation based on single-layer artificial neural networks with a focus on defeating the Runge phenomenon</title>
      <link>https://arxiv.org/abs/2308.10720</link>
      <description>arXiv:2308.10720v2 Announce Type: replace 
Abstract: In the present paper, we consider one-hidden layer ANNs with a feedforward architecture, also referred to as shallow or two-layer networks, so that the structure is determined by the number and types of neurons. The determination of the parameters that define the function, called training, is done via the resolution of the approximation problem, so by imposing the interpolation through a set of specific nodes. We present the case where the parameters are trained using a procedure that is referred to as Extreme Learning Machine (ELM) that leads to a linear interpolation problem. In such hypotheses, the existence of an ANN interpolating function is guaranteed. The focus is then on the accuracy of the interpolation outside of the given sampling interpolation nodes when they are the equispaced, the Chebychev, and the randomly selected ones. The study is motivated by the well-known bell-shaped Runge example, which makes it clear that the construction of a global interpolating polynomial is accurate only if trained on suitably chosen nodes, ad example the Chebychev ones. In order to evaluate the behavior when growing the number of interpolation nodes, we raise the number of neurons in our network and compare it with the interpolating polynomial. We test using Runge's function and other well-known examples with different regularities. As expected, the accuracy of the approximation with a global polynomial increases only if the Chebychev nodes are considered. Instead, the error for the ANN interpolating function always decays and in most cases we observe that the convergence follows what is observed in the polynomial case on Chebychev nodes, despite the set of nodes used for training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.10720v2</guid>
      <category>math.NA</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ferdinando Auricchio, Maria Roberta Belardo, Gianluca Fabiani, Francesco Calabr\`o, Ariel F. Pascaner</dc:creator>
    </item>
    <item>
      <title>Second-order exponential splittings in the presence of unbounded and time-dependent operators: construction and convergence</title>
      <link>https://arxiv.org/abs/2310.01556</link>
      <description>arXiv:2310.01556v3 Announce Type: replace 
Abstract: For linear differential equations of the form $u'(t)=[A + B(t)] u(t)$, $t\geq0$, with a possibly unbounded operator $A$, we construct and deduce error bounds for two families of second-order exponential splittings. The role of quadratures when integrating the twice-iterated Duhamel's formula is reformulated: we show that their choice defines the structure of the splitting. Furthermore, the reformulation allows us to consider quadratures based on the Birkhoff interpolation to obtain not only pure-stages splittings but also those containing derivatives of $B(t)$ and commutators of $A$ and $B(t)$. In this approach, the construction and error analysis of the splittings are carried out simultaneously. We discuss the accuracy of the members of the families. Numerical experiments are presented to complement the theoretical consideration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01556v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karolina Kropielnicka, Juan Carlos del Valle</dc:creator>
    </item>
    <item>
      <title>On regularized polynomial functional regression</title>
      <link>https://arxiv.org/abs/2311.03036</link>
      <description>arXiv:2311.03036v2 Announce Type: replace 
Abstract: This article offers a comprehensive treatment of polynomial functional regression, culminating in the establishment of a novel finite sample bound. This bound encompasses various aspects, including general smoothness conditions, capacity conditions, and regularization techniques. In doing so, it extends and generalizes several findings from the context of linear functional regression as well. We also provide numerical evidence that using higher order polynomial terms can lead to an improved performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03036v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jco.2024.101853</arxiv:DOI>
      <arxiv:journal_reference>Journal of Complexity, Volume 83, August 2024, 101853</arxiv:journal_reference>
      <dc:creator>Markus Holzleitner, Sergei Pereverzyev</dc:creator>
    </item>
    <item>
      <title>Splitting Methods for differential equations</title>
      <link>https://arxiv.org/abs/2401.01722</link>
      <description>arXiv:2401.01722v3 Announce Type: replace 
Abstract: This overview is devoted to splitting methods, a class of numerical integrators intended for differential equations that can be subdivided into different problems easier to solve than the original system. Closely connected with this class of integrators are composition methods, in which one or several low-order schemes are composed to construct higher-order numerical approximations to the exact solution. We analyze in detail the order conditions that have to be satisfied by these classes of methods to achieve a given order, and provide some insight about their qualitative properties in connection with geometric numerical integration and the treatment of highly oscillatory problems. Since splitting methods have received considerable attention in the realm of partial differential equations, we also cover this subject in the present survey, with special attention to parabolic equations and their problems. An exhaustive list of methods of different orders is collected and tested on simple examples. Finally, some applications of splitting methods in different areas, ranging from celestial mechanics to statistics, are also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01722v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sergio Blanes, Fernando Casas, Ander Murua</dc:creator>
    </item>
    <item>
      <title>Perspectives on locally weighted ensemble Kalman methods</title>
      <link>https://arxiv.org/abs/2402.00027</link>
      <description>arXiv:2402.00027v2 Announce Type: replace 
Abstract: This manuscript derives locally weighted ensemble Kalman methods from the point of view of ensemble-based function approximation. This is done by using pointwise evaluations to build up a local linear or quadratic approximation of a function, tapering off the effect of distant particles via local weighting. This introduces a candidate method (the locally weighted Ensemble Kalman method for inversion) with the motivation of combining some of the strengths of the particle filter (ability to cope with nonlinear maps and non-Gaussian distributions) and the Ensemble Kalman filter (no filter degeneracy).</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00027v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Wacker</dc:creator>
    </item>
    <item>
      <title>Spectral convergence of a semi-discretized numerical system for the spatially homogeneous Boltzmann equation with uncertainties</title>
      <link>https://arxiv.org/abs/2402.07060</link>
      <description>arXiv:2402.07060v2 Announce Type: replace 
Abstract: In this paper, we study the Boltzmann equation with uncertainties and prove that the spectral convergence of the semi-discretized numerical system holds in a combined velocity and random space, where the Fourier-spectral method is applied for approximation in the velocity space whereas the generalized polynomial chaos (gPC)-based stochastic Galerkin (SG) method is employed to discretize the random variable. Our proof is based on a delicate energy estimate for showing the well-posedness of the numerical solution as well as a rigorous control of its negative part in our well-designed functional space that involves high-order derivatives of both the velocity and random variables. This paper rigorously justifies the statement proposed in [Remark 4.4, J. Hu and S. Jin, J. Comput. Phys., 315 (2016), pp. 150-168].</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07060v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liu Liu, Kunlun Qi</dc:creator>
    </item>
    <item>
      <title>Efficient inverse $Z$-transform and Wiener-Hopf factorization</title>
      <link>https://arxiv.org/abs/2404.19290</link>
      <description>arXiv:2404.19290v2 Announce Type: replace 
Abstract: We suggest new closely related methods for numerical inversion of $Z$-transform and Wiener-Hopf factorization of functions on the unit circle, based on sinh-deformations of the contours of integration, corresponding changes of variables and the simplified trapezoid rule. As applications, we consider evaluation of high moments of probability distributions and construction of causal filters. Programs in Matlab running on a Mac with moderate characteristics achieves the precision E-14 in several dozen of microseconds and E-11 in several milliseconds, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19290v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>q-fin.CP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Svetlana Boyarchenko, Sergei Levendorski\u{i}</dc:creator>
    </item>
    <item>
      <title>Sampling discretization of the uniform norm and applications</title>
      <link>https://arxiv.org/abs/2306.14207</link>
      <description>arXiv:2306.14207v2 Announce Type: replace-cross 
Abstract: Discretization of the uniform norm of functions from a given finite dimensional subspace of continuous functions is studied. Previous known results show that for any $N$-dimensional subspace of the space of continuous functions it is sufficient to use $e^{CN}$ sample points for an accurate upper bound for the uniform norm by the discrete norm and that one cannot improve on the exponential growth of the number of sampling points for a good discretization theorem in the uniform norm. In this paper we focus on two types of results, which allow us to obtain good discretization of the uniform norm with polynomial in $N$ number of points. In the first way we weaken the discretization inequality by allowing a bound of the uniform norm by the discrete norm multiplied by an extra factor, which may depend on $N$. In the second way we impose restrictions on the finite dimensional subspace under consideration. In particular, we prove a general result, which connects the upper bound on the number of sampling points in the discretization theorem for the uniform norm with the best $m$-term bilinear approximation of the Dirichlet kernel associated with the given subspace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14207v2</guid>
      <category>math.FA</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jmaa.2024.128431</arxiv:DOI>
      <arxiv:journal_reference>Journal of Mathematical Analysis and Applications, 538(2), 2024, 128431</arxiv:journal_reference>
      <dc:creator>E. D. Kosov, V. N. Temlyakov</dc:creator>
    </item>
    <item>
      <title>DPOT: Auto-Regressive Denoising Operator Transformer for Large-Scale PDE Pre-Training</title>
      <link>https://arxiv.org/abs/2403.03542</link>
      <description>arXiv:2403.03542v4 Announce Type: replace-cross 
Abstract: Pre-training has been investigated to improve the efficiency and performance of training neural operators in data-scarce settings. However, it is largely in its infancy due to the inherent complexity and diversity, such as long trajectories, multiple scales and varying dimensions of partial differential equations (PDEs) data. In this paper, we present a new auto-regressive denoising pre-training strategy, which allows for more stable and efficient pre-training on PDE data and generalizes to various downstream tasks. Moreover, by designing a flexible and scalable model architecture based on Fourier attention, we can easily scale up the model for large-scale pre-training. We train our PDE foundation model with up to 0.5B parameters on 10+ PDE datasets with more than 100k trajectories. Extensive experiments show that we achieve SOTA on these benchmarks and validate the strong generalizability of our model to significantly enhance performance on diverse downstream PDE tasks like 3D data. Code is available at \url{https://github.com/thu-ml/DPOT}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03542v4</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongkai Hao, Chang Su, Songming Liu, Julius Berner, Chengyang Ying, Hang Su, Anima Anandkumar, Jian Song, Jun Zhu</dc:creator>
    </item>
    <item>
      <title>Differentiating Through Linear Solvers</title>
      <link>https://arxiv.org/abs/2404.17039</link>
      <description>arXiv:2404.17039v2 Announce Type: replace-cross 
Abstract: Computer programs containing calls to linear solvers are a known challenge for automatic differentiation. Previous publications advise against differentiating through the low-level solver implementation, and instead advocate for high-level approaches that express the derivative in terms of a modified linear system that can be solved with a separate solver call. Despite this ubiquitous advice, we are not aware of prior work comparing the accuracy of both approaches. With this article we thus empirically study a simple question: What happens if we ignore common wisdom, and differentiate through linear solvers?</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17039v2</guid>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Paul Hovland, Jan H\"uckelheim</dc:creator>
    </item>
  </channel>
</rss>
