<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Jul 2025 04:00:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Zeroth-order log-concave sampling</title>
      <link>https://arxiv.org/abs/2507.18021</link>
      <description>arXiv:2507.18021v1 Announce Type: new 
Abstract: We study the zeroth-order query complexity of log-concave sampling, specifically uniform sampling from convex bodies using membership oracles. We propose a simple variant of the proximal sampler that achieves the query complexity with matched R\'enyi orders between the initial warmness and output guarantee. Specifically, for any $\varepsilon&gt;0$ and $q\geq2$, the sampler, initialized at $\pi_{0}$, outputs a sample whose law is $\varepsilon$-close in $q$-R\'enyi divergence to $\pi$, the uniform distribution over a convex body in $\mathbb{R}^{d}$, using $\widetilde{O}(qM_{q}^{q/(q-1)}d^{2}\,\lVert\operatorname{cov}\pi\rVert\log\frac{1}{\varepsilon})$ membership queries, where $M_{q}=\lVert\text{d}\pi_{0}/\text{d}\pi\rVert_{L^{q}(\pi)}$.
  We further introduce a simple annealing scheme that produces a warm start in $q$-R\'enyi divergence (i.e., $M_{q}=O(1)$) using $\widetilde{O}(qd^{2}R^{3/2}\,\lVert\operatorname{cov}\pi\rVert^{1/4})$ queries, where $R^{2}=\mathbb{E}_{\pi}[|\cdot|^{2}]$. This interpolates between known complexities for warm-start generation in total variation and R\'enyi-infinity divergence. To relay a R\'enyi warmness across the annealing scheme, we establish hypercontractivity under simultaneous heat flow and translate it into an improved mixing guarantee for the proximal sampler under a logarithmic Sobolev inequality. These results extend naturally to general log-concave distributions accessible via evaluation oracles, incurring additional quadratic queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18021v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.FA</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunbum Kook</dc:creator>
    </item>
    <item>
      <title>Bounding Conditional Value-at-Risk via Auxiliary Distributions with Bounded Discrepancies</title>
      <link>https://arxiv.org/abs/2507.18129</link>
      <description>arXiv:2507.18129v1 Announce Type: new 
Abstract: In this paper, we develop a theoretical framework for bounding the CVaR of a random variable $X$ using another related random variable $Y$, under assumptions on their cumulative and density functions. Our results yield practical tools for approximating $\operatorname{CVaR}_\alpha(X)$ when direct information about $X$ is limited or sampling is computationally expensive, by exploiting a more tractable or observable random variable $Y$. Moreover, the derived bounds provide interpretable concentration inequalities that quantify how the tail risk of $X$ can be controlled via $Y$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18129v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaacov Pariente, Vadim Indelman</dc:creator>
    </item>
    <item>
      <title>Trek-Based Parameter Identification for Linear Causal Models With Arbitrarily Structured Latent Variables</title>
      <link>https://arxiv.org/abs/2507.18170</link>
      <description>arXiv:2507.18170v1 Announce Type: new 
Abstract: We develop a criterion to certify whether causal effects are identifiable in linear structural equation models with latent variables. Linear structural equation models correspond to directed graphs whose nodes represent the random variables of interest and whose edges are weighted with linear coefficients that correspond to direct causal effects. In contrast to previous identification methods, we do not restrict ourselves to settings where the latent variables constitute independent latent factors (i.e., to source nodes in the graphical representation of the model). Our novel latent-subgraph criterion is a purely graphical condition that is sufficient for identifiability of causal effects by rational formulas in the covariance matrix. To check the latent-subgraph criterion, we provide a sound and complete algorithm that operates by solving an integer linear program. While it targets effects involving observed variables, our new criterion is also useful for identifying effects between latent variables, as it allows one to transform the given model into a simpler measurement model for which other existing tools become applicable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18170v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nils Sturma, Mathias Drton</dc:creator>
    </item>
    <item>
      <title>Data assimilation with the 2D Navier-Stokes equations: Optimal Gaussian asymptotics for the posterior measure</title>
      <link>https://arxiv.org/abs/2507.18279</link>
      <description>arXiv:2507.18279v1 Announce Type: new 
Abstract: A functional Bernstein - von Mises theorem is proved for posterior measures arising in a data assimilation problem with the two-dimensional Navier-Stokes equation where a Gaussian process prior is assigned to the initial condition of the system. The posterior measure, which provides the update in the space of all trajectories arising from a discrete sample of the (deterministic) dynamics, is shown to be approximated by a Gaussian random vector field arising from the solution to a linear parabolic PDE with Gaussian initial condition. The approximation holds in the strong sense of the supremum norm on the regression functions, showing that predicting future states of Navier-Stokes systems admits $1/\sqrt N$-consistent estimators even for commonly used nonparametric models. Consequences for coverage of credible bands and uncertainty quantification are discussed. A local asymptotic minimax theorem is derived that describes the lower bound for estimating the state of the nonlinear system, which is shown to be attained by the Bayesian data assimilation algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18279v1</guid>
      <category>math.ST</category>
      <category>math.AP</category>
      <category>math.DS</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dimiri Konen, Richard Nickl</dc:creator>
    </item>
    <item>
      <title>ROC curves for LDA classifiers</title>
      <link>https://arxiv.org/abs/2507.18307</link>
      <description>arXiv:2507.18307v1 Announce Type: new 
Abstract: In the paper, we derive an analytic formula for the ROC curves of the LDA classifiers. We establish elementary properties of these curves (monotonicity and concavity) and provide formulae for the area under curve (AUC) and the Youden J-index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18307v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mateusz Krukowski</dc:creator>
    </item>
    <item>
      <title>LSD of sample covariances of superposition of matrices with separable covariance structure</title>
      <link>https://arxiv.org/abs/2507.18505</link>
      <description>arXiv:2507.18505v1 Announce Type: new 
Abstract: We study the asymptotic behavior of the spectra of matrices of the form $S_n = \frac{1}{n}XX^*$ where $X =\sum_{r=1}^K X_r$, where $X_r = A_r^\frac{1}{2}Z_rB_r^\frac{1}{2}$, $K \in \mathbb{N}$ and $A_r,B_r$ are sequences of positive semi-definite matrices of dimensions $p\times p$ and $n\times n$, respectively. We establish the existence of a limiting spectral distribution for $S_n$ by assuming that matrices $\{A_r\}_{r=1}^K$ are simultaneously diagonalizable and $\{B_r\}_{r=1}^K$ are simultaneously digaonalizable, and that the joint spectral distributions of $\{A_r\}_{r=1}^K$ and $\{B_r\}_{r=1}^K$ converge to $K$-dimensional distributions, as $p,n\to \infty$ such that $p/n \to c \in (0,\infty)$. The LSD of $S_n$ is characterized by system of equations with unique solutions within the class of Stieltjes transforms of measures on $\mathbb{R}_+$. These results generalize existing results on the LSD of sample covariances when the data matrices have a separable covariance structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18505v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Javed Hazarika, Debashis Paul</dc:creator>
    </item>
    <item>
      <title>Sliding Window Informative Canonical Correlation Analysis</title>
      <link>https://arxiv.org/abs/2507.17921</link>
      <description>arXiv:2507.17921v1 Announce Type: cross 
Abstract: Canonical correlation analysis (CCA) is a technique for finding correlated sets of features between two datasets. In this paper, we propose a novel extension of CCA to the online, streaming data setting: Sliding Window Informative Canonical Correlation Analysis (SWICCA). Our method uses a streaming principal component analysis (PCA) algorithm as a backend and uses these outputs combined with a small sliding window of samples to estimate the CCA components in real time. We motivate and describe our algorithm, provide numerical simulations to characterize its performance, and provide a theoretical performance guarantee. The SWICCA method is applicable and scalable to extremely high dimensions, and we provide a real-data example that demonstrates this capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17921v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arvind Prasadan</dc:creator>
    </item>
    <item>
      <title>Large-scale entity resolution via microclustering Ewens--Pitman random partitions</title>
      <link>https://arxiv.org/abs/2507.18101</link>
      <description>arXiv:2507.18101v1 Announce Type: cross 
Abstract: We introduce the microclustering Ewens--Pitman model for random partitions, obtained by scaling the strength parameter of the Ewens--Pitman model linearly with the sample size. The resulting random partition is shown to have the microclustering property, namely: the size of the largest cluster grows sub-linearly with the sample size, while the number of clusters grows linearly. By leveraging the interplay between the Ewens--Pitman random partition with the Pitman--Yor process, we develop efficient variational inference schemes for posterior computation in entity resolution. Our approach achieves a speed-up of three orders of magnitude over existing Bayesian methods for entity resolution, while maintaining competitive empirical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18101v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Stefano Favaro</dc:creator>
    </item>
    <item>
      <title>Moment Martingale Posteriors for Semiparametric Predictive Bayes</title>
      <link>https://arxiv.org/abs/2507.18148</link>
      <description>arXiv:2507.18148v1 Announce Type: cross 
Abstract: The predictive Bayesian view involves eliciting a sequence of one-step-ahead predictive distributions in lieu of specifying a likelihood function and prior distribution. Recent methods have leveraged predictive distributions which are either nonparametric or parametric, but not a combination of the two. This paper introduces a semiparametric martingale posterior which utilizes a predictive distribution that is a mixture of a parametric and nonparametric component. The semiparametric nature of the predictive allows for regularization of the nonparametric component when the sample size is small, and robustness to model misspecification of the parametric component when the sample size is large. We call this approach the moment martingale posterior, as the core of our proposed methodology is to utilize the method of moments as the vehicle for tying the nonparametric and parametric components together. In particular, the predictives are constructed so that the moments are martingales, which allows us to verify convergence under predictive resampling. A key contribution of this work is a novel procedure based on the energy score to optimally weigh between the parametric and nonparametric components, which has desirable asymptotic properties. The effectiveness of the proposed approach is demonstrated through simulations and a real world example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18148v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiu Yin Yung, Stephen M. S. Lee, Edwin Fong</dc:creator>
    </item>
    <item>
      <title>On Reconstructing Training Data From Bayesian Posteriors and Trained Models</title>
      <link>https://arxiv.org/abs/2507.18372</link>
      <description>arXiv:2507.18372v1 Announce Type: cross 
Abstract: Publicly releasing the specification of a model with its trained parameters means an adversary can attempt to reconstruct information about the training data via training data reconstruction attacks, a major vulnerability of modern machine learning methods. This paper makes three primary contributions: establishing a mathematical framework to express the problem, characterising the features of the training data that are vulnerable via a maximum mean discrepancy equivalance and outlining a score matching framework for reconstructing data in both Bayesian and non-Bayesian models, the former is a first in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18372v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Wynne</dc:creator>
    </item>
    <item>
      <title>Euclidean Distance Deflation Under High-Dimensional Heteroskedastic Noise</title>
      <link>https://arxiv.org/abs/2507.18520</link>
      <description>arXiv:2507.18520v1 Announce Type: cross 
Abstract: Pairwise Euclidean distance calculation is a fundamental step in many machine learning and data analysis algorithms. In real-world applications, however, these distances are frequently distorted by heteroskedastic noise$\unicode{x2014}$a prevalent form of inhomogeneous corruption characterized by variable noise magnitudes across data observations. Such noise inflates the computed distances in a nontrivial way, leading to misrepresentations of the underlying data geometry. In this work, we address the tasks of estimating the noise magnitudes per observation and correcting the pairwise Euclidean distances under heteroskedastic noise. Perhaps surprisingly, we show that in general high-dimensional settings and without assuming prior knowledge on the clean data structure or noise distribution, both tasks can be performed reliably, even when the noise levels vary considerably. Specifically, we develop a principled, hyperparameter-free approach that jointly estimates the noise magnitudes and corrects the distances. We provide theoretical guarantees for our approach, establishing probabilistic bounds on the estimation errors of both noise magnitudes and distances. These bounds, measured in the normalized $\ell_1$ norm, converge to zero at polynomial rates as both feature dimension and dataset size increase. Experiments on synthetic datasets demonstrate that our method accurately estimates distances in challenging regimes, significantly improving the robustness of subsequent distance-based computations. Notably, when applied to single-cell RNA sequencing data, our method yields noise magnitude estimates consistent with an established prototypical model, enabling accurate nearest neighbor identification that is fundamental to many downstream analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18520v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keyi Li, Yuval Kluger, Boris Landa</dc:creator>
    </item>
    <item>
      <title>How weak are weak factors? Uniform inference for signal strength in signal plus noise models</title>
      <link>https://arxiv.org/abs/2507.18554</link>
      <description>arXiv:2507.18554v1 Announce Type: cross 
Abstract: The paper analyzes four classical signal-plus-noise models: the factor model, spiked sample covariance matrices, the sum of a Wigner matrix and a low-rank perturbation, and canonical correlation analysis with low-rank dependencies. The objective is to construct confidence intervals for the signal strength that are uniformly valid across all regimes - strong, weak, and critical signals. We demonstrate that traditional Gaussian approximations fail in the critical regime. Instead, we introduce a universal transitional distribution that enables valid inference across the entire spectrum of signal strengths. The approach is illustrated through applications in macroeconomics and finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18554v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Bykhovskaya, Vadim Gorin, Sasha Sodin</dc:creator>
    </item>
    <item>
      <title>An omnibus goodness-of-fit test based on trigonometric moments</title>
      <link>https://arxiv.org/abs/2507.18591</link>
      <description>arXiv:2507.18591v1 Announce Type: cross 
Abstract: We present a versatile omnibus goodness-of-fit test based on the first two trigonometric moments of probability-integral-transformed data, which rectifies the covariance scaling errors made by Langholz and Kronmal [J. Amer. Statist. Assoc. 86 (1991), 1077--1084]. Once properly scaled, the quadratic-form statistic asymptotically follows a $\chi_2^2$ distribution under the null hypothesis. The covariance scalings and parameter estimators are provided for $32$ null distribution families, covering heavy-tailed, light-tailed, asymmetric, and bounded-support cases, so the test is ready to be applied directly. Using recent advances in non-degenerate multivariate $U$-statistics with estimated nuisance parameters, we also showcase its asymptotic distribution under local alternatives for three specific examples. Our procedure shows excellent power; in particular, simulations testing the Laplace model against a range of $400$ alternatives reveal that it surpasses all $40$ existing tests for moderate to large sample sizes. A real-data application involving 48-hour-ahead surface temperature forecast errors further demonstrates the practical utility of the test. To ensure full reproducibility, the R code that generated our numerical results is publicly accessible online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18591v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alain Desgagn\'e, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Unbiased estimation in one-parameter exponential families for the inverse of the natural parameter with extensions</title>
      <link>https://arxiv.org/abs/2507.15077</link>
      <description>arXiv:2507.15077v2 Announce Type: replace 
Abstract: For one-parameter continuous exponential families, we identify an unbiased estimator of the inverse of the natural parameter $\theta$ for cases where $\theta &gt; 0$, extending an earlier result of \cite{voinov1985unbiased} applicable to a normal model. We provide various applications for Gamma models, Inverse Gaussian models, distributions obtained by truncation, and ratios of normal means. Moreover, we extend the findings to estimating negative powers $\theta^{-k}$, and more generally to complete monotone functions $q(\theta)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15077v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pankaj Bhagwat, Eric Marchand</dc:creator>
    </item>
    <item>
      <title>Robust and Smooth Estimation of the Extreme Tail Index via Weighted Minimum Density Power Divergence</title>
      <link>https://arxiv.org/abs/2507.15744</link>
      <description>arXiv:2507.15744v2 Announce Type: replace 
Abstract: By introducing a weight function into the density power divergence, we develop a new class of robust and smooth estimators for the tail index of Pareto-type distributions, offering improved efficiency in the presence of outliers. These estimators can be viewed as a robust generalization of both weighted least squares and kernel-based tail index estimators. We establish the consistency and asymptotic normality of the proposed class. A simulation study is conducted to assess their finite-sample performance in comparison with existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15744v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saida Mancer, Abdelhakim Necir, Djamel Meraghni</dc:creator>
    </item>
    <item>
      <title>Local Polynomial Estimation of Time-Varying Parameters in Nonlinear Models</title>
      <link>https://arxiv.org/abs/1904.05209</link>
      <description>arXiv:1904.05209v3 Announce Type: replace-cross 
Abstract: We develop a novel asymptotic theory for local polynomial extremum estimators of time-varying parameters in a broad class of nonlinear time series models. We show the proposed estimators are consistent and follow normal distributions in large samples under weak conditions. We also provide a precise characterisation of the leading bias term due to smoothing, which has not been done before. We demonstrate the usefulness of our general results by establishing primitive conditions for local (quasi-)maximum-likelihood estimators of time-varying models threshold autoregressions, ARCH models and Poisson autogressions with exogenous co--variates, to be normally distributed in large samples and characterise their leading biases. An empirical study of US corporate default counts demonstrates the applicability of the proposed local linear estimator for Poisson autoregression, shedding new light on the dynamic properties of US corporate defaults.</description>
      <guid isPermaLink="false">oai:arXiv.org:1904.05209v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dennis Kristensen, Young Jun Lee</dc:creator>
    </item>
    <item>
      <title>Pseudo-Labeling for Kernel Ridge Regression under Covariate Shift</title>
      <link>https://arxiv.org/abs/2302.10160</link>
      <description>arXiv:2302.10160v4 Announce Type: replace-cross 
Abstract: We develop and analyze a principled approach to kernel ridge regression under covariate shift. The goal is to learn a regression function with small mean squared error over a target distribution, based on unlabeled data from there and labeled data that may have a different feature distribution. We propose to split the labeled data into two subsets, and conduct kernel ridge regression on them separately to obtain a collection of candidate models and an imputation model. We use the latter to fill the missing labels and then select the best candidate accordingly. Our non-asymptotic excess risk bounds demonstrate that our estimator adapts effectively to both the structure of the target distribution and the covariate shift. This adaptation is quantified through a notion of effective sample size that reflects the value of labeled source data for the target regression task. Our estimator achieves the minimax optimal error rate up to a polylogarithmic factor, and we find that using pseudo-labels for model selection does not significantly hinder performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.10160v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaizheng Wang</dc:creator>
    </item>
    <item>
      <title>Fine-Grained Uncertainty Quantification via Collisions</title>
      <link>https://arxiv.org/abs/2411.12127</link>
      <description>arXiv:2411.12127v4 Announce Type: replace-cross 
Abstract: We propose a new and intuitive metric for aleatoric uncertainty quantification (UQ), the prevalence of class collisions defined as the same input being observed in different classes. We use the rate of class collisions to define the collision matrix, a novel and uniquely fine-grained measure of uncertainty. For a classification problem involving $K$ classes, the $K\times K$ collision matrix $S$ measures the inherent difficulty in distinguishing between each pair of classes. We discuss several applications of the collision matrix, establish its fundamental mathematical properties, as well as show its relationship with existing UQ methods, including the Bayes error rate (BER). We also address the new problem of estimating the collision matrix using one-hot labeled data by proposing a series of innovative techniques to estimate $S$. First, we learn a pair-wise contrastive model which accepts two inputs and determines if they belong to the same class. We then show that this contrastive model (which is PAC learnable) can be used to estimate the Gramian matrix of $S$, defined as $G=S^TS$. Finally, we show that under reasonable assumptions, $G$ can be used to uniquely recover $S$, a new result on non-negative matrices which could be of independent interest. With a method to estimate $S$ established, we demonstrate how this estimate of $S$, in conjunction with the contrastive model, can be used to estimate the posterior class portability distribution of any point. Experimental results are also presented to validate our methods of estimating the collision matrix and class posterior distributions on several datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12127v4</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse Friedbaum, Sudarshan Adiga, Ravi Tandon</dc:creator>
    </item>
    <item>
      <title>Outcome-Based Online Reinforcement Learning: Algorithms and Fundamental Limits</title>
      <link>https://arxiv.org/abs/2505.20268</link>
      <description>arXiv:2505.20268v2 Announce Type: replace-cross 
Abstract: Reinforcement learning with outcome-based feedback faces a fundamental challenge: when rewards are only observed at trajectory endpoints, how do we assign credit to the right actions? This paper provides the first comprehensive analysis of this problem in online RL with general function approximation. We develop a provably sample-efficient algorithm achieving $\widetilde{O}({C_{\rm cov} H^3}/{\epsilon^2})$ sample complexity, where $C_{\rm cov}$ is the coverability coefficient of the underlying MDP. By leveraging general function approximation, our approach works effectively in large or infinite state spaces where tabular methods fail, requiring only that value functions and reward functions can be represented by appropriate function classes. Our results also characterize when outcome-based feedback is statistically separated from per-step rewards, revealing an unavoidable exponential separation for certain MDPs. For deterministic MDPs, we show how to eliminate the completeness assumption, dramatically simplifying the algorithm. We further extend our approach to preference-based feedback settings, proving that equivalent statistical efficiency can be achieved even under more limited information. Together, these results constitute a theoretical foundation for understanding the statistical properties of outcome-based reinforcement learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20268v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Chen, Zeyu Jia, Alexander Rakhlin, Tengyang Xie</dc:creator>
    </item>
  </channel>
</rss>
