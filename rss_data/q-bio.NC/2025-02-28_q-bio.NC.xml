<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Feb 2025 05:01:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Noise-induced Extreme Events in Hodgkin-Huxley Neural Networks</title>
      <link>https://arxiv.org/abs/2502.19565</link>
      <description>arXiv:2502.19565v1 Announce Type: new 
Abstract: Extreme events are rare, large-scale deviations from typical system behavior that can occur in nonlinear dynamical systems. In this study, we explore the emergence of extreme events within a network of identical stochastic Hodgkin-Huxley neurons with mean-field coupling. The neurons are exposed to uncorrelated noise, which introduces stochastic electrical fluctuations that influence their spiking activity. Analyzing the variations in the amplitude of the mean field, we observe a smooth transition from small-amplitude, out-of-sync activity to synchronized spiking activity as the coupling parameter increases, while an abrupt transition occurs with increasing noise intensity. However, beyond a certain threshold, the coupling abruptly suppresses the spiking activity of the network. Our analysis reveals that the influence of noise combined with neuronal coupling near the abrupt transitions can trigger cascades of synchronized spiking activity, identified as extreme events. The analysis of the entropy of the mean field allows us to detect the parameter region where these events occur. We characterize the statistics of these events and find that, as the network size increases, the parameter range where they occur decreases significantly. Our findings shed light on the mechanisms driving extreme events in neural networks and how noise and neural coupling shape collective behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19565v1</guid>
      <category>q-bio.NC</category>
      <category>physics.app-ph</category>
      <category>physics.bio-ph</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.chaos.2025.116133</arxiv:DOI>
      <arxiv:journal_reference>Chaos, Solitons &amp; Fractals, Volume 194, May 2025, 116133</arxiv:journal_reference>
      <dc:creator>Bruno R. R. Boaretto, Elbert E. N. Macau, Cristina Masoller</dc:creator>
    </item>
    <item>
      <title>Research on Event-Related Desynchronization of Motor Imagery and Movement Based on Localized EEG Cortical Sources</title>
      <link>https://arxiv.org/abs/2502.19869</link>
      <description>arXiv:2502.19869v1 Announce Type: new 
Abstract: This study investigates event-related desynchronization (ERD) phenomena during motor imagery and actual movement. Using sLORETA software, we analyzed the cortical current source density distributions in Mu and Beta frequency bands for 33 subjects during rest, motor imagery, and actual movement conditions. The results were normalized for analysis. Using sLORETA's statistical tools, paired t-tests were conducted to compare the normalized current source density results between rest and motor imagery, rest and actual movement, and motor imagery and actual movement conditions in both frequency bands. The findings revealed: In both Mu and Beta frequency bands, during motor imagery, significant ERD (P&lt;0.01) was observed in the salience network, supplementary motor area, primary motor area, premotor cortex, primary somatosensory cortex, and parietofrontal mirror neuron system. During actual movement, significant ERD (P&lt;0.05) was observed in the primary somatosensory cortex, primary motor area, and parietofrontal mirror neuron system in both frequency bands. Comparing motor imagery to actual movement, the current source density in the primary somatosensory cortex and parietofrontal mirror neuron system was higher during motor imagery, though this difference was not statistically significant (P&gt;0.05). This paper analyzes the factors contributing to these statistical results and proposes preliminary solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19869v1</guid>
      <category>q-bio.NC</category>
      <category>eess.SP</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.19353/j.cnki.dzsj.2017.24.014</arxiv:DOI>
      <dc:creator>Yuqing Wang</dc:creator>
    </item>
    <item>
      <title>Naturalistic Computational Cognitive Science: Towards generalizable models and theories that capture the full range of natural behavior</title>
      <link>https://arxiv.org/abs/2502.20349</link>
      <description>arXiv:2502.20349v1 Announce Type: new 
Abstract: Artificial Intelligence increasingly pursues large, complex models that perform many tasks within increasingly realistic domains. How, if at all, should these developments in AI influence cognitive science?
  We argue that progress in AI offers timely opportunities for cognitive science to embrace experiments with increasingly naturalistic stimuli, tasks, and behaviors; and computational models that can accommodate these changes. We first review a growing body of research spanning neuroscience, cognitive science, and AI that suggests that incorporating a broader range of naturalistic experimental paradigms (and models that accommodate them) may be necessary to resolve some aspects of natural intelligence and ensure that our theories generalize. We then suggest that integrating recent progress in AI and cognitive science will enable us to engage with more naturalistic phenomena without giving up experimental control or the pursuit of theoretically grounded understanding. We offer practical guidance on how methodological practices can contribute to cumulative progress in naturalistic computational cognitive science, and illustrate a path towards building computational models that solve the real problems of natural cognition - together with a reductive understanding of the processes and principles by which they do so.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20349v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wilka Carvalho, Andrew Lampinen</dc:creator>
    </item>
    <item>
      <title>Spectral Analysis of Representational Similarity with Limited Neurons</title>
      <link>https://arxiv.org/abs/2502.19648</link>
      <description>arXiv:2502.19648v1 Announce Type: cross 
Abstract: Measuring representational similarity between neural recordings and computational models is challenging due to constraints on the number of neurons that can be recorded simultaneously. In this work, we investigate how such limitations affect similarity measures, focusing on Canonical Correlation Analysis (CCA) and Centered Kernel Alignment (CKA). Leveraging tools from Random Matrix Theory, we develop a predictive spectral framework for these measures and demonstrate that finite neuron sampling systematically underestimates similarity due to eigenvector delocalization. To overcome this, we introduce a denoising method to infer population-level similarity, enabling accurate analysis even with small neuron samples. Our theory is validated on synthetic and real datasets, offering practical strategies for interpreting neural data under finite sampling constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19648v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyunmo Kang, Abdulkadir Canatar, SueYeon Chung</dc:creator>
    </item>
    <item>
      <title>On Conformal Isometry of Grid Cells: Learning Distance-Preserving Position Embedding</title>
      <link>https://arxiv.org/abs/2405.16865</link>
      <description>arXiv:2405.16865v4 Announce Type: replace 
Abstract: This paper investigates the conformal isometry hypothesis as a potential explanation for the hexagonal periodic patterns in grid cell response maps. We posit that grid cell activities form a high-dimensional vector in neural space, encoding the agent's position in 2D physical space. As the agent moves, this vector rotates within a 2D manifold in the neural space, driven by a recurrent neural network. The conformal hypothesis proposes that this neural manifold is a conformal isometric embedding of 2D physical space, where local physical distance is preserved by the embedding up to a scaling factor (or unit of metric). Such distance-preserving position embedding is indispensable for path planning in navigation, especially planning local straight path segments. We conduct numerical experiments to show that this hypothesis leads to the hexagonal grid firing patterns by learning maximally distance-preserving position embedding, agnostic to the choice of the recurrent neural network. Furthermore, we present a theoretical explanation of why hexagon periodic patterns emerge by minimizing our loss function by showing that hexagon flat torus is maximally distance preserving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16865v4</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dehong Xu, Ruiqi Gao, Wen-Hao Zhang, Xue-Xin Wei, Ying Nian Wu</dc:creator>
    </item>
    <item>
      <title>Quantum Models of Consciousness from a Quantum Information Science Perspective</title>
      <link>https://arxiv.org/abs/2501.03241</link>
      <description>arXiv:2501.03241v2 Announce Type: replace 
Abstract: This perspective explores various quantum models of consciousness from the viewpoint of quantum information science, offering potential ideas and insights. The models under consideration can be categorized into three distinct groups based on the level at which quantum mechanics might operate within the brain: those suggesting that consciousness arises from electron delocalization within microtubules inside neurons, those proposing it emerges from the electromagnetic field surrounding the entire neural network, and those positing it originates from the interactions between individual neurons governed by neurotransmitter molecules. Our focus is particularly on the Posner model of cognition, for which we provide preliminary calculations on the preservation of entanglement of phosphate molecules within the geometric structure of Posner clusters. These findings provide valuable insights into how quantum information theory can enhance our understanding of brain functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03241v2</guid>
      <category>q-bio.NC</category>
      <category>quant-ph</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3390/e27030243</arxiv:DOI>
      <arxiv:journal_reference>Entropy 2025, 27, 243</arxiv:journal_reference>
      <dc:creator>Lea Gassab, Onur Pusuluk, Marco Cattaneo, \"Ozg\"ur E. M\"ustecapl{\i}o\u{g}lu</dc:creator>
    </item>
    <item>
      <title>Graceful forgetting: Memory as a process</title>
      <link>https://arxiv.org/abs/2502.11105</link>
      <description>arXiv:2502.11105v2 Announce Type: replace 
Abstract: A rational theory of memory is proposed to explain how we can accommodate unbounded sensory input within bounded storage space. Memory is stored as statistics, organized into complex structures that are constantly summarized and compressed to make room for new input. This process, driven by space constraints, is guided by heuristics that optimize the memory for future needs. Sensory input is rapidly encoded as simple statistics that are more slowly elaborated into more abstract constructs. This theory differs from previous accounts of memory by (a) its reliance on statistics, (b) its use of heuristics to guide the choice of statistics, and (c) the emphasis on memory as a process that is intensive, complex, and expensive. The theory is intended as an aid to make sense of our extensive knowledge of memory, and bring us closer to an understanding of memory in functional and mechanistic terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11105v2</guid>
      <category>q-bio.NC</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alain de Cheveign\'e</dc:creator>
    </item>
    <item>
      <title>Hierarchy of chaotic dynamics in random modular networks</title>
      <link>https://arxiv.org/abs/2410.06361</link>
      <description>arXiv:2410.06361v2 Announce Type: replace-cross 
Abstract: We introduce a model of randomly connected neural populations and study its dynamics by means of the dynamical mean-field theory and simulations. Our analysis uncovers a rich phase diagram, featuring high- and low-dimensional chaotic phases, separated by a crossover region characterized by low values of the maximal Lyapunov exponent and participation ratio dimension, but with high values of the Lyapunov dimension that change significantly across the region. Counterintuitively, chaos can be attenuated by either adding noise to strongly modular connectivity or by introducing modularity into random connectivity. Extending the model to include a multilevel, hierarchical connectivity reveals that a loose balance between activities across levels drives the system towards the edge of chaos.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06361v2</guid>
      <category>physics.bio-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.NE</category>
      <category>nlin.CD</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>{\L}ukasz Ku\'smierz, Ulises Pereira-Obilinovic, Zhixin Lu, Dana Mastrovito, Stefan Mihalas</dc:creator>
    </item>
  </channel>
</rss>
