<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Sep 2025 04:02:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Hierarchical Importance Sampling for Estimating Occupation Time for SDE Solutions</title>
      <link>https://arxiv.org/abs/2509.13950</link>
      <description>arXiv:2509.13950v1 Announce Type: cross 
Abstract: This study considers the estimation of the complementary cumulative distribution function of the occupation time (i.e., the time spent below a threshold) for a process governed by a stochastic differential equation. The focus is on the right tail, where the underlying event becomes rare, and using variance reduction techniques is essential to obtain computationally efficient estimates. Building on recent developments that relate importance sampling (IS) to stochastic optimal control, this work develops an optimal single level IS (SLIS) estimator based on the solution of an auxiliary Hamilton Jacobi Bellman (HJB) partial differential equation (PDE). The cost of solving the HJB-PDE is incorporated into the total computational work, and an optimized trade off between preprocessing and sampling is proposed to minimize the overall cost. The SLIS approach is extended to the multilevel setting to enhance efficiency, yielding a multilevel IS (MLIS) estimator. A necessary and sufficient condition under which the MLIS method outperforms the SLIS method is established, and a common likelihood MLIS formulation is introduced that satisfies this condition under appropriate regularity assumptions. The classical multilevel Monte Carlo complexity theory can be extended to accommodate settings where the single-level variance varies with the discretization level. As a special case, the variance-decay behavior observed in the IS framework stems from the zero variance property of the optimal control. Notably, the total work complexity of MLIS can be better than an order of two. Numerical experiments in the context of fade duration estimation demonstrate the benefits of the proposed approach and validate these theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13950v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eya Ben Amar, Nadhir Ben Rached, Raul Tempone</dc:creator>
    </item>
    <item>
      <title>Sample Size Calculations for the Development of Risk Prediction Models that Account for Performance Variability</title>
      <link>https://arxiv.org/abs/2509.14028</link>
      <description>arXiv:2509.14028v1 Announce Type: cross 
Abstract: Existing approaches to sample size calculations for developing clinical prediction models have focused on ensuring that the expected value of a chosen performance measure meets a pre-specified target. For example, to limit model-overfitting, the sample size is commonly chosen such that the expected calibration slope (CS) is 0.9, close to 1 for a perfectly calibrated model. In practice, due to sampling variability, model performance can vary considerably across different development samples of the recommended size. If this variability is high, the probability of obtaining a model with performance close to the target for a given measure may be unacceptably low. To address this, we propose an adapted approach to sample size calculations that explicitly incorporates performance variability by targeting the probability of acceptable performance (PrAP). For example, in the context of calibration, we may define a model as acceptably calibrated if CS falls in a pre-defined range, e.g. between 0.85 and 1.15. Then we choose the required sample size to ensure that PrAP(CS)=80%. For binary outcomes we implemented our approach for CS within a simulation-based framework via the R package `samplesizedev'. Additionally, for CS specifically, we have proposed an equivalent analytical calculation which is computationally efficient. While we focused on CS, the simulation-based framework is flexible and can be easily extended to accommodate other performance measures and types of outcomes. When adhering to existing recommendations, we found that performance variability increased substantially as the number of predictors, p, decreased. Consequently, PrAP(CS) was often low. For example, with 5 predictors, PrAP(CS) was around 50%. Our adapted approach resulted in considerably larger sample sizes, especially for p&lt;10. Applying shrinkage tends to improve PrAP(CS).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14028v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Menelaos Pavlou, Rumana Z. Omar, Gareth Ambler</dc:creator>
    </item>
    <item>
      <title>Anytime-Valid Inference in Adaptive Experiments: Covariate Adjustment and Balanced Power</title>
      <link>https://arxiv.org/abs/2506.20523</link>
      <description>arXiv:2506.20523v3 Announce Type: replace-cross 
Abstract: Adaptive experiments such as multi-armed bandits offer efficiency gains over traditional randomized experiments but pose two major challenges: invalid inference on the Average Treatment Effect (ATE) due to adaptive sampling and low statistical power for sub-optimal treatments. We address both issues by extending the Mixture Adaptive Design framework (arXiv:2311.05794). First, we propose MADCovar, a covariate-adjusted ATE estimator that is unbiased and preserves anytime-valid inference guarantees while substantially improving ATE precision. Second, we introduce MADMod, which dynamically reallocates samples to underpowered arms, enabling more balanced statistical power across treatments without sacrificing valid inference. Both methods retain MAD's core advantage of constructing asymptotic confidence sequences (CSs) that allow researchers to continuously monitor ATE estimates and stop data collection once a desired precision or significance criterion is met. Empirically, we validate both methods using simulations and real-world data. In simulations, MADCovar reduces CS width by up to $60\%$ relative to MAD. In a large-scale political RCT with $\approx32,000$ participants, MADCovar achieves similar precision gains. MADMod improves statistical power and inferential precision across all treatment arms, particularly for suboptimal treatments. Simulations show that MADMod sharply reduces Type II error while preserving the efficiency benefits of adaptive allocation. Together, MADCovar and MADMod make adaptive experiments more practical, reliable, and efficient for applied researchers across many domains. Our proposed methods are implemented through an open-source software package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20523v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Molitor, Samantha Gold</dc:creator>
    </item>
    <item>
      <title>Group-averaged Markov chains: mixing improvement</title>
      <link>https://arxiv.org/abs/2509.02996</link>
      <description>arXiv:2509.02996v2 Announce Type: replace-cross 
Abstract: For Markov kernels $P$ on a general state space $\mathcal{X}$, we introduce a new class of averaged Markov kernels $P_{da}(G,\nu)$ of $P$ induced by a group $G$ that acts on $\mathcal{X}$ and a probability measure $\nu$ on $G \times G$. Notable special cases are the group-orbit average $\overline{P}$, left-average $P_{la}$, right-average $P_{ra}$ and the independent-double-average $(P_{la})_{ra}$. For $\pi$-stationary $P$ in which $\pi$ is invariant with respect to $G$, we show that in general $P_{da}$ enjoys favorable convergence properties than $P$ based on metrics such as spectral gap or asymptotic variance, and within the family of $P_{da}$ the most preferable kernel is in general $(P_{la})_{ra}$. We demonstrate that $P_{la}, P_{ra}, (P_{la})_{ra}$ are comparable in terms of mixing times, which supports the use of $P_{la}, P_{ra}$ in practice as computationally cheaper alternatives over $(P_{la})_{ra}$. These averaged kernels also admit natural geometric interpretations: they emerge as unique projections of $P$ onto specific $G$-invariant structures under the Kullback-Leibler divergence or the Hilbert-Schmidt norm and satisfy Pythagorean identities. On the other hand, in the general case if $\pi$ is not invariant with respect to $G$, we propose and study a technique that we call state-dependent averaging of Markov kernels which generalizes the earlier results to this setting. As examples and applications, this averaging perspective not only allows us to recast state-of-the-art Markov chain samplers such as Hamiltonian Monte Carlo or piecewise-deterministic Markov processes as specific cases of $P_{da}$, but also enables improvements to existing samplers such as Metropolis-Hastings, achieving rapid mixing in some toy models or when $\pi$ is the discrete uniform distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02996v2</guid>
      <category>math.PR</category>
      <category>cs.IT</category>
      <category>math.GR</category>
      <category>math.IT</category>
      <category>stat.CO</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael C. H. Choi, Youjia Wang</dc:creator>
    </item>
  </channel>
</rss>
