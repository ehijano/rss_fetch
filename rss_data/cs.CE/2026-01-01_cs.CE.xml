<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Jan 2026 05:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Wind Speed Weibull Model Identification in Oman, and Computed Normalized Annual Energy Production (NAEP) From Wind Turbines Based on Data From Weather Stations</title>
      <link>https://arxiv.org/abs/2512.23715</link>
      <description>arXiv:2512.23715v1 Announce Type: new 
Abstract: Using observation records of wind speeds from weather stations in the Sultanate of Oman between 2000 and 2023, we compute estimators of the two Weibull distribution parameters (namely, the Weibull distribution's shape parameter and the Weibull distribution's scale parameter) in 10 weather station locations within eight Omani governorates. The 10 weather station locations in Oman and their corresponding governorates are Seeb (in Muscat), Salalah (in Dhofar), Buraimi (in Al Buraimi), Masirah (in Ash Sharqiyah South), Thumrait (in Dhofar), Sur (in Ash Sharqiyah South), Khasab (in Musandam), Sohar (in Sohar), Fahud (in Az Zahirah), and Saiq (in Ad Dakhiliyah). The obtained wind speed distributions at these weather stations are then used to predict the annual energy production (AEP) for a proposed reference amount of 1 MWp of wind turbine capacity, and this specific AEP is designated here by the term "normalized annual energy production (NAEP)." The direction of the wind is also analyzed statistically over the same period to identify the more probable wind directions. Four locations were clearly distinguishable as being windy compared to the others. The simulated probability of exceeding a feasible 6 m/s (21.6 km/h) wind speed in these locations is 41.71% in Thumrait, 37.77% in Masirah, 29.53% in Sur, and 17.03% in Fahud. The NAEP values in these four locations are estimated as 1.727 GWh/MWp/year, 1.419 GWh/MWp/year, 1.038 GWh/MWp/year, and 0.602 GWh/MWp/year, respectively. The wind in the location of Thumrait is not only the fastest (on average) among the selected locations but also the most unidirectional, blowing almost always from the south-south-east (SSE) direction, and both features make this non-coastal location in southern Oman, with an altitude of about 467 m, an attractive site for utility-scale wind farms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23715v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/eng2.70089</arxiv:DOI>
      <arxiv:journal_reference>Engineering Reports. 7(3), article e70089 (2025)</arxiv:journal_reference>
      <dc:creator>Osama A. Marzouk</dc:creator>
    </item>
    <item>
      <title>A Survey of AI Methods for Geometry Preparation and Mesh Generation in Engineering Simulation</title>
      <link>https://arxiv.org/abs/2512.23719</link>
      <description>arXiv:2512.23719v1 Announce Type: new 
Abstract: Artificial intelligence is beginning to ease long-standing bottlenecks in the CAD-to-mesh pipeline. This survey reviews recent advances where machine learning aids part classification, mesh quality prediction, and defeaturing. We explore methods that improve unstructured and block-structured meshing, support volumetric parameterizations, and accelerate parallel mesh generation. We also examine emerging tools for scripting automation, including reinforcement learning and large language models. Across these efforts, AI acts as an assistive technology, extending the capabilities of traditional geometry and meshing tools. The survey highlights representative methods, practical deployments, and key research challenges that will shape the next generation of data-driven meshing workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23719v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Steven Owen, Nathan Brown, Nikos Chrisochoides, Rao Garimella, Xianfeng Gu, Franck Ledoux, Na Lei, Roshan Quadros, Navamita Ray, Nicolas Winovich, Yongjie Jessica Zhang</dc:creator>
    </item>
    <item>
      <title>Networked Markets, Fragmented Data: Adaptive Graph Learning for Customer Risk Analytics and Policy Design</title>
      <link>https://arxiv.org/abs/2512.24487</link>
      <description>arXiv:2512.24487v1 Announce Type: new 
Abstract: Financial institutions face escalating challenges in identifying high-risk customer behaviors within massive transaction networks, where fraudulent activities exploit market fragmentation and institutional boundaries. We address three fundamental problems in customer risk analytics: data silos preventing holistic relationship assessment, extreme behavioral class imbalance, and suboptimal customer intervention strategies that fail to balance compliance costs with relationship value. We develop an integrated customer intelligence framework combining federated learning, relational network analysis, and adaptive targeting policies. Our federated graph neural network enables collaborative behavior modeling across competing institutions without compromising proprietary customer data, using privacy-preserving embeddings to capture cross-market relational patterns. We introduce cross-bank Personalized PageRank to identify coordinated behavioral clusters providing interpretable customer network segmentation for risk managers. A hierarchical reinforcement learning mechanism optimizes dynamic intervention targeting, calibrating escalation policies to maximize prevention value while minimizing customer friction and operational costs. Analyzing 1.4 million customer transactions across seven markets, our approach reduces false positive and false negative rates to 4.64% and 11.07%, substantially outperforming single-institution models. The framework prevents 79.25% of potential losses versus 49.41% under fixed-rule policies, with optimal market-specific targeting thresholds reflecting heterogeneous customer base characteristics. These findings demonstrate that federated customer analytics materially improve both risk management effectiveness and customer relationship outcomes in networked competitive markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24487v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lecheng Zheng, Jian Ni, Chris Zobel, John R Birge</dc:creator>
    </item>
    <item>
      <title>Integrating Domain Knowledge for Financial QA: A Multi-Retriever RAG Approach with LLMs</title>
      <link>https://arxiv.org/abs/2512.23848</link>
      <description>arXiv:2512.23848v1 Announce Type: cross 
Abstract: This research project addresses the errors of financial numerical reasoning Question Answering (QA) tasks due to the lack of domain knowledge in finance. Despite recent advances in Large Language Models (LLMs), financial numerical questions remain challenging because they require specific domain knowledge in finance and complex multi-step numeric reasoning. We implement a multi-retriever Retrieval Augmented Generators (RAG) system to retrieve both external domain knowledge and internal question contexts, and utilize the latest LLM to tackle these tasks. Through comprehensive ablation experiments and error analysis, we find that domain-specific training with the SecBERT encoder significantly contributes to our best neural symbolic model surpassing the FinQA paper's top model, which serves as our baseline. This suggests the potential superior performance of domain-specific training. Furthermore, our best prompt-based LLM generator achieves the state-of-the-art (SOTA) performance with significant improvement (&gt;7%), yet it is still below the human expert performance. This study highlights the trade-off between hallucinations loss and external knowledge gains in smaller models and few-shot examples. For larger models, the gains from external facts typically outweigh the hallucination loss. Finally, our findings confirm the enhanced numerical reasoning capabilities of the latest LLM, optimized for few-shot learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23848v1</guid>
      <category>cs.CL</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yukun Zhang, Stefan Elbl Droguett, Samyak Jain</dc:creator>
    </item>
    <item>
      <title>Generative AI-enhanced Sector-based Investment Portfolio Construction</title>
      <link>https://arxiv.org/abs/2512.24526</link>
      <description>arXiv:2512.24526v1 Announce Type: cross 
Abstract: This paper investigates how Large Language Models (LLMs) from leading providers (OpenAI, Google, Anthropic, DeepSeek, and xAI) can be applied to quantitative sector-based portfolio construction. We use LLMs to identify investable universes of stocks within S&amp;P 500 sector indices and evaluate how their selections perform when combined with classical portfolio optimization methods. Each model was prompted to select and weight 20 stocks per sector, and the resulting portfolios were compared with their respective sector indices across two distinct out-of-sample periods: a stable market phase (January-March 2025) and a volatile phase (April-June 2025).
  Our results reveal a strong temporal dependence in LLM portfolio performance. During stable market conditions, LLM-weighted portfolios frequently outperformed sector indices on both cumulative return and risk-adjusted (Sharpe ratio) measures. However, during the volatile period, many LLM portfolios underperformed, suggesting that current models may struggle to adapt to regime shifts or high-volatility environments underrepresented in their training data. Importantly, when LLM-based stock selection is combined with traditional optimization techniques, portfolio outcomes improve in both performance and consistency.
  This study contributes one of the first multi-model, cross-provider evaluations of generative AI algorithms in investment management. It highlights that while LLMs can effectively complement quantitative finance by enhancing stock selection and interpretability, their reliability remains market-dependent. The findings underscore the potential of hybrid AI-quantitative frameworks, integrating LLM reasoning with established optimization techniques, to produce more robust and adaptive investment strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24526v1</guid>
      <category>q-fin.PM</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>q-fin.CP</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alina Voronina, Oleksandr Romanko, Ruiwen Cao, Roy H. Kwon, Rafael Mendoza-Arriaga</dc:creator>
    </item>
    <item>
      <title>A Scalable Framework for logP Prediction: From Terabyte-Scale Data Integration to Interpretable Ensemble Modeling</title>
      <link>https://arxiv.org/abs/2512.24643</link>
      <description>arXiv:2512.24643v1 Announce Type: cross 
Abstract: This study presents a large-scale predictive modeling framework for logP prediction using 426850 bioactive compounds rigorously curated from the intersection of three authoritative chemical databases: PubChem, ChEMBL, and eMolecules. We developed a novel computational infrastructure to address the data integration challenge, reducing processing time from a projected over 100 days to 3.2 hours through byte-offset indexing architecture, a 740-fold improvement. Our comprehensive analysis revealed critical insights into the multivariate nature of lipophilicity: while molecular weight exhibited weak bivariate correlation with logP, SHAP analysis on ensemble models identified it as the single most important predictor globally. We systematically evaluated multiple modeling approaches, discovering that linear models suffered from inherent heteroskedasticity that classical remediation strategies, including weighted least squares and Box-Cox transformation, failed to address. Tree-based ensemble methods, including Random Forest and XGBoost, proved inherently robust to this violation, achieving an R-squared of 0.765 and RMSE of 0.731 logP units on the test set. Furthermore, a stratified modeling strategy, employing specialized models for drug-like molecules (91 percent of dataset) and extreme cases (nine percent), achieved optimal performance: an RMSE of 0.838 for the drug-like subset and an R-squared of 0.767 for extreme molecules, the highest of all evaluated approaches. These findings provide actionable guidance for molecular design, establish robust baselines for lipophilicity prediction using only 2D descriptors, and demonstrate that well-curated, descriptor-based ensemble models remain competitive with state-of-the-art graph neural network architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24643v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.DB</category>
      <category>q-bio.BM</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator> Malikussaid, Septian Caesar Floresko, Ade Romadhony, Isman Kurniawan, Warih Maharani, Hilal Hudan Nuha</dc:creator>
    </item>
    <item>
      <title>Advances in Agentic AI: Back to the Future</title>
      <link>https://arxiv.org/abs/2512.24856</link>
      <description>arXiv:2512.24856v1 Announce Type: cross 
Abstract: In light of the recent convergence between Agentic AI and our field of Algorithmization, this paper seeks to restore conceptual clarity and provide a structured analytical framework for an increasingly fragmented discourse. First, (a) it examines the contemporary landscape and proposes precise definitions for the key notions involved, ranging from intelligence to Agentic AI. Second, (b) it reviews our prior body of work to contextualize the evolution of methodologies and technological advances developed over the past decade, highlighting their interdependencies and cumulative trajectory. Third, (c) by distinguishing Machine and Learning efforts within the field of Machine Learning (d) it introduces the first Machine in Machine Learning (M1) as the underlying platform enabling today's LLM-based Agentic AI, conceptualized as an extension of B2C information-retrieval user experiences now being repurposed for B2B transformation. Building on this distinction, (e) the white paper develops the notion of the second Machine in Machine Learning (M2) as the architectural prerequisite for holistic, production-grade B2B transformation, characterizing it as Strategies-based Agentic AI and grounding its definition in the structural barriers-to-entry that such systems must overcome to be operationally viable. Further, (f) it offers conceptual and technical insight into what appears to be the first fully realized implementation of an M2. Finally, drawing on the demonstrated accuracy of the two previous decades of professional and academic experience in developing the foundational architectures of Algorithmization, (g) it outlines a forward-looking research and transformation agenda for the coming two decades.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24856v1</guid>
      <category>econ.TH</category>
      <category>cs.AR</category>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sergio Alvarez-Telena, Marta Diez-Fernandez</dc:creator>
    </item>
    <item>
      <title>FinMMDocR: Benchmarking Financial Multimodal Reasoning with Scenario Awareness, Document Understanding, and Multi-Step Computation</title>
      <link>https://arxiv.org/abs/2512.24903</link>
      <description>arXiv:2512.24903v1 Announce Type: cross 
Abstract: We introduce FinMMDocR, a novel bilingual multimodal benchmark for evaluating multimodal large language models (MLLMs) on real-world financial numerical reasoning. Compared to existing benchmarks, our work delivers three major advancements. (1) Scenario Awareness: 57.9% of 1,200 expert-annotated problems incorporate 12 types of implicit financial scenarios (e.g., Portfolio Management), challenging models to perform expert-level reasoning based on assumptions; (2) Document Understanding: 837 Chinese/English documents spanning 9 types (e.g., Company Research) average 50.8 pages with rich visual elements, significantly surpassing existing benchmarks in both breadth and depth of financial documents; (3) Multi-Step Computation: Problems demand 11-step reasoning on average (5.3 extraction + 5.7 calculation steps), with 65.0% requiring cross-page evidence (2.4 pages average). The best-performing MLLM achieves only 58.0% accuracy, and different retrieval-augmented generation (RAG) methods show significant performance variations on this task. We expect FinMMDocR to drive improvements in MLLMs and reasoning-enhanced methods on complex multimodal reasoning tasks in real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24903v1</guid>
      <category>cs.CV</category>
      <category>cs.CE</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zichen Tang, Haihong E, Rongjin Li, Jiacheng Liu, Linwei Jia, Zhuodi Hao, Zhongjun Yang, Yuanze Li, Haolin Tian, Xinyi Hu, Peizhi Zhao, Yuan Liu, Zhengyu Wang, Xianghe Wang, Yiling Huang, Xueyuan Lin, Ruofei Bai, Zijian Xie, Qian Huang, Ruining Cao, Haocheng Gao</dc:creator>
    </item>
    <item>
      <title>SymX: Energy-based Simulation from Symbolic Expressions</title>
      <link>https://arxiv.org/abs/2303.02156</link>
      <description>arXiv:2303.02156v2 Announce Type: replace 
Abstract: Optimization time integrators are effective at solving complex multi-physics problems including deformable solids with non-linear material models, contact with friction, strain limiting, etc. For challenging problems, Newton-type optimizers are often used, which necessitates first- and second-order derivatives of the global non-linear objective function. Manually differentiating, implementing, testing, optimizing, and maintaining the resulting code is extremely time-consuming, error-prone, and precludes quick changes to the model, even when using tools that assist with parts of such pipeline.
  We present SymX, an open source framework that computes the required derivatives of the different energy contributions by symbolic differentiation, generates optimized code, compiles it on-the-fly, and performs the global assembly. The user only has to provide the symbolic expression of each energy for a single representative element in its corresponding discretization and our system will determine the assembled derivatives for the whole simulation. We demonstrate the versatility of SymX in complex simulations featuring different non-linear materials, high-order finite elements, rigid body systems, adaptive discretizations, frictional contact, and coupling of multiple interacting physical systems.
  SymX's derivatives offer performance on par with SymPy, an established off-the-shelf symbolic engine, and produces simulations at least one order of magnitude faster than TinyAD, an alternative state-of-the-art integral solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.02156v2</guid>
      <category>cs.CE</category>
      <category>cs.GR</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3764928</arxiv:DOI>
      <arxiv:journal_reference>ACM Trans. Graph., Vol. 45, No. 1, Article 5. Pages 1 - 19. Publication date: October 2025</arxiv:journal_reference>
      <dc:creator>Jos\'e Antonio Fern\'andez-Fern\'andez, Fabian L\"oschner, Lukas Westhofen, Andreas Longva, Jan Bender</dc:creator>
    </item>
    <item>
      <title>Multi-fidelity Bayesian Optimization: A Review</title>
      <link>https://arxiv.org/abs/2311.13050</link>
      <description>arXiv:2311.13050v3 Announce Type: replace 
Abstract: Resided at the intersection of multi-fidelity optimization (MFO) and Bayesian optimization (BO), MF BO has found a niche in solving expensive engineering design optimization problems, thanks to its advantages in incorporating physical and mathematical understandings of the problems, saving resources, addressing exploitation-exploration trade-off, considering uncertainty, and processing parallel computing. The increasing number of works dedicated to MF BO suggests the need for a comprehensive review of this advanced optimization technique. In this paper, we survey recent developments of two essential ingredients of MF BO: Gaussian process (GP) based MF surrogates and acquisition functions. We first categorize the existing MF modeling methods and MFO strategies to locate MF BO in a large family of surrogate-based optimization and MFO algorithms. We then exploit the common properties shared between the methods from each ingredient of MF BO to describe important GP-based MF surrogate models and review various acquisition functions. By doing so, we expect to provide a structured understanding of MF BO. Finally, we attempt to reveal important aspects that require further research for applications of MF BO in solving intricate yet important design optimization problems, including constrained optimization, high-dimensional optimization, optimization under uncertainty, and multi-objective optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13050v3</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.2514/1.J063812</arxiv:DOI>
      <arxiv:journal_reference>AIAA Journal 63:6 (2025) 2286-2322</arxiv:journal_reference>
      <dc:creator>Bach Do, Ruda Zhang</dc:creator>
    </item>
    <item>
      <title>Rapid prediction of cardiac activation in the left ventricle with geometric deep learning: a step towards cardiac resynchronization therapy planning</title>
      <link>https://arxiv.org/abs/2506.08987</link>
      <description>arXiv:2506.08987v3 Announce Type: replace 
Abstract: Cardiac resynchronization therapy (CRT) is a common intervention for patients with dyssynchronous heart failure, yet approximately one-third of recipients fail to respond, partly due to suboptimal lead placement. Identifying optimal pacing sites remains challenging, largely due to patient-specific anatomical variability and limitations of current individualized planning strategies. In a step toward an in-silico approach, we develop two geometric deep learning models, based on graph neural network (GNN) and geometry-informed neural operator (GINO), to predict activation time maps on left ventricular (LV) geometries in real time. Trained on a large dataset generated from finite-element simulations spanning a wide range of synthetic LV shapes, pacing site configurations, and tissue conductivities, the GINO model outperforms the GNN on synthetic cases (1.38% vs 2.44% error), while both demonstrate comparable performance on real-world LV geometries (GINO: 4.79% vs GNN: 4.07%). Using the trained models, we develop a workflow to identify an optimal pacing site on the LV from a given activation time map and show that both models can robustly recover ground-truth subject-specific parameters from noisy inputs. In conjunction with an interactive web-based interface (https://dcsim.egr.msu.edu/), this study shows potential and motivates future extension toward a clinical decision-support tool for personalized pre-procedural CRT optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08987v3</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ehsan Naghavi, Haifeng Wang, Vahid Ziaei Rad, Julius Guccione, Ghassan Kassab, Vishnu Boddeti, Seungik Baek, Lik-Chuan Lee</dc:creator>
    </item>
    <item>
      <title>Value of Information-based assessment of strain-based thickness loss monitoring in ship hull structures</title>
      <link>https://arxiv.org/abs/2505.07427</link>
      <description>arXiv:2505.07427v2 Announce Type: replace-cross 
Abstract: Recent advances in Structural Health Monitoring (SHM) have attracted industry interest, yet real-world applications, such as in ship structures remain scarce. Despite SHM's potential to optimise maintenance, its adoption in ships is limited due to the lack of clearly quantifiable benefits for hull maintenance. This study employs a Bayesian pre-posterior decision analysis to quantify the value of information (VoI) from SHM systems monitoring corrosion-induced thickness loss (CITL) in ship hulls, in a first-of-its-kind analysis for ship structures. We define decision-making consequence cost functions based on exceedance probabilities relative to a target CITL threshold, which can be set by the decision-maker. This introduces a practical aspect to our framework, that enables implicitly modelling the decision-maker's risk perception. We apply this framework to a large-scale, high-fidelity numerical model of a commercial vessel and examine the relative benefits of different CITL monitoring strategies, including strain-based SHM and traditional on-site inspections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07427v2</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicholas E. Silionis, Konstantinos N. Anyfantis</dc:creator>
    </item>
  </channel>
</rss>
