<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Jul 2025 04:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Alignment between Brains and AI: Evidence for Convergent Evolution across Modalities, Scales and Training Trajectories</title>
      <link>https://arxiv.org/abs/2507.01966</link>
      <description>arXiv:2507.01966v1 Announce Type: new 
Abstract: Artificial and biological systems may evolve similar computational solutions despite fundamental differences in architecture and learning mechanisms -- a form of convergent evolution. We demonstrate this phenomenon through large-scale analysis of alignment between human brain activity and internal representations of over 600 AI models spanning language and vision domains, from 1.33M to 72B parameters. Analyzing 60 million alignment measurements reveals that higher-performing models spontaneously develop stronger brain alignment without explicit neural constraints, with language models showing markedly stronger correlation (r=0.89, p&lt;7.5e-13) than vision models (r=0.53, p&lt;2.0e-44). Crucially, longitudinal analysis demonstrates that brain alignment consistently precedes performance improvements during training, suggesting that developing brain-like representations may be a necessary stepping stone toward higher capabilities. We find systematic patterns: language models exhibit strongest alignment with limbic and integrative regions, while vision models show progressive alignment with visual cortices; deeper processing layers converge across modalities; and as representational scale increases, alignment systematically shifts from primary sensory to higher-order associative regions. These findings provide compelling evidence that optimization for task performance naturally drives AI systems toward brain-like computational strategies, offering both fundamental insights into principles of intelligent information processing and practical guidance for developing more capable AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01966v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guobin Shen, Dongcheng Zhao, Yiting Dong, Qian Zhang, Yi Zeng</dc:creator>
    </item>
    <item>
      <title>Ghost in the Machine: Examining the Philosophical Implications of Recursive Algorithms in Artificial Intelligence Systems</title>
      <link>https://arxiv.org/abs/2507.01967</link>
      <description>arXiv:2507.01967v1 Announce Type: new 
Abstract: This paper investigates whether contemporary AI architectures employing deep recursion, meta-learning, and self-referential mechanisms provide evidence of machine consciousness. Integrating philosophical history, cognitive science, and AI engineering, it situates recursive algorithms within a lineage spanning Cartesian dualism, Husserlian intentionality, Integrated Information Theory, the Global Workspace model, and enactivist perspectives. The argument proceeds through textual analysis, comparative architecture review, and synthesis of neuroscience findings on integration and prediction. Methodologically, the study combines conceptual analysis, case studies, and normative risk assessment informed by phenomenology and embodied cognition. Technical examples, including transformer self-attention, meta-cognitive agents, and neuromorphic chips, illustrate how functional self-modeling can arise without subjective experience. By distinguishing functional from phenomenal consciousness, the paper argues that symbol grounding, embodiment, and affective qualia remain unresolved barriers to attributing sentience to current AI. Ethical analysis explores risks of premature anthropomorphism versus neglect of future sentient systems; legal implications include personhood, liability, authorship, and labor impacts. Future directions include quantum architectures, embodied robotics, unsupervised world modeling, and empirical tests for non-biological phenomenality. The study reframes the "hard problem" as a graded and increasingly testable phenomenon, rather than a metaphysical impasse. It concludes that recursive self-referential design enhances capability but does not entail consciousness or justify moral status. Keywords: Recursive algorithms; self-reference; machine consciousness; AI ethics; AI consciousness</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01967v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Llewellin RG Jegels</dc:creator>
    </item>
    <item>
      <title>REMI: Reconstructing Episodic Memory During Intrinsic Path Planning</title>
      <link>https://arxiv.org/abs/2507.02064</link>
      <description>arXiv:2507.02064v1 Announce Type: new 
Abstract: Grid cells in the medial entorhinal cortex (MEC) are believed to path integrate speed and direction signals to activate at triangular grids of locations in an environment, thus implementing a population code for position. In parallel, place cells in the hippocampus (HC) fire at spatially confined locations, with selectivity tuned not only to allocentric position but also to environmental contexts, such as sensory cues. Although grid and place cells both encode spatial information and support memory for multiple locations, why animals maintain two such representations remains unclear. Noting that place representations seem to have other functional roles in intrinsically motivated tasks such as recalling locations from sensory cues, we propose that animals maintain grid and place representations together to support planning. Specifically, we posit that place cells auto-associate not only sensory information relayed from the MEC but also grid cell patterns, enabling recall of goal location grid patterns from sensory and motivational cues, permitting subsequent planning with only grid representations. We extend a previous theoretical framework for grid-cell-based planning and show that local transition rules can generalize to long-distance path forecasting. We further show that a planning network can sequentially update grid cell states toward the goal. During this process, intermediate grid activity can trigger place cell pattern completion, reconstructing experiences along the planned path. We demonstrate all these effects using a single-layer RNN that simultaneously models the HC-MEC loop and the planning subnetwork. We show that such recurrent mechanisms for grid cell-based planning, with goal recall driven by the place system, make several characteristic, testable predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02064v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhaoze Wang, Genela Morris, Dori Derdikman, Pratik Chaudhari, Vijay Balasubramanian</dc:creator>
    </item>
    <item>
      <title>NLP4Neuro: Sequence-to-sequence learning for neural population decoding</title>
      <link>https://arxiv.org/abs/2507.02264</link>
      <description>arXiv:2507.02264v1 Announce Type: new 
Abstract: Delineating how animal behavior arises from neural activity is a foundational goal of neuroscience. However, as the computations underlying behavior unfold in networks of thousands of individual neurons across the entire brain, this presents challenges for investigating neural roles and computational mechanisms in large, densely wired mammalian brains during behavior. Transformers, the backbones of modern large language models (LLMs), have become powerful tools for neural decoding from smaller neural populations. These modern LLMs have benefited from extensive pre-training, and their sequence-to-sequence learning has been shown to generalize to novel tasks and data modalities, which may also confer advantages for neural decoding from larger, brain-wide activity recordings. Here, we present a systematic evaluation of off-the-shelf LLMs to decode behavior from brain-wide populations, termed NLP4Neuro, which we used to test LLMs on simultaneous calcium imaging and behavior recordings in larval zebrafish exposed to visual motion stimuli. Through NLP4Neuro, we found that LLMs become better at neural decoding when they use pre-trained weights learned from textual natural language data. Moreover, we found that a recent mixture-of-experts LLM, DeepSeek Coder-7b, significantly improved behavioral decoding accuracy, predicted tail movements over long timescales, and provided anatomically consistent highly interpretable readouts of neuron salience. NLP4Neuro demonstrates that LLMs are highly capable of informing brain-wide neural circuit dissection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02264v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacob J. Morra, Kaitlyn E. Fouke, Kexin Hang, Zichen He, Owen Traubert, Timothy W. Dunn, Eva A. Naumann</dc:creator>
    </item>
    <item>
      <title>Nonlinear Network Reconstruction by Pairwise Time-delayed Transfer Entropy</title>
      <link>https://arxiv.org/abs/2507.02304</link>
      <description>arXiv:2507.02304v1 Announce Type: new 
Abstract: Analyzing network structural connectivity is crucial for understanding dynamics and functions of complex networks across disciplines. In many networks, structural connectivity is not observable, which requires to be inferred via causal inference methods. Among them, transfer entropy (TE) is one of the most broadly applied causality measure due to its model-free property. However, TE often faces the curse of dimensionality in high-dimensional probability estimation, and the relation between the inferred causal connectivity and the underlying structural connectivity remains poorly understood. Here we address these issues by proposing a pairwise time-delayed transfer entropy (PTD-TE) method. We theoretically establish a quadratic relationship between PTD-TE values and node coupling strengths, and demonstrate its immunity to dimensionality issues and broad applicability. Tests on biological neuronal networks, nonlinear physical systems, and electrophysiological data show PTD-TE achieves consistent, high-performance reconstructions. Compared to a bunch of existing approaches for network connectivity reconstruction, PTD-TE outperforms these methods across various network systems in accuracy and robustness against noise. Our framework provides a scalable, model-agnostic tool for structural connectivity inference in nonlinear real-world networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02304v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kai Chen, Zhong-qi K. Tian, Yifei Chen, Songting Li, Douglas Zhou</dc:creator>
    </item>
    <item>
      <title>Network structural change point detection and reconstruction for balanced neuronal networks</title>
      <link>https://arxiv.org/abs/2507.02450</link>
      <description>arXiv:2507.02450v1 Announce Type: new 
Abstract: Understanding brain dynamics and functions critically depends on knowledge of the network connectivity among neurons. However, the complexity of brain structural connectivity, coupled with continuous modifications driven by synaptic plasticity, makes its direct experimental measurement particularly challenging. Conventional connectivity inference methods based on neuronal recordings often assumes a static underlying structural connectivity and requires stable statistical features of neural activities, making them unsuitable for reconstructing structural connectivity that undergoes changes. To fulfill the needs of reconstructing networks undergoing potential structural changes, we propose a unified network reconstruction framework that combines connectivity-induced change point detection (CPD) with pairwise time-delayed correlation coefficient (TDCC) method. For general neuronal networks in balanced regimes, we develop a theoretical analysis for discriminating changes in structural connectivity based on the fluctuation of neuronal voltage time series. We then demonstrate a pairwise TDCC method to reconstruct the network using spike train recordings segmented at the detected change points. We show the effectiveness of our CPD-TDCC network reconstruction using large-scale network simulations with multiple neuronal models. Crucially, our method accommodates networks with changes in both network topologies and synaptic coupling strengths while retaining accuracy even with sparsely sampled subnetwork data, achieving a critical advancement for practical applications in real experimental situations. Our CPD-TDCC framework addresses the critical gap in network reconstruction by accounting connectivity-induced changes points, potentially offering a valuable tool for studying structure and dynamics in the cortical brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02450v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kai Chen, Mingzhang Wang, Songting Li, Douglas Zhou</dc:creator>
    </item>
    <item>
      <title>A Dopamine-Serotonin Theory of Consciousness</title>
      <link>https://arxiv.org/abs/2507.02614</link>
      <description>arXiv:2507.02614v1 Announce Type: new 
Abstract: This work presents a comprehensive theory of consciousness grounded in mathematical formalism and supported by clinical data analysis. The framework developed herein demonstrates that consciousness exists as a continuous, non-monotonic function across a high-dimensional neurochemical space, with dopamine serving as the primary intensity regulator and serotonin (5-HT2A) as the complexity modulator. This work offers mechanistic explanations for the full spectrum of conscious states, from deep sleep and psychosis to the ultimate collapse in neural death. The theory explains paradoxical phenomena such as prefrontal cortex hypoactivity during seizures, the evolutionary persistence of psychosis-prone individuals, and why controlled administration of classical 5-HT2A agonists shows a comparatively low incidence of serious medical events (&lt; 0.01 % in modern clinical trials), while dopaminergic excess proves rapidly lethal. The framework is tested using 70,290 sleep nights from 242 Parkinson's disease patients, using disease severity (UPDRS) as a proxy for system integrity and medication (LEDD) as a proxy for dopaminergic input. The analysis reveals a significant LEDD x UPDRS interaction (beta=-1.7, p&lt;.0001), confirming the model's prediction of state-dependent, non-linear dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02614v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.OT</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diogo Sousa</dc:creator>
    </item>
    <item>
      <title>What Neuroscience Can Teach AI About Learning in Continuously Changing Environments</title>
      <link>https://arxiv.org/abs/2507.02103</link>
      <description>arXiv:2507.02103v1 Announce Type: cross 
Abstract: Modern AI models, such as large language models, are usually trained once on a huge corpus of data, potentially fine-tuned for a specific task, and then deployed with fixed parameters. Their training is costly, slow, and gradual, requiring billions of repetitions. In stark contrast, animals continuously adapt to the ever-changing contingencies in their environments. This is particularly important for social species, where behavioral policies and reward outcomes may frequently change in interaction with peers. The underlying computational processes are often marked by rapid shifts in an animal's behaviour and rather sudden transitions in neuronal population activity. Such computational capacities are of growing importance for AI systems operating in the real world, like those guiding robots or autonomous vehicles, or for agentic AI interacting with humans online. Can AI learn from neuroscience? This Perspective explores this question, integrating the literature on continual and in-context learning in AI with the neuroscience of learning on behavioral tasks with shifting rules, reward probabilities, or outcomes. We will outline an agenda for how specifically insights from neuroscience may inform current developments in AI in this area, and - vice versa - what neuroscience may learn from AI, contributing to the evolving field of NeuroAI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02103v1</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Durstewitz, Bruno Averbeck, Georgia Koppe</dc:creator>
    </item>
    <item>
      <title>Towards a (meta-)mathematical theory of consciousness: universal (mapping) properties of experience</title>
      <link>https://arxiv.org/abs/2412.12179</link>
      <description>arXiv:2412.12179v2 Announce Type: replace 
Abstract: Conscious experience permeates our daily lives, yet general consensus on a theory of consciousness remains elusive. In the face of such difficulty, an alternative strategy is to address a more general (meta-level) version of the problem for insights into the original problem at hand. Category theory was developed for this purpose, i.e. as an axiomatic (meta-)mathematical theory for comparison of mathematical structures, and so affords a (formally) formal approach towards a theory of consciousness. In this way, category theory is used for comparison with Information Integration Theory (IIT) as a supposed axiomatic theory of consciousness, which says that every conscious state involves six axiomatic properties: the IIT axioms for consciousness. All six axioms are shown to follow from the categorical notion of a universal mapping property: a unique-existence condition for all instances in the domain of interest. Accordingly, this categorical approach affords a formal basis for further development of a (meta-)mathematical theory of consciousness, whence the slogan, ``Consciousness is a universal property.''</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12179v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven Phillips, Naotsugu Tsuchiya</dc:creator>
    </item>
    <item>
      <title>Building functional and mechanistic models of cortical computation based on canonical cell type connectivity</title>
      <link>https://arxiv.org/abs/2504.03031</link>
      <description>arXiv:2504.03031v2 Announce Type: replace 
Abstract: Neuronal circuits of the cerebral cortex are the structural basis of mammalian cognition. The same qualitative components and connectivity motifs are repeated across functionally specialized cortical areas and mammalian species, suggesting a single underlying algorithmic motif. Here, we propose a perspective on current knowledge of the cortical structure, from which we extract two core principles for computational modeling. The first principle is that cortical cell types fulfill distinct computational roles. The second principle is that cortical connectivity can be efficiently characterized by only a few canonical blueprints of connectivity between cell types. Starting with these two foundational principles, we outline a general framework for building functional and mechanistic models of cortical circuits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03031v2</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arno Granier, Katharina A Wilmes, Mihai A Petrovici, Walter Senn</dc:creator>
    </item>
    <item>
      <title>Reduced Efficiency in the Attentional Network During Distractor Suppression in Mild Cognitive Impairment</title>
      <link>https://arxiv.org/abs/2507.01433</link>
      <description>arXiv:2507.01433v2 Announce Type: replace 
Abstract: Mild Cognitive Impairment (MCI) is a critical transitional stage between normal cognitive aging and dementia, making its early detection essential. This study investigates the neural mechanisms of distractor suppression in MCI patients using EEG and behavioral data during an attention-cueing Eriksen flanker task. A cohort of 56 MCIs and 26 healthy controls (HCs) performed tasks with congruent and incongruent stimuli of varying saliency levels. During these tasks, EEG data were analyzed for alpha band coherence's functional connectivity, focusing on Global Efficiency (GE), while Reaction Time (RT) and Hit Rate (HR) were also collected.
  Our findings reveal significant interactions between congruency, saliency, and cognitive status on GE, RT, and HR. In HCs, congruent conditions resulted in higher GE (p = 0.0114, multivariate t-distribution correction, MVT), faster RTs (p &lt; 0.0001, MVT), and higher HRs (p &lt; 0.0001, MVT) compared to incongruent conditions. HCs also showed increased GE in salient conditions for incongruent trials (p = 0.0406, MVT). MCIs exhibited benefits from congruent conditions with shorter RTs and higher HRs (both p &lt; 0.0001, MVT) compared to incongruent conditions but showed reduced adaptability in GE, with no significant GE differences between conditions.
  These results highlight the potential of alpha band coherence and GE as early markers for cognitive impairment. By integrating GE, RT, and HR, this study provides insights into the interplay between neural efficiency, processing speed, and task accuracy. This approach offers valuable insights into cognitive load management and interference effects, indicating benefits for interventions aimed at improving attentional control and processing speed in MCIs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01433v2</guid>
      <category>q-bio.NC</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jatupong Oboun, Piyanon Charoenpoonpanich, Anna Raksapatcharawong, Chaipat Chunharas, Itthi Chatnuntawech, Chainarong Amornbunchornvej, Sirawaj Itthipuripat</dc:creator>
    </item>
    <item>
      <title>Spot solutions to a neural field equation on oblate spheroids</title>
      <link>https://arxiv.org/abs/2504.16342</link>
      <description>arXiv:2504.16342v2 Announce Type: replace-cross 
Abstract: Understanding the dynamics of excitation patterns in neural fields is an important topic in neuroscience. Neural field equations are mathematical models that describe the excitation dynamics of interacting neurons to perform the theoretical analysis. Although many analyses of neural field equations focus on the effect of neuronal interactions on the flat surface, the geometric constraint of the dynamics is also an attractive topic when modeling organs such as the brain. This paper reports pattern dynamics in a neural field equation defined on spheroids as model curved surfaces. We treat spot solutions as localized patterns and discuss how the geometric properties of the curved surface change their properties. To analyze spot patterns on spheroids with small flattening, we first construct exact stationary spot solutions on the spherical surface and reveal their stability. We then extend the analysis to show the existence and stability of stationary spot solutions in the spheroidal case. One of our theoretical results is the derivation of a stability criterion for stationary spot solutions localized at poles on oblate spheroids. The criterion determines whether a spot solution remains at a pole or moves away. Finally, we conduct numerical simulations to discuss the dynamics of spot solutions with the insight of our theoretical predictions. Our results show that the dynamics of spot solutions depend on the curved surface and the coordination of neural interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16342v2</guid>
      <category>nlin.PS</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 04 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroshi Ishii, Riku Watanabe</dc:creator>
    </item>
  </channel>
</rss>
