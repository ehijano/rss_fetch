<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Sep 2025 03:33:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>C-QUERI: Congressional Questions, Exchanges, and Responses in Institutions Dataset</title>
      <link>https://arxiv.org/abs/2509.21548</link>
      <description>arXiv:2509.21548v1 Announce Type: new 
Abstract: Questions in political interviews and hearings serve strategic purposes beyond information gathering including advancing partisan narratives and shaping public perceptions. However, these strategic aspects remain understudied due to the lack of large-scale datasets for studying such discourse. Congressional hearings provide an especially rich and tractable site for studying political questioning: Interactions are structured by formal rules, witnesses are obliged to respond, and members with different political affiliations are guaranteed opportunities to ask questions, enabling comparisons of behaviors across the political spectrum.
  We develop a pipeline to extract question-answer pairs from unstructured hearing transcripts and construct a novel dataset of committee hearings from the 108th--117th Congress. Our analysis reveals systematic differences in questioning strategies across parties, by showing the party affiliation of questioners can be predicted from their questions alone. Our dataset and methods not only advance the study of congressional politics, but also provide a general framework for analyzing question-answering across interview-like settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21548v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manjari Rudra, Daniel Magleby, Sujoy Sikdar</dc:creator>
    </item>
    <item>
      <title>Developing Strategies to Increase Capacity in AI Education</title>
      <link>https://arxiv.org/abs/2509.21713</link>
      <description>arXiv:2509.21713v1 Announce Type: new 
Abstract: Many institutions are currently grappling with teaching artificial intelligence (AI) in the face of growing demand and relevance in our world. The Computing Research Association (CRA) has conducted 32 moderated virtual roundtable discussions of 202 experts committed to improving AI education. These discussions slot into four focus areas: AI Knowledge Areas and Pedagogy, Infrastructure Challenges in AI Education, Strategies to Increase Capacity in AI Education, and AI Education for All. Roundtables were organized around institution type to consider the particular goals and resources of different AI education environments. We identified the following high-level community needs to increase capacity in AI education. A significant digital divide creates major infrastructure hurdles, especially for smaller and under-resourced institutions. These challenges manifest as a shortage of faculty with AI expertise, who also face limited time for reskilling; a lack of computational infrastructure for students and faculty to develop and test AI models; and insufficient institutional technical support. Compounding these issues is the large burden associated with updating curricula and creating new programs. To address the faculty gap, accessible and continuous professional development is crucial for faculty to learn about AI and its ethical dimensions. This support is particularly needed for under-resourced institutions and must extend to faculty both within and outside of computing programs to ensure all students have access to AI education. We have compiled and organized a list of resources that our participant experts mentioned throughout this study. These resources contribute to a frequent request heard during the roundtables: a central repository of AI education resources for institutions to freely use across higher education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21713v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noah Q. Cowit, Sri Yash Tadimalla, Stephanie T. Jones, Mary Lou Maher, Tracy Camp, Enrico Pontelli</dc:creator>
    </item>
    <item>
      <title>Malaysia's AI-Driven Education Landscape: Policies, Applications, and Comparative Insights for a Digital Future</title>
      <link>https://arxiv.org/abs/2509.21858</link>
      <description>arXiv:2509.21858v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) is transforming education globally, and Malaysia is leveraging this potential through strategic policies to enhance learning and prepare students for a digital future. This article explores Malaysia's AI-driven education landscape, emphasising the National Artificial Intelligence Roadmap 2021-2025 and the Digital Education Policy. Employing a policy-driven analysis, it maps AI applications in pedagogy, curriculum design, administration, and teacher training across primary to tertiary levels. The study evaluates national strategies, identifies challenges like digital divides and ethical concerns, and conducts a comparative analysis with the United Kingdom, the United States, China, and India to draw best practices in AI policy and digital transformation. Findings highlight Malaysia's progress in AI literacy and personalised learning, alongside gaps in rural infrastructure and teacher readiness. Recommendations include strengthening governance, investing in equitable infrastructure, and fostering public-private partnerships. Targeting researchers, policymakers, and educators, this study informs Malaysia's path to becoming a regional leader in AI-driven education and contributes to global comparative education discourse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21858v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fadhilah Jamaluddin, Ahmad Hakiim Jamaluddin, Faridzah Jamaluddin, Faathirah Jamaluddin</dc:creator>
    </item>
    <item>
      <title>Opening Knowledge Gaps Drives Scientific Progress</title>
      <link>https://arxiv.org/abs/2509.21899</link>
      <description>arXiv:2509.21899v1 Announce Type: new 
Abstract: Knowledge production is often viewed as an endogenous process in which discovery arises through the recombination of existing theories, findings, and concepts. Yet given the vast space of potential recombinations, not all are equally valuable, and identifying those that may prove most generative remains challenging. We argue that a crucial form of recombination occurs when linking concepts creates knowledge gaps-empty regions in the conceptual landscape that focus scientific attention on proximal, unexplored connections and signal promising directions for future research. Using computational topology, we develop a method to systematically identify knowledge gaps in science at scale. Applying this approach to millions of articles from Microsoft Academic Graph (n = 34,363,623) over a 120-year period (1900-2020), we uncover papers that create topological gaps in concept networks, tracking how these gap-opening works reshape the scientific knowledge landscape. Our results indicate that gap-opening papers are more likely to rank among the most highly cited works (top 1-20%) compared with papers that do not introduce novel concept pairings. In contrast, papers that introduce novel combinations without opening gaps are not more likely to rank in the top 1% for citation counts, and are even less likely than baseline papers to appear in the top 5% to 20%. Our findings also suggest that gap-opening papers are more disruptive, highlighting their generative role in stimulating new directions for scientific inquiry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21899v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kara Kedrick, Wenlong Yang, Thomas Gebhart, Yang Wang, Russell J. Funk</dc:creator>
    </item>
    <item>
      <title>From Superficial Outputs to Superficial Learning: Risks of Large Language Models in Education</title>
      <link>https://arxiv.org/abs/2509.21972</link>
      <description>arXiv:2509.21972v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are transforming education by enabling personalization, feedback, and knowledge access, while also raising concerns about risks to students and learning systems. Yet empirical evidence on these risks remains fragmented. This paper presents a systematic review of 70 empirical studies across computer science, education, and psychology. Guided by four research questions, we examine: (i) which applications of LLMs in education have been most frequently explored; (ii) how researchers have measured their impact; (iii) which risks stem from such applications; and (iv) what mitigation strategies have been proposed. We find that research on LLMs clusters around three domains: operational effectiveness, personalized applications, and interactive learning tools. Across these, model-level risks include superficial understanding, bias, limited robustness, anthropomorphism, hallucinations, privacy concerns, and knowledge constraints. When learners interact with LLMs, these risks extend to cognitive and behavioural outcomes, including reduced neural activity, over-reliance, diminished independent learning skills, and a loss of student agency. To capture this progression, we propose an LLM-Risk Adapted Learning Model that illustrates how technical risks cascade through interaction and interpretation to shape educational outcomes. As the first synthesis of empirically assessed risks, this review provides a foundation for responsible, human-centred integration of LLMs in education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21972v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iris Delikoura (May), Yi. R (May),  Fung, Pan Hui</dc:creator>
    </item>
    <item>
      <title>AI Ethics Education in India: A Syllabus-Level Review of Computing Courses</title>
      <link>https://arxiv.org/abs/2509.22329</link>
      <description>arXiv:2509.22329v1 Announce Type: new 
Abstract: The pervasive integration of artificial intelligence (AI) across domains such as healthcare, governance, finance, and education has intensified scrutiny of its ethical implications, including algorithmic bias, privacy risks, accountability, and societal impact. While ethics has received growing attention in computer science (CS) education more broadly, the specific pedagogical treatment of {AI ethics} remains under-examined. This study addresses that gap through a large-scale analysis of 3,395 publicly accessible syllabi from CS and allied areas at leading Indian institutions. Among them, only 75 syllabi (2.21%) included any substantive AI ethics content. Three key findings emerged: (1) AI ethics is typically integrated as a minor module within broader technical courses rather than as a standalone course; (2) ethics coverage is often limited to just one or two instructional sessions; and (3) recurring topics include algorithmic fairness, privacy and data governance, transparency, and societal impact. While these themes reflect growing awareness, current curricular practices reveal limited depth and consistency. This work highlights both the progress and the gaps in preparing future technologists to engage meaningfully with the ethical dimensions of AI, and it offers suggestions to strengthen the integration of AI ethics within computing curricula.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22329v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anshu M Mittal, P D Parthasarathy, Swaroop Joshi</dc:creator>
    </item>
    <item>
      <title>MakOne: Behavioural Data of University Students' Smart Devices in Uganda</title>
      <link>https://arxiv.org/abs/2509.22334</link>
      <description>arXiv:2509.22334v1 Announce Type: new 
Abstract: Understanding student behaviour in higher education is essential for improving academic performance, supporting mental well-being, and informing institutional policies. However, most existing behavioural datasets originate from Western institutions and overlook the unique socioeconomic and infrastructural contexts of African institutions, limiting the global applicability of resulting insights. This paper introduces MakOne, a novel multimodal dataset collected over six weeks from 72 students at Makerere University, Kampala, using iLog, a mobile sensing application. The dataset integrates passive smartphone sensor data-including location, physical activity, and screen usage-with ecological momentary assessments (EMAs) that capture students' moods and daily routines. Designed to reflect the lived experiences of students in an African setting, MakOne offers a foundation for research in behaviour modeling, inclusive context-aware system design, mental health analytics, and culturally grounded educational technologies. It contributes a critical African perspective to the growing body of data-driven studies on student behaviour.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22334v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Michael Kizito, Ivan Kayongo, Hawa Nyende, Halimu Chongomweru, Lillian Muyama, Roy Alia Asiku, Alice Mugisha</dc:creator>
    </item>
    <item>
      <title>LLM-Augmented and Fair Machine Learning Framework for University Admission Prediction</title>
      <link>https://arxiv.org/abs/2509.22560</link>
      <description>arXiv:2509.22560v1 Announce Type: new 
Abstract: Universities face surging applications and heightened expectations for fairness, making accurate admission prediction increasingly vital. This work presents a comprehensive framework that fuses machine learning, deep learning, and large language model techniques to combine structured academic and demographic variables with unstructured text signals. Drawing on more than 2,000 student records, the study benchmarks logistic regression, Naive Bayes, random forests, deep neural networks, and a stacked ensemble. Logistic regression offers a strong, interpretable baseline at 89.5% accuracy, while the stacked ensemble achieves the best performance at 91.0%, with Naive Bayes and random forests close behind. To probe text integration, GPT-4-simulated evaluations of personal statements are added as features, yielding modest gains but demonstrating feasibility for authentic essays and recommendation letters. Transparency is ensured through feature-importance visualizations and fairness audits. The audits reveal a 9% gender gap (67% male vs. 76% female) and an 11% gap by parental education, underscoring the need for continued monitoring. The framework is interpretable, fairness-aware, and deployable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22560v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Abbadi, Yassine Himeur, Shadi Atalla, Dahlia Mansoor, Wathiq Mansoor</dc:creator>
    </item>
    <item>
      <title>A Systematic Review: Affective Perception on Urban Facades</title>
      <link>https://arxiv.org/abs/2509.22599</link>
      <description>arXiv:2509.22599v1 Announce Type: new 
Abstract: Architectural facades critically shape affective perception in urban environments. Here, affect is understood as a multidimensional psychological construct encompassing valence (pleasure-displeasure) and arousal (activation-deactivation). Despite growing interest in affective responses to the built environment, the affective impact of urban architectural facades remains under-theorized. This study conducts a systematic review of 61 works, guided by the PRISMA framework, to identify which facade attributes most strongly predict affective responses operationalized as valence and arousal. Through multi-scalar synthesis and knowledge mapping, the review highlights complexity, materiality, symmetry, and bibliophilic integration as consistent predictors of affective perception across urban, building, and detail levels. Computational tools such as eye-tracking, CNN-based analysis, and parametric modeling are increasingly employed, yet remain fragmented and often overlook intangible dimensions like narrative coherence and cultural symbolism. By consolidating cross-disciplinary evidence, this review proposes a theoretical model linking physical design features to affective outcomes, and identifies methodological gaps, particularly the lack of integrative, mixed-method approaches. The findings offer a foundation for affect-aware facade design, advancing evidence-based strategies to support psychological well-being in urban contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22599v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenxi Wang, Haining Ding, Michal Gath-Morad</dc:creator>
    </item>
    <item>
      <title>Training-Free Multimodal Deepfake Detection via Graph Reasoning</title>
      <link>https://arxiv.org/abs/2509.21774</link>
      <description>arXiv:2509.21774v1 Announce Type: cross 
Abstract: Multimodal deepfake detection (MDD) aims to uncover manipulations across visual, textual, and auditory modalities, thereby reinforcing the reliability of modern information systems. Although large vision-language models (LVLMs) exhibit strong multimodal reasoning, their effectiveness in MDD is limited by challenges in capturing subtle forgery cues, resolving cross-modal inconsistencies, and performing task-aligned retrieval. To this end, we propose Guided Adaptive Scorer and Propagation In-Context Learning (GASP-ICL), a training-free framework for MDD. GASP-ICL employs a pipeline to preserve semantic relevance while injecting task-aware knowledge into LVLMs. We leverage an MDD-adapted feature extractor to retrieve aligned image-text pairs and build a candidate set. We further design the Graph-Structured Taylor Adaptive Scorer (GSTAS) to capture cross-sample relations and propagate query-aligned signals, producing discriminative exemplars. This enables precise selection of semantically aligned, task-relevant demonstrations, enhancing LVLMs for robust MDD. Experiments on four forgery types show that GASP-ICL surpasses strong baselines, delivering gains without LVLM fine-tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21774v1</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxin Liu, Fei Wang, Kun Li, Yiqi Nie, Junjie Chen, Yanyan Wei, Zhangling Duan, Zhaohong Jia</dc:creator>
    </item>
    <item>
      <title>The Dark Art of Financial Disguise in Web3: Money Laundering Schemes and Countermeasures</title>
      <link>https://arxiv.org/abs/2509.21831</link>
      <description>arXiv:2509.21831v1 Announce Type: cross 
Abstract: The rise of Web3 and Decentralized Finance (DeFi) has enabled borderless access to financial services empowered by smart contracts and blockchain technology. However, the ecosystem's trustless, permissionless, and borderless nature presents substantial regulatory challenges. The absence of centralized oversight and the technical complexity create fertile ground for financial crimes. Among these, money laundering is particularly concerning, as in the event of successful scams, code exploits, and market manipulations, it facilitates covert movement of illicit gains. Beyond this, there is a growing concern that cryptocurrencies can be leveraged to launder proceeds from drug trafficking, or to transfer funds linked to terrorism financing.
  This survey aims to outline a taxonomy of high-level strategies and underlying mechanisms exploited to facilitate money laundering in Web3. We examine how criminals leverage the pseudonymous nature of Web3, alongside weak regulatory frameworks, to obscure illicit financial activities. Our study seeks to bridge existing knowledge gaps on laundering schemes, identify open challenges in the detection and prevention of such activities, and propose future research directions to foster a more transparent Web3 financial ecosystem -- offering valuable insights for researchers, policymakers, and industry practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21831v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hesam Sarkhosh, Uzma Maroof, Diogo Barradas</dc:creator>
    </item>
    <item>
      <title>"In my defense, only three hours on Instagram": Designing Toward Digital Self-Awareness and Wellbeing</title>
      <link>https://arxiv.org/abs/2509.21860</link>
      <description>arXiv:2509.21860v1 Announce Type: cross 
Abstract: Screen use pervades daily life, shaping work, leisure, and social connections while raising concerns for digital wellbeing. Yet, reducing screen time alone risks oversimplifying technology's role and neglecting its potential for meaningful engagement. We posit self-awareness -- reflecting on one's digital behavior -- as a critical pathway to digital wellbeing. We developed WellScreen, a lightweight probe that scaffolds daily reflection by asking people to estimate and report smartphone use. In a two-week deployment (N=25), we examined how discrepancies between estimated and actual usage shaped digital awareness and wellbeing. Participants often underestimated productivity and social media while overestimating entertainment app use. They showed a 10% improvement in positive affect, rating WellScreen as moderately useful. Interviews revealed that structured reflection supported recognition of patterns, adjustment of expectations, and more intentional engagement with technology. Our findings highlight the promise of lightweight reflective interventions for supporting self-awareness and intentional digital engagement, offering implications for designing digital wellbeing tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21860v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karthik S. Bhat, Jiayue Melissa Shi, Wenxuan Song, Dong Whi Yoo, Koustuv Saha</dc:creator>
    </item>
    <item>
      <title>Extracting Actionable Insights from Building Energy Data using Vision LLMs on Wavelet and 3D Recurrence Representations</title>
      <link>https://arxiv.org/abs/2509.21934</link>
      <description>arXiv:2509.21934v1 Announce Type: cross 
Abstract: The analysis of complex building time-series for actionable insights and recommendations remains challenging due to the nonlinear and multi-scale characteristics of energy data. To address this, we propose a framework that fine-tunes visual language large models (VLLMs) on 3D graphical representations of the data. The approach converts 1D time-series into 3D representations using continuous wavelet transforms (CWTs) and recurrence plots (RPs), which capture temporal dynamics and localize frequency anomalies. These 3D encodings enable VLLMs to visually interpret energy-consumption patterns, detect anomalies, and provide recommendations for energy efficiency. We demonstrate the framework on real-world building-energy datasets, where fine-tuned VLLMs successfully monitor building states, identify recurring anomalies, and generate optimization recommendations. Quantitatively, the Idefics-7B VLLM achieves validation losses of 0.0952 with CWTs and 0.1064 with RPs on the University of Sharjah energy dataset, outperforming direct fine-tuning on raw time-series data (0.1176) for anomaly detection. This work bridges time-series analysis and visualization, providing a scalable and interpretable framework for energy analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.21934v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amine Bechar, Adel Oulefki, Abbes Amira, Fatih Kurogollu, Yassine Himeur</dc:creator>
    </item>
    <item>
      <title>Chronic Stress, Immune Suppression, and Cancer Occurrence: Unveiling the Connection using Survey Data and Predictive Models</title>
      <link>https://arxiv.org/abs/2509.22275</link>
      <description>arXiv:2509.22275v1 Announce Type: cross 
Abstract: Chronic stress was implicated in cancer occurrence, but a direct causal connection has not been consistently established. Machine learning and causal modeling offer opportunities to explore complex causal interactions between psychological chronic stress and cancer occurrences. We developed predictive models employing variables from stress indicators, cancer history, and demographic data from self-reported surveys, unveiling the direct and immune suppression mitigated connection between chronic stress and cancer occurrence. The models were corroborated by traditional statistical methods. Our findings indicated significant causal correlations between stress frequency, stress level and perceived health impact, and cancer incidence. Although stress alone showed limited predictive power, integrating socio-demographic and familial cancer history data significantly enhanced model accuracy. These results highlight the multidimensional nature of cancer risk, with stress emerging as a notable factor alongside genetic predisposition. These findings strengthen the case for addressing chronic stress as a modifiable cancer risk factor, supporting its integration into personalized prevention strategies and public health interventions to reduce cancer incidence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22275v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Teddy Lazebnik, Vered Aharonson</dc:creator>
    </item>
    <item>
      <title>What Is The Political Content in LLMs' Pre- and Post-Training Data?</title>
      <link>https://arxiv.org/abs/2509.22367</link>
      <description>arXiv:2509.22367v1 Announce Type: cross 
Abstract: Large language models (LLMs) are known to generate politically biased text, yet how such biases arise remains unclear. A crucial step toward answering this question is the analysis of training data, whose political content remains largely underexplored in current LLM research. To address this gap, we present in this paper an analysis of the pre- and post-training corpora of OLMO2, the largest fully open-source model released together with its complete dataset. From these corpora, we draw large random samples, automatically annotate documents for political orientation, and analyze their source domains and content. We then assess how political content in the training data correlates with models' stance on specific policy issues. Our analysis shows that left-leaning documents predominate across datasets, with pre-training corpora containing significantly more politically engaged content than post-training data. We also find that left- and right-leaning documents frame similar topics through distinct values and sources of legitimacy. Finally, the predominant stance in the training data strongly correlates with models' political biases when evaluated on policy issues. These findings underscore the need to integrate political content analysis into future data curation pipelines as well as in-depth documentation of filtering strategies for transparency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22367v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanise Ceron, Dmitry Nikolaev, Dominik Stammbach, Debora Nozza</dc:creator>
    </item>
    <item>
      <title>Context-Specific Instruction: A Longitudinal Study on Debugging Skill Acquisition and Retention for Novice Programmers</title>
      <link>https://arxiv.org/abs/2509.22420</link>
      <description>arXiv:2509.22420v1 Announce Type: cross 
Abstract: Bug localization is a critical skill, yet novices often lack systematic approaches. Prior work tested abstract guidelines and general concrete steps; the impact of context-specific instruction is unclear. We ran an eight-week longitudinal study with four conditions: no instruction (G1), abstract guidelines (G2), concrete steps (G3), and our context-specific instruction that pairs concrete bug-localization steps with problem-specific details (G4). Forty-four undergraduates participated; 41 completed all five sessions (S1-S5). Each session included 2-3 debugging tasks to identify the minimal code element containing a seeded logical fault. We measured correctness (binary), time to completion, self-perceived scores (stress, difficulty, satisfaction, and strategy adherence). G4 achieved higher correctness and shorter time to completion: it reached 80% correctness after one session (vs. 20-44% for other groups) and maintained 80% after three weeks, outperforming all groups (p &lt; 0.05); its time to completion stabilized at 13-15 minutes in S1, whereas other groups took 2-3 sessions to stabilize at 22-27 minutes. Qualitative responses showed lower stress and higher satisfaction in G4, with participants internalizing strategies via contextual examples. We conclude that context-specific instruction yields faster skill acquisition and stronger retention than abstract guidelines or context-agnostic steps. Even 1-2 sessions produced significant gains, while extended practice optimized and stabilized performance. Integrating contextual examples with abstract principles may bridge theory-practice gaps in bug-localization education and provide a more equitable path for novices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22420v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyi Zhang, Devjeet Roy, Venera Arnaoudova</dc:creator>
    </item>
    <item>
      <title>Mental Health Impacts of AI Companions: Triangulating Social Media Quasi-Experiments, User Perspectives, and Relational Theory</title>
      <link>https://arxiv.org/abs/2509.22505</link>
      <description>arXiv:2509.22505v1 Announce Type: cross 
Abstract: AI-powered companion chatbots (AICCs) such as Replika are increasingly popular, offering empathetic interactions, yet their psychosocial impacts remain unclear. We examined how engaging with AICCs shaped wellbeing and how users perceived these experiences. First, we conducted a large-scale quasi-experimental study of longitudinal Reddit data, applying stratified propensity score matching and Difference-in-Differences regression. Findings revealed mixed effects -- greater affective and grief expression, readability, and interpersonal focus, alongside increases in language about loneliness and suicidal ideation. Second, we complemented these results with 15 semi-structured interviews, which we thematically analyzed and contextualized using Knapp's relationship development model. We identified trajectories of initiation, escalation, and bonding, wherein AICCs provided emotional validation and social rehearsal but also carried risks of over-reliance and withdrawal. Triangulating across methods, we offer design implications for AI companions that scaffold healthy boundaries, support mindful engagement, support disclosure without dependency, and surface relationship stages -- maximizing psychosocial benefits while mitigating risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22505v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhao Yuan, Jiaxun Zhang, Talayeh Aledavood, Renwen Zhang, Koustuv Saha</dc:creator>
    </item>
    <item>
      <title>Does AI Coaching Prepare us for Workplace Negotiations?</title>
      <link>https://arxiv.org/abs/2509.22545</link>
      <description>arXiv:2509.22545v1 Announce Type: cross 
Abstract: Workplace negotiations are undermined by psychological barriers, which can even derail well-prepared tactics. AI offers personalized and always -- available negotiation coaching, yet its effectiveness for negotiation preparedness remains unclear. We built Trucey, a prototype AI coach grounded in Brett's negotiation model. We conducted a between-subjects experiment (N=267), comparing Trucey, ChatGPT, and a traditional negotiation Handbook, followed by in-depth interviews (N=15). While Trucey showed the strongest reductions in fear relative to both comparison conditions, the Handbook outperformed both AIs in usability and psychological empowerment. Interviews revealed that the Handbook's comprehensive, reviewable content was crucial for participants' confidence and preparedness. In contrast, although participants valued AI's rehearsal capability, its guidance often felt verbose and fragmented -- delivered in bits and pieces that required additional effort -- leaving them uncertain or overwhelmed. These findings challenge assumptions of AI superiority and motivate hybrid designs that integrate structured, theory-driven content with targeted rehearsal, clear boundaries, and adaptive scaffolds to address psychological barriers and support negotiation preparedness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22545v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Veda Duddu, Jash Rajesh Parekh, Andy Mao, Hanyi Min, Ziang Xiao, Vedant Das Swain, Koustuv Saha</dc:creator>
    </item>
    <item>
      <title>Bridging Technical Capability and User Accessibility: Off-grid Civilian Emergency Communication</title>
      <link>https://arxiv.org/abs/2509.22568</link>
      <description>arXiv:2509.22568v2 Announce Type: cross 
Abstract: During large-scale crises disrupting cellular and Internet infrastructure, civilians lack reliable methods for communication, aid coordination, and access to trustworthy information. This paper presents a unified emergency communication system integrating a low-power, long-range network with a crisis-oriented smartphone application, enabling decentralized and off-grid civilian communication. Unlike previous solutions separating physical layer resilience from user layer usability, our design merges these aspects into a cohesive crisis-tailored framework.
  The system is evaluated in two dimensions: communication performance and application functionality. Field experiments in urban Z\"urich demonstrate that the 868 MHz band, using the LongFast configuration, achieves a communication range of up to 1.2 km with 92% Packet Delivery Ratio, validating network robustness under real-world infrastructure degraded conditions. In parallel, a purpose-built mobile application featuring peer-to-peer messaging, identity verification, and community moderation was evaluated through a requirements-based analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22568v2</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karim Khamaisi, Oliver Kamer, Bruno Rodrigues, Jan von der Assen, Burkhard Stiller</dc:creator>
    </item>
    <item>
      <title>Biospheric AI</title>
      <link>https://arxiv.org/abs/2401.17805</link>
      <description>arXiv:2401.17805v2 Announce Type: replace 
Abstract: The dominant paradigm in AI ethics and value alignment is highly anthropocentric. The focus of these disciplines is strictly on human values which limits the depth and breadth of their insights. Recently, attempts to expand to a sentientist perspective have been initiated. We argue that neither of these outlooks is sufficient to capture the actual complexity of the biosphere and ensure that AI does not damage it. Thus, we propose a new paradigm -- Biospheric AI that assumes an ecocentric perspective. We discuss hypothetical ways in which such an AI might be designed. Moreover, we give directions for research and application of the modern AI models that would be consistent with the biospheric interests. All in all, this work attempts to take first steps towards a comprehensive program of research that focuses on the interactions between AI and the biosphere.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17805v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marcin Korecki</dc:creator>
    </item>
    <item>
      <title>Ethical AI for Young Digital Citizens: A Call to Action on Privacy Governance</title>
      <link>https://arxiv.org/abs/2503.11947</link>
      <description>arXiv:2503.11947v3 Announce Type: replace 
Abstract: The rapid expansion of Artificial Intelligence (AI) in digital platforms used by youth has created significant challenges related to privacy, autonomy, and data protection. While AI-driven personalization offers enhanced user experiences, it often operates without clear ethical boundaries, leaving young users vulnerable to data exploitation and algorithmic biases. This paper presents a call to action for ethical AI governance, advocating for a structured framework that ensures youth-centred privacy protections, transparent data practices, and regulatory oversight. We outline key areas requiring urgent intervention, including algorithmic transparency, privacy education, parental data-sharing ethics, and accountability measures. Through this approach, we seek to empower youth with greater control over their digital identities and propose actionable strategies for policymakers, AI developers, and educators to build a fairer and more accountable AI ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11947v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Austin Shouli, Ankur Barthwal, Molly Campbell, Ajay Kumar Shrestha</dc:creator>
    </item>
    <item>
      <title>Formalising Human-in-the-Loop: Computational Reductions, Failure Modes, and Legal-Moral Responsibility</title>
      <link>https://arxiv.org/abs/2505.10426</link>
      <description>arXiv:2505.10426v2 Announce Type: replace 
Abstract: We use the notion of oracle machines and reductions from computability theory to formalise different Human-in-the-loop (HITL) setups for AI systems, distinguishing between trivial human monitoring (i.e., total functions), single endpoint human action (i.e., many-one reductions), and highly involved human-AI interaction (i.e., Turing reductions). We then proceed to show that the legal status and safety of different setups vary greatly. We present a taxonomy to categorise HITL failure modes, highlighting the practical limitations of HITL setups. We then identify omissions in UK and EU legal frameworks, which focus on HITL setups that may not always achieve the desired ethical, legal, and sociotechnical outcomes. We suggest areas where the law should recognise the effectiveness of different HITL setups and assign responsibility in these contexts, avoiding human "scapegoating". Our work shows an unavoidable trade-off between attribution of legal responsibility, and technical explainability. Overall, we show how HITL setups involve many technical design decisions, and can be prone to failures out of the humans' control. Our formalisation and taxonomy opens up a new analytic perspective on the challenges in creating HITL setups, helping inform AI developers and lawmakers on designing HITL setups to better achieve their desired outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10426v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>math.HO</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maurice Chiodo, Dennis M\"uller, Paul Siewert, Jean-Luc Wetherall, Zoya Yasmine, John Burden</dc:creator>
    </item>
    <item>
      <title>The Measurement Imbalance in Agentic AI Evaluation Undermines Industry Productivity Claims</title>
      <link>https://arxiv.org/abs/2506.02064</link>
      <description>arXiv:2506.02064v2 Announce Type: replace 
Abstract: As industry reports claim agentic AI systems deliver double-digit productivity gains and multi-trillion dollar economic potential, the validity of these claims has become critical for investment decisions, regulatory policy, and responsible technology adoption. However, this paper demonstrates that current evaluation practices for agentic AI systems exhibit a systemic imbalance that calls into question prevailing industry productivity claims. Our systematic review of 84 papers (2023--2025) reveals an evaluation imbalance where technical metrics dominate assessments (83%), while human-centered (30%), safety (53%), and economic assessments (30%) remain peripheral, with only 15% incorporating both technical and human dimensions. This measurement gap creates a fundamental disconnect between benchmark success and deployment value. We present evidence from healthcare, finance, and retail sectors where systems excelling on technical metrics failed in real-world implementation due to unmeasured human, temporal, and contextual factors. Our position is not against agentic AI's potential, but rather that current evaluation frameworks systematically privilege narrow technical metrics while neglecting dimensions critical to real-world success. We propose a balanced four-axis evaluation model and call on the community to lead this paradigm shift because benchmark-driven optimization shapes what we build. By redefining evaluation practices, we can better align industry claims with deployment realities and ensure responsible scaling of agentic systems in high-stakes domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02064v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kiana Jafari Meimandi, Gabriela Ar\'anguiz-Dias, Grace Ra Kim, Lana Saadeddin, Mykel J. Kochenderfer</dc:creator>
    </item>
    <item>
      <title>Position: Simulating Society Requires Simulating Thought</title>
      <link>https://arxiv.org/abs/2506.06958</link>
      <description>arXiv:2506.06958v2 Announce Type: replace 
Abstract: Simulating society with large language models (LLMs), we argue, requires more than generating plausible behavior; it demands cognitively grounded reasoning that is structured, revisable, and traceable. LLM-based agents are increasingly used to emulate individual and group behavior, primarily through prompting and supervised fine-tuning. Yet they often lack internal coherence, causal reasoning, and belief traceability, making them unreliable for simulating how people reason, deliberate, and respond to interventions.
  To address this, we present a conceptual modeling paradigm, Generative Minds (GenMinds), which draws from cognitive science to support structured belief representations in generative agents. To evaluate such agents, we introduce the RECAP (REconstructing CAusal Paths) framework, a benchmark designed to assess reasoning fidelity via causal traceability, demographic grounding, and intervention consistency. These contributions advance a broader shift: from surface-level mimicry to generative agents that simulate thought -- not just language -- for social simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06958v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chance Jiajie Li, Jiayi Wu, Zhenze Mo, Ao Qu, Yuhan Tang, Kaiya Ivy Zhao, Yulu Gan, Jie Fan, Jiangbo Yu, Jinhua Zhao, Paul Liang, Luis Alonso, Kent Larson</dc:creator>
    </item>
    <item>
      <title>Towards an AI-Augmented Textbook</title>
      <link>https://arxiv.org/abs/2509.13348</link>
      <description>arXiv:2509.13348v3 Announce Type: replace 
Abstract: Textbooks are a cornerstone of education, but they have a fundamental limitation: they are a one-size-fits-all medium. Any new material or alternative representation requires arduous human effort, so that textbooks cannot be adapted in a scalable manner. We present an approach for transforming and augmenting textbooks using generative AI, adding layers of multiple representations and personalization while maintaining content integrity and quality. We refer to the system built with this approach as Learn Your Way. We report pedagogical evaluations of the different transformations and augmentations, and present the results of a a randomized control trial, highlighting the advantages of learning with Learn Your Way over regular textbook usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13348v3</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator> LearnLM Team,  Google,  :, Alicia Mart\'in, Amir Globerson, Amy Wang, Anirudh Shekhawat, Anna Iurchenko, Anisha Choudhury, Avinatan Hassidim, Ay\c{c}a \c{C}akmakli, Ayelet Shasha Evron, Charlie Yang, Courtney Heldreth, Diana Akrong, Gal Elidan, Hairong Mu, Ian Li, Ido Cohen, Katherine Chou, Komal Singh, Lev Borovoi, Lidan Hackmon, Lior Belinsky, Michael Fink, Niv Efron, Preeti Singh, Rena Levitt, Shashank Agarwal, Shay Sharon, Tracey Lee-Joe, Xiaohong Hao, Yael Gold-Zamir, Yael Haramaty, Yishay Mor, Yoav Bar Sinai, Yossi Matias</dc:creator>
    </item>
    <item>
      <title>Justice in Judgment: Unveiling (Hidden) Bias in LLM-assisted Peer Reviews</title>
      <link>https://arxiv.org/abs/2509.13400</link>
      <description>arXiv:2509.13400v4 Announce Type: replace 
Abstract: The adoption of large language models (LLMs) is transforming the peer review process, from assisting reviewers in writing more detailed evaluations to generating entire reviews automatically. While these capabilities offer exciting opportunities, they also raise critical concerns about fairness and reliability. In this paper, we investigate bias in LLM-generated peer reviews by conducting controlled experiments on sensitive metadata, including author affiliation and gender. Our analysis consistently shows affiliation bias favoring institutions highly ranked on common academic rankings. Additionally, we find some gender preferences, which, even though subtle in magnitude, have the potential to compound over time. Notably, we uncover implicit biases that become more evident with token-based soft ratings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13400v4</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sai Suresh Macharla Vasu, Ivaxi Sheth, Hui-Po Wang, Ruta Binkyte, Mario Fritz</dc:creator>
    </item>
    <item>
      <title>Longitudinal and Multimodal Recording System to Capture Real-World Patient-Clinician Conversations for AI and Encounter Research: Protocol</title>
      <link>https://arxiv.org/abs/2509.16378</link>
      <description>arXiv:2509.16378v2 Announce Type: replace 
Abstract: The promise of AI in medicine depends on learning from data that reflect what matters to patients and clinicians. Most existing models are trained on electronic health records (EHRs), which capture biological measures but rarely patient-clinician interactions. These relationships, central to care, unfold across voice, text, and video, yet remain absent from datasets. As a result, AI systems trained solely on EHRs risk perpetuating a narrow biomedical view of medicine and overlooking the lived exchanges that define clinical encounters. Our objective is to design, implement, and evaluate the feasibility of a longitudinal, multimodal system for capturing patient-clinician encounters, linking 360 degree video/audio recordings with surveys and EHR data to create a dataset for AI research. This single site study is in an academic outpatient endocrinology clinic at Mayo Clinic. Adult patients with in-person visits to participating clinicians are invited to enroll. Encounters are recorded with a 360 degree video camera. After each visit, patients complete a survey on empathy, satisfaction, pace, and treatment burden. Demographic and clinical data are extracted from the EHR. Feasibility is assessed using five endpoints: clinician consent, patient consent, recording success, survey completion, and data linkage across modalities. Recruitment began in January 2025. By August 2025, 35 of 36 eligible clinicians (97%) and 212 of 281 approached patients (75%) had consented. Of consented encounters, 162 (76%) had complete recordings and 204 (96%) completed the survey. This study aims to demonstrate the feasibility of a replicable framework for capturing the multimodal dynamics of patient-clinician encounters. By detailing workflows, endpoints, and ethical safeguards, it provides a template for longitudinal datasets and lays the foundation for AI models that incorporate the complexity of care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16378v2</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Misk Al Zahidy, Kerly Guevara Maldonado, Luis Vilatuna Andrango, Ana Cristina Proano, Ana Gabriela Claros, Maria Lizarazo Jimenez, David Toro-Tobon, Victor M. Montori, Oscar J. Ponce-Ponte, Juan P. Brito</dc:creator>
    </item>
    <item>
      <title>Attributing Responsibility in AI-Induced Incidents: A Computational Reflective Equilibrium Framework for Accountability</title>
      <link>https://arxiv.org/abs/2404.16957</link>
      <description>arXiv:2404.16957v2 Announce Type: replace-cross 
Abstract: The pervasive integration of Artificial Intelligence (AI) has introduced complex challenges in the responsibility and accountability in the event of incidents involving AI-enabled systems. The interconnectivity of these systems, ethical concerns of AI-induced incidents, coupled with uncertainties in AI technology and the absence of corresponding regulations, have made traditional responsibility attribution challenging. To this end, this work proposes a Computational Reflective Equilibrium (CRE) approach to establish a coherent and ethically acceptable responsibility attribution framework for all stakeholders. The computational approach provides a structured analysis that overcomes the limitations of conceptual approaches in dealing with dynamic and multifaceted scenarios, showcasing the framework's traceability, coherence, and adaptivity properties in the responsibility attribution process. We examine the pivotal role of the initial activation level associated with claims in equilibrium computation. Using an AI-assisted medical decision-support system as a case study, we illustrate how different initializations lead to diverse responsibility distributions. The framework offers valuable insights into accountability in AI-induced incidents, facilitating the development of a sustainable and resilient system through continuous monitoring, revision, and reflection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16957v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunfei Ge, Ya-Ting Yang, Quanyan Zhu</dc:creator>
    </item>
    <item>
      <title>The Internet of Us</title>
      <link>https://arxiv.org/abs/2503.16448</link>
      <description>arXiv:2503.16448v2 Announce Type: replace-cross 
Abstract: Social Media and the Internet have catalyzed an unprecedented potential for exposure to human diversity in terms of demographics, language, culture, knowledge, opinions, talents and the like. Access to people's diversity gives us the possibility of exploiting skills and competences that we do not have, that we may not even know they exist, the so called unknown unknowns, but which, however, could be exactly what we need when looking for help in the solution of the our current problem. However, this potential has not come with new, much needed, instruments and skills to harness the complications which rise when trying to exploit the diversity of people. This paper presents our vision of the Internet of Us (IoU), a new type of online diversity-aware social interactions capable of promoting richer and deeper social relations. We discuss the multiple facets of diversity in social settings and the multidisciplinary work that is required to reap the benefits of the IoU, towards a IoU-enabled diversity-aware hybrid human-AI society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16448v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Loizos Michael, Ivano Bison, Matteo Busso, Luca Cernuzzi, Amalia De G\"otzen, Shyam Diwakar, Kobi Gal, Amarsanaa Ganbold, George Gaskell, Daniel Gatica-Perez, Jessica Heesen, Daniele Miorandi, Salvador Ruiz-Correa, Laura Schelenz, Avi Segal, Carles Sierra, Hao Xu, Fausto Giunchiglia</dc:creator>
    </item>
    <item>
      <title>Urban Green Governance: IoT-Driven Management and Enhancement of Urban Green Spaces in Campobasso</title>
      <link>https://arxiv.org/abs/2507.12106</link>
      <description>arXiv:2507.12106v5 Announce Type: replace-cross 
Abstract: The efficient design and management of public green spaces is a key factor in promoting the health and well-being of urban population, as emphasized by the WHO, UNEP, and EEA. These areas serve as the "green lungs" of the urban ecosystem, playing a vital role in enhancing quality of life thanks to the provision of ecosystem services. In this context, the Smart Green City use case in Campobasso municipality, funded by the Italian Ministry of Enterprises (MIMIT), emerges as an innovative model for the sustainable management of green urban areas through the adoption of an advanced system of emerging technologies integrated and interoperable. The project integrates IoT systems and data-driven governance platforms, enabling real-time monitoring of the health status of trees and green areas via a Decision Support System (DSS). It also facilitates the collection and analysis of data from diverse sources, including weather conditions, air quality, soil moisture, pollution levels. The resulting cloud-based platform supports a holistic real time decision making for green urban managers, technical experts and operational staff. It enables intelligent control and management of urban green spaces using Tree Talker sensors, integrated with soil moisture and water potential monitoring systems. Thanks to predictive models based on machine learning algorithms and real time data provided by IoT sensors, irrigation of public parks can be optimized by providing suggestions on when and how much water to apply. Customized alerts layers are also activated warning users when monitored parameters, such as soil temperature, humidity, or water potential, exceed predefined thresholds. This Use Case demonstrates how digitalization, IoT sensors fusion and technological innovation can support sustainable urban governance, fostering environmental resilience and improving citizens quality of life.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12106v5</guid>
      <category>cs.DC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio Salis, Gabriele Troina, Gianluca Boanelli, Marco Ottaviano, Paola Fortini, Soraya Versace</dc:creator>
    </item>
    <item>
      <title>Evaluation of A National Digitally-Enabled Health Promotion Campaign for Mental Health Awareness using Social Media Platforms Tik Tok, Facebook, Instagram, and YouTube</title>
      <link>https://arxiv.org/abs/2508.20142</link>
      <description>arXiv:2508.20142v3 Announce Type: replace-cross 
Abstract: Mental health disorders rank among the 10 leading contributors to the global burden of diseases, yet persistent stigma and care barriers delay early intervention. This has inspired efforts to leverage digital platforms for scalable health promotion to engage at-risk populations. To evaluate the effectiveness of a digitally-enabled mental health promotion (DEHP) campaign, we conducted an observational cross-sectional study of a 3-month (February-April 2025) nation-wide campaign in Singapore. Campaign materials were developed using a marketing funnel framework and disseminated across YouTube, Facebook, Instagram, and TikTok. This included narrative videos and infographics to promote symptom awareness, coping strategies, and/or patient navigation to Singapore's Mindline website, as the intended endpoint for user engagement and support. Primary outcomes include anonymised performance analytics (impressions, unique reach, video content view, engagements) stratified by demographics, device types, and sector. Secondary outcomes measured cost-efficiency metrics and traffic to the Mindline website respectively. This campaign generated 3.49 million total impressions and reached 1.39 million unique residents, with a Cost Per Click at 29.33 SGD, Cost Per Mille at 26.90 SGD and Cost Per Action at 6.06 SGD. Narrative videos accumulated over 630,000 views and 18,768 engagements. Overall, we demonstrate that DEHP campaigns can achieve national engagement for mental health awareness through multi-channel distribution and creative, narrative-driven designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20142v3</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samantha Bei Yi Yan (for the MINDLINE Study Group), Dinesh Visva Gunasekeran (for the MINDLINE Study Group), Caitlyn Tan (for the MINDLINE Study Group), Kai En Chan (for the MINDLINE Study Group), Caleb Tan (for the MINDLINE Study Group), Charmaine Shi Min Lim (for the MINDLINE Study Group), Audrey Chia (for the MINDLINE Study Group), Hsien-Hsien Lei (for the MINDLINE Study Group), Robert Morris (for the MINDLINE Study Group), Janice Huiqin Weng (for the MINDLINE Study Group)</dc:creator>
    </item>
    <item>
      <title>EigenBench: A Comparative Behavioral Measure of Value Alignment</title>
      <link>https://arxiv.org/abs/2509.01938</link>
      <description>arXiv:2509.01938v3 Announce Type: replace-cross 
Abstract: Aligning AI with human values is a pressing unsolved problem. To address the lack of quantitative metrics for value alignment, we propose EigenBench: a black-box method for comparatively benchmarking language models' values. Given an ensemble of models, a constitution describing a value system, and a dataset of scenarios, our method returns a vector of scores quantifying each model's alignment to the given constitution. To produce these scores, each model judges the outputs of other models across many scenarios, and these judgments are aggregated with EigenTrust (Kamvar et al., 2003), yielding scores that reflect a weighted consensus judgment of the whole ensemble. EigenBench uses no ground truth labels, as it is designed to quantify subjective traits for which reasonable judges may disagree on the correct label. Hence, to validate our method, we collect human judgments on the same ensemble of models and show that EigenBench's judgments align closely with those of human evaluators. We further demonstrate that EigenBench can recover model rankings on the GPQA benchmark without access to objective labels, supporting its viability as a framework for evaluating subjective values for which no ground truths exist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01938v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathn Chang, Leonhard Piff, Suvadip Sana, Jasmine X. Li, Lionel Levine</dc:creator>
    </item>
    <item>
      <title>AI for Scientific Discovery is a Social Problem</title>
      <link>https://arxiv.org/abs/2509.06580</link>
      <description>arXiv:2509.06580v3 Announce Type: replace-cross 
Abstract: Artificial intelligence promises to accelerate scientific discovery, yet its benefits remain unevenly distributed. While technical obstacles such as scarce data, fragmented standards, and unequal access to computation are significant, we argue that the primary barriers are social and institutional. Narratives that defer progress to speculative "AI scientists," the undervaluing of data and infrastructure contributions, misaligned incentives, and gaps between domain experts and machine learning researchers all constrain impact. We highlight four interconnected challenges: community dysfunction, research priorities misaligned with upstream needs, data fragmentation, and infrastructure inequities. We argue that their roots lie in cultural and organizational practices. Addressing them requires not only technical innovation but also intentional community-building, cross-disciplinary education, shared benchmarks, and accessible infrastructure. We call for reframing AI for science as a collective social project, where sustainable collaboration and equitable participation are treated as prerequisites for technical progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06580v3</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgia Channing, Avijit Ghosh</dc:creator>
    </item>
    <item>
      <title>Safety and Security Analysis of Large Language Models: Benchmarking Risk Profile and Harm Potential</title>
      <link>https://arxiv.org/abs/2509.10655</link>
      <description>arXiv:2509.10655v2 Announce Type: replace-cross 
Abstract: While the widespread deployment of Large Language Models (LLMs) holds great potential for society, their vulnerabilities to adversarial manipulation and exploitation can pose serious safety, security, and ethical risks. As new threats continue to emerge, it becomes critically necessary to assess the landscape of LLMs' safety and security against evolving adversarial prompt techniques. To understand the behavior of LLMs, this research provides an empirical analysis and risk profile of nine prominent LLMs, Claude Opus 4, DeepSeek V3 (both open-source and online), Gemini 2.5 Flash, GPT-4o, Grok 3, Llama 4 Scout, Mistral 7B, and Qwen 3 1.7B, against 24 different security and safety categories. These LLMs are evaluated on their ability to produce harmful responses for adversarially crafted prompts (dataset has been made public) for a broad range of safety and security topics, such as promotion of violent criminal behavior, promotion of non-violent criminal activity, societal harms related to safety, illegal sexual content, dangerous code generation, and cybersecurity threats beyond code. Our study introduces the Risk Severity Index (RSI), an agile and scalable evaluation score, to quantify and compare the security posture and creating a risk profile of LLMs. As the LLM development landscape progresses, the RSI is intended to be a valuable metric for comparing the risks of LLMs across evolving threats. This research finds widespread vulnerabilities in the safety filters of the LLMs tested and highlights the urgent need for stronger alignment, responsible deployment practices, and model governance, particularly for open-access and rapidly iterated models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10655v2</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Charankumar Akiri, Harrison Simpson, Kshitiz Aryal, Aarav Khanna, Maanak Gupta</dc:creator>
    </item>
    <item>
      <title>A Longitudinal Randomized Control Study of Companion Chatbot Use: Anthropomorphism and Its Mediating Role on Social Impacts</title>
      <link>https://arxiv.org/abs/2509.19515</link>
      <description>arXiv:2509.19515v2 Announce Type: replace-cross 
Abstract: Relationships with social artificial intelligence (AI) agents are on the rise. People report forming friendships, mentorships, and romantic partnerships with chatbots such as Replika, a type of social AI agent that is designed specifically for companionship. Concerns that companion chatbot relationships may harm or replace human ones have been raised, but whether and how these social consequences occur remains unclear. Prior research suggests that people's states of social need and their anthropomorphism of the AI agent may play a role in how human-AI interaction impacts human-human interaction. In this longitudinal study (N = 183), participants were randomly assigned to converse with a companion chatbot over text or to play text-based word games for 10 minutes a day for 21 consecutive days. During these 21 days, participants also completed four surveys and two audio-recorded interviews. We found that people's social health and relationships were not significantly impacted by interacting with a companion chatbot across 21 days compared to the control group. However, people who had a higher desire to socially connect anthropomorphized the chatbot more. Those who anthropomorphized the chatbot more indicated that the human-chatbot interaction had greater impacts on their social interactions and relationships with family and friends. A mediation analysis suggested that the impact of human-AI interaction on human-human social outcomes was mediated by the extent to which people anthropomorphized the AI agent, which itself was related to the desire to socially connect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19515v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 29 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rose E. Guingrich, Michael S. A. Graziano</dc:creator>
    </item>
  </channel>
</rss>
