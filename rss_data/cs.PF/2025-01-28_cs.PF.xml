<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Jan 2025 02:54:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Profiling Apple Silicon Performance for ML Training</title>
      <link>https://arxiv.org/abs/2501.14925</link>
      <description>arXiv:2501.14925v2 Announce Type: new 
Abstract: Apple Silicon has attracted much attention for its performance and role in machine learning (ML) training. Unlike NVIDIA GPUs, which have traditionally dominated ML training, Apple Silicon has a significant difference in memory architecture. It uses Unified Memory, which integrates CPU and GPU memory instead of separate CPU memory and GPU VRAM. However, it is difficult to tell whether Unified Memory means more performance benefits.
  This paper investigates the performance differences by training several large language model (LLM) workloads end-to-end under different memory scenarios. The results show a significant performance gap between Apple Silicon and NVIDIA GPUs. This paper attributes this gap to system-level factors such as page faults, power consumption, and kernel launch time. In addition, the performance difference of basic linear algebra subprograms (BLAS) on the NVIDIA GPUs and Apple Silicon chips is analyzed to further explain the observed gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14925v2</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dahua Feng, Zhiming Xu, Rongxiang Wang, Felix Xiaozhu Lin</dc:creator>
    </item>
    <item>
      <title>Do LLMs Have Visualization Literacy? An Evaluation on Modified Visualizations to Test Generalization in Data Interpretation</title>
      <link>https://arxiv.org/abs/2501.16277</link>
      <description>arXiv:2501.16277v1 Announce Type: new 
Abstract: In this paper, we assess the visualization literacy of two prominent Large Language Models (LLMs): OpenAI's Generative Pretrained Transformers (GPT), the backend of ChatGPT, and Google's Gemini, previously known as Bard, to establish benchmarks for assessing their visualization capabilities. While LLMs have shown promise in generating chart descriptions, captions, and design suggestions, their potential for evaluating visualizations remains under-explored. Collecting data from humans for evaluations has been a bottleneck for visualization research in terms of both time and money, and if LLMs were able to serve, even in some limited role, as evaluators, they could be a significant resource. To investigate the feasibility of using LLMs in the visualization evaluation process, we explore the extent to which LLMs possess visualization literacy -- a crucial factor for their effective utility in the field. We conducted a series of experiments using a modified 53-item Visualization Literacy Assessment Test (VLAT) for GPT-4 and Gemini. Our findings indicate that the LLMs we explored currently fail to achieve the same levels of visualization literacy when compared to data from the general public reported in VLAT, and LLMs heavily relied on their pre-existing knowledge to answer questions instead of utilizing the information provided by the visualization when answering questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16277v1</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiayi Hong, Christian Seto, Arlen Fan, Ross Maciejewski</dc:creator>
    </item>
    <item>
      <title>Orthrus: Accelerating Multi-BFT Consensus through Concurrent Partial Ordering of Transactions</title>
      <link>https://arxiv.org/abs/2501.14732</link>
      <description>arXiv:2501.14732v1 Announce Type: cross 
Abstract: Multi-Byzantine Fault Tolerant (Multi-BFT) consensus allows multiple consensus instances to run in parallel, resolving the leader bottleneck problem inherent in classic BFT consensus. However, the global ordering of Multi-BFT consensus enforces a strict serialized sequence of transactions, imposing additional confirmation latency and also limiting concurrency. In this paper, we introduce Orthrus, a Multi-BFT protocol that accelerates transaction confirmation through partial ordering while reserving global ordering for transactions requiring stricter sequencing. To this end, Orthrus strategically partitions transactions to maximize concurrency and ensure consistency. Additionally, it incorporates an escrow mechanism to manage interactions between partially and globally ordered transactions. We evaluated Orthrus through extensive experiments in realistic settings, deploying 128 replicas in WAN and LAN environments. Our findings demonstrate latency reductions of up to 87% in WAN compared to existing Multi-BFT protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14732v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanzheng Lyu, Shaokang Xie, Jianyu Niu, Ivan Beschastnikh, Yinqian Zhang, Mohammad Sadoghi, Chen Feng</dc:creator>
    </item>
    <item>
      <title>KVDirect: Distributed Disaggregated LLM Inference</title>
      <link>https://arxiv.org/abs/2501.14743</link>
      <description>arXiv:2501.14743v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have become the new foundation for many applications, reshaping human society like a storm. Disaggregated inference, which separates prefill and decode stages, is a promising approach to improving hardware utilization and service quality. However, due to inefficient inter-node communication, existing systems restrict disaggregated inference to a single node, limiting resource allocation flexibility and reducing service capacity. This paper introduces KVDirect, which optimizes KV cache transfer to enable a distributed disaggregated LLM inference. KVDirect achieves this through the following contributions. First, we propose a novel tensor-centric communication mechanism that reduces the synchronization overhead in traditional distributed GPU systems. Second, we design a custom communication library to support dynamic GPU resource scheduling and efficient KV cache transfer. Third, we introduce a pull-based KV cache transfer strategy that reduces GPU resource idling and improves latency. Finally, we implement KVDirect as an open-source LLM inference framework. Our evaluation demonstrates that KVDirect reduces per-request latency by 55% compared to the baseline across diverse workloads under the same resource constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14743v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shiyang Chen, Rain Jiang, Dezhi Yu, Jinlai Xu, Mengyuan Chao, Fanlong Meng, Chenyu Jiang, Wei Xu, Hang Liu</dc:creator>
    </item>
    <item>
      <title>Intent-driven scheduling of backup jobs</title>
      <link>https://arxiv.org/abs/2501.14763</link>
      <description>arXiv:2501.14763v1 Announce Type: cross 
Abstract: Job scheduling under various constraints to achieve global optimization is a well-studied problem. However, in scenarios that involve time-dependent constraints, such as scheduling backup jobs, achieving global optimization may not always be desirable. This paper presents a framework for scheduling new backup jobs in the presence of existing job schedules, focusing on satisfying intent-based constraints without disrupting current schedules. The proposed method accommodates various scheduling intents and constraints, and its effectiveness is validated through extensive testing against a variety of backup scenarios on real-world data from Veritas Netbackup customer policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14763v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Souvik Dutta, Suri Brahmaroutu</dc:creator>
    </item>
    <item>
      <title>OptiSeq: Optimizing Example Ordering for In-Context Learning</title>
      <link>https://arxiv.org/abs/2501.15030</link>
      <description>arXiv:2501.15030v1 Announce Type: cross 
Abstract: Developers using LLMs in their applications and agents have provided plenty of anecdotal evidence that in-context-learning (ICL) is fragile. In addition to the quantity and quality of examples, we show that the order in which the in-context examples are listed in the prompt affects the output of the LLM and, consequently, their performance. In this paper, we present OptiSeq, which introduces a score based on log probabilities of LLM outputs to prune the universe of possible example orderings in few-shot ICL and recommend the best order(s) by distinguishing between correct and incorrect outputs resulting from different order permutations. Through a detailed empirical evaluation on multiple LLMs, datasets and prompts, we demonstrate that OptiSeq improves accuracy by 6 - 10.5 percentage points across multiple tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15030v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.PF</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahul Atul Bhope, Praveen Venkateswaran, K. R. Jayaram, Vatche Isahagian, Vinod Muthusamy, Nalini Venkatasubramanian</dc:creator>
    </item>
    <item>
      <title>rcpptimer: Rcpp Tic-Toc Timer with OpenMP Support</title>
      <link>https://arxiv.org/abs/2501.15856</link>
      <description>arXiv:2501.15856v1 Announce Type: cross 
Abstract: Efficient code writing is both a critical and challenging task, especially with the growing demand for computationally intensive algorithms in statistical and machine-learning applications. Despite the availability of significant computational power today, the need for optimized algorithm implementations remains crucial. Many R users rely on Rcpp to write performant code in C++, but writing and benchmarking C++ code presents its own difficulties. While R's benchmarking tools are insufficient for measuring the execution times of C++ code segments, C++'s native profiling tools often come with a steep learning curve. The rcpptimer package bridges this gap by offering a simple and efficient solution for timing C++ code within the Rcpp ecosystem. This novel package introduces a user-friendly tic-toc class that supports overlapping and nested timers and OpenMP parallelism, providing nanosecond-level time resolution. Results, including summary statistics, are seamlessly passed back to R without requiring users to write any C++ code. This paper contextualizes the rcpptimer package within the broader ecosystem of R and C++ profiling tools, explains the motivation behind its development, and offers a comprehensive overview of its implementation. Supplementary to this paper, we provide multiple vignettes that thoroughly explain this package's usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15856v1</guid>
      <category>cs.MS</category>
      <category>cs.PF</category>
      <category>stat.CO</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Berrisch</dc:creator>
    </item>
    <item>
      <title>SP-IMPact: A Framework for Static Partitioning Interference Mitigation and Performance Analysis</title>
      <link>https://arxiv.org/abs/2501.16245</link>
      <description>arXiv:2501.16245v1 Announce Type: cross 
Abstract: Modern embedded systems are evolving toward complex, heterogeneous architectures to accommodate increasingly demanding applications. Driven by SWAP-C constraints, this shift has led to consolidating multiple systems onto single hardware platforms. Static Partitioning Hypervisors offer a promising solution to partition hardware resources and provide spatial isolation between critical workloads. However, shared resources like the Last-Level Cache and system bus can introduce temporal interference between virtual machines (VMs), negatively impacting performance and predictability. Over the past decade, academia and industry have developed interference mitigation techniques, such as cache partitioning and memory bandwidth reservation. However, configuring these techniques is complex and time-consuming. Cache partitioning requires balancing cache sections across VMs, while memory bandwidth reservation needs tuning bandwidth budgets and periods. Testing all configurations is impractical and often leads to suboptimal results. Moreover, understanding how these techniques interact is limited, as their combined use can produce compounded or conflicting effects on performance. Static analysis tools estimating worst-case execution times offer guidance for configuring mitigation techniques but often fail to capture the complexity of modern multi-core systems. They typically focus on limited shared resources while neglecting others, such as IOMMUs and interrupt controllers. To address these challenges, we present SP-IMPact, an open-source framework for analyzing and guiding interference mitigation configurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth reservation, while evaluating their interactions and cumulative impact. By providing insights on real hardware, SP-IMPact helps optimize configurations for mixed-criticality systems, ensuring performance and predictability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16245v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diogo Costa, Gon\c{c}alo Moreira, Afonso Oliveira, Jos\'e Martins, Sandro Pinto</dc:creator>
    </item>
    <item>
      <title>Adaptive Iterative Compression for High-Resolution Files: an Approach Focused on Preserving Visual Quality in Cinematic Workflows</title>
      <link>https://arxiv.org/abs/2501.16319</link>
      <description>arXiv:2501.16319v1 Announce Type: cross 
Abstract: This study presents an iterative adaptive compression model for high-resolution DPX-derived TIFF files used in cinematographic workflows and digital preservation. The model employs SSIM and PSNR metrics to dynamically adjust compression parameters across three configurations (C0, C1, C2), achieving storage reductions up to 83.4 % while maintaining high visual fidelity (SSIM &gt; 0.95). Validation across three diverse productions - black and white classic, soft-palette drama, and complex action film - demonstrated the method's effectiveness in preserving critical visual elements while significantly reducing storage requirements. Professional evaluators reported 90% acceptance rate for the optimal C1 configuration, with artifacts remaining below perceptual threshold in critical areas. Comparative analysis with JPEG2000 and H.265 showed superior quality preservation at equivalent compression rates, particularly for high bit-depth content. While requiring additional computational overhead, the method's storage benefits and quality control capabilities make it suitable for professional workflows, with potential applications in medical imaging and cloud storage optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16319v1</guid>
      <category>cs.CV</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.14745542</arxiv:DOI>
      <dc:creator>Leonardo Melo, Filipe Litaiff</dc:creator>
    </item>
  </channel>
</rss>
