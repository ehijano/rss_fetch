<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Aug 2025 01:21:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Statistical Inference for Subgraph Frequencies of Exchangeable Hyperedge Models</title>
      <link>https://arxiv.org/abs/2508.13258</link>
      <description>arXiv:2508.13258v1 Announce Type: new 
Abstract: In statistical network analysis, models for binary adjacency matrices satisfying vertex exchangeability are commonly used. However, such models may fail to capture key features of the data-generating process when interactions, rather than nodes, are fundamental units. We study statistical inference for subgraph counts under an exchangeable hyperedge model. We introduce several classes of subgraph statistics for hypergraphs and develop inferential tools for subgraph frequencies that account for edge multiplicity. We show that a subclass of these subgraph statistics is robust to the deletion of low-degree nodes, enabling inference in settings where low-degree nodes are more likely to be missing. We also examine a more traditional notion of subgraph frequency that ignores multiplicity, showing that while inference based on limiting distributions is feasible in some cases, a non-degenerate limiting distribution may not exist in others. Empirically, we assess our methods through simulations and newly collected real-world hypergraph data on academic and movie collaborations, where our inferential tools outperform traditional approaches based on binary adjacency matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13258v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ayoushman Bhattacharya, Nilanjan Chakraborty, Robert Lunde</dc:creator>
    </item>
    <item>
      <title>Identification and Estimation of Multi-order Tensor Factor Models</title>
      <link>https://arxiv.org/abs/2508.13418</link>
      <description>arXiv:2508.13418v1 Announce Type: new 
Abstract: We propose a novel framework in high-dimensional factor models to simultaneously analyze multiple tensor time series, each with potentially different tensor orders and dimensionality. The connection between different tensor time series is through their global factors that are correlated to each other. A salient feature of our model is that when all tensor time series have the same order, it can be regarded as an extension of multilevel factor models from vectors to general tensors. Under very mild conditions, we separate the global and local components in the proposed model. Parameter estimation is thoroughly discussed. With strong correlation between global factors and noise allowed, we derive the rates of convergence of our estimators, which can be more superior than those of existing methods for multilevel factor models. We also develop estimators that are more computationally efficient, with rates of convergence spelt out. Extensive experiments are performed under various settings, corroborating with the pronounced theoretical results. As a real application example, we analyze a set of taxi data to study the traffic flow between Times Squares and its neighboring areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13418v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zetai Cen</dc:creator>
    </item>
    <item>
      <title>Multinomial probit model based on joint quantile regression</title>
      <link>https://arxiv.org/abs/2508.13556</link>
      <description>arXiv:2508.13556v1 Announce Type: new 
Abstract: The multinomial probit model is a typical statistical model for multiple-choice data applied in many research areas. When we are interested in some quantiles of relative utilities for understanding the distribution of these utilities, the multinomial probit model is unsuitable because we only interpret the expectation of relative utilities based on it. We thus propose quantile regression analysis methods for multinomial choice data based on joint quantile regression and multinomial probit models to compare relative utilities with some quantiles. Using a joint quantile regression model allows us to consider the conditional quantile points of relative utilities and explicitly describe the correlation structure in the latent variables. We derive the full conditional distribution under several prior distributions and estimate the model's parameters from the posterior distribution by Gibbs sampling. The ability to calculate by Gibbs sampling is not only computationally less expensive than the Metropolis--Hastings method, but also easier to implement. We also apply the proposed model to several datasets. Consequently, we obtain interpretable results about different parameters by quantile.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13556v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masaaki Okabe, Koki Matsuoka, Jun Tsuchida, Hiroshi Yadohisa</dc:creator>
    </item>
    <item>
      <title>Extropy-Based Generalized Divergence and Similarity Ratios: Theory and Applications</title>
      <link>https://arxiv.org/abs/2508.13696</link>
      <description>arXiv:2508.13696v1 Announce Type: new 
Abstract: In this article, we propose two classes of relative information measures based on extropy, viz., the generalized extropy similarity ratio (GESR) and generalized extropy divergence ratio (GEDR), that measure the similarity and discrepancy between two probability distributions, respectively. Definitions of GESR and GEDR are proposed along with their fundamental axioms, properties, and some measures satisfying those axioms are also introduced. The relationship of GESR with the popular cosine similarity is also established in the study. Various properties of GESR and GEDR, including bounds under the proportional hazards model and the proportional reversed hazards model, are derived. Nonparametric estimators of GESR are defined, and their performance is evaluated using simulation studies. Applications of the GESR in lifetime data analysis and image analysis are also demonstrated in this study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13696v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saranya P., Sunoj S. M</dc:creator>
    </item>
    <item>
      <title>Diffusion-Driven High-Dimensional Variable Selection</title>
      <link>https://arxiv.org/abs/2508.13890</link>
      <description>arXiv:2508.13890v1 Announce Type: new 
Abstract: Variable selection for high-dimensional, highly correlated data has long been a challenging problem, often yielding unstable and unreliable models. We propose a resample-aggregate framework that exploits diffusion models' ability to generate high-fidelity synthetic data. Specifically, we draw multiple pseudo-data sets from a diffusion model fitted to the original data, apply any off-the-shelf selector (e.g., lasso or SCAD), and store the resulting inclusion indicators and coefficients. Aggregating across replicas produces a stable subset of predictors with calibrated stability scores for variable selection. Theoretically, we show that the proposed method is selection consistent under mild assumptions. Because the generative model imports knowledge from large pre-trained weights, the procedure naturally benefits from transfer learning, boosting power when the observed sample is small or noisy. We also extend the framework of aggregating synthetic data to other model selection problems, including graphical model selection, and statistical inference that supports valid confidence intervals and hypothesis tests. Extensive simulations show consistent gains over the lasso, stability selection, and knockoff baselines, especially when predictors are strongly correlated, achieving higher true-positive rates and lower false-discovery proportions. By coupling diffusion-based data augmentation with principled aggregation, our method advances variable selection methodology and broadens the toolkit for interpretable, statistically rigorous analysis in complex scientific applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13890v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minjie Wang, Xiaotong Shen, Wei Pan</dc:creator>
    </item>
    <item>
      <title>What makes a study design quasi-experimental? The case of difference-in-differences</title>
      <link>https://arxiv.org/abs/2508.13945</link>
      <description>arXiv:2508.13945v1 Announce Type: new 
Abstract: Study designs classified as quasi- or natural experiments are typically accorded more face validity than observational study designs more broadly. However, there is ambiguity in the literature about what qualifies as a quasi-experiment. Here, we attempt to resolve this ambiguity by distinguishing two different ways of defining this term. One definition is based on identifying assumptions being uncontroversial, and the other is based on the ability to account for unobserved sources of confounding (under assumptions). We argue that only the former deserves an additional measure of credibility for reasons of design. We use the difference-in-differences approach to illustrate our discussion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13945v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Audrey Renson, Daniel Westreich</dc:creator>
    </item>
    <item>
      <title>Partial Identification of Causal Effects for Endogenous Continuous Treatments</title>
      <link>https://arxiv.org/abs/2508.13946</link>
      <description>arXiv:2508.13946v1 Announce Type: new 
Abstract: No unmeasured confounding is a common assumption when reasoning about counterfactual outcomes, but such an assumption may not be plausible in observational studies. Sensitivity analysis is often employed to assess the robustness of causal conclusions to unmeasured confounding, but existing methods are predominantly designed for binary treatments. In this paper, we provide natural extensions of two extensively used sensitivity frameworks -- the Rosenbaum and Marginal sensitivity models -- to the setting of continuous exposures. Our generalization replaces scalar sensitivity parameters with sensitivity functions that vary with exposure level, enabling richer modeling and sharper identification bounds. We develop a unified pseudo-outcome regression formulation for bounding the counterfactual dose-response curve under both models, and propose corresponding nonparametric estimators which have second order bias. These estimators accommodate modern machine learning methods for obtaining nuisance parameter estimators, which are shown to achieve $L^2$- consistency, minimax rates of convergence under suitable conditions. Our resulting estimators of bounds for the counterfactual dose-response curve are shown to be consistent and asymptotic normal allowing for a user-specified bound on the degree of uncontrolled exposure endogeneity. We also offer a geometric interpretation that relates the Rosenbaum and Marginal sensitivity model and guides their practical usage in global versus targeted sensitivity analysis. The methods are validated through simulations and a real-data application on the effect of second-hand smoke exposure on blood lead levels in children.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13946v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhinandan Dalal, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Structural Foundations for Leading Digit Laws: Beyond Probabilistic Mixtures</title>
      <link>https://arxiv.org/abs/2508.13237</link>
      <description>arXiv:2508.13237v1 Announce Type: cross 
Abstract: This article presents a modern deterministic framework for the study of leading significant digit distributions in numerical data. Rather than relying on traditional probabilistic or mixture-based explanations, we demonstrate that the observed frequencies of leading digits are determined by the underlying arithmetic, algorithmic, and structural properties of the data-generating process. Our approach centers on a shift-invariant functional equation, whose general solution is given by explicit affine-plus-periodic formulas. This structural formulation explains the diversity of digit distributions encountered in both empirical and mathematical datasets, including cases with pronounced deviations from logarithmic or scale-invariant profiles.
  We systematically analyze digit distributions in finite and infinite datasets, address deterministic sequences such as prime numbers and recurrence relations, and highlight the emergence of block-structured and fractal features. The article provides critical examination of probabilistic models, explicit examples and counterexamples, and discusses limitations and open problems for further research. Overall, this work establishes a unified mathematical foundation for digital phenomena and offers a versatile toolset for modeling and analyzing digit patterns in applied and theoretical contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13237v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Berman</dc:creator>
    </item>
    <item>
      <title>On the Voigt profile and its dual</title>
      <link>https://arxiv.org/abs/2508.13252</link>
      <description>arXiv:2508.13252v1 Announce Type: cross 
Abstract: The Voigt profile is the density obtained from the convolution of a Gaussian and a Cauchy and it is widely used in atomic and molecular spectroscopy. We show that the Voigt profile is a scale mixture of Gaussian distributions, with mixing Levy distribution. A consequence of this result is that there exists a dual of the Voigt distribution, which is itself a normal scale mixture. Both the Dual Voigt and its mixing are transformations, via truncation and reflection, of the Normal and Levy random variables. We discuss the dual Voigt characteristics, propose algorithms for parameter estimation and outline further developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13252v1</guid>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Massimo Cannas</dc:creator>
    </item>
    <item>
      <title>Counterfactual Probabilistic Diffusion with Expert Models</title>
      <link>https://arxiv.org/abs/2508.13355</link>
      <description>arXiv:2508.13355v1 Announce Type: cross 
Abstract: Predicting counterfactual distributions in complex dynamical systems is essential for scientific modeling and decision-making in domains such as public health and medicine. However, existing methods often rely on point estimates or purely data-driven models, which tend to falter under data scarcity. We propose a time series diffusion-based framework that incorporates guidance from imperfect expert models by extracting high-level signals to serve as structured priors for generative modeling. Our method, ODE-Diff, bridges mechanistic and data-driven approaches, enabling more reliable and interpretable causal inference. We evaluate ODE-Diff across semi-synthetic COVID-19 simulations, synthetic pharmacological dynamics, and real-world case studies, demonstrating that it consistently outperforms strong baselines in both point prediction and distributional accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13355v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenhao Mu, Zhi Cao, Mehmed Uludag, Alexander Rodr\'iguez</dc:creator>
    </item>
    <item>
      <title>Monotonic Path-Specific Effects: Application to Estimating Educational Returns</title>
      <link>https://arxiv.org/abs/2508.13366</link>
      <description>arXiv:2508.13366v1 Announce Type: cross 
Abstract: Conventional research on educational effects typically either employs a "years of schooling" measure of education, or dichotomizes attainment as a point-in-time treatment. Yet, such a conceptualization of education is misaligned with the sequential process by which individuals make educational transitions. In this paper, I propose a causal mediation framework for the study of educational effects on outcomes such as earnings. The framework considers the effect of a given educational transition as operating indirectly, via progression through subsequent transitions, as well as directly, net of these transitions. I demonstrate that the average treatment effect (ATE) of education can be additively decomposed into mutually exclusive components that capture these direct and indirect effects. The decomposition has several special properties which distinguish it from conventional mediation decompositions of the ATE, properties which facilitate less restrictive identification assumptions as well as identification of all causal paths in the decomposition. An analysis of the returns to high school completion in the NLSY97 cohort suggests that the payoff to a high school degree stems overwhelmingly from its direct labor market returns. Mediation via college attendance, completion and graduate school attendance is small because of individuals' low counterfactual progression rates through these subsequent transitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13366v1</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aleksei Opacic</dc:creator>
    </item>
    <item>
      <title>Collapsing ROC approach for risk prediction research on both common and rare variants</title>
      <link>https://arxiv.org/abs/2508.13552</link>
      <description>arXiv:2508.13552v1 Announce Type: cross 
Abstract: Risk prediction that capitalizes on emerging genetic findings holds great promise for improving public health and clinical care. However, recent risk prediction research has shown that predictive tests formed on existing common genetic loci, including those from genome-wide association studies, have lacked sufficient accuracy for clinical use. Because most rare variants on the genome have not yet been studied for their role in risk prediction, future disease prediction discoveries should shift toward a more comprehensive risk prediction strategy that takes into account both common and rare variants. We are proposing a collapsing receiver operating characteristic CROC approach for risk prediction research on both common and rare variants. The new approach is an extension of a previously developed forward ROC FROC approach, with additional procedures for handling rare variants. The approach was evaluated through the use of 533 single-nucleotide polymorphisms SNPs in 37 candidate genes from the Genetic Analysis Workshop 17 mini-exome data set. We found that a prediction model built on all SNPs gained more accuracy AUC = 0.605 than one built on common variants alone AUC = 0.585. We further evaluated the performance of two approaches by gradually reducing the number of common variants in the analysis. We found that the CROC method attained more accuracy than the FROC method when the number of common variants in the data decreased. In an extreme scenario, when there are only rare variants in the data, the CROC reached an AUC value of 0.603, whereas the FROC had an AUC value of 0.524.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13552v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1186/1753-6561-5-S9-S42</arxiv:DOI>
      <dc:creator>Changshuai Wei, Qing Lu</dc:creator>
    </item>
    <item>
      <title>Bounding Causal Effects and Counterfactuals</title>
      <link>https://arxiv.org/abs/2508.13607</link>
      <description>arXiv:2508.13607v1 Announce Type: cross 
Abstract: Causal inference often hinges on strong assumptions - such as no unmeasured confounding or perfect compliance - that are rarely satisfied in practice. Partial identification offers a principled alternative: instead of relying on unverifiable assumptions to estimate causal effects precisely, it derives bounds that reflect the uncertainty inherent in the data. Despite its theoretical appeal, partial identification remains underutilized in applied work, in part due to the fragmented nature of existing methods and the lack of practical guidance. This thesis addresses these challenges by systematically comparing a diverse set of bounding algorithms across multiple causal scenarios. We implement, extend, and unify state-of-the-art methods - including symbolic, optimization-based, and information-theoretic approaches - within a common evaluation framework. In particular, we propose an extension of a recently introduced entropy-bounded method, making it applicable to counterfactual queries such as the Probability of Necessity and Sufficiency (PNS). Our empirical study spans thousands of randomized simulations involving both discrete and continuous data-generating processes. We assess each method in terms of bound tightness, computational efficiency, and robustness to assumption violations. To support practitioners, we distill our findings into a practical decision tree for algorithm selection and train a machine learning model to predict the best-performing method based on observable data characteristics.
  All implementations are released as part of an open-source Python package, CausalBoundingEngine, which enables users to apply and compare bounding methods through a unified interface.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13607v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias Maringgele</dc:creator>
    </item>
    <item>
      <title>A Nonparametric Approach to Augmenting a Bayesian VAR with Nonlinear Factors</title>
      <link>https://arxiv.org/abs/2508.13972</link>
      <description>arXiv:2508.13972v1 Announce Type: cross 
Abstract: This paper proposes a Vector Autoregression augmented with nonlinear factors that are modeled nonparametrically using regression trees. There are four main advantages of our model. First, modeling potential nonlinearities nonparametrically lessens the risk of mis-specification. Second, the use of factor methods ensures that departures from linearity are modeled parsimoniously. In particular, they exhibit functional pooling where a small number of nonlinear factors are used to model common nonlinearities across variables. Third, Bayesian computation using MCMC is straightforward even in very high dimensional models, allowing for efficient, equation by equation estimation, thus avoiding computational bottlenecks that arise in popular alternatives such as the time varying parameter VAR. Fourth, existing methods for identifying structural economic shocks in linear factor models can be adapted for the nonlinear case in a straightforward fashion using our model. Exercises involving artificial and macroeconomic data illustrate the properties of our model and its usefulness for forecasting and structural economic analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13972v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Todd Clark, Florian Huber, Gary Koop</dc:creator>
    </item>
    <item>
      <title>Robustly estimating heterogeneity in factorial data using Rashomon Partitions</title>
      <link>https://arxiv.org/abs/2404.02141</link>
      <description>arXiv:2404.02141v4 Announce Type: replace 
Abstract: In both observational data and randomized control trials, researchers select statistical models to articulate how the outcome of interest varies with combinations of observable covariates. Choosing a model that is too simple can obfuscate important heterogeneity in outcomes between covariate groups, while too much complexity risks identifying spurious patterns. In this paper, we propose a novel Bayesian framework for model uncertainty called Rashomon Partition Sets (RPSs). The RPS consists of all models that have posterior density close to the maximum a posteriori (MAP) model. We construct the RPS by enumeration, rather than sampling, which ensures that we explore all models models with high evidence in the data, even if they offer dramatically different substantive explanations. We use a l0 prior, which allows the allows us to capture complex heterogeneity without imposing strong assumptions about the associations between effects, showing this prior is minimax optimal from an information-theoretic perspective. We characterize the approximation error of (functions of) parameters computed conditional on being in the RPS relative to the entire posterior. We propose an algorithm to enumerate the RPS from the class of models that are interpretable and unique, then provide bounds on the size of the RPS. We give simulation evidence along with three empirical examples: price effects on charitable giving, heterogeneity in chromosomal structure, and the introduction of microfinance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02141v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aparajithan Venkateswaran, Anirudh Sankar, Arun G. Chandrasekhar, Tyler H. McCormick</dc:creator>
    </item>
    <item>
      <title>Climate Change in Austria: Precipitation and Dry Spells over the last 60 years</title>
      <link>https://arxiv.org/abs/2408.11497</link>
      <description>arXiv:2408.11497v3 Announce Type: replace 
Abstract: This study unveils localised changes in Austria's precipitation patterns, often missed by broader assessments, by comparing the 1961-1990 and 1991-2020 climate normal periods on a high resolution 2x2 km grid. Our extended model explicitly accounts for diverse topographical influences, including slope, aspect, and a monthly-varying elevation effect, when analysing monthly normals of mean precipitation and maximum daily sums, as well as maximum dry spell lengths. We found that while mean precipitation generally declined early in the year, it notably increased in March, September, and October (up to +50%). In contrast, the maximum duration of dry spells extended significantly in January, February, and June, particularly in the southern regions (up to +30%). Maximum daily precipitation amounts surged in late summer and autumn (up to +30%). This research offers a transferable modelling approach for understanding critical shifts, vital for climate adaptation both within Austria and globally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11497v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Corinna Perchtold</dc:creator>
    </item>
    <item>
      <title>LEARNER: A Transfer Learning Method for Low-Rank Matrix Estimation</title>
      <link>https://arxiv.org/abs/2412.20605</link>
      <description>arXiv:2412.20605v2 Announce Type: replace 
Abstract: Low-rank matrix estimation is a fundamental problem in statistics and machine learning with applications across biomedical sciences, including genetics, medical imaging, drug discovery, and electronic health record data analysis. In the context of heterogeneous data generated from diverse sources, a key challenge lies in leveraging data from a source population to enhance the estimation of a low-rank matrix in a target population of interest. We propose an approach that leverages similarity in the latent row and column spaces between the source and target populations to improve estimation in the target population, which we refer to as LatEnt spAce-based tRaNsfer lEaRning (LEARNER). LEARNER is based on performing a low-rank approximation of the target population data which penalizes differences between the latent row and column spaces between the source and target populations. We present a cross-validation approach that allows the method to adapt to the degree of heterogeneity across populations. We conducted extensive simulations which found that LEARNER often outperforms the benchmark approach that only uses the target population data, especially as the signal-to-noise ratio in the source population increases. We also performed an illustrative application and empirical comparison of LEARNER and benchmark approaches in a re-analysis of summary statistics from a genome-wide association study in the BioBank Japan cohort. LEARNER is implemented in the R package learner and the Python package learner-py.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20605v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sean McGrath, Cenhao Zhu, Ryan O'Dea, Min Guo, Rui Duan</dc:creator>
    </item>
    <item>
      <title>Bayesian Shrinkage in High-Dimensional VAR Models: A Comparative Study</title>
      <link>https://arxiv.org/abs/2504.05489</link>
      <description>arXiv:2504.05489v2 Announce Type: replace 
Abstract: High-dimensional vector autoregressive (VAR) models offer a versatile framework for multivariate time series analysis, yet face critical challenges from over-parameterization and uncertain lag order. In this paper, we systematically compare three Bayesian shrinkage priors (horseshoe, lasso, and normal) and two frequentist regularization approaches (ridge and nonparametric shrinkage) under three carefully crafted simulation scenarios. These scenarios encompass (i) overfitting in a low-dimensional setting, (ii) sparse high-dimensional processes, and (iii) a combined scenario where both large dimension and overfitting complicate inference.
  We evaluate each method in quality of parameter estimation (root mean squared error, coverage, and interval length) and out-of-sample forecasting (one-step-ahead forecast RMSE). Our findings show that local-global Bayesian methods, particularly the horseshoe, dominate in maintaining accurate coverage and minimizing parameter error, even when the model is heavily over-parameterized. Frequentist ridge often yields competitive point forecasts but underestimates uncertainty, leading to sub-nominal coverage. A real-data application using macroeconomic variables from Canada illustrates how these methods perform in practice, reinforcing the advantages of local-global priors in stabilizing inference when dimension or lag order is inflated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05489v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Katz, Robert E. Weiss</dc:creator>
    </item>
    <item>
      <title>On a Debiased and Semiparametric Efficient Changes-in-Changes Estimator</title>
      <link>https://arxiv.org/abs/2507.07228</link>
      <description>arXiv:2507.07228v2 Announce Type: replace 
Abstract: We present a novel extension of the influential changes-in-changes (CiC) framework of Athey and Imbens (2006) for estimating the average treatment effect on the treated (ATT) and distributional causal effects in panel data with unmeasured confounding. While CiC relaxes the parallel trends assumption in difference-in-differences (DiD), existing methods typically assume a scalar unobserved confounder and monotonic outcome relationships, and lack inference tools that accommodate continuous covariates flexibly. Motivated by empirical settings with complex confounding and rich covariate information, we make two main contributions. First, we establish nonparametric identification under relaxed assumptions that allow high-dimensional, non-monotonic unmeasured confounding. Second, we derive semiparametrically efficient estimators that are Neyman orthogonal to infinite-dimensional nuisance parameters, enabling valid inference even with machine learning-based estimation of nuisance components. We illustrate the utility of our approach in an empirical analysis of mass shootings and U.S. electoral outcomes, where key confounders, such as political mobilization or local gun culture, are typically unobserved and challenging to quantify.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07228v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinghao Sun, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Dice, but don't slice: Optimizing the efficiency of ONEAudit</title>
      <link>https://arxiv.org/abs/2507.22179</link>
      <description>arXiv:2507.22179v2 Announce Type: replace 
Abstract: ONEAudit provides more efficient risk-limiting audits than other extant methods when the voting system cannot report a cast-vote record linked to each cast card. It obviates the need for re-scanning; it is simpler and more efficient than 'hybrid' audits; and it is far more efficient than batch-level comparison audits. There may be room to improve the efficiency of ONEAudit further by tuning the statistical tests it uses and by using stratified sampling. We show that tuning the tests by optimizing for the reported batch-level tallies or integrating over a distribution reduces expected workloads by 70-85% compared to the current ONEAudit implementation across a range of simulated elections. The improved tests reduce the expected workload to audit the 2024 Mayoral race in San Francisco, California, by half -- from about 200 cards to about 100 cards. In contrast, stratified sampling does not help: it increases workloads by about 25% on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22179v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jacob V Spertus, Amanda K Glazer, Philip B Stark</dc:creator>
    </item>
    <item>
      <title>Rate Optimality and Phase Transition for User-Level Local Differential Privacy</title>
      <link>https://arxiv.org/abs/2405.11923</link>
      <description>arXiv:2405.11923v2 Announce Type: replace-cross 
Abstract: Most of the literature on differential privacy considers the item-level case where each user has a single observation, but a growing field of interest is that of user-level privacy where each of the $n$ users holds $T$ observations and wishes to maintain the privacy of their entire collection.
  In this paper, we derive a general minimax lower bound, which shows that, for locally private user-level estimation problems, the risk cannot, in general, be made to vanish for a fixed number of users even when each user holds an arbitrarily large number of observations. We then derive matching, up to logarithmic factors, lower and upper bounds for univariate and multidimensional mean estimation, sparse mean estimation and non-parametric density estimation. In particular, with other model parameters held fixed, we observe phase transition phenomena in the minimax rates as $T$ the number of observations each user holds varies.
  In the case of (non-sparse) mean estimation and density estimation, we see that, for $T$ below a phase transition boundary, the rate is the same as having $nT$ users in the item-level setting. Different behaviour is however observed in the case of $s$-sparse $d$-dimensional mean estimation, wherein consistent estimation is impossible when $d$ exceeds the number of observations in the item-level setting, but is possible in the user-level setting when $T \gtrsim s \log (d)$, up to logarithmic factors. This may be of independent interest for applications as an example of a high-dimensional problem that is feasible under local privacy constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11923v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Kent, Thomas B. Berrett, Yi Yu</dc:creator>
    </item>
    <item>
      <title>Huber means on Riemannian manifolds</title>
      <link>https://arxiv.org/abs/2407.15764</link>
      <description>arXiv:2407.15764v2 Announce Type: replace-cross 
Abstract: This article introduces Huber means on Riemannian manifolds, providing a robust alternative to the Frechet mean by integrating elements of both square and absolute loss functions. The Huber means are designed to be highly resistant to outliers while maintaining efficiency, making it a valuable generalization of Huber's M-estimator for manifold-valued data. We comprehensively investigate the statistical and computational aspects of Huber means, demonstrating their utility in manifold-valued data analysis. Specifically, we establish nearly minimal conditions for ensuring the existence and uniqueness of the Huber mean and discuss regularity conditions for unbiasedness. The Huber means are consistent and enjoy the central limit theorem. Additionally, we propose a novel moment-based estimator for the limiting covariance matrix, which is used to construct a robust one-sample location test procedure and an approximate confidence region for location parameters. The Huber mean is shown to be highly robust and efficient in the presence of outliers or under heavy-tailed distributions. Specifically, it achieves a breakdown point of at least 0.5, the highest among all isometric equivariant estimators, and is more efficient than the Frechet mean under heavy-tailed distributions. Numerical examples on spheres and the space of symmetric positive-definite matrices further illustrate the efficiency and reliability of the proposed Huber means on Riemannian manifolds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15764v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jongmin Lee, Sungkyu Jung</dc:creator>
    </item>
    <item>
      <title>Bayesian Analysis of Spiked Covariance Models: Correcting Eigenvalue Bias and Determining the Number of Spikes</title>
      <link>https://arxiv.org/abs/2412.10753</link>
      <description>arXiv:2412.10753v2 Announce Type: replace-cross 
Abstract: We study Bayesian inference in the spiked covariance model, where a small number of spiked eigenvalues dominate the spectrum. Our goal is to infer the spiked eigenvalues, their corresponding eigenvectors, and the number of spikes, providing a Bayesian solution to principal component analysis with uncertainty quantification. We place an inverse-Wishart prior on the covariance matrix to derive posterior distributions for the spiked eigenvalues and eigenvectors. Although posterior sampling is computationally efficient due to conjugacy, a bias may exist in the posterior eigenvalue estimates under high-dimensional settings. To address this, we propose two bias correction strategies: (i) a hyperparameter adjustment method, and (ii) a post-hoc multiplicative correction. For inferring the number of spikes, we develop a BIC-type approximation to the marginal likelihood and prove posterior consistency in the high-dimensional regime $p&gt;n$. Furthermore, we establish concentration inequalities and posterior contraction rates for the leading eigenstructure, demonstrating minimax optimality for the spiked eigenvector in the single-spike case. Simulation studies and a real data application show that our method performs better than existing approaches in providing accurate quantification of uncertainty for both eigenstructure estimation and estimation of the number of spikes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10753v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kwangmin Lee, Sewon Park, Seongmin Kim, Jaeyong Lee</dc:creator>
    </item>
    <item>
      <title>Stochastic highway capacity: Unsuitable Kaplan-Meier estimator, revised maximum likelihood estimator, and impact of speed harmonisation</title>
      <link>https://arxiv.org/abs/2507.00893</link>
      <description>arXiv:2507.00893v2 Announce Type: replace-cross 
Abstract: The Kaplan-Meier estimate, also known as the product-limit method (PLM), is a widely used non-parametric maximum likelihood estimator (MLE) in survival analysis. In the context of highway engineering, it has been repeatedly applied to estimate stochastic traffic flow capacity. However, this paper demonstrates that PLM is fundamentally unsuitable for this purpose. The method implicitly assumes continuous exposure to failure risk over time - a premise invalid for traffic flow, where intensity does not increase linearly, and capacity is not even directly observable. Although parametric MLE approach offers a viable alternative, its earlier derivation for this use case suffers from flawed likelihood formulation, likely due to attempt to preserve consistency with PLM. This study derives a corrected likelihood formula for stochastic capacity MLE and validates it using two empirical datasets. The proposed method is then applied in a case study examining the effect of a variable speed limit (VSL) system used for traffic flow speed harmonisation at a 2-to-1 lane drop. Results show that the VSL improved capacity by approximately 10 % or reduced breakdown probability at the same flow intensity by up to 50 %. The findings underscore the methodological importance of correct model formulation and highlight the practical relevance of stochastic capacity estimation for evaluating traffic control strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00893v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Igor Mikol\'a\v{s}ek</dc:creator>
    </item>
  </channel>
</rss>
