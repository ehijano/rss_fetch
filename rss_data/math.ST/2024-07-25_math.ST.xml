<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Jul 2024 04:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Estimating the hyperuniformity exponent of point processes</title>
      <link>https://arxiv.org/abs/2407.16797</link>
      <description>arXiv:2407.16797v1 Announce Type: new 
Abstract: We address the challenge of estimating the hyperuniformity exponent $\alpha$ of a spatial point process, given only one realization of it. Assuming that the structure factor $S$ of the point process follows a vanishing power law at the origin (the typical case of a hyperuniform point process), this exponent is defined as the slope near the origin of $\log S$. Our estimator is built upon the (expanding window) asymptotic variance of some wavelet transforms of the point process. By combining several scales and several wavelets, we develop a multi-scale, multi-taper estimator $\widehat{\alpha}$. We analyze its asymptotic behavior, proving its consistency under various settings, and enabling the construction of asymptotic confidence intervals for $\alpha$ when $\alpha &lt; d$ and under Brillinger mixing. This construction is derived from a multivariate central limit theorem where the normalisations are non-standard and vary among the components. We also present a non-asymptotic deviation inequality providing insights into the influence of tapers on the bias-variance trade-off of $\widehat{\alpha}$. Finally, we investigate the performance of $\widehat{\alpha}$ through simulations, and we apply our method to the analysis of hyperuniformity in a real dataset of marine algae.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16797v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Mastrilli, Bart{\l}omiej B{\l}aszczyszyn, Fr\'ed\'eric Lavancier</dc:creator>
    </item>
    <item>
      <title>Convergence of Poisson point processes and of optimal transport regularization with application in variational analysis of PET reconstruction</title>
      <link>https://arxiv.org/abs/2407.17135</link>
      <description>arXiv:2407.17135v1 Announce Type: new 
Abstract: Poisson distributed measurements in inverse problems often stem from Poisson point processes that are observed through discretized or finite-resolution detectors, one of the most prominent examples being positron emission tomography (PET). These inverse problems are typically reconstructed via Bayesian methods. A natural question then is whether and how the reconstruction converges as the signal-to-noise ratio tends to infinity and how this convergence interacts with other parameters such as the detector size. In this article we carry out a corresponding variational analysis for the exemplary Bayesian reconstruction functional from [arXiv:2311.17784,arXiv:1902.07521], which considers dynamic PET imaging (i.e.\ the object to be reconstructed changes over time) and uses an optimal transport regularization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17135v1</guid>
      <category>math.ST</category>
      <category>math.CA</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Mauritz, Benedikt Wirth</dc:creator>
    </item>
    <item>
      <title>Provable Benefit of Annealed Langevin Monte Carlo for Non-log-concave Sampling</title>
      <link>https://arxiv.org/abs/2407.16936</link>
      <description>arXiv:2407.16936v1 Announce Type: cross 
Abstract: We address the outstanding problem of sampling from an unnormalized density that may be non-log-concave and multimodal. To enhance the performance of simple Markov chain Monte Carlo (MCMC) methods, techniques of annealing type have been widely used. However, quantitative theoretical guarantees of these techniques are under-explored. This study takes a first step toward providing a non-asymptotic analysis of annealed MCMC. Specifically, we establish, for the first time, an oracle complexity of $\widetilde{O}\left(\frac{d\beta^2{\cal A}^2}{\varepsilon^6}\right)$ for simple annealed Langevin Monte Carlo algorithm to achieve $\varepsilon^2$ accuracy in Kullback-Leibler divergence to the target distribution $\pi\propto{\rm e}^{-V}$ on $\mathbb{R}^d$ with $\beta$-smooth potential $V$. Here, ${\cal A}$ represents the action of a curve of probability measures interpolating the target distribution $\pi$ and a readily sampleable distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16936v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Guo, Molei Tao, Yongxin Chen</dc:creator>
    </item>
    <item>
      <title>Low dimensional representation of multi-patient flow cytometry datasets using optimal transport for minimal residual disease detection in leukemia</title>
      <link>https://arxiv.org/abs/2407.17329</link>
      <description>arXiv:2407.17329v1 Announce Type: cross 
Abstract: Representing and quantifying Minimal Residual Disease (MRD) in Acute Myeloid Leukemia (AML), a type of cancer that affects the blood and bone marrow, is essential in the prognosis and follow-up of AML patients. As traditional cytological analysis cannot detect leukemia cells below 5\%, the analysis of flow cytometry dataset is expected to provide more reliable results. In this paper, we explore statistical learning methods based on optimal transport (OT) to achieve a relevant low-dimensional representation of multi-patient flow cytometry measurements (FCM) datasets considered as high-dimensional probability distributions. Using the framework of OT, we justify the use of the K-means algorithm for dimensionality reduction of multiple large-scale point clouds through mean measure quantization by merging all the data into a single point cloud. After this quantization step, the visualization of the intra and inter-patients FCM variability is carried out by embedding low-dimensional quantized probability measures into a linear space using either Wasserstein Principal Component Analysis (PCA) through linearized OT or log-ratio PCA of compositional data. Using a publicly available FCM dataset and a FCM dataset from Bordeaux University Hospital, we demonstrate the benefits of our approach over the popular kernel mean embedding technique for statistical learning from multiple high-dimensional probability distributions. We also highlight the usefulness of our methodology for low-dimensional projection and clustering patient measurements according to their level of MRD in AML from FCM. In particular, our OT-based approach allows a relevant and informative two-dimensional representation of the results of the FlowSom algorithm, a state-of-the-art method for the detection of MRD in AML using multi-patient FCM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17329v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erell Gachon, J\'er\'emie Bigot, Elsa Cazelles, Aguirre Mimoun, Jean-Philippe Vial</dc:creator>
    </item>
    <item>
      <title>The piranha problem: Large effects swimming in a small pond</title>
      <link>https://arxiv.org/abs/2105.13445</link>
      <description>arXiv:2105.13445v5 Announce Type: replace 
Abstract: In some scientific fields, it is common to have certain variables of interest that are of particular importance and for which there are many studies indicating a relationship with different explanatory variables. In such cases, particularly those where no relationships are known among the explanatory variables, it is worth asking under what conditions it is possible for all such claimed effects to exist simultaneously. This paper addresses this question by reviewing some theorems from multivariate analysis showing that, unless the explanatory variables also have sizable dependencies with each other, it is impossible to have many such large effects. We discuss implications for the replication crisis in social science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.13445v5</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Tosh, Philip Greengard, Ben Goodrich, Andrew Gelman, Aki Vehtari, Daniel Hsu</dc:creator>
    </item>
    <item>
      <title>Optimal worst-risk minimization in structural equation models with random coefficients</title>
      <link>https://arxiv.org/abs/2307.15350</link>
      <description>arXiv:2307.15350v3 Announce Type: replace 
Abstract: The insight that causal parameters are particularly suitable for out-of-sample prediction has sparked a lot development of causal-like predictors. However, the connection with strict causal targets, has limited the development with good risk minimization properties, but without a direct causal interpretation. In this manuscript we derive the optimal out-of-sample risk minimizing predictor of a certain target $Y$ in a non-linear system $(X,Y)$ that has been trained in several within-sample environments. We consider data from an observation environment, and several shifted environments. Each environment corresponds to a structural equation model (SEM), with random coefficients and with its own shift and noise vector, both in $L^2$. Unlike previous approaches, we also allow shifts in the target value. We define a sieve of out-of-sample environments, consisting of all shifts $\tilde{A}$ that are at most $\gamma$ times as strong as any weighted average of the observed shift vectors. For each $\beta\in\mathbb{R}^p$ we show that the supremum of the risk functions $R_{\tilde{A}}(\beta)$ has a worst-risk decomposition into a (positive) non-linear combination of risk functions, depending on $\gamma$. We then define the set $\mathcal{B}_\gamma$, as minimizers of this risk. The main result of the paper is that there is a unique minimizer ($|\mathcal{B}_\gamma|=1$) that can be consistently estimated by an explicit estimator, outside a set of zero Lebesgue measure in the parameter space. A practical obstacle for the initial method of estimation is that it involves the solution of a general degree polynomials. Therefore, we prove that an approximate estimator using the bisection method is also consistent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15350v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philip Kennerberg, Ernst Wit</dc:creator>
    </item>
    <item>
      <title>Causal Identification for Complex Continuous-time Functional Longitudinal Studies</title>
      <link>https://arxiv.org/abs/2206.12525</link>
      <description>arXiv:2206.12525v4 Announce Type: replace-cross 
Abstract: Real-time monitoring in modern medical research introduces functional longitudinal data, characterized by continuous-time measurements of outcomes, treatments, and confounders. This complexity leads to uncountably infinite treatment-confounder feedbacks, which traditional causal inference methodologies cannot handle. Inspired by the coarsened data framework, we adopt stochastic process theory, measure theory, and net convergence to propose a nonparametric causal identification framework. This framework generalizes classical g-computation, inverse probability weighting, and doubly robust formulas, accommodating time-varying outcomes subject to mortality and censoring. Our approach addresses significant gaps in current methodologies, providing a solution for complex, real-time longitudinal data and paving the way for future estimation work in this domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.12525v4</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Ying</dc:creator>
    </item>
    <item>
      <title>When Does Bottom-up Beat Top-down in Hierarchical Community Detection?</title>
      <link>https://arxiv.org/abs/2306.00833</link>
      <description>arXiv:2306.00833v2 Announce Type: replace-cross 
Abstract: Hierarchical clustering of networks consists in finding a tree of communities, such that lower levels of the hierarchy reveal finer-grained community structures. There are two main classes of algorithms tackling this problem. Divisive ($\textit{top-down}$) algorithms recursively partition the nodes into two communities, until a stopping rule indicates that no further split is needed. In contrast, agglomerative ($\textit{bottom-up}$) algorithms first identify the smallest community structure and then repeatedly merge the communities using a $\textit{linkage}$ method. In this article, we establish theoretical guarantees for the recovery of the hierarchical tree and community structure of a Hierarchical Stochastic Block Model by a bottom-up algorithm. We also establish that this bottom-up algorithm attains the information-theoretic threshold for exact recovery at intermediate levels of the hierarchy. Notably, these recovery conditions are less restrictive compared to those existing for top-down algorithms. This shows that bottom-up algorithms extend the feasible region for achieving exact recovery at intermediate levels. Numerical experiments on both synthetic and real data sets confirm the superiority of bottom-up algorithms over top-down algorithms. We also observe that top-down algorithms can produce dendrograms with inversions. These findings contribute to a better understanding of hierarchical clustering techniques and their applications in network analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.00833v2</guid>
      <category>cs.SI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maximilien Dreveton, Daichi Kuroda, Matthias Grossglauser, Patrick Thiran</dc:creator>
    </item>
    <item>
      <title>Permutation Capacity Region of Adder Multiple-Access Channels</title>
      <link>https://arxiv.org/abs/2309.08054</link>
      <description>arXiv:2309.08054v2 Announce Type: replace-cross 
Abstract: Point-to-point permutation channels are useful models of communication networks and biological storage mechanisms and have received theoretical attention in recent years. Propelled by relevant advances in this area, we analyze the permutation adder multiple-access channel (PAMAC) in this work. In the PAMAC network model, $d$ senders communicate with a single receiver by transmitting $p$-ary codewords through an adder multiple-access channel whose output is subsequently shuffled by a random permutation block. We define a suitable notion of permutation capacity region $\mathcal{C}_\mathsf{perm}$ for this model, and establish that $\mathcal{C}_\mathsf{perm}$ is the simplex consisting of all rate $d$-tuples that sum to $d(p - 1) / 2$ or less. We achieve this sum-rate by encoding messages as i.i.d. samples from categorical distributions with carefully chosen parameters, and we derive an inner bound on $\mathcal{C}_\mathsf{perm}$ by extending the concept of time sharing to the permutation channel setting. Our proof notably illuminates various connections between mixed-radix numerical systems and coding schemes for multiple-access channels. Furthermore, we derive an alternative inner bound on $\mathcal{C}_\mathsf{perm}$ for the binary PAMAC by analyzing the root stability of the probability generating function of the adder's output distribution. Using eigenvalue perturbation results, we obtain error bounds on the spectrum of the probability generating function's companion matrix, providing quantitative estimates of decoding performance. Finally, we obtain a converse bound on $\mathcal{C}_\mathsf{perm}$ matching our achievability result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08054v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TIT.2024.3350436</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Information Theory, vol. 70, no. 7, Jul. 2024</arxiv:journal_reference>
      <dc:creator>William Lu, Anuran Makur</dc:creator>
    </item>
    <item>
      <title>Efficient Unbiased Sparsification</title>
      <link>https://arxiv.org/abs/2402.14925</link>
      <description>arXiv:2402.14925v2 Announce Type: replace-cross 
Abstract: An unbiased $m$-sparsification of a vector $p\in \mathbb{R}^n$ is a random vector $Q\in \mathbb{R}^n$ with mean $p$ that has at most $m&lt;n$ nonzero coordinates. Unbiased sparsification compresses the original vector without introducing bias; it arises in various contexts, such as in federated learning and sampling sparse probability distributions. Ideally, unbiased sparsification should also minimize the expected value of a divergence function $\mathsf{Div}(Q,p)$ that measures how far away $Q$ is from the original $p$. If $Q$ is optimal in this sense, then we call it efficient. Our main results describe efficient unbiased sparsifications for divergences that are either permutation-invariant or additively separable. Surprisingly, the characterization for permutation-invariant divergences is robust to the choice of divergence function, in the sense that our class of optimal $Q$ for squared Euclidean distance coincides with our class of optimal $Q$ for Kullback-Leibler divergence, or indeed any of a wide variety of divergences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14925v2</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leighton Barnes, Stephen Cameron, Timothy Chow, Emma Cohen, Keith Frankston, Benjamin Howard, Fred Kochman, Daniel Scheinerman, Jeffrey VanderKam</dc:creator>
    </item>
    <item>
      <title>Correlation functions between singular values and eigenvalues</title>
      <link>https://arxiv.org/abs/2403.19157</link>
      <description>arXiv:2403.19157v3 Announce Type: replace-cross 
Abstract: Exploiting the explicit bijection between the density of singular values and the density of eigenvalues for bi-unitarily invariant complex random matrix ensembles of finite matrix size, we aim at finding the induced probability measure on $j$ eigenvalues and $k$ singular values that we coin $j,k$-point correlation measure. We find an expression for the $1,k$-point correlation measure which simplifies drastically when assuming that the singular values follow a polynomial ensemble, yielding a closed formula in terms of the kernel corresponding to the determinantal point process of the singular value statistics. These expressions simplify even further when the singular values are drawn from a P\'{o}lya ensemble and extend known results between the eigenvalue and singular value statistics of the corresponding bi-unitarily invariant ensemble.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19157v3</guid>
      <category>math.PR</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Allard, Mario Kieburg</dc:creator>
    </item>
    <item>
      <title>High-dimensional Covariance Estimation by Pairwise Likelihood Truncation</title>
      <link>https://arxiv.org/abs/2407.07717</link>
      <description>arXiv:2407.07717v2 Announce Type: replace-cross 
Abstract: Pairwise likelihood is a useful approximation to the full likelihood function for covariance estimation in high-dimensional context. It simplifies high-dimensional dependencies by combining marginal bivariate likelihood objects, thus making estimation more manageable. In certain models, including the Gaussian model, both pairwise and full likelihoods are maximized by the same parameter values, thus retaining optimal statistical efficiency, when the number of variables is fixed. Leveraging on this insight, we introduce estimation of sparse high-dimensional covariance matrices by maximizing a truncated version of the pairwise likelihood function, obtained by including pairwise terms corresponding to nonzero covariance elements. To achieve a meaningful truncation, we propose to minimize the $L_2$-distance between pairwise and full likelihood scores plus an $L_1$-penalty discouraging the inclusion of uninformative terms. Differently from other regularization approaches, our method focuses on selecting whole pairwise likelihood objects rather than shrinking individual covariance parameters, thus retaining the inherent unbiasedness of the pairwise likelihood estimating equations. This selection procedure is shown to have the selection consistency property as the covariance dimension increases exponentially fast. Consequently, the implied pairwise likelihood estimator is consistent and converges to the oracle maximum likelihood estimator assuming knowledge of nonzero covariance entries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07717v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessandro Casa, Davide Ferrari, Zhendong Huang</dc:creator>
    </item>
  </channel>
</rss>
