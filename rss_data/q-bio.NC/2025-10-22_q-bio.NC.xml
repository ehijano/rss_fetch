<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 Oct 2025 01:42:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Standard Model of the Retina</title>
      <link>https://arxiv.org/abs/2510.17820</link>
      <description>arXiv:2510.17820v1 Announce Type: new 
Abstract: The scientific study of the retina has reached a remarkable state of completion. We can now explain many aspects of early visual processing based on a relatively simple model of neural circuitry in the retina. The same model, with different parameters, produces a great diversity of neural computations. In this article I lay out what that "standard model" is and how it accounts for such a diversity of phenomena. The emergence of such a powerful standard model is unique in systems neuroscience, and I consider what conditions made it possible. The standard model now serves as a baseline from which to organize future retinal research, either by testing the model's assumptions directly, or by identifying phenomena that remain unexplained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17820v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markus Meister</dc:creator>
    </item>
    <item>
      <title>A Biophysical-Model-Informed Source Separation Framework For EMG Decomposition</title>
      <link>https://arxiv.org/abs/2510.17822</link>
      <description>arXiv:2510.17822v1 Announce Type: new 
Abstract: Recent advances in neural interfacing have enabled significant improvements in human-computer interaction, rehabilitation, and neuromuscular diagnostics. Motor unit (MU) decomposition from surface electromyography (sEMG) is a key technique for extracting neural drive information, but traditional blind source separation (BSS) methods fail to incorporate biophysical constraints, limiting their accuracy and interpretability. In this work, we introduce a novel Biophysical-Model-Informed Source Separation (BMISS) framework, which integrates anatomically accurate forward EMG models into the decomposition process. By leveraging MRI-based anatomical reconstructions and generative modeling, our approach enables direct inversion of a biophysically accurate forward model to estimate both neural drive and motor neuron properties in an unsupervised manner. Empirical validation in a controlled simulated setting demonstrates that BMISS achieves higher fidelity motor unit estimation while significantly reducing computational cost compared to traditional methods. This framework paves the way for non-invasive, personalized neuromuscular assessments, with potential applications in clinical diagnostics, prosthetic control, and neurorehabilitation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17822v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>D. Halatsis, P. Mamidanna, J. Pereira, D. Farina</dc:creator>
    </item>
    <item>
      <title>Brain-Language Model Alignment: Insights into the Platonic Hypothesis and Intermediate-Layer Advantage</title>
      <link>https://arxiv.org/abs/2510.17833</link>
      <description>arXiv:2510.17833v1 Announce Type: new 
Abstract: Do brains and language models converge toward the same internal representations of the world? Recent years have seen a rise in studies of neural activations and model alignment. In this work, we review 25 fMRI-based studies published between 2023 and 2025 and explicitly confront their findings with two key hypotheses: (i) the Platonic Representation Hypothesis -- that as models scale and improve, they converge to a representation of the real world, and (ii) the Intermediate-Layer Advantage -- that intermediate (mid-depth) layers often encode richer, more generalizable features. Our findings provide converging evidence that models and brains may share abstract representational structures, supporting both hypotheses and motivating further research on brain-model alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17833v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>\'Angela L\'opez-Cardona, Sebasti\'an Idesis, Mireia Masias-Bruns, Sergi Abadal, Ioannis Arapakis</dc:creator>
    </item>
    <item>
      <title>Dynamic threshold curves and response precision in forced excitable systems</title>
      <link>https://arxiv.org/abs/2510.17837</link>
      <description>arXiv:2510.17837v1 Announce Type: new 
Abstract: We investigate here various properties of the responses of excitable systems subject to periodic forcing and noise. While the properties of intrinsic oscillators, subject to added periodic signals, are well understood, much less is known about the factors that determine the response precision of excitable units, intrinsically at rest, when activated by periodic forcing and stochastic noise. One motivation for considering this issue comes from the behavior of auditory neurons. These neurons reportedly have the ability to fire spikes in a precise range of phases in response to incoming sound waves, a behavior for which the mechanism is unknown. To account for such a response precision, we introduce the notion of dynamic threshold curve (DTC), which estimates at each time the effective likelihood that noise will subsequently generate a spike. The DTC effectively summarizes, in a single curve, a representation of the response precision of an excitable model, as we demonstrate by showing that the distribution of spike times produced in this setting is well captured by the first passage time of a simple, Gaussian stochastic process to the distance to the DTC. This result shows that peaks and troughs of the DTC, but also their slopes, convey fine information about spike timing in response to noise. In particular, it explains properties of Type 2 and Type 3 excitable cells studied previously and provides a framework to predict the DTC properties necessary to support the response precision of auditory neurons, as we illustrate in a well-established auditory neuron model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17837v1</guid>
      <category>q-bio.NC</category>
      <category>math.DS</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan E. Rubin, Justyna Signerska-Rynkowska, Jonathan Touboul</dc:creator>
    </item>
    <item>
      <title>Decoding Dynamic Visual Experience from Calcium Imaging via Cell-Pattern-Aware SSL</title>
      <link>https://arxiv.org/abs/2510.18516</link>
      <description>arXiv:2510.18516v1 Announce Type: new 
Abstract: Self-supervised learning (SSL) holds a great deal of promise for applications in neuroscience, due to the lack of large-scale, consistently labeled neural datasets. However, most neural datasets contain heterogeneous populations that mix stable, predictable cells with highly stochastic, stimulus-contingent ones, which has made it hard to identify consistent activity patterns during SSL. As a result, self-supervised pretraining has yet to show clear signs of benefits from scale on neural data. Here, we present a novel approach to self-supervised pretraining, POYO-SSL that exploits the heterogeneity of neural data to improve pre-training and achieve benefits of scale. Specifically, in POYO-SSL we pretrain only on predictable (statistically regular) neurons-identified on the pretraining split via simple higher-order statistics (skewness and kurtosis)-then we fine-tune on the unpredictable population for downstream tasks. On the Allen Brain Observatory dataset, this strategy yields approximately 12-13% relative gains over from-scratch training and exhibits smooth, monotonic scaling with model size. In contrast, existing state-of-the-art baselines plateau or destabilize as model size increases. By making predictability an explicit metric for crafting the data diet, POYO-SSL turns heterogeneity from a liability into an asset, providing a robust, biologically grounded recipe for scalable neural decoding and a path toward foundation models of neural dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18516v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sangyoon Bae, Mehdi Azabou, Jiook Cha, Blake Richards</dc:creator>
    </item>
    <item>
      <title>Information Capacity of EEG: Theoretical and Computational Limits of Recoverable Neural Information</title>
      <link>https://arxiv.org/abs/2510.17841</link>
      <description>arXiv:2510.17841v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) is widely used to study human brain dynamics, yet its quantitative information capacity remains unclear. Here, we combine information theory and synthetic forward modeling to estimate the mutual information between latent cortical sources and EEG recordings. Using Gaussian-channel theory and empirical simulations, we find that scalp EEG conveys only tens of bits per sample about low-dimensional neural activity. Information saturates with approximately 64-128 electrodes and scales logarithmically with signal-to-noise ratio (SNR). Linear decoders capture nearly all variance that is linearly recoverable, but the mutual information they recover remains far below the analytic channel capacity, indicating that measurement physics - not algorithmic complexity - is the dominant limitation. These results outline the intrinsic ceiling on how much structure about brain state or thought content can be inferred from EEG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17841v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ishir Rao</dc:creator>
    </item>
    <item>
      <title>Self-Evidencing Through Hierarchical Gradient Decomposition: A Dissipative System That Maintains Non-Equilibrium Steady-State by Minimizing Variational Free Energy</title>
      <link>https://arxiv.org/abs/2510.17916</link>
      <description>arXiv:2510.17916v1 Announce Type: cross 
Abstract: The Free Energy Principle (FEP) states that self-organizing systems must minimize variational free energy to persist, but the path from principle to implementable algorithm has remained unclear. We present a constructive proof that the FEP can be realized through exact local credit assignment. The system decomposes gradient computation hierarchically: spatial credit via feedback alignment, temporal credit via eligibility traces, and structural credit via a Trophic Field Map (TFM) that estimates expected gradient magnitude for each connection block. We prove these mechanisms are exact at their respective levels and validate the central claim empirically: the TFM achieves 0.9693 Pearson correlation with oracle gradients. This exactness produces emergent capabilities including 98.6% retention after task interference, autonomous recovery from 75% structural damage, self-organized criticality (spectral radius p ~= 1.0$), and sample-efficient reinforcement learning on continuous control tasks without replay buffers. The architecture unifies Prigogine's dissipative structures, Friston's free energy minimization, and Hopfield's attractor dynamics, demonstrating that exact hierarchical inference over network topology can be implemented with local, biologically plausible rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17916v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael James McCulloch</dc:creator>
    </item>
    <item>
      <title>Benchmarking Probabilistic Time Series Forecasting Models on Neural Activity</title>
      <link>https://arxiv.org/abs/2510.18037</link>
      <description>arXiv:2510.18037v2 Announce Type: cross 
Abstract: Neural activity forecasting is central to understanding neural systems and enabling closed-loop control. While deep learning has recently advanced the state-of-the-art in the time series forecasting literature, its application to neural activity forecasting remains limited. To bridge this gap, we systematically evaluated eight probabilistic deep learning models, including two foundation models, that have demonstrated strong performance on general forecasting benchmarks. We compared them against four classical statistical models and two baseline methods on spontaneous neural activity recorded from mouse cortex via widefield imaging. Across prediction horizons, several deep learning models consistently outperformed classical approaches, with the best model producing informative forecasts up to 1.5 seconds into the future. Our findings point toward future control applications and open new avenues for probing the intrinsic temporal structure of neural activity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18037v2</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyu Lu, Anna J. Li, Alexander E. Ladd, Pascha Matveev, Aditya Deole, Eric Shea-Brown, J. Nathan Kutz, Nicholas A. Steinmetz</dc:creator>
    </item>
    <item>
      <title>On Biologically Plausible Learning in Continuous Time</title>
      <link>https://arxiv.org/abs/2510.18808</link>
      <description>arXiv:2510.18808v1 Announce Type: cross 
Abstract: Biological learning unfolds continuously in time, yet most algorithmic models rely on discrete updates and separate inference and learning phases. We study a continuous-time neural model that unifies several biologically plausible learning algorithms and removes the need for phase separation. Rules including stochastic gradient descent (SGD), feedback alignment (FA), direct feedback alignment (DFA), and Kolen-Pollack (KP) emerge naturally as limiting cases of the dynamics. Simulations show that these continuous-time networks stably learn at biological timescales, even under temporal mismatches and integration noise. Through analysis and simulation, we show that learning depends on temporal overlap: a synapse updates correctly only when its input and the corresponding error signal coincide in time. When inputs are held constant, learning strength declines linearly as the delay between input and error approaches the stimulus duration, explaining observed robustness and failure across network depths. Critically, robust learning requires the synaptic plasticity timescale to exceed the stimulus duration by one to two orders of magnitude. For typical cortical stimuli (tens of milliseconds), this places the functional plasticity window in the few-second range, a testable prediction that identifies seconds-scale eligibility traces as necessary for error-driven learning in biological circuits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18808v1</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc Gong Bacvanski, Liu Ziyin, Tomaso Poggio</dc:creator>
    </item>
    <item>
      <title>Driving factors of auditory category learning success</title>
      <link>https://arxiv.org/abs/2506.01508</link>
      <description>arXiv:2506.01508v2 Announce Type: replace 
Abstract: Our brain learns to update its mental model of the environment by abstracting sensory experiences for adaptation and survival. Learning to categorize sounds is one essential abstracting process for high-level human cognition, such as speech perception, but it is also challenging due to the variable nature of auditory signals and their dynamic contexts. To overcome these learning challenges and enhance learner performance, it is essential to identify the impact of learning-related factors in developing better training protocols. Here, we conducted an extensive meta-analysis of auditory category learning studies, including a total of 111 experiments and 4,521 participants, and examined to what extent three hidden factors (i.e., variability, intensity, and engagement) derived from 12 experimental variables contributed to learning success (i.e., effect sizes). Variables related to intensity and training variability outweigh others in predicting learning effect size. Activation likelihood estimation (ALE) meta-analysis of the neuroimaging studies revealed training-induced systematic changes in the frontotemporal-parietal networks. Increased brain activities in speech and motor-related auditory-frontotemporal regions and decreased activities in cuneus and precuneus areas are associated with increased learning effect sizes. These findings not only enhance our understanding of the driving forces behind speech and auditory category learning success, along with its neural changes, but also guide researchers and practitioners in designing more effective training protocols that consider the three key aspects of learning to facilitate learner success.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01508v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nan Wang, Gangyi Feng</dc:creator>
    </item>
    <item>
      <title>Brain-Like Processing Pathways Form in Models With Heterogeneous Experts</title>
      <link>https://arxiv.org/abs/2506.02813</link>
      <description>arXiv:2506.02813v2 Announce Type: replace 
Abstract: Examples of such pathways can be found in the interactions between cortical and subcortical networks during learning, or in sub-networks specializing for task characteristics such as difficulty or modality. Despite the large role these pathways play in cognition, the mechanisms through which brain regions organize into pathways remain unclear. In this work, we use an extension of the Heterogeneous Mixture-of-Experts architecture to show that heterogeneous regions do not form processing pathways by themselves, implying that the brain likely implements specific constraints which result in the reliable formation of pathways. We identify three biologically relevant inductive biases that encourage pathway formation: a routing cost imposed on the use of more complex regions, a scaling factor that reduces this cost when task performance is low, and randomized expert dropout. When comparing our resulting \textit{Mixture-of-Pathways} model with the brain, we observe that the artificial pathways in our model match how the brain uses cortical and subcortical systems to learn and solve tasks of varying difficulty. In summary, we introduce a novel framework for investigating how the brain forms task-specific pathways through inductive biases, and the effects these biases have on the behavior of Mixture-of-Experts models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02813v2</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jack Cook, Danyal Akarca, Rui Ponte Costa, Jascha Achterberg</dc:creator>
    </item>
    <item>
      <title>Joint encoding of "what" and "when" predictions through error-modulated plasticity in reservoir spiking networks</title>
      <link>https://arxiv.org/abs/2510.14382</link>
      <description>arXiv:2510.14382v2 Announce Type: replace 
Abstract: The brain predicts the external world through an internal model refined by prediction errors. A complete prediction specifies what will happen, when it will happen, and with what probability, a construct we call the "prediction object." Existing models usually capture only what and when, omit probabilities, and rely on algorithms that are not biologically plausible. We show that a single population of spiking neurons can learn the full prediction object through a biologically grounded three factor Hebbian rule. In a heterogeneous Izhikevich reservoir, online timing learning and offline identity consolidation allow the network to fire at the correct times with amplitudes proportional to probability and to adapt instantly when the environment changes. Unlike global least squares methods such as FORCE, which require resets to relearn, our model recalibrates continuously through local error gated modulation. This single circuit provides a biologically grounded, flexible mechanism for predictive cognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14382v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yohei Yamada, Zenas C. Chao</dc:creator>
    </item>
  </channel>
</rss>
