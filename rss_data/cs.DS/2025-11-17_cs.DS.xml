<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Nov 2025 04:04:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Discounted Cuts: A Stackelberg Approach to Network Disruption</title>
      <link>https://arxiv.org/abs/2511.10804</link>
      <description>arXiv:2511.10804v1 Announce Type: new 
Abstract: We study a Stackelberg variant of the classical Most Vital Links problem, modeled as a one-round adversarial game between an attacker and a defender. The attacker strategically removes up to $k$ edges from a flow network to maximally disrupt flow between a source $s$ and a sink $t$, after which the defender optimally reroutes the remaining flow. To capture this attacker--defender interaction, we introduce a new mathematical model of discounted cuts, in which the cost of a cut is evaluated by excluding its $k$ most expensive edges. This model generalizes the Most Vital Links problem and uncovers novel algorithmic and complexity-theoretic properties.
  We develop a unified algorithmic framework for analyzing various forms of discounted cut problems, including minimizing or maximizing the cost of a cut under discount mechanisms that exclude either the $k$ most expensive or the $k$ cheapest edges. While most variants are NP-complete on general graphs, our main result establishes polynomial-time solvability for all discounted cut problems in our framework when the input is restricted to bounded-genus graphs, a relevant class that includes many real-world networks such as transportation and infrastructure networks. With this work, we aim to open collaborative bridges between artificial intelligence, algorithmic game theory, and operations research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10804v1</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>P{\aa}l Gr{\o}n{\aa}s Drange, Fedor V. Fomin, Petr Golovach, Danil Sagunov</dc:creator>
    </item>
    <item>
      <title>Beating Meet-in-the-Middle for Subset Balancing Problems</title>
      <link>https://arxiv.org/abs/2511.10823</link>
      <description>arXiv:2511.10823v1 Announce Type: new 
Abstract: We consider exact algorithms for Subset Balancing, a family of related problems that generalizes Subset Sum, Partition, and Equal Subset Sum. Specifically, given as input an integer vector $\vec{x} \in \mathbb{Z}^n$ and a constant-size coefficient set $C \subset \mathbb{Z}$, we seek a nonzero solution vector $\vec{c} \in C^n$ satisfying $\vec{c} \cdot \vec{x} = 0$.
  For $C = \{-d,\ldots,d\}$, $d &gt; 1$ and $C = \{-d,\ldots,d\}\setminus\{0\}$, $d &gt; 2$, we present algorithms that run in time $O(|C|^{(0.5 - \epsilon)n})$ for a constant $\epsilon &gt; 0$ that depends only on $C$. These are the first algorithms that break the $O(|C|^{n/2})$-time ``Meet-in-the-Middle barrier'' for these coefficient sets in the worst case. This improves on the result of Chen, Jin, Randolph and Servedio (SODA 2022), who broke the Meet-in-the-Middle barrier on these coefficient sets in the average-case setting. We also improve the best exact algorithm for Equal Subset Sum (Subset Balancing with $C = \{-1,0,1\}$), due to Mucha, Nederlof, Pawlewicz, and W\k{e}grzycki (ESA 2019), by an exponential margin. This positively answers an open question of Jin, Williams, and Zhang (ESA 2025). Our results leave two natural cases in which we cannot yet break the Meet-in-the-Middle barrier: $C = \{-2, -1, 1, 2\}$ and $C = \{-1, 1\}$ (Partition).
  Our results bring the representation technique of Howgrave-Graham and Joux (CRYPTO 2010) from average-case to worst-case inputs for many $C$. This requires a variety of new techniques: we present strategies for (1) achieving good ``mixing'' with worst-case inputs, (2) creating flexible input representations for coefficient sets without 0, and (3) quickly recovering compatible solution pairs from sets of vectors containing ``pseudosolution pairs''. These techniques may find application to other algorithmic problems on integer sums or be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10823v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim Randolph, Karol W\k{e}grzycki</dc:creator>
    </item>
    <item>
      <title>A number-theoretic conjecture implying faster algorithms for polynomial factorization and integer factorization</title>
      <link>https://arxiv.org/abs/2511.10851</link>
      <description>arXiv:2511.10851v1 Announce Type: new 
Abstract: The fastest known algorithm for factoring a degree $n$ univariate polynomial over a finite field $\mathbb{F}_q$ runs in time $O(n^{3/2 + o(1)}\text{polylog } q)$, and there is a reason to believe that the $3/2$ exponent represents a ''barrier'' inherent in algorithms that employ a so-called baby-steps-giant-steps strategy. In this paper, we propose a new strategy with the potential to overcome the $3/2$ barrier. In doing so we are led to a number-theoretic conjecture, one form of which is that there are sets $S, T$ of cardinality $n^\beta$, consisting of positive integers of magnitude at most $\exp(n^\alpha)$, such that every integer $i \in [n]$ divides $s-t$ for some $s \in S, t \in T$. Achieving $\alpha + \beta \le 1 + o(1)$ is trivial; we show that achieving $\alpha, \beta &lt; 1/2$ (together with an assumption that $S, T$ are structured) implies an improvement to the exponent 3/2 for univariate polynomial factorization. Achieving $\alpha = \beta = 1/3$ is best-possible and would imply an exponent 4/3 algorithm for univariate polynomial factorization. Interestingly, a second consequence would be a reduction of the current-best exponent for deterministic (exponential) algorithms for factoring integers, from $1/5$ to $1/6$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10851v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>math.NT</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chris Umans, Siki Wang</dc:creator>
    </item>
    <item>
      <title>Cycle Basis Algorithms for Reducing Maximum Edge Participation</title>
      <link>https://arxiv.org/abs/2511.10961</link>
      <description>arXiv:2511.10961v1 Announce Type: new 
Abstract: We study the problem of constructing cycle bases of graphs with low maximum edge participation, defined as the maximum number of basis cycles that share any single edge. This quantity, though less studied than total weight or length, plays a critical role in quantum fault tolerance because it directly impacts the overhead of lattice surgery procedures used to implement an almost universal quantum gate set. Building on a recursive algorithm of Freedman and Hastings, we introduce a family of load-aware heuristics that adaptively select vertices and edges to minimize edge participation throughout the cycle basis construction. Our approach improves empirical performance on random regular graphs and on graphs derived from small quantum codes. We further analyze a simplified balls-into-bins process to establish lower bounds on edge participation. While the model differs from the cycle basis algorithm on real graphs, it captures what can be proven for our heuristics without using complex graph theoretic properties related to the distribution of cycles in the graph. Our analysis suggests that the maximum load of our heuristics grows on the order of (log n)^2. Our results indicate that careful cycle basis construction can yield significant practical benefits in the design of fault-tolerant quantum systems. This question also carries theoretical interest, as it is essentially identical to the basis number of a graph, defined as the minimum possible maximum edge participation over all cycle bases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10961v1</guid>
      <category>cs.DS</category>
      <category>quant-ph</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Wang, Sandy Irani</dc:creator>
    </item>
    <item>
      <title>R-enum Revisited: Speedup and Extension for Context-Sensitive Repeats and Net Frequencies</title>
      <link>https://arxiv.org/abs/2511.11057</link>
      <description>arXiv:2511.11057v1 Announce Type: new 
Abstract: Nishimoto and Tabei [CPM, 2021] proposed r-enum, an algorithm to enumerate various characteristic substrings, including maximal repeats, in a string $T$ of length $n$ in $O(r)$ words of compressed working space, where $r \le n$ is the number of runs in the Burrows-Wheeler transform (BWT) of $T$. Given the run-length encoded BWT (RLBWT) of $T$, r-enum runs in $O(n\log\log_{w}(n/r))$ time in addition to the time linear to the number of output strings, where $w=\Theta(\log n)$ is the word size. In this paper, we improve the $O(n\log\log_{w}(n/r))$ term to $O(n)$. We also extend r-enum to compute other context-sensitive repeats such as near-supermaximal repeats (NSMRs) and supermaximal repeats, and the context diversity for every maximal repeat in the same complexities. Furthermore, we study the occurrences that witness NSMRs, which have recently attracted attention under the name of net occurrences: An occurrence of a repeat is called a net occurrence if it is not covered by another repeat, and the net frequency of a repeat is the number of its net occurrences. With this terminology, an NSMR is defined to be a repeat with a positive net frequency. Given the RLBWT of $T$, we show how to compute the set $S^{nsmr}$ of all NSMRs in $T$ together with their net frequency/occurrences in $O(n)$ time and $O(r)$ space. We also show that an $O(r)$-space data structure can be built from the RLBWT to support queries of computing the net frequency of any query pattern $P$ in $O(|P|)$ time. The data structure is built in $O(r)$ space and in $O(n)$ time with high probability or deterministic $O(n+|S^{nsmr}|\log\log\min(\sigma,|S^{nsmr}|))$ time, where $\sigma\le r$ is the alphabet size of $T$. To achieve this, we prove that the total number of net occurrences is less than $2r$. We also get a new upper bound $2r$ of the number of minimal unique substrings in $T$, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11057v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kotaro Kimura, Tomohiro I</dc:creator>
    </item>
    <item>
      <title>An Efficient Algorithm for Minimizing Ordered Norms in Fractional Load Balancing</title>
      <link>https://arxiv.org/abs/2511.11237</link>
      <description>arXiv:2511.11237v1 Announce Type: new 
Abstract: We study the problem of minimizing an ordered norm of a load vector (indexed by a set of $d$ resources), where a finite number $n$ of customers $c$ contribute to the load of each resource by choosing a solution $x_c$ in a convex set $X_c \subseteq \mathbb{R}^d_{\geq 0}$; so we minimize $||\sum_{c}x_c||$ for some fixed ordered norm $||\cdot||$. We devise a randomized algorithm that computes a $(1+\varepsilon)$-approximate solution to this problem and makes, with high probability, $\mathcal{O}((n+d) (\varepsilon^{-2}+\log\log d)\log (n+d))$ calls to oracles that minimize linear functions (with non-negative coefficients) over $X_c$. While this has been known for the $\ell_{\infty}$ norm via the multiplicative weights update method, existing proof techniques do not extend to arbitrary ordered norms. Our algorithm uses a resource price mechanism that is motivated by the follow-the-regularized-leader paradigm, and is expressed by smooth approximations of ordered norms. We need and show that these have non-trivial stability properties, which may be of independent interest. For each customer, we define dynamic cost budgets, which evolve throughout the algorithm, to determine the allowed step sizes. This leads to non-uniform updates and may even reject certain oracle solutions. Using non-uniform sampling together with a martingale argument, we can guarantee sufficient expected progress in each iteration, and thus bound the total number of oracle calls with high probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11237v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel Blankenburg, Antonia Ellerbrock, Thomas Kesselheim, Jens Vygen</dc:creator>
    </item>
    <item>
      <title>Improved Differentially Private Algorithms for Rank Aggregation</title>
      <link>https://arxiv.org/abs/2511.11319</link>
      <description>arXiv:2511.11319v1 Announce Type: new 
Abstract: Rank aggregation is a task of combining the rankings of items from multiple users into a single ranking that best represents the users' rankings. Alabi et al. (AAAI'22) presents differentially-private (DP) polynomial-time approximation schemes (PTASes) and $5$-approximation algorithms with certain additive errors for the Kemeny rank aggregation problem in both central and local models. In this paper, we present improved DP PTASes with smaller additive error in the central model. Furthermore, we are first to study the footrule rank aggregation problem under DP. We give a near-optimal algorithm for this problem; as a corollary, this leads to 2-approximation algorithms with the same additive error as the $5$-approximation algorithms of Alabi et al. for the Kemeny rank aggregation problem in both central and local models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11319v1</guid>
      <category>cs.DS</category>
      <category>cs.CR</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quentin Hillebrand, Pasin Manurangsi, Vorapong Suppakitpaisarn, Phanu Vajanopath</dc:creator>
    </item>
    <item>
      <title>Learning and Testing Convex Functions</title>
      <link>https://arxiv.org/abs/2511.11498</link>
      <description>arXiv:2511.11498v1 Announce Type: new 
Abstract: We consider the problems of \emph{learning} and \emph{testing} real-valued convex functions over Gaussian space. Despite the extensive study of function convexity across mathematics, statistics, and computer science, its learnability and testability have largely been examined only in discrete or restricted settings -- typically with respect to the Hamming distance, which is ill-suited for real-valued functions.
  In contrast, we study these problems in high dimensions under the standard Gaussian measure, assuming sample access to the function and a mild smoothness condition, namely Lipschitzness. A smoothness assumption is natural and, in fact, necessary even in one dimension: without it, convexity cannot be inferred from finitely many samples. As our main results, we give:
  - Learning Convex Functions: An agnostic proper learning algorithm for Lipschitz convex functions that achieves error $\varepsilon$ using $n^{O(1/\varepsilon^2)}$ samples, together with a complementary lower bound of $n^{\mathrm{poly}(1/\varepsilon)}$ samples in the \emph{correlational statistical query (CSQ)} model.
  - Testing Convex Functions: A tolerant (two-sided) tester for convexity of Lipschitz functions with the same sample complexity (as a corollary of our learning result), and a one-sided tester (which never rejects convex functions) using $O(\sqrt{n}/\varepsilon)^n$ samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11498v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Renato Ferreira Pinto Jr., Cassandra Marcussen, Elchanan Mossel, Shivam Nadimpalli</dc:creator>
    </item>
    <item>
      <title>Faster MAX-CUT on Bounded Threshold Rank Graphs</title>
      <link>https://arxiv.org/abs/2511.11499</link>
      <description>arXiv:2511.11499v1 Announce Type: new 
Abstract: We design new algorithms for approximating 2CSPs on graphs with bounded threshold rank, that is, whose normalized adjacency matrix has few eigenvalues larger than $\varepsilon$, smaller than $-\varepsilon$, or both. Unlike on worst-case graphs, 2CSPs on bounded threshold rank graphs can be $(1+O(\varepsilon))$-approximated efficiently. Prior approximation algorithms for this problem run in time exponential in the threshold rank and $1/\varepsilon$. Our algorithm has running time which is polynomial in $1/\varepsilon$ and exponential in the threshold rank of the label-extended graph, and near-linear in the input size. As a consequence, we obtain the first $(1+O(\varepsilon))$ approximation for MAX-CUT on bounded threshold rank graphs running in $\mathrm{poly}(1/\varepsilon)$ time. We also improve the state-of-the-art running time for 2CSPs on bounded threshold-rank graphs from polynomial in input size to near-linear via a new comparison inequality between the threshold rank of the label-extended graph and base graph. Our algorithm is a simple yet novel combination of subspace enumeration and semidefinite programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11499v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prashanti Anderson, Samuel B. Hopkins, Amit Rajaraman, David Steurer</dc:creator>
    </item>
    <item>
      <title>Support Recovery in One-bit Compressed Sensing with Near-Optimal Measurements and Sublinear Time</title>
      <link>https://arxiv.org/abs/2511.10777</link>
      <description>arXiv:2511.10777v2 Announce Type: cross 
Abstract: The problem of support recovery in one-bit compressed sensing (1bCS) aim to recover the support of a signal $x\in \mathbb{R}^n$, denoted as supp$(x)$, from the observation $y=\text{sign}(Ax)$, where $A\in \mathbb{R}^{m\times n}$ is a sensing matrix and $|\text{supp}(x)|\leq k, k \ll n$. Under this setting, most preexisting works have a recovery runtime $\Omega(n)$. In this paper, we propose two schemes that have sublinear $o(n)$ runtime. (1): For the universal exact support recovery, a scheme of $m=O(k^2\log(n/k)\log n)$ measurements and runtime $D=O(km)$. For the universal $\epsilon$-approximate support recovery, the same scheme with $m=O(k\epsilon^{-1}\log(n/k)\log n)$ and runtime $D=O(\epsilon^{-1}m)$, improving the runtime significantly with an extra $O(\log n)$ factor in the number of measurements compared to the current optimal (Matsumoto et al., 2023). (2): For the probabilistic exact support recovery in the sublinear regime, a scheme of $m:=O(k\frac{\log k}{\log\log k}\log n)$ measurements and runtime $O(m)$, with vanishing error probability, improving the recent result of Yang et al., 2025.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10777v2</guid>
      <category>cs.IT</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.IT</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaxin Li, Arya Mazumdar</dc:creator>
    </item>
    <item>
      <title>Faster Algorithms for Structured Matrix Multiplication via Flip Graph Search</title>
      <link>https://arxiv.org/abs/2511.10786</link>
      <description>arXiv:2511.10786v1 Announce Type: cross 
Abstract: We give explicit low-rank bilinear non-commutative schemes for multiplying structured $n \times n$ matrices with $2 \leq n \leq 5$, which serve as building blocks for recursive algorithms with improved multiplicative factors in asymptotic complexity. Our schemes are discovered over $\mathbb{F}_2$ or $\mathbb{F}_3$ and lifted to $\mathbb{Z}$ or $\mathbb{Q}$. Using a flip graph search over tensor decompositions, we derive schemes for general, upper-triangular, lower-triangular, symmetric, and skew-symmetric inputs, as well as products of a structured matrix with its transpose. In particular, we obtain $4 \times 4$ rank-34 schemes: (i) multiplying a general matrix by its transpose using 10 recursive calls, improving the factor from 26/41 (0.634) to 8/13 (0.615); and (ii) multiplying an upper-triangular matrix by a general matrix using 12 recursive calls, improving the factor from 8/13 (0.615) to 22/37 (0.595). Additionally, using $\mathbb{F}_3$ flip graphs, we discover schemes over $\mathbb{Q}$ that fundamentally require the inverse of 2, including a $2 \times 2$ symmetric-symmetric multiplication of rank 5 and a $3 \times 3$ skew-symmetric-general multiplication of rank 14 (improving upon AlphaTensor's 15).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10786v1</guid>
      <category>cs.SC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kirill Khoruzhii, Patrick Gel{\ss}, Sebastian Pokutta</dc:creator>
    </item>
    <item>
      <title>TSP integrality gap via 2-edge-connected multisubgraph problem under coincident IP optima</title>
      <link>https://arxiv.org/abs/2511.11215</link>
      <description>arXiv:2511.11215v1 Announce Type: cross 
Abstract: Determining the integrality gap of the linear programming (LP) relaxation of the metric traveling salesman problem (TSP) remains a long-standing open problem. We introduce a transfer principle: when the integer optimum of the 2-edge-connected multisubgraph problem (2ECM) is a unique Hamiltonian cycle $T$, any $\alpha$-approximation algorithm for 2ECM that outputs a Hamiltonian cycle yields an $\alpha$-approximation for TSP. We further develop a cut-margin uniqueness framework that certifies $T$ as the unique integer optimum for both problems and is stable under $\ell_\infty$-bounded perturbations. We show that, if instances exist where the 2ECM has both a unique Hamiltonian cycle integer optimum and a half-integral LP solution, then the TSP integrality gap is at most 4/3 by the algorithm of Boyd et al. (SIAM Journal on Discrete Mathematics 36:1730--1747, 2022). Constructing such instances remains an open problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11215v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toshiaki Yamanaka</dc:creator>
    </item>
    <item>
      <title>Linear-Space Extragradient Methods for Fast, Large-Scale Optimal Transport</title>
      <link>https://arxiv.org/abs/2511.11359</link>
      <description>arXiv:2511.11359v1 Announce Type: cross 
Abstract: Optimal transport (OT) and its entropy-regularized form (EOT) have become increasingly prominent computational problems, with applications in machine learning and statistics. Recent years have seen a commensurate surge in first-order methods aiming to improve the complexity of large-scale (E)OT. However, there has been a consistent tradeoff: attaining state-of-the-art rates requires $\mathcal{O}(n^2)$ storage to enable ergodic primal averaging. In this work, we demonstrate that recently proposed primal-dual extragradient methods (PDXG) can be implemented entirely in the dual with $\mathcal{O}(n)$ storage. Additionally, we prove that regularizing the reformulated OT problem is equivalent to EOT with extensions to entropy-regularized barycenter problems, further widening the applications of the proposed method. The proposed dual-only extragradient method (DXG) is the first algorithm to achieve $\mathcal{O}(n^2\varepsilon^{-1})$ complexity for $\varepsilon$-approximate OT with $\mathcal{O}(n)$ memory. Numerical experiments demonstrate that the dual extragradient method scales favorably in non/weakly-regularized regimes compared to existing algorithms, though future work is needed to improve performance in certain problem classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11359v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew X. Burns, Jiaming Liang</dc:creator>
    </item>
    <item>
      <title>Public Goods Games in Directed Networks with Constraints on Sharing</title>
      <link>https://arxiv.org/abs/2511.11475</link>
      <description>arXiv:2511.11475v1 Announce Type: cross 
Abstract: In a public goods game, every player chooses whether or not to buy a good that all neighboring players will have access to. We consider a setting in which the good is indivisible, neighboring players are out-neighbors in a directed graph, and there is a capacity constraint on their number, k, that can benefit from the good. This means that each player makes a two-pronged decision: decide whether or not to buy and, conditional on buying, choose which k out-neighbors to share access. We examine both pure and mixed Nash equilibria in the model from the perspective of existence, computation, and efficiency. We perform a comprehensive study for these three dimensions with respect to both sharing capacity (k) and the network structure (the underlying directed graph), and establish sharp complexity dichotomies for each.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11475v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>econ.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Argyrios Deligkas, Gregory Gutin, Mark Jones, Philip R. Neary, Anders Yeo</dc:creator>
    </item>
    <item>
      <title>Deviation Dynamics in Cardinal Hedonic Games</title>
      <link>https://arxiv.org/abs/2511.11531</link>
      <description>arXiv:2511.11531v1 Announce Type: cross 
Abstract: Computing stable partitions in hedonic games is a challenging task because there exist games in which stable outcomes do not exist. Even more, these No-instances can often be leveraged to prove computational hardness results. We make this impression rigorous in a dynamic model of cardinal hedonic games by providing meta theorems. These imply hardness of deciding about the possible or necessary convergence of deviation dynamics based on the mere existence of No-instances. Our results hold for additively separable, fractional, and modified fractional hedonic games (ASHGs, FHGs, and MFHGs). Moreover, they encompass essentially all reasonable stability notions based on single-agent deviations. In addition, we propose dynamics as a method to find individually rational and contractually individual stable (CIS) partitions in ASHGs. In particular, we find that CIS dynamics from the singleton partition possibly converge after a linear number of deviations but may require an exponential number of deviations in the worst case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11531v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Valentin Zech, Martin Bullinger</dc:creator>
    </item>
    <item>
      <title>Nonlinear Laplacians: Tunable principal component analysis under directional prior information</title>
      <link>https://arxiv.org/abs/2505.12528</link>
      <description>arXiv:2505.12528v2 Announce Type: replace-cross 
Abstract: We introduce a new family of algorithms for detecting and estimating a rank-one signal from a noisy observation under prior information about that signal's direction, focusing on examples where the signal is known to have entries biased to be positive. Given a matrix observation $\mathbf{Y}$, our algorithms construct a nonlinear Laplacian, another matrix of the form $\mathbf{Y}+\mathrm{diag}(\sigma(\mathbf{Y1}))$ for a nonlinear $\sigma:\mathbb{R}\to\mathbb{R}$, and examine the top eigenvalue and eigenvector of this matrix. When $\mathbf{Y}$ is the (suitably normalized) adjacency matrix of a graph, our approach gives a class of algorithms that search for unusually dense subgraphs by computing a spectrum of the graph "deformed" by the degree profile $\mathbf{Y1}$. We study the performance of such algorithms compared to direct spectral algorithms (the case $\sigma=0$) on models of sparse principal component analysis with biased signals, including the Gaussian planted submatrix problem. For such models, we rigorously characterize the strength of rank-one signal, as a function of $\sigma$, required for an outlier eigenvalue to appear in the spectrum of a nonlinear Laplacian matrix. While identifying the $\sigma$ that minimizes the required signal strength in closed form seems intractable, we explore three approaches to design $\sigma$ numerically: exhaustively searching over simple classes of $\sigma$, learning $\sigma$ from datasets of problem instances, and tuning $\sigma$ using black-box optimization of the critical signal strength. We find both theoretically and empirically that, if $\sigma$ is chosen appropriately, then nonlinear Laplacian spectral algorithms substantially outperform direct spectral algorithms, while retaining the conceptual simplicity of spectral methods compared to broader classes of computations like approximate message passing or general first order methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12528v2</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxin Ma, Dmitriy Kunisky</dc:creator>
    </item>
    <item>
      <title>$\mathrm{TIME}[t]\subseteq \mathrm{SPACE}[O(\sqrt{t})]$ via Tree Height Compression</title>
      <link>https://arxiv.org/abs/2508.14831</link>
      <description>arXiv:2508.14831v3 Announce Type: replace-cross 
Abstract: We prove a square-root space simulation for deterministic multitape Turing machines, showing $\mathrm{TIME}[t]\subseteq \mathrm{SPACE}[O(\sqrt{t})]$ \emph{measured in tape cells over a fixed finite alphabet}. The key step is a Height Compression Theorem that uniformly (and in logspace) reshapes the canonical left-deep succinct computation tree for a block-respecting run into a binary tree whose evaluation-stack depth along any DFS path is $O(\log T)$ for $T=\lceil t/b\rceil$, while preserving $O(b)$ workspace at leaves and $O(1)$ at internal nodes. Edges have \emph{addressing/topology} checkable in $O(\log t)$ space, and \emph{semantic} correctness across merges is witnessed by an exact $O(b)$ bounded-window replay at the unique interface. Algorithmically, an Algebraic Replay Engine with constant-degree maps over a constant-size field, together with pointerless DFS, index-free streaming, and a \emph{rolling boundary buffer that prevents accumulation of leaf summaries}, ensures constant-size per-level tokens and eliminates wide counters, yielding the additive tradeoff $S(b)=O(b+t/b)$. Choosing $b=\Theta(\sqrt{t})$ gives $O(\sqrt{t})$ space with no residual multiplicative polylog factors. The construction is uniform, relativizes, and is robust to standard model choices. Consequences include branching-program upper bounds $2^{O(\sqrt{s})}$ for size-$s$ bounded-fan-in circuits, tightened quadratic-time lower bounds for $\mathrm{SPACE}[n]$-complete problems via the standard hierarchy argument, and $O(\sqrt{t})$-space certifying interpreters; under explicit locality assumptions, the framework extends to geometric $d$-dimensional models. Conceptually, the work isolates path bookkeeping as the chief obstruction to $O(\sqrt{t})$ and removes it via structural height compression with per-path analysis rather than barrier-prone techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14831v3</guid>
      <category>cs.CC</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Logan Nye</dc:creator>
    </item>
    <item>
      <title>The Contiguous Art Gallery Problem is in {\Theta}(n log n)</title>
      <link>https://arxiv.org/abs/2511.02960</link>
      <description>arXiv:2511.02960v2 Announce Type: replace-cross 
Abstract: Recently, a natural variant of the Art Gallery problem, known as the \emph{Contiguous Art Gallery problem} was proposed. Given a simple polygon $P$, the goal is to partition its boundary $\partial P$ into the smallest number of contiguous segments such that each segment is completely visible from some point in $P$. Unlike the classical Art Gallery problem, which is NP-hard, this variant is polynomial-time solvable. At SoCG~2025, three independent works presented algorithms for this problem, each achieving a running time of $O(k n^5 \log n)$ (or $O(n^6\log n)$), where $k$ is the size of an optimal solution. Interestingly, these results were obtained using entirely different approaches, yet all led to roughly the same asymptotic complexity, suggesting that such a running time might be inherent to the problem.
  We show that this is not the case. In the real RAM-model, the prevalent model in computational geometry, we present an $O(n \log n)$-time algorithm, achieving an $O(k n^4)$ factor speed-up over the previous state-of-the-art. We also give a straightforward sorting-based lower bound by reducing from the set intersection problem. We thus show that the Contiguous Art Gallery problem is in $\Theta(n \log n)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02960v2</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarita de Berg, Jacobus Conradi, Ivor van der Hoog, Eva Rotenberg</dc:creator>
    </item>
  </channel>
</rss>
