<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Nov 2025 05:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>National and state-level datasets of United States forensic DNA databases 2001--2025</title>
      <link>https://arxiv.org/abs/2511.11953</link>
      <description>arXiv:2511.11953v1 Announce Type: new 
Abstract: Forensic DNA databases in the United States have expanded substantially over the past two decades. However, comprehensive, harmonized data describing database structure and composition remain limited. This dataset series documents forensic DNA infrastructure across national and state levels from 2001 to 2025. It includes a reconstructed time series of monthly National DNA Index System (NDIS) statistics from FBI archives, capturing counts of offender, arrestee, and forensic profiles, participating laboratory totals, and investigations aided. A complementary dataset compiles publicly available state-level statistics and policy metadata on arrestee collection laws, familial search practices, and DNA collection statutes across all 50 states. A third dataset provides standardized demographic and annual collection data obtained through previously published public records requests, including racial and gender composition where reported. Together, these resources provide a foundation for studying the historical development of forensic DNA systems in the U.S., enabling longitudinal and cross-sectional analyses of database growth, policy variation, and reporting practices across jurisdictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11953v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yemko Pryor, Joao Pedro Donadio, Samantha C. Muller, Jenna Wilson, Tina Lasisi</dc:creator>
    </item>
    <item>
      <title>Moving Pictures of Thought: Extracting Visual Knowledge in Charles S. Peirce's Manuscripts with Vision-Language Models</title>
      <link>https://arxiv.org/abs/2511.13378</link>
      <description>arXiv:2511.13378v1 Announce Type: new 
Abstract: Diagrams are crucial yet underexplored tools in many disciplines, demonstrating the close connection between visual representation and scholarly reasoning. However, their iconic form poses obstacles to visual studies, intermedial analysis, and text-based digital workflows. In particular, Charles S. Peirce consistently advocated the use of diagrams as essential for reasoning and explanation. His manuscripts, often combining textual content with complex visual artifacts, provide a challenging case for studying documents involving heterogeneous materials. In this preliminary study, we investigate whether Visual Language Models (VLMs) can effectively help us identify and interpret such hybrid pages in context. First, we propose a workflow that (i) segments manuscript page layouts, (ii) reconnects each segment to IIIF-compliant annotations, and (iii) submits fragments containing diagrams to a VLM. In addition, by adopting Peirce's semiotic framework, we designed prompts to extract key knowledge about diagrams and produce concise captions. Finally, we integrated these captions into knowledge graphs, enabling structured representations of diagrammatic content within composite sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13378v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MM</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlo Teo Pedretti, Davide Picca, Dario Rodighiero</dc:creator>
    </item>
    <item>
      <title>Towards autonomous quantum physics research using LLM agents with access to intelligent tools</title>
      <link>https://arxiv.org/abs/2511.11752</link>
      <description>arXiv:2511.11752v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) is used in numerous fields of science, yet the initial research questions and targets are still almost always provided by human researchers. AI-generated creative ideas in science are rare and often vague, so that it remains a human task to execute them. Automating idea generation and implementation in one coherent system would significantly shift the role of humans in the scientific process. Here we present AI-Mandel, an LLM agent that can generate and implement ideas in quantum physics. AI-Mandel formulates ideas from the literature and uses a domain-specific AI tool to turn them into concrete experiment designs that can readily be implemented in laboratories. The generated ideas by AI-Mandel are often scientifically interesting - for two of them we have already written independent scientific follow-up papers. The ideas include new variations of quantum teleportation, primitives of quantum networks in indefinite causal orders, and new concepts of geometric phases based on closed loops of quantum information transfer. AI-Mandel is a prototypical demonstration of an AI physicist that can generate and implement concrete, actionable ideas. Building such a system is not only useful to accelerate science, but it also reveals concrete open challenges on the path to human-level artificial scientists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11752v1</guid>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <category>quant-ph</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S\"oren Arlt, Xuemei Gu, Mario Krenn</dc:creator>
    </item>
    <item>
      <title>DataOps-driven CI/CD for analytics repositories</title>
      <link>https://arxiv.org/abs/2511.12277</link>
      <description>arXiv:2511.12277v1 Announce Type: cross 
Abstract: The proliferation of SQL for data processing has often occurred without the rigor of traditional software development, leading to siloed efforts, logic replication, and increased risk. This ad-hoc approach hampers data governance and makes validation nearly impossible. Organizations are adopting DataOps, a methodology combining Agile, Lean, and DevOps principles to address these challenges to treat analytics pipelines as production systems. However, a standardized framework for implementing DataOps is lacking. This perspective proposes a qualitative design for a DataOps-aligned validation framework. It introduces a DataOps Controls Scorecard, derived from a multivocal literature review, which distills key concepts into twelve testable controls. These controls are then mapped to a modular, extensible CI/CD pipeline framework designed to govern a single source of truth (SOT) SQL repository. The framework consists of five stages: Lint, Optimize, Parse, Validate, and Observe, each containing specific, automated checks. A Requirements Traceability Matrix (RTM) demonstrates how each high-level control is enforced by concrete pipeline checks, ensuring qualitative completeness. This approach provides a structured mechanism for enhancing data quality, governance, and collaboration, allowing teams to scale analytics development with transparency and control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12277v1</guid>
      <category>eess.SY</category>
      <category>cs.DL</category>
      <category>cs.SE</category>
      <category>cs.SY</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dmytro Valiaiev</dc:creator>
    </item>
    <item>
      <title>Academ-AI: documenting the undisclosed use of generative artificial intelligence in academic publishing</title>
      <link>https://arxiv.org/abs/2411.15218</link>
      <description>arXiv:2411.15218v2 Announce Type: replace 
Abstract: Since generative artificial intelligence (AI) tools such as OpenAI's ChatGPT became widely available, researchers have used them in the writing process. The consensus of the academic publishing community is that such usage must be declared in the published article. Academ-AI documents examples of suspected undeclared AI usage in the academic literature, discernible primarily due to the appearance in research papers of idiosyncratic verbiage characteristic of large language model (LLM)-based chatbots. This analysis of the first 768 examples collected reveals that the problem is widespread, penetrating the journals, conference proceedings, and textbooks of highly respected publishers. Undeclared AI seems to appear in journals with higher citation metrics and higher article processing charges (APCs), precisely those outlets that should theoretically have the resources and expertise to avoid such oversights. An extremely small minority of cases are corrected post publication, and the corrections are often insufficient to rectify the problem. The 768 examples analyzed here likely represent a small fraction of the undeclared AI present in the academic literature, much of which may be undetectable. Publishers must enforce their policies against undeclared AI usage in cases that are detectable; this is the best defense currently available to the academic publishing community against the proliferation of undisclosed AI. This is an updated version of a previous preprint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15218v2</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Glynn</dc:creator>
    </item>
    <item>
      <title>Can Recombination Displace Dominant Scientific Ideas</title>
      <link>https://arxiv.org/abs/2506.15959</link>
      <description>arXiv:2506.15959v4 Announce Type: replace 
Abstract: The combination of diverse, pre-existing knowledge is a common explanation for scientific breakthroughs. However, a paradox exists: while scientific output and the potential for such recombination have grown exponentially, the rate of breakthrough discoveries has not. To explore this paradox, our study examines 41 million scientific articles from 1965 to 2024. We measure two key properties for each paper: atypicality, which quantifies the combination of knowledge from conceptually distant areas, and disruption. We demonstrate that these metrics capture distinct processes. Atypicality is characteristic of work that extends established concepts into new topical areas (a form of cross-topic recombination). Disruption, in contrast, signifies the replacement of a dominant idea within its own topic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15959v4</guid>
      <category>cs.DL</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linzhuo Li, Yiling Lin, Lingfei Wu</dc:creator>
    </item>
    <item>
      <title>GRIN Transfer: A production-ready tool for libraries to retrieve digital copies from Google Books</title>
      <link>https://arxiv.org/abs/2511.11447</link>
      <description>arXiv:2511.11447v2 Announce Type: replace 
Abstract: Publicly launched in 2004, the Google Books project has scanned tens of millions of items in partnership with libraries around the world. As part of this project, Google created the Google Return Interface (GRIN). Through this platform, libraries can access their scanned collections, the associated metadata, and the ongoing OCR and metadata improvements that become available as Google reprocesses these collections using new technologies. When downloading the Harvard Library Google Books collection from GRIN to develop the Institutional Books dataset, we encountered several challenges related to rate-limiting and atomized metadata within the GRIN platform. To overcome these challenges and help other libraries make more robust use of their Google Books collections, this technical report introduces the initial release of GRIN Transfer. This open-source and production-ready Python pipeline allows partner libraries to efficiently retrieve their Google Books collections from GRIN. This report also introduces an updated version of our Institutional Books 1.0 pipeline, initially used to analyze, augment, and assemble the Institutional Books 1.0 dataset. We have revised this pipeline for compatibility with the output format of GRIN Transfer. A library could pair these two tools to create an end-to-end processing pipeline for their Google Books collection to retrieve, structure, and enhance data available from GRIN. This report gives an overview of how GRIN Transfer was designed to optimize for reliability and usability in different environments, as well as guidance on configuration for various use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11447v2</guid>
      <category>cs.DL</category>
      <category>cs.IR</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liza Daly, Matteo Cargnelutti, Catherine Brobston, John Hess, Greg Leppert, Amanda Watson, Jonathan Zittrain</dc:creator>
    </item>
    <item>
      <title>Novelty and Impact of Economics Papers</title>
      <link>https://arxiv.org/abs/2511.01211</link>
      <description>arXiv:2511.01211v3 Announce Type: replace-cross 
Abstract: We propose a framework that recasts scientific novelty not as a single attribute of a paper, but as a reflection of its position within the evolving intellectual landscape. We decompose this position into two orthogonal dimensions: \textit{spatial novelty}, which measures a paper's intellectual distinctiveness from its neighbors, and \textit{temporal novelty}, which captures its engagement with a dynamic research frontier. To operationalize these concepts, we leverage Large Language Models to develop semantic isolation metrics that quantify a paper's location relative to the full-text literature. Applying this framework to a large corpus of economics articles, we uncover a fundamental trade-off: these two dimensions predict systematically different outcomes. Temporal novelty primarily predicts citation counts, whereas spatial novelty predicts disruptive impact. This distinction allows us to construct a typology of semantic neighborhoods, identifying four archetypes associated with distinct and predictable impact profiles. Our findings demonstrate that novelty can be understood as a multidimensional construct whose different forms, reflecting a paper's strategic location, have measurable and fundamentally distinct consequences for scientific progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01211v3</guid>
      <category>econ.GN</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chaofeng Wu</dc:creator>
    </item>
  </channel>
</rss>
