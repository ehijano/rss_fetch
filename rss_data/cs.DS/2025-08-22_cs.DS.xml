<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Aug 2025 04:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Time-Optimal Directed q-Analysis</title>
      <link>https://arxiv.org/abs/2508.15583</link>
      <description>arXiv:2508.15583v1 Announce Type: new 
Abstract: Directed q-analysis is a recent extension of q-analysis, an established method for extracting structure from networks, to directed graphs. Until recently, a lack of efficient algorithms heavily restricted the application of this technique: Previous approaches scale with the square of the input size, which is also the maximal size of the output, rendering such approaches worst-case optimal. In practice, output sizes of relevant networks are usually far from the worst case, a fact that could be exploited by an (efficient) output-sensitive algorithm. We develop such an algorithm and formally describe it in detail. The key insight, obtained by carefully studying various approaches to directed q-analysis and how they relate to each other, is that inverting the order of computation leads to significant complexity gains. Targeted precomputation and caching tactics further reduce the introduced overhead, enough to achieve (under mild assumptions) a time complexity that is linear in output size. The resulting algorithm for performing directed q-analysis is shown to be time-optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15583v1</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Felix Windisch, Florian Unger</dc:creator>
    </item>
    <item>
      <title>A Scalable Trie Building Algorithm for High-Throughput Phyloanalysis of Wafer-Scale Digital Evolution Experiments</title>
      <link>https://arxiv.org/abs/2508.15074</link>
      <description>arXiv:2508.15074v1 Announce Type: cross 
Abstract: Agent-based simulation platforms play a key role in enabling fast-to-run evolution experiments that can be precisely controlled and observed in detail. Availability of high-resolution snapshots of lineage ancestries from digital experiments, in particular, is key to investigations of evolvability and open-ended evolution, as well as in providing a validation testbed for bioinformatics method development. Ongoing advances in AI/ML hardware accelerator devices, such as the 850,000-processor Cerebras Wafer-Scale Engine (WSE), are poised to broaden the scope of evolutionary questions that can be investigated in silico. However, constraints in memory capacity and locality characteristic of these systems introduce difficulties in exhaustively tracking phylogenies at runtime. To overcome these challenges, recent work on hereditary stratigraphy algorithms has developed space-efficient genetic markers to facilitate fully decentralized estimation of relatedness among digital organisms. However, in existing work, compute time to reconstruct phylogenies from these genetic markers has proven a limiting factor in achieving large-scale phyloanalyses. Here, we detail an improved trie-building algorithm designed to produce reconstructions equivalent to existing approaches. For modestly-sized 10,000-tip trees, the proposed approach achieves a 300-fold speedup versus existing state-of-the-art. Finally, using 1 billion genome datasets drawn from WSE simulations encompassing 954 trillion replication events, we report a pair of large-scale phylogeny reconstruction trials, achieving end-to-end reconstruction times of 2.6 and 2.9 hours. In substantially improving reconstruction scaling and throughput, presented work establishes a key foundation to enable powerful high-throughput phyloanalysis techniques in large-scale digital evolution experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15074v1</guid>
      <category>cs.NE</category>
      <category>cs.DS</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vivaan Singhvi, Joey Wagner, Emily Dolson, Luis Zaman, Matthew Andres Moreno</dc:creator>
    </item>
    <item>
      <title>Private Hyperparameter Tuning with Ex-Post Guarantee</title>
      <link>https://arxiv.org/abs/2508.15183</link>
      <description>arXiv:2508.15183v1 Announce Type: cross 
Abstract: The conventional approach in differential privacy (DP) literature formulates the privacy-utility trade-off with a "privacy-first" perspective: for a predetermined level of privacy, a certain utility is achievable. However, practitioners often operate under a "utility-first" paradigm, prioritizing a desired level of utility and then determining the corresponding privacy cost.
  Wu et al. [2019] initiated a formal study of this "utility-first" perspective by introducing ex-post DP. They demonstrated that by adding correlated Laplace noise and progressively reducing it on demand, a sequence of increasingly accurate estimates of a private parameter can be generated, with the privacy cost attributed only to the least noisy iterate released. This led to a Laplace mechanism variant that achieves a specified utility with minimal privacy loss. However, their work, and similar findings by Whitehouse et al. [2022], are primarily limited to simple mechanisms based on Laplace or Gaussian noise.
  In this paper, we significantly generalize these results. In particular, we extend the work of Wu et al. [2019] and Liu and Talwar [2019] to support any sequence of private estimators, incurring at most a doubling of the original privacy budget. Furthermore, we demonstrate that hyperparameter tuning for these estimators, including the selection of an optimal privacy budget, can be performed without additional privacy cost. Finally, we extend our results to ex-post Renyi DP, further broadening the applicability of utility-first privacy mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15183v1</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Badih Ghazi, Pritish Kamath, Alexander Knop, Ravi Kumar, Pasin Manurangsi, Chiyuan Zhang</dc:creator>
    </item>
    <item>
      <title>Fermion-to-Fermion Low-Density Parity-Check Codes</title>
      <link>https://arxiv.org/abs/2508.15323</link>
      <description>arXiv:2508.15323v1 Announce Type: cross 
Abstract: Simulating fermionic systems on qubit-based quantum computers often demands significant computational resources due to the requirement to map fermions to qubits. Thus, designing a fault-tolerant quantum computer that operates directly with fermions offers an effective solution to this challenge. Here, we introduce a protocol for fault-tolerant fermionic quantum computation utilizing fermion-to-fermion low-density parity-check codes. Our method employs a fermionic low-density parity-check code memory, which transfers its state to fermionic color code processors assisted by lattice surgery, where logical operations are subsequently performed. To construct the fermionic low-density parity-check memory, we develop a systematic approach for creating fermionic stabilizer codes based on self-dual Calderbank-Shor-Steane codes. We present examples demonstrating these codes' significantly improved coding rate compared to the fermionic color code. Finally, we simulate the dynamics of a fermionic system using our protocol, illustrating effective error suppression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15323v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chong-Yuan Xu, Ze-Chuan Liu, Yong Xu</dc:creator>
    </item>
    <item>
      <title>Almost and Approximate EFX for Few Types of Agents</title>
      <link>https://arxiv.org/abs/2508.15380</link>
      <description>arXiv:2508.15380v1 Announce Type: cross 
Abstract: We study the problem of fair allocation of a set of indivisible goods among $n$ agents with $k$ distinct additive valuations, with the goal of achieving approximate envy-freeness up to any good ($\alpha-\mathrm{EFX}$).
  It is known that EFX allocations exist for $n$ agents when there are at most three distinct valuations due to HV et al. Furthermore, Amanatidis et al. showed that a $\frac{2}{3}-\mathrm{EFX}$ allocation is guaranteed to exist when number of agents is at most seven. In this paper, we show that a $\frac{2}{3}-\mathrm{EFX}$ allocation exists for any number of agents when there are at most four distinct valuations.
  Secondly, we consider a relaxation called $\mathrm{EFX}$ with charity, where some goods remain unallocated such that no agent envies the set of unallocated goods. Akrami et al. showed that for $n$ agents and any $\varepsilon \in \left(0, \frac{1}{2}\right]$, there exists a $(1-\varepsilon)-\mathrm{EFX}$ allocation with at most $\tilde{\mathcal{O}}((n/\varepsilon)^{\frac{1}{2}})$ goods to charity. In this paper, we show that a $(1-\varepsilon)-\mathrm{EFX}$ allocation with a $\tilde{\mathcal{O}}(k/\varepsilon)^{\frac{1}{2}}$ charity exists for any number of agents when there are at most $k$ distinct valuations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15380v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Vishwa Prakash HV, Ruta Mehta, Prajakta Nimbhorkar</dc:creator>
    </item>
    <item>
      <title>On the Effectiveness of Graph Reordering for Accelerating Approximate Nearest Neighbor Search on GPU</title>
      <link>https://arxiv.org/abs/2508.15436</link>
      <description>arXiv:2508.15436v1 Announce Type: cross 
Abstract: We present the first systematic investigation of graph reordering effects for graph-based Approximate Nearest Neighbor Search (ANNS) on a GPU. While graph-based ANNS has become the dominant paradigm for modern AI applications, recent approaches focus on algorithmic innovations while neglecting memory layout considerations that significantly affect execution time. Our unified evaluation framework enables comprehensive evaluation of diverse reordering strategies across different graph indices through a graph adapter that converts arbitrary graph topologies into a common representation and a GPU-optimized graph traversal engine. We conduct a comprehensive analysis across diverse datasets and state-of-the-art graph indices, introducing analysis metrics that quantify the relationship between structural properties and memory layout effectiveness. Our GPU-targeted reordering achieves up to 15$\%$ QPS improvements while preserving search accuracy, demonstrating that memory layout optimization operates orthogonally to existing algorithmic innovations. We will release all code upon publication to facilitate reproducibility and foster further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15436v1</guid>
      <category>cs.IR</category>
      <category>cs.CV</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yutaro Oguri, Mai Nishimura, Yusuke Matsui</dc:creator>
    </item>
    <item>
      <title>An Objective Improvement Approach to Solving Discounted Payoff Games</title>
      <link>https://arxiv.org/abs/2404.04124</link>
      <description>arXiv:2404.04124v4 Announce Type: replace 
Abstract: While discounted payoff games and classic games that reduce to them, like parity and mean-payoff games, are symmetric, their solutions are not. We have taken a fresh view on the properties that optimal solutions need to have, and devised a novel way to converge to them, which is entirely symmetric. We achieve this by building a constraint system that uses every edge to define an inequation, and update the objective function by taking a single outgoing edge for each vertex into account. These edges loosely represent strategies of both players, where the objective function intuitively asks to make the inequation to these edges sharp. In fact, where they are not sharp, there is an `error' represented by the difference between the two sides of the inequation, which is 0 where the inequation is sharp. Hence, the objective is to minimise the sum of these errors. For co-optimal strategies, and only for them, it can be achieved that all selected inequations are sharp or, equivalently, that the sum of these errors is zero. While no co-optimal strategies have been found, we step-wise improve the error by improving the solution for a given objective function or by improving the objective function for a given solution. This also challenges the gospel that methods for solving payoff games are either based on strategy improvement or on value iteration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04124v4</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Dell'Erba, Arthur Dumas, Sven Schewe</dc:creator>
    </item>
    <item>
      <title>You (Almost) Can't Beat Brute Force for 3-Matroid Intersection</title>
      <link>https://arxiv.org/abs/2412.02217</link>
      <description>arXiv:2412.02217v2 Announce Type: replace 
Abstract: The $\ell$-matroid intersection ($\ell$-MI) problem asks if $\ell$ given matroids share a common basis. Already for $\ell = 3$, notable canonical NP-complete special cases are $3$-Dimensional Matching and Hamiltonian Path on directed graphs. However, while these problems admit exponential-time algorithms that improve the simple brute force, the fastest known algorithm for $3$-MI is exactly brute force with runtime $2^{n}/poly(n)$, where $n$ is the number of elements. Our first result shows that in fact, brute force cannot be significantly improved, by ruling out an algorithm for $\ell$-MI with runtime $o\left(2^{n-5 \cdot n^{\frac{1}{\ell-1}} \cdot \log (n)}\right)$, for any fixed $\ell\geq 3$. We further obtain: (i) an algorithm that solves $\ell$-MI faster than brute force in time $2^{n-\Omega\left(\log^2 (n)\right)} $ (ii) a parameterized running time lower bound of $2^{(\ell-2) \cdot k \cdot \log k} \cdot poly(n)$ for $\ell$-MI, where the parameter $k$ is the rank of the matroids. We obtain these two results by generalizing the Monotone Local Search technique of Fomin et al. (J. ACM'19). Broadly speaking, our generalization converts any parameterized algorithm for a subset problem into an exponential-time algorithm which is faster than brute-force.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02217v2</guid>
      <category>cs.DS</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilan Doron-Arad, Ariel Kulik, Hadas Shachnai</dc:creator>
    </item>
    <item>
      <title>Sublinear-Time Approximation for Graph Frequency Vectors in Hyperfinite Graphs</title>
      <link>https://arxiv.org/abs/2508.14324</link>
      <description>arXiv:2508.14324v2 Announce Type: replace 
Abstract: In this work, we address the problem of approximating the $k$-disc distribution ("frequency vector") of a bounded-degree graph in sublinear-time under the assumption of hyperfiniteness. We revisit the partition-oracle framework of Hassidim, Kelner, Nguyen, and Onak [HKNO09], and provide a concise, self-contained analysis that explicitly separates the two sources of error: (i) the cut error, controlled by hyperfiniteness parameter $\phi$, which incurs at most $\varepsilon/2$ in $\ell_1$-distance by removing at most $\phi |V|$ edges; and (ii) the sampling error, controlled by the accuracy parameter $\varepsilon$, bounded by $\varepsilon/2$ via $N=\Theta(\varepsilon^{-2})$ random vertex queries and a Chernoff and union bound argument. Combining these yields an overall $\ell_1$-error of $\varepsilon$ with high probability. Algorithmically, we show that by sampling $N=\lceil C\varepsilon^{-2} \rceil$ vertices and querying the local partition oracle, one can in time $poly(d,k,\varepsilon^{-1})$ construct a summary graph $H$ of size $|H|=poly(d^k,1/\varepsilon)$ whose $k$-disc frequency vector approximates that of the original graph within $\varepsilon$ in $\ell_1$-distance. Our approach clarifies the dependence of both runtime and summary-size on the parameter $d$,$k$, and $\varepsilon$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14324v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregory Moroie</dc:creator>
    </item>
    <item>
      <title>On the Constant-Depth Circuit Complexity of Generating Quasigroups</title>
      <link>https://arxiv.org/abs/2402.00133</link>
      <description>arXiv:2402.00133v5 Announce Type: replace-cross 
Abstract: We investigate the constant-depth circuit complexity of the Isomorphism Problem, Minimum Generating Set Problem (MGS), and Sub(quasi)group Membership Problem (Membership) for groups and quasigroups (=Latin squares), given as input in terms of their multiplication (Cayley) tables. Despite decades of research on these problems, lower bounds for these problems even against depth-$2$ AC circuits remain unknown. Perhaps surprisingly, Chattopadhyay, Tor\'an, and Wagner (FSTTCS 2010; ACM Trans. Comput. Theory, 2013) showed that Quasigroup Isomorphism could be solved by AC circuits of depth $O(\log \log n)$ using $O(\log^2 n)$ nondeterministic bits, a class we denote $\exists^{\log^2(n)}FOLL$. We narrow this gap by improving the upper bound for many of these problems to $quasiAC^0$, thus decreasing the depth to constant.
  In particular, we show:
  - MGS for quasigroups is in $\exists^{\log^2(n)}\forall^{\log n}NTIME(\mathrm{polylog}(n))\subseteq quasiAC^0$. Papadimitriou and Yannakakis (J. Comput. Syst. Sci., 1996) conjectured that this problem was $\exists^{\log^2(n)}P$-complete; our results refute a version of that conjecture for completeness under $quasiAC^0$ reductions unconditionally, and under polylog-space reductions assuming EXP $\neq$ PSPACE.
  - MGS for groups is in $AC^{1}(L)$, improving on the previous upper bound of $P$ (Lucchini &amp; Thakkar, J. Algebra, 2024).
  - Quasigroup Isomorphism belongs to $\exists^{\log^2(n)}AC^0(DTISP(\mathrm{polylog},\log)\subseteq quasiAC^0$, improving on the previous bound of $\exists^{\log^2(n)}L\cap\exists^{\log^2(n)}FOLL\subseteq quasiFOLL$ (Chattopadhyay, Tor\'an, &amp; Wagner, ibid.; Levet, Australas. J. Combin., 2023).
  Our results suggest that understanding the constant-depth circuit complexity may be key to resolving the complexity of problems concerning (quasi)groups in the multiplication table model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00133v5</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <category>math.GR</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.46298/theoretics.25.19</arxiv:DOI>
      <arxiv:journal_reference>TheoretiCS, Volume 4 (2025), Article 19, 1-39</arxiv:journal_reference>
      <dc:creator>Nathaniel A. Collins, Joshua A. Grochow, Michael Levet, Armin Wei{\ss}</dc:creator>
    </item>
    <item>
      <title>Algorithmic Improvements to List Decoding of Folded Reed-Solomon Codes</title>
      <link>https://arxiv.org/abs/2508.12548</link>
      <description>arXiv:2508.12548v2 Announce Type: replace-cross 
Abstract: Folded Reed-Solomon (FRS) codes are a well-studied family of codes, known for achieving list decoding capacity. In this work, we give improved deterministic and randomized algorithms for list decoding FRS codes of rate $R$ up to radius $1-R-\varepsilon$.
  We present a deterministic decoder that runs in near-linear time $\widetilde{O}_{\varepsilon}(n)$, improving upon the best-known runtime $n^{\Omega(1/\varepsilon)}$ for decoding FRS codes. Prior to our work, no capacity achieving code was known whose deterministic decoding could be done in time $\widetilde{O}_{\varepsilon}(n)$.
  We also present a randomized decoder that runs in fully polynomial time $\mathrm{poly}(1/\varepsilon) \cdot \widetilde{O}(n)$, improving the best-known runtime $\mathrm{exp}(1/\varepsilon)\cdot \widetilde{O}(n)$ for decoding FRS codes. Again, prior to our work, no capacity achieving code was known whose decoding time depended polynomially on $1/\varepsilon$.
  Our results are based on improved pruning procedures for finding the list of codewords inside a constant-dimensional affine subspace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12548v2</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>math.IT</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vikrant Ashvinkumar, Mursalin Habib, Shashank Srivastava</dc:creator>
    </item>
  </channel>
</rss>
