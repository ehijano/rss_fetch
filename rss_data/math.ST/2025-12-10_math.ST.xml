<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Dec 2025 05:02:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Mixed Exponential Statistical Structures and Their Approximation Operators</title>
      <link>https://arxiv.org/abs/2512.07870</link>
      <description>arXiv:2512.07870v1 Announce Type: new 
Abstract: The paper examines the construction and analysis of a new class of mixed exponential statistical structures that combine the properties of stochastic models and linear positive operators.The relevance of the topic is driven by the growing need to develop a unified theoretical framework capable of describing both continuous and discrete random structures that possess approximation properties. The aim of the study is to introduce and analyze a generalized family of mixed exponential statistical structures and their corresponding linear positive operators, which include known operators as particular cases. We define auxiliary statistical structures B and H through differential relations between their elements, and construct the main Phillips-type structure. Recurrent relations for the central moments are obtained, their properties are established, and the convergence and approximation accuracy of the constructed operators are investigated. The proposed approach allows mixed exponential structures to be viewed as a generalization of known statistical systems, providing a unified analytical and stochastic description. The results demonstrate that mixed exponential statistical structures can be used to develop new classes of positive operators with controllable preservation and approximation properties. The proposed methodology forms a basis for further research in constructing multidimensional statistical structures, analyzing operators in weighted spaces, and studying their asymptotic characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07870v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yurii Volkov, Oleksandr Volkov</dc:creator>
    </item>
    <item>
      <title>The limit joint distributions of some statistics used in testing the quality of random number generators</title>
      <link>https://arxiv.org/abs/2512.08002</link>
      <description>arXiv:2512.08002v1 Announce Type: new 
Abstract: The limit joint distribution of statistics that are generalizations of some statistics from the NIST STS, TestU01, and other packages is found under the following hypotheses $H_0$ and $H_1$. Hypothesis $H_0$ states that the tested sequence is a sequence of independent random vectors with a known distribution, and the simple alternative hypothesis $H_1$ converges in some sense to $H_0$ with increasing sample size. In addition, an analogue of the Berry-Esseen inequality is obtained for the statistics under consideration, and conditions for their asymptotic independence are found.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08002v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. P. Savelov</dc:creator>
    </item>
    <item>
      <title>A multivariate generalization of Hall's theorem for Edgeworth expansions of bootstrap distributions</title>
      <link>https://arxiv.org/abs/2512.08200</link>
      <description>arXiv:2512.08200v1 Announce Type: new 
Abstract: Theorem 5.1 in the monograph by Hall (1992) provides rigorous in-probability justification of Edgeworth expansions of bootstrap distributions. Proving this result was rather challenging because bootstrap distributions do not satisfy the classical Cram\'er condition and therefore classical methods for justifying Edgeworth expansions, e.g. Bhattacharya and Rao (1976) and Bhattacharya and Ghosh (1978), are not available. Hall's (1992) theorem is for a univariate statistic which can be expressed as a smooth function of means, though the underlying population can be multivariate. However, there are a number of applications where a multivariate version of Hall's theorem is needed, and generalizing the proof from the univariate case to the multivariate case is not immediate. Our primary purpose in this article is to fill this gap by stating a multivariate version of the theorem and sketching the modifications to the proof of Hall's (1992) Theorem 5.1 that are needed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08200v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrew T. A. Wood</dc:creator>
    </item>
    <item>
      <title>Causal inference under interference: computational barriers and algorithmic solutions</title>
      <link>https://arxiv.org/abs/2512.08252</link>
      <description>arXiv:2512.08252v1 Announce Type: new 
Abstract: We study causal effect estimation under interference from network data. We work under the chain-graph formulation pioneered in Tchetgen Tchetgen et. al (2021). Our first result shows that polynomial time evaluation of treatment effects is computationally hard in this framework without additional assumptions on the underlying chain graph. Subsequently, we assume that the interactions among the study units are governed either by (i) a dense graph or (ii) an i.i.d. Gaussian matrix. In each case, we show that the treatment effects have well-defined limits as the population size diverges to infinity. Additionally, we develop polynomial time algorithms to consistently evaluate the treatment effects in each case. Finally, we estimate the unknown parameters from the observed data using maximum pseudo-likelihood estimates, and establish the stability of our causal effect estimators under this perturbation. Our algorithms provably approximate the causal effects in polynomial time even in low-temperature regimes where the canonical MCMC samplers are slow mixing. For dense graphs, our results use the notion of regularity partitions; for Gaussian interactions, our approach uses ideas from spin glass theory and Approximate Message Passing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08252v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sohom Bhattacharya, Subhabrata Sen</dc:creator>
    </item>
    <item>
      <title>Point and interval estimators of a changepoint in stochastical dominance between two distributions</title>
      <link>https://arxiv.org/abs/2512.08823</link>
      <description>arXiv:2512.08823v1 Announce Type: new 
Abstract: For differences between means of continuous data from independent groups, the customary scale-free measure of effect is the standardized mean difference (SMD). To justify use of SMD, one should be reasonably confident that the group-level variances are equal. Empirical evidence often contradicts this assumption. Thus, we have investigated an alternate approach, based on stochastic ordering of the treatment and control distributions, that takes into account means and variances. For applying stochastic ordering, our development yields a key quantity, $\mathsf{A}$, the outcome value at which the direction of the ordering of the treatment and control distributions changes.
  Using an extensive simulation, we studied relative bias of point estimators of $\mathsf{A}$ and coverage and relative width of bootstrap confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08823v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Kulinskaya, David C. Hoaglin</dc:creator>
    </item>
    <item>
      <title>Provable Diffusion Posterior Sampling for Bayesian Inversion</title>
      <link>https://arxiv.org/abs/2512.08022</link>
      <description>arXiv:2512.08022v1 Announce Type: cross 
Abstract: This paper proposes a novel diffusion-based posterior sampling method within a plug-and-play (PnP) framework. Our approach constructs a probability transport from an easy-to-sample terminal distribution to the target posterior, using a warm-start strategy to initialize the particles. To approximate the posterior score, we develop a Monte Carlo estimator in which particles are generated using Langevin dynamics, avoiding the heuristic approximations commonly used in prior work. The score governing the Langevin dynamics is learned from data, enabling the model to capture rich structural features of the underlying prior distribution. On the theoretical side, we provide non-asymptotic error bounds, showing that the method converges even for complex, multi-modal target posterior distributions. These bounds explicitly quantify the errors arising from posterior score estimation, the warm-start initialization, and the posterior sampling procedure. Our analysis further clarifies how the prior score-matching error and the condition number of the Bayesian inverse problem influence overall performance. Finally, we present numerical experiments demonstrating the effectiveness of the proposed method across a range of inverse problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08022v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Chenguang Duan, Yuling Jiao, Ruoxuan Li, Jerry Zhijian Yang, Cheng Yuan</dc:creator>
    </item>
    <item>
      <title>Uncertainty quantification for mixed membership in multilayer networks with degree heterogeneity using Gaussian variational inference</title>
      <link>https://arxiv.org/abs/2512.08146</link>
      <description>arXiv:2512.08146v1 Announce Type: cross 
Abstract: Analyzing multilayer networks is central to understanding complex relational measurements collected across multiple conditions or over time. A pivotal task in this setting is to quantify uncertainty in community structure while appropriately pooling information across layers and accommodating layer-specific heterogeneity. Building on the multilayer degree-corrected mixed-membership (ML-DCMM) model, which captures both stable community membership profiles and layer-specific vertex activity levels, we propose a Bayesian inference framework based on a spectral-assisted likelihood. We then develop a computationally efficient Gaussian variational inference algorithm implemented via stochastic gradient descent. Our theoretical analysis establishes a variational Bernstein--von Mises theorem, which provides a frequentist guarantee for using the variational posterior to construct confidence sets for mixed memberships. We demonstrate the utility of the method on a U.S. airport longitudinal network, where the procedure yields robust estimates, natural uncertainty quantification, and competitive performance relative to state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08146v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fangzheng Xie, Hsin-Hsiung Huang</dc:creator>
    </item>
    <item>
      <title>Bayesian Semiparametric Mixture Cure (Frailty) Models</title>
      <link>https://arxiv.org/abs/2512.08173</link>
      <description>arXiv:2512.08173v1 Announce Type: cross 
Abstract: In recent years, mixture cure models have gained increasing popularity in survival analysis as an alternative to the Cox proportional hazards model, particularly in settings where a subset of patients is considered cured. The proportional hazards mixture cure model is especially advantageous when the presence of a cured fraction can be reasonably assumed, providing a more accurate representation of long-term survival dynamics. In this study, we propose a novel hierarchical Bayesian framework for the semiparametric mixture cure model, which accommodates both the inclusion and exclusion of a frailty component, allowing for greater flexibility in capturing unobserved heterogeneity among patients. Samples from the posterior distribution are obtained using a Markov chain Monte Carlo method, leveraging a hierarchical structure inspired by Bayesian Lasso. Comprehensive simulation studies are conducted across diverse scenarios to evaluate the performance and robustness of the proposed models. Bayesian model comparison and assessment are performed using various criteria. Finally, the proposed approaches are applied to two well-known datasets in the cure model literature: the E1690 melanoma trial and a colon cancer clinical trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08173v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fatih K{\i}z{\i}laslan, Valeria Vitelli</dc:creator>
    </item>
    <item>
      <title>Wishart kernel density estimation for strongly mixing time series on the cone of positive definite matrices</title>
      <link>https://arxiv.org/abs/2512.08232</link>
      <description>arXiv:2512.08232v1 Announce Type: cross 
Abstract: A Wishart kernel density estimator (KDE) is introduced for density estimation in the cone of positive definite matrices. The estimator is boundary-aware and mitigates the boundary bias suffered by conventional KDEs, while remaining simple to implement. Its mean squared error, uniform strong consistency on expanding compact sets, and asymptotic normality are established under the Lebesgue measure and suitable mixing conditions. This work represents the first study of density estimation on this space under any metric. For independent observations, an asymptotic upper bound on the mean absolute error is also derived. A simulation study compares the performance of the Wishart KDE to another boundary-aware KDE that relies on the matrix-variate lognormal distribution proposed by Schwartzman [Int. Stat. Rev., 2016, 84(3), 456-486]. Results suggest that the Wishart KDE is superior for a selection of autoregressive coefficient matrices and innovation covariance matrices when estimating the stationary marginal density of a Wishart autoregressive process. To illustrate the practical utility of the Wishart KDE, an application to finance is made by estimating the marginal density function of a time series of realized covariance matrices, calculated from 5-minute intra-day returns, between the share prices of Amazon Corp. and the Standard &amp; Poor's 500 exchange-traded fund over a one-year period. All code is publicly available via the R package ksm to facilitate implementation of the method and reproducibility of the findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08232v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'eo R. Belzile, Christian Genest, Fr\'ed\'eric Ouimet, Donald Richards</dc:creator>
    </item>
    <item>
      <title>A Distribution Testing Approach to Clustering Distributions</title>
      <link>https://arxiv.org/abs/2512.08376</link>
      <description>arXiv:2512.08376v1 Announce Type: cross 
Abstract: We study the following distribution clustering problem: Given a hidden partition of $k$ distributions into two groups, such that the distributions within each group are the same, and the two distributions associated with the two clusters are $\varepsilon$-far in total variation, the goal is to recover the partition. We establish upper and lower bounds on the sample complexity for two fundamental cases: (1) when one of the cluster's distributions is known, and (2) when both are unknown. Our upper and lower bounds characterize the sample complexity's dependence on the domain size $n$, number of distributions $k$, size $r$ of one of the clusters, and distance $\varepsilon$. In particular, we achieve tightness with respect to $(n,k,r,\varepsilon)$ (up to an $O(\log k)$ factor) for all regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08376v1</guid>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gunjan Kumar, Yash Pote, Jonathan Scarlett</dc:creator>
    </item>
    <item>
      <title>Minimax and Bayes Optimal Adaptive Experimental Design for Treatment Choice</title>
      <link>https://arxiv.org/abs/2512.08513</link>
      <description>arXiv:2512.08513v1 Announce Type: cross 
Abstract: We consider an adaptive experiment for treatment choice and design a minimax and Bayes optimal adaptive experiment with respect to regret. Given binary treatments, the experimenter's goal is to choose the treatment with the highest expected outcome through an adaptive experiment, in order to maximize welfare. We consider adaptive experiments that consist of two phases, the treatment allocation phase and the treatment choice phase. The experiment starts with the treatment allocation phase, where the experimenter allocates treatments to experimental subjects to gather observations. During this phase, the experimenter can adaptively update the allocation probabilities using the observations obtained in the experiment. After the allocation phase, the experimenter proceeds to the treatment choice phase, where one of the treatments is selected as the best. For this adaptive experimental procedure, we propose an adaptive experiment that splits the treatment allocation phase into two stages, where we first estimate the standard deviations and then allocate each treatment proportionally to its standard deviation. We show that this experiment, often referred to as Neyman allocation, is minimax and Bayes optimal in the sense that its regret upper bounds exactly match the lower bounds that we derive. To show this optimality, we derive minimax and Bayes lower bounds for the regret using change-of-measure arguments. Then, we evaluate the corresponding upper bounds using the central limit theorem and large deviation bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.08513v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Limit results for distributed estimation of invariant subspaces in multiple networks inference and PCA</title>
      <link>https://arxiv.org/abs/2206.04306</link>
      <description>arXiv:2206.04306v5 Announce Type: replace 
Abstract: Several statistical problems, such as multiple heterogeneous graph analysis, distributed PCA, integrative data analysis, and simultaneous dimension reduction of images, can involve a collection of $m$ matrices whose leading subspaces $U^{(i)}$ consist of a shared subspace $U_c$ and individual subspaces $U_s^{(i)}$. We consider a distributed estimation procedure that first obtains $\hat U^{(i)}$ as the leading singular vectors for each observed noisy matrix, then computes the leading left singular vectors of the concatenated matrix $[\hat U^{(1)}|\hat U^{(2)}|\dots|\hat U^{(m)}]$ as $\hat U_c$, and finally computes the leading singular vectors of the projection of each $\hat U^{(i)}$ onto the orthogonal complement of $\hat U_c$ as $\hat U_s^{(i)}$. In this paper, we provide a framework for deriving limit results for such distributed estimation procedures, including expansions of estimation errors in both common and individual subspaces and their asymptotically normal approximations. We apply this framework specifically to (1) parameter estimation for multiple heterogeneous random graphs with shared subspaces, and (2) distributed PCA for independent sub-Gaussian random vectors with spiked covariance structures. Leveraging these results, we also consider a two-sample test for the null hypothesis that a pair of random graphs have the same edge probabilities, and present a test statistic whose limiting distribution converges to a central (resp., non-central) $\chi^2$ distribution under the null (resp., local alternative) hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.04306v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runbing Zheng, Minh Tang</dc:creator>
    </item>
    <item>
      <title>Minimax optimal seriation in polynomial time</title>
      <link>https://arxiv.org/abs/2405.08747</link>
      <description>arXiv:2405.08747v3 Announce Type: replace 
Abstract: We consider the seriation problem, whose goal is to recover a hidden ordering from a noisy observation of a permuted Robinson matrix. We establish sharp minimax rates under average-Lipschitz conditions that strictly extend the bi-Lipschitz framework of [Giraud et al., 2023]. We further design a polynomial-time algorithm that attains these optimal rates, thereby resolving two open questions raised in [Giraud et al., 2023]. Finally, our analysis extends to a broader class of matrices beyond those generated by exact permutations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08747v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yann Issartel, Christophe Giraud, Nicolas Verzelen</dc:creator>
    </item>
    <item>
      <title>Simple proof of robustness for Bayesian heavy-tailed linear regression models</title>
      <link>https://arxiv.org/abs/2501.06349</link>
      <description>arXiv:2501.06349v3 Announce Type: replace 
Abstract: In the Bayesian literature, a line of research called resolution of conflict is about the characterization of robustness against outliers of statistical models. The robustness characterization of a model is achieved by establishing the limiting behaviour of the posterior distribution under an asymptotic framework in which the outliers move away from the bulk of the data. The proofs of the robustness characterization results, especially the recent ones for regression models, are technical and not intuitive, limiting the accessibility and preventing the development of theory in that line of research. In this paper, we highlight that the proof complexity is due to the generality of the assumptions on the prior distribution. To address the issue of accessibility, we present a significantly simpler proof for a linear regression model with a specific class of prior distributions, among which we find typically used prior distributions. The class of prior distributions is such that each regression coefficient has a sub-exponential distribution, which allows to exploit a tail bound, contrarily to previous approaches. The proof is intuitive and uses classical results of probability theory. The generality of the assumption on the error distribution is also appealing; essentially, it can be any distribution with regularly varying or log-regularly varying tails. So far, there does not exist a result in such generality for models with regularly varying distributions. We also investigate the necessity of the assumptions. To promote the development of theory in resolution of conflict, we highlight how the key steps of the proof can be adapted for other models and present an application of the proof technique in the context of generalized linear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06349v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philippe Gagnon</dc:creator>
    </item>
    <item>
      <title>Sufficient digits and density estimation: A Bayesian nonparametric approach using generalized finite P\'olya trees</title>
      <link>https://arxiv.org/abs/2506.09437</link>
      <description>arXiv:2506.09437v3 Announce Type: replace-cross 
Abstract: This paper proposes a novel approach for statistical modelling of a continuous random variable $X$ on $[0, 1)$, based on its digit representation $X=.X_1X_2\ldots$. In general, $X$ can be coupled with a latent random variable $N$ so that $(X_1,\ldots,X_N)$ becomes a sufficient statistics and $.X_{N+1}X_{N+2}\ldots$ is uniformly distributed. In line with this fact, and focusing on binary digits for simplicity, we propose a family of generalized finite P{\'o}lya trees that induces a random density for a sample, which becomes a flexible tool for density estimation. Here, the digit system may be random and learned from the data. We provide a detailed Bayesian analysis, including closed form expression for the posterior distribution. We analyse the frequentist properties as the sample size increases, and provide sufficient conditions for consistency of the posterior distributions of the random density and $N$. We consider an extension to data spanning multiple orders of magnitude, and propose a prior distribution that encodes the so-called extended Newcomb-Benford law. Such a model shows promising results for density estimation of human-activity data. Our methodology is illustrated on several synthetic and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09437v3</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Jesper M{\o}ller</dc:creator>
    </item>
    <item>
      <title>Gaussian Approximation for Two-Timescale Linear Stochastic Approximation</title>
      <link>https://arxiv.org/abs/2508.07928</link>
      <description>arXiv:2508.07928v2 Announce Type: replace-cross 
Abstract: In this paper, we establish non-asymptotic bounds for accuracy of normal approximation for linear two-timescale stochastic approximation (TTSA) algorithms driven by martingale difference or Markov noise. Focusing on both the last iterate and Polyak-Ruppert averaging regimes, we derive bounds for normal approximation in terms of the convex distance between probability distributions. Our analysis reveals a non-trivial interaction between the fast and slow timescales: the normal approximation rate for the last iterate improves as the timescale separation increases, while it decreases in the Polyak-Ruppert averaged setting. We also provide the high-order moment bounds for the error of linear TTSA algorithm, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07928v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bogdan Butyrin, Artemy Rubtsov, Alexey Naumov, Vladimir Ulyanov, Sergey Samsonov</dc:creator>
    </item>
    <item>
      <title>A Case for a "Refutations and Critiques" Track in Statistics Journals</title>
      <link>https://arxiv.org/abs/2509.03702</link>
      <description>arXiv:2509.03702v3 Announce Type: replace-cross 
Abstract: The statistics community, which has traditionally lacked a transparent and open peer-review system, faces a challenge of inconsistent paper quality, with some published work containing substantial errors. This problem resonates with concerns raised by Schaeffer et al. (2025) regarding the rapid growth of machine learning research. They argue that peer review has proven insufficient to prevent the publication of ``misleading, incorrect, flawed or perhaps even fraudulent studies'' and that a ``dynamic self-correcting research ecosystem'' is needed. This note provides a concrete illustration of this problem by examining two published papers, Wang, Zhou and Lin (2025) and Liu et al. (2023), and exposing striking and critical errors in their proofs. The presence of such errors in major journals raises a fundamental question about the importance and verification of mathematical proofs in our field. Echoing the proposal from Schaeffer et al. (2025), we argue that reforming the peer-review system itself is likely impractical. Instead, we propose a more viable path forward: the creation of a high-profile, reputable platform, such as a ``Refutations and Critiques'' track on arXiv, to provide visibility to vital research that critically challenges prior work. Such a mechanism would be crucial for enhancing the reliability and credibility of statistical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03702v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhen Li</dc:creator>
    </item>
  </channel>
</rss>
