<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Jan 2025 05:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Polynomial Kernel and Incompressibility for Prison-Free Edge Deletion and Completion</title>
      <link>https://arxiv.org/abs/2501.15952</link>
      <description>arXiv:2501.15952v1 Announce Type: new 
Abstract: Given a graph $G$ and an integer $k$, the $H$-free Edge Deletion problem asks whether there exists a set of at most $k$ edges of $G$ whose deletion makes $G$ free of induced copies of $H$. Significant attention has been given to the kernelizability aspects of this problem -- i.e., for which graphs $H$ does the problem admit an "efficient preprocessing" procedure, known as a polynomial kernelization, where an instance $I$ of the problem with parameter $k$ is reduced to an equivalent instance $I'$ whose size and parameter value are bounded polynomially in $k$? Although such routines are known for many graphs $H$ where the class of $H$-free graphs has significant restricted structure, it is also clear that for most graphs $H$ the problem is incompressible, i.e., admits no polynomial kernelization parameterized by $k$ unless the polynomial hierarchy collapses. These results led Marx and Sandeep to the conjecture that $H$-free Edge Deletion is incompressible for any graph $H$ with at least five vertices, unless $H$ is complete or has at most one edge (JCSS 2022). This conjecture was reduced to the incompressibility of $H$-free Edge Deletion for a finite list of graphs $H$. We consider one of these graphs, which we dub the prison, and show that Prison-Free Edge Deletion has a polynomial kernel, refuting the conjecture. On the other hand, the same problem for the complement of the prison is incompressible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15952v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S\'ehane Bel Houari-Durand, Eduard Eiben, Magnus Wahlstr\"om</dc:creator>
    </item>
    <item>
      <title>Complexity of Minimal Faithful Permutation Degree for Fitting-free Groups</title>
      <link>https://arxiv.org/abs/2501.16039</link>
      <description>arXiv:2501.16039v1 Announce Type: new 
Abstract: In this paper, we investigate the complexity of computing the minimal faithful permutation degree for groups without abelian normal subgroups. When our groups are given as quotients of permutation groups, we establish that this problem is in $\textsf{P}$. Furthermore, in the setting of permutation groups, we obtain an upper bound of $\textsf{NC}$ for this problem. This improves upon the work of Das and Thakkar (STOC 2024), who established a Las Vegas polynomial-time algorithm for this class in the setting of permutation groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16039v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>math.GR</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Levet, Pranjal Srivastava, Dhara Thakkar</dc:creator>
    </item>
    <item>
      <title>Online Allocation with Multi-Class Arrivals: Group Fairness vs Individual Welfare</title>
      <link>https://arxiv.org/abs/2501.15782</link>
      <description>arXiv:2501.15782v1 Announce Type: cross 
Abstract: We introduce and study a multi-class online resource allocation problem with group fairness guarantees. The problem involves allocating a fixed amount of resources to a sequence of agents, each belonging to a specific group. The primary objective is to ensure fairness across different groups in an online setting. We focus on three fairness notions: one based on quantity and two based on utility. To achieve fair allocations, we develop two threshold-based online algorithms, proving their optimality under two fairness notions and near-optimality for the more challenging one. Additionally, we demonstrate a fundamental trade-off between group fairness and individual welfare using a novel representative function-based approach. To address this trade-off, we propose a set-aside multi-threshold algorithm that reserves a portion of the resource to ensure fairness across groups while utilizing the remaining resource to optimize efficiency under utility-based fairness notions. This algorithm is proven to achieve the Pareto-optimal trade-off. We also demonstrate that our problem can model a wide range of real-world applications, including network caching and cloud computing, and empirically evaluate our proposed algorithms in the network caching problem using real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15782v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Faraz Zargari, Hossein Nekouyan Jazi, Bo Sun, Xiaoqi Tan</dc:creator>
    </item>
    <item>
      <title>Maintaining $\mathsf{CMSO}_2$ properties on dynamic structures with bounded feedback vertex number</title>
      <link>https://arxiv.org/abs/2107.06232</link>
      <description>arXiv:2107.06232v2 Announce Type: replace 
Abstract: Let $\varphi$ be a sentence of $\mathsf{CMSO}_2$ (monadic second-order logic with quantification over edge subsets and counting modular predicates) over the signature of graphs. We present a dynamic data structure that for a given graph $G$ that is updated by edge insertions and edge deletions, maintains whether $\varphi$ is satisfied in $G$. The data structure is required to correctly report the outcome only when the feedback vertex number of $G$ does not exceed a fixed constant $k$, otherwise it reports that the feedback vertex number is too large. With this assumption, we guarantee amortized update time ${\cal O}_{\varphi,k}(\log n)$. If we additionally assume that the feedback vertex number of $G$ never exceeds $k$, this update time guarantee is worst-case.
  By combining this result with a classic theorem of Erd\H{o}s and P\'osa, we give a fully dynamic data structure that maintains whether a graph contains a packing of $k$ vertex-disjoint cycles with amortized update time ${\cal O}_{k}(\log n)$. Our data structure also works in a larger generality of relational structures over binary signatures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.06232v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.LO</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konrad Majewski, Micha{\l} Pilipczuk, Marek Soko{\l}owski</dc:creator>
    </item>
    <item>
      <title>Improved Integrality Gap in Max-Min Allocation: or Topology at the North Pole</title>
      <link>https://arxiv.org/abs/2202.01143</link>
      <description>arXiv:2202.01143v3 Announce Type: replace 
Abstract: In the max-min allocation problem a set $P$ of players are to be allocated disjoint subsets of a set $R$ of indivisible resources, such that the minimum utility among all players is maximized. We study the restricted variant, also known as the Santa Claus problem, where each resource has an intrinsic positive value, and each player covets a subset of the resources. Bez\'akov\'a and Dani showed that this problem is NP-hard to approximate within a factor less than $2$, consequently a great deal of work has focused on approximate solutions. The principal approach for obtaining approximation algorithms has been via the Configuration LP (CLP) of Bansal and Sviridenko. Accordingly, there has been much interest in bounding the integrality gap of this CLP. The existing algorithms and integrality gap estimations are all based one way or another on the combinatorial augmenting tree argument of Haxell for finding perfect matchings in certain hypergraphs. Our main innovation in this paper is to introduce the use of topological methods for the restricted max-min allocation problem, to replace the combinatorial argument. This approach yields substantial improvements in the integrality gap of the CLP. In particular we improve the previously best known bound of $3.808$ to $3.534$. We also study the $(1,\varepsilon)$-restricted version, in which resources can take only two values, and improve the integrality gap in most cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.01143v3</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Penny Haxell, Tibor Szab\'o</dc:creator>
    </item>
    <item>
      <title>Classification via Two-Way Comparisons</title>
      <link>https://arxiv.org/abs/2302.09692</link>
      <description>arXiv:2302.09692v2 Announce Type: replace 
Abstract: Given a weighted, ordered query set $Q$ and a partition of $Q$ into classes, we study the problem of computing a minimum-cost decision tree that, given any query $q$ in $Q$, uses equality tests and less-than comparisons to determine the class to which $q$ belongs. Such a tree can be much smaller than a lookup table, and much faster and smaller than a conventional search tree. We give the first polynomial-time algorithm for the problem. The algorithm extends naturally to the setting where each query has multiple allowed classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09692v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3709361</arxiv:DOI>
      <arxiv:journal_reference>ACM Transactions on Algorithms (2024)</arxiv:journal_reference>
      <dc:creator>Marek Chrobak, Neal E. Young</dc:creator>
    </item>
    <item>
      <title>Tight bounds for the sensitivity of CDAWGs with left-end edits</title>
      <link>https://arxiv.org/abs/2303.01726</link>
      <description>arXiv:2303.01726v5 Announce Type: replace 
Abstract: Compact directed acyclic word graphs (CDAWGs) [Blumer et al. 1987] are a fundamental data structure on strings with applications in text pattern searching, data compression, and pattern discovery. Intuitively, the CDAWG of a string $T$ is obtained by merging isomorphic subtrees of the suffix tree [Weiner 1973] of the same string $T$, thus CDAWGs are a compact indexing structure. In this paper, we investigate the sensitivity of CDAWGs when a single character edit operation (insertion, deletion, or substitution) is performed at the left-end of the input string $T$, namely, we are interested in the worst-case increase in the size of the CDAWG after a left-end edit operation. We prove that if $e$ is the number of edges of the CDAWG for string $T$, then the number of new edges added to the CDAWG after a left-end edit operation on $T$ does not exceed $e$. Further, we present a matching lower bound on the sensitivity of CDAWGs for left-end insertions, and almost matching lower bounds for left-end deletions and substitutions. We then generalize our lower-bound instance for left-end insertions to leftward online construction of the CDAWG, and show that it requires $\Omega(n^2)$ time for some string of length $n$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.01726v5</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroto Fujimaru, Yuto Nakashima, Shunsuke Inenaga</dc:creator>
    </item>
    <item>
      <title>Let them have CAKES: A Cutting-Edge Algorithm for Scalable, Efficient, and Exact Search on Big Data</title>
      <link>https://arxiv.org/abs/2309.05491</link>
      <description>arXiv:2309.05491v3 Announce Type: replace 
Abstract: The ongoing Big Data explosion has created a demand for efficient and scalable algorithms for similarity search. Most recent work has focused on \textit{approximate} $k$-NN search, and while this may be sufficient for some applications, \textit{exact} $k$-NN search would be ideal for many applications.
  We present CAKES, a set of three novel, exact algorithms for $k$-NN search. CAKES's algorithms are generic over \textit{any} distance function, and they \textit{do not} scale with the cardinality or embedding dimension of the dataset, but rather with its metric entropy and fractal dimension. We test these claims on datasets from the ANN-Benchmarks suite under commonly-used distance functions, as well as on a genomic dataset with Levenshtein distance and a radio-frequency dataset with Dynamic Time Warping distance. We demonstrate that CAKES exhibits near-constant scaling with cardinality on data conforming to the manifold hypothesis, and has perfect recall on data in \textit{metric} spaces. We also demonstrate that CAKES exhibits significantly higher recall than state-of-the-art $k$-NN search algorithms when the distance function is not a metric. Additionally, we show that indexing and tuning time for CAKES is an order of magnitude, or more, faster than state-of-the-art approaches. We conclude that CAKES is a highly efficient and scalable algorithm for exact $k$-NN search on Big Data. We provide a Rust implementation of CAKES under an MIT license at https://github.com/URI-ABD/clam</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.05491v3</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Morgan E. Prior, Thomas J. Howard III, Oliver McLaughlin, Terrence Ferguson, Najib Ishaq, Noah M. Daniels</dc:creator>
    </item>
    <item>
      <title>Faster Weighted and Unweighted Tree Edit Distance and APSP Equivalence</title>
      <link>https://arxiv.org/abs/2411.06502</link>
      <description>arXiv:2411.06502v2 Announce Type: replace 
Abstract: The tree edit distance (TED) between two rooted ordered trees with $n$ nodes labeled from an alphabet $\Sigma$ is the minimum cost of transforming one tree into the other by a sequence of valid operations consisting of insertions, deletions and relabeling of nodes. The tree edit distance is a well-known generalization of string edit distance and has been studied since the 1970s. Years of steady improvements have led to an $O(n^3)$ algorithm [DMRW 2010]. Fine-grained complexity casts light onto the hardness of TED showing that a truly subcubic time algorithm for TED implies a truly subcubic time algorithm for All-Pairs Shortest Paths (APSP) [BGMW 2020]. Therefore, under the popular APSP hypothesis, a truly subcubic time algorithm for TED cannot exist. However, unlike many problems in fine-grained complexity for which conditional hardness based on APSP also comes with equivalence to APSP, whether TED can be reduced to APSP has remained unknown.
  In this paper, we resolve this. Not only we show that TED is fine-grained equivalent to APSP, our reduction is tight enough, so that combined with the fastest APSP algorithm to-date [Williams 2018] it gives the first ever subcubic time algorithm for TED running in $n^3/2^{\Omega(\sqrt{\log{n}})}$ time.
  We also consider the unweighted tree edit distance problem in which the cost of each edit is one. For unweighted TED, a truly subcubic algorithm is known due to Mao [Mao 2022], later improved slightly by D\"{u}rr [D\"{u}rr 2023] to run in $O(n^{2.9148})$. Their algorithm uses bounded monotone min-plus product as a crucial subroutine, and the best running time for this product is $\tilde{O}(n^{\frac{3+\omega}{2}})\leq O(n^{2.6857})$ (where $\omega$ is the exponent of fast matrix multiplication). In this work, we close this gap and give an algorithm for unweighted TED that runs in $\tilde{O}(n^{\frac{3+\omega}{2}})$ time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06502v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakob Nogler, Adam Polak, Barna Saha, Virginia Vassilevska Williams, Yinzhan Xu, Christopher Ye</dc:creator>
    </item>
    <item>
      <title>An Efficient Algorithm for Permutation Iteration Using a Singly Linked List</title>
      <link>https://arxiv.org/abs/2501.10102</link>
      <description>arXiv:2501.10102v4 Announce Type: replace 
Abstract: We present a new algorithm for iterating over all permutations of a sequence. The algorithm leverages elementary $O(1)$ operations on recursive lists. As a result, no new nodes are allocated during the computation. Instead, all elements are rearranged within the original nodes of the singly linked list throughout the process. While permutations are generated in an unusual order, the transitions between consecutive permutations remain smooth. A proof of concept written in the Lisp programming language is proposed and discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10102v4</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Thomas Baruchel</dc:creator>
    </item>
    <item>
      <title>Scalable Experimental Bounds for Entangled Quantum State Fidelities</title>
      <link>https://arxiv.org/abs/2210.03048</link>
      <description>arXiv:2210.03048v3 Announce Type: replace-cross 
Abstract: Estimating the state preparation fidelity of highly entangled states on noisy intermediate-scale quantum (NISQ) devices is important for benchmarking and application considerations. Unfortunately, exact fidelity measurements quickly become prohibitively expensive, as they scale exponentially as $O(3^N)$ for $N$-qubit states, using full state tomography with measurements in all Pauli bases combinations. However, Somma and others [PhysRevA.74.052302] established that the complexity could be drastically reduced when looking at fidelity lower bounds for states that exhibit symmetries, such as Dicke States and GHZ States. These bounds must still be tight enough for larger states to provide reasonable estimations on NISQ devices.
  For the first time and more than 15 years after the theoretical introduction, we report meaningful lower bounds for the state preparation fidelity of all Dicke States up to $N=10$ and all GHZ states up to $N=20$ on Quantinuum H1 ion-trap systems using efficient implementations of recently proposed scalable circuits for these states. Our achieved lower bounds match or exceed previously reported exact fidelities on superconducting systems for much smaller states. Furthermore, we provide evidence that for large Dicke States $D^N_{N/2}$, we may resort to a GHZ-based approximate state preparation to achieve better fidelity. This work provides a path forward to benchmarking entanglement as NISQ devices improve in size and quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.03048v3</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>ACM Transactions on Quantum Computing 5, no. 4 (2024): 1-21</arxiv:journal_reference>
      <dc:creator>Shamminuj Aktar, Andreas B\"artschi, Abdel-Hameed A. Badawy, Stephan Eidenbenz</dc:creator>
    </item>
    <item>
      <title>Compressing CFI Graphs and Lower Bounds for the Weisfeiler-Leman Refinements</title>
      <link>https://arxiv.org/abs/2308.11970</link>
      <description>arXiv:2308.11970v2 Announce Type: replace-cross 
Abstract: The $k$-dimensional Weisfeiler-Leman ($k$-WL) algorithm is a simple combinatorial algorithm that was originally designed as a graph isomorphism heuristic. It naturally finds applications in Babai's quasipolynomial time isomorphism algorithm, practical isomorphism solvers, and algebraic graph theory. However, it also has surprising connections to other areas such as logic, proof complexity, combinatorial optimization, and machine learning.
  The algorithm iteratively computes a coloring of the $k$-tuples of vertices of a graph. Since F\"urer's linear lower bound [ICALP 2001], it has been an open question whether there is a super-linear lower bound for the iteration number for $k$-WL on graphs. We answer this question affirmatively, establishing an $\Omega(n^{k/2})$-lower bound for all $k$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.11970v2</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Grohe, Moritz Lichter, Daniel Neuen, Pascal Schweitzer</dc:creator>
    </item>
    <item>
      <title>Quantum Realization of the Finite Element Method</title>
      <link>https://arxiv.org/abs/2403.19512</link>
      <description>arXiv:2403.19512v3 Announce Type: replace-cross 
Abstract: This paper presents a quantum algorithm for the solution of prototypical second-order linear elliptic partial differential equations discretized by $d$-linear finite elements on Cartesian grids of a bounded $d$-dimensional domain. An essential step in the construction is a BPX preconditioner, which transforms the linear system into a sufficiently well-conditioned one, making it amenable to quantum computation. We provide a constructive proof demonstrating that, for any fixed dimension, our quantum algorithm can compute suitable functionals of the solution to a given tolerance $\mathtt{tol}$ with an optimal complexity of order $\mathtt{tol}^{-1}$ up to logarithmic terms, significantly improving over existing approaches. Notably, this approach does not rely on regularity of the solution and achieves quantum advantage over classical solvers in two dimensions, whereas prior quantum methods required at least four dimensions for asymptotic benefits. We further detail the design and implementation of a quantum circuit capable of executing our algorithm, present simulator results, and report numerical experiments on current quantum hardware, confirming the feasibility of preconditioned finite element methods for near-term quantum computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19512v3</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Deiml, Daniel Peterseim</dc:creator>
    </item>
    <item>
      <title>Retrieval with Learned Similarities</title>
      <link>https://arxiv.org/abs/2407.15462</link>
      <description>arXiv:2407.15462v4 Announce Type: replace-cross 
Abstract: Retrieval plays a fundamental role in recommendation systems, search, and natural language processing (NLP) by efficiently finding relevant items from a large corpus given a query. Dot products have been widely used as the similarity function in such tasks, enabled by Maximum Inner Product Search (MIPS) algorithms for efficient retrieval. However, state-of-the-art retrieval algorithms have migrated to learned similarities. These advanced approaches encompass multiple query embeddings, complex neural networks, direct item ID decoding via beam search, and hybrid solutions. Unfortunately, we lack efficient solutions for retrieval in these state-of-the-art setups. Our work addresses this gap by investigating efficient retrieval techniques with expressive learned similarity functions. We establish Mixture-of-Logits (MoL) as a universal approximator of similarity functions, demonstrate that MoL's expressiveness can be realized empirically to achieve superior performance on diverse retrieval scenarios, and propose techniques to retrieve the approximate top-k results using MoL with tight error bounds. Through extensive experimentation, we show that MoL, enhanced by our proposed mutual information-based load balancing loss, sets new state-of-the-art results across heterogeneous scenarios, including sequential retrieval models in recommendation systems and finetuning language models for question answering; and our approximate top-$k$ algorithms outperform baselines by up to 66x in latency while achieving &gt;.99 recall rate compared to exact algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15462v4</guid>
      <category>cs.IR</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3696410.3714822</arxiv:DOI>
      <dc:creator>Bailu Ding, Jiaqi Zhai</dc:creator>
    </item>
    <item>
      <title>Enumeration of Minimal Hitting Sets Parameterized by Treewidth</title>
      <link>https://arxiv.org/abs/2408.15776</link>
      <description>arXiv:2408.15776v2 Announce Type: replace-cross 
Abstract: Enumerating the minimal hitting sets of a hypergraph is a problem which arises in many data management applications that include constraint mining, discovering unique column combinations, and enumerating database repairs. Previously, Eiter et al. showed that the minimal hitting sets of an $n$-vertex hypergraph, with treewidth $w$, can be enumerated with delay $O^*(n^{w})$ (ignoring polynomial factors), with space requirements that scale with the output size. We improve this to fixed-parameter-linear delay, following an FPT preprocessing phase. The memory consumption of our algorithm is exponential with respect to the treewidth of the hypergraph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15776v2</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Batya Kenig, Dan Shlomo Mizrahi</dc:creator>
    </item>
    <item>
      <title>Optimal Binary Variable-Length Codes with a Bounded Number of 1's per Codeword: Design, Analysis, and Applications</title>
      <link>https://arxiv.org/abs/2501.11129</link>
      <description>arXiv:2501.11129v2 Announce Type: replace-cross 
Abstract: In this paper, we consider the problem of constructing optimal average-length binary codes under the constraint that each codeword must contain at most $D$ ones, where $D$ is a given input parameter. We provide an $O(n^2D)$-time complexity algorithm for the construction of such codes, where $n$ is the number of codewords. We also describe several scenarios where the need to design these kinds of codes naturally arises. Our algorithms allow us to construct both optimal average-length prefix binary codes and optimal average-length alphabetic binary codes. In the former case, our $O(n^2D)$-time algorithm substantially improves on the previously known $O(n^{2+D})$-time complexity algorithm for the same problem. We also provide a Kraft-like inequality for the existence of (optimal) variable-length binary codes, subject to the above-described constraint on the number of 1's in each codeword.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11129v2</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>math.IT</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Bruno, Roberto De Prisco, Ugo Vaccaro</dc:creator>
    </item>
    <item>
      <title>Improved Decoding of Tanner Codes</title>
      <link>https://arxiv.org/abs/2501.12293</link>
      <description>arXiv:2501.12293v2 Announce Type: replace-cross 
Abstract: In this paper, we present improved decoding algorithms for expander-based Tanner codes.
  We begin by developing a randomized linear-time decoding algorithm that, under the condition that $ \delta d_0 &gt; 2 $, corrects up to $ \alpha n $ errors for a Tanner code $ T(G, C_0) $, where $ G $ is a $ (c, d, \alpha, \delta) $-bipartite expander with $n$ left vertices, and $ C_0 \subseteq \mathbb{F}_2^d $ is a linear inner code with minimum distance $ d_0 $. This result improves upon the previous work of Cheng, Ouyang, Shangguan, and Shen (RANDOM 2024), which required $ \delta d_0 &gt; 3 $.
  We further derandomize the algorithm to obtain a deterministic linear-time decoding algorithm with the same decoding radius. Our algorithm improves upon the previous deterministic algorithm of Cheng et al.\ by achieving a decoding radius of $ \alpha n $, compared with the previous radius of $ \frac{2\alpha}{d_0(1 + 0.5c\delta) }n$.
  Additionally, we investigate the size-expansion trade-off introduced by the recent work of Chen, Cheng, Li, and Ouyang (IEEE TIT 2023), and use it to provide new bounds on the minimum distance of Tanner codes. Specifically, we prove that the minimum distance of a Tanner code $T(G,C_0)$ is approximately $f_\delta^{-1} \left( \frac{1}{d_0} \right) \alpha n $, where $ f_\delta(\cdot) $ is the Size-Expansion Function. As another application, we improve the decoding radius of our decoding algorithms from $\alpha n$ to approximately $f_\delta^{-1}(\frac{2}{d_0})\alpha n$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12293v2</guid>
      <category>cs.IT</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.IT</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaienhe Zhou, Zeyu Guo</dc:creator>
    </item>
  </channel>
</rss>
