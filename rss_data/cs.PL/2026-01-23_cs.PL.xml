<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Jan 2026 05:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Remarks on Algebraic Reconstruction of Types and Effects</title>
      <link>https://arxiv.org/abs/2601.15455</link>
      <description>arXiv:2601.15455v1 Announce Type: new 
Abstract: In their 1991 paper "Algebraic Reconstruction of Types and Effects," Pierre Jouvelot and David Gifford presented a type-and-effect reconstruction algorithm based on an algebraic structure of effects. Their work is considered a milestone in the development of type-and-effect systems, and has inspired numerous subsequent works in the area of static analysis. However, unlike the later research it spawned, the original algorithm considered a language with higher-rank polymorphism, a feature which is challenging to implement correctly. In this note, we identify subtle bugs related to variable binding in their approach to this feature. We revisit their type system and reconstruction algorithm, and describe the discovered issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15455v1</guid>
      <category>cs.PL</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrycja Balik, Szymon J\k{e}dras, Piotr Polesiuk</dc:creator>
    </item>
    <item>
      <title>Prioritizing Configuration Relevance via Compiler-Based Refined Feature Ranking</title>
      <link>https://arxiv.org/abs/2601.16008</link>
      <description>arXiv:2601.16008v1 Announce Type: new 
Abstract: Modern programming languages, most notably Rust, offer advanced linguistic constructs for building highly configurable software systems as aggregation of features -- identified by a configuration. However, they pose substantial challenges for program analysis, optimization, and testing, as the combinatorial explosion of configurations often makes exhaustive exploration infeasible. In this manuscript, we present the first compiler-based method for prioritizing configurations. Our approach consists of four main steps: 1. extracting a tailored intermediate representation from the Rust compiler, 2. constructing two complementary graph-based data structures, 3. using centrality measures to rank features, and 4. refining the ranking by considering the extent of code they impact. A fixed number of most relevant configurations are generated based on the achieved feature ranking. The validity of the generated configurations is guaranteed by using a SAT solver that takes a representation of this graph in conjunctive normal form. We formalized this approach and implemented it in a prototype, RustyEx, by instrumenting the Rust compiler. An empirical evaluation on higher-ranked open source Rust projects shows that RustyEx efficiently generates user-specified sets of configurations within bounded resources, while ensuring soundness by construction. The results demonstrate that centrality-guided configuration prioritization enables effective and practical exploration of large configuration spaces, paving the way for future research in configuration-aware analysis and optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16008v1</guid>
      <category>cs.PL</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Federico Bruzzone, Walter Cazzola, Luca Favini</dc:creator>
    </item>
    <item>
      <title>KnowTeX: Visualizing Mathematical Dependencies</title>
      <link>https://arxiv.org/abs/2601.15294</link>
      <description>arXiv:2601.15294v1 Announce Type: cross 
Abstract: Mathematical knowledge exists in many forms, ranging from informal textbooks and lecture notes to large formal proof libraries, yet moving between these representations remains difficult. Informal texts hide dependencies, while formal systems expose every detail in ways that are not always human-readable. Dependency graphs offer a middle ground by making visible the structure of results, definitions, and proofs. We present KnowTeX, a standalone, user-friendly tool that extends the ideas of Lean's Blueprints, enabling the visualization of conceptual dependencies directly from LaTeX sources. Using a simple "uses" command, KnowTeX extracts relationships among statements and generates previewable graphs in DOT and TikZ formats. Applied to mathematical texts, such graphs clarify core results, support education and formalization, and provide a resource for aligning informal and formal mathematical representations. We argue that dependency graphs should become a standard feature of mathematical writing, benefiting both human readers and automated systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15294v1</guid>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <category>cs.PL</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elif Uskuplu, Lawrence S. Moss, Valeria de Paiva</dc:creator>
    </item>
    <item>
      <title>ToolCaching: Towards Efficient Caching for LLM Tool-calling</title>
      <link>https://arxiv.org/abs/2601.15335</link>
      <description>arXiv:2601.15335v1 Announce Type: cross 
Abstract: Recent advances in Large Language Models (LLMs) have revolutionized web applications, enabling intelligent search, recommendation, and assistant services with natural language interfaces. Tool-calling extends LLMs with the ability to interact with external APIs, greatly enhancing their practical utility. While prior research has improved tool-calling performance by adopting traditional computer systems techniques, such as parallel and asynchronous execution, the challenge of redundant or repeated tool-calling requests remains largely unaddressed. Caching is a classic solution to this problem, but applying it to LLM tool-calling introduces new difficulties due to heterogeneous request semantics, dynamic workloads, and varying freshness requirements, which render conventional cache policies ineffective. To address these issues, we propose ToolCaching, an efficient feature-driven and adaptive caching framework for LLM tool-calling systems. ToolCaching systematically integrates semantic and system-level features to evaluate request cacheability and estimate caching value. At its core, the VAAC algorithm integrates bandit-based admission with value-driven, multi-factor eviction, jointly accounting for request frequency, recency, and caching value. Extensive experiments on synthetic and public tool-calling workloads demonstrate that ToolCaching with VAAC achieves up to 11% higher cache hit ratios and 34% lower latency compared to standard policies, effectively accelerating LLM tool-calling in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15335v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Zhai, Dian Shen, Junzhou Luo, Bin Yang</dc:creator>
    </item>
    <item>
      <title>All for One and One for All: Program Logics for Exploiting Internal Determinism in Parallel Programs</title>
      <link>https://arxiv.org/abs/2511.23283</link>
      <description>arXiv:2511.23283v2 Announce Type: replace 
Abstract: Nondeterminism makes parallel programs challenging to write and reason about. To avoid these challenges, researchers have developed techniques for internally deterministic parallel programming, in which the steps of a parallel computation proceed in a deterministic way. Internal determinism is useful because it lets a programmer reason about a program as if it executed in a sequential order. However, no verification framework exists to exploit this property and simplify formal reasoning about internally deterministic programs.
  To capture the essence of why internally deterministic programs should be easier to reason about, this paper defines a property called schedule-independent safety. A program satisfies schedule-independent safety, if, to show that the program is safe across all orderings, it suffices to show that one terminating execution of the program is safe. We then present a separation logic called Musketeer for proving that a program satisfies schedule-independent safety. Once a parallel program has been shown to satisfy schedule-independent safety, we can verify it with a new logic called Angelic, which allows one to dynamically select and verify just one sequential ordering of the program.
  Using Musketeer, we prove the soundness of MiniDet, an affine type system for enforcing internal determinism. MiniDet supports several core algorithmic primitives for internally deterministic programming that have been identified in the research literature, including a deterministic version of a concurrent hash set. Because any syntactically well-typed MiniDet program satisfies schedule-independent safety, we can apply Angelic to verify such programs.
  All results in this paper have been verified in Rocq using the Iris separation logic framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23283v2</guid>
      <category>cs.PL</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3776668</arxiv:DOI>
      <dc:creator>Alexandre Moine, Sam Westrick, Joseph Tassarotti</dc:creator>
    </item>
    <item>
      <title>Dependently-Typed AARA: A Non-Affine Approach for Resource Analysis of Higher-Order Programs</title>
      <link>https://arxiv.org/abs/2601.12943</link>
      <description>arXiv:2601.12943v2 Announce Type: replace 
Abstract: Static resource analysis determines the resource consumption (e.g., time complexity) of a program without executing it. Among the numerous existing approaches for resource analysis, affine type systems have been one dominant approach. However, these affine type systems fall short of deriving precise resource behavior of higher-order programs, particularly in cases that involve partial applications.
  This article presents \lambda_\ms{amor}^\ms{na}}, a non-affine AARA-style dependent type system for resource reasoning about higher-order functional programs. The key observation is that the main issue in previous approaches comes from (i) the close coupling of types and resources, and (ii) the conflict between affine and higher-order typing mechanisms. To derive precise resource behavior of higher-order functions, \lambda_\ms{amor}^\ms{na}} decouples resources from types and follows a non-affine typing mechanism. The non-affine type system of \lambda_\ms{amor}^\ms{na}} achieves this by using dependent types, which allows expressing type-level potential functions separate from ordinary types. This article formalizes \lambda_\ms{amor}^\ms{na}}'s syntax and semantics, and proves its soundness, which guarantees the correctness of resource bounds. Several challenging classic and higher-order examples are presented to demonstrate the expressiveness and compositionality of \lambda_\ms{amor}^\ms{na}}'s reasoning capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.12943v2</guid>
      <category>cs.PL</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Han Xu, Di Wang</dc:creator>
    </item>
    <item>
      <title>Evaluating Compiler Optimization Impacts on zkVM Performance</title>
      <link>https://arxiv.org/abs/2508.17518</link>
      <description>arXiv:2508.17518v2 Announce Type: replace-cross 
Abstract: Zero-knowledge proofs (ZKPs) are the cornerstone of programmable cryptography. They enable (1) privacy-preserving and verifiable computation across blockchains, and (2) an expanding range of off-chain applications such as credential schemes. Zero-knowledge virtual machines (zkVMs) lower the barrier by turning ZKPs into a drop-in backend for standard compilation pipelines. This lets developers write proof-generating programs in conventional languages (e.g., Rust or C++) instead of hand-crafting arithmetic circuits. However, these VMs inherit compiler infrastructures tuned for traditional architectures rather than for proof systems. In particular, standard compiler optimizations assume features that are absent in zkVMs, including cache locality, branch prediction, or instruction-level parallelism. Therefore, their impact on proof generation is questionable.
  We present the first systematic study of the impact of compiler optimizations on zkVMs. We evaluate 64 LLVM passes, six standard optimization levels, and an unoptimized baseline across 58 benchmarks on two RISC-V-based zkVMs (RISC Zero and SP1). While standard LLVM optimization levels do improve zkVM performance (over 40\%), their impact is far smaller than on traditional CPUs, since their decisions rely on hardware features rather than proof constraints. Guided by a fine-grained pass-level analysis, we~\emph{slightly} refine a small set of LLVM passes to be zkVM-aware, improving zkVM execution time by up to 45\% (average +4.6\% on RISC Zero, +1\% on SP1) and achieving consistent proving-time gains. Our work highlights the potential of compiler-level optimizations for zkVM performance and opens new direction for zkVM-specific passes, backends, and superoptimizers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17518v2</guid>
      <category>cs.PF</category>
      <category>cs.PL</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3779212.3790159</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 31st ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2, 2026</arxiv:journal_reference>
      <dc:creator>Thomas Gassmann, Stefanos Chaliasos, Thodoris Sotiropoulos, Zhendong Su</dc:creator>
    </item>
    <item>
      <title>Identification capacity and rate-query tradeoffs in classification systems</title>
      <link>https://arxiv.org/abs/2601.14252</link>
      <description>arXiv:2601.14252v2 Announce Type: replace-cross 
Abstract: We extend classical rate-distortion theory to a discrete classification setting with three resources: tag rate $L$ (bits of storage per entity), identification cost $W$ (queries to determine class membership), and distortion $D$ (misidentification probability). We prove an information barrier: when distinct classes share identical attribute profiles (i.e., the attribute-profile map $\pi$ is not injective on classes), zero-error identification from attribute queries alone is impossible. We characterize the unique Pareto-optimal zero-error point in the $(L,W,D)$ tradeoff space: a nominal tag of length $L=\lceil\log_2 k\rceil$ bits for $k$ classes yields $W=O(1)$ and $D=0$. Without tags ($L=0$), zero-error identification requires $W=\Omega(d)$ attribute queries, where $d$ is the distinguishing dimension; in the worst case $d=n$ (the ambient attribute count), giving $W=\Omega(n)$. In the presence of attribute collisions, any tag-free scheme incurs $D&gt;0$. Conversely, in any information-barrier domain, any scheme achieving $D=0$ requires $L\ge \log_2 k$ bits; this is tight. We show minimal sufficient query sets form the bases of a matroid, so the distinguishing dimension is well-defined, connecting to zero-error source coding via graph entropy. We instantiate the theory to type systems, databases, and biological taxonomy. All results are machine-checked in Lean 4 (6000+ lines, 0 sorry).</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14252v2</guid>
      <category>cs.IT</category>
      <category>cs.PL</category>
      <category>math.IT</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tristan Simas</dc:creator>
    </item>
  </channel>
</rss>
