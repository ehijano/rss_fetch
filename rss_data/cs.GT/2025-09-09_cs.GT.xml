<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Sep 2025 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bi-Level Game-Theoretic Planning of Cyber Deception for Cognitive Arbitrage</title>
      <link>https://arxiv.org/abs/2509.05498</link>
      <description>arXiv:2509.05498v1 Announce Type: new 
Abstract: Cognitive vulnerabilities shape human decision-making and arise primarily from two sources: (1) cognitive capabilities, which include disparities in knowledge, education, expertise, or access to information, and (2) cognitive biases, such as rational inattention, confirmation bias, and base rate neglect, which influence how individuals perceive and process information. Exploiting these vulnerabilities allows an entity with superior cognitive awareness to gain a strategic advantage, a concept referred to as cognitive arbitrage. This paper investigates how to exploit the cognitive vulnerabilities of Advanced Persistent Threat (APT) attackers and proposes cognition-aware defenses that leverage windows of superiority to counteract attacks. Specifically, the proposed bi-level cyber warfare game focuses on "strategic-level" design for defensive deception mechanisms, which then facilitates "operational-level" actions and tactical-level execution of Tactics, Techniques, and Procedures (TTPs). Game-theoretic reasoning and analysis play a significant role in the cross-echelon quantitative modeling and design of cognitive arbitrage strategies. Our numerical results demonstrate that although the defender's initial advantage diminishes over time, strategically timed and deployed deception techniques can turn a negative value for the attacker into a positive one during the planning phase, and achieve at least a 40% improvement in total rewards during execution. This demonstrates that the defender can amplify even small initial advantages, sustain a strategic edge over the attacker, and secure long-term objectives, such as protecting critical assets throughout the attacker's lifecycle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05498v1</guid>
      <category>cs.GT</category>
      <category>cs.CR</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ya-Ting Yang, Quanyan Zhu</dc:creator>
    </item>
    <item>
      <title>Knapsack Contracts and the Importance of Return-on-Investment</title>
      <link>https://arxiv.org/abs/2509.05956</link>
      <description>arXiv:2509.05956v1 Announce Type: new 
Abstract: We formulate the Knapsack Contracts problem -- a strategic version of the classic Stochastic Knapsack problem, which builds upon the inherent randomness shared by stochastic optimization and contract design. In this problem, the principal incentivizes agents to perform jobs with stochastic processing times, the realization of which depends on the agents' efforts.
  Algorithmically, we show that Knapsack Contracts can be viewed as Stochastic Knapsack with costs and multi-choice, features that introduce significant new challenges. We identify a crucial and economically meaningful parameter -- the Return on Investment (ROI) value. We show that the Inverse of ROI (or IOR for short) precisely characterizes the extent to which the approximation guarantees for Stochastic Knapsack extend to its strategic counterpart.
  For IOR of $\alpha$, we develop an algorithm that finds an $O(\alpha)$-approximation policy that does not rely on adaptivity. We establish matching $\Omega(\alpha)$ lower bounds, both on the adaptivity gap, and on what can be achieved without full distributional knowledge of the processing times. Taken together, our results show that IOR is fundamental to understanding the complexity and approximability of Knapsack Contracts, and bounding it is both necessary and sufficient for achieving non-trivial approximation guarantees. Our results highlight the computational challenges arising when stochasticity in optimization problems is controlled by strategic effort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05956v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zohar Barak, Asnat Berlin, Ilan Reuven Cohen, Alon Eden, Omri Porat, Inbal Talgam-Cohen</dc:creator>
    </item>
    <item>
      <title>The Keychain Problem: On Minimizing the Opportunity Cost of Uncertainty</title>
      <link>https://arxiv.org/abs/2509.06187</link>
      <description>arXiv:2509.06187v1 Announce Type: new 
Abstract: In this paper, we introduce a family of sequential decision-making problems, collectively called the Keychain Problem, that involve exploring a set of actions to maximize expected payoff when only a subset of actions are available in each stage. In an instance of the Keychain Problem, a locksmith faces a sequence of choices, each of which involves selecting one key from a specified subset (a keychain) to attempt to open a lock. Given a Bayesian prior on the effectiveness of keys, the locksmith's goal is to maximize the expected number of rounds in which the lock is opened -- or equivalently, minimize the opportunity cost which is the expected number of rounds in which the chain has a correct key but our selected key is incorrect. We investigate Keychain Problems under three assumptions on the order in which keychains are tested by the locksmith: a fixed, known order; a random order sampled from a known distribution on a set of ``scenarios''; or an order selected by the locksmith themself. We present an exact algorithm for the simplest of these settings, and we present approximation algorithms and hardness results for the others. In the Probabilistic Scenarios setting, our approximation algorithm is based on a novel connection between combinatorial auctions and policy design for sequential decision-making problems. To illustrate the generality of this technique, we apply the same ideas to obtain Philosopher Inequalities for Online Bipartite Matching and some of its extensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06187v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ramiro N. Deo-Campo Vuong, Robert Kleinberg, Aditya Prasad, Eric Xiao, Haifeng Xu</dc:creator>
    </item>
    <item>
      <title>Benchmarking Robust Aggregation in Decentralized Gradient Marketplaces</title>
      <link>https://arxiv.org/abs/2509.05833</link>
      <description>arXiv:2509.05833v1 Announce Type: cross 
Abstract: The rise of distributed and privacy-preserving machine learning has sparked interest in decentralized gradient marketplaces, where participants trade intermediate artifacts like gradients. However, existing Federated Learning (FL) benchmarks overlook critical economic and systemic factors unique to such marketplaces-cost-effectiveness, fairness to sellers, and market stability-especially when a buyer relies on a private baseline dataset for evaluation.
  We introduce a comprehensive benchmark framework to holistically evaluate robust gradient aggregation methods within these buyer-baseline-reliant marketplaces. Our contributions include: (1) a simulation environment modeling marketplace dynamics with a variable buyer baseline and diverse seller distributions; (2) an evaluation methodology augmenting standard FL metrics with marketplace-centric dimensions such as Economic Efficiency, Fairness, and Selection Dynamics; (3) an in-depth empirical analysis of the existing Distributed Gradient Marketplace framework, MartFL, including the integration and comparative evaluation of adapted FLTrust and SkyMask as alternative aggregation strategies within it. This benchmark spans diverse datasets, local attacks, and Sybil attacks targeting the marketplace selection process; and (4) actionable insights into the trade-offs between model performance, robustness, cost, fairness, and stability.
  This benchmark equips the community with essential tools and empirical evidence to evaluate and design more robust, equitable, and economically viable decentralized gradient marketplaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05833v1</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeyu Song, Sainyam Galhotra, Shagufta Mehnaz</dc:creator>
    </item>
    <item>
      <title>Fixed-Point Theorems and the Ethics of Radical Transparency: A Logic-First Treatment</title>
      <link>https://arxiv.org/abs/2509.06055</link>
      <description>arXiv:2509.06055v1 Announce Type: cross 
Abstract: This paper establishes a formal framework, grounded in mathematical logic and order theory, to analyze the inherent limitations of radical transparency. We demonstrate that self-referential disclosure policies inevitably encounter fixed-point phenomena and diagonalization barriers, imposing fundamental trade-offs between openness and stability.
  Key results include: (i) an impossibility theorem showing no sufficiently expressive system can define a total, consistent transparency predicate for its own statements; (ii) a categorical fixed-point argument (Lawvere) for the inevitability of self-referential equilibria; (iii) order-theoretic design theorems (Knaster-Tarski) proving extremal fixed points exist and that the least fixed point minimizes a formal ethical risk functional; (iv) a construction for consistent partial transparency using Kripkean truth; (v) an analysis of self-endorsement hazards via L\"ob's Theorem; (vi) a recursion-theoretic exploitation theorem (Kleene) formalizing Goodhart's Law under full disclosure; (vii) an exploration of non-classical logics for circumventing classical paradoxes; and (viii) a modal $\mu$-calculus formulation for safety invariants under iterative disclosure.
  Our analysis provides a mathematical foundation for transparency design, proving that optimal policies are necessarily partial and must balance accountability against strategic gaming and paradox. We conclude with equilibrium analysis and lattice-theoretic optimality conditions, offering a principled calculus for ethical disclosure in complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06055v1</guid>
      <category>math.LO</category>
      <category>cs.GT</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Faruk Alpay, Hamdi Alakkad</dc:creator>
    </item>
    <item>
      <title>A Game-Theoretic Framework for Distributed Load Balancing: Static and Dynamic Game Models</title>
      <link>https://arxiv.org/abs/2501.15324</link>
      <description>arXiv:2501.15324v2 Announce Type: replace 
Abstract: Motivated by applications in job scheduling, queuing networks, and load balancing in cyber-physical systems, we develop and analyze a game-theoretic framework to balance the load among servers in static and dynamic settings. In these applications, jobs/tasks are held by selfish entities that do not want to coordinate with each other, yet the goal is to balance the load among servers in a distributed manner. First, we provide a static game formulation in which each player holds a job with a specific processing requirement and wants to schedule it fractionally among a set of heterogeneous servers to minimize its average processing time. We show that this static game is a potential game with a pure Nash equilibrium (NE). In particular, the best-response dynamics converge to such an NE after $n$ iterations, where $n$ is the number of players. Additionally, we bound the price of anarchy (PoA) of the static game in terms of game parameters. We then extend our results to a dynamic game setting, where jobs arrive and get processed, and players observe the load on the servers to decide how to schedule their jobs. In this setting, we show that if the players update their strategies using dynamic best-response, the system eventually becomes fully load-balanced and the players' strategies converge to the pure NE of the static game. In particular, we show that the convergence time scales only polynomially with respect to the game parameters. Finally, we provide numerical results to evaluate the performance of our proposed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15324v2</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fatemeh Fardno, S. Rasoul Etesami</dc:creator>
    </item>
    <item>
      <title>Playing Markov Games Without Observing Payoffs</title>
      <link>https://arxiv.org/abs/2509.00179</link>
      <description>arXiv:2509.00179v2 Announce Type: replace 
Abstract: Optimization under uncertainty is a fundamental problem in learning and decision-making, particularly in multi-agent systems. Previously, Feldman, Kalai, and Tennenholtz [2010] demonstrated the ability to efficiently compete in repeated symmetric two-player matrix games without observing payoffs, as long as the opponents actions are observed. In this paper, we introduce and formalize a new class of zero-sum symmetric Markov games, which extends the notion of symmetry from matrix games to the Markovian setting. We show that even without observing payoffs, a player who knows the transition dynamics and observes only the opponents sequence of actions can still compete against an adversary who may have complete knowledge of the game. We formalize three distinct notions of symmetry in this setting and show that, under these conditions, the learning problem can be reduced to an instance of online learning, enabling the player to asymptotically match the return of the opponent despite lacking payoff observations. Our algorithms apply to both matrix and Markov games, and run in polynomial time with respect to the size of the game and the number of episodes. Our work broadens the class of games in which robust learning is possible under severe informational disadvantage and deepens the connection between online learning and adversarial game theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00179v2</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Ablin, Alon Cohen</dc:creator>
    </item>
    <item>
      <title>The Average-Value Allocation Problem</title>
      <link>https://arxiv.org/abs/2407.10401</link>
      <description>arXiv:2407.10401v2 Announce Type: replace-cross 
Abstract: We initiate the study of centralized algorithms for welfare-maximizing allocation of goods to buyers subject to average-value constraints. We show that this problem is NP-hard to approximate beyond a factor of $\frac{e}{e-1}$, and provide a $\frac{4e}{e-1}$-approximate offline algorithm. For the online setting, we show that no non-trivial approximations are achievable under adversarial arrivals. Under i.i.d. arrivals, we present a polytime online algorithm that provides a constant approximation of the optimal (computationally-unbounded) online algorithm. In contrast, we show that no constant approximation of the ex-post optimum is achievable by an online algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10401v2</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kshipra Bhawalkar, Zhe Feng, Anupam Gupta, Aranyak Mehta, David Wajc, Di Wang</dc:creator>
    </item>
    <item>
      <title>Game Theory and Multi-Agent Reinforcement Learning for Zonal Ancillary Markets</title>
      <link>https://arxiv.org/abs/2505.03288</link>
      <description>arXiv:2505.03288v3 Announce Type: replace-cross 
Abstract: We characterize zonal ancillary market coupling relying on noncooperative game theory. To that purpose, we formulate the ancillary market as a multi-leader single follower bilevel problem, that we subsequently cast as a generalized Nash game with side constraints and nonconvex feasibility sets. We determine conditions for equilibrium existence and show that the game has a generalized potential game structure. To compute market equilibrium, we rely on two exact approaches: an integrated optimization approach and Gauss-Seidel best-response, that we compare against multi-agent deep reinforcement learning. On real data from Germany and Austria, simulations indicate that multi-agent deep reinforcement learning achieves the smallest convergence rate but requires pretraining, while best-response is the slowest. On the economics side, multi-agent deep reinforcement learning results in smaller market costs compared to the exact methods, but at the cost of higher variability in the profit allocation among stakeholders. Further, stronger coupling between zones tends to reduce costs for larger zones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03288v3</guid>
      <category>cs.MA</category>
      <category>cs.GT</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Morri, H\'el\`ene Le Cadre, Pierre Gruet, Luce Brotcorne</dc:creator>
    </item>
  </channel>
</rss>
