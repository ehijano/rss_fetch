<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Dec 2024 05:05:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Resource Allocation Influence on Application Performance in Sliced Testbeds</title>
      <link>https://arxiv.org/abs/2412.16716</link>
      <description>arXiv:2412.16716v1 Announce Type: new 
Abstract: Modern network architectures have shaped market segments, governments, and communities with intelligent and pervasive applications. Ongoing digital transformation through technologies such as softwarization, network slicing, and AI drives this evolution, along with research into Beyond 5G (B5G) and 6G architectures. Network slices require seamless management, observability, and intelligent-native resource allocation, considering user satisfaction, cost efficiency, security, and energy. Slicing orchestration architectures have been extensively studied to accommodate these requirements, particularly in resource allocation for network slices. This study explored the observability of resource allocation regarding network slice performance in two nationwide testbeds. We examined their allocation effects on slicing connectivity latency using a partial factorial experimental method with Central Processing Unit (CPU) and memory combinations. The results reveal different resource impacts across the testbeds, indicating a non-uniform influence on the CPU and memory within the same network slice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16716v1</guid>
      <category>cs.PF</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5753/wpeif.2024.2095</arxiv:DOI>
      <dc:creator>Rodrigo Moreira, Larissa F. Rodrigues Moreira, Tereza C. Carvalho, Fl\'avio de Oliveira Silva</dc:creator>
    </item>
    <item>
      <title>Rethinking Performance Analysis for Configurable Software Systems: A Case Study from a Fitness Landscape Perspective</title>
      <link>https://arxiv.org/abs/2412.16888</link>
      <description>arXiv:2412.16888v1 Announce Type: new 
Abstract: Modern software systems are often highly configurable to tailor varied requirements from diverse stakeholders. Understanding the mapping between configurations and the desired performance attributes plays a fundamental role in advancing the controllability and tuning of the underlying system, yet has long been a dark hole of knowledge due to its black-box nature. While there have been previous efforts in performance analysis for these systems, they analyze the configurations as isolated data points without considering their inherent spatial relationships. This renders them incapable of interrogating many important aspects of the configuration space like local optima. In this work, we advocate a novel perspective to rethink performance analysis -- modeling the configuration space as a structured ``landscape''. To support this proposition, we designed \our, an open-source, graph data mining empowered fitness landscape analysis (FLA) framework. By applying this framework to $86$M benchmarked configurations from $32$ running workloads of $3$ real-world systems, we arrived at $6$ main findings, which together constitute a holistic picture of the landscape topography, with thorough discussions about their implications on both configuration tuning and performance modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16888v1</guid>
      <category>cs.PF</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ISSTA 2025</arxiv:journal_reference>
      <dc:creator>Mingyu Huang, Peili Mao, Ke Li</dc:creator>
    </item>
    <item>
      <title>HashEvict: A Pre-Attention KV Cache Eviction Strategy using Locality-Sensitive Hashing</title>
      <link>https://arxiv.org/abs/2412.16187</link>
      <description>arXiv:2412.16187v1 Announce Type: cross 
Abstract: Transformer-based large language models (LLMs) use the key-value (KV) cache to significantly accelerate inference by storing the key and value embeddings of past tokens. However, this cache consumes significant GPU memory. In this work, we introduce LSH-E, an algorithm that uses locality-sensitive hashing (LSH) to compress the KV cache. LSH-E quickly locates tokens in the cache that are cosine dissimilar to the current query token. This is achieved by computing the Hamming distance between binarized Gaussian projections of the current token query and cached token keys, with a projection length much smaller than the embedding dimension. We maintain a lightweight binary structure in GPU memory to facilitate these calculations. Unlike existing compression strategies that compute attention to determine token retention, LSH-E makes these decisions pre-attention, thereby reducing computational costs. Additionally, LSH-E is dynamic - at every decoding step, the key and value of the current token replace the embeddings of a token expected to produce the lowest attention score. We demonstrate that LSH-E can compress the KV cache by 30%-70% while maintaining high performance across reasoning, multiple-choice, long-context retrieval and summarization tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16187v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DS</category>
      <category>cs.PF</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minghui Liu, Tahseen Rabbani, Tony O'Halloran, Ananth Sankaralingam, Mary-Anne Hartley, Brian Gravelle, Furong Huang, Cornelia Ferm\"uller, Yiannis Aloimonos</dc:creator>
    </item>
    <item>
      <title>Performance evaluation of mixed-precision Runge-Kutta methods for the solution of partial differential equations</title>
      <link>https://arxiv.org/abs/2412.16638</link>
      <description>arXiv:2412.16638v1 Announce Type: cross 
Abstract: This work focuses on the numerical study of a recently published class of Runge-Kutta methods designed for mixed-precision arithmetic. We employ the methods in solving partial differential equations on modern hardware. In particular we investigate what speedups are achievable by the use of mixed precision and the dependence of the methods algorithmic compatibility with the computational hardware. We use state-of-the-art software, utilizing the Ginkgo library, which is designed to incorporate mixed precision arithmetic, and perform numerical tests of 3D problems on both GPU and CPU architectures. We show that significant speedups can be achieved but that performance depends on solver parameters and performance of software kernels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16638v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivo Dravins, Marcel Koch, Victoria Griehl, Katharina Kormann</dc:creator>
    </item>
    <item>
      <title>Performance evaluation of accelerated real and complex multiple-precision sparse matrix-vector multiplication</title>
      <link>https://arxiv.org/abs/2412.17510</link>
      <description>arXiv:2412.17510v1 Announce Type: cross 
Abstract: Sparse matrices have recently played a significant and impactful role in scientific computing, including artificial intelligence-related fields. According to historical studies on sparse matrix--vector multiplication (SpMV), Krylov subspace methods are particularly sensitive to the effects of round-off errors when using floating-point arithmetic. By employing multiple-precision linear computation, convergence can be stabilized by reducing these round-off errors. In this paper, we present the performance of our accelerated SpMV using SIMD instructions, demonstrating its effectiveness through various examples, including Krylov subspace methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17510v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>cs.PF</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomonori Kouya</dc:creator>
    </item>
    <item>
      <title>On the Optimization of Singular Spectrum Analyses: A Pragmatic Approach</title>
      <link>https://arxiv.org/abs/2412.17793</link>
      <description>arXiv:2412.17793v1 Announce Type: cross 
Abstract: Singular Spectrum Analysis (SSA) occupies a prominent place in the real signal analysis toolkit alongside Fourier and Wavelet analysis. In addition to the two aforementioned analyses, SSA allows the separation of patterns directly from the data space into the data space, with data that need not be strictly stationary, continuous, or even normally sampled. In most cases, SSA relies on a combination of Hankel or Toeplitz matrices and Singular Value Decomposition (SVD). Like Fourier and Wavelet analysis, SSA has its limitations. The main bottleneck of the method can be summarized in three points. The first is the diagonalization of the Hankel/Toeplitz matrix, which can become a major problem from a memory and/or computational point of view if the time series to be analyzed is very long or heavily sampled. The second point concerns the size of the analysis window, typically denoted as 'L', which will affect the detection of patterns in the time series as well as the dimensions of the Hankel/Toeplitz matrix. Finally, the third point concerns pattern reconstruction: how to easily identify in the eigenvector/eigenvalue space which patterns should be grouped. We propose to address each of these issues by describing a hopefully effective approach that we have been developing for over 10 years and that has yielded good results in our research work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17793v1</guid>
      <category>physics.geo-ph</category>
      <category>cs.PF</category>
      <category>physics.bio-ph</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando Lopes, Dominique Gibert, Vincent Courtillot, Jean-Louis Le Mou\"el, Jean-Baptiste Boul\'e</dc:creator>
    </item>
    <item>
      <title>Computational Complexity-Constrained Spectral Efficiency Analysis for 6G Waveforms</title>
      <link>https://arxiv.org/abs/2407.05805</link>
      <description>arXiv:2407.05805v2 Announce Type: replace-cross 
Abstract: In this work, we present a tutorial on how to account for the computational time complexity overhead of signal processing in the spectral efficiency (SE) analysis of wireless waveforms. Our methodology is particularly relevant in scenarios where achieving higher SE entails a penalty in complexity, a common trade-off present in 6G candidate waveforms. We consider that SE derives from the data rate, which is impacted by time-dependent overheads. Thus, neglecting the computational complexity overhead in the SE analysis grants an unfair advantage to more computationally complex waveforms, as they require larger computational resources to meet a signal processing runtime below the symbol period. We demonstrate our points with two case studies. In the first, we refer to IEEE 802.11a-compliant baseband processors from the literature to show that their runtime significantly impacts the SE perceived by upper layers. In the second case study, we show that waveforms considered less efficient in terms of SE can outperform their more computationally expensive counterparts if provided with equivalent high-performance computational resources. Based on these cases, we believe our tutorial can address the comparative SE analysis of waveforms that operate under different computational resource constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05805v2</guid>
      <category>eess.SP</category>
      <category>cs.CC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saulo Queiroz, Jo\~ao P. Vilela, Benjamin Koon Kei Ng, Chan-Tong Lam, Edmundo Monteiro</dc:creator>
    </item>
    <item>
      <title>Matryoshka: Optimization of Dynamic Diverse Quantum Chemistry Systems via Elastic Parallelism Transformation</title>
      <link>https://arxiv.org/abs/2412.13203</link>
      <description>arXiv:2412.13203v2 Announce Type: replace-cross 
Abstract: AI infrastructures, predominantly GPUs, have delivered remarkable performance gains for deep learning. Conversely, scientific computing, exemplified by quantum chemistry systems, suffers from dynamic diversity, where computational patterns are more diverse and vary dynamically, posing a significant challenge to sponge acceleration off GPUs.
  In this paper, we propose Matryoshka, a novel elastically-parallel technique for the efficient execution of quantum chemistry system with dynamic diversity on GPU. Matryoshka capitalizes on Elastic Parallelism Transformation, a property prevalent in scientific systems yet underexplored for dynamic diversity, to elastically realign parallel patterns with GPU architecture. Structured around three transformation primitives (Permutation, Deconstruction, and Combination), Matryoshka encompasses three core components. The Block Constructor serves as the central orchestrator, which reformulates data structures accommodating dynamic inputs and constructs fine-grained GPU-efficient compute blocks. Within each compute block, the Graph Compiler operates offline, generating high-performance code with clear computational path through an automated compilation process. The Workload Allocator dynamically schedules workloads with varying operational intensities to threads online. It achieves highly efficient parallelism for compute-intensive operations and facilitates fusion with neighboring memory-intensive operations automatically. Extensive evaluation shows that Matryoshka effectively addresses dynamic diversity, yielding acceleration improvements of up to 13.86x (average 9.41x) over prevailing state-of-the-art approaches on 13 quantum chemistry systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13203v2</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 24 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuowei Wang, Kun Li, Donglin Bai, Fusong Ju, Leo Xia, Ting Cao, Ju Ren, Yaoxue Zhang, Mao Yang</dc:creator>
    </item>
  </channel>
</rss>
