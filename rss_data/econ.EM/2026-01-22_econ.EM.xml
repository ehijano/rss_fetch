<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Jan 2026 02:38:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On the falsification of instrumental variable models for heterogeneous treatment effects</title>
      <link>https://arxiv.org/abs/2601.14464</link>
      <description>arXiv:2601.14464v1 Announce Type: new 
Abstract: In this paper I derive a set of testable implications for econometric models defined by three assumptions: (i) the existence of strictly exogenous discrete instruments, (ii) restrictions on how the instruments affect adoption of a finite number of treatment types (such as monotonicity), and (iii) the assumption that the instruments only affect outcomes through their effect on treatment adoption (i.e. an exclusion restriction). The testable implications aggregate (via integration) an otherwise potentially infinite set of inequalities that must hold for every measurable subset of the outcome's support. For binary instruments the testable implications are sharp. Furthermore, I propose an implementation that links restrictions on latent response types to a generalization of first-order stochastic dominance and random utility models, allowing to distinguish violations of the exclusion restriction from violations of monotonicity-type assumptions. The testable implications extend naturally to the many instruments case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14464v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ricardo E. Miranda</dc:creator>
    </item>
    <item>
      <title>The Maintenance and Necessity of Universal Rules: Scale, Hierarchy, the Cost of Justice, and Civilizational Development</title>
      <link>https://arxiv.org/abs/2601.14325</link>
      <description>arXiv:2601.14325v2 Announce Type: cross 
Abstract: Building upon previous research, this paper further explores the topological foundations for maintaining universal rules within ultra-large-scale societies. It finds that in small-scale societies, absolute egalitarianism and the rule of law can be compatible through peer monitoring within a fully connected network. However, in ultra-large-scale societies, to maintain high-dimensional rules capable of protecting innovation and property rights, a complex hierarchical structure including "high-fragility" nodes must be constructed. Through quantitative analysis of power structures, this paper proves that a flattened, two-tier structure inevitably leads to the degradation of the rule of law. Only a social topology with sufficient hierarchical depth can escape the deathly trap of the Leviathan while expanding in scale, thereby sustaining the dynamic evolution of civilization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14325v2</guid>
      <category>physics.soc-ph</category>
      <category>econ.EM</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Li Tuobang</dc:creator>
    </item>
    <item>
      <title>Implementing Substance Over Form: A Novel Metric for Taxing E-commerce to Address Deterritorialization</title>
      <link>https://arxiv.org/abs/2601.14616</link>
      <description>arXiv:2601.14616v1 Announce Type: cross 
Abstract: Against the backdrop of e-commerce restructuring consumption patterns, last-mile delivery stations have substantially fulfilled the function of community retail distribution. However, the current tax system only levies a low labor service tax on delivery fees, resulting in a tax contribution from the massive circulating goods value that is significantly lower than that of retail supermarkets of equivalent scale. This disparity not only triggers local tax base erosion but also fosters unfair competition. Based on the "substance over form" principle, this paper proposes a tax rate calculation method using "delivery fee plus insurance premium" as the base, corrected through "goods value conversion." This method aims to align the substantive tax burden of e-commerce with that of community retail at the terminal stage, effectively internalizing the high negative externalities of delivery stations through fiscal instruments, addressing E-commerce Deterritorialization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14616v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Li Tuobang</dc:creator>
    </item>
    <item>
      <title>Semidiscrete optimal transport with unknown costs</title>
      <link>https://arxiv.org/abs/2310.00786</link>
      <description>arXiv:2310.00786v4 Announce Type: replace 
Abstract: Semidiscrete optimal transport is a challenging generalization of the classical transportation problem in linear programming. The goal is to design a joint distribution for two random variables (one continuous, one discrete) with fixed marginals, in a way that minimizes expected cost. We formulate a novel variant of this problem in which the cost functions are unknown, but can be learned through noisy observations; however, only one function can be sampled at a time. We develop a semi-myopic algorithm that couples online learning with stochastic approximation, and prove that it achieves optimal convergence rates, despite the non-smoothness of the stochastic gradient and the lack of strong concavity in the objective function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00786v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinchu Zhu, Ilya O. Ryzhov</dc:creator>
    </item>
    <item>
      <title>Identifying Peer Effects in Networks with Unobserved Effort and Isolated Students</title>
      <link>https://arxiv.org/abs/2405.06850</link>
      <description>arXiv:2405.06850v2 Announce Type: replace 
Abstract: Peer influence on effort devoted to some activity is often studied when effort is unobserved, and the researcher instead observes an outcome that combines effort with other shocks. For instance, in education, achievement measures such as GPA reflect both effort and idiosyncratic GPA shocks. We propose an alternative approach that circumvents this approximation. Our framework distinguishes unobserved shocks to GPA that do not affect effort from preference shocks that do affect effort levels. We show that peer effects estimates obtained using our approach can differ significantly from classical estimates (where effort is approximated) if the network includes isolated students. Applying our approach to data on high school students in the United States, we find that peer effect estimates relying on GPA as a proxy for effort are 40% lower than those obtained using our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06850v2</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aristide Houndetoungan, Cristelle Kouame, Michael Vlassopoulos</dc:creator>
    </item>
    <item>
      <title>Possibilistic Instrumental Variable Regression</title>
      <link>https://arxiv.org/abs/2511.16029</link>
      <description>arXiv:2511.16029v2 Announce Type: replace-cross 
Abstract: Instrumental variable regression is a common approach for causal inference in the presence of unobserved confounding. However, identifying valid instruments is often difficult in practice. In this paper, we propose a novel method based on possibility theory that performs posterior inference on the treatment effect, conditional on a user-specified set of potential violations of the exogeneity assumption. Our method can provide informative results even when only a single, potentially invalid, instrument is available, offering a natural and principled framework for sensitivity analysis. Simulation experiments and a real-data application indicate strong performance of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16029v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gregor Steiner, Jeremie Houssineau, Mark F. J. Steel</dc:creator>
    </item>
  </channel>
</rss>
