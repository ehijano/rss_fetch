<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Feb 2025 04:07:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Infer-EDGE: Dynamic DNN Inference Optimization in 'Just-in-time' Edge-AI Implementations</title>
      <link>https://arxiv.org/abs/2501.18842</link>
      <description>arXiv:2501.18842v1 Announce Type: new 
Abstract: Balancing mutually diverging performance metrics, such as end-to-end latency, accuracy, and device energy consumption, is a challenging undertaking for deep neural network (DNN) inference in Just-in-Time edge environments that are inherently resource-constrained and loosely coupled. In this paper, we design and develop the Infer-EDGE framework that seeks to strike such a balance for latency-sensitive video processing applications. First, using comprehensive benchmarking experiments, we develop intuitions about the trade-off characteristics, which are then used by the framework to develop an Advantage Actor-Critic (A2C) Reinforcement Learning (RL) approach that can choose optimal run-time DNN inference parameters aligning the performance metrics based on the application requirements. Using real-world DNNs and a hardware testbed, we evaluate the benefits of the Infer-EDGE framework in terms of device energy savings, inference accuracy improvement, and end-to-end inference latency reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18842v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Motahare Mounesan, Xiaojie Zhang, Saptarshi Debroy</dc:creator>
    </item>
    <item>
      <title>CPU vs. GPU for Community Detection: Performance Insights from GVE-Louvain and $\nu$-Louvain</title>
      <link>https://arxiv.org/abs/2501.19004</link>
      <description>arXiv:2501.19004v1 Announce Type: new 
Abstract: Community detection involves identifying natural divisions in networks, a crucial task for many large-scale applications. This report presents GVE-Louvain, one of the most efficient multicore implementations of the Louvain algorithm, a high-quality method for community detection. Running on a dual 16-core Intel Xeon Gold 6226R server, GVE-Louvain outperforms Vite, Grappolo, NetworKit Louvain, and cuGraph Louvain (on an NVIDIA A100 GPU) by factors of 50x, 22x, 20x, and 5.8x, respectively, achieving a processing rate of 560M edges per second on a 3.8B-edge graph. Additionally, it scales efficiently, improving performance by 1.6x for every thread doubling. The paper also presents $\nu$-Louvain, a GPU-based implementation. When evaluated on an NVIDIA A100 GPU, $\nu$-Louvain performs only on par with GVE-Louvain, largely due to reduced workload and parallelism in later algorithmic passes. These results suggest that CPUs, with their flexibility in handling irregular workloads, may be better suited for community detection tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19004v1</guid>
      <category>cs.DC</category>
      <category>cs.SI</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Subhajit Sahu</dc:creator>
    </item>
    <item>
      <title>FL-APU: A Software Architecture to Ease Practical Implementation of Cross-Silo Federated Learning</title>
      <link>https://arxiv.org/abs/2501.19091</link>
      <description>arXiv:2501.19091v1 Announce Type: new 
Abstract: Federated Learning (FL) is an upcoming technology that is increasingly applied in real-world applications. Early applications focused on cross-device scenarios, where many participants with limited resources train machine learning (ML) models together, e.g., in the case of Google's GBoard. Contrarily, cross-silo scenarios have only few participants but with many resources, e.g., in the healthcare domain. Despite such early efforts, FL is still rarely used in practice and best practices are, hence, missing. For new applications, in our case inter-organizational cross-silo applications, overcoming this lack of role models is a significant challenge.
  In order to ease the use of FL in real-world cross-silo applications, we here propose a scenario-based architecture for the practical use of FL in the context of multiple companies collaborating to improve the quality of their ML models. The architecture emphasizes the collaboration between the participants and the FL server and extends basic interactions with domain-specific features. First, it combines governance with authentication, creating an environment where only trusted participants can join. Second, it offers traceability of governance decisions and tracking of training processes, which are also crucial in a production environment. Beyond presenting the architectural design, we analyze requirements for the real-world use of FL and evaluate the architecture with a scenario-based analysis method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19091v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/FLTA63145.2024.10839980</arxiv:DOI>
      <arxiv:journal_reference>2024 2nd International Conference on Federated Learning Technologies and Applications (FLTA), Valencia, Spain, 2024, pp. 63-70</arxiv:journal_reference>
      <dc:creator>F. Stricker, J. A. Peregrina, D. Bermbach, C. Zirpins</dc:creator>
    </item>
    <item>
      <title>STaleX: A Spatiotemporal-Aware Adaptive Auto-scaling Framework for Microservices</title>
      <link>https://arxiv.org/abs/2501.18734</link>
      <description>arXiv:2501.18734v1 Announce Type: cross 
Abstract: While cloud environments and auto-scaling solutions have been widely applied to traditional monolithic applications, they face significant limitations when it comes to microservices-based architectures. Microservices introduce additional challenges due to their dynamic and spatiotemporal characteristics, which require more efficient and specialized auto-scaling strategies. Centralized auto-scaling for the entire microservice application is insufficient, as each service within a chain has distinct specifications and performance requirements. Therefore, each service requires its own dedicated auto-scaler to address its unique scaling needs effectively, while also considering the dependencies with other services in the chain and the overall application. This paper presents a combination of control theory, machine learning, and heuristics to address these challenges. We propose an adaptive auto-scaling framework, STaleX, for microservices that integrates spatiotemporal features, enabling real-time resource adjustments to minimize SLO violations. STaleX employs a set of weighted Proportional-Integral-Derivative (PID) controllers for each service, where weights are dynamically adjusted based on a supervisory unit that integrates spatiotemporal features. This supervisory unit continuously monitors and adjusts both the weights and the resources allocated to each service. Our framework accounts for spatial features, including service specifications and dependencies among services, as well as temporal variations in workload, ensuring that resource allocation is continuously optimized. Through experiments on a microservice-based demo application deployed on a Kubernetes cluster, we demonstrate the effectiveness of our framework in improving performance and reducing costs compared to traditional scaling methods like Kubernetes Horizontal Pod Autoscaler (HPA) with a 26.9% reduction in resource usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18734v1</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Majid Dashtbani, Ladan Tahvildari</dc:creator>
    </item>
    <item>
      <title>Continuous-Time Analysis of Federated Averaging</title>
      <link>https://arxiv.org/abs/2501.18870</link>
      <description>arXiv:2501.18870v1 Announce Type: cross 
Abstract: Federated averaging (FedAvg) is a popular algorithm for horizontal federated learning (FL), where samples are gathered across different clients and are not shared with each other or a central server. Extensive convergence analysis of FedAvg exists for the discrete iteration setting, guaranteeing convergence for a range of loss functions and varying levels of data heterogeneity. We extend this analysis to the continuous-time setting where the global weights evolve according to a multivariate stochastic differential equation (SDE), which is the first time FedAvg has been studied from the continuous-time perspective. We use techniques from stochastic processes to establish convergence guarantees under different loss functions, some of which are more general than existing work in the discrete setting. We also provide conditions for which FedAvg updates to the server weights can be approximated as normal random variables. Finally, we use the continuous-time formulation to reveal generalization properties of FedAvg.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18870v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom Overman, Diego Klabjan</dc:creator>
    </item>
    <item>
      <title>A Bias-Correction Decentralized Stochastic Gradient Algorithm with Momentum Acceleration</title>
      <link>https://arxiv.org/abs/2501.19082</link>
      <description>arXiv:2501.19082v1 Announce Type: cross 
Abstract: Distributed stochastic optimization algorithms can handle large-scale data simultaneously and accelerate model training. However, the sparsity of distributed networks and the heterogeneity of data limit these advantages. This paper proposes a momentum-accelerated distributed stochastic gradient algorithm, referred to as Exact-Diffusion with Momentum (EDM), which can correct the bias caused by data heterogeneity and introduces the momentum method commonly used in deep learning to accelerate the convergence of the algorithm. We theoretically demonstrate that this algorithm converges to the neighborhood of the optimum sub-linearly irrelevant to data heterogeneity when applied to non-convex objective functions and linearly under the Polyak-{\L}ojasiewicz condition (a weaker assumption than $\mu$-strongly convexity). Finally, we evaluate the performance of the proposed algorithm by simulation, comparing it with a range of existing decentralized optimization algorithms to demonstrate its effectiveness in addressing data heterogeneity and network sparsity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19082v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchen Hu, Xi Chen, Weidong Liu, Xiaojun Mao</dc:creator>
    </item>
    <item>
      <title>S-VOTE: Similarity-based Voting for Client Selection in Decentralized Federated Learning</title>
      <link>https://arxiv.org/abs/2501.19279</link>
      <description>arXiv:2501.19279v1 Announce Type: cross 
Abstract: Decentralized Federated Learning (DFL) enables collaborative, privacy-preserving model training without relying on a central server. This decentralized approach reduces bottlenecks and eliminates single points of failure, enhancing scalability and resilience. However, DFL also introduces challenges such as suboptimal models with non-IID data distributions, increased communication overhead, and resource usage. Thus, this work proposes S-VOTE, a voting-based client selection mechanism that optimizes resource usage and enhances model performance in federations with non-IID data conditions. S-VOTE considers an adaptive strategy for spontaneous local training that addresses participation imbalance, allowing underutilized clients to contribute without significantly increasing resource costs. Extensive experiments on benchmark datasets demonstrate the S-VOTE effectiveness. More in detail, it achieves lower communication costs by up to 21%, 4-6% faster convergence, and improves local performance by 9-17% compared to baseline methods in some configurations, all while achieving a 14-24% energy consumption reduction. These results highlight the potential of S-VOTE to address DFL challenges in heterogeneous environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19279v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro Miguel S\'anchez S\'anchez, Enrique Tom\'as Mart\'inez Beltr\'an, Chao Feng, G\'er\^ome Bovet, Gregorio Mart\'inez P\'erez, Alberto Huertas Celdr\'an</dc:creator>
    </item>
    <item>
      <title>Integrating Blockchain technology within an Information Ecosystem</title>
      <link>https://arxiv.org/abs/2402.13191</link>
      <description>arXiv:2402.13191v2 Announce Type: replace 
Abstract: Context: Blockchain-based Information Ecosystems (BBIEs) are a type of information ecosystem in which blockchain technology is used to provide a trust mechanism among parties and to manage shared business logic, breaking the traditional scheme of Information Ecosystems dominated by a leading company and leveraging the decentralization of data management, information flow, and business logic. Objective: In this paper, we propose architecture and technical aspects concerning the creation of a BBIE, underlining the advantages supplied and the logic decomposition among the business and storage components. Method: The requirements are derived from the current needs of the collaborative business and the data collected by surveying practitioners. To get these needs we followed the Grounded Theory research approach. We validate our architectural schema against a case study dealing with the management of a wine supply chain, also involving different companies and supervision authorities. Results: The proposed solution integrates blockchain-based applications with the existing information system as a module of the ecosystem, leveraging on the low costs, scalability, and high-level security because of the restricted access to the network. Conclusion: We must go a long way in deepening and refining the possibilities offered by technology in supporting innovative multi-organizational business models. BBIEs can contribute substantially to paving the way in such a direction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13191v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.bcra.2024.100225</arxiv:DOI>
      <arxiv:journal_reference>Blockchain: Research and Applications 5 (2024) 100225</arxiv:journal_reference>
      <dc:creator>Francesco Salzano, Lodovica Marchesi, Remo Pareschi, Roberto Tonelli</dc:creator>
    </item>
    <item>
      <title>Personalized Federated Learning on Data with Dynamic Heterogeneity under Limited Storage</title>
      <link>https://arxiv.org/abs/2410.01502</link>
      <description>arXiv:2410.01502v2 Announce Type: replace 
Abstract: Recently, a large number of data sources opened up by informatization intensify the data heterogeneity, the faster speed of data generation and the gradual implementation of data regulations limit the storage time of data. In personalized Federated Learning (pFL), clients train customized models to meet their personal objectives. However, due to the time-varying local data heterogeneity and the inaccessibility of previous data, existing pFL methods not only fail to solve the catastrophic forgetting of local models, but also difficult to estimate the degree of collaboration between clients. To address this issue, our core idea is a low consumption and high-quality generative replay architecture. Specifically, we decouple the generator by category to reduce the generation error of each category while mitigating catastrophic forgetting, use local model to improving the quality of generated data and reducing the update frequency of generator, and propose a local data reconstruction scheme to reduce data generation while adjusting the proportion of data categories. Based on above, we propose our pFL framework, pFedGRP, to achieve personalized aggregation and local knowledge transfer. Comprehensive experiments on five datasets with multiple settings show the superiority of pFedGRP over eight baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01502v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sixing Tan, Xianmin Liu</dc:creator>
    </item>
    <item>
      <title>Not eXactly Byzantine: Efficient and Resilient TEE-Based State Machine Replication</title>
      <link>https://arxiv.org/abs/2501.11051</link>
      <description>arXiv:2501.11051v2 Announce Type: replace 
Abstract: We propose, implement, and evaluate NxBFT, a practical State Machine Replication protocol that tolerates minority corruptions by using Trusted Execution Environments (TEEs). NxBFT focuses on a "Not eXactly Byzantine" operating model as a middle ground between crash and Byzantine fault tolerance. NxBFT is designed as an asynchronous protocol except for liveness of setup and recovery. As a leaderless protocol based on TEE-Rider, it provides build-in load balancing in the number of replicas, which is in contrast to leader-based and leader-rotating approaches. With quadratic communication complexity, a TEE-based common coin as source of randomness, a crash recovery procedure, solutions for request deduplication, and progress in low-load scenarios, NxBFT achieves a throughput of 400 kOp/s at an average end-to-end-latency of 1 s for 40 replicas and shows competitive performance under faults. We provide a comparison with a leader-based (MinBFT) and a leader-rotating protocol (Damysus) and analyze benefits and challenges that result from the combination of asynchrony and TEEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11051v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Leinweber, Hannes Hartenstein</dc:creator>
    </item>
    <item>
      <title>The Smart Buildings Control Suite: A Diverse Open Source Benchmark to Evaluate and Scale HVAC Control Policies for Sustainability</title>
      <link>https://arxiv.org/abs/2410.03756</link>
      <description>arXiv:2410.03756v2 Announce Type: replace-cross 
Abstract: Commercial buildings account for 17% of U.S. carbon emissions, with roughly half of that from Heating, Ventilation, and Air Conditioning (HVAC). HVAC devices form a complex thermodynamic system, and while Model Predictive Control and Reinforcement Learning have been used to optimize control policies, scaling to thousands of buildings remains a significant unsolved challenge. Most current algorithms are over-optimized for specific buildings and rely on proprietary data or hard-to-configure simulations. We present the Smart Buildings Control Suite, the first open source interactive HVAC control benchmark with a focus on solutions that scale. It consists of 3 components: real-world telemetric data extracted from 11 buildings over 6 years, a lightweight data-driven simulator for each building, and a modular Physically Informed Neural Network (PINN) building model as a simulator alternative. The buildings span a variety of climates, management systems, and sizes, and both the simulator and PINN easily scale to new buildings, ensuring solutions using this benchmark are robust to these factors and only reliant on fully scalable building models. This represents a major step towards scaling HVAC optimization from the lab to buildings everywhere. To facilitate use, our benchmark is compatible with the Gym standard, and our data is part of TensorFlow Datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03756v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Judah Goldfeder, Victoria Dean, Zixin Jiang, Xuezheng Wang, Bing dong, Hod Lipson, John Sipple</dc:creator>
    </item>
    <item>
      <title>Improving Parallel Program Performance with LLM Optimizers via Agent-System Interface</title>
      <link>https://arxiv.org/abs/2410.15625</link>
      <description>arXiv:2410.15625v2 Announce Type: replace-cross 
Abstract: Modern scientific discovery increasingly relies on high-performance computing for complex modeling and simulation. A key challenge in improving parallel program performance is efficiently mapping tasks to processors and data to memory, a process dictated by intricate, low-level system code known as mappers. Developing high-performance mappers demands days of manual tuning, posing a significant barrier for domain scientists without systems expertise. We introduce a framework that automates mapper development with generative optimization, leveraging richer feedback beyond scalar performance metrics. Our approach features the Agent-System Interface, which includes a Domain-Specific Language (DSL) to abstract away low-level complexity of system code and define a structured search space, as well as AutoGuide, a mechanism that interprets raw execution output into actionable feedback. Unlike traditional reinforcement learning methods such as OpenTuner, which rely solely on scalar feedback, our method finds superior mappers in far fewer iterations. With just 10 iterations, it outperforms OpenTuner even after 1000 iterations, achieving 3.8X faster performance. Our approach finds mappers that surpass expert-written mappers by up to 1.34X speedup across nine benchmarks while reducing tuning time from days to minutes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15625v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anjiang Wei, Allen Nie, Thiago S. F. X. Teixeira, Rohan Yadav, Wonchan Lee, Ke Wang, Alex Aiken</dc:creator>
    </item>
    <item>
      <title>Uncoded Download in Lagrange-Coded Elastic Computing with Straggler Tolerance</title>
      <link>https://arxiv.org/abs/2501.16298</link>
      <description>arXiv:2501.16298v2 Announce Type: replace-cross 
Abstract: Coded elastic computing, introduced by Yang et al. in 2018, is a technique designed to mitigate the impact of elasticity in cloud computing systems, where machines can be preempted or be added during computing rounds. This approach utilizes maximum distance separable (MDS) coding for both storage and download in matrix-matrix multiplications. The proposed scheme is unable to tolerate stragglers and has high encoding complexity and upload cost. In 2023, we addressed these limitations by employing uncoded storage and Lagrange-coded download. However, it results in a large storage size. To address the challenges of storage size and upload cost, in this paper, we focus on Lagrange-coded elastic computing based on uncoded download. We propose a new class of elastic computing schemes, using Lagrange-coded storage with uncoded download (LCSUD). Our proposed schemes address both elasticity and straggler challenges while achieving lower storage size, reduced encoding complexity, and upload cost compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16298v2</guid>
      <category>cs.IT</category>
      <category>cs.DC</category>
      <category>math.IT</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xi Zhong, Samuel Lu, Joerg Kliewer, Mingyue Ji</dc:creator>
    </item>
  </channel>
</rss>
