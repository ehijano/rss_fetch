<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Jul 2024 01:38:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Spatially-clustered spatial autoregressive models with application to agricultural market concentration in Europe</title>
      <link>https://arxiv.org/abs/2407.15874</link>
      <description>arXiv:2407.15874v1 Announce Type: new 
Abstract: In this paper, we present an extension of the spatially-clustered linear regression models, namely, the spatially-clustered spatial autoregression (SCSAR) model, to deal with spatial heterogeneity issues in clustering procedures. In particular, we extend classical spatial econometrics models, such as the spatial autoregressive model, the spatial error model, and the spatially-lagged model, by allowing the regression coefficients to be spatially varying according to a cluster-wise structure. Cluster memberships and regression coefficients are jointly estimated through a penalized maximum likelihood algorithm which encourages neighboring units to belong to the same spatial cluster with shared regression coefficients. Motivated by the increase of observed values of the Gini index for the agricultural production in Europe between 2010 and 2020, the proposed methodology is employed to assess the presence of local spatial spillovers on the market concentration index for the European regions in the last decade. Empirical findings support the hypothesis of fragmentation of the European agricultural market, as the regions can be well represented by a clustering structure partitioning the continent into three-groups, roughly approximated by a division among Western, North Central and Southeastern regions. Also, we detect heterogeneous local effects induced by the selected explanatory variables on the regional market concentration. In particular, we find that variables associated with social, territorial and economic relevance of the agricultural sector seem to act differently throughout the spatial dimension, across the clusters and with respect to the pooled model, and temporal dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15874v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roy Cerqueti (Department of Social,Economic Sciences, Sapienza University of Rome, Italy,GRANEM, University of Angers, France), Paolo Maranzano (Department Economics, Management,Statistics), Raffaele Mattera (Department of Social,Economic Sciences, Sapienza University of Rome, Italy)</dc:creator>
    </item>
    <item>
      <title>Generalized functional dynamic principal component analysis</title>
      <link>https://arxiv.org/abs/2407.16024</link>
      <description>arXiv:2407.16024v1 Announce Type: new 
Abstract: In this paper, we explore dimension reduction for time series of functional data within both stationary and non-stationary frameworks. We introduce a functional framework of generalized dynamic principal component analysis (GDPCA). The concept of GDPCA aims for better adaptation to possible nonstationary features of the series. We define the functional generalized dynamic principal component (GDPC) as static factor time series in a functional dynamic factor model and obtain the multivariate GDPC from a truncation of the functional dynamic factor model. GDFPCA uses a minimum squared error criterion to evaluate the reconstruction of the original functional time series. The computation of GDPC involves a two-step estimation of the coefficient vector of the loading curves in a basis expansion. We provide a proof of the consistency of the reconstruction of the original functional time series with GDPC converging in mean square to the original functional time series. Monte Carlo simulation studies indicate that the proposed GDFPCA is comparable to dynamic functional principal component analysis (DFPCA) when the data generating process is stationary, and outperforms DFPCA and FPCA when the data generating process is non-stationary. The results of applications to real data reaffirm the findings in simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16024v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tzung Hsuen Khoo, Issa-Mbenard Dabo, Dharini Pathmanathan, Sophie Dabo-Niang</dc:creator>
    </item>
    <item>
      <title>Robust and consistent model evaluation criteria in high-dimensional regression</title>
      <link>https://arxiv.org/abs/2407.16116</link>
      <description>arXiv:2407.16116v2 Announce Type: new 
Abstract: In the last two decades, sparse regularization methods such as the LASSO have been applied in various fields. Most of the regularization methods have one or more regularization parameters, and to select the value of the regularization parameter is essentially equal to select a model, thus we need to determine the regularization parameter adequately. Regarding the determination of the regularization parameter in the linear regression model, we often apply the information criteria like the AIC and BIC, however, it has been pointed out that these criteria are sensitive to outliers and tend not to perform well in high-dimensional settings. Outliers generally have a negative influence on not only estimation but also model selection, consequently, it is important to employ a selection method that is robust against outliers. In addition, when the number of explanatory variables is quite large, most conventional criteria are prone to select unnecessary explanatory variables. In this paper, we propose model evaluation criteria via the statistical divergence with excellence in robustness in both of parametric estimation and model selection. Furthermore, our proposed criteria simultaneously achieve the selection consistency with the robustness even in high-dimensional settings. We also report the results of some numerical examples to verify that the proposed criteria perform robust and consistent variable selection compared with the conventional selection methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16116v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sumito Kurata, Kei Hirose</dc:creator>
    </item>
    <item>
      <title>Optimal experimental design: Formulations and computations</title>
      <link>https://arxiv.org/abs/2407.16212</link>
      <description>arXiv:2407.16212v1 Announce Type: new 
Abstract: Questions of `how best to acquire data' are essential to modeling and prediction in the natural and social sciences, engineering applications, and beyond. Optimal experimental design (OED) formalizes these questions and creates computational methods to answer them. This article presents a systematic survey of modern OED, from its foundations in classical design theory to current research involving OED for complex models. We begin by reviewing criteria used to formulate an OED problem and thus to encode the goal of performing an experiment. We emphasize the flexibility of the Bayesian and decision-theoretic approach, which encompasses information-based criteria that are well-suited to nonlinear and non-Gaussian statistical models. We then discuss methods for estimating or bounding the values of these design criteria; this endeavor can be quite challenging due to strong nonlinearities, high parameter dimension, large per-sample costs, or settings where the model is implicit. A complementary set of computational issues involves optimization methods used to find a design; we discuss such methods in the discrete (combinatorial) setting of observation selection and in settings where an exact design can be continuously parameterized. Finally we present emerging methods for sequential OED that build non-myopic design policies, rather than explicit designs; these methods naturally adapt to the outcomes of past experiments in proposing new experiments, while seeking coordination among all experiments to be performed. Throughout, we highlight important open questions and challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16212v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xun Huan, Jayanth Jagalur, Youssef Marzouk</dc:creator>
    </item>
    <item>
      <title>Sparse outlier-robust PCA for multi-source data</title>
      <link>https://arxiv.org/abs/2407.16299</link>
      <description>arXiv:2407.16299v1 Announce Type: new 
Abstract: Sparse and outlier-robust Principal Component Analysis (PCA) has been a very active field of research recently. Yet, most existing methods apply PCA to a single dataset whereas multi-source data-i.e. multiple related datasets requiring joint analysis-arise across many scientific areas. We introduce a novel PCA methodology that simultaneously (i) selects important features, (ii) allows for the detection of global sparse patterns across multiple data sources as well as local source-specific patterns, and (iii) is resistant to outliers. To this end, we develop a regularization problem with a penalty that accommodates global-local structured sparsity patterns, and where the ssMRCD estimator is used as plug-in to permit joint outlier-robust analysis across multiple data sources. We provide an efficient implementation of our proposal via the Alternating Direction Method of Multiplier and illustrate its practical advantages in simulation and in applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16299v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patricia Puchhammer, Ines Wilms, Peter Filzmoser</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian Model Averaging for Linear Regression Models With Heavy-Tailed Errors</title>
      <link>https://arxiv.org/abs/2407.16366</link>
      <description>arXiv:2407.16366v1 Announce Type: new 
Abstract: In this article, our goal is to develop a method for Bayesian model averaging in linear regression models to accommodate heavier tailed error distributions than the normal distribution. Motivated by the use of the Huber loss function in presence of outliers, Park and Casella (2008) proposed the concept of the Bayesian Huberized lasso, which has been recently developed and implemented by Kawakami and Hashimoto (2023), with hyperbolic errors. Because the Huberized lasso cannot enforce regression coefficients to be exactly zero, we propose a fully Bayesian variable selection approach with spike and slab priors, that can address sparsity more effectively. Furthermore, while the hyperbolic distribution has heavier tails than a normal distribution, its tails are less heavy in comparison to a Cauchy distribution.Thus, we propose a regression model, with an error distribution that encompasses both hyperbolic and Student-t distributions. Our model aims to capture the benefit of using Huber loss, but it can also adapt to heavier tails, and unknown levels of sparsity, as necessitated by the data. We develop an efficient Gibbs sampler with Metropolis Hastings steps for posterior computation. Through simulation studies, and analyses of the benchmark Boston housing dataset and NBA player salaries in the 2022-2023 season, we show that our method is competitive with various state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16366v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shamriddha De, Joyee Ghosh</dc:creator>
    </item>
    <item>
      <title>A unified framework for multivariate two-sample and k-sample kernel-based quadratic distance goodness-of-fit tests</title>
      <link>https://arxiv.org/abs/2407.16374</link>
      <description>arXiv:2407.16374v1 Announce Type: new 
Abstract: In the statistical literature, as well as in artificial intelligence and machine learning, measures of discrepancy between two probability distributions are largely used to develop measures of goodness-of-fit. We concentrate on quadratic distances, which depend on a non-negative definite kernel. We propose a unified framework for the study of two-sample and k-sample goodness of fit tests based on the concept of matrix distance. We provide a succinct review of the goodness of fit literature related to the use of distance measures, and specifically to quadratic distances. We show that the quadratic distance kernel-based two-sample test has the same functional form with the maximum mean discrepancy test. We develop tests for the $k$-sample scenario, where the two-sample problem is a special case. We derive their asymptotic distribution under the null hypothesis and discuss computational aspects of the test procedures. We assess their performance, in terms of level and power, via extensive simulations and a real data example. The proposed framework is implemented in the QuadratiK package, available in both R and Python environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16374v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Marianthi Markatou, Giovanni Saraceno</dc:creator>
    </item>
    <item>
      <title>A Kernel-Based Conditional Two-Sample Test Using Nearest Neighbors (with Applications to Calibration, Regression Curves, and Simulation-Based Inference)</title>
      <link>https://arxiv.org/abs/2407.16550</link>
      <description>arXiv:2407.16550v1 Announce Type: new 
Abstract: In this paper we introduce a kernel-based measure for detecting differences between two conditional distributions. Using the `kernel trick' and nearest-neighbor graphs, we propose a consistent estimate of this measure which can be computed in nearly linear time (for a fixed number of nearest neighbors). Moreover, when the two conditional distributions are the same, the estimate has a Gaussian limit and its asymptotic variance has a simple form that can be easily estimated from the data. The resulting test attains precise asymptotic level and is universally consistent for detecting differences between two conditional distributions. We also provide a resampling based test using our estimate that applies to the conditional goodness-of-fit problem, which controls Type I error in finite samples and is asymptotically consistent with only a finite number of resamples. A method to de-randomize the resampling test is also presented. The proposed methods can be readily applied to a broad range of problems, ranging from classical nonparametric statistics to modern machine learning. Specifically, we explore three applications: testing model calibration, regression curve evaluation, and validation of emulator models in simulation-based inference. We illustrate the superior performance of our method for these tasks, both in simulations as well as on real data. In particular, we apply our method to (1) assess the calibration of neural network models trained on the CIFAR-10 dataset, (2) compare regression functions for wind power generation across two different turbines, and (3) validate emulator models on benchmark examples with intractable posteriors and for generating synthetic `redshift' associated with galaxy images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16550v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Anirban Chatterjee, Ziang Niu, Bhaswar B. Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Decoding Digital Influence: The Role of Social Media Behavior in Scientific Stratification Through Logistic Attribution Method</title>
      <link>https://arxiv.org/abs/2407.15854</link>
      <description>arXiv:2407.15854v1 Announce Type: cross 
Abstract: Scientific social stratification is a classic theme in the sociology of science. The deep integration of social media has bridged the gap between scientometrics and sociology of science. This study comprehensively analyzes the impact of social media on scientific stratification and mobility, delving into the complex interplay between academic status and social media activity in the digital age. [Research Method] Innovatively, this paper employs An Explainable Logistic Attribution Analysis from a meso-level perspective to explore the correlation between social media behaviors and scientific social stratification. It examines the impact of scientists' use of social media in the digital age on scientific stratification and mobility, uniquely combining statistical methods with machine learning. This fusion effectively integrates hypothesis testing with a substantive interpretation of the contribution of independent variables to the model. [Research Conclusion] Empirical evidence demonstrates that social media promotes stratification and mobility within the scientific community, revealing a nuanced and non-linear facilitation mechanism. Social media activities positively impact scientists' status within the scientific social hierarchy to a certain extent, but beyond a specific threshold, this impact turns negative. It shows that the advent of social media has opened new channels for academic influence, transcending the limitations of traditional academic publishing, and prompting changes in scientific stratification. Additionally, the study acknowledges the limitations of its experimental design and suggests future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15854v1</guid>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Yue</dc:creator>
    </item>
    <item>
      <title>A Survey on Differential Privacy for SpatioTemporal Data in Transportation Research</title>
      <link>https://arxiv.org/abs/2407.15868</link>
      <description>arXiv:2407.15868v1 Announce Type: cross 
Abstract: With low-cost computing devices, improved sensor technology, and the proliferation of data-driven algorithms, we have more data than we know what to do with. In transportation, we are seeing a surge in spatiotemporal data collection. At the same time, concerns over user privacy have led to research on differential privacy in applied settings. In this paper, we look at some recent developments in differential privacy in the context of spatiotemporal data. Spatiotemporal data contain not only features about users but also the geographical locations of their frequent visits. Hence, the public release of such data carries extreme risks. To address the need for such data in research and inference without exposing private information, significant work has been proposed. This survey paper aims to summarize these efforts and provide a review of differential privacy mechanisms and related software. We also discuss related work in transportation where such mechanisms have been applied. Furthermore, we address the challenges in the deployment and mass adoption of differential privacy in transportation spatiotemporal data for downstream analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15868v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahul Bhadani</dc:creator>
    </item>
    <item>
      <title>Discovering overlapping communities in multi-layer directed networks</title>
      <link>https://arxiv.org/abs/2407.16152</link>
      <description>arXiv:2407.16152v1 Announce Type: cross 
Abstract: This article explores the challenging problem of detecting overlapping communities in multi-layer directed networks. Our goal is to understand the underlying asymmetric overlapping community structure by analyzing the mixed memberships of nodes. We introduce a new model, the multi-layer mixed membership stochastic co-block model (multi-layer MM-ScBM), to model multi-layer directed networks in which nodes can belong to multiple communities. We develop a spectral procedure to estimate nodes' memberships in both sending and receiving patterns. Our method uses a successive projection algorithm on a few leading eigenvectors of two debiased aggregation matrices. To our knowledge, this is the first work to detect asymmetric overlapping communities in multi-layer directed networks. We demonstrate the consistent estimation properties of our method by providing per-node error rates under the multi-layer MM-ScBM framework. Our theoretical analysis reveals that increasing the overall sparsity, the number of nodes, or the number of layers can improve the accuracy of overlapping community detection. Extensive numerical experiments are conducted to validate these theoretical findings. We also apply our method to one real-world multi-layer directed network, gaining insightful results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16152v1</guid>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huan Qing</dc:creator>
    </item>
    <item>
      <title>A Randomized Exchange Algorithm for Optimal Design of Multi-Response Experiments</title>
      <link>https://arxiv.org/abs/2407.16283</link>
      <description>arXiv:2407.16283v1 Announce Type: cross 
Abstract: Despite the increasing prevalence of vector observations, computation of optimal experimental design for multi-response models has received limited attention. To address this problem within the framework of approximate designs, we introduce mREX, an algorithm that generalizes the randomized exchange algorithm REX (J Am Stat Assoc 115:529, 2020), originally specialized for single-response models. The mREX algorithm incorporates several improvements: a novel method for computing efficient sparse initial designs, an extension to all differentiable Kiefer's optimality criteria, and an efficient method for performing optimal exchanges of weights. For the most commonly used D-optimality criterion, we propose a technique for optimal weight exchanges based on the characteristic matrix polynomial. The mREX algorithm is applicable to linear, nonlinear, and generalized linear models, and scales well to large problems. It typically converges to optimal designs faster than available alternative methods, although it does not require advanced mathematical programming solvers. We demonstrate the application of mREX to bivariate dose-response Emax models for clinical trials, both without and with the inclusion of covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16283v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P\'al Somogyi, Samuel Rosa, Radoslav Harman</dc:creator>
    </item>
    <item>
      <title>Bayesian Autoregressive Online Change-Point Detection with Time-Varying Parameters</title>
      <link>https://arxiv.org/abs/2407.16376</link>
      <description>arXiv:2407.16376v1 Announce Type: cross 
Abstract: Change points in real-world systems mark significant regime shifts in system dynamics, possibly triggered by exogenous or endogenous factors. These points define regimes for the time evolution of the system and are crucial for understanding transitions in financial, economic, social, environmental, and technological contexts. Building upon the Bayesian approach introduced in \cite{c:07}, we devise a new method for online change point detection in the mean of a univariate time series, which is well suited for real-time applications and is able to handle the general temporal patterns displayed by data in many empirical contexts. We first describe time series as an autoregressive process of an arbitrary order. Second, the variance and correlation of the data are allowed to vary within each regime driven by a scoring rule that updates the value of the parameters for a better fit of the observations. Finally, a change point is detected in a probabilistic framework via the posterior distribution of the current regime length. By modeling temporal dependencies and time-varying parameters, the proposed approach enhances both the estimate accuracy and the forecasting power. Empirical validations using various datasets demonstrate the method's effectiveness in capturing memory and dynamic patterns, offering deeper insights into the non-stationary dynamics of real-world systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16376v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioanna-Yvonni Tsaknaki, Fabrizio Lillo, Piero Mazzarisi</dc:creator>
    </item>
    <item>
      <title>CASTRO -- Efficient constrained sampling method for material and chemical experimental design</title>
      <link>https://arxiv.org/abs/2407.16567</link>
      <description>arXiv:2407.16567v1 Announce Type: cross 
Abstract: The exploration of multicomponent material composition space requires significant time and financial investments, necessitating efficient use of resources for statistically relevant compositions. This article introduces a novel methodology, implemented in the open-source CASTRO (ConstrAined Sequential laTin hypeRcube sampling methOd) software package, to overcome equality-mixture constraints and ensure comprehensive design space coverage. Our approach leverages Latin hypercube sampling (LHS) and LHS with multidimensional uniformity (LHSMDU) using a divide-and-conquer strategy to manage high-dimensional problems effectively. By incorporating previous experimental knowledge within a limited budget, our method strategically recommends a feasible number of experiments to explore the design space. We validate our methodology with two examples: a four-dimensional problem with near-uniform distributions and a nine-dimensional problem with additional mixture constraints, yielding specific component distributions. Our constrained sequential LHS or LHSMDU approach enables thorough design space exploration, proving robustness for experimental design. This research not only advances material science but also offers promising solutions for efficiency challenges in pharmaceuticals and chemicals. CASTRO and the case studies are available for free download on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16567v1</guid>
      <category>stat.CO</category>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christina Schenk, Maciej Haranczyk</dc:creator>
    </item>
    <item>
      <title>Online network change point detection with missing values and temporal dependence</title>
      <link>https://arxiv.org/abs/2110.06450</link>
      <description>arXiv:2110.06450v3 Announce Type: replace 
Abstract: In this paper we study online change point detection in dynamic networks with time heterogeneous missing pattern within networks and dependence across the time course. The missingness probabilities, the entrywise sparsity of networks, the rank of networks and the jump size in terms of the Frobenius norm, are all allowed to vary as functions of the pre-change sample size. On top of a thorough handling of all the model parameters, we notably allow the edges and missingness to be dependent. To the best of our knowledge, such general framework has not been rigorously nor systematically studied before in the literature. We propose a polynomial time change point detection algorithm, with a version of soft-impute algorithm (e.g. Mazumder et al., 2010; Klopp, 2015) as the imputation sub-routine. Piecing up these standard sub-routines algorithms, we are able to solve a brand new problem with sharp detection delay subject to an overall Type-I error control. Extensive numerical experiments are conducted demonstrating the outstanding performances of our proposed method in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.06450v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haotian Xu, Paromita Dubey, Yi Yu</dc:creator>
    </item>
    <item>
      <title>Performing global sensitivity analysis on simulations of a continuous-time Markov chain model motivated by epidemiology</title>
      <link>https://arxiv.org/abs/2202.07277</link>
      <description>arXiv:2202.07277v4 Announce Type: replace 
Abstract: In this paper we apply a methodology introduced in Navarro Jimenez et al (2016) in the framework of chemical reaction networks to perform a global sensitivity analysis on simulations of a continuous-time Markov chain model motivated by epidemiology. Our goal is to quantify not only the effects of uncertain parameters such as epidemic parameters (transmission rate,  mean sojourn duration in compartments), but also  those of intrinsic randomness and interactions between epidemic parameters and intrinsic randomness. For that purpose, following what was proposed in Navarro Jimenez et al, we leverage three exact simulation algorithms for continuous-time Markov chains from the state of the art which we combine with common tools from variance-based sensitivity analysis as introduced in Sobol (1993). Also, we discuss the impact of the choice of the simulation algorithm used for the simulations on the results of sensitivity analysis. Such a discussion is new, at least to our knowledge. In a numerical section, we implement and compare three sensitivity analyses based on simulations obtained from different exact simulation algorithms of a SARS-CoV-2 epidemic model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.07277v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henri Mermoz Kouye (INRAE, MaIAGE, AIRSEA), Gildas Mazo (INRAE, MaIAGE), Cl\'ementine Prieur (AIRSEA), Elisabeta Vergu (INRAE, MaIAGE)</dc:creator>
    </item>
    <item>
      <title>Mixture of segmentation for heterogeneous functional data</title>
      <link>https://arxiv.org/abs/2303.10712</link>
      <description>arXiv:2303.10712v3 Announce Type: replace 
Abstract: In this paper we consider functional data with heterogeneity in time and in population. We propose a mixture model with segmentation of time to represent this heterogeneity while keeping the functional structure. Maximum likelihood estimator is considered, proved to be identifiable and consistent. In practice, an EM algorithm is used, combined with dynamic programming for the maximization step, to approximate the maximum likelihood estimator. The method is illustrated on a simulated dataset, and used on a real dataset of electricity consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.10712v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Brault, \'Emilie Devijver, Charlotte Laclau</dc:creator>
    </item>
    <item>
      <title>Pairwise likelihood estimation and limited information goodness-of-fit test statistics for binary factor analysis models under complex survey sampling</title>
      <link>https://arxiv.org/abs/2311.02543</link>
      <description>arXiv:2311.02543v3 Announce Type: replace 
Abstract: This paper discusses estimation and limited information goodness-of-fit test statistics in factor models for binary data using pairwise likelihood estimation and sampling weights. The paper extends the applicability of pairwise likelihood estimation for factor models with binary data to accommodate complex sampling designs. Additionally, it introduces two key limited information test statistics: the Pearson chi-squared test and the Wald test. To enhance computational efficiency, the paper introduces modifications to both test statistics. The performance of the estimation and the proposed test statistics under simple random sampling and unequal probability sampling is evaluated using simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02543v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haziq Jamil, Irini Moustaki, Chris Skinner</dc:creator>
    </item>
    <item>
      <title>On the construction of stationary processes and random fields</title>
      <link>https://arxiv.org/abs/2312.07775</link>
      <description>arXiv:2312.07775v2 Announce Type: replace 
Abstract: We propose a new method to construct a stationary process and random field with a given decreasing covariance function and any one-dimensional marginal distribution. The result is a new class of stationary processes and random fields. The construction method utilizes a correlated binary sequence, and it allows a simple and practical way to model dependence structures in a stationary process and random field as its dependence structure is induced by the correlation structure of a few disjoint sets in the support set of the marginal distribution. Simulation results of the proposed models are provided, which show the empirical behavior of a sample path.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07775v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jeonghwa Lee</dc:creator>
    </item>
    <item>
      <title>A unified generalization of inverse regression via adaptive column selection</title>
      <link>https://arxiv.org/abs/2404.08284</link>
      <description>arXiv:2404.08284v2 Announce Type: replace 
Abstract: A bottleneck of sufficient dimension reduction (SDR) in the modern era is that, among numerous methods, only the sliced inverse regression (SIR) is generally applicable under the high-dimensional settings. The higher-order inverse regression methods, which form a major family of SDR methods that are superior to SIR in the population level, suffer from the dimensionality of their intermediate matrix-valued parameters that have an excessive number of columns. In this paper, we propose the generic idea of using a small subset of columns of the matrix-valued parameter for SDR estimation, which breaks the convention of using the ambient matrix for the higher-order inverse regression methods. With the aid of a quick column selection procedure, we then generalize these methods as well as their ensembles towards sparsity under the ultrahigh-dimensional settings, in a uniform manner that resembles sparse SIR and without additional assumptions. This is the first promising attempt in the literature to free the higher-order inverse regression methods from their dimensionality, which facilitates the applicability of SDR. The gain of column selection with respect to SDR estimation efficiency is also studied under the fixed-dimensional settings. Simulation studies and a real data example are provided at the end.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08284v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yin Jin, Wei Luo</dc:creator>
    </item>
    <item>
      <title>Dynamic Factor Analysis with Dependent Gaussian Processes for High-Dimensional Gene Expression Trajectories</title>
      <link>https://arxiv.org/abs/2307.02781</link>
      <description>arXiv:2307.02781v2 Announce Type: replace-cross 
Abstract: The increasing availability of high-dimensional, longitudinal measures of gene expression can facilitate understanding of biological mechanisms, as required for precision medicine. Biological knowledge suggests that it may be best to describe complex diseases at the level of underlying pathways, which may interact with one another. We propose a Bayesian approach that allows for characterising such correlation among different pathways through Dependent Gaussian Processes (DGP) and mapping the observed high-dimensional gene expression trajectories into unobserved low-dimensional pathway expression trajectories via Bayesian Sparse Factor Analysis. Our proposal is the first attempt to relax the classical assumption of independent factors for longitudinal data and has demonstrated a superior performance in recovering the shape of pathway expression trajectories, revealing the relationships between genes and pathways, and predicting gene expressions (closer point estimates and narrower predictive intervals), as demonstrated through simulations and real data analysis. To fit the model, we propose a Monte Carlo Expectation Maximization (MCEM) scheme that can be implemented conveniently by combining a standard Markov Chain Monte Carlo sampler and an R package GPFDA (Konzen and others, 2021), which returns the maximum likelihood estimates of DGP hyperparameters. The modular structure of MCEM makes it generalizable to other complex models involving the DGP model component. Our R package DGP4LCF that implements the proposed approach is available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.02781v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiachen Cai, Robert J. B. Goudie, Colin Starr, Brian D. M. Tom</dc:creator>
    </item>
    <item>
      <title>Differentially private projection-depth-based medians</title>
      <link>https://arxiv.org/abs/2312.07792</link>
      <description>arXiv:2312.07792v3 Announce Type: replace-cross 
Abstract: We develop $(\epsilon,\delta)$-differentially private projection-depth-based medians using the propose-test-release (PTR) and exponential mechanisms. Under general conditions on the input parameters and the population measure, (e.g. we do not assume any moment bounds), we quantify the probability the test in PTR fails, as well as the cost of privacy via finite sample deviation bounds. Next, we show that when some observations are contaminated, the private projection-depth-based median does not break down, provided its input location and scale estimators do not break down. We demonstrate our main results on the canonical projection-depth-based median, as well as on projection-depth-based medians derived from trimmed estimators. In the Gaussian setting, we show that the resulting deviation bound matches the known lower bound for private Gaussian mean estimation. In the Cauchy setting, we show that the ``outlier error amplification'' effect resulting from the heavy tails outweighs the cost of privacy. This result is then verified via numerical simulations. Additionally, we present results on general PTR mechanisms and a uniform concentration result on the projected spacings of order statistics, which may be of general interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07792v3</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kelly Ramsay, Dylan Spicker</dc:creator>
    </item>
  </channel>
</rss>
