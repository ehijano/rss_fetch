<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Aug 2024 07:29:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Detecting eigenvectors of an operator that are near a specified subspace</title>
      <link>https://arxiv.org/abs/2408.05394</link>
      <description>arXiv:2408.05394v1 Announce Type: new 
Abstract: In modeling quantum systems or wave phenomena, one is often interested in identifying eigenstates that approximately carry a specified property; scattering states approximately align with incoming and outgoing traveling waves, for instance, and electron states in molecules often approximately align with superpositions of simple atomic orbitals. These examples -- and many others -- can be formulated as the following eigenproblem: given a self-adjoint operator $\mathcal{L}$ on a Hilbert space $\mathcal{H}$ and a closed subspace $W\subset\mathcal{H}$, can we identify all eigenvectors of $\mathcal{L}$ that lie approximately in $W$?
  We develop an approach to answer this question efficiently, with a user-defined tolerance and range of eigenvalues, building upon recent work for spatial localization in diffusion operators (Ovall and Reid, 2023). Namely, by perturbing $\mathcal{L}$ appropriately along the subspace $W$, we collect the eigenvectors near $W$ into a well-isolated region of the spectrum, which can then be explored using any of several existing methods. We prove key bounds on perturbations of both eigenvalues and eigenvectors, showing that our algorithm correctly identifies desired eigenpairs, and we support our results with several numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05394v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Darrow, Jeffrey S. Ovall</dc:creator>
    </item>
    <item>
      <title>Greedy randomized block Kaczmarz method for matrix equation AXB=C and its applications in color image restoration</title>
      <link>https://arxiv.org/abs/2408.05444</link>
      <description>arXiv:2408.05444v1 Announce Type: new 
Abstract: In view of the advantages of simplicity and effectiveness of the Kaczmarz method, which was originally employed to solve the large-scale system of linear equations $Ax=b$, we study the greedy randomized block Kaczmarz method (ME-GRBK) and its relaxation and deterministic versions to solve the matrix equation $AXB=C$, which is commonly encountered in the applications of engineering sciences. It is demonstrated that our algorithms converge to the unique least-norm solution of the matrix equation when it is consistent and their convergence rate is faster than that of the randomized block Kaczmarz method (ME-RBK). Moreover, the block Kaczmarz method (ME-BK) for solving the matrix equation $AXB=C$ is investigated and it is found that the ME-BK method converges to the solution $A^{+}CB^{+}+X^{0}-A^{+}AX^{0}BB^{+}$ when it is consistent. The numerical tests verify the theoretical results and the methods presented in this paper are applied to the color image restoration problem to obtain satisfactory restored images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05444v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenli Wang, Duo Liu, Gangrong Qu, Caiqin Song</dc:creator>
    </item>
    <item>
      <title>Fourth-Order Paired-Explicit Runge-Kutta Methods</title>
      <link>https://arxiv.org/abs/2408.05470</link>
      <description>arXiv:2408.05470v1 Announce Type: new 
Abstract: In this paper, we extend the Paired-Explicit Runge-Kutta schemes by Vermeire et. al. to fourth-order of consistency. Based on the order conditions for partitioned Runge-Kutta methods we motivate a specific form of the Butcher arrays which leads to a family of fourth-order accurate methods. The employed form of the Butcher arrays results in a special structure of the stability polynomials, which needs to be adhered to for an efficient optimization of the domain of absolute stability. We demonstrate that the constructed fourth-order Paired-Explicit Runge-Kutta methods satisfy linear stability, internal consistency, designed order of convergence, and conservation of linear invariants. At the same time, these schemes are seamlessly coupled for codes employing a method-of-lines approach, in particular without any modifications of the spatial discretization. We apply the multirate Paired-Explicit Runge-Kutta (P-ERK) schemes to inviscid and viscous problems with locally varying wave speeds, which may be induced by non-uniform grids or multiscale properties of the governing partial differential equation. Compared to state-of-the-art optimized standalone methods, the multirate P-ERK schemes allow significant reductions in right-hand-side evaluations and wall-clock time, ranging from 40% up to factors greater than three.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05470v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Doehring, Lars Christmann, Michael Schlottke-Lakemper, Gregor J. Gassner, Manuel Torrilhon</dc:creator>
    </item>
    <item>
      <title>Generalized minimal residual method for systems with multiple right-hand sides</title>
      <link>https://arxiv.org/abs/2408.05513</link>
      <description>arXiv:2408.05513v1 Announce Type: new 
Abstract: A new variant of the GMRES method is presented for solving linear systems with the same matrix and subsequently obtained multiple right-hand sides. The new method keeps such properties of the classical GMRES algorithm as follows. Both bases of the search space and its image are maintained orthonormal that increases the robustness of the method. Moreover there is no need to store both bases since they are effectively represented within a common basis. Along with it our method is theoretically equivalent to the GCR method extended for a case of multiple right-hand sides but is more numerically robust and requires less memory.
  The main result of the paper is a mechanism of adding an arbitrary direction vector to the search space that can be easily adopted for flexible GMRES or GMRES with deflated restarting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05513v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S. Sukmanyuk, D. Zheltkov, B. Valiakhmetov</dc:creator>
    </item>
    <item>
      <title>Low-rank approximation of parameter-dependent matrices via CUR decomposition</title>
      <link>https://arxiv.org/abs/2408.05595</link>
      <description>arXiv:2408.05595v1 Announce Type: new 
Abstract: A low-rank approximation of a parameter-dependent matrix $A(t)$ is an important task in the computational sciences appearing for example in dynamical systems and compression of a series of images. In this work, we introduce AdaCUR, an efficient algorithm for computing a low-rank approximation of parameter-dependent matrices via CUR decomposition. The key idea for this algorithm is that for nearby parameter values, the column and row indices for the CUR decomposition can often be reused. AdaCUR is rank-adaptive, certifiable and has complexity that compares favourably against existing methods. A faster algorithm which we call FastAdaCUR that prioritizes speed over accuracy is also given, which is rank-adaptive and has complexity which is at most linear in the number of rows or columns, but without certification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05595v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taejun Park, Yuji Nakatsukasa</dc:creator>
    </item>
    <item>
      <title>A forward differential deep learning-based algorithm for solving high-dimensional nonlinear backward stochastic differential equations</title>
      <link>https://arxiv.org/abs/2408.05620</link>
      <description>arXiv:2408.05620v1 Announce Type: new 
Abstract: In this work, we present a novel forward differential deep learning-based algorithm for solving high-dimensional nonlinear backward stochastic differential equations (BSDEs). Motivated by the fact that differential deep learning can efficiently approximate the labels and their derivatives with respect to inputs, we transform the BSDE problem into a differential deep learning problem. This is done by leveraging Malliavin calculus, resulting in a system of BSDEs. The unknown solution of the BSDE system is a triple of processes $(Y, Z, \Gamma)$, representing the solution, its gradient, and the Hessian matrix. The main idea of our algorithm is to discretize the integrals using the Euler-Maruyama method and approximate the unknown discrete solution triple using three deep neural networks. The parameters of these networks are then optimized by globally minimizing a differential learning loss function, which is novelty defined as a weighted sum of the dynamics of the discretized system of BSDEs. Through various high-dimensional examples, we demonstrate that our proposed scheme is more efficient in terms of accuracy and computation time compared to other contemporary forward deep learning-based methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05620v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>q-fin.CP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenc Kapllani, Long Teng</dc:creator>
    </item>
    <item>
      <title>The pressure-robust weak Galerkin finite element method for Stokes-Darcy problem</title>
      <link>https://arxiv.org/abs/2408.05658</link>
      <description>arXiv:2408.05658v1 Announce Type: new 
Abstract: In this paper, we propose a pressure-robust weak Galerkin (WG) finite element scheme to solve the Stokes-Darcy problem. To construct the pressure-robust numerical scheme, we use the divergence-free velocity reconstruction operator to modify the test function on the right side of the numerical scheme. We prove the error between the velocity function and its numerical solution is independent of the pressure function and viscosity coefficient. Moreover, the errors of the velocity function and the pressure function reach the optimal convergence orders under the energy norm, as validated by both theoretical analysis and numerical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05658v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiwei Jia, Lin Yang, Qilong Zhai</dc:creator>
    </item>
    <item>
      <title>Tensor Decomposition Meets RKHS: Efficient Algorithms for Smooth and Misaligned Data</title>
      <link>https://arxiv.org/abs/2408.05677</link>
      <description>arXiv:2408.05677v1 Announce Type: new 
Abstract: The canonical polyadic (CP) tensor decomposition decomposes a multidimensional data array into a sum of outer products of finite-dimensional vectors. Instead, we can replace some or all of the vectors with continuous functions (infinite-dimensional vectors) from a reproducing kernel Hilbert space (RKHS). We refer to tensors with some infinite-dimensional modes as quasitensors, and the approach of decomposing a tensor with some continuous RKHS modes is referred to as CP-HiFi (hybrid infinite and finite dimensional) tensor decomposition. An advantage of CP-HiFi is that it can enforce smoothness in the infinite dimensional modes. Further, CP-HiFi does not require the observed data to lie on a regular and finite rectangular grid and naturally incorporates misaligned data. We detail the methodology and illustrate it on a synthetic example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05677v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brett W. Larsen, Tamara G. Kolda, Anru R. Zhang, Alex H. Williams</dc:creator>
    </item>
    <item>
      <title>Parallel transport on matrix manifolds and Exponential Action</title>
      <link>https://arxiv.org/abs/2408.06054</link>
      <description>arXiv:2408.06054v1 Announce Type: new 
Abstract: We express parallel transport for several common matrix Lie groups with a family of pseudo-Riemannian metrics in terms of matrix exponential and exponential actions. The expression for parallel transport is preserved by taking the quotient under certain scenarios. In particular, for a Stiefel manifold of orthogonal matrices of size $n\times d$, we give an expression for parallel transport along a geodesic from time zero to $t$, that could be computed with time complexity of $O(nd^2)$ for small $t$, and of $O(td^3)$ for large t, contributing a step in a long-standing open problem in matrix manifolds. A similar result holds for flag manifolds with the canonical metric. We also show the parallel transport formulas for the generalized linear group, and the special orthogonal group under these metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06054v1</guid>
      <category>math.NA</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Du Nguyen, Stefan Sommer</dc:creator>
    </item>
    <item>
      <title>Neural Ordinary Differential Equations for Model Order Reduction of Stiff Systems</title>
      <link>https://arxiv.org/abs/2408.06073</link>
      <description>arXiv:2408.06073v1 Announce Type: new 
Abstract: Neural Ordinary Differential Equations (ODEs) represent a significant advancement at the intersection of machine learning and dynamical systems, offering a continuous-time analog to discrete neural networks. Despite their promise, deploying neural ODEs in practical applications often encounters the challenge of stiffness, a condition where rapid variations in some components of the solution demand prohibitively small time steps for explicit solvers. This work addresses the stiffness issue when employing neural ODEs for model order reduction by introducing a suitable reparametrization in time. The considered map is data-driven and it is induced by the adaptive time-stepping of an implicit solver on a reference solution. We show the map produces a nonstiff system that can be cheaply solved with an explicit time integration scheme. The original, stiff, time dynamic is recovered by means of a map learnt by a neural network that connects the state space to the time reparametrization. We validate our method through extensive experiments, demonstrating improvements in efficiency for the neural ODE inference while maintaining robustness and accuracy. The neural network model also showcases good generalization properties for times beyond the training data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06073v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Caldana, Jan S. Hesthaven</dc:creator>
    </item>
    <item>
      <title>Stability and error analysis of pressure-correction scheme for the Navier-Stokes-Planck-Nernst-Poisson equations</title>
      <link>https://arxiv.org/abs/2408.06085</link>
      <description>arXiv:2408.06085v1 Announce Type: new 
Abstract: In this paper, we propose and analyze first-order time-stepping pressure-correction projection scheme for the Navier-Stokes-Planck-Nernst-Poisson equations. By introducing a governing equation for the auxiliary variable through the ionic concentration equations, we reconstruct the original equations into an equivalent system and develop a first-order decoupled and linearized scheme. This scheme preserves non-negativity and mass conservation of the concentration components and is unconditionally energy stable. We derive the rigorous error estimates in the two dimensional case for the ionic concentrations, electric potential, velocity and pressure in the $L^2$- and $H^1$-norms. Numerical examples are presented to validate the proposed scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06085v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuyu He, Hongtao Chen</dc:creator>
    </item>
    <item>
      <title>New Ensemble Domain Decomposition Method for the Steady-state Random Stokes-Darcy Coupled Problems with Uncertain Parameters</title>
      <link>https://arxiv.org/abs/2408.06169</link>
      <description>arXiv:2408.06169v1 Announce Type: new 
Abstract: This paper presents two novel ensemble domain decomposition methods for fast-solving the Stokes-Darcy coupled models with random hydraulic conductivity and body force. To address such random systems, we employ the Monte Carlo (MC) method to generate a set of independent and identically distributed deterministic model samples. To facilitate the fast calculation of these samples, we adroitly integrate the ensemble idea with the domain decomposition method (DDM). This approach not only allows multiple linear problems to share a standard coefficient matrix but also enables easy-to-use and convenient parallel computing. By selecting appropriate Robin parameters, we rigorously prove that the proposed algorithm has mesh-dependent and mesh-independent convergence rates. For cases that require mesh-independent convergence, we additionally provide optimized Robin parameters to achieve optimal convergence rates. We further adopt the multi-level Monte Carlo (MLMC) method to significantly lower the computational cost in the probability space, as the number of samples drops quickly when the mesh becomes finer. Building on our findings, we propose two novel algorithms: MC ensemble DDM and MLMC ensemble DDM, specifically for random models. Furthermore, we strictly give the optimal convergence order for both algorithms. Finally, we present several sets of numerical experiments to showcase the efficiency of our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06169v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chunchi Liu, Yao Rong, Yizhong Sun, Jiaping Yu, Haibiao Zheng</dc:creator>
    </item>
    <item>
      <title>A Finite Volume scheme for the solution of discontinuous magnetic field distributions on non-orthogonal meshes</title>
      <link>https://arxiv.org/abs/2408.06280</link>
      <description>arXiv:2408.06280v1 Announce Type: new 
Abstract: We present a Finite Volume formulation for determining discontinuous distributions of magnetic fields within non-orthogonal and non-uniform meshes. The numerical approach is based on the discretization of the vector potential variant of the equations governing static magnetic field distribution in magnetized, permeable and current carrying media. After outlining the derivation of the magnetostatic balance equations and its associated boundary conditions, we propose a cell-centered Finite Volume framework for spatial discretization and a Block Gauss-Seidel multi-region scheme for solution. We discuss the structure of the solver, emphasizing its effectiveness and addressing stabilization and correction techniques to enhance computational robustness. We validate the accuracy and efficacy of the approach through numerical experiments and comparisons with the Finite Element method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06280v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Augusto Riedinger, Mart\'in Saravia, Jos\'e Ram\'irez</dc:creator>
    </item>
    <item>
      <title>An improved Shifted CholeskyQR based on columns</title>
      <link>https://arxiv.org/abs/2408.06311</link>
      <description>arXiv:2408.06311v1 Announce Type: new 
Abstract: Shifted CholeskyQR3 is designed to address the QR factorization of ill-conditioned matrices. This algorithm introduces a shift parameter $s$ to prevent failure during the initial Cholesky factorization step. The choice of this shift parameter $s$ is critical for the algorithm's effectiveness. Our goal is to identify a smaller $s$ compared to the traditional selection involving $\|X\|_2$. In this research, we propose a new norm $\|X\|_g$, which is based on the column properties of $X$, to obtain a reduced shift parameter $s$ for the Shifted CholeskyQR3 algorithm. We provide rigorous proofs of orthogonality and residuals for the improved algorithm with our proposed $s$. Numerical experiments confirm the enhanced numerical stability of orthogonality and residuals with the reduced $s$. Furthermore, we compare CPU times with other algorithms to assess performance improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06311v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuwei Fan, Haoran Guan, Zhonghua Qiao</dc:creator>
    </item>
    <item>
      <title>Implementing a Restricted Function Space Class in Firedrake</title>
      <link>https://arxiv.org/abs/2408.05217</link>
      <description>arXiv:2408.05217v1 Announce Type: cross 
Abstract: The implementation process of a $\texttt{RestrictedFunctionSpace}$ class in Firedrake, a Python library which numerically solves partial differential equations through the use of the finite element method, is documented. This includes an introduction to the current $\texttt{FunctionSpace}$ class in Firedrake, and the key features that it has. With the current $\texttt{FunctionSpace}$ class, the limitations of the capabilities of the solvers in Firedrake when imposing Dirichlet boundary conditions are explored, as well as what the $\texttt{RestrictedFunctionSpace}$ class does differently to remove these issues. These will be considered in both a mathematical way, and in the code as an abstraction of the mathematical ideas presented. Finally, the benefits to the user of the $\texttt{RestrictedFunctionSpace}$ class are considered, and demonstrated through tests and comparisons. This leads to the conclusion that in particular, the eigensolver in Firedrake is improved through the use of the $\texttt{RestrictedFunctionSpace}$, through the removal of eigenvalues associated with the Dirichlet boundary conditions for a system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05217v1</guid>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Rothwell</dc:creator>
    </item>
    <item>
      <title>Non-Negative Reduced Biquaternion Matrix Factorization with Applications in Color Face Recognition</title>
      <link>https://arxiv.org/abs/2408.05582</link>
      <description>arXiv:2408.05582v1 Announce Type: cross 
Abstract: Reduced biquaternion (RB), as a four-dimensional algebra highly suitable for representing color pixels, has recently garnered significant attention from numerous scholars. In this paper, for color image processing problems, we introduce a concept of the non-negative RB matrix and then use the multiplication properties of RB to propose a non-negative RB matrix factorization (NRBMF) model. The NRBMF model is introduced to address the challenge of reasonably establishing a non-negative quaternion matrix factorization model, which is primarily hindered by the multiplication properties of traditional quaternions. Furthermore, this paper transforms the problem of solving the NRBMF model into an RB alternating non-negative least squares (RB-ANNLS) problem. Then, by introducing a method to compute the gradient of the real function with RB matrix variables, we solve the RB-ANNLS optimization problem using the RB projected gradient algorithm and conduct a convergence analysis of the algorithm. Finally, we validate the effectiveness and superiority of the proposed NRBMF model in color face recognition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05582v1</guid>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jifei Miao, Junjun Pan, Michael K. Ng</dc:creator>
    </item>
    <item>
      <title>Low-Rank Approximation, Adaptation, and Other Tales</title>
      <link>https://arxiv.org/abs/2408.05883</link>
      <description>arXiv:2408.05883v1 Announce Type: cross 
Abstract: Low-rank approximation is a fundamental technique in modern data analysis, widely utilized across various fields such as signal processing, machine learning, and natural language processing. Despite its ubiquity, the mechanics of low-rank approximation and its application in adaptation can sometimes be obscure, leaving practitioners and researchers with questions about its true capabilities and limitations. This paper seeks to clarify low-rank approximation and adaptation by offering a comprehensive guide that reveals their inner workings and explains their utility in a clear and accessible way. Our focus here is to develop a solid intuition for how low-rank approximation and adaptation operate, and why they are so effective. We begin with basic concepts and gradually build up to the mathematical underpinnings, ensuring that readers of all backgrounds can gain a deeper understanding of low-rank approximation and adaptation. We strive to strike a balance between informal explanations and rigorous mathematics, ensuring that both newcomers and experienced experts can benefit from this survey. Additionally, we introduce new low-rank decomposition and adaptation algorithms that have not yet been explored in the field, hoping that future researchers will investigate their potential applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05883v1</guid>
      <category>cs.LG</category>
      <category>cs.IR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun Lu</dc:creator>
    </item>
    <item>
      <title>Layer-Specific Optimization: Sensitivity Based Convolution Layers Basis Search</title>
      <link>https://arxiv.org/abs/2408.06024</link>
      <description>arXiv:2408.06024v1 Announce Type: cross 
Abstract: Deep neural network models have a complex architecture and are overparameterized. The number of parameters is more than the whole dataset, which is highly resource-consuming. This complicates their application and limits its usage on different devices. Reduction in the number of network parameters helps to reduce the size of the model, but at the same time, thoughtlessly applied, can lead to a deterioration in the quality of the network. One way to reduce the number of model parameters is matrix decomposition, where a matrix is represented as a product of smaller matrices. In this paper, we propose a new way of applying the matrix decomposition with respect to the weights of convolutional layers. The essence of the method is to train not all convolutions, but only the subset of convolutions (basis convolutions), and represent the rest as linear combinations of the basis ones. Experiments on models from the ResNet family and the CIFAR-10 dataset demonstrate that basis convolutions can not only reduce the size of the model but also accelerate the forward and backward passes of the network. Another contribution of this work is that we propose a fast method for selecting a subset of network layers in which the use of matrix decomposition does not degrade the quality of the final model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06024v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vasiliy Alekseev, Ilya Lukashevich, Ilia Zharikov, Ilya Vasiliev</dc:creator>
    </item>
    <item>
      <title>Generalization capabilities of MeshGraphNets to unseen geometries for fluid dynamics</title>
      <link>https://arxiv.org/abs/2408.06101</link>
      <description>arXiv:2408.06101v1 Announce Type: cross 
Abstract: This works investigates the generalization capabilities of MeshGraphNets (MGN) [Pfaff et al. Learning Mesh-Based Simulation with Graph Networks. ICML 2021] to unseen geometries for fluid dynamics, e.g. predicting the flow around a new obstacle that was not part of the training data. For this purpose, we create a new benchmark dataset for data-driven computational fluid dynamics (CFD) which extends DeepMind's flow around a cylinder dataset by including different shapes and multiple objects. We then use this new dataset to extend the generalization experiments conducted by DeepMind on MGNs by testing how well an MGN can generalize to different shapes. In our numerical tests, we show that MGNs can sometimes generalize well to various shapes by training on a dataset of one obstacle shape and testing on a dataset of another obstacle shape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06101v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robin Schm\"ocker, Alexander Henkes, Julian Roth, Thomas Wick</dc:creator>
    </item>
    <item>
      <title>The Westervelt-Rayleigh-Plesset model of ultrasound contrast imaging with microbubbles: analysis and simulation</title>
      <link>https://arxiv.org/abs/2408.06108</link>
      <description>arXiv:2408.06108v1 Announce Type: cross 
Abstract: Ultrasound contrast imaging is a specialized imaging technique that applies microbubble contrast agents to traditional medical sonography, providing real-time visualization of blood flow and vessels. Gas-filled microbubbles are injected into the body, where they undergo compression and rarefaction and interact nonlinearly with the ultrasound waves. Therefore, the propagation of sound through a bubbly liquid is a strongly nonlinear problem that can be modeled by a nonlinear acoustic wave equation for the propagation of the pressure waves coupled via the source terms to a nonlinear ordinary differential equation of Rayleigh-Plesset type for the bubble dynamics. In this work, we first derive a hierarchy of such coupled models based on constitutive laws. We then focus on the coupling of Westervelt's acoustic equation to Rayleigh-Plesset type equations, where we rigorously show the existence of solutions locally in time under suitable conditions on the initial pressure-microbubble data and final time. Thirdly, we devise and discuss numerical experiments on both single-bubble dynamics and the interaction of microbubbles with ultrasound waves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06108v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vanja Nikoli\'c, Teresa Rauscher</dc:creator>
    </item>
    <item>
      <title>Reinsurance with neural networks</title>
      <link>https://arxiv.org/abs/2408.06168</link>
      <description>arXiv:2408.06168v1 Announce Type: cross 
Abstract: We consider an insurance company which faces financial risk in the form of insurance claims and market-dependent surplus fluctuations. The company aims to simultaneously control its terminal wealth (e.g. at the end of an accounting period) and the ruin probability in a finite time interval by purchasing reinsurance. The target functional is given by the expected utility of terminal wealth perturbed by a modified Gerber-Shiu penalty function. We solve the problem of finding the optimal reinsurance strategy and the corresponding maximal target functional via neural networks. The procedure is illustrated by a numerical example, where the surplus process is given by a Cram\'er-Lundberg model perturbed by a mean-reverting Ornstein-Uhlenbeck process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06168v1</guid>
      <category>q-fin.RM</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>q-fin.CP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aleksandar Arandjelovi\'c, Julia Eisenberg</dc:creator>
    </item>
    <item>
      <title>Rigorous Hausdorff dimension estimates for conformal fractals</title>
      <link>https://arxiv.org/abs/2408.06330</link>
      <description>arXiv:2408.06330v1 Announce Type: cross 
Abstract: We develop a versatile framework which allows us to rigorously estimate the Hausdorff dimension of maximal conformal graph directed Markov systems in $\mathbb{R}^n$ for $n \geq 2$. Our method is based on piecewise linear approximations of the eigenfunctions of the Perron-Frobenius operator via a finite element framework for discretization and iterative mesh schemes. One key element in our approach is obtaining bounds for the derivatives of these eigenfunctions, which, besides being essential for the implementation of our method, are of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06330v1</guid>
      <category>math.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vasileios Chousionis, Dmitriy Leykekhman, Mariusz Urba\'nski, Erik Wendt</dc:creator>
    </item>
    <item>
      <title>On the linear convergence of additive Schwarz methods for the $p$-Laplacian</title>
      <link>https://arxiv.org/abs/2210.09183</link>
      <description>arXiv:2210.09183v5 Announce Type: replace 
Abstract: We consider additive Schwarz methods for boundary value problems involving the $p$-Laplacian. While existing theoretical estimates suggest a sublinear convergence rate for these methods, empirical evidence from numerical experiments demonstrates a linear convergence rate. In this paper, we narrow the gap between these theoretical and empirical results by presenting a novel convergence analysis. Firstly, we present a new convergence theory for additive Schwarz methods written in terms of a quasi-norm. This quasi-norm exhibits behavior akin to the Bregman distance of the convex energy functional associated with the problem. Secondly, we provide a quasi-norm version of the Poincar'{e}--Friedrichs inequality, which plays a crucial role in deriving a quasi-norm stable decomposition for a two-level domain decomposition setting. By utilizing these key elements, we establish the asymptotic linear convergence of additive Schwarz methods for the $p$-Laplacian.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.09183v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Young-Ju Lee, Jongho Park</dc:creator>
    </item>
    <item>
      <title>Open Source Implementations of Numerical Algorithms for Computing the Complete Elliptic Integral of the First Kind</title>
      <link>https://arxiv.org/abs/2212.05694</link>
      <description>arXiv:2212.05694v5 Announce Type: replace 
Abstract: The complete elliptic integral of the first kind (CEI-1) plays a significant role in mathematics, physics and engineering. There is no simple formula for its computation, thus numerical algorithms are essential for coping with the practical problems involved. The commercial implementations for the numerical solutions, such as the functions \lstinline|ellipticK| and \lstinline|EllipticK| provided by MATLAB and Mathematica respectively, are based on $\mathcal{K}_{\mathrm{cs}}(m)$ instead of the usual form $K(k)$ such that $\mathcal{K}_{\mathrm{cs}}(k^2) =K(k)$ and $m=k^2$. It is necessary to develop open source implementations for the computation of the CEI-1 in order to avoid potential risks of using commercial software and possible limitations due to the unknown factors. In this paper, the infinite series method, arithmetic-geometric mean (AGM) method, Gauss-Chebyshev method and Gauss-Legendre methods are discussed in details with a top-down strategy. The four key algorithms for computing CEI-1 are designed, verified, validated and tested, which can be utilized in R\&amp; D and be reused properly. Numerical results show that our open source implementations based on $K(k)$ are equivalent to the commercial implementation based on $\mathcal{K}_{\mathrm{cs}}(m)$. The general algorithms for computing orthogonal polynomials developed are significant byproducts in the sense of STEM education and scientific computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05694v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.rinam.2024.100479</arxiv:DOI>
      <arxiv:journal_reference>Results in Applied Mathematics, 2024, 23(8):e100479</arxiv:journal_reference>
      <dc:creator>Hong-Yan Zhang, Wen-Juan Jiang</dc:creator>
    </item>
    <item>
      <title>Smooth digital terrain modelling in irregular domain using finite element thin plate splines and adaptive refinement</title>
      <link>https://arxiv.org/abs/2302.12974</link>
      <description>arXiv:2302.12974v2 Announce Type: replace 
Abstract: Digital terrain models (DTMs) are created using elevation data collected in geological surveys using varied sampling techniques like airborne lidar and depth soundings. This often leads to large data sets with different distribution patterns, which may require smooth data approximations in irregular domains with complex boundaries. The thin plate spline (TPS) interpolates scattered data and produces visually pleasing surfaces, but it is too computationally expensive for large data sizes. The finite element thin plate spline (TPSFEM) possesses smoothing properties similar to those of the TPS and interpolates large data sets efficiently. This article investigates the performance of the TPSFEM and adaptive mesh refinement in irregular domains. Boundary conditions are critical for the accuracy of the solution in domains with arbitrary-shaped boundaries and are approximated using the TPS with a subset of sampled points. Numerical experiments are conducted on aerial, terrestrial and bathymetric surveys. It is shown that the TPSFEM works well in square and irregular domains for modelling terrain surfaces and adaptive refinement significantly improves its efficiency. A comparison of the TPSFEM, TPS and compactly supported radial basis functions indicates its competitiveness in terms of accuracy and costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.12974v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lishan Fang</dc:creator>
    </item>
    <item>
      <title>A Finite Expression Method for Solving High-Dimensional Committor Problems</title>
      <link>https://arxiv.org/abs/2306.12268</link>
      <description>arXiv:2306.12268v2 Announce Type: replace 
Abstract: Transition path theory (TPT) is a mathematical framework for quantifying rare transition events between a pair of selected metastable states $A$ and $B$. Central to TPT is the committor function, which describes the probability to hit the metastable state $B$ prior to $A$ from any given starting point of the phase space. Once the committor is computed, the transition channels and the transition rate can be readily found. The committor is the solution to the backward Kolmogorov equation with appropriate boundary conditions. However, solving it is a challenging task in high dimensions due to the need to mesh a whole region of the ambient space. In this work, we explore the finite expression method (FEX, Liang and Yang (2022)) as a tool for computing the committor. FEX approximates the committor by an algebraic expression involving a fixed finite number of nonlinear functions and binary arithmetic operations. The optimal nonlinear functions, the binary operations, and the numerical coefficients in the expression template are found via reinforcement learning. The FEX-based committor solver is tested on several high-dimensional benchmark problems. It gives comparable or better results than neural network-based solvers. Most importantly, FEX is capable of correctly identifying the algebraic structure of the solution which allows one to reduce the committor problem to a low-dimensional one and find the committor with any desired accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.12268v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>cs.SC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zezheng Song, Maria K. Cameron, Haizhao Yang</dc:creator>
    </item>
    <item>
      <title>The fast committor machine: Interpretable prediction with kernels</title>
      <link>https://arxiv.org/abs/2405.10410</link>
      <description>arXiv:2405.10410v3 Announce Type: replace 
Abstract: In the study of stochastic systems, the committor function describes the probability that a system starting from an initial configuration $x$ will reach a set $B$ before a set $A$. This paper introduces an efficient and interpretable algorithm for approximating the committor, called the "fast committor machine" (FCM). The FCM uses simulated trajectory data to build a kernel-based model of the committor. The kernel function is constructed to emphasize low-dimensional subspaces that optimally describe the $A$ to $B$ transitions. The coefficients in the kernel model are determined using randomized linear algebra, leading to a runtime that scales linearly in the number of data points. In numerical experiments involving a triple-well potential and alanine dipeptide, the FCM yields higher accuracy and trains more quickly than a neural network with the same number of parameters. The FCM is also more interpretable than the neural net.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10410v3</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>D. Aristoff, M. Johnson, G. Simpson, R. J. Webber</dc:creator>
    </item>
    <item>
      <title>Monotonicity, bounds and extrapolation of Block-Gauss and Gauss-Radau quadrature for computing $B^T \phi (A) B$</title>
      <link>https://arxiv.org/abs/2407.21505</link>
      <description>arXiv:2407.21505v2 Announce Type: replace 
Abstract: In this paper, we explore quadratures for the evaluation of $B^T \phi(A) B$ where $A$ is a symmetric nonnegative-definite matrix in $\mathbb{R}^{n \times n}$, $B$ is a tall matrix in $\mathbb{R}^{n \times p}$, and $\phi(\cdot)$ represents a matrix function that is regular enough in the neighborhood of $A$'s spectrum, e.g., a Stieltjes or exponential function. These formulations, for example, commonly arise in the computation of multiple-input multiple-output (MIMO) transfer functions for diffusion PDEs.
  We propose an approximation scheme for $B^T \phi(A) B$ leveraging the block Lanczos algorithm and its equivalent representation through Stieltjes matrix continued fractions. We extend the notion of Gauss-Radau quadrature to the block case, facilitating the derivation of easily computable error bounds. For problems stemming from the discretization of self-adjoint operators with a continuous spectrum, we obtain sharp estimates grounded in potential theory for Pad\'e approximations and justify extrapolation algorithms at no added computational cost. The obtained results are illustrated on large-scale examples of 2D diffusion and 3D Maxwell's equations as well as a graph from the SNAP repository. We also present promising experimental results on convergence acceleration via random enrichment of the initial block $B$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21505v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J\"orn Zimmerling, Vladimir Druskin, Valeria Simoncini</dc:creator>
    </item>
    <item>
      <title>Mixed precision HODLR matrices</title>
      <link>https://arxiv.org/abs/2407.21637</link>
      <description>arXiv:2407.21637v3 Announce Type: replace 
Abstract: Hierarchical matrix computations have attracted significant attention in the science and engineering community as exploiting data-sparse structures can significantly reduce the computational complexity of many important kernels. One particularly popular option within this class is the Hierarchical Off-Diagonal Low-Rank (HODLR) format. In this paper, we show that the off-diagonal blocks of HODLR matrices that are approximated by low-rank matrices can be represented in low precision without degenerating the quality of the overall approximation (with the error growth bounded by a factor of $2$). We also present an adaptive-precision scheme for constructing and storing HODLR matrices, and we prove that the use of mixed precision does not compromise the numerical stability of the resulting HOLDR matrix--vector product and LU factorization. That is, the resulting error in these computations is not significantly greater than the case where we use one precision (say, double) for constructing and storing the HOLDR matrix. Our analyses further give insight on how one must choose the working precision in HOLDR matrix computations relative to the approximation error in order to not observe the effects of finite precision. Intuitively, when a HOLDR matrix is subject to a high degree of approximation error, subsequent computations can be performed in a lower precision without detriment. We demonstrate the validity of our theoretical results through a range of numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21637v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erin Carson, Xinye Chen, Xiaobo Liu</dc:creator>
    </item>
    <item>
      <title>Explicit expression and fast algorithms for the inverse of some matrices arising from implicit time integration</title>
      <link>https://arxiv.org/abs/2408.04316</link>
      <description>arXiv:2408.04316v2 Announce Type: replace 
Abstract: In this paper, we first present an explicit expression for the inverse\emph{} of a type of matrices. As special applications, the inverse of some matrices arising from implicit time integration techniques, such as the well-known implicit Runge-Kutta schemes and block implicit methods, can also be explicitly determined. Adiitionally, we introduce three fast algorithms for computing the elements of the inverse of these matrices in $O(n^2)$ arithmetic operations, i.e., the first one is based on Traub algorithm for fast inversion of Vandermonde matrices, while the other two utilize the special structure of the matrices. Finally, some symbolic and numerical results are presented to show that our algorithms are both highly efficient and accurate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04316v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Li Shishun, Wei Huile</dc:creator>
    </item>
    <item>
      <title>From Monte Carlo to neural networks approximations of boundary value problems</title>
      <link>https://arxiv.org/abs/2209.01432</link>
      <description>arXiv:2209.01432v3 Announce Type: replace-cross 
Abstract: In this paper we study probabilistic and neural network approximations for solutions to Poisson equation subject to Holder data in general bounded domains of $\mathbb{R}^d$. We aim at two fundamental goals.
  The first, and the most important, we show that the solution to Poisson equation can be numerically approximated in the sup-norm by Monte Carlo methods, and that this can be done highly efficiently if we use a modified version of the walk on spheres algorithm as an acceleration method. This provides estimates which are efficient with respect to the prescribed approximation error and with polynomial complexity in the dimension and the reciprocal of the error. A crucial feature is that the overall number of samples does not not depend on the point at which the approximation is performed.
  As a second goal, we show that the obtained Monte Carlo solver renders in a constructive way ReLU deep neural network (DNN) solutions to Poisson problem, whose sizes depend at most polynomialy in the dimension $d$ and in the desired error. In fact we show that the random DNN provides with high probability a small approximation error and low polynomial complexity in the dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.01432v3</guid>
      <category>math.PR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucian Beznea, Iulian Cimpean, Oana Lupascu-Stamate, Ionel Popescu, Arghir Zarnescu</dc:creator>
    </item>
    <item>
      <title>Integral formulation of Klein-Gordon singular waveguides</title>
      <link>https://arxiv.org/abs/2212.12619</link>
      <description>arXiv:2212.12619v2 Announce Type: replace-cross 
Abstract: We consider the analysis of singular waveguides separating insulating phases in two-space dimensions. The insulating domains are modeled by a massive Schr\"odinger equation and the singular waveguide by appropriate jump conditions along the one-dimensional interface separating the insulators. We present an integral formulation of the problem and analyze its mathematical properties. We also implement a fast multipole and sweeping-accelerated iterative algorithm for solving the integral equations, and demonstrate numerically the fast convergence of this method. Several numerical examples of solutions and scattering effects illustrate our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.12619v2</guid>
      <category>math-ph</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillaume Bal, Jeremy Hoskins, Solomon Quinn, Manas Rachh</dc:creator>
    </item>
    <item>
      <title>An Eulerian hyperbolic model for heat transfer derived via Hamilton's principle: analytical and numerical study</title>
      <link>https://arxiv.org/abs/2305.12229</link>
      <description>arXiv:2305.12229v2 Announce Type: replace-cross 
Abstract: In this paper, we present a new model for heat transfer in compressible fluid flows. The model is derived from Hamilton's principle of stationary action in Eulerian coordinates, in a setting where the entropy conservation is recovered as an Euler--Lagrange equation. The governing system is shown to be hyperbolic. It is asymptotically consistent with the Euler equations for compressible heat conducting fluids, provided the addition of suitable relaxation terms. A study of the Rankine--Hugoniot conditions and the Clausius--Duhem inequality reveals that contact discontinuities cannot exist while expansion waves and compression fans are possible solutions to the governing equations. Evidence of these properties is provided on a set of numerical test cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.12229v2</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1098/rspa.2023.0440</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the Royal Society Volume 480, Issue 2283 (2024)</arxiv:journal_reference>
      <dc:creator>Firas Dhaouadi, Sergey Gavrilyuk</dc:creator>
    </item>
    <item>
      <title>Maximum principle preserving nonlocal diffusion model with Dirichlet boundary condition</title>
      <link>https://arxiv.org/abs/2310.01221</link>
      <description>arXiv:2310.01221v3 Announce Type: replace-cross 
Abstract: In this paper, we propose nonlocal diffusion models with Dirichlet boundary. These nonlocal diffusion models preserve the maximum principle and also have corresponding variational form. With these good properties, we can prove the well-posedness and the vanishing nonlocality convergence. Furthermore, by specifically designed weight function, we can get a nonlocal diffusion model with second order convergence which is optimal for nonlocal diffusion models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01221v3</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanzun Meng, Zuoqiang Shi</dc:creator>
    </item>
    <item>
      <title>Efficient Parallel Data Optimization for Homogeneous Diffusion Inpainting of 4K Images</title>
      <link>https://arxiv.org/abs/2401.06747</link>
      <description>arXiv:2401.06747v2 Announce Type: replace-cross 
Abstract: Homogeneous diffusion inpainting can reconstruct missing image areas with high quality from a sparse subset of known pixels, provided that their location as well as their gray or color values are well optimized. This property is exploited in inpainting-based image compression, which is a promising alternative to classical transform-based codecs such as JPEG and JPEG2000. However, optimizing the inpainting data is a challenging task. Current approaches are either fairly slow or do not produce high quality results. As a remedy we propose fast spatial and tonal optimization algorithms for homogeneous diffusion inpainting that efficiently utilize GPU parallelism, with a careful adaptation of some of the most successful numerical concepts. We propose a densification strategy using ideas from error-map dithering combined with a Delaunay triangulation for the spatial optimization. For the tonal optimization we design a domain decomposition solver that solves the corresponding normal equations in a matrix-free fashion and supplement it with a Voronoi-based initialization strategy. With our proposed methods we are able to generate high quality inpainting masks for homogeneous diffusion and optimized tonal values in a runtime that outperforms prior state-of-the-art by a wide margin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06747v2</guid>
      <category>eess.IV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niklas K\"amper, Vassillen Chizhov, Joachim Weickert</dc:creator>
    </item>
    <item>
      <title>Volume-preserving geometric shape optimization of the Dirichlet energy using variational neural networks</title>
      <link>https://arxiv.org/abs/2407.19064</link>
      <description>arXiv:2407.19064v3 Announce Type: replace-cross 
Abstract: In this work, we explore the numerical solution of geometric shape optimization problems using neural network-based approaches. This involves minimizing a numerical criterion that includes solving a partial differential equation with respect to a domain, often under geometric constraints like constant volume. Our goal is to develop a proof of concept using a flexible and parallelizable methodology to tackle these problems. We focus on a prototypal problem: minimizing the so-called Dirichlet energy with respect to the domain under a volume constraint, involving a Poisson equation in $\mathbb R^2$. We use physics-informed neural networks (PINN) to approximate the Poisson equation's solution on a given domain and represent the shape through a neural network that approximates a volume-preserving transformation from an initial shape to an optimal one. These processes are combined in a single optimization algorithm that minimizes the Dirichlet energy. One of the significant advantages of this approach is its parallelizable nature, which makes it easy to handle the addition of parameters. Additionally, it does not rely on shape derivative or adjoint calculations. Our approach is tested on Dirichlet and Robin boundary conditions, parametric right-hand sides, and extended to Bernoulli-type free boundary problems. The source code for solving the shape optimization problem is open-source and freely available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19064v3</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amaury B\'eli\`eres--Frendo, Emmanuel Franck, Victor Michel-Dansac, Yannick Privat</dc:creator>
    </item>
  </channel>
</rss>
