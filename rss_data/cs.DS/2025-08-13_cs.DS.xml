<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Aug 2025 06:31:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Competitive Online Transportation Simplified</title>
      <link>https://arxiv.org/abs/2508.08381</link>
      <description>arXiv:2508.08381v1 Announce Type: new 
Abstract: The setting for the online transportation problem is a metric space $M$, populated by $m$ parking garages of varying capacities. Over time cars arrive in $M$, and must be irrevocably assigned to a parking garage upon arrival in a way that respects the garage capacities. The objective is to minimize the aggregate distance traveled by the cars. In 1998, Kalyanasundaram and Pruhs conjectured that there is a $(2m-1)$-competitive deterministic algorithm for the online transportation problem, matching the optimal competitive ratio for the simpler online metric matching problem. Recently, Harada and Itoh presented the first $O(m)$-competitive deterministic algorithm for the online transportation problem. Our contribution is an alternative algorithm design and analysis that we believe is simpler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08381v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Arndt, Benjamin Moseley, Kirk Pruhs, Marc Uetz</dc:creator>
    </item>
    <item>
      <title>Two for One, One for All: Deterministic LDC-based Robust Computation in Congested Clique</title>
      <link>https://arxiv.org/abs/2508.08740</link>
      <description>arXiv:2508.08740v1 Announce Type: new 
Abstract: We design a deterministic compiler that makes any computation in the Congested Clique model robust to a constant fraction $\alpha&lt;1$ of adversarial crash faults. In particular, we show how a network of $n$ nodes can compute any circuit of depth $d$, width $\omega$, and gate total fan $\Delta$, in $d\cdot\lceil\frac{\omega}{n^2}+\frac{\Delta}{n}\rceil\cdot 2^{O(\sqrt{\log{n}}\log\log{n})}$ rounds in such a faulty model. As a corollary, any $T$-round Congested Clique algorithm can be compiled into an algorithm that completes in $T^2 n^{o(1)}$ rounds in this model.
  Our compiler obtains resilience to node crashes by coding information across the network, where we leverage locally-decodable codes (LDCs) to maintain a low complexity overhead, as these allow recovering the information needed at each computational step by querying only small parts of the codeword.
  The main technical contribution is that because erasures occur in known locations, which correspond to crashed nodes, we can derandomize classical LDC constructions by deterministically selecting query sets that avoid sufficiently many erasures. Moreover, when decoding multiple codewords in parallel, our derandomization load-balances the queries per-node, thereby preventing congestion and maintaining a low round complexity.
  Deterministic decoding of LDCs presents a new challenge: the adversary can target precisely the (few) nodes that are queried for decoding a certain codeword. We overcome this issue via an adaptive doubling strategy: if a decoding attempt for a codeword fails, the node doubles the number of its decoding attempts. Similarly, when the adversary crashes the decoding node itself, we replace it dynamically with two other non-crashed nodes. By carefully combining these two doubling processes, we overcome the challenges posed by the combination of a deterministic LDC with a worst case pattern of crashes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08740v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keren Censor-Hillel, Orr Fischer, Ran Gelles, Pedro Soto</dc:creator>
    </item>
    <item>
      <title>Robust Scheduling on Uniform Machines - New Results Using a Relaxed Approximation Guarantee</title>
      <link>https://arxiv.org/abs/2508.08979</link>
      <description>arXiv:2508.08979v1 Announce Type: new 
Abstract: We consider the problem of scheduling $n$ jobs on $m$ uniform machines while minimizing the makespan ($Q||C_{\max}$) and maximizing the minimum completion time ($Q||C_{\min}$) in an online setting with migration of jobs. In this online setting, the jobs are inserted or deleted over time, and at each step, the goal is to compute a near-optimal solution while reassigning some jobs, such that the overall processing time of reassigned jobs, called migration, is bounded by some factor $\beta$ times the processing time of the job added or removed.
  We propose Efficient Polynomial Time Approximation Schemes (EPTASs) with an additional load error of $\mathcal{O}(\varepsilon p_{\max})$ for both problems, with constant amortized migration factor $\beta$, where $p_{\max}$ is the maximum processing time in the instance over all steps. As an intermediate step, we obtain Efficient Parameterized Approximation Schemes (EPASs) for both problems, $(1+\varepsilon)$-competitive algorithms parameterized by $p_{\max}$ and the number of different processing times $d$ in an instance, with $\beta$ bounded in a function of $p_{\max}$, $d$ and $\varepsilon$.
  This is the first result in the direction of a polynomial time approximation scheme in the field of online scheduling with bounded reassignment on uniform machines; before, such results were known only for the considered problems on identical machines. Crucial to our result is a division of the machines into large and small machines depending on the current approximate objective value, allowing for different approaches on either machine set, as well as a new way of rounding the instance that does not depend on the current objective value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08979v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hauke Brinkop, David Fischer, Klaus Jansen</dc:creator>
    </item>
    <item>
      <title>Extremely Scalable Distributed Computation of Contour Trees via Pre-Simplification</title>
      <link>https://arxiv.org/abs/2508.08433</link>
      <description>arXiv:2508.08433v1 Announce Type: cross 
Abstract: Contour trees offer an abstract representation of the level set topology in scalar fields and are widely used in topological data analysis and visualization. However, applying contour trees to large-scale scientific datasets remains challenging due to scalability limitations. Recent developments in distributed hierarchical contour trees have addressed these challenges by enabling scalable computation across distributed systems. Building on these structures, advanced analytical tasks -- such as volumetric branch decomposition and contour extraction -- have been introduced to facilitate large-scale scientific analysis. Despite these advancements, such analytical tasks substantially increase memory usage, which hampers scalability. In this paper, we propose a pre-simplification strategy to significantly reduce the memory overhead associated with analytical tasks on distributed hierarchical contour trees. We demonstrate enhanced scalability through strong scaling experiments, constructing the largest known contour tree -- comprising over half a trillion nodes with complex topology -- in under 15 minutes on a dataset containing 550 billion elements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08433v1</guid>
      <category>cs.CG</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingzhe Li, Hamish Carr, Oliver R\"ubel, Bei Wang, Gunther H. Weber</dc:creator>
    </item>
    <item>
      <title>Graph-based method for constructing consensus trees</title>
      <link>https://arxiv.org/abs/2508.08569</link>
      <description>arXiv:2508.08569v1 Announce Type: cross 
Abstract: A consensus tree is a phylogenetic tree that synthesizes a given collection of phylogenetic trees, all of which share the same leaf labels but may have different topologies, typically obtained through bootstrapping. Our research focuses on creating a consensus tree from a collection of phylogenetic trees, each detailed with branch-length data. We integrate branch lengths into the consensus to encapsulate the progression rate of genetic mutations. However, traditional consensus trees, such as the strict consensus tree, primarily focus on the topological structure of these trees, often neglecting the informative value of branch lengths. This oversight disregards a crucial aspect of evolutionary study and highlights a notable gap in traditional phylogenetic approaches. In this paper, we extend \textit{PrimConsTree}\footnote{A preliminary version of this article was presented at \emph{the Fifteenth International Conference on Bioscience, Biochemistry, and Bioinformatics (ICBBB~2025)}~(reference~\cite{torquet2005icbbb}).}, a graph-based method for constructing consensus trees. This algorithm incorporates topological information, edge frequency, clade frequency, and branch length to construct a more robust and comprehensive consensus tree. Our adaptation of the well-known Prim algorithm efficiently identifies the maximum frequency branch and maximum frequency nodes to build the optimal consensus tree. This strategy was pre-processed with clustering steps to calibrate the robustness and accuracy of the consensus tree.\\ \textbf{Availability and implementation:} The source code of PrimConsTree is freely available on GitHub at https://github.com/tahiri-lab/PrimConsTree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08569v1</guid>
      <category>q-bio.PE</category>
      <category>cs.DS</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elio Torquet, Jesper Jansson, Nadia Tahiri</dc:creator>
    </item>
    <item>
      <title>Fast Tensor Completion via Approximate Richardson Iteration</title>
      <link>https://arxiv.org/abs/2502.09534</link>
      <description>arXiv:2502.09534v2 Announce Type: replace 
Abstract: We study tensor completion (TC) through the lens of low-rank tensor decomposition (TD). Many TD algorithms use fast alternating minimization methods to solve highly structured linear regression problems at each step (e.g., for CP, Tucker, and tensor-train decompositions). However, such algebraic structure is often lost in TC regression problems, making direct extensions unclear. This work proposes a novel lifting method for approximately solving TC regression problems using structured TD regression algorithms as blackbox subroutines, enabling sublinear-time methods. We analyze the convergence rate of our approximate Richardson iteration-based algorithm, and our empirical study shows that it can be 100x faster than direct methods for CP completion on real-world tensors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09534v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 42nd International Conference on Machine Learning (ICML 2025)</arxiv:journal_reference>
      <dc:creator>Mehrdad Ghadiri, Matthew Fahrbach, Yunbum Kook, Ali Jadbabaie</dc:creator>
    </item>
    <item>
      <title>Structural Optimal Jacobian Accumulation and Minimum Edge Count are NP-Complete Under Vertex Elimination</title>
      <link>https://arxiv.org/abs/2506.17521</link>
      <description>arXiv:2506.17521v2 Announce Type: replace 
Abstract: We study graph-theoretic formulations of two fundamental problems in algorithmic differentiation. The first (Structural Optimal Jacobian Accumulation) is that of computing a Jacobian while minimizing multiplications. The second (Minimum Edge Count) is to find a minimum-size computational graph. For both problems, we consider the vertex elimination operation. Our main contribution is to show that both problems are NP-complete, thus resolving longstanding open questions. In contrast to prior work, our reduction for Structural Optimal Jacobian Accumulation does not rely on any assumptions about the algebraic relationships between local partial derivatives; we allow these values to be mutually independent. We also provide $O^*(2^n)$-time exact algorithms for both problems, and show that under the exponential time hypothesis these running times are essentially tight. Finally, we provide a data reduction rule for Structural Optimal Jacobian Accumulation by showing that false twins may always be eliminated consecutively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17521v2</guid>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Bentert, Alex Crane, P{\aa}l Gr{\o}n{\aa}s Drange, Yosuke Mizutani, Blair D. Sullivan</dc:creator>
    </item>
    <item>
      <title>Nearly Optimal Bounds for Stochastic Online Sorting</title>
      <link>https://arxiv.org/abs/2508.07823</link>
      <description>arXiv:2508.07823v2 Announce Type: replace 
Abstract: In the online sorting problem, we have an array $A$ of $n$ cells, and receive a stream of $n$ items $x_1,\dots,x_n\in [0,1]$. When an item arrives, we need to immediately and irrevocably place it into an empty cell. The goal is to minimize the sum of absolute differences between adjacent items, which is called the \emph{cost} of the algorithm. It has been shown by Aamand, Abrahamsen, Beretta, and Kleist (SODA 2023) that when the stream $x_1,\dots,x_n$ is generated adversarially, the optimal cost bound for any deterministic algorithm is $\Theta(\sqrt{n})$.
  In this paper, we study the stochastic version of online sorting, where the input items $x_1,\dots,x_n$ are sampled uniformly at random. Despite the intuition that the stochastic version should yield much better cost bounds, the previous best algorithm for stochastic online sorting by Abrahamsen, Bercea, Beretta, Klausen and Kozma (ESA 2024) only achieves $\tilde{O}(n^{1/4})$ cost, which seems far from optimal. We show that stochastic online sorting indeed allows for much more efficient algorithms, by presenting an algorithm that achieves expected cost $\log n\cdot 2^{O(\log^* n)}$. We also prove a cost lower bound of $\Omega(\log n)$, thus show that our algorithm is nearly optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07823v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Hu</dc:creator>
    </item>
    <item>
      <title>Discrete and Continuous Difference of Submodular Minimization</title>
      <link>https://arxiv.org/abs/2506.07952</link>
      <description>arXiv:2506.07952v2 Announce Type: replace-cross 
Abstract: Submodular functions, defined on continuous or discrete domains, arise in numerous applications. We study the minimization of the difference of two submodular (DS) functions, over both domains, extending prior work restricted to set functions. We show that all functions on discrete domains and all smooth functions on continuous domains are DS. For discrete domains, we observe that DS minimization is equivalent to minimizing the difference of two convex (DC) functions, as in the set function case. We propose a novel variant of the DC Algorithm (DCA) and apply it to the resulting DC Program, obtaining comparable theoretical guarantees as in the set function case. The algorithm can be applied to continuous domains via discretization. Experiments demonstrate that our method outperforms baselines in integer compressive sensing and integer least squares.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07952v2</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 42nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025</arxiv:journal_reference>
      <dc:creator>George Orfanides, Tim Hoheisel, Marwa El Halabi</dc:creator>
    </item>
    <item>
      <title>Algorithmic Delegated Choice: An Annotated Reading List</title>
      <link>https://arxiv.org/abs/2508.06562</link>
      <description>arXiv:2508.06562v2 Announce Type: replace-cross 
Abstract: The problem of delegated choice has been of long interest in economics and recently on computer science. We overview a list of papers on delegated choice problem, from classic works to recent papers with algorithmic perspectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06562v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>econ.TH</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad T. Hajiaghayi, Suho Shin</dc:creator>
    </item>
  </channel>
</rss>
