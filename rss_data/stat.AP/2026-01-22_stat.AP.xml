<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 22 Jan 2026 05:01:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Implementing Substance Over Form: A Novel Metric for Taxing E-commerce to Address Deterritorialization</title>
      <link>https://arxiv.org/abs/2601.14616</link>
      <description>arXiv:2601.14616v1 Announce Type: new 
Abstract: Against the backdrop of e-commerce restructuring consumption patterns, last-mile delivery stations have substantially fulfilled the function of community retail distribution. However, the current tax system only levies a low labor service tax on delivery fees, resulting in a tax contribution from the massive circulating goods value that is significantly lower than that of retail supermarkets of equivalent scale. This disparity not only triggers local tax base erosion but also fosters unfair competition. Based on the "substance over form" principle, this paper proposes a tax rate calculation method using "delivery fee plus insurance premium" as the base, corrected through "goods value conversion." This method aims to align the substantive tax burden of e-commerce with that of community retail at the terminal stage, effectively internalizing the high negative externalities of delivery stations through fiscal instruments, addressing E-commerce Deterritorialization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14616v1</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Li Tuobang</dc:creator>
    </item>
    <item>
      <title>Regulatory Expectations for Bayesian Methods in Drug and Biologic Clinical Trials: A Practical Perspective on FDA's 2026 Draft Guidance</title>
      <link>https://arxiv.org/abs/2601.14701</link>
      <description>arXiv:2601.14701v1 Announce Type: new 
Abstract: The U.S. Food and Drug Administration (FDA) released a landmark draft guidance in January 2026 on the use of Bayesian methodology to support primary inference in clinical trials of drugs and biological products. For sponsors, the central message is not merely that ``Bayes is allowed,'' but that Bayesian designs should be justified through explicit success criteria, thoughtful priors (especially when borrowing external information), prospective operating-characteristic evaluation (often via simulation when simulation is used), and computational transparency suitable for regulatory review. This paper provides a practical, regulatory-oriented synthesis of the draft guidance, highlighting where Bayesian designs can be calibrated to traditional frequentist error-rate targets and where, with sponsor--FDA agreement, alternative Bayesian operating metrics may be appropriate. We illustrate expectations through examples discussed in the guidance (e.g., platform trials, external/nonconcurrent controls, pediatric extrapolation) and conclude with an actionable checklist for planning documents and submission packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14701v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Ji, Ph. D</dc:creator>
    </item>
    <item>
      <title>A Practical Guide to Modern Imputation</title>
      <link>https://arxiv.org/abs/2601.14796</link>
      <description>arXiv:2601.14796v1 Announce Type: new 
Abstract: This guide based on recent papers should help researchers avoid some of the most common pitfalls of missing value imputation imputation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14796v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey N\"af</dc:creator>
    </item>
    <item>
      <title>Zero-inflated binary Tree P\'olya splitting regression for multivariate count data</title>
      <link>https://arxiv.org/abs/2601.14815</link>
      <description>arXiv:2601.14815v1 Announce Type: new 
Abstract: Species distribution models (SDMs) are widely used to assess the effects of environmental factors on species distributions. However, classical SDMs ignore inter-species dependencies. Multivariate SDMs (MSDMs), especially those based on latent Gaussian fields such as the multivariate Poisson log-normal (MPLN), address this limitation but face challenges related to computation, dimensionality, and interpretability. P\'olya-splitting (PS) distributions offer an alternative, combining a model for total abundance with a multivariate allocation structure, and have natural interpretations from ecological process models. Yet, they lack flexibility in modeling correlation structures. Tree P\'olya-splitting (TPS) distributions overcome this by introducing hierarchical structure such as a phylogenetic tree. In this paper, we extend TPS to account for zero-inflation, leading to the zero-inflated tree P\'olya-splitting (Z-TPS) family. We detail its statistical properties, show how standard software enables efficient inference, and illustrate its ecological relevance using tree abundance data from over 180 genera across the Congo Basin tropical rainforest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14815v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabrice Moudjieu (ENSPY, UPR For\^ets et Soci\'et\'es), Jean Peyhardi (IMAG), Maxime R\'ejou-M\'echain (UMR AMAP), Patrice Soh Takam (UY1), Fr\'ed\'eric Mortier (UMR AMAP)</dc:creator>
    </item>
    <item>
      <title>New techniques to investigate the AGN-SF connection with integral field spectroscopy</title>
      <link>https://arxiv.org/abs/2601.14542</link>
      <description>arXiv:2601.14542v1 Announce Type: cross 
Abstract: Understanding the connection between active galactic nuclei and star-formation (the AGN-SF connection) is one of the longest standing problems in modern astrophysics. In the age of large Integral Field Unit (IFU) surveys, studies of the AGN-SF connection greatly benefit from spatially resolving AGN and SF contributions to study the two processes independently. Using IFU data for 54 local active galaxies from the S7 sample, we present a new method to separate emission from AGN activity and SF using mixing sequences observed in the [NII]$\lambda 6584$/H$\alpha$ vs. [OIII]$\lambda 5007$/H$\beta$ Baldwin-Phillips-Terlevich (BPT) diagram. We use the new decomposition method to calculate the H$\alpha$ star-formation rate and AGN [OIII] luminosity for the galaxies. Our new method is robust to outliers in the line-ratio distribution and can be applied to large galaxy samples with little manual intervention. We infer star-formation histories (SFHs) using pPXF, conducting detailed recovery tests to determine the quantities that can be considered robust. We test the correlation between the AGN Eddington ratio, using the proxy L[OIII]/$\sigma_*^4$, and star-formation properties. We find a moderately strong correlation between the Eddington ratio and the star-formation rate (SFR). We also observe marginally significant correlations between the AGN Eddington ratio and the light-weighted stellar age under 100 Myr. Our results point to higher AGN accretion being associated with young nuclear star formation under 100 Myr, consistent with timelines presented in previous studies. The correlations found in this paper are relatively weak; extending our methods to larger samples, including radio-quiet galaxies, will help better constrain the physical mechanisms and timescales of the AGN-SF connection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14542v1</guid>
      <category>astro-ph.GA</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aman Chopra (Australian National University, Canberra, ACT 2611, Australia, ARC Centre of Excellence for All Sky Astrophysics in 3 Dimensions), Henry R. M. Zovaro (Australian National University, Canberra, ACT 2611, Australia, ARC Centre of Excellence for All Sky Astrophysics in 3 Dimensions), Rebecca L. Davies (ARC Centre of Excellence for All Sky Astrophysics in 3 Dimensions, Swinburne University of Technology, Hawthorn, VIC 3122, Australia)</dc:creator>
    </item>
    <item>
      <title>Citation of scientific evidence from video description and its association with attention and impact</title>
      <link>https://arxiv.org/abs/2601.14916</link>
      <description>arXiv:2601.14916v1 Announce Type: cross 
Abstract: This study investigates how YouTube content creators utilize scientific evidence in videos. Log-linear regression examines the influence of alternative communication channels on video creators in Biotechnology, using data from 81,302 papers (2018-2023). This reveals a positive association with news articles and Wikipedia pages, but a negative association with scientific papers, policy documents, and patents. Despite the potential for enriching discussions, science video creators seem to favor materials with wider public attention over influential science, technology, and policy papers. These findings suggest a need for improved dissemination strategies for scientific research. Authors, universities, and journals should consider how their work can be made more accessible and engaging for science communicators on video.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14916v1</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Dorta-Gonz\'alez, Mar\'ia Isabel Dorta-Gonz\'alez</dc:creator>
    </item>
    <item>
      <title>Geostatistics from Elliptic Boundary-Value Problems: Green Operators, Transmission Conditions, and Schur Complements</title>
      <link>https://arxiv.org/abs/2601.14937</link>
      <description>arXiv:2601.14937v1 Announce Type: cross 
Abstract: Classical geostatistics encodes spatial dependence by prescribing variograms or covariance kernels on Euclidean domains, whereas the SPDE--GMRF paradigm specifies Gaussian fields through an elliptic precision operator whose inverse is the corresponding Green operator. We develop an operator-based formulation of Gaussian spatial random fields on bounded domains and manifolds with internal interfaces, treating boundary and transmission conditions as explicit components of the statistical model. Starting from coercive quadratic energy functionals, variational theory yields a precise precision--covariance correspondence and shows that variograms are derived quadratic functionals of the Green operator, hence depend on boundary conditions and domain geometry. Conditioning and kriging follow from standard Gaussian update identities in both covariance and precision form, with hard constraints represented equivalently by exact interpolation constraints or by distributional source terms. Interfaces are modelled via surface penalty terms; taking variations produces flux-jump transmission conditions and induces controlled attenuation of cross-interface covariance. Finally, boundary-driven prediction and domain reduction are formulated through Dirichlet-to-Neumann operators and Schur complements, providing an operator language for upscaling, change of support, and subdomain-to-boundary mappings. Throughout, we use tools standard in spatial statistics and elliptic PDE theory to keep boundary and interface effects explicit in covariance modeling and prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14937v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan J. Segura</dc:creator>
    </item>
    <item>
      <title>BlockingPy: approximate nearest neighbours for blocking of records for entity resolution</title>
      <link>https://arxiv.org/abs/2504.04266</link>
      <description>arXiv:2504.04266v4 Announce Type: replace 
Abstract: Entity resolution (probabilistic record linkage, deduplication) is a key step in scientific analysis and data science pipelines involving multiple data sources. The objective of entity resolution is to link records without common unique identifiers that refer to the same entity (e.g., person, company). However, without identifiers, researchers need to specify which records to compare in order to calculate matching probability and reduce computational complexity. One solution is to deterministically block records based on some common variables, such as names, dates of birth or sex or use phonetic algorithms. However, this approach assumes that these variables are free of errors and completely observed, which is often not the case. To address this challenge, we have developed a Python package, BlockingPy, which uses blocking using modern approximate nearest neighbour search and graph algorithms to reduce the number of comparisons. The package supports both CPU and GPU execution. In this paper, we present the design of the package, its functionalities and two case studies related to official statistics. The presented software will be useful for researchers interested in linking data from various sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04266v4</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.CO</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tymoteusz Strojny, Maciej Ber\k{e}sewicz</dc:creator>
    </item>
    <item>
      <title>Dynamic Prediction for Hospital Readmission in Patients with Chronic Heart Failure</title>
      <link>https://arxiv.org/abs/2512.16463</link>
      <description>arXiv:2512.16463v2 Announce Type: replace 
Abstract: Hospital readmission among patients with chronic heart failure (HF) is a major clinical and economic burden. Dynamic prediction models that leverage longitudinal biomarkers may improve risk stratification over traditional static models. This study aims to develop and validate a joint model using longitudinal N-terminal pro-B-type natriuretic peptide (NT-proBNP) measurements to predict the risk of rehospitalization or death in HF patients.
  We analyzed real-world data from the TriNetX database, including patients with an incident HF diagnosis between 2016 and 2022. The final selected cohort included 1,804 patients. A Bayesian joint modeling framework was developed to link patient-specific NT-proBNP trajectories to the risk of a composite endpoint (HF rehospitalization or all-cause mortality) within a 180-day window following hospital discharge. The model's performance was evaluated using 5-fold cross-validation and assessed with the Integrated Brier Score and Integrated Calibration Index.
  The joint model demonstrated a strong predictive advantage over a benchmark static model, particularly when making updated predictions at later time points (180-360 days). A joint model trained on patients with more frequent NT-proBNP measurements achieved the highest accuracy. The main joint model showed excellent calibration, suggesting its risk estimates are reliable.
  Our findings suggest that modeling the full trajectory of NT-proBNP with a joint modeling framework enables more accurate and dynamic risk assessment compared to static, single-timepoint methods. This approach supports the development of adaptive clinical decision-support tools for personalized HF management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16463v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rebecca Farina, Francois Mercier, Christian Wohlfart, Serge Masson, Silvia Metelli</dc:creator>
    </item>
    <item>
      <title>Adversarial Drift-Aware Predictive Transfer: Toward Durable Clinical AI</title>
      <link>https://arxiv.org/abs/2601.11860</link>
      <description>arXiv:2601.11860v2 Announce Type: replace 
Abstract: Clinical AI systems frequently suffer performance decay post-deployment due to temporal data shifts, such as evolving populations, diagnostic coding updates (e.g., ICD-9 to ICD-10), and systemic shocks like the COVID-19 pandemic. Addressing this ``aging'' effect via frequent retraining is often impractical due to computational costs and privacy constraints. To overcome these hurdles, we introduce Adversarial Drift-Aware Predictive Transfer (ADAPT), a novel framework designed to confer durability against temporal drift with minimal retraining. ADAPT innovatively constructs an uncertainty set of plausible future models by combining historical source models and limited current data. By optimizing worst-case performance over this set, it balances current accuracy with robustness against degradation due to future drifts. Crucially, ADAPT requires only summary-level model estimators from historical periods, preserving data privacy and ensuring operational simplicity. Validated on longitudinal suicide risk prediction using electronic health records from Mass General Brigham (2005--2021) and Duke University Health Systems, ADAPT demonstrated superior stability across coding transitions and pandemic-induced shifts. By minimizing annual performance decay without labeling or retraining future data, ADAPT offers a scalable pathway for sustaining reliable AI in high-stakes healthcare environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11860v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xin Xiong, Zijian Guo, Haobo Zhu, Chuan Hong, Jordan W Smoller, Tianxi Cai, Molei Liu</dc:creator>
    </item>
    <item>
      <title>Adaptive partition Factor Analysis</title>
      <link>https://arxiv.org/abs/2410.18939</link>
      <description>arXiv:2410.18939v3 Announce Type: replace-cross 
Abstract: Factor Analysis has traditionally been utilized across diverse disciplines to extrapolate latent traits that influence the behavior of multivariate observed variables. Historically, the focus has been on analyzing data from a single study, neglecting the potential study-specific variations present in data from multiple studies. Multi-study factor analysis has emerged as a recent methodological advancement that addresses this gap by distinguishing between latent traits shared across studies and study-specific components arising from artifactual or population-specific sources of variation. In this paper, we extend the current methodologies by introducing novel shrinkage priors for the latent factors, thereby accommodating a broader spectrum of scenarios -- from the absence of study-specific latent factors to models in which factors pertain only to small subgroups nested within or shared between the studies. For the proposed construction we provide conditions for identifiability of factor loadings and guidelines to perform straightforward posterior computation via Gibbs sampling. Through comprehensive simulation studies, we demonstrate that our proposed method exhibits competing performance across a variety of scenarios compared to existing methods, yet providing richer insights. The practical benefits of our approach are further illustrated through applications to bird species co-occurrence data and ovarian cancer gene expression data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18939v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Bortolato, Antonio Canale</dc:creator>
    </item>
    <item>
      <title>Depth-Based Local Center Clustering: A Framework for Handling Different Clustering Scenarios</title>
      <link>https://arxiv.org/abs/2505.09516</link>
      <description>arXiv:2505.09516v2 Announce Type: replace-cross 
Abstract: Cluster analysis, or clustering, plays a crucial role across numerous scientific and engineering domains. Despite the wealth of clustering methods proposed over the past decades, each method is typically designed for specific scenarios and presents certain limitations in practical applications. In this paper, we propose depth-based local center clustering (DLCC). This novel method makes use of data depth, which is known to produce a center-outward ordering of sample points in a multivariate space. However, data depth typically fails to capture the multimodal characteristics of {data}, something of the utmost importance in the context of clustering. To overcome this, DLCC makes use of a local version of data depth that is based on subsets of {data}. From this, local centers can be identified as well as clusters of varying shapes. Furthermore, we propose a new internal metric based on density-based clustering to evaluate clustering performance on {non-convex clusters}. Overall, DLCC is a flexible clustering approach that seems to overcome some limitations of traditional clustering methods, thereby enhancing data analysis capabilities across a wide range of application scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09516v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyi Wang, Alexandre Leblanc, Paul D. McNicholas</dc:creator>
    </item>
    <item>
      <title>Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems</title>
      <link>https://arxiv.org/abs/2512.11150</link>
      <description>arXiv:2512.11150v3 Announce Type: replace-cross 
Abstract: Measuring long-run LLM outcomes (user satisfaction, expert judgment, downstream KPIs) is expensive. Teams default to cheap LLM judges, but uncalibrated proxies can invert rankings entirely. Causal Judge Evaluation (CJE) makes it affordable to aim at the right target: calibrate cheap scores against a small oracle slice, then evaluate at scale with valid uncertainty. We treat surrogate validity as auditable: for each policy or deployment context, a small oracle audit tests whether the learned calibration remains mean-unbiased, turning an uncheckable identification condition into a falsifiable diagnostic. On 4,961 Chatbot Arena prompts comparing five policies with a 16x oracle/judge cost ratio, at a 5% oracle fraction CJE achieves 99% pairwise ranking accuracy at 14x lower cost; across all configurations (5-50% oracle, varying n), accuracy averages 94%. An adversarial policy fails the transport audit and is correctly flagged; in such cases CJE refuses level claims rather than reporting biased estimates. Key findings: naive confidence intervals on raw judge scores achieve 0% coverage (CJE: ~95%); importance-weighted estimators fail despite &gt;90% effective sample size; and the Coverage-Limited Efficiency (CLE) bound and its TTC diagnostic explain why.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11150v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eddie Landesberg, Manjari Narayan</dc:creator>
    </item>
    <item>
      <title>Scalable Stewardship of an LLM-Assisted Clinical Benchmark with Physician Oversight</title>
      <link>https://arxiv.org/abs/2512.19691</link>
      <description>arXiv:2512.19691v2 Announce Type: replace-cross 
Abstract: We examine the reliability of a widely used clinical AI benchmark whose reference labels were partially generated by LLMs, and find that a substantial fraction are clinically misaligned. We introduce a phased stewardship procedure to amplify the positive impact of physician experts' feedback and then demonstrate, via a controlled RL experiment, how uncaught label bias can materially affect downstream LLM evaluation and alignment. Our results demonstrate that partially LLM-generated labels can embed systemic errors that distort not only evaluation but also downstream model alignment. By adopting a hybrid oversight system, we can prioritize scarce expert feedback to maintain benchmarks as living, clinically-grounded documents. Ensuring this alignment is a prerequisite for the safe deployment of LLMs in high-stakes medical decision support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19691v2</guid>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junze Ye, Daniel Tawfik, Alex J. Goodell, Nikhil V. Kotha, Mark K. Buyyounouski, Mohsen Bayati</dc:creator>
    </item>
    <item>
      <title>The use of spectral indices in environmental monitoring of smouldering coal-waste dumps</title>
      <link>https://arxiv.org/abs/2601.11603</link>
      <description>arXiv:2601.11603v2 Announce Type: replace-cross 
Abstract: The study aimed to evaluate the applicability of environmental indices in the monitoring of smouldering coal-waste dumps. A dump located in the Upper Silesian Coal Basin served as the research site for a multi-method analysis combining remote sensing and field-based data. Two UAV survey campaigns were conducted, capturing RGB, infrared, and multispectral imagery. These were supplemented with direct ground measurements of subsurface temperature and detailed vegetation mapping. Additionally, publicly available satellite data from the Landsat and Sentinel missions were analysed. A range of vegetation and fire-related indices (NDVI, SAVI, EVI, BAI, among others) were calculated to identify thermally active zones and assess vegetation conditions within these degraded areas. The results revealed strong seasonal variability in vegetation indices on thermally active sites, with evidence of disrupted vegetation cycles, including winter greening in moderately heated root zones - a pattern indicative of stress and degradation processes. While satellite data proved useful in reconstructing the fire history of the dump, their spatial resolution was insufficient for detailed monitoring of small-scale thermal anomalies. The study highlights the diagnostic potential of UAV-based remote sensing in post-industrial environments undergoing land degradation but emphasises the importance of field validation for accurate environmental assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11603v2</guid>
      <category>physics.geo-ph</category>
      <category>physics.ins-det</category>
      <category>stat.AP</category>
      <pubDate>Thu, 22 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.rsase.2025.101865</arxiv:DOI>
      <arxiv:journal_reference>Remote Sens. Appl.: Soc. Environ., 41 (2026)</arxiv:journal_reference>
      <dc:creator>Anna Abramowicz, Michal Laska, Adam Nadudvari, Oimahmad Rahmonov</dc:creator>
    </item>
  </channel>
</rss>
