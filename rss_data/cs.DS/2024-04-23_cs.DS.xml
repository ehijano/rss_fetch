<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Apr 2024 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On multidimensional generalization of binary search</title>
      <link>https://arxiv.org/abs/2404.13193</link>
      <description>arXiv:2404.13193v1 Announce Type: new 
Abstract: This work generalizes the binary search problem to a $d$-dimensional domain $S_1\times\cdots\times S_d$, where $S_i=\{0, 1, \ldots,n_i-1\}$ and $d\geq 1$, in the following way. Given $(t_1,\ldots,t_d)$, the target element to be found, the result of a comparison of a selected element $(x_1,\ldots,x_d)$ is the sequence of inequalities each stating that either $t_i &lt; x_i$ or $t_i&gt;x_i$, for $i\in\{1,\ldots,d\}$, for which at least one is correct, and the algorithm does not know the coordinate $i$ on which the correct direction to the target is given. Among other cases, we show asymptotically almost matching lower and upper bounds of the query complexity to be in $\Omega(n^{d-1}/d)$ and $O(n^d)$ for the case of $n_i=n$. In particular, for fixed $d$ these bounds asymptotically do match. This problem is equivalent to the classical binary search in case of one dimension and shows interesting differences for higher dimensions. For example, if one would impose that each of the $d$ inequalities is correct, then the search can be completed in $\log_2\max\{n_1,\ldots,n_d\}$ queries. In an intermediate model when the algorithm knows which one of the inequalities is correct the sufficient number of queries is $\log_2(n_1\cdot\ldots\cdot n_d)$. The latter follows from a graph search model proposed by Emamjomeh-Zadeh et al. [STOC 2016].</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13193v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dariusz Dereniowski, Przemys{\l}aw Gordinowicz, Karolina Wr\'obel</dc:creator>
    </item>
    <item>
      <title>Non-Linear Paging</title>
      <link>https://arxiv.org/abs/2404.13334</link>
      <description>arXiv:2404.13334v1 Announce Type: new 
Abstract: We formulate and study non-linear paging - a broad model of online paging where the size of subsets of pages is determined by a monotone non-linear set function of the pages. This model captures the well-studied classic weighted paging and generalized paging problems, and also submodular and supermodular paging, studied here for the first time, that have a range of applications from virtual memory to machine learning.
  Unlike classic paging, the cache threshold parameter $k$ does not yield good competitive ratios for non-linear paging. Instead, we introduce a novel parameter $\ell$ that generalizes the notion of cache size to the non-linear setting. We obtain a tight deterministic $\ell$-competitive algorithm for general non-linear paging and a $o\left(\log^2 (\ell)\right)$-competitive lower bound for randomized algorithms. Our algorithm is based on a new generic LP for the problem that captures both submodular and supermodular paging, in contrast to LPs used for submodular cover settings. We finally focus on the supermodular paging problem, which is a variant of online set cover and online submodular cover, where sets are repeatedly requested to be removed from the cover. We obtain polylogarithmic lower and upper bounds and an offline approximation algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13334v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilan Doron-Arad (Seffi),  Joseph (Seffi),  Naor</dc:creator>
    </item>
    <item>
      <title>New Structures and Algorithms for Length-Constrained Expander Decompositions</title>
      <link>https://arxiv.org/abs/2404.13446</link>
      <description>arXiv:2404.13446v1 Announce Type: new 
Abstract: Expander decompositions form the basis of one of the most flexible paradigms for close-to-linear-time graph algorithms. Length-constrained expander decompositions generalize this paradigm to better work for problems with lengths, distances and costs. Roughly, an $(h,s)$-length $\phi$-expander decomposition is a small collection of length increases to a graph so that nodes within distance $h$ can route flow over paths of length $hs$ with congestion at most $1/\phi$.
  In this work, we give a close-to-linear time algorithm for computing length-constrained expander decompositions in graphs with general lengths and capacities. Notably, and unlike previous works, our algorithm allows for one to trade off off between the size of the decomposition and the length of routing paths: for any $\epsilon &gt; 0$ not too small, our algorithm computes in close-to-linear time an $(h,s)$-length $\phi$-expander decomposition of size $m \cdot \phi \cdot n^\epsilon$ where $s = \exp(\text{poly}(1/\epsilon))$. The key foundations of our algorithm are: (1) a simple yet powerful structural theorem which states that the union of a sequence of sparse length-constrained cuts is itself sparse and (2) new algorithms for efficiently computing sparse length-constrained flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13446v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernhard Haeupler, D Ellis Hershkowitz, Zihan Tan</dc:creator>
    </item>
    <item>
      <title>An Optimal MPC Algorithm for Subunit-Monge Matrix Multiplication, with Applications to LIS</title>
      <link>https://arxiv.org/abs/2404.13486</link>
      <description>arXiv:2404.13486v1 Announce Type: new 
Abstract: We present an $O(1)$-round fully-scalable deterministic massively parallel algorithm for computing the min-plus matrix multiplication of unit-Monge matrices. We use this to derive a $O(\log n)$-round fully-scalable massively parallel algorithm for solving the exact longest increasing subsequence (LIS) problem. For a fully-scalable MPC regime, this result substantially improves the previously known algorithm of $O(\log^4 n)$-round complexity, and matches the best algorithm for computing the $(1+\epsilon)$-approximation of LIS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13486v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3626183.3659974</arxiv:DOI>
      <dc:creator>Jaehyun Koo</dc:creator>
    </item>
    <item>
      <title>Optimal Non-Adaptive Tolerant Junta Testing via Local Estimators</title>
      <link>https://arxiv.org/abs/2404.13502</link>
      <description>arXiv:2404.13502v1 Announce Type: new 
Abstract: We give a non-adaptive algorithm that makes $2^{\tilde{O}(\sqrt{k\log(1/\varepsilon_2 - \varepsilon_1)})}$ queries to a Boolean function $f:\{\pm 1\}^n \rightarrow \{\pm 1\}$ and distinguishes between $f$ being $\varepsilon_1$-close to some $k$-junta versus $\varepsilon_2$-far from every $k$-junta. At the heart of our algorithm is a local mean estimation procedure for Boolean functions that may be of independent interest. We complement our upper bound with a matching lower bound, improving a recent lower bound obtained by Chen et al. We thus obtain the first tight bounds for a natural property of Boolean functions in the tolerant testing model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13502v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shivam Nadimpalli, Shyamal Patel</dc:creator>
    </item>
    <item>
      <title>Stochastic Multi-round Submodular Optimization with Budget</title>
      <link>https://arxiv.org/abs/2404.13737</link>
      <description>arXiv:2404.13737v1 Announce Type: new 
Abstract: In this work we study the problem of Stochastic Budgeted Multi-round Submodular Maximization (SBMSm), in which we would like to maximize the sum over multiple rounds of the value of a monotone and submodular objective function, subject to the fact that the values of this function depend on the realization of stochastic events and the number of observations that we can make over all rounds is limited by a given budget. This problem extends, and generalizes to multiple round settings, well-studied problems such as (adaptive) influence maximization and stochastic probing.
  We first show that whenever a certain single-round optimization problem can be optimally solved in polynomial time, then there is a polynomial time dynamic programming algorithm that returns the same solution as the optimal algorithm, that can adaptively choose both which observations to make and in which round to have them. Unfortunately, this dynamic programming approach cannot be extended to work when the single-round optimization problem cannot be efficiently solved (even if we allow it would be approximated within an arbitrary small constant). Anyway, in this case we are able to provide a simple greedy algorithm for the problem. It guarantees a $(1/2-\epsilon)$-approximation to the optimal value, even if it non-adaptively allocates the budget to rounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13737v1</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincenzo Auletta, Diodato Ferraioli, Cosimo Vinci</dc:creator>
    </item>
    <item>
      <title>Sublinear Time Low-Rank Approximation of Toeplitz Matrices</title>
      <link>https://arxiv.org/abs/2404.13757</link>
      <description>arXiv:2404.13757v1 Announce Type: new 
Abstract: We present a sublinear time algorithm for computing a near optimal low-rank approximation to any positive semidefinite (PSD) Toeplitz matrix $T\in \mathbb{R}^{d\times d}$, given noisy access to its entries. In particular, given entrywise query access to $T+E$ for an arbitrary noise matrix $E\in \mathbb{R}^{d\times d}$, integer rank $k\leq d$, and error parameter $\delta&gt;0$, our algorithm runs in time $\text{poly}(k,\log(d/\delta))$ and outputs (in factored form) a Toeplitz matrix $\widetilde{T} \in \mathbb{R}^{d \times d}$ with rank $\text{poly}(k,\log(d/\delta))$ satisfying, for some fixed constant $C$, \begin{equation*}
  \|T-\widetilde{T}\|_F \leq C \cdot \max\{\|E\|_F,\|T-T_k\|_F\} + \delta \cdot \|T\|_F. \end{equation*} Here $\|\cdot \|_F$ is the Frobenius norm and $T_k$ is the best (not necessarily Toeplitz) rank-$k$ approximation to $T$ in the Frobenius norm, given by projecting $T$ onto its top $k$ eigenvectors.
  Our result has the following applications. When $E = 0$, we obtain the first sublinear time near-relative-error low-rank approximation algorithm for PSD Toeplitz matrices, resolving the main open problem of Kapralov et al. SODA `23, whose algorithm had sublinear query complexity but exponential runtime. Our algorithm can also be applied to approximate the unknown Toeplitz covariance matrix of a multivariate Gaussian distribution, given sample access to this distribution, resolving an open question of Eldar et al. SODA `20.
  Our algorithm applies sparse Fourier transform techniques to recover a low-rank Toeplitz matrix using its Fourier structure. Our key technical contribution is the first polynomial time algorithm for \emph{discrete time off-grid} sparse Fourier recovery, which may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13757v1</guid>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cameron Musco, Kshiteej Sheth</dc:creator>
    </item>
    <item>
      <title>Faster Algorithms for Dual-Failure Replacement Paths</title>
      <link>https://arxiv.org/abs/2404.13907</link>
      <description>arXiv:2404.13907v1 Announce Type: new 
Abstract: Given a simple weighted directed graph $G = (V, E, \omega)$ on $n$ vertices as well as two designated terminals $s, t\in V$, our goal is to compute the shortest path from $s$ to $t$ avoiding any pair of presumably failed edges $f_1, f_2\in E$, which is a natural generalization of the classical replacement path problem which considers single edge failures only.
  This dual failure replacement paths problem was recently studied by Vassilevska Williams, Woldeghebriel and Xu [FOCS 2022] who designed a cubic time algorithm for general weighted digraphs which is conditionally optimal; in the same paper, for unweighted graphs where $\omega \equiv 1$, the authors presented an algebraic algorithm with runtime $\tilde{O}(n^{2.9146})$, as well as a conditional lower bound of $n^{8/3-o(1)}$ against combinatorial algorithms. However, it was unknown in their work whether fast matrix multiplication is necessary for a subcubic runtime in unweighted digraphs.
  As our primary result, we present the first truly subcubic combinatorial algorithm for dual failure replacement paths in unweighted digraphs. Our runtime is $\tilde{O}(n^{3-1/18})$. Besides, we also study algebraic algorithms for digraphs with small integer edge weights from $\{-M, -M+1, \cdots, M-1, M\}$. As our secondary result, we obtained a runtime of $\tilde{O}(Mn^{2.8716})$, which is faster than the previous bound of $\tilde{O}(M^{2/3}n^{2.9144} + Mn^{2.8716})$ from [Vassilevska Williams, Woldeghebriela and Xu, 2022].</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13907v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiri Chechik, Tianyi Zhang</dc:creator>
    </item>
    <item>
      <title>Engineering Edge Orientation Algorithms</title>
      <link>https://arxiv.org/abs/2404.13997</link>
      <description>arXiv:2404.13997v1 Announce Type: new 
Abstract: Given an undirected graph G, the edge orientation problem asks for assigning a direction to each edge to convert G into a directed graph. The aim is to minimize the maximum out degree of a vertex in the resulting directed graph. This problem, which is solvable in polynomial time, arises in many applications. An ongoing challenge in edge orientation algorithms is their scalability, particularly in handling large-scale networks with millions or billions of edges efficiently. We propose a novel algorithmic framework based on finding and manipulating simple paths to face this challenge. Our framework is based on an existing algorithm and allows many algorithmic choices. By carefully exploring these choices and engineering the underlying algorithms, we obtain an implementation which is more efficient and scalable than the current state-of-the-art. Our experiments demonstrate significant performance improvements compared to state-of-the-art solvers. On average our algorithm is 6.59 times faster when compared to the state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13997v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H. Reinst\"adtler, C. Schulz, B. U\c{c}ar</dc:creator>
    </item>
    <item>
      <title>Decline and Fall of the ICALP 2008 Modular Decomposition algorithm</title>
      <link>https://arxiv.org/abs/2404.14049</link>
      <description>arXiv:2404.14049v1 Announce Type: new 
Abstract: We provide a counterexample to a crucial lemma in the ICALP 2008 paper "Simpler Linear-Time Modular Decomposition Via Recursive Factorizing Permutations", invalidating the algorithm described there.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14049v1</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>William Atherton, Dmitrii V. Pasechnik</dc:creator>
    </item>
    <item>
      <title>A Tight Subexponential-time Algorithm for Two-Page Book Embedding</title>
      <link>https://arxiv.org/abs/2404.14087</link>
      <description>arXiv:2404.14087v1 Announce Type: new 
Abstract: A book embedding of a graph is a drawing that maps vertices onto a line and edges to simple pairwise non-crossing curves drawn into pages, which are half-planes bounded by that line. Two-page book embeddings, i.e., book embeddings into 2 pages, are of special importance as they are both NP-hard to compute and have specific applications. We obtain a 2^(O(\sqrt{n})) algorithm for computing a book embedding of an n-vertex graph on two pages -- a result which is asymptotically tight under the Exponential Time Hypothesis. As a key tool in our approach, we obtain a single-exponential fixed-parameter algorithm for the same problem when parameterized by the treewidth of the input graph. We conclude by establishing the fixed-parameter tractability of computing minimum-page book embeddings when parameterized by the feedback edge number, settling an open question arising from previous work on the problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14087v1</guid>
      <category>cs.DS</category>
      <category>cs.CG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Ganian, Haiko Mueller, Sebastian Ordyniak, Giacomo Paesani, Mateusz Rychlicki</dc:creator>
    </item>
    <item>
      <title>Semirandom Planted Clique and the Restricted Isometry Property</title>
      <link>https://arxiv.org/abs/2404.14159</link>
      <description>arXiv:2404.14159v1 Announce Type: new 
Abstract: We give a simple, greedy $O(n^{\omega+0.5})=O(n^{2.872})$-time algorithm to list-decode planted cliques in a semirandom model introduced in [CSV17] (following [FK01]) that succeeds whenever the size of the planted clique is $k\geq O(\sqrt{n} \log^2 n)$. In the model, the edges touching the vertices in the planted $k$-clique are drawn independently with probability $p=1/2$ while the edges not touching the planted clique are chosen by an adversary in response to the random choices. Our result shows that the computational threshold in the semirandom setting is within a $O(\log^2 n)$ factor of the information-theoretic one [Ste17] thus resolving an open question of Steinhardt. This threshold also essentially matches the conjectured computational threshold for the well-studied special case of fully random planted clique.
  All previous algorithms [CSV17, MMT20, BKS23] in this model are based on rather sophisticated rounding algorithms for entropy-constrained semidefinite programming relaxations and their sum-of-squares strengthenings and the best known guarantee is a $n^{O(1/\epsilon)}$-time algorithm to list-decode planted cliques of size $k \geq \tilde{O}(n^{1/2+\epsilon})$. In particular, the guarantee trivializes to quasi-polynomial time if the planted clique is of size $O(\sqrt{n} \operatorname{polylog} n)$. Our algorithm achieves an almost optimal guarantee with a surprisingly simple greedy algorithm.
  The prior state-of-the-art algorithmic result above is based on a reduction to certifying bounds on the size of unbalanced bicliques in random graphs -- closely related to certifying the restricted isometry property (RIP) of certain random matrices and known to be hard in the low-degree polynomial model. Our key idea is a new approach that relies on the truth of -- but not efficient certificates for -- RIP of a new class of matrices built from the input graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14159v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaros{\l}aw B{\l}asiok, Rares-Darius Buhai, Pravesh K. Kothari, David Steurer</dc:creator>
    </item>
    <item>
      <title>Minimizing the Number of Tardy Jobs with Uniform Processing Times on Parallel Machines</title>
      <link>https://arxiv.org/abs/2404.14208</link>
      <description>arXiv:2404.14208v1 Announce Type: new 
Abstract: In this work, we study the computational (parameterized) complexity of $P \mid r_j, p_j=p \mid \sum_j w_j U_j$. Here, we are given $m$ identical parallel machines and $n$ jobs with equal processing time, each characterized by a release date, a due date, and a weight. The task is to find a feasible schedule, that is, an assignment of the jobs to starting times on machines, such that no job starts before its release date and no machine processes several jobs at the same time, that minimizes the weighted number of tardy jobs. A job is considered tardy if it finishes after its due date.
  Our main contribution is showing that $P \mid r_j, p_j=p \mid \sum_j U_j$ (the unweighted version of the problem) is NP-hard and W[2]-hard when parameterized by the number of machines. The former resolves an open problem in Note 2.1.19 by Kravchenko and Werner [Journal of Scheduling, 2011] and Open Problem 2 by Sgall [ESA, 2012], and the latter resolves Open Problem 7 by Mnich and van Bevern [Computers &amp; Operations Research, 2018]. Furthermore, our result shows that the known XP-algorithm for $P \mid r_j, p_j=p \mid \sum_j w_j U_j$ parameterized by the number of machines is optimal from a classification standpoint.
  On the algorithmic side, we provide alternative running time bounds for the above-mentioned known XP-algorithm. Our analysis shows that $P \mid r_j, p_j=p \mid \sum_j w_j U_j$ is contained in XP when parameterized by the processing time, and that it is contained in FPT when parameterized by the combination of the number of machines and the processing time. Finally, we give an FPT-algorithm for $P \mid r_j, p_j=p \mid \sum_j w_j U_j$ parameterized by the number of release dates or the number of due dates. With this work, we lay out the foundation for a systematic study of the parameterized complexity of $P \mid r_j, p_j=p \mid \sum_j w_j U_j$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14208v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Klaus Heeger, Hendrik Molter</dc:creator>
    </item>
    <item>
      <title>Computing the LCP Array of a Labeled Graph</title>
      <link>https://arxiv.org/abs/2404.14235</link>
      <description>arXiv:2404.14235v1 Announce Type: new 
Abstract: The LCP array is an important tool in stringology, allowing to speed up pattern matching algorithms and enabling compact representations of the suffix tree. Recently, Conte et al. [DCC 2023] and Cotumaccio et al. [SPIRE 2023] extended the definition of this array to Wheeler DFAs and, ultimately, to arbitrary labeled graphs, proving that it can be used to efficiently solve matching statistics queries on the graph's paths. In this paper, we provide the first efficient algorithm building the LCP array of a directed labeled graph with $n$ nodes and $m$ edges labeled over an alphabet of size $\sigma$. After arguing that the natural generalization of a compact-space LCP-construction algorithm by Beller et al. [J. Discrete Algorithms 2013] runs in time $\Omega(n\sigma)$, we present a new algorithm based on dynamic range stabbing building the LCP array in $O(n\log \sigma)$ time and $O(n\log\sigma)$ bits of working space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14235v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jarno Alanko, Davide Cenzato, Nicola Cotumaccio, Sung-Hwan Kim, Giovanni Manzini, Nicola Prezza</dc:creator>
    </item>
    <item>
      <title>Declarative Concurrent Data Structures</title>
      <link>https://arxiv.org/abs/2404.13359</link>
      <description>arXiv:2404.13359v1 Announce Type: cross 
Abstract: Implementing concurrent data structures is challenging and requires a deep understanding of concurrency concepts and careful design to ensure correctness, performance, and scalability. Further, composing operations on two or more concurrent data structures often requires a synchronization wrapper to ensure the operations are applied together atomically, resulting in serialization and, thereby, giving up the performance benefit of the individual data structures. DBMS provides generalized concurrency control (CC) and is a good fit for implementing concurrent data structures. However, DBMSs are over-generalized for this use case, which fails to match the performance of specialized implementations.
  This paper makes the case for the Declarative Concurrent Data Structures (DCDS) framework for automatically generating concurrent data structures from a serial specification. In DCDS, users declare the attributes and methods needed for their desired data structure through an embedded DSL at design time. DCDS automatically injects CC at build-time, generating a concurrent intermediate representation (IR) compiled into machine code. A declarative interface for designing data structure enables efficient composability through co-optimizing component structures; optimizations are applied to both the composed serial specification and the generated concurrent IR. We realize the DCDS framework in our prototype system Rosti and experimentally show that data structures declared in Rosti can be efficiently composed by co-optimizing their logical functionality and the generated CC protocol. Our evaluation shows that composing a map and a list to create an LRU container can benefit up to 2X performance scalability in Rosti compared to an open-source library. We demonstrate the applicability of DCDS as an in-process OLTP by comparing it with in-memory DBMS, Proteus, and showing up to 2X performance gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13359v1</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.PL</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aun Raza, Hamish Nicholson, Ioanna Tsakalidou, Anna Herlihy, Prathamesh Tagore, Anastasia Ailamaki</dc:creator>
    </item>
    <item>
      <title>Predict to Minimize Swap Regret for All Payoff-Bounded Tasks</title>
      <link>https://arxiv.org/abs/2404.13503</link>
      <description>arXiv:2404.13503v1 Announce Type: cross 
Abstract: A sequence of predictions is calibrated if and only if it induces no swap regret to all down-stream decision tasks. We study the Maximum Swap Regret (MSR) of predictions for binary events: the swap regret maximized over all downstream tasks with bounded payoffs. Previously, the best online prediction algorithm for minimizing MSR is obtained by minimizing the K1 calibration error, which upper bounds MSR up to a constant factor. However, recent work (Qiao and Valiant, 2021) gives an ${\Omega}(T^{0.528})$ lower bound for the worst-case expected K1 calibration error incurred by any randomized algorithm in T rounds, presenting a barrier to achieving better rates for MSR. Several relaxations of MSR have been considered to overcome this barrier, via external regret (Kleinberg et al., 2023) and regret bounds depending polynomially on the number of actions in downstream tasks (Noarov et al., 2023; Roth and Shi, 2024). We show that the barrier can be surpassed without any relaxations: we give an efficient randomized prediction algorithm that guarantees $O(TlogT)$ expected MSR. We also discuss the economic utility of calibration by viewing MSR as a decision-theoretic calibration error metric and study its relationship to existing metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13503v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lunjia Hu, Yifan Wu</dc:creator>
    </item>
    <item>
      <title>Individual Rationality in Topological Distance Games is Surprisingly Hard</title>
      <link>https://arxiv.org/abs/2404.14128</link>
      <description>arXiv:2404.14128v1 Announce Type: cross 
Abstract: In the recently introduced topological distance games, strategic agents need to be assigned to a subset of vertices of a topology. In the assignment, the utility of an agent depends on both the agent's inherent utilities for other agents and its distance from them on the topology. We study the computational complexity of finding individually rational outcomes; this notion is widely assumed to be the very minimal stability requirement and requires that the utility of every agent in a solution is non-negative. We perform a comprehensive study of the problem's complexity, and we prove that even in very basic cases, deciding whether an individually rational solution exists is intractable. To reach at least some tractability, one needs to combine multiple restrictions of the input instance, including the number of agents and the topology and the influence of distant agents on the utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14128v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Argyrios Deligkas, Eduard Eiben, Du\v{s}an Knop, \v{S}imon Schierreich</dc:creator>
    </item>
    <item>
      <title>Construction of Sparse Suffix Trees and LCE Indexes in Optimal Time and Space</title>
      <link>https://arxiv.org/abs/2105.03782</link>
      <description>arXiv:2105.03782v3 Announce Type: replace 
Abstract: The notions of synchronizing and partitioning sets are recently introduced variants of locally consistent parsings with great potential in problem-solving. In this paper we propose a deterministic algorithm that constructs for a given readonly string of length $n$ over the alphabet $\{0,1,\ldots,n^{\mathcal{O}(1)}\}$ a variant of $\tau$-partitioning set with size $\mathcal{O}(b)$ and $\tau = \frac{n}{b}$ using $\mathcal{O}(b)$ space and $\mathcal{O}(\frac{1}{\epsilon}n)$ time provided $b \ge n^\epsilon$, for $\epsilon &gt; 0$. As a corollary, for $b \ge n^\epsilon$ and constant $\epsilon &gt; 0$, we obtain linear construction algorithms with $\mathcal{O}(b)$ space on top of the string for two major small-space indexes: a sparse suffix tree, which is a compacted trie built on $b$ chosen suffixes of the string, and a longest common extension (LCE) index, which occupies $\mathcal{O}(b)$ space and allows us to compute the longest common prefix for any pair of substrings in $\mathcal{O}(n/b)$ time. For both, the $\mathcal{O}(b)$ construction storage is asymptotically optimal since the tree itself takes $\mathcal{O}(b)$ space and any LCE index with $\mathcal{O}(n/b)$ query time must occupy at least $\mathcal{O}(b)$ space by a known trade-off (at least for $b \ge \Omega(n / \log n)$). In case of arbitrary $b \ge \Omega(\log^2 n)$, we present construction algorithms for the partitioning set, sparse suffix tree, and LCE index with $\mathcal{O}(n\log_b n)$ running time and $\mathcal{O}(b)$ space, thus also improving the state of the art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.03782v3</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dmitry Kosolobov, Nikita Sivukhin</dc:creator>
    </item>
    <item>
      <title>Optimal Adjacency Labels for Subgraphs of Cartesian Products</title>
      <link>https://arxiv.org/abs/2206.02872</link>
      <description>arXiv:2206.02872v3 Announce Type: replace 
Abstract: For any hereditary graph class $F$, we construct optimal adjacency labeling schemes for the classes of subgraphs and induced subgraphs of Cartesian products of graphs in $F$. As a consequence, we show that, if $F$ admits efficient adjacency labels (or, equivalently, small induced-universal graphs) meeting the information-theoretic minimum, then the classes of subgraphs and induced subgraphs of Cartesian products of graphs in $F$ do too. Our proof uses ideas from randomized communication complexity, hashing, and additive combinatorics, and improves upon recent results of Chepoi, Labourel, and Ratel [Journal of Graph Theory, 2020].</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.02872v3</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Louis Esperet, Nathaniel Harms, Viktor Zamaraev</dc:creator>
    </item>
    <item>
      <title>Scalable Multilevel and Memetic Signed Graph Clustering</title>
      <link>https://arxiv.org/abs/2208.13618</link>
      <description>arXiv:2208.13618v2 Announce Type: replace 
Abstract: In this study, we address the complex issue of graph clustering in signed graphs, which are characterized by positive and negative weighted edges representing attraction and repulsion among nodes, respectively. The primary objective is to efficiently partition the graph into clusters, ensuring that nodes within a cluster are closely linked by positive edges while minimizing negative edge connections between them. To tackle this challenge, we first develop a scalable multilevel algorithm based on label propagation and FM local search. Then we develop a memetic algorithm that incorporates a multilevel strategy. This approach meticulously combines elements of evolutionary algorithms with local refinement techniques, aiming to explore the search space more effectively than repeated executions. Our experimental analysis reveals that this our new algorithms significantly outperforms existing state-of-the-art algorithms. For example, our memetic algorithm can reach solution quality of the previous state-of-the-art algorithm up to four orders of magnitude faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.13618v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix Hausberger, Marcelo Fonseca Faraj, Christian Schulz</dc:creator>
    </item>
    <item>
      <title>Folklore Sampling is Optimal for Exact Hopsets: Confirming the $\sqrt{n}$ Barrier</title>
      <link>https://arxiv.org/abs/2304.02193</link>
      <description>arXiv:2304.02193v3 Announce Type: replace 
Abstract: For a graph $G$, a $D$-diameter-reducing exact hopset is a small set of additional edges $H$ that, when added to $G$, maintains its graph metric but guarantees that all node pairs have a shortest path in $G \cup H$ using at most $D$ edges. A shortcut set is the analogous concept for reachability. These objects have been studied since the early '90s due to applications in parallel, distributed, dynamic, and streaming graph algorithms.
  For most of their history, the state-of-the-art construction for either object was a simple folklore algorithm, based on randomly sampling nodes to hit long paths in the graph. However, recent breakthroughs of Kogan and Parter [SODA '22] and Bernstein and Wein [SODA '23] have finally improved over the folklore diameter bound of $\widetilde{O}(n^{1/2})$ for shortcut sets and for $(1+\epsilon)$-approximate hopsets. For both objects it is now known that one can use $O(n)$ hop-edges to reduce diameter to $\widetilde{O}(n^{1/3})$. The only setting where folklore sampling remains unimproved is for exact hopsets. Can these improvements be continued?
  We settle this question negatively by constructing graphs on which any exact hopset of $O(n)$ edges has diameter $\widetilde{\Omega}(n^{1/2})$. This improves on the previous lower bound of $\widetilde{\Omega}(n^{1/3})$ by Kogan and Parter [FOCS '22]. Using similar ideas, we also polynomially improve the current lower bounds for shortcut sets, constructing graphs on which any shortcut set of $O(n)$ edges reduces diameter to $\widetilde{\Omega}(n^{1/4})$. This improves on the previous lower bound of $\Omega(n^{1/6})$ by Huang and Pettie [SIAM J. Disc. Math. '18]. We also extend our constructions to provide lower bounds against $O(p)$-size exact hopsets and shortcut sets for other values of $p$; in particular, we show that folklore sampling is near-optimal for exact hopsets in the entire range of $p \in [1, n^2]$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.02193v3</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Greg Bodwin, Gary Hoppenworth</dc:creator>
    </item>
    <item>
      <title>Hypergraph dualization with FPT-delay parameterized by the degeneracy and dimension</title>
      <link>https://arxiv.org/abs/2305.06974</link>
      <description>arXiv:2305.06974v4 Announce Type: replace 
Abstract: At STOC 2002, Eiter, Gottlob, and Makino presented a technique called ordered generation that yields an $n^{O(d)}$-delay algorithm listing all minimal transversals of an $n$-vertex hypergraph of degeneracy $d$. Recently at IWOCA 2019, Conte, Kant\'e, Marino, and Uno asked whether this XP-delay algorithm parameterized by $d$ could be made FPT-delay for a weaker notion of degeneracy, or even parameterized by the maximum degree $\Delta$, i.e., whether it can be turned into an algorithm with delay $f(\Delta)\cdot n^{O(1)}$ for some computable function $f$. Moreover, and as a first step toward answering that question, they note that they could not achieve these time bounds even for the particular case of minimal dominating sets enumeration. In this paper, using ordered generation, we show that an FPT-delay algorithm can be devised for minimal transversals enumeration parameterized by the degeneracy and dimension, giving a positive and more general answer to the latter question.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.06974v4</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Valentin Bartier, Oscar Defrain, Fionn Mc Inerney</dc:creator>
    </item>
    <item>
      <title>A Simple $(1-\epsilon)$-Approximation Semi-Streaming Algorithm for Maximum (Weighted) Matching</title>
      <link>https://arxiv.org/abs/2307.02968</link>
      <description>arXiv:2307.02968v2 Announce Type: replace 
Abstract: We present a simple semi-streaming algorithm for $(1-\epsilon)$-approximation of bipartite matching in $O(\log{\!(n)}/\epsilon)$ passes. This matches the performance of state-of-the-art "$\epsilon$-efficient" algorithms -- the ones with much better dependence on $\epsilon$ albeit with some mild dependence on $n$ -- while being considerably simpler.
  The algorithm relies on a direct application of the multiplicative weight update method with a self-contained primal-dual analysis that can be of independent interest. To show case this, we use the same ideas, alongside standard tools from matching theory, to present an equally simple semi-streaming algorithm for $(1-\epsilon)$-approximation of weighted matchings in general (not necessarily bipartite) graphs, again in $O(\log{\!(n)}/\epsilon)$ passes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.02968v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sepehr Assadi</dc:creator>
    </item>
    <item>
      <title>Lower Bounds for Matroid Optimization Problems with a Linear Constraint</title>
      <link>https://arxiv.org/abs/2307.07773</link>
      <description>arXiv:2307.07773v2 Announce Type: replace 
Abstract: We study a family of matroid optimization problems with a linear constraint (MOL). In these problems, we seek a subset of elements which optimizes (i.e., maximizes or minimizes) a linear objective function subject to (i) a matroid independent set, or a matroid basis constraint, (ii) additional linear constraint. A notable member in this family is budgeted matroid independent set (BM), which can be viewed as classic $0/1$-knapsack with a matroid constraint. While special cases of BM, such as knapsack with cardinality constraint and multiple-choice knapsack, admit a fully polynomial-time approximation scheme (Fully PTAS), the best known result for BM on a general matroid is an Efficient PTAS. Prior to this work, the existence of a Fully PTAS for BM, and more generally, for any problem in the family of MOL problems, has been open.
  In this paper, we answer this question negatively by showing that none of the (non-trivial) problems in this family admits a Fully PTAS. This resolves the complexity status of several well studied problems. Our main result is obtained by showing first that exact weight matroid basis (EMB) does not admit a pseudo-polynomial time algorithm. This distinguishes EMB from the special cases of $k$-subset sum and EMB on a linear matroid, which are solvable in pseudo-polynomial time. We then obtain unconditional hardness results for the family of MOL problems in the oracle model (even if randomization is allowed), and show that the same results hold when the matroids are encoded as part of the input, assuming $P \neq NP$. For the hardness proof of EMB, we introduce the $\Pi$-matroid family. This intricate subclass of matroids, which exploits the interaction between a weight function and the matroid constraint, may find use in tackling other matroid optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.07773v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilan Doron-Arad, Ariel Kulik, Hadas Shachnai</dc:creator>
    </item>
    <item>
      <title>On the Length of Strongly Monotone Descending Chains over $\mathbb{N}^d$</title>
      <link>https://arxiv.org/abs/2310.02847</link>
      <description>arXiv:2310.02847v2 Announce Type: replace 
Abstract: A recent breakthrough by K\"unnemann, Mazowiecki, Sch\"utze, Sinclair-Banks, and Wegrzycki (ICALP, 2023) bounds the running time for the coverability problem in $d$-dimensional vector addition systems under unary encoding to $n^{2^{O(d)}}$, improving on Rackoff's $n^{2^{O(d\lg d)}}$ upper bound (Theor. Comput. Sci., 1978), and provides conditional matching lower bounds.
  In this paper, we revisit Lazi\'c and Schmitz' "ideal view" of the backward coverability algorithm (Inform. Comput., 2021) in the light of this breakthrough. We show that the controlled strongly monotone descending chains of downwards-closed sets over $\mathbb{N}^d$ that arise from the dual backward coverability algorithm of Lazi\'c and Schmitz on $d$-dimensional unary vector addition systems also enjoy this tight $n^{2^{O(d)}}$ upper bound on their length, and that this also translates into the same bound on the running time of the backward coverability algorithm.
  Furthermore, our analysis takes place in a more general setting than that of Lazi\'c and Schmitz, which allows to show the same results and improve on the 2EXPSPACE upper bound derived by Benedikt, Duff, Sharad, and Worrell (LICS, 2017) for the coverability problem in invertible affine nets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02847v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sylvain Schmitz, Lia Sch\"utze</dc:creator>
    </item>
    <item>
      <title>Local Max-Cut on Sparse Graphs</title>
      <link>https://arxiv.org/abs/2311.00182</link>
      <description>arXiv:2311.00182v3 Announce Type: replace 
Abstract: We bound the smoothed running time of the FLIP algorithm for local Max-Cut as a function of $\alpha$, the arboricity of the input graph. We show that, with high probability and in expectation, the following holds (where $n$ is the number of nodes and $\phi$ is the smoothing parameter):
  1) When $\alpha = O(\log^{1-\delta} n)$ FLIP terminates in $\phi poly(n)$ iterations, where $\delta \in (0,1]$ is an arbitrarily small constant. Previous to our results the only graph families for which FLIP was known to achieve a smoothed polynomial running time were complete graphs and graphs with logarithmic maximum degree.
  2) For arbitrary values of $\alpha$ we get a running time of $\phi n^{O(\frac{\alpha}{\log n} + \log \alpha)}$. This improves over the best known running time for general graphs of $\phi n^{O(\sqrt{ \log n })}$ for $\alpha = o(\log^{1.5} n)$. Specifically, when $\alpha = O(\log n)$ we get a significantly faster running time of $\phi n^{O(\log \log n)}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00182v3</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregory Schwartzman</dc:creator>
    </item>
    <item>
      <title>The Discrepancy of Shortest Paths</title>
      <link>https://arxiv.org/abs/2401.15781</link>
      <description>arXiv:2401.15781v3 Announce Type: replace 
Abstract: The hereditary discrepancy of a set system is a certain quantitative measure of the pseudorandom properties of the system. Roughly, hereditary discrepancy measures how well one can $2$-color the elements of the system so that each set contains approximately the same number of elements of each color. Hereditary discrepancy has well-studied applications e.g. in communication complexity and derandomization. More recently, the hereditary discrepancy of set systems of shortest paths has found applications in differential privacy [Chen et al.~SODA 23].
  The contribution of this paper is to improve the upper and lower bounds on the hereditary discrepancy of set systems of unique shortest paths in graphs. In particular, we show that any system of unique shortest paths in an undirected weighted graph has hereditary discrepancy $\widetilde{O}(n^{1/4})$, and we construct lower bound examples demonstrating that this bound is tight up to hidden $\text{polylog } n$ factors. Our lower bounds apply even in the planar and bipartite settings, and they improve on a previous lower bound of $\Omega(n^{1/6})$ obtained by applying the trace bound of Chazelle and Lvov [SoCG'00] to a classical point-line system of Erd\H{o}s.
  As applications, we improve the lower bound on the additive error for differentially-private all pairs shortest distances from $\Omega(n^{1/6})$ [Chen et al.~SODA 23] to $\Omega(n^{1/4})$, and we improve the lower bound on additive error for the differentially-private all sets range queries problem to $\Omega(n^{1/4})$, which is tight up to hidden $\text{polylog } n$ factors [Deng et al.~WADS 23].</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15781v3</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Greg Bodwin, Chengyuan Deng, Jie Gao, Gary Hoppenworth, Jalaj Upadhyay, Chen Wang</dc:creator>
    </item>
    <item>
      <title>Naively Sorting Evolving Data is Optimal and Robust</title>
      <link>https://arxiv.org/abs/2404.08162</link>
      <description>arXiv:2404.08162v2 Announce Type: replace 
Abstract: We study comparison sorting in the evolving data model [AKMU11], where the true total order changes while the sorting algorithm is processing the input. More precisely, each comparison operation of the algorithm is followed by a sequence of evolution steps, where an evolution step perturbs the rank of a random item by a "small" random value. The goal is to maintain an ordering that remains close to the true order over time. Previous works have analyzed adaptations of classic sorting algorithms, assuming that an evolution step changes the rank of an item by just one, and that a fixed constant number $b$ of evolution steps take place between two comparisons. In fact, the only previous result achieving optimal $O(n)$ total deviation from the true order, where $n$ is the number of items, applies just for $b=1$ [BDEGJ18].
  We analyze a very simple sorting algorithm suggested in [M14], which samples a random pair of adjacent items in each step and swaps them if they are out of order. We show that the algorithm achieves and maintains, w.h.p., optimal total deviation, $O(n)$, and optimal maximum deviation, $O(\log n)$, under very general model settings. Namely, the perturbation introduced by each evolution step follows a distribution of bounded moment generating function, and over a linear number of steps, on average the number of evolution steps between two sorting steps is bounded by an arbitrary constant.
  Our proof consists of a novel potential function argument that inserts "gaps" in the list of items, and a general framework which separates the analysis of sorting from that of the evolution steps, and is applicable to a variety of settings for which previous approaches do not apply. Our results settle conjectures by [AKMU11] and [M14], and provide theoretical support for the empirical evidence that simple quadratic algorithms are optimal and robust for sorting evolving data [BDEGJ18].</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08162v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Giakkoupis, Marcos Kiwi, Dimitrios Los</dc:creator>
    </item>
    <item>
      <title>A Feasibility-Preserved Quantum Approximate Solver for the Capacitated Vehicle Routing Problem</title>
      <link>https://arxiv.org/abs/2308.08785</link>
      <description>arXiv:2308.08785v3 Announce Type: replace-cross 
Abstract: The Capacitated Vehicle Routing Problem (CVRP) is an NP-optimization problem (NPO) that arises in various fields including transportation and logistics. The CVRP extends from the Vehicle Routing Problem (VRP), aiming to determine the most efficient plan for a fleet of vehicles to deliver goods to a set of customers, subject to the limited carrying capacity of each vehicle. As the number of possible solutions skyrockets when the number of customers increases, finding the optimal solution remains a significant challenge. Recently, the Quantum Approximate Optimization Algorithm (QAOA), a quantum-classical hybrid algorithm, has exhibited enhanced performance in certain combinatorial optimization problems compared to classical heuristics. However, its ability diminishes notably in solving constrained optimization problems including the CVRP. This limitation primarily arises from the typical approach of encoding the given problems as penalty-inclusive binary optimization problems. In this case, the QAOA faces challenges in sampling solutions satisfying all constraints. Addressing this, our work presents a new binary encoding for the CVRP, with an alternative objective function of minimizing the shortest path that bypasses the vehicle capacity constraint of the CVRP. The search space is further restricted by the constraint-preserving mixing operation. We examine and discuss the effectiveness of the proposed encoding under the framework of the variant of the QAOA, Quantum Alternating Operator Ansatz (AOA), through its application to several illustrative examples. Compared to the typical QAOA approach, the proposed method not only preserves the feasibility but also achieves a significant enhancement in the probability of measuring optimal solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.08785v3</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ningyi Xie, Xinwei Lee, Dongsheng Cai, Yoshiyuki Saito, Nobuyoshi Asai, Hoong Chuin Lau</dc:creator>
    </item>
  </channel>
</rss>
