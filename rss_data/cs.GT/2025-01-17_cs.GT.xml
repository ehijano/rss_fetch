<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Jan 2025 05:02:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Solving Infinite-Player Games with Player-to-Strategy Networks</title>
      <link>https://arxiv.org/abs/2501.09330</link>
      <description>arXiv:2501.09330v1 Announce Type: new 
Abstract: We present a new approach to solving games with a countably or uncountably infinite number of players. Such games are often used to model multiagent systems with a large number of agents. The latter are frequently encountered in economics, financial markets, crowd dynamics, congestion analysis, epidemiology, and population ecology, among other fields. Our two primary contributions are as follows. First, we present a way to represent strategy profiles for an infinite number of players, which we name a Player-to-Strategy Network (P2SN). Such a network maps players to strategies, and exploits the generalization capabilities of neural networks to learn across an infinite number of inputs (players) simultaneously. Second, we present an algorithm, which we name Shared-Parameter Simultaneous Gradient (SPSG), for training such a network, with the goal of finding an approximate Nash equilibrium. This algorithm generalizes simultaneous gradient ascent and its variants, which are classical equilibrium-seeking dynamics used for multiagent reinforcement learning. We test our approach on infinite-player games and observe its convergence to approximate Nash equilibria. Our method can handle games with infinitely many states, infinitely many players, infinitely many actions (and mixed strategies on them), and discontinuous utility functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09330v1</guid>
      <category>cs.GT</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Martin, Tuomas Sandholm</dc:creator>
    </item>
    <item>
      <title>Regulation of Algorithmic Collusion, Refined: Testing Pessimistic Calibrated Regret</title>
      <link>https://arxiv.org/abs/2501.09740</link>
      <description>arXiv:2501.09740v1 Announce Type: new 
Abstract: We study the regulation of algorithmic (non-)collusion amongst sellers in dynamic imperfect price competition by auditing their data as introduced by Hartline et al. [2024].
  We develop an auditing method that tests whether a seller's pessimistic calibrated regret is low. The pessimistic calibrated regret is the highest calibrated regret of outcomes compatible with the observed data. This method relaxes the previous requirement that a pricing algorithm must use fully-supported price distributions to be auditable. This method is at least as permissive as any auditing method that has a high probability of failing algorithmic outcomes with non-vanishing calibrated regret. Additionally, we strengthen the justification for using vanishing calibrated regret, versus vanishing best-in-hindsight regret, as the non-collusion definition, by showing that even without any side information, the pricing algorithms that only satisfy weaker vanishing best-in-hindsight regret allow an opponent to manipulate them into posting supra-competitive prices. This manipulation cannot be excluded with a non-collusion definition of vanishing best-in-hindsight regret.
  We motivate and interpret the approach of auditing algorithms from their data as suggesting a per se rule. However, we demonstrate that it is possible for algorithms to pass the audit by pretending to have higher costs than they actually do. For such scenarios, the rule of reason can be applied to bound the range of costs to those that are reasonable for the domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09740v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason D. Hartline, Chang Wang, Chenhao Zhang</dc:creator>
    </item>
    <item>
      <title>Clone-Robust AI Alignment</title>
      <link>https://arxiv.org/abs/2501.09254</link>
      <description>arXiv:2501.09254v1 Announce Type: cross 
Abstract: A key challenge in training Large Language Models (LLMs) is properly aligning them with human preferences. Reinforcement Learning with Human Feedback (RLHF) uses pairwise comparisons from human annotators to train reward functions and has emerged as a popular alignment method. However, input datasets in RLHF are not necessarily balanced in the types of questions and answers that are included. Therefore, we want RLHF algorithms to perform well even when the set of alternatives is not uniformly distributed. Drawing on insights from social choice theory, we introduce robustness to approximate clones, a desirable property of RLHF algorithms which requires that adding near-duplicate alternatives does not significantly change the learned reward function. We first demonstrate that the standard RLHF algorithm based on regularized maximum likelihood estimation (MLE) fails to satisfy this property. We then propose the weighted MLE, a new RLHF algorithm that modifies the standard regularized MLE by weighting alternatives based on their similarity to other alternatives. This new algorithm guarantees robustness to approximate clones while preserving desirable theoretical properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09254v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ariel D. Procaccia, Benjamin Schiffer, Shirley Zhang</dc:creator>
    </item>
    <item>
      <title>An upper bound for the number of chess diagrams without promotion</title>
      <link>https://arxiv.org/abs/2112.09386</link>
      <description>arXiv:2112.09386v2 Announce Type: replace 
Abstract: In 2015, Steinerberger showed that the number of legal chess diagrams without promotion is bounded from above by $2\times 10^{40}$. This number was obtained by restricting both bishops and pawns position and by a precise bound when no chessman has been captured. We improve this estimate and show that the number of legal diagrams is less than $4\times 10^{37}$. To achieve this, we define a graph on the set of diagrams and a notion of class of pawn arrangements, leading to a method for bounding pawn positions with any number of men on the board.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.09386v2</guid>
      <category>cs.GT</category>
      <category>math.CO</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel Gourion (LMA)</dc:creator>
    </item>
    <item>
      <title>Convex Markov Games: A Framework for Creativity, Imitation, Fairness, and Safety in Multiagent Learning</title>
      <link>https://arxiv.org/abs/2410.16600</link>
      <description>arXiv:2410.16600v2 Announce Type: replace 
Abstract: Behavioral diversity, expert imitation, fairness, safety goals and others give rise to preferences in sequential decision making domains that do not decompose additively across time. We introduce the class of convex Markov games that allow general convex preferences over occupancy measures. Despite infinite time horizon and strictly higher generality than Markov games, pure strategy Nash equilibria exist. Furthermore, equilibria can be approximated empirically by performing gradient descent on an upper bound of exploitability. Our experiments reveal novel solutions to classic repeated normal-form games, find fair solutions in a repeated asymmetric coordination game, and prioritize safe long-term behavior in a robot warehouse environment. In the prisoner's dilemma, our algorithm leverages transient imitation to find a policy profile that deviates from observed human play only slightly, yet achieves higher per-player utility while also being three orders of magnitude less exploitable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16600v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ian Gemp, Andreas Haupt, Luke Marris, Siqi Liu, Georgios Piliouras</dc:creator>
    </item>
    <item>
      <title>Local hedging approximately solves Pandora's box problems with nonobligatory inspection</title>
      <link>https://arxiv.org/abs/2410.19011</link>
      <description>arXiv:2410.19011v2 Announce Type: replace 
Abstract: We consider search problems with nonobligatory inspection and single-item or combinatorial selection. A decision maker is presented with a number of items, each of which contains an unknown price, and can pay an inspection cost to observe the item's price before selecting it. Under single-item selection, the decision maker must select one item; under combinatorial selection, the decision maker must select a set of items that satisfies certain constraints. In our nonobligatory inspection setting, the decision maker can select items without first inspecting them. It is well-known that search with nonobligatory inspection is harder than the well-studied obligatory inspection case, for which the optimal policy for single-item selection (Weitzman, 1979) and approximation algorithms for combinatorial selection (Singla, 2018) are known.
  We introduce a technique, local hedging, for constructing policies with good approximation ratios in the nonobligatory inspection setting. Local hedging transforms policies for the obligatory inspection setting into policies for the nonobligatory inspection setting, at the cost of an extra factor in the approximation ratio. The factor is instance-dependent but is at most 4/3. We thus obtain the first approximation algorithms for a variety of combinatorial selection problems, including matroid basis, matching, and facility location.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19011v2</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <category>math.PR</category>
      <pubDate>Fri, 17 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziv Scully, Laura Doval</dc:creator>
    </item>
  </channel>
</rss>
