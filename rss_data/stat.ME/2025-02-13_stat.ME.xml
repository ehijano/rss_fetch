<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Feb 2025 02:48:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Model-free Methods for Event History Analysis and Efficient Adjustment (PhD Thesis)</title>
      <link>https://arxiv.org/abs/2502.07906</link>
      <description>arXiv:2502.07906v1 Announce Type: new 
Abstract: This thesis contains a series of independent contributions to statistics, unified by a model-free perspective. The first chapter elaborates on how a model-free perspective can be used to formulate flexible methods that leverage prediction techniques from machine learning. Mathematical insights are obtained from concrete examples, and these insights are generalized to principles that permeate the rest of the thesis. The second chapter studies the concept of local independence, which describes whether the evolution of one stochastic process is directly influenced by another. To test local independence, we define a model-free parameter called the Local Covariance Measure (LCM). We formulate an estimator for the LCM, from which a test of local independence is proposed. We discuss how the size and power of the proposed test can be controlled uniformly and investigate the test in a simulation study. The third chapter focuses on covariate adjustment, a method used to estimate the effect of a treatment by accounting for observed confounding. We formulate a general framework that facilitates adjustment for any subset of covariate information. We identify the optimal covariate information for adjustment and, based on this, introduce the Debiased Outcome-adapted Propensity Estimator (DOPE) for efficient estimation of treatment effects. An instance of DOPE is implemented using neural networks, and we demonstrate its performance on simulated and real data. The fourth and final chapter introduces a model-free measure of the conditional association between an exposure and a time-to-event, which we call the Aalen Covariance Measure (ACM). We develop a model-free estimation method and show that it is doubly robust, ensuring $\sqrt{n}$-consistency provided that the nuisance functions can be estimated with modest rates. A simulation study demonstrates the use of our estimator in several settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07906v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Mangulad Christgau</dc:creator>
    </item>
    <item>
      <title>Small Area Estimation of Education Levels in Low- and Middle-Income Countries</title>
      <link>https://arxiv.org/abs/2502.07946</link>
      <description>arXiv:2502.07946v1 Announce Type: new 
Abstract: Education is a key driver of social and economic mobility, yet disparities in attainment persist, particularly in low- and middle-income countries (LMICs). Existing indicators, such as mean years of schooling for adults aged 25 and older (MYS25) and expected years of schooling (EYS), offer a snapshot of an educational system, but lack either cohort-specific or temporal granularity. To address these limitations, we introduce the ultimate years of schooling (UYS)-a birth cohort-based metric targeting the final educational attainment of any individual cohort, including those with ongoing schooling trajectories. As with many attainment indicators, we propose to estimate UYS with cross-sectional household surveys. However, for younger cohorts, estimation fails, because these individuals are right-censored leading to severe downwards bias. To correct for this, we propose to re-frame educational attainment as a time-to-event process and deploy discrete-time survival models that explicitly account for censoring in the observations. At the national level, we estimate the parameters of the model using survey-weighted logistic regression, while for finer spatial resolutions, where sample sizes are smaller, we embed the discrete-time survival model within a Bayesian spatiotemporal framework to improve stability and precision. Applying our proposed methods to data from the 2022 Tanzania Demographic and Health Surveys, we estimate female educational trajectories corrected for censoring biases, and reveal substantial subnational disparities. By providing a dynamic, bias-corrected, and spatially disaggregated measure, our approach enhances education monitoring; it equips policymakers and researchers with a more precise tool for monitoring current progress towards education goals, and for designing future targeted policy interventions in LMICs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07946v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhan Wu, Ameer Dharamshi, Jon Wakefield</dc:creator>
    </item>
    <item>
      <title>Bad estimation, good prediction: the Lasso in dense regimes</title>
      <link>https://arxiv.org/abs/2502.07959</link>
      <description>arXiv:2502.07959v1 Announce Type: new 
Abstract: For high-dimensional omics data, sparsity-inducing regularization methods such as the Lasso are widely used and often yield strong predictive performance, even in settings when the assumption of sparsity is likely violated. We demonstrate that under a specific dense model, namely the high-dimensional joint latent variable model, the Lasso produces sparse prediction rules with favorable prediction error bounds, even when the underlying regression coefficient vector is not sparse at all. We further argue that this model better represents many types of omics data than sparse linear regression models. We prove that the prediction bound under this model in fact decreases with increasing number of predictors, and confirm this through simulation examples. These results highlight the need for caution when interpreting sparse prediction rules, as strong prediction accuracy of a sparse prediction rule may not imply underlying biological significance of the individual predictors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07959v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Bratsberg, Magne Thoresen, Jelle J. Goeman</dc:creator>
    </item>
    <item>
      <title>Cost Effectiveness Analyses for Sequential Multiple Assignment Randomized Trials</title>
      <link>https://arxiv.org/abs/2502.07973</link>
      <description>arXiv:2502.07973v1 Announce Type: new 
Abstract: Sequential multiple assignment randomized trials (SMARTs) have grown in popularity in recent years, and many of their study protocols propose conducting a cost effectiveness analysis of the adaptive strategies embedded within them. The cost effectiveness of these regimes is often proposed to be assessed using incremental cost effectiveness ratios (ICERs). In this paper, we present an estimation and inference procedure for such cost effectiveness measures for the embedded dynamic treatment regimes within a SMART design. In particular, we describe a targeted maximum likelihood estimator for the ICER of a SMART's embedded regimes with influence curve-based inference. We illustrate the performance of these methods using simulations. Throughout, we use as illustration a cost effectiveness analysis for the Adaptive Strategies for Preventing and Treating Lapses of Retention in HIV Care (ADAPT-R; NCT02338739) trial, presenting estimated ICERs (with inference) for embedded regimes aimed at increasing HIV care adherence. This manuscript is one of the first to present cost effectiveness analysis results from a SMART.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07973v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lina M. Montoya, Elvin H. Geng, Harriet F. Adhiambo, Eliud Akama, Starley B. Shade, Assurah Elly, Thomas Odeny, Maya L. Petersen</dc:creator>
    </item>
    <item>
      <title>Simulating Longitudinal Data from Marginal Structural Models</title>
      <link>https://arxiv.org/abs/2502.07991</link>
      <description>arXiv:2502.07991v1 Announce Type: new 
Abstract: Simulating longitudinal data from specified marginal structural models is a crucial but challenging task for evaluating causal inference methods and designing clinical trials. While data generation typically proceeds in a fully conditional manner using structural equations according to a temporal ordering, marginal structural models require capturing causal effects that are marginal over time-dependent confounders, making it difficult to align conditional distributions with target marginal quantities. To address this, we propose a flexible and efficient algorithm for simulating longitudinal data that adheres exactly to a specified marginal structural model. Recognizing the importance of time-to-event outcomes in clinical trials, we extend the method to accommodate survival models. Compared to existing approaches, our method offers several key advantages: it enables exact simulation from a known causal model rather than relying on approximations; it avoids imposing restrictive assumptions on the data-generating process; and it is efficient as we need only evaluate analytic functions. This last benefit contrasts with methods that use computationally intensive techniques such as Monte Carlo approximations or numerical integration. Through simulation studies replicating realistic scenarios, we validate the method's accuracy and utility. Our method will allow researchers to effectively simulate data with target causal structures for their specific scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07991v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xi Lin, Daniel de Vassimon Manela, Chase Mathis, Jens Magelund Tarp, Robin J. Evans</dc:creator>
    </item>
    <item>
      <title>A Bayesian Non-linear Mixed-Effects Model for Accurate Detection of the Onset of Cognitive Decline in Longitudinal Aging Studies</title>
      <link>https://arxiv.org/abs/2502.08418</link>
      <description>arXiv:2502.08418v1 Announce Type: new 
Abstract: Change-point models are frequently considered when modeling phenomena where a regime shift occurs at an unknown time. In ageing research, these models are commonly adopted to estimate of the onset of cognitive decline. Yet commonly used models present several limitations. Here, we present a Bayesian non-linear mixed-effects model based on a differential equation designed for longitudinal studies to overcome some limitations of classical change point models used in ageing research. We demonstrate the ability of the proposed model to avoid biases in estimates of the onset of cognitive impairment in a simulated study. Finally, the methodology presented in this work is illustrated by analysing results from memory tests from older adults who participated in the English Longitudinal Study of Ageing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08418v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fernando Massa, Marco Scavino, Graciela Muniz-Terrera</dc:creator>
    </item>
    <item>
      <title>Uniform confidence bands for joint angles across different fatigue phases</title>
      <link>https://arxiv.org/abs/2502.08430</link>
      <description>arXiv:2502.08430v1 Announce Type: new 
Abstract: We develop uniform confidence bands for the mean function of stationary time series as a post-hoc analysis of multiple change point detection in functional time series. In particular, the methodology in this work provides bands for those segments where the jump size exceeds a certain threshold $\Delta$. In \cite{bastian2024multiplechangepointdetection} such exceedences of $\Delta$ were related to fatigue states of a running athlete. The extension to confidence bands stems from an interest in understanding the range of motion (ROM) of lower-extremity joints of running athletes under fatiguing conditions. From a biomechanical perspective, ROM serves as a proxy for joint flexibility under varying fatigue states, offering individualized insights into potentially problematic movement patterns. The new methodology provides a valuable tool for understanding the dynamic behavior of joint motion and its relationship to fatigue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08430v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Bastian, Rupsa Basu, Holger Dette</dc:creator>
    </item>
    <item>
      <title>Tutorial for Surrogate Endpoint Validation Using Joint modeling and Mediation Analysis</title>
      <link>https://arxiv.org/abs/2502.08443</link>
      <description>arXiv:2502.08443v1 Announce Type: new 
Abstract: The use of valid surrogate endpoints is an important stake in clinical research to help reduce both the duration and cost of a clinical trial and speed up the evaluation of interesting treatments. Several methods have been proposed in the statistical literature to validate putative surrogate endpoints. Two main approaches have been proposed: the meta-analytic approach and the mediation analysis approach. The former uses data from meta-analyses to derive associations measures between the surrogate and the final endpoint at the individual and trial levels. The latter rather uses the proportion of the treatment effect on the final endpoint through the surrogate as a measure of surrogacy in a causal inference framework. Both approaches have remained separated as the meta-analytic approach does not estimate the treatment effect on the final endpoint through the surrogate while the mediation analysis approach have been limited to single-trial setting. However, these two approaches are complementary. In this work we propose an approach that combines the meta-analytic and mediation analysis approaches using joint modeling for surrogate validation. We focus on the cases where the final endpoint is a time-to-event endpoint (such as time-to-death) and the surrogate is either a time-to-event or a longitudinal biomarker. Two new joint models were proposed depending on the nature of the surrogate. These model are implemented in the R package frailtypack. We illustrate the developed approaches in three applications on real datasets in oncology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08443v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quentin Le Coent, Virginie Rondeau, Catherine Legrand</dc:creator>
    </item>
    <item>
      <title>Anytime-valid FDR control with the stopped e-BH procedure</title>
      <link>https://arxiv.org/abs/2502.08539</link>
      <description>arXiv:2502.08539v1 Announce Type: new 
Abstract: The recent e-Benjamini-Hochberg (e-BH) procedure for multiple hypothesis testing is known to control the false discovery rate (FDR) under arbitrary dependence between the input e-values. This paper points out an important subtlety when applying the e-BH procedure with e-processes, which are sequential generalizations of e-values (where the data are observed sequentially). Since adaptively stopped e-processes are e-values, the e-BH procedure can be repeatedly applied at every time step, and one can continuously monitor the e-processes and the rejection sets obtained. One would hope that the "stopped e-BH procedure" (se-BH) has an FDR guarantee for the rejection set obtained at any stopping time. However, while this is true if the data in different streams are independent, it is not true in full generality, because each stopped e-process is an e-value only for stopping times in its own local filtration, but the se-BH procedure employs a stopping time with respect to a global filtration. This can cause information to leak across time, allowing one stream to know its future by knowing past data of another stream. This paper formulates a simple causal condition under which local e-processes are also global e-processes and thus the se-BH procedure does indeed control the FDR. The condition excludes unobserved confounding from the past and is met under most reasonable scenarios including genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08539v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Wang, Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Likelihood-based Nonparametric Receiver Operating Characteristic Curve Analysis in the Presence of Imperfect Reference Standard</title>
      <link>https://arxiv.org/abs/2502.08569</link>
      <description>arXiv:2502.08569v1 Announce Type: new 
Abstract: In diagnostic studies, researchers frequently encounter imperfect reference standards with some misclassified labels. Treating these as gold standards can bias receiver operating characteristic (ROC) curve analysis. To address this issue, we propose a novel likelihood-based method under a nonparametric density ratio model. This approach enables the reliable estimation of the ROC curve, area under the curve (AUC), partial AUC, and Youden's index with favorable statistical properties. To implement the method, we develop an efficient expectation-maximization algorithm algorithm. Extensive simulations evaluate its finite-sample performance, showing smaller mean squared errors in estimating the ROC curve, partial AUC, and Youden's index compared to existing methods. We apply the proposed approach to a malaria study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08569v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Sun, Peijun Sang, Qinglong Tian, Pengfei Li</dc:creator>
    </item>
    <item>
      <title>Knowledge-Guided Wasserstein Distributionally Robust Optimization</title>
      <link>https://arxiv.org/abs/2502.08146</link>
      <description>arXiv:2502.08146v1 Announce Type: cross 
Abstract: Transfer learning is a popular strategy to leverage external knowledge and improve statistical efficiency, particularly with a limited target sample. We propose a novel knowledge-guided Wasserstein Distributionally Robust Optimization (KG-WDRO) framework that adaptively incorporates multiple sources of external knowledge to overcome the conservativeness of vanilla WDRO, which often results in overly pessimistic shrinkage toward zero. Our method constructs smaller Wasserstein ambiguity sets by controlling the transportation along directions informed by the source knowledge. This strategy can alleviate perturbations on the predictive projection of the covariates and protect against information loss. Theoretically, we establish the equivalence between our WDRO formulation and the knowledge-guided shrinkage estimation based on collinear similarity, ensuring tractability and geometrizing the feasible set. This also reveals a novel and general interpretation for recent shrinkage-based transfer learning approaches from the perspective of distributional robustness. In addition, our framework can adjust for scaling differences in the regression models between the source and target and accommodates general types of regularization such as lasso and ridge. Extensive simulations demonstrate the superior performance and adaptivity of KG-WDRO in enhancing small-sample transfer learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08146v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zitao Wang, Ziyuan Wang, Molei Liu, Nian Si</dc:creator>
    </item>
    <item>
      <title>A comparison of Dirichlet kernel regression methods on the simplex</title>
      <link>https://arxiv.org/abs/2502.08461</link>
      <description>arXiv:2502.08461v1 Announce Type: cross 
Abstract: An asymmetric Dirichlet kernel version of the Gasser-M\"uller estimator is introduced for regression surfaces on the simplex, extending the univariate analog proposed by Chen [Statist. Sinica, 10(1) (2000), pp. 73-91]. Its asymptotic properties are investigated under the condition that the design points are known and fixed, including an analysis of its mean integrated squared error (MISE) and its asymptotic normality. The estimator is also applicable in a random design setting. A simulation study compares its performance with two recently proposed alternatives: the Nadaraya--Watson estimator with Dirichlet kernel and the local linear smoother with Dirichlet kernel. The results show that the local linear smoother consistently outperforms the others. To illustrate its applicability, the local linear smoother is applied to the GEMAS dataset to analyze the relationship between soil composition and pH levels across various agricultural and grazing lands in Europe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08461v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanen Daayeb, Christian Genest, Salah Khardani, Nicolas Klutchnikoff, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Network Goodness-of-Fit for the block-model family</title>
      <link>https://arxiv.org/abs/2502.08609</link>
      <description>arXiv:2502.08609v1 Announce Type: cross 
Abstract: The block-model family has four popular network models (SBM, DCBM, MMSBM, and DCMM). A fundamental problem is, how well each of these models fits with real networks. We propose GoF-MSCORE as a new Goodness-of-Fit (GoF) metric for DCMM (the broadest one among the four), with two main ideas. The first is to use cycle count statistics as a general recipe for GoF. The second is a novel network fitting scheme. GoF-MSCORE is a flexible GoF approach, and we further extend it to SBM, DCBM, and MMSBM. This gives rise to a series of GoF metrics covering each of the four models in the block-model family.
  We show that for each of the four models, if the assumed model is correct, then the corresponding GoF metric converges to $N(0, 1)$ as the network sizes diverge. We also analyze the powers and show that these metrics are optimal in many settings. In comparison, many other GoF ideas face challenges: they may lack a parameter-free limiting null, or are non-optimal in power, or face an analytical hurdle. Note that a parameter-free limiting null is especially desirable as many network models have a large number of unknown parameters. The limiting nulls of our GoF metrics are always $N(0,1)$, which are parameter-free as desired.
  For 12 frequently-used real networks, we use the proposed GoF metrics to show that DCMM fits well with almost all of them. We also show that SBM, DCBM, and MMSBM do not fit well with many of these networks, especially when the networks are relatively large. To complement with our study on GoF, we also show that the DCMM is nearly as broad as the rank-$K$ network model. Based on these results, we recommend the DCMM as a promising model for undirected networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08609v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiashun Jin, Zheng Tracy Ke, Jiajun Tang, Jingming Wang</dc:creator>
    </item>
    <item>
      <title>An Extension of the Unified Skew-Normal Family of Distributions and Application to Bayesian Binary Regression</title>
      <link>https://arxiv.org/abs/2209.03474</link>
      <description>arXiv:2209.03474v4 Announce Type: replace 
Abstract: We consider the Bayesian binary regression model and we introduce a new class of distributions, the Perturbed Unified Skew-Normal (pSUN, henceforth), which generalizes the Unified Skew-Normal (SUN) class. We show that the new class is conjugate to any binary regression model, provided that the link function may be expressed as a scale mixture of Gaussian CDFs. We discuss in detail the popular logit case, and we show that, when a logistic regression model is combined with a Gaussian prior, posterior summaries such as cumulants and normalizing constants can easily be obtained through the use of an importance sampling approach, opening the way to straightforward variable selection procedures. For more general prior distributions, the proposed methodology is based on a simple Gibbs sampler algorithm. We also claim that, in the p&gt;n case, our proposal presents better performances - both in terms of mixing and accuracy - compared to the existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.03474v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paolo Onorati, Brunero Liseo</dc:creator>
    </item>
    <item>
      <title>Nested Instrumental Variables Design: Switcher Average Treatment Effect, Identification, Efficient Estimation and Generalizability</title>
      <link>https://arxiv.org/abs/2405.07102</link>
      <description>arXiv:2405.07102v3 Announce Type: replace 
Abstract: Instrumental variables (IV) are a commonly used tool to estimate causal effects from non-randomized data. An archetype of an IV is a randomized trial with non-compliance where the randomized treatment assignment serves as an IV for the non-ignorable treatment received. Under a monotonicity assumption, a valid IV non-parametrically identifies the average treatment effect among a non-identified, latent complier subgroup, whose generalizability is often under debate. In many studies, there could exist multiple versions of an IV, for instance, different nudges to take the same treatment in different study sites in a multicentre clinical trial. These different versions of an IV may result in different compliance rates and offer a unique opportunity to study IV estimates' generalizability. In this article, we introduce a novel nested IV assumption and study identification of the average treatment effect among two latent subgroups: always-compliers and switchers, who are defined based on the joint potential treatment received under two versions of a binary IV. We derive the efficient influence function for the SWitcher Average Treatment Effect (SWATE) under a non-parametric model and propose efficient estimators. We then propose formal statistical tests of the generalizability of IV estimates under the nested IV framework. We apply the proposed method to the Prostate, Lung, Colorectal and Ovarian (PLCO) Cancer Screening Trial and study the causal effect of colorectal cancer screening and its generalizability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07102v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Wang, Ying-Qi Zhao, Oliver Dukes, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian approach for reliability prognosis of nondestructive one-shot devices under cumulative risk model</title>
      <link>https://arxiv.org/abs/2406.08867</link>
      <description>arXiv:2406.08867v2 Announce Type: replace 
Abstract: The present study aims to determine the lifetime prognosis of highly durable nondestructive one-shot devices (NOSD) units under a step-stress accelerated life testing (SSALT) experiment applying a cumulative risk model (CRM). In an SSALT experiment, CRM retains the continuity of hazard function by allowing the lag period before the effects of stress change emerge. When prior information about the model parameters is available, Bayesian inference is crucial. In a Bayesian analysis of such lifetime data, conventional likelihood-based Bayesian estimation frequently fails in the presence of outliers in the dataset. This work incorporates a robust Bayesian approach utilizing a robustified posterior based on the density power divergence measure. The order restriction on shape parameters has been incorporated as a prior assumption to reflect the decreasing expected lifetime with increasing stress levels. In testing of hypothesis, a Bayes factor is implemented based on the robustified posterior. In Bayesian estimation, we exploit Hamiltonian Monte Carlo, which has certain advantages over the conventional Metropolis-Hastings algorithms. Further, the influence functions are examined to evaluate the robust behaviour of the estimators and the Bayes factor. Finally, the analytical development is validated through a simulation study and a real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08867v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shanya Baghel, Shuvashree Mondal</dc:creator>
    </item>
    <item>
      <title>Balancing events, not patients, maximizes power of the logrank test: and other insights on unequal randomization in survival trials</title>
      <link>https://arxiv.org/abs/2407.03420</link>
      <description>arXiv:2407.03420v2 Announce Type: replace 
Abstract: We revisit the question of what randomization ratio (RR) maximizes power of the logrank test in event-driven survival trials under proportional hazards (PH). By comparing three approximations of the logrank test (Schoenfeld, Freedman, Rubinstein) to empirical simulations, we find that the RR that maximizes power is the RR that balances number of events across treatment arms at the end of the trial. This contradicts the common misconception implied by Schoenfeld's approximation that 1:1 randomization maximizes power. Besides power, we consider other factors that might influence the choice of RR (accrual, trial duration, sample size, etc.). We perform simulations to better understand how unequal randomization might impact these factors in practice. Altogether, we derive 6 insights to guide statisticians in the design of survival trials considering unequal randomization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03420v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Godwin Yung, Kaspar Rufibach, Marcel Wolbers, Ray Lin, Yi Liu</dc:creator>
    </item>
    <item>
      <title>Bias Mitigation in Matched Observational Studies with Continuous Treatments: Calipered Non-Bipartite Matching and Bias-Corrected Estimation and Inference</title>
      <link>https://arxiv.org/abs/2409.11701</link>
      <description>arXiv:2409.11701v2 Announce Type: replace 
Abstract: In matched observational studies with continuous treatments, individuals with different treatment doses but the same or similar covariate values are paired for causal inference. While inexact covariate matching (i.e., covariate imbalance after matching) is common in practice, previous matched studies with continuous treatments have often overlooked this issue as long as post-matching covariate balance meets certain criteria. Through re-analyzing a matched observational study on the social distancing effect on COVID-19 case counts, we show that this routine practice can introduce severe bias for causal inference. Motivated by this finding, we propose a general framework for mitigating bias due to inexact matching in matched observational studies with continuous treatments, covering the matching, estimation, and inference stages. In the matching stage, we propose a carefully designed caliper that incorporates both covariate and treatment dose information to improve matching for downstream treatment effect estimation and inference. For the estimation and inference, we introduce a bias-corrected Neyman estimator paired with a corresponding bias-corrected variance estimator. The effectiveness of our proposed framework is demonstrated through numerical studies and a re-analysis of the aforementioned observational study on the effect of social distancing on COVID-19 case counts. An open-source R package for implementing our framework has also been developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11701v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anthony Frazier, Siyu Heng, Wen Zhou</dc:creator>
    </item>
    <item>
      <title>CMHSU: An R Statistical Software Package to Detect Mental Health Status, Substance Use Status, and their Concurrent Status in the North American Healthcare Administrative Databases</title>
      <link>https://arxiv.org/abs/2501.06435</link>
      <description>arXiv:2501.06435v2 Announce Type: replace 
Abstract: The concept of concurrent mental health and substance use (MHSU) and its detection in patients has garnered growing interest among psychiatrists and healthcare policymakers over the past four decades. Researchers have proposed various diagnostic methods, including the Data-Driven Diagnostic Method (DDDM), for the identification of MHSU. However, the absence of a standalone statistical software package to facilitate DDDM for large healthcare administrative databases has remained a significant gap. This paper introduces the R statistical software package CMHSU , available on the Comprehensive R Archive Network (CRAN), for the diagnosis of mental health (MH), substance use (SU), and their concurrent status (MHSU). The package implements DDDM using hospital and medical service physician visit counts along with maximum time span parameters for MH, SU, and MHSU diagnoses. A working example with a simulated real-world dataset is presented to explore three critical dimensions of MHSU detection based on the DDDM. Additionally, the limitations of the CMHSU package and potential directions for its future extension are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06435v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Soltanifar, Chel Hee Lee</dc:creator>
    </item>
    <item>
      <title>Wrapped Gaussian on the manifold of Symmetric Positive Definite Matrices</title>
      <link>https://arxiv.org/abs/2502.01512</link>
      <description>arXiv:2502.01512v2 Announce Type: replace 
Abstract: Circular and non-flat data distributions are prevalent across diverse domains of data science, yet their specific geometric structures often remain underutilized in machine learning frameworks. A principled approach to accounting for the underlying geometry of such data is pivotal, particularly when extending statistical models, like the pervasive Gaussian distribution. In this work, we tackle those issue by focusing on the manifold of symmetric positive definite matrices, a key focus in information geometry. We introduced a non-isotropic wrapped Gaussian by leveraging the exponential map, we derive theoretical properties of this distribution and propose a maximum likelihood framework for parameter estimation. Furthermore, we reinterpret established classifiers on SPD through a probabilistic lens and introduce new classifiers based on the wrapped Gaussian model. Experiments on synthetic and real-world datasets demonstrate the robustness and flexibility of this geometry-aware distribution, underscoring its potential to advance manifold-based data analysis. This work lays the groundwork for extending classical machine learning and statistical methods to more complex and structured data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01512v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thibault de Surrel, Fabien Lotte, Sylvain Chevallier, Florian Yger</dc:creator>
    </item>
    <item>
      <title>Score-Based Causal Discovery with Temporal Background Information</title>
      <link>https://arxiv.org/abs/2502.06232</link>
      <description>arXiv:2502.06232v2 Announce Type: replace 
Abstract: Temporal background information can improve causal discovery algorithms by orienting edges and identifying relevant adjustment sets. We develop the Temporal Greedy Equivalence Search (TGES) algorithm and terminology essential for score-based causal discovery with tiered background knowledge. TGES learns a restricted Markov equivalence class of directed acyclic graphs (DAGs) using observational data and tiered background knowledge. To construct TGES we formulate a scoring criterion that accounts for tiered background knowledge. We establish theoretical results for TGES, stating that the algorithm always returns a tiered maximally oriented partially directed acyclic graph (tiered MPDAG) and that this tiered MPDAG contains the true DAG in the large sample limit. We present a simulation study indicating a gain from using tiered background knowledge and an improved precision-recall trade-off compared to the temporal PC algorithm. We provide a real-world example on life-course health data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06232v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tobias Ellegaard Larsen, Claus Thorn Ekstr{\o}m, Anne Helby Petersen</dc:creator>
    </item>
    <item>
      <title>Improving sampling efficacy on high dimensional distributions with thin high density regions using Conservative Hamiltonian Monte Carlo</title>
      <link>https://arxiv.org/abs/2206.06901</link>
      <description>arXiv:2206.06901v2 Announce Type: replace-cross 
Abstract: Hamiltonian Monte Carlo is a prominent Markov Chain Monte Carlo algorithm, which employs symplectic integrators to sample from high dimensional target distributions in many applications, such as statistical mechanics, Bayesian statistics and generative models. However, such distributions tend to have thin high density regions, posing a significant challenge for symplectic integrators to maintain the small energy errors needed for a high acceptance probability. Instead, we propose a variant called Conservative Hamiltonian Monte Carlo, using $R$--reversible energy-preserving integrators to retain a high acceptance probability. We show our algorithm can achieve approximate stationarity with an error determined by the Jacobian approximation of the energy-preserving proposal map. Numerical evidence shows improved convergence and robustness over integration parameters on target distributions with thin high density regions and in high dimensions. Moreover, a version of our algorithm can also be applied to target distributions without gradient information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.06901v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geoffrey McGregor, Andy T. S. Wan</dc:creator>
    </item>
    <item>
      <title>Predicting Cellular Responses with Variational Causal Inference and Refined Relational Information</title>
      <link>https://arxiv.org/abs/2210.00116</link>
      <description>arXiv:2210.00116v3 Announce Type: replace-cross 
Abstract: Predicting the responses of a cell under perturbations may bring important benefits to drug discovery and personalized therapeutics. In this work, we propose a novel graph variational Bayesian causal inference framework to predict a cell's gene expressions under counterfactual perturbations (perturbations that this cell did not factually receive), leveraging information representing biological knowledge in the form of gene regulatory networks (GRNs) to aid individualized cellular response predictions. Aiming at a data-adaptive GRN, we also developed an adjacency matrix updating technique for graph convolutional networks and used it to refine GRNs during pre-training, which generated more insights on gene relations and enhanced model performance. Additionally, we propose a robust estimator within our framework for the asymptotically efficient estimation of marginal perturbation effect, which is yet to be carried out in previous works. With extensive experiments, we exhibited the advantage of our approach over state-of-the-art deep learning models for individual response prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.00116v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.GN</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yulun Wu, Robert A. Barton, Zichen Wang, Vassilis N. Ioannidis, Carlo De Donno, Layne C. Price, Luis F. Voloch, George Karypis</dc:creator>
    </item>
    <item>
      <title>Assessing Heterogeneity of Treatment Effects</title>
      <link>https://arxiv.org/abs/2306.15048</link>
      <description>arXiv:2306.15048v3 Announce Type: replace-cross 
Abstract: Treatment effect heterogeneity is of major interest in economics, but its assessment is often hindered by the fundamental lack of identification of the individual treatment effects. For example, we may want to assess the effect of a poverty reduction measure at different levels of poverty, but the causal effects on wealth at different wealth levels are not identified. Or, we may be interested in the proportion of workers who benefit from the minimum wage increase, but the proportion is not identified in the absence of counterfactuals. This paper derives bounds useful in such situations, which only depend on the marginal distributions of the outcomes. The bounds are nonparametrically sharp, making clear the maximum extent to which the data can speak about the heterogeneity of the treatment effects. An application to microfinance shows that the bounds can be informative even when the average treatment effects are not significant. Another application to the welfare reform identifies a nonnegligible portion of workers who increased and decreased working hours due to the reform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15048v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tetsuya Kaji, Jianfei Cao</dc:creator>
    </item>
    <item>
      <title>Using Multiple Outcomes to Improve the Synthetic Control Method</title>
      <link>https://arxiv.org/abs/2311.16260</link>
      <description>arXiv:2311.16260v3 Announce Type: replace-cross 
Abstract: When there are multiple outcome series of interest, Synthetic Control analyses typically proceed by estimating separate weights for each outcome. In this paper, we instead propose estimating a common set of weights across outcomes, by balancing either a vector of all outcomes or an index or average of them. Under a low-rank factor model, we show that these approaches lead to lower bias bounds than separate weights, and that averaging leads to further gains when the number of outcomes grows. We illustrate this via a re-analysis of the impact of the Flint water crisis on educational outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16260v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liyang Sun, Eli Ben-Michael, Avi Feller</dc:creator>
    </item>
  </channel>
</rss>
