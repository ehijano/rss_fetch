<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 May 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Classifying Inconsistency in AHP Pairwise Comparison Matrices Using Machine Learning</title>
      <link>https://arxiv.org/abs/2505.06293</link>
      <description>arXiv:2505.06293v1 Announce Type: new 
Abstract: Assessing consistency in Pairwise Comparison Matrices (PCMs) within the Analytical Hierarchy Process (AHP) poses significant challenges when using the traditional Consistency Ratio (CR) method. This study introduces a novel alternative that leverages triadic preference reversals (PR) to provide a more robust and interpretable assessment of consistency. Triadic preference reversals capture inconsistencies between a pair of elements by comparing the direction of preference derived from the global eigenvector with that from a 3x3 submatrix (triad) containing the same pair, highlighting local-global preference conflicts. This method detects a reversal when one eigen ratio exceeds one while another falls below one, signaling inconsistency. We identify two key features: the proportion of preference reversals and the maximum reversal, which mediate the impact of a PCM's order on its consistency. Using these features simulated PCMs are clustered into consistent and inconsistent classes through k-means clustering, followed by training a logistic classifier for consistency evaluation. The PR method achieves 97\% accuracy, significantly surpassing the Consistency Ratio (CR) method's 50%, with a false negative rate of only 2.6\% compared to 5.5\%. These findings demonstrate the PR method's superior accuracy in assessing AHP consistency, thereby enabling more reliable decision-making. The proposed triadic preference reversal (PR) approach is implemented in the R package AHPtools publicly available on the Comprehensive R Archive Network (CRAN).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06293v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amarnath Bose</dc:creator>
    </item>
    <item>
      <title>Critical issues with the Pearson's chi-square test</title>
      <link>https://arxiv.org/abs/2505.06318</link>
      <description>arXiv:2505.06318v1 Announce Type: new 
Abstract: Pearson's chi-square tests are among the most commonly applied statistical tools across a wide range of scientific disciplines, including medicine, engineering, biology, sociology, marketing and business. However, its usage in some areas is not correct.
  For example, the chi-square test for homogeneity of proportions (that is, comparing proportions across groups in a contingency table) is frequently used to verify if the rows of a given nonnegative $m \times n$ (contingency) matrix $A$ are proportional. The null-hypothesis $H_0$: ``$m$ rows are proportional'' (for the whole population) is rejected with confidence level $1 - \alpha$ if and only if $\chi^2_{stat} &gt; \chi^2_{crit}$, where the first term is given by Pearson's formula, while the second one depends only on $m, n$, and $\alpha$, but not on the entries of $A$.
  It is immediate to notice that the Pearson's formula is not invariant. More precisely, whenever we multiply all entries of $A$ by a constant $c$, the value $\chi^2_{stat}(A)$ is multiplied by $c$, too, $\chi^2_{stat}(cA) = c \chi^2_{stat} (A)$. Thus, if all rows of $A$ are exactly proportional then $\chi^2_{stat}(cA) = c \chi^2_{stat}(A) = 0$ for any $c$ and any $\alpha$. Otherwise, $\chi^2_{stat} (cA)$ becomes arbitrary large or small, as positive $c$ is increasing or decreasing. Hence, at any fixed significance level $\alpha$, the null hypothesis $H_0$ will be rejected with confidence $1 - \alpha$, when $c$ is sufficiently large and not rejected when $c$ is sufficiently small, Yet, obviously, the rows of $cA$ should be proportional or not for all $c$ simultaneously.
  Thus, any reasonable formula for the test statistic must be invariant, that is, take the same value for matrices $cA$ for all real positive $c$.
  KEY WORDS: Pearson chi-square test, difference between two proportions, goodness of fit, contingency tables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06318v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladimir Gurvich, Mariya Naumova</dc:creator>
    </item>
    <item>
      <title>A note on wavelet shrinkage in nonparametric regression models with ARFIMA errors</title>
      <link>https://arxiv.org/abs/2505.06485</link>
      <description>arXiv:2505.06485v1 Announce Type: new 
Abstract: In this paper we propose a shrinkage wavelet-based method to estimate the signal in a nonparametric regression model with Autoregressive Fractionally Integrated Moving Average (ARFIMA) errors. Monte Carlo experiments indicate that the proposed method is better than the universal thresholding rule which is widely used in data analysis via wavelet regression models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06485v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Rodrigo dos S. Sousa, Mauricio Zevallos</dc:creator>
    </item>
    <item>
      <title>Borrowing strength between unaligned binary time-series via Bayesian nonparametric rescaling of Unified Skewed Normal priors</title>
      <link>https://arxiv.org/abs/2505.06491</link>
      <description>arXiv:2505.06491v1 Announce Type: new 
Abstract: We define a Bayesian semi-parametric model to effectively conduct inference with unaligned longitudinal binary data. The proposed strategy is motivated by data from the Human Epilepsy Project (HEP), which collects seizure occurrence data for epilepsy patients, together with relevant covariates. The model is designed to flexibly accommodate the particular challenges that arise with such data. First, epilepsy data require models that can allow for extensive heterogeneity, across both patients and time. With this regard, state space models offer a flexible, yet still analytically amenable class of models. Nevertheless, seizure time-series might share similar behavioral patterns, such as local prolonged periods of elevated seizure presence, which we refer to as "clumping". Such similarities can be used to share strength across patients and define subgroups. However, due to the lack of alignment, straightforward hierarchical modeling of latent state space parameters is not practicable. To overcome this constraint, we construct a strategy that preserves the flexibility of individual trajectories while also exploiting similarities across individuals to borrow information through a nonparametric prior. On the one hand, heterogeneity is ensured by (almost) subject-specific state-space submodels. On the other, borrowing of information is obtained by introducing a Pitman-Yor prior on group-specific probabilities for patterns of clinical interest. We design a posterior sampling strategy that leverages recent developments of binary state space models using the Unified Skewed Normal family (SUN). The model, which allows the sharing of information across individuals with similar disease traits over time, can more generally be adapted to any setting characterized by unaligned binary longitudinal data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06491v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beatrice Cantoni, Giovanni Poli, Elizabeth Juarez-Colunga, Peter M\"uller</dc:creator>
    </item>
    <item>
      <title>Post-treatment problems: What can we say about the effect of a treatment among sub-groups who (would) respond in some way?</title>
      <link>https://arxiv.org/abs/2505.06754</link>
      <description>arXiv:2505.06754v1 Announce Type: new 
Abstract: Investigators are often interested in how a treatment affects an outcome for units responding to treatment in a certain way. We may wish to know the effect among units that, for example, meaningfully implemented the intervention, passed an attention check, or survived to the endpoint. Simply conditioning on the observed value of the relevant post-treatment variable introduces problematic biases. Further, assumptions such as "no unobserved confounding" (of the post-treatment mediator and the outcome) or of "no direct effect" (of treatment on outcome) required of several existing strategies are often indefensible. We propose the Treatment Reactive Average Causal Effect (TRACE), which we define as the total effect of the treatment in the group that, if treated, would realize a particular value of the relevant post-treatment variable. Given the total effect of treatment, and by reasoning about the treatment effect among the "non-reactive" group, we can identify and estimate the range of plausible values for the TRACE. We discuss this approach and its connection to existing estimands and identification strategies, then demonstrate its use with one hypothetical and two applied examples: (i) using hypothetical data to illustrate how a small number of sample statistics allow for point identification of the effect of police-perceived race on police violence during traffic stops, (ii) estimating effects of a community-policing intervention in Liberia, in locations where the project was meaningfully implemented, and (iii) studying how in-person canvassing affects support for transgender rights in the United States, among participants whose feelings towards transgender people become more positive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06754v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chad Hazlett, Nina McMurry, Tanvi Shinkre</dc:creator>
    </item>
    <item>
      <title>Quantifying uncertainty and stability among highly correlated predictors: a subspace perspective</title>
      <link>https://arxiv.org/abs/2505.06760</link>
      <description>arXiv:2505.06760v1 Announce Type: new 
Abstract: We study the problem of linear feature selection when features are highly correlated. This setting presents two main challenges. First, how should false positives be defined? Intuitively, selecting a null feature that is highly correlated with a true one may be less problematic than selecting a completely uncorrelated null feature. Second, correlation among features can cause variable selection methods to produce very different feature sets across runs, making it hard to identify stable features. To address these issues, we propose a new framework based on feature subspaces -- the subspaces spanned by selected columns of the feature matrix. This framework leads to a new definition of false positives and negatives based on the "similarity" of feature subspaces. Further, instead of measuring stability of individual features, we measure stability with respect to feature subspaces. We propose and theoretically analyze a subspace generalization of stability selection (Meinshausen and Buhlmann, 2010). This procedure outputs multiple candidate stable models which can be considered interchangeable due to multicollinearity. We also propose a method for identifying substitute structures -- features that can be swapped and yield "equivalent" models. Finally, we demonstrate our framework and algorithms using both synthetic and real gene expression data. Our methods are implemented in the R package substab.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06760v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaozhu Zhang, Jacob Bien, Armeen Taeb</dc:creator>
    </item>
    <item>
      <title>Accelerated inference for stochastic compartmental models with over-dispersed partial observations</title>
      <link>https://arxiv.org/abs/2505.06935</link>
      <description>arXiv:2505.06935v1 Announce Type: new 
Abstract: An assumed density approximate likelihood is derived for a class of partially observed stochastic compartmental models which permit observational over-dispersion. This is achieved by treating time-varying reporting probabilities as latent variables and integrating them out using Laplace approximations within Poisson Approximate Likelihoods (LawPAL), resulting in a fast deterministic approximation to the marginal likelihood and filtering distributions. We derive an asymptotically exact filtering result in the large population regime, demonstrating the approximation's ability to recover latent disease states and reporting probabilities. Through simulations we: 1) demonstrate favorable behavior of the maximum approximate likelihood estimator in the large population and time horizon regime in terms of ground truth recovery; 2) demonstrate order of magnitude computational speed gains over a sequential Monte Carlo likelihood based approach, and explore the statistical compromises our approximation implicitly makes. We conclude by embedding our methodology within the probabilistic programming language Stan for automated Bayesian inference to develop a model of practical interest using data from the Covid-19 outbreak in Switzerland.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06935v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Whitehouse</dc:creator>
    </item>
    <item>
      <title>Semiparametric Weighted Spline Regression (SWSR) in Confirmatory Clinical Trials with Time-Varying Placebo Effects</title>
      <link>https://arxiv.org/abs/2505.06939</link>
      <description>arXiv:2505.06939v1 Announce Type: new 
Abstract: In confirmatory Phase 3 clinical trials with recruitment over the years, the underlying placebo effect may follow an unknown temporal trend. Taking a clinical trial on Hidradenitis Suppurativa (HS) as an example, fluctuations or variabilities are common in HS-related endpoints, mainly due to the natural disease characteristics, variations of evaluation from different physicians, and standard of care evolvement. The adjustment of time-varying placebo effects receives some attention in adaptive clinical trials and platform trials, but is usually ignored in traditional non-adaptive designs. However, under the impact of such a time drift, some existing methods may not simultaneously control the type I error rate and achieve satisfactory power. In this article, we propose SWSR (Semiparametric Weighted Spline Regression) to estimate the treatment effect with B-splines to accommodate the time-varying placebo effects nonparametrically. Our method aims to achieve the following three objectives: a proper type I error rate control under varying settings, an overall high power to detect a potential treatment effect, and robustness to unknown time-varying placebo effects. Simulation studies and a case study provide supporting evidence. Those three key features make SWSR an appealing option to be pre-specified for practical confirmatory clinical trials. Supplemental materials, including the R code, additional simulation results and theoretical discussion, are available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06939v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianyu Zhan, Yihua Gu</dc:creator>
    </item>
    <item>
      <title>Enhancing Inference for Small Cohorts via Transfer Learning and Weighted Integration of Multiple Datasets</title>
      <link>https://arxiv.org/abs/2505.07153</link>
      <description>arXiv:2505.07153v1 Announce Type: new 
Abstract: Lung sepsis remains a significant concern in the Northeastern U.S., yet the national eICU Collaborative Database includes only a small number of patients from this region, highlighting underrepresentation. Understanding clinical variables such as FiO2, creatinine, platelets, and lactate, which reflect oxygenation, kidney function, coagulation, and metabolism, is crucial because these markers influence sepsis outcomes and may vary by sex. Transfer learning helps address small sample sizes by borrowing information from larger datasets, although differences in covariates and outcome-generating mechanisms between the target and external cohorts can complicate the process. We propose a novel weighting method, TRANSfer LeArning wiTh wEights (TRANSLATE), to integrate data from various sources by incorporating domain-specific characteristics through learned weights that align external data with the target cohort. These weights adjust for cohort differences, are proportional to each cohort's effective sample size, and downweight dissimilar cohorts. TRANSLATE offers theoretical guarantees for improved precision and applies to a wide range of estimands, including means, variances, and distribution functions. Simulations and a real-data application to sepsis outcomes in the Northeast cohort, using a much larger sample from other U.S. regions, show that the method enhances inference while accounting for regional heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07153v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subharup Guha, Mengqi Xu, Yi Li</dc:creator>
    </item>
    <item>
      <title>Spatial Confounding in Multivariate Areal Data Analysis</title>
      <link>https://arxiv.org/abs/2505.07232</link>
      <description>arXiv:2505.07232v1 Announce Type: new 
Abstract: We investigate spatial confounding in the presence of multivariate disease dependence. In the "analysis model perspective" of spatial confounding, adding a spatially dependent random effect can lead to significant variance inflation of the posterior distribution of the fixed effects. The "data generation perspective" views covariates as stochastic and correlated with an unobserved spatial confounder, leading to inferior statistical inference over multiple realizations. While multiple methods have been proposed for adjusting statistical models to mitigate spatial confounding in estimating regression coefficients, results on interactions between spatial confounding and multivariate dependence are very limited. We contribute to this domain by investigating spatial confounding from the analysis and data generation perspectives in a Bayesian coregionalized areal regression model. We derive novel results that distinguish variance inflation due to spatial confounding from inflation based on multicollinearity between predictors and provide insights into the estimation efficiency of a spatial estimator under a spatially confounded data generation model. We demonstrate favorable performance of spatial analysis compared to a non-spatial model in our simulation experiments even in the presence of spatial confounding and a misspecified spatial structure. In this regard, we align with several other authors in the defense of traditional hierarchical spatial models (Gilbert et al., 2025; Khan and Berrett, 2023; Zimmerman and Ver Hoef, 2022) and extend this defense to multivariate areal models. We analyze county-level data from the US on obesity / diabetes prevalence and diabetes-related cancer mortality, comparing the results with and without spatial random effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07232v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Lin Wu, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>FCPCA: Fuzzy clustering of high-dimensional time series based on common principal component analysis</title>
      <link>https://arxiv.org/abs/2505.07276</link>
      <description>arXiv:2505.07276v1 Announce Type: new 
Abstract: Clustering multivariate time series data is a crucial task in many domains, as it enables the identification of meaningful patterns and groups in time-evolving data. Traditional approaches, such as crisp clustering, rely on the assumption that clusters are sufficiently separated with little overlap. However, real-world data often defy this assumption, exhibiting overlapping distributions or overlapping clouds of points and blurred boundaries between clusters. Fuzzy clustering offers a compelling alternative by allowing partial membership in multiple clusters, making it well-suited for these ambiguous scenarios. Despite its advantages, current fuzzy clustering methods primarily focus on univariate time series, and for multivariate cases, even datasets of moderate dimensionality become computationally prohibitive. This challenge is further exacerbated when dealing with time series of varying lengths, leaving a clear gap in addressing the complexities of modern datasets. This work introduces a novel fuzzy clustering approach based on common principal component analysis to address the aforementioned shortcomings. Our method has the advantage of efficiently handling high-dimensional multivariate time series by reducing dimensionality while preserving critical temporal features. Extensive numerical results show that our proposed clustering method outperforms several existing approaches in the literature. An interesting application involving brain signals from different drivers recorded from a simulated driving experiment illustrates the potential of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07276v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ziling Ma, \'Angel L\'opez-Oriona, Hernando Ombao, Ying Sun</dc:creator>
    </item>
    <item>
      <title>On Data Sharpening in Nonparametric Autoregressive Models</title>
      <link>https://arxiv.org/abs/2505.07283</link>
      <description>arXiv:2505.07283v1 Announce Type: new 
Abstract: Data sharpening has been shown to reduce bias in nonparametric regression and density estimation. Its performance on nonlinear first order autoregressive models is studied theoretically and numerically in this paper. Although the asymptotic properties of data sharpening are not as favourable in the presence of serial dependence as in bivariate regression with independent responses, it is still found to reduce bias under mild conditions on the autoregression function. Numerical comparisons with the bias reduction method of Cheng et al. (2018) indicate that data sharpening is competitive in this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07283v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Snyman, Lengyi Han, W. John Braun</dc:creator>
    </item>
    <item>
      <title>Separable models for dynamic signed networks</title>
      <link>https://arxiv.org/abs/2505.07669</link>
      <description>arXiv:2505.07669v1 Announce Type: new 
Abstract: Signed networks capture the polarity of relationships between nodes, providing valuable insights into complex systems where both supportive and antagonistic interactions play a critical role in shaping the network's dynamics. We propose a separable temporal generative framework based on multi-layer exponential random graph models, characterised by the assumption of conditional independence between the sign and interaction effects. This structure preserves the flexibly and explanatory power inherent in the binary network specification while adhering to consistent balance theory assumptions. Using a fully probabilistic Bayesian paradigm, we infer the doubly intractable posterior distribution of model parameters via an adaptive Metropolis-Hastings approximate exchange algorithm. We illustrate the interpretability of our model by analysing signed relations among U.S. Senators during Ronald Reagan's second term (1985-1989). Specifically, we aim to understand whether these relations are consistent and balanced or reflect patterns of supportive or antagonistic alliances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07669v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alberto Caimo, Isabella Gollini</dc:creator>
    </item>
    <item>
      <title>Nonparametric Instrumental Variable Inference with Many Weak Instruments</title>
      <link>https://arxiv.org/abs/2505.07729</link>
      <description>arXiv:2505.07729v1 Announce Type: new 
Abstract: We study inference on linear functionals in the nonparametric instrumental variable (NPIV) problem with a discretely-valued instrument under a many-weak-instruments asymptotic regime, where the number of instrument values grows with the sample size. A key motivating example is estimating long-term causal effects in a new experiment with only short-term outcomes, using past experiments to instrument for the effect of short- on long-term outcomes. Here, the assignment to a past experiment serves as the instrument: we have many past experiments but only a limited number of units in each. Since the structural function is nonparametric but constrained by only finitely many moment restrictions, point identification typically fails. To address this, we consider linear functionals of the minimum-norm solution to the moment restrictions, which is always well-defined. As the number of instrument levels grows, these functionals define an approximating sequence to a target functional, replacing point identification with a weaker asymptotic notion suited to discrete instruments. Extending the Jackknife Instrumental Variable Estimator (JIVE) beyond the classical parametric setting, we propose npJIVE, a nonparametric estimator for solutions to linear inverse problems with many weak instruments. We construct automatic debiased machine learning estimators for linear functionals of both the structural function and its minimum-norm projection, and establish their efficiency in the many-weak-instruments regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07729v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, Nathan Kallus, Aur\'elien Bibaut</dc:creator>
    </item>
    <item>
      <title>Moderation effects and elasticities in compositional regression with a total. Application to Bayesian spatiotemporal modelling of all-cause mortality from environmental stressors</title>
      <link>https://arxiv.org/abs/2505.07800</link>
      <description>arXiv:2505.07800v1 Announce Type: new 
Abstract: Compositional regression models with a real-valued response variable can generally be specified as log-contrast models subject to a zero-sum constraint on the model coefficients. This formulation emphasises the relative information conveyed in the composition, while the overall total is regarded irrelevant. In this work, such a setting is extended to account not only for total effects, formally defined in a so-called T-space, but also for moderation or interaction effects. This is applied in the context of complex spatiotemporal data modelling, through an adaptation of the integrated nested Laplace approximation (INLA) method within a Bayesian estimation framework. Particular emphasis is placed on the interpretation of model coefficients and results, both on the original scale of the response variable and in terms of elasticities.
  The methodology is demonstrated through a detailed case study investigating the relationship between all-cause mortality and the interaction between extreme temperatures, air pollution composition, and total air pollution in Catalonia, Spain, during the summer of 2022. The results indicate that extreme temperatures are associated with an increased risk of mortality four days after exposure. Additionally, exposure to total air pollution, especially to NO2, is linked to elevated mortality risk regardless of temperature. In contrast, particulate matter is associated to increased mortality only when exposure occurs on days of extreme heat.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07800v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Germ\`a Coenders (Research Group on Statistics, Econometrics and Health, Centro de Investigaci\'on Biom\'edica en Red de Epidemiolog\'ia y Salud P\'ublica, Instituto de Salud Carlos III), Javier Palarea-Albaladejo (Dept. of Computer Science, Applied Mathematics and Statistics. University of Girona), Marc Saez (Research Group on Statistics, Econometrics and Health, Centro de Investigaci\'on Biom\'edica en Red de Epidemiolog\'ia y Salud P\'ublica, Instituto de Salud Carlos III), Maria A. Barcel\'o (Research Group on Statistics, Econometrics and Health, Centro de Investigaci\'on Biom\'edica en Red de Epidemiolog\'ia y Salud P\'ublica, Instituto de Salud Carlos III)</dc:creator>
    </item>
    <item>
      <title>Implementing Errors on Errors: Bayesian vs Frequentist</title>
      <link>https://arxiv.org/abs/2505.06521</link>
      <description>arXiv:2505.06521v1 Announce Type: cross 
Abstract: When combining apparently inconsistent experimental results, one often implements errors on errors. The Particle Data Group's phenomenological prescription offers a practical solution but lacks a firm theoretical foundation. To address this, D'Agostini and Cowan have proposed Bayesian and frequentist approaches, respectively, both introducing gamma-distributed auxiliary variables to model uncertainty in quoted errors. In this Letter, we show that these two formulations admit a parameter-by-parameter correspondence, and are structurally equivalent. This identification clarifies how Bayesian prior choices can be interpreted in terms of frequentist sampling assumptions, providing a unified probabilistic framework for modeling uncertainty in quoted variances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06521v1</guid>
      <category>hep-ph</category>
      <category>astro-ph.IM</category>
      <category>hep-ex</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Satoshi Mishima, Kin-ya Oda</dc:creator>
    </item>
    <item>
      <title>Streaming Sliced Optimal Transport</title>
      <link>https://arxiv.org/abs/2505.06835</link>
      <description>arXiv:2505.06835v1 Announce Type: cross 
Abstract: Sliced optimal transport (SOT) or sliced Wasserstein (SW) distance is widely recognized for its statistical and computational scalability. In this work, we further enhance the computational scalability by proposing the first method for computing SW from sample streams, called \emph{streaming sliced Wasserstein} (Stream-SW). To define Stream-SW, we first introduce the streaming computation of the one-dimensional Wasserstein distance. Since the one-dimensional Wasserstein (1DW) distance has a closed-form expression, given by the absolute difference between the quantile functions of the compared distributions, we leverage quantile approximation techniques for sample streams to define the streaming 1DW distance. By applying streaming 1DW to all projections, we obtain Stream-SW. The key advantage of Stream-SW is its low memory complexity while providing theoretical guarantees on the approximation error. We demonstrate that Stream-SW achieves a more accurate approximation of SW than random subsampling, with lower memory consumption, in comparing Gaussian distributions and mixtures of Gaussians from streaming samples. Additionally, we conduct experiments on point cloud classification, point cloud gradient flows, and streaming change point detection to further highlight the favorable performance of Stream-SW.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06835v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen</dc:creator>
    </item>
    <item>
      <title>Outperformance Score: A Universal Standardization Method for Confusion-Matrix-Based Classification Performance Metrics</title>
      <link>https://arxiv.org/abs/2505.07033</link>
      <description>arXiv:2505.07033v1 Announce Type: cross 
Abstract: Many classification performance metrics exist, each suited to a specific application. However, these metrics often differ in scale and can exhibit varying sensitivity to class imbalance rates in the test set. As a result, it is difficult to use the nominal values of these metrics to interpret and evaluate classification performances, especially when imbalance rates vary. To address this problem, we introduce the outperformance score function, a universal standardization method for confusion-matrix-based classification performance (CMBCP) metrics. It maps any given metric to a common scale of $[0,1]$, while providing a clear and consistent interpretation. Specifically, the outperformance score represents the percentile rank of the observed classification performance within a reference distribution of possible performances. This unified framework enables meaningful comparison and monitoring of classification performance across test sets with differing imbalance rates. We illustrate how the outperformance scores can be applied to a variety of commonly used classification performance metrics and demonstrate the robustness of our method through experiments on real-world datasets spanning multiple classification applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07033v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ningsheng Zhao, Trang Bui, Jia Yuan Yu, Krzysztof Dzieciolowski</dc:creator>
    </item>
    <item>
      <title>GMM with Many Weak Moment Conditions and Nuisance Parameters: General Theory and Applications to Causal Inference</title>
      <link>https://arxiv.org/abs/2505.07295</link>
      <description>arXiv:2505.07295v1 Announce Type: cross 
Abstract: Weak identification is a common issue for many statistical problems -- for example, when instrumental variables are weakly correlated with treatment, or when proxy variables are weakly correlated with unmeasured confounders. Under weak identification, standard estimation methods, such as the generalized method of moments (GMM), can have sizeable bias in finite samples or even asymptotically. In addition, many practical settings involve a growing number of nuisance parameters, adding further complexity to the problem. In this paper, we study estimation and inference under a general nonlinear moment model with many weak moment conditions and many nuisance parameters. To obtain debiased inference for finite-dimensional target parameters, we demonstrate that Neyman orthogonality plays a stronger role than in conventional settings with strong identification. We study a general two-step debiasing estimator that allows for possibly nonparametric first-step estimation of nuisance parameters, and we establish its consistency and asymptotic normality under a many weak moment asymptotic regime. Our theory accommodates both high-dimensional moment conditions and infinite-dimensional nuisance parameters. We provide high-level assumptions for a general setting and discuss specific applications to the problems of estimation and inference with weak instruments and weak proxies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07295v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Wang, Kwun Chuen Gary Chan, Ting Ye</dc:creator>
    </item>
    <item>
      <title>Generalization Bounds and Stopping Rules for Learning with Self-Selected Data</title>
      <link>https://arxiv.org/abs/2505.07367</link>
      <description>arXiv:2505.07367v1 Announce Type: cross 
Abstract: Many learning paradigms self-select training data in light of previously learned parameters. Examples include active learning, semi-supervised learning, bandits, or boosting. Rodemann et al. (2024) unify them under the framework of "reciprocal learning". In this article, we address the question of how well these methods can generalize from their self-selected samples. In particular, we prove universal generalization bounds for reciprocal learning using covering numbers and Wasserstein ambiguity sets. Our results require no assumptions on the distribution of self-selected data, only verifiable conditions on the algorithms. We prove results for both convergent and finite iteration solutions. The latter are anytime valid, thereby giving rise to stopping rules for a practitioner seeking to guarantee the out-of-sample performance of their reciprocal learning algorithm. Finally, we illustrate our bounds and stopping rules for reciprocal learning's special case of semi-supervised learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07367v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian Rodemann, James Bailie</dc:creator>
    </item>
    <item>
      <title>A Framework for Statistical Inference via Randomized Algorithms</title>
      <link>https://arxiv.org/abs/2307.11255</link>
      <description>arXiv:2307.11255v5 Announce Type: replace 
Abstract: Randomized algorithms, such as randomized sketching or stochastic optimization, are a promising approach to ease the computational burden in analyzing large datasets. However, randomized algorithms also produce non-deterministic outputs, leading to the problem of evaluating their accuracy. In this paper, we develop a statistical inference framework for quantifying the uncertainty of the outputs of randomized algorithms.
  Our key conclusion is that one can perform statistical inference for the target of a sequence of randomized algorithms as long as in the limit, their outputs fluctuate around the target according to any (possibly unknown) probability distribution. In this setting, we develop appropriate statistical inference methods -- sub-randomization, multi-run plug-in and multi-run aggregation -- by estimating the unknown parameters of the limiting distribution either using multiple runs of the randomized algorithm, or by tailored estimates.
  As illustrations, we develop methods for statistical inference when using stochastic optimization (such as Polyak-Ruppert averaging in stochastic gradient descent and stochastic optimization with momentum). We also illustrate our methods in inference for least squares parameters via randomized sketching, by characterizing the limiting distributions of sketching estimates in a possibly growing dimensional case. We further characterize the computation and communication cost of our methods, showing that in certain cases, they add negligible overhead. The results are supported via a broad range of simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11255v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhixiang Zhang, Sokbae Lee, Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Reconciling Overt Bias and Hidden Bias in Sensitivity Analysis for Matched Observational Studies</title>
      <link>https://arxiv.org/abs/2311.11216</link>
      <description>arXiv:2311.11216v3 Announce Type: replace 
Abstract: Matching is one of the most widely used causal inference designs in observational studies, but post-matching confounding bias remains a challenge. This bias includes overt bias from inexact matching on measured confounders and hidden bias from the existence of unmeasured confounders. Researchers commonly apply the Rosenbaum-type sensitivity analysis framework after matching to assess the impact of these biases on causal conclusions. In this work, we show that this approach is often conservative because the solution to the Rosenbaum-type sensitivity model may allocate hypothetical hidden bias in ways that contradict the overt bias observed in the matched dataset. To address this problem, we propose an iterative convex programming approach that enhances sensitivity analysis by ensuring consistency between hidden and overt biases. The validity of our approach does not rely on modeling assumptions for treatment or outcome variables. Extensive simulations demonstrate substantial gains in statistical power of sensitivity analysis, and a real-world data application illustrates the practical benefits of our approach. We have also developed an open-source R package to facilitate the implementation of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11216v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyu Heng, Yanxin Shen, Pengyun Wang</dc:creator>
    </item>
    <item>
      <title>Generalized M-Estimation in Censored Regression Model under Endogeneity</title>
      <link>https://arxiv.org/abs/2312.10690</link>
      <description>arXiv:2312.10690v2 Announce Type: replace 
Abstract: We propose and study M-estimation to estimate the parameters in the censored regression model in the presence of endogeneity, i.e., the Tobit model. In the course of this study, we follow two-stage procedures: the first stage consists of applying control function procedures to address the issue of endogeneity using instrumental variables, and the second stage applies the M-estimation technique to estimate the unknown parameters involved in the model. The large sample properties of the proposed estimators are derived and analyzed. The finite sample properties of the estimators are studied through Monte Carlo simulation and a real data application related to women's labor force participation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10690v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swati Shukla, Subhra Sankar Dhar,  Shalabh</dc:creator>
    </item>
    <item>
      <title>Debiased calibration estimation using generalized entropy in survey sampling</title>
      <link>https://arxiv.org/abs/2404.01076</link>
      <description>arXiv:2404.01076v4 Announce Type: replace 
Abstract: Incorporating the auxiliary information into the survey estimation is a fundamental problem in survey sampling. Calibration weighting is a popular tool for incorporating the auxiliary information. The calibration weighting method of Deville and Sarndal (1992) uses a distance measure between the design weights and the final weights to solve the optimization problem with calibration constraints. This paper introduces a novel framework that leverages generalized entropy as the objective function for optimization, where design weights play a role in the constraints to ensure design consistency, rather than being part of the objective function. This innovative calibration framework is particularly attractive due to its generality and its ability to generate more efficient calibration weights compared to traditional methods based on Deville and Sarndal (1992). Furthermore, we identify the optimal choice of the generalized entropy function that achieves the minimum variance across various choices of the generalized entropy function under the same constraints. Asymptotic properties, such as design consistency and asymptotic normality, are presented rigorously. The results from a limited simulation study are also presented. We demonstrate a real-life application using agricultural survey data collected from Kynetec, Inc.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01076v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yonghyun Kwon, Jae Kwang Kim, Yumou Qiu</dc:creator>
    </item>
    <item>
      <title>Reverse time-to-death as time-scale in time-to-event analysis for studies of advanced illness and palliative care</title>
      <link>https://arxiv.org/abs/2407.02178</link>
      <description>arXiv:2407.02178v2 Announce Type: replace 
Abstract: Background: Incidence of adverse outcome events rises as patients with advanced illness approach end-of-life. Exposures that tend to occur near end-of-life, e.g., use of wheelchair, oxygen therapy and palliative care, may therefore be found associated with the incidence of the adverse outcomes. We propose a strategy for time-to-event analysis to mitigate the time-varying confounding. Methods: We propose a concept of reverse time-to-death (rTTD) and its use for the time-scale in time-to-event analysis. We used data on community-based palliative care uptake (exposure) and emergency department visits (outcome) among patients with advanced cancer in Singapore to illustrate. We compare the results against that of the common practice of using time-on-study (TOS) as time-scale. Results: Graphical analysis demonstrated that cancer patients receiving palliative care had higher rate of emergency department visits than non-recipients mainly because they were closer to end-of-life, and that rTTD analysis made comparison between patients at the same time-to-death. Analysis of emergency department visits in relation to palliative care using TOS time-scale showed significant increase in hazard ratio estimate when observed time-varying covariates were omitted from statistical adjustment (change-in-estimate=0.38; 95% CI 0.15 to 0.60). There was no such change in otherwise the same analysis using rTTD (change-in-estimate=0.04; 95% CI -0.02 to 0.11), demonstrating the ability of rTTD time-scale to mitigate confounding that intensifies in relation to time-to-death. Conclusion: Use of rTTD as time-scale in time-to-event analysis provides a simple and robust approach to control time-varying confounding in studies of advanced illness, even if the confounders are unmeasured.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02178v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Statistics in Medicine 2025; 44(3-4)</arxiv:journal_reference>
      <dc:creator>Yin Bun Cheung, Xiangmei Ma, Isha Chaudhry, Nan Liu, Qingyuan Zhuang, Grace Meijuan Yang, Chetna Malhotra, Eric Andrew Finkelstein</dc:creator>
    </item>
    <item>
      <title>Deep Fr\'echet Regression</title>
      <link>https://arxiv.org/abs/2407.21407</link>
      <description>arXiv:2407.21407v2 Announce Type: replace 
Abstract: Advancements in modern science have led to the increasing availability of non-Euclidean data in metric spaces. This paper addresses the challenge of modeling relationships between non-Euclidean responses and multivariate Euclidean predictors. We propose a flexible regression model capable of handling high-dimensional predictors without imposing parametric assumptions. Two primary challenges are addressed: the curse of dimensionality in nonparametric regression and the absence of linear structure in general metric spaces. The former is tackled using deep neural networks, while for the latter we demonstrate the feasibility of mapping the metric space where responses reside to a low-dimensional Euclidean space using manifold learning. We introduce a reverse mapping approach, employing local Fr\'echet regression, to map the low-dimensional manifold representations back to objects in the original metric space. We develop a theoretical framework, investigating the convergence rate of deep neural networks under dependent sub-Gaussian noise with bias. The convergence rate of the proposed regression model is then obtained by expanding the scope of local Fr\'echet regression to accommodate multivariate predictors in the presence of errors in predictors. Simulations and case studies show that the proposed model outperforms existing methods for non-Euclidean responses, focusing on the special cases of probability distributions and networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21407v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Su I Iao, Yidong Zhou, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>Adjusted Nelson--Aalen estimators by inverse treatment probability weighting with an estimated propensity score</title>
      <link>https://arxiv.org/abs/2410.00338</link>
      <description>arXiv:2410.00338v2 Announce Type: replace 
Abstract: Inverse probability of treatment weighting (IPW) has been well applied in causal inference to estimate population-level estimands from observational studies. For time-to-event outcomes, the failure time distribution can be estimated by estimating the cumulative hazard in the presence of random right censoring. IPW can be performed by weighting the event counting process and at-risk process by the inverse treatment probability, resulting in an adjusted Nelson--Aalen estimator for the population-level counterfactual cumulative incidence function. We consider the adjusted Nelson--Aalen estimator with an estimated propensity score in the competing risks setting. When the estimated propensity score is regular and asymptotically linear, we derive the influence functions for the counterfactual cumulative hazard and cumulative incidence. Then we establish the asymptotic properties for the estimators. We show that the uncertainty in the estimated propensity score contributes to an additional variation in the estimators. However, through simulation and real-data application, we find that such an additional variation is usually small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00338v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhao Deng, Rui Wang</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametric Sensitivity Analysis of Multiple Test Procedures Under Dependence</title>
      <link>https://arxiv.org/abs/2410.08080</link>
      <description>arXiv:2410.08080v3 Announce Type: replace 
Abstract: This article introduces a sensitivity analysis method for Multiple Testing Procedures (MTPs) using marginal $p$-values. The method is based on the Dirichlet process (DP) prior distribution, specified to support the entire space of MTPs, where each MTP controls either the family-wise error rate (FWER) or the false discovery rate (FDR) under arbitrary dependence between $p$-values. The DP MTP sensitivity analysis method accounts for uncertainty in the selection of such MTPs and their respective cut-off points and decisions regarding which subset of $p$-values are significant from a given set of hypothesis tested, while measuring each $p$-value's probability of significance over the DP prior predictive distribution of this space of all MTPs, and reducing the possible conservativeness of using only one such MTP for multiple testing. The DP MTP sensitivity analysis method is illustrated through the analysis of twenty-eight thousand $p$-values arising from hypothesis tests performed on a 2022 dataset of a representative sample of three million U.S. high school students observed on 239 variables. They include tests that relate variables about the disruption caused by school closures during the COVID-19 pandemic, with variables on mathematical cognition and academic achievement, and with student background variables. R software code for the DP MTP sensitivity analysis method is provided in the Appendix and in Supplementary Information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08080v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Karabatsos</dc:creator>
    </item>
    <item>
      <title>A New One Parameter Unit Distribution: Median Based Unit Rayleigh (MBUR): Parametric Quantile Regression Model</title>
      <link>https://arxiv.org/abs/2410.14857</link>
      <description>arXiv:2410.14857v2 Announce Type: replace 
Abstract: Parametric quantile regression is illustrated for the one parameter new unit Rayleigh distribution called Median Based Unit Rayleigh distribution (MBUR) distribution. The estimation process using re-parameterized maximum likelihood function is highlighted with real dataset example. The inference and goodness of fit is also explored.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14857v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohamed Attia</dc:creator>
    </item>
    <item>
      <title>Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)</title>
      <link>https://arxiv.org/abs/2501.19047</link>
      <description>arXiv:2501.19047v4 Announce Type: replace 
Abstract: To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19047v4</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>https://iclr-blogposts.github.io/2025/blog/calibration/</arxiv:journal_reference>
      <dc:creator>Maja Pavlovic</dc:creator>
    </item>
    <item>
      <title>Identification and estimation of vaccine effectiveness in the test-negative design under equi-confounding</title>
      <link>https://arxiv.org/abs/2504.20360</link>
      <description>arXiv:2504.20360v3 Announce Type: replace 
Abstract: The test-negative design (TND) is frequently used to evaluate vaccine effectiveness in real-world settings. In a TND study, individuals with similar symptoms who seek care are tested for the disease of interest, and vaccine effectiveness is estimated by comparing the vaccination history of test-positive cases and test-negative controls. Traditional approaches justify the TND by assuming either (a) receiving a test is a perfect proxy for unmeasured health-seeking behavior or (b) vaccination is unconfounded given measured covariates -- both of which may be unrealistic in practice. In this paper, we return to the original motivation for the TND and propose an alternative justification based on the assumption of odds ratio equi-confounding, where unmeasured confounders influence test-positive and test-negative individuals equivalently on the odds ratio scale. We discuss the implications of this assumption for TND design and provide alternative estimators for the marginal risk ratio among the vaccinated under equi-confounding, including estimators based on outcome modeling and inverse probability weighting as well as a semiparametric estimator that is doubly-robust. When the equi-confounding assumption does not hold, we suggest a sensitivity analysis that parameterizes the magnitude of the deviation on the odds ratio scale. We conduct a simulation study to evaluate the empirical performance of our proposed estimators under a wide range of scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20360v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Christopher B. Boyer, Kendrick Qijun Li, Xu Shi, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Design-Based Inference under Random Potential Outcomes via Riesz Representation</title>
      <link>https://arxiv.org/abs/2505.01324</link>
      <description>arXiv:2505.01324v4 Announce Type: replace 
Abstract: We introduce a general framework for design-based causal inference that accommodates random potential outcomes, thereby extending the classical Neyman-Rubin setup in which outcomes are treated as fixed. In our formulation, each unit's potential outcome is modelled as a function \( \tilde{y}_i(z, \omega) \), where \( \omega \) denotes latent randomness external to the treatment assignment. Building on recent work connecting design-based estimation with the Riesz representation theorem, we construct causal estimators by embedding potential outcomes in a Hilbert space and defining treatment effects as linear functionals. This approach yields unbiased and consistent estimators, even when outcomes exhibit random variation. The framework retains the key strength of design-based analysis, identification via a known randomisation scheme, while enabling inference in settings with outcome-level stochasticity. We establish large-sample properties under local dependence and propose plug-in variance estimators, including a correlation-based version that improves efficiency under sparse dependence. A simulation study illustrates the finite-sample behaviour of the estimator. Our results unify design-based reasoning with random outcome modelling, broadening the applicability of causal inference in complex experimental environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01324v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yukai Yang</dc:creator>
    </item>
    <item>
      <title>Robust Estimation and Inference in Panels with Interactive Fixed Effects</title>
      <link>https://arxiv.org/abs/2210.06639</link>
      <description>arXiv:2210.06639v4 Announce Type: replace-cross 
Abstract: We consider estimation and inference for a regression coefficient in panels with interactive fixed effects (i.e., with a factor structure). We demonstrate that existing estimators and confidence intervals (CIs) can be heavily biased and size-distorted when some of the factors are weak. We propose estimators with improved rates of convergence and bias-aware CIs that remain valid uniformly, regardless of factor strength. Our approach applies the theory of minimax linear estimation to form a debiased estimate, using a nuclear norm bound on the error of an initial estimate of the interactive fixed effects. Our resulting bias-aware CIs take into account the remaining bias caused by weak factors. Monte Carlo experiments show substantial improvements over conventional methods when factors are weak, with minimal costs to estimation accuracy when factors are strong.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.06639v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy B. Armstrong, Martin Weidner, Andrei Zeleneev</dc:creator>
    </item>
    <item>
      <title>Optimal Cross-Validation for Sparse Linear Regression</title>
      <link>https://arxiv.org/abs/2306.14851</link>
      <description>arXiv:2306.14851v3 Announce Type: replace-cross 
Abstract: Given a high-dimensional covariate matrix and a response vector, ridge-regularized sparse linear regression selects a subset of features that explains the relationship between covariates and the response in an interpretable manner. To select the sparsity and robustness of linear regressors, techniques like k-fold cross-validation are commonly used for hyperparameter tuning. However, cross-validation substantially increases the computational cost of sparse regression as it requires solving many mixed-integer optimization problems (MIOs) for each hyperparameter combination. To improve upon this state of affairs, we obtain computationally tractable relaxations of k-fold cross-validation metrics, facilitating hyperparameter selection after solving 50-80% fewer MIOs in practice. These relaxations result in an efficient cyclic coordinate descent scheme, achieving 10%-30% lower validation errors than via traditional methods such as grid search with MCP or GLMNet across a suite of 13 real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14851v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Cory-Wright, Andr\'es G\'omez</dc:creator>
    </item>
    <item>
      <title>Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach</title>
      <link>https://arxiv.org/abs/2402.01454</link>
      <description>arXiv:2402.01454v5 Announce Type: replace-cross 
Abstract: In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is important for reasonable causal models reflecting the broad knowledge of domain experts, despite the challenges in the systematic acquisition of background knowledge. To overcome these challenges, this paper proposes a novel method for causal inference, in which SCD and knowledge-based causal inference (KBCI) with a large language model (LLM) are synthesized through ``statistical causal prompting (SCP)'' for LLMs and prior knowledge augmentation for SCD. The experiments in this work have revealed that the results of LLM-KBCI and SCD augmented with LLM-KBCI approach the ground truths, more than the SCD result without prior knowledge. These experiments have also revealed that the SCD result can be further improved if the LLM undergoes SCP. Furthermore, with an unpublished real-world dataset, we have demonstrated that the background knowledge provided by the LLM can improve the SCD on this dataset, even if this dataset has never been included in the training data of the LLM. For future practical application of this proposed method across important domains such as healthcare, we also thoroughly discuss the limitations, risks of critical errors, expected improvement of techniques around LLMs, and realistic integration of expert checks of the results into this automatic process, with SCP simulations under various conditions both in successful and failure scenarios. The careful and appropriate application of the proposed approach in this work, with improvement and customization for each domain, can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains.
  The code used in this work is publicly available at: www.github.com/mas-takayama/LLM-and-SCD</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01454v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Published in Transactions in Machine Learning Research (05/2025) https://openreview.net/forum?id=Reh1S8rxfh</arxiv:journal_reference>
      <dc:creator>Masayuki Takayama, Tadahisa Okuda, Thong Pham, Tatsuyoshi Ikenoue, Shingo Fukuma, Shohei Shimizu, Akiyoshi Sannai</dc:creator>
    </item>
    <item>
      <title>Local linear smoothing for regression surfaces on the simplex using Dirichlet kernels</title>
      <link>https://arxiv.org/abs/2408.07209</link>
      <description>arXiv:2408.07209v4 Announce Type: replace-cross 
Abstract: This paper introduces a local linear smoother for regression surfaces on the simplex. The estimator solves a least-squares regression problem weighted by a locally adaptive Dirichlet kernel, ensuring good boundary properties. Asymptotic results for the bias, variance, mean squared error, and mean integrated squared error are derived, generalizing the univariate results of Chen [Ann. Inst. Statist. Math., 54(2) (2002), pp. 312-323]. A simulation study shows that the proposed local linear estimator with Dirichlet kernel outperforms its only direct competitor in the literature, the Nadaraya-Watson estimator with Dirichlet kernel due to Bouzebda, Nezzal and Elhattab [AIMS Math., 9(9) (2024), pp. 26195-26282].</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07209v4</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Genest, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Isolated Causal Effects of Natural Language</title>
      <link>https://arxiv.org/abs/2410.14812</link>
      <description>arXiv:2410.14812v2 Announce Type: replace-cross 
Abstract: As language technologies become widespread, it is important to understand how changes in language affect reader perceptions and behaviors. These relationships may be formalized as the isolated causal effect of some focal language-encoded intervention (e.g., factual inaccuracies) on an external outcome (e.g., readers' beliefs). In this paper, we introduce a formal estimation framework for isolated causal effects of language. We show that a core challenge of estimating isolated effects is the need to approximate all non-focal language outside of the intervention. Drawing on the principle of omitted variable bias, we provide measures for evaluating the quality of both non-focal language approximations and isolated effect estimates themselves. We find that poor approximation of non-focal language can lead to bias in the corresponding isolated effect estimates due to omission of relevant variables, and we show how to assess the sensitivity of effect estimates to such bias along the two key axes of fidelity and overlap. In experiments on semi-synthetic and real-world data, we validate the ability of our framework to correctly recover isolated effects and demonstrate the utility of our proposed measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14812v2</guid>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael</dc:creator>
    </item>
    <item>
      <title>The Leaderboard Illusion</title>
      <link>https://arxiv.org/abs/2504.20879</link>
      <description>arXiv:2504.20879v2 Announce Type: replace-cross 
Abstract: Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable AI systems. Yet, in this work we identify systematic issues that have resulted in a distorted playing field. We find that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. We establish that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, we identify 27 private LLM variants tested by Meta in the lead-up to the Llama-4 release. We also establish that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7% of the total data. We show that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution, based on our conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. We offer actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking for the field</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20879v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shivalika Singh, Yiyang Nan, Alex Wang, Daniel D'Souza, Sayash Kapoor, Ahmet \"Ust\"un, Sanmi Koyejo, Yuntian Deng, Shayne Longpre, Noah A. Smith, Beyza Ermis, Marzieh Fadaee, Sara Hooker</dc:creator>
    </item>
    <item>
      <title>Particle Gibbs without the Gibbs bit</title>
      <link>https://arxiv.org/abs/2505.04611</link>
      <description>arXiv:2505.04611v3 Announce Type: replace-cross 
Abstract: Exact parameter and trajectory inference in state-space models is typically achieved by one of two methods: particle marginal Metropolis-Hastings (PMMH) or particle Gibbs (PGibbs). PMMH is a pseudo-marginal algorithm which jointly proposes a new trajectory and parameter, and accepts or rejects both at once. PGibbs instead alternates between sampling from the trajectory, using an algorithm known as conditional sequential Monte Carlo (CSMC) and the parameter in a Hastings-within-Gibbs fashion. While particle independent Metropolis Hastings (PIMH), the parameter-free version of PMMH, is known to be statistically worse than CSMC, PGibbs can induce a slow mixing if the parameter and the state trajectory are very correlated. This has made PMMH the method of choice for many practitioners, despite theory and experiments favouring CSMC over PIMH for the parameter-free problem. In this article, we describe a formulation of PGibbs which bypasses the Gibbs step, essentially marginalizing over the trajectory distribution in a fashion similar to PMMH. This is achieved by considering the implementation of a CSMC algortihm for the state-space model integrated over the joint distribution of the current parameter and the parameter proposal. We illustrate the benefits of method on a simple example known to be challenging for PMMH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04611v3</guid>
      <category>stat.CO</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 13 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Corenflos</dc:creator>
    </item>
  </channel>
</rss>
