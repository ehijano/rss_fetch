<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Nov 2024 03:49:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Adaptive Physics-Guided Neural Network</title>
      <link>https://arxiv.org/abs/2411.10064</link>
      <description>arXiv:2411.10064v1 Announce Type: new 
Abstract: This paper introduces an adaptive physics-guided neural network (APGNN) framework for predicting quality attributes from image data by integrating physical laws into deep learning models. The APGNN adaptively balances data-driven and physics-informed predictions, enhancing model accuracy and robustness across different environments. Our approach is evaluated on both synthetic and real-world datasets, with comparisons to conventional data-driven models such as ResNet. For the synthetic data, 2D domains were generated using three distinct governing equations: the diffusion equation, the advection-diffusion equation, and the Poisson equation. Non-linear transformations were applied to these domains to emulate complex physical processes in image form.
  In real-world experiments, the APGNN consistently demonstrated superior performance in the diverse thermal image dataset. On the cucumber dataset, characterized by low material diversity and controlled conditions, APGNN and PGNN showed similar performance, both outperforming the data-driven ResNet. However, in the more complex thermal dataset, particularly for outdoor materials with higher environmental variability, APGNN outperformed both PGNN and ResNet by dynamically adjusting its reliance on physics-based versus data-driven insights. This adaptability allowed APGNN to maintain robust performance across structured, low-variability settings and more heterogeneous scenarios. These findings underscore the potential of adaptive physics-guided learning to integrate physical constraints effectively, even in challenging real-world contexts with diverse environmental conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10064v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>David Shulman, Itai Dattner</dc:creator>
    </item>
    <item>
      <title>G-computation for increasing performances of clinical trials with individual randomization and binary response</title>
      <link>https://arxiv.org/abs/2411.10089</link>
      <description>arXiv:2411.10089v1 Announce Type: new 
Abstract: In a clinical trial, the random allocation aims to balance prognostic factors between arms, preventing true confounders. However, residual differences due to chance may introduce near-confounders. Adjusting on prognostic factors is therefore recommended, especially because the related increase of the power. In this paper, we hypothesized that G-computation associated with machine learning could be a suitable method for randomized clinical trials even with small sample sizes. It allows for flexible estimation of the outcome model, even when the covariates' relationships with outcomes are complex. Through simulations, penalized regressions (Lasso, Elasticnet) and algorithm-based methods (neural network, support vector machine, super learner) were compared. Penalized regressions reduced variance but may introduce a slight increase in bias. The associated reductions in sample size ranged from 17\% to 54\%. In contrast, algorithm-based methods, while effective for larger and more complex data structures, underestimated the standard deviation, especially with small sample sizes. In conclusion, G-computation with penalized models, particularly Elasticnet with splines when appropriate, represents a relevant approach for increasing the power of RCTs and accounting for potential near-confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10089v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joe de Keizer, R\'emi Lenain, Rapha\"el Porcher, Sarah Zoha, Arthur Chatton, Yohann Foucher</dc:creator>
    </item>
    <item>
      <title>Quadratic Form based Multiple Contrast Tests for Comparison of Group Means</title>
      <link>https://arxiv.org/abs/2411.10121</link>
      <description>arXiv:2411.10121v1 Announce Type: new 
Abstract: Comparing the mean vectors across different groups is a cornerstone in the realm of multivariate statistics, with quadratic forms commonly serving as test statistics. However, when the overall hypothesis is rejected, identifying specific vector components or determining the groups among which differences exist requires additional investigations. Conversely, employing multiple contrast tests (MCT) allows conclusions about which components or groups contribute to these differences. However, they come with a trade-off, as MCT lose some benefits inherent to quadratic forms. In this paper, we combine both approaches to get a quadratic form based multiple contrast test that leverages the advantages of both. To understand its theoretical properties, we investigate its asymptotic distribution in a semiparametric model. We thereby focus on two common quadratic forms - the Wald-type statistic and the Anova-type statistic - although our findings are applicable to any quadratic form.
  Furthermore, we employ Monte-Carlo and resampling techniques to enhance the test's performance in small sample scenarios. Through an extensive simulation study, we assess the performance of our proposed tests against existing alternatives, highlighting their advantages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10121v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paavo Sattler, Markus Pauly, Merle Munko</dc:creator>
    </item>
    <item>
      <title>Fused Gromov-Wasserstein Variance Decomposition with Linear Optimal Transport</title>
      <link>https://arxiv.org/abs/2411.10204</link>
      <description>arXiv:2411.10204v1 Announce Type: new 
Abstract: Wasserstein distances form a family of metrics on spaces of probability measures that have recently seen many applications. However, statistical analysis in these spaces is complex due to the nonlinearity of Wasserstein spaces. One potential solution to this problem is Linear Optimal Transport (LOT). This method allows one to find a Euclidean embedding, called LOT embedding, of measures in some Wasserstein spaces, but some information is lost in this embedding. So, to understand whether statistical analysis relying on LOT embeddings can make valid inferences about original data, it is helpful to quantify how well these embeddings describe that data. To answer this question, we present a decomposition of the Fr\'echet variance of a set of measures in the 2-Wasserstein space, which allows one to compute the percentage of variance explained by LOT embeddings of those measures. We then extend this decomposition to the Fused Gromov-Wasserstein setting. We also present several experiments that explore the relationship between the dimension of the LOT embedding, the percentage of variance explained by the embedding, and the classification accuracy of machine learning classifiers built on the embedded data. We use the MNIST handwritten digits dataset, IMDB-50000 dataset, and Diffusion Tensor MRI images for these experiments. Our results illustrate the effectiveness of low dimensional LOT embeddings in terms of the percentage of variance explained and the classification accuracy of models built on the embedded data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10204v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Michael Wilson, Tom Needham, Anuj Srivastava</dc:creator>
    </item>
    <item>
      <title>Bayesian Adaptive Tucker Decompositions for Tensor Factorization</title>
      <link>https://arxiv.org/abs/2411.10218</link>
      <description>arXiv:2411.10218v1 Announce Type: new 
Abstract: Tucker tensor decomposition offers a more effective representation for multiway data compared to the widely used PARAFAC model. However, its flexibility brings the challenge of selecting the appropriate latent multi-rank. To overcome the issue of pre-selecting the latent multi-rank, we introduce a Bayesian adaptive Tucker decomposition model that infers the multi-rank automatically via an infinite increasing shrinkage prior. The model introduces local sparsity in the core tensor, inducing rich and at the same time parsimonious dependency structures. Posterior inference proceeds via an efficient adaptive Gibbs sampler, supporting both continuous and binary data and allowing for straightforward missing data imputation when dealing with incomplete multiway data. We discuss fundamental properties of the proposed modeling framework, providing theoretical justification. Simulation studies and applications to chemometrics and complex ecological data offer compelling evidence of its advantages over existing tensor factorization methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10218v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federica Stolf, Antonio Canale</dc:creator>
    </item>
    <item>
      <title>Generalized Conditional Functional Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2411.10312</link>
      <description>arXiv:2411.10312v1 Announce Type: new 
Abstract: We propose generalized conditional functional principal components analysis (GC-FPCA) for the joint modeling of the fixed and random effects of non-Gaussian functional outcomes. The method scales up to very large functional data sets by estimating the principal components of the covariance matrix on the linear predictor scale conditional on the fixed effects. This is achieved by combining three modeling innovations: (1) fit local generalized linear mixed models (GLMMs) conditional on covariates in windows along the functional domain; (2) conduct a functional principal component analysis (FPCA) on the person-specific functional effects obtained by assembling the estimated random effects from the local GLMMs; and (3) fit a joint functional mixed effects model conditional on covariates and the estimated principal components from the previous step. GC-FPCA was motivated by modeling the minute-level active/inactive profiles over the day ($1{,}440$ 0/1 measurements per person) for $8{,}700$ study participants in the National Health and Nutrition Examination Survey (NHANES) 2011-2014. We show that state-of-the-art approaches cannot handle data of this size and complexity, while GC-FPCA can.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10312v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Lu, Xinkai Zhou, Erjia Cui, Dustin Rogers, Ciprian M. Crainiceanu, Julia Wrobel, Andrew Leroux</dc:creator>
    </item>
    <item>
      <title>An Instrumental Variables Framework to Unite Spatial Confounding Methods</title>
      <link>https://arxiv.org/abs/2411.10381</link>
      <description>arXiv:2411.10381v1 Announce Type: new 
Abstract: Studies investigating the causal effects of spatially varying exposures on health$\unicode{x2013}$such as air pollution, green space, or crime$\unicode{x2013}$often rely on observational and spatially indexed data. A prevalent challenge is unmeasured spatial confounding, where an unobserved spatially varying variable affects both exposure and outcome, leading to biased causal estimates and invalid confidence intervals. In this paper, we introduce a general framework based on instrumental variables (IV) that encompasses and unites most of the existing methods designed to account for an unmeasured spatial confounder. We show that a common feature of all existing methods is their reliance on small-scale variation in exposure, which functions as an IV. In this framework, we outline the underlying assumptions and the estimation strategy of each method. Furthermore, we demonstrate that the IV can be used to identify and estimate the exposure-response curve under more relaxed assumptions. We conclude by estimating the exposure-response curve between long-term exposure to fine particulate matter and all-cause mortality among 33,454 zip codes in the United States while adjusting for unmeasured spatial confounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10381v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sophie M. Woodward, Mauricio Tec, Francesca Dominici</dc:creator>
    </item>
    <item>
      <title>Spatio-Temporal Jump Model for Urban Thermal Comfort Monitoring</title>
      <link>https://arxiv.org/abs/2411.09726</link>
      <description>arXiv:2411.09726v2 Announce Type: cross 
Abstract: Thermal comfort is essential for well-being in urban spaces, especially as cities face increasing heat from urbanization and climate change. Existing thermal comfort models usually overlook temporal dynamics alongside spatial dependencies. We address this problem by introducing a spatio-temporal jump model that clusters data with persistence across both spatial and temporal dimensions. This framework enhances interpretability, minimizes abrupt state changes, and easily handles missing data. We validate our approach through extensive simulations, demonstrating its accuracy in recovering the true underlying partition. When applied to hourly environmental data gathered from a set of weather stations located across the city of Singapore, our proposal identifies meaningful thermal comfort regimes, demonstrating its effectiveness in dynamic urban settings and suitability for real-world monitoring. The comparison of these regimes with feedback on thermal preference indicates the potential of an unsupervised approach to avoid extensive surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09726v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Federico P. Cortese, Antonio Pievatolo</dc:creator>
    </item>
    <item>
      <title>Estimating the Cost of Informal Care with a Novel Two-Stage Approach to Individual Synthetic Control</title>
      <link>https://arxiv.org/abs/2411.10314</link>
      <description>arXiv:2411.10314v1 Announce Type: cross 
Abstract: Informal carers provide the majority of care for people living with challenges related to older age, long-term illness, or disability. However, the care they provide often results in a significant income penalty for carers, a factor largely overlooked in the economics literature and policy discourse. Leveraging data from the UK Household Longitudinal Study, this paper provides the first robust causal estimates of the caring income penalty using a novel individual synthetic control based method that accounts for unit-level heterogeneity in post-treatment trajectories over time. Our baseline estimates identify an average relative income gap of up to 45%, with an average decrease of {\pounds}162 in monthly income, peaking at {\pounds}192 per month after 4 years, based on the difference between informal carers providing the highest-intensity of care and their synthetic counterparts. We find that the income penalty is more pronounced for women than for men, and varies by ethnicity and age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10314v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Petrillo, Daniel Valdenegro Ibarra, Charles Rahal, Yanan Zhang, Gwilym Pryce, Matthew R. Bennett</dc:creator>
    </item>
    <item>
      <title>A novel family of beta mixture models for the differential analysis of DNA methylation data: an application to prostate cancer</title>
      <link>https://arxiv.org/abs/2211.01938</link>
      <description>arXiv:2211.01938v4 Announce Type: replace 
Abstract: Identifying differentially methylated cytosine-guanine dinucleotide (CpG) sites between benign and tumour samples can assist in understanding disease. However, differential analysis of bounded DNA methylation data often requires data transformation, reducing biological interpretability. To address this, a family of beta mixture models (BMMs) is proposed that (i) objectively infers methylation state thresholds and (ii) identifies differentially methylated CpG sites (DMCs) given untransformed, beta-valued methylation data. The BMMs achieve this through model-based clustering of CpG sites and by employing parameter constraints, facilitating application to different study settings. Inference proceeds via an expectation-maximisation algorithm, with an approximate maximization step providing tractability and computational feasibility.
  Performance of the BMMs is assessed through thorough simulation studies, and the BMMs are used for differential analyses of DNA methylation data from a prostate cancer study. Intuitive and biologically interpretable methylation state thresholds are inferred and DMCs are identified, including those related to genes such as GSTP1, RASSF1 and RARB, known for their role in prostate cancer development. Gene ontology analysis of the DMCs revealed significant enrichment in cancer-related pathways, demonstrating the utility of BMMs to reveal biologically relevant insights. An R package betaclust facilitates widespread use of BMMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.01938v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Koyel Majumdar, Romina Silva, Antoinette Sabrina Perry, Ronald William Watson, Andrea Rau, Florence Jaffrezic, Thomas Brendan Murphy, Isobel Claire Gormley</dc:creator>
    </item>
    <item>
      <title>Causal Inference under Network Interference Using a Mixture of Randomized Experiments</title>
      <link>https://arxiv.org/abs/2309.00141</link>
      <description>arXiv:2309.00141v2 Announce Type: replace 
Abstract: In randomized experiments, the classic Stable Unit Treatment Value Assumption (SUTVA) posits that the outcome for one experimental unit is unaffected by the treatment assignments of other units. However, this assumption is frequently violated in settings such as online marketplaces and social networks, where interference between units is common. We address the estimation of the total treatment effect in a network interference model by employing a mixed randomization design that combines two widely used experimental methods: Bernoulli randomization, where treatment is assigned independently to each unit, and cluster-based randomization, where treatment is assigned at the aggregate level. The mixed randomization design simultaneously incorporates both methods, thereby mitigating the bias present in cluster-based designs. We propose an unbiased estimator for the total treatment effect under this mixed design and show that its variance is bounded by $O(d^2 n^{-1} p^{-1} (1-p)^{-1})$, where $d$ is the maximum degree of the network, $n$ is the network size, and $p$ is the treatment probability. Additionally, we establish a lower bound of $\Omega(d^{1.5} n^{-1} p^{-1} (1-p)^{-1})$ for the variance of any mixed design. Moreover, when the interference weights on the network's edges are unknown, we propose a weight-invariant design that achieves a variance bound of $O(d^3 n^{-1} p^{-1} (1-p)^{-1})$, which is aligned with the estimator introduced by Cortez-Rodriguez et al. (2023) under similar conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.00141v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiming Jiang, He Wang</dc:creator>
    </item>
    <item>
      <title>A Class of Semiparametric Yang and Prentice Frailty Models</title>
      <link>https://arxiv.org/abs/2403.07650</link>
      <description>arXiv:2403.07650v2 Announce Type: replace 
Abstract: The Yang and Prentice (YP) regression models have garnered interest from the scientific community due to their ability to analyze data whose survival curves exhibit intersection. These models include proportional hazards (PH) and proportional odds (PO) models as specific cases. However, they encounter limitations when dealing with multivariate survival data due to potential dependencies between the times-to-event. A solution is introducing a frailty term into the hazard functions, making it possible for the times-to-event to be considered independent, given the frailty term. In this study, we propose a new class of YP models that incorporate frailty. We use the exponential distribution, the piecewise exponential distribution (PE), and Bernstein polynomials (BP) as baseline functions. Our approach adopts a Bayesian methodology. The proposed models are evaluated through a simulation study, which shows that the YP frailty models with BP and PE baselines perform similarly to the generator parametric model of the data. We apply the models in two real data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07650v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cassius Henrique Xavier Oliveira, Fabio Nogueira Demarqui, Vinicius Diniz Mayrink</dc:creator>
    </item>
    <item>
      <title>Bayesian compositional regression with flexible microbiome feature aggregation and selection</title>
      <link>https://arxiv.org/abs/2406.01557</link>
      <description>arXiv:2406.01557v2 Announce Type: replace 
Abstract: Ongoing advances in microbiome profiling have allowed unprecedented insights into the molecular activities of microbial communities. This has fueled a strong scientific interest in understanding the critical role the microbiome plays in governing human health, by identifying microbial features associated with clinical outcomes of interest. Several aspects of microbiome data limit the applicability of existing variable selection approaches. In particular, microbiome data are high-dimensional, extremely sparse, and compositional. Importantly, many of the observed features, although categorized as different taxa, may play related functional roles. To address these challenges, we propose a novel compositional regression approach that leverages the data-adaptive clustering and variable selection properties of the spiked Dirichlet process to identify taxa that exhibit similar functional roles. Our proposed method, Bayesian Regression with Agglomerated Compositional Effects using a dirichLET process (BRACElet), enables the identification of a sparse set of features with shared impacts on the outcome, facilitating dimension reduction and model interpretation. We demonstrate that BRACElet outperforms existing approaches for microbiome variable selection through simulation studies and an application elucidating the impact of oral microbiome composition on insulin resistance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01557v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Satabdi Saha, Liangliang Zhang, Kim-Anh Do, Christine B. Peterson</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Treatment Effects under Network Interference: A Nonparametric Approach Based on Node Connectivity</title>
      <link>https://arxiv.org/abs/2410.11797</link>
      <description>arXiv:2410.11797v2 Announce Type: replace 
Abstract: In network settings, interference between units makes causal inference more challenging as outcomes may depend on the treatments received by others in the network. Typical estimands in network settings focus on treatment effects aggregated across individuals in the population. We propose a framework for estimating node-wise counterfactual means, allowing for more granular insights into the impact of network structure on treatment effect heterogeneity. We develop a doubly robust and non-parametric estimation procedure, KECENI (Kernel Estimation of Causal Effect under Network Interference), which offers consistency and asymptotic normality under network dependence. The utility of this method is demonstrated through an application to microfinance data, revealing the impact of network characteristics on treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11797v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejong Bong, Colin B. Fogarty, Elizaveta Levina, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>Adaptive Transfer Clustering: A Unified Framework</title>
      <link>https://arxiv.org/abs/2410.21263</link>
      <description>arXiv:2410.21263v3 Announce Type: replace 
Abstract: We propose a general transfer learning framework for clustering given a main dataset and an auxiliary one about the same subjects. The two datasets may reflect similar but different latent grouping structures of the subjects. We propose an adaptive transfer clustering (ATC) algorithm that automatically leverages the commonality in the presence of unknown discrepancy, by optimizing an estimated bias-variance decomposition. It applies to a broad class of statistical models including Gaussian mixture models, stochastic block models, and latent class models. A theoretical analysis proves the optimality of ATC under the Gaussian mixture model and explicitly quantifies the benefit of transfer. Extensive simulations and real data experiments confirm our method's effectiveness in various scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21263v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuqi Gu, Zhongyuan Lyu, Kaizheng Wang</dc:creator>
    </item>
    <item>
      <title>Reproducible Aggregation of Sample-Split Statistics</title>
      <link>https://arxiv.org/abs/2311.14204</link>
      <description>arXiv:2311.14204v3 Announce Type: replace-cross 
Abstract: Statistical inference is often simplified by sample-splitting. This simplification comes at the cost of the introduction of randomness not native to the data. We propose a simple procedure for sequentially aggregating statistics constructed with multiple splits of the same sample. The user specifies a bound and a nominal error rate. If the procedure is implemented twice on the same data, the nominal error rate approximates the chance that the results differ by more than the bound. We illustrate the application of the procedure to several widely applied econometric methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14204v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David M. Ritzwoller, Joseph P. Romano</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach to uncover local and temporal determinants of heterogeneity in repeated cross-sectional health surveys</title>
      <link>https://arxiv.org/abs/2402.19162</link>
      <description>arXiv:2402.19162v2 Announce Type: replace-cross 
Abstract: In several countries, including Italy, a prominent approach to population health surveillance involves conducting repeated cross-sectional surveys at short intervals of time. These surveys gather information on the health status of individual respondents, including details on their behaviours, risk factors, and relevant socio-demographic information. While the collected data undoubtedly provides valuable information, modelling such data presents several challenges. For instance, in health risk models, it is essential to consider behavioural information, local and temporal dynamics, and disease co-occurrence. In response to these challenges, our work proposes a multivariate temporal logistic model for chronic disease diagnoses at local level. Linear predictors are modelled using individual risk factor covariates and a latent individual propensity to diseases. Leveraging a state space formulation of the model, we construct a framework in which temporal heterogeneity in regression coefficients is informed by exogenous information at local level, correspond ing to different contextual risk factors that may affect the occurrence of chronic diseases in different ways. To explore the utility and the effectiveness of our method, we analyse behavioural and risk factor surveillance data collected in Italy (PASSI), which is well-known as a country characterised by high peculiar administrative, social and territorial diversities reflected on high variability in morbidity among population subgroups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.19162v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mattia Stival, Lorenzo Schiavon, Stefano Campostrini</dc:creator>
    </item>
    <item>
      <title>FGCE: Feasible Group Counterfactual Explanations for Auditing Fairness</title>
      <link>https://arxiv.org/abs/2410.22591</link>
      <description>arXiv:2410.22591v2 Announce Type: replace-cross 
Abstract: This paper introduces the first graph-based framework for generating group counterfactual explanations to audit model fairness, a crucial aspect of trustworthy machine learning. Counterfactual explanations are instrumental in understanding and mitigating unfairness by revealing how inputs should change to achieve a desired outcome. Our framework, named Feasible Group Counterfactual Explanations (FGCEs), captures real-world feasibility constraints and constructs subgroups with similar counterfactuals, setting it apart from existing methods. It also addresses key trade-offs in counterfactual generation, including the balance between the number of counterfactuals, their associated costs, and the breadth of coverage achieved. To evaluate these trade-offs and assess fairness, we propose measures tailored to group counterfactual generation. Our experimental results on benchmark datasets demonstrate the effectiveness of our approach in managing feasibility constraints and trade-offs, as well as the potential of our proposed metrics in identifying and quantifying fairness issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22591v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christos Fragkathoulas, Vasiliki Papanikou, Evaggelia Pitoura, Evimaria Terzi</dc:creator>
    </item>
  </channel>
</rss>
