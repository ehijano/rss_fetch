<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 25 Nov 2024 05:00:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Combining missing data imputation and internal validation in clinical risk prediction models</title>
      <link>https://arxiv.org/abs/2411.14542</link>
      <description>arXiv:2411.14542v1 Announce Type: new 
Abstract: Methods to handle missing data have been extensively explored in the context of estimation and descriptive studies, with multiple imputation being the most widely used method in clinical research. However, in the context of clinical risk prediction models, where the goal is often to achieve high prediction accuracy and to make predictions for future patients, there are different considerations regarding the handling of missing data. As a result, deterministic imputation is better suited to the setting of clinical risk prediction models, since the outcome is not included in the imputation model and the imputation method can be easily applied to future patients. In this paper, we provide a tutorial demonstrating how to conduct bootstrapping followed by deterministic imputation of missing data to construct and internally validate the performance of a clinical risk prediction model in the presence of missing data. Extensive simulation study results are provided to help guide decision-making in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14542v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhui Mi, Rahul D. Tendulkar, Sarah M. C. Sittenfeld, Sujata Patil, Emily C. Zabor</dc:creator>
    </item>
    <item>
      <title>A Random-Effects Approach to Linear Mixed Model Analysis of Incomplete Longitudinal Data</title>
      <link>https://arxiv.org/abs/2411.14548</link>
      <description>arXiv:2411.14548v1 Announce Type: new 
Abstract: We propose a random-effects approach to missing values for linear mixed model (LMM) analysis. The method converts a LMM with missing covariates to another LMM without missing covariates. The standard LMM analysis tools for longitudinal data then apply. Performance of the method is evaluated empirically, and compared with alternative approaches, including the popular MICE procedure of multiple imputation. Theoretical explanations are given for the patterns observed in the simulation studies. A real-data example is discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14548v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thuan Nguyen, Jiangshan Zhang, Jiming Jiang</dc:creator>
    </item>
    <item>
      <title>Gradient-based optimization for variational empirical Bayes multiple regression</title>
      <link>https://arxiv.org/abs/2411.14570</link>
      <description>arXiv:2411.14570v1 Announce Type: new 
Abstract: Variational empirical Bayes (VEB) methods provide a practically attractive approach to fitting large, sparse, multiple regression models. These methods usually use coordinate ascent to optimize the variational objective function, an approach known as coordinate ascent variational inference (CAVI). Here we propose alternative optimization approaches based on gradient-based (quasi-Newton) methods, which we call gradient-based variational inference (GradVI). GradVI exploits a recent result from Kim et. al. [arXiv:2208.10910] which writes the VEB regression objective function as a penalized regression. Unfortunately the penalty function is not available in closed form, and we present and compare two approaches to dealing with this problem. In simple situations where CAVI performs well, we show that GradVI produces similar predictive performance, and GradVI converges in fewer iterations when the predictors are highly correlated. Furthermore, unlike CAVI, the key computations in GradVI are simple matrix-vector products, and so GradVI is much faster than CAVI in settings where the design matrix admits fast matrix-vector products (e.g., as we show here, trendfiltering applications) and lends itself to parallelized implementations in ways that CAVI does not. GradVI is also very flexible, and could exploit automatic differentiation to easily implement different prior families. Our methods are implemented in an open-source Python software, GradVI (available from https://github.com/stephenslab/gradvi ).</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14570v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saikat Banerjee, Peter Carbonetto, Matthew Stephens</dc:creator>
    </item>
    <item>
      <title>Modelling Loss of Complexity in Intermittent Time Series and its Application</title>
      <link>https://arxiv.org/abs/2411.14635</link>
      <description>arXiv:2411.14635v1 Announce Type: new 
Abstract: In this paper, we developed a nonparametric relative entropy (RlEn) for modelling loss of complexity in intermittent time series. This technique consists of two steps. First, we carry out a nonlinear autoregressive model where the lag order is determined by a Bayesian Information Criterion (BIC), and complexity of each intermittent time series is obtained by our novel relative entropy. Second, change-points in complexity were detected by using the cumulative sum (CUSUM) based method. Using simulations and compared to the popular method appropriate entropy (ApEN), the performance of RlEn was assessed for its (1) ability to localise complexity change-points in intermittent time series; (2) ability to faithfully estimate underlying nonlinear models. The performance of the proposal was then examined in a real analysis of fatigue-induced changes in the complexity of human motor outputs. The results demonstrated that the proposed method outperformed the ApEn in accurately detecting complexity changes in intermittent time series segments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14635v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jie Li, Jian Zhang, Samantha L. Winter, Mark Burnley</dc:creator>
    </item>
    <item>
      <title>Summarizing Bayesian Nonparametric Mixture Posterior -- Sliced Optimal Transport Metrics for Gaussian Mixtures</title>
      <link>https://arxiv.org/abs/2411.14674</link>
      <description>arXiv:2411.14674v1 Announce Type: new 
Abstract: Existing methods to summarize posterior inference for mixture models focus on identifying a point estimate of the implied random partition for clustering, with density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et al., 2022). We propose a novel approach for summarizing posterior inference in nonparametric Bayesian mixture models, prioritizing density estimation of the mixing measure (or mixture) as an inference target. One of the key features is the model-agnostic nature of the approach, which remains valid under arbitrarily complex dependence structures in the underlying sampling model. Using a decision-theoretic framework, our method identifies a point estimate by minimizing posterior expected loss. A loss function is defined as a discrepancy between mixing measures as the loss function. Estimating the mixing measure implies inference on the mixture density. Exploiting the discrete nature of the mixing measure, we use a version of sliced Wasserstein distance. We introduce two specific variants for Gaussian mixtures. The first, mixed sliced Wasserstein, applies generalized geodesic projections on the product of the Euclidean space and the manifold of symmetric positive definite matrices. The second, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture measures for efficient projection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14674v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen, Peter Mueller</dc:creator>
    </item>
    <item>
      <title>Deep Gaussian Process Emulation and Uncertainty Quantification for Large Computer Experiments</title>
      <link>https://arxiv.org/abs/2411.14690</link>
      <description>arXiv:2411.14690v1 Announce Type: new 
Abstract: Computer models are used as a way to explore complex physical systems. Stationary Gaussian process emulators, with their accompanying uncertainty quantification, are popular surrogates for computer models. However, many computer models are not well represented by stationary Gaussian processes models. Deep Gaussian processes have been shown to be capable of capturing non-stationary behaviors and abrupt regime changes in the computer model response. In this paper, we explore the properties of two deep Gaussian process formulations within the context of computer model emulation. For one of these formulations, we introduce a new parameter that controls the amount of smoothness in the deep Gaussian process layers. We adapt a stochastic variational approach to inference for this model, allowing for prior specification and posterior exploration of the smoothness of the response surface. Our approach can be applied to a large class of computer models, and scales to arbitrarily large simulation designs. The proposed methodology was motivated by the need to emulate an astrophysical model of the formation of binary black hole mergers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14690v1</guid>
      <category>stat.ME</category>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Faezeh Yazdi, Derek Bingham, Daniel Williamson</dc:creator>
    </item>
    <item>
      <title>Bayesian optimal change point detection in high-dimensions</title>
      <link>https://arxiv.org/abs/2411.14864</link>
      <description>arXiv:2411.14864v1 Announce Type: new 
Abstract: We propose the first Bayesian methods for detecting change points in high-dimensional mean and covariance structures. These methods are constructed using pairwise Bayes factors, leveraging modularization to identify significant changes in individual components efficiently. We establish that the proposed methods consistently detect and estimate change points under much milder conditions than existing approaches in the literature. Additionally, we demonstrate that their localization rates are nearly optimal in terms of rates. The practical performance of the proposed methods is evaluated through extensive simulation studies, where they are compared to state-of-the-art techniques. The results show comparable or superior performance across most scenarios. Notably, the methods effectively detect change points whenever signals of sufficient magnitude are present, irrespective of the number of signals. Finally, we apply the proposed methods to genetic and financial datasets, illustrating their practical utility in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14864v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaehoon Kim, Kyoungjae Lee, Lizhen Lin</dc:creator>
    </item>
    <item>
      <title>The EE-Classifier: A classification method for functional data based on extremality indexes</title>
      <link>https://arxiv.org/abs/2411.14999</link>
      <description>arXiv:2411.14999v1 Announce Type: new 
Abstract: Functional data analysis has gained significant attention due to its wide applicability. This research explores the extension of statistical analysis methods for functional data, with a primary focus on supervised classification techniques. It provides a review on the existing depth-based methods used in functional data samples. Building on this foundation, it introduces an extremality-based approach, which takes the modified epigraph and hypograph indexes properties as classification techniques. To demonstrate the effectiveness of the classifier, it is applied to both real-world and synthetic data sets. The results show its efficacy in accurately classifying functional data. Additionally, the classifier is used to analyze the fluctuations in the S\&amp;P 500 stock value. This research contributes to the field of functional data analysis by introducing a new extremality-based classifier. The successful application to various data sets shows its potential for supervised classification tasks and provides valuable insights into financial data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14999v1</guid>
      <category>stat.ME</category>
      <category>math.FA</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Catalina Lesmes, Francisco Zuluaga, Henry Laniado, Andres Gomez, Andrea Carvajal</dc:creator>
    </item>
    <item>
      <title>Exploring the Potential Role of Generative AI in the TRAPD Procedure for Survey Translation</title>
      <link>https://arxiv.org/abs/2411.14472</link>
      <description>arXiv:2411.14472v1 Announce Type: cross 
Abstract: This paper explores and assesses in what ways generative AI can assist in translating survey instruments. Writing effective survey questions is a challenging and complex task, made even more difficult for surveys that will be translated and deployed in multiple linguistic and cultural settings. Translation errors can be detrimental, with known errors rendering data unusable for its intended purpose and undetected errors leading to incorrect conclusions. A growing number of institutions face this problem as surveys deployed by private and academic organizations globalize, and the success of their current efforts depends heavily on researchers' and translators' expertise and the amount of time each party has to contribute to the task. Thus, multilinguistic and multicultural surveys produced by teams with limited expertise, budgets, or time are at significant risk for translation-based errors in their data. We implement a zero-shot prompt experiment using ChatGPT to explore generative AI's ability to identify features of questions that might be difficult to translate to a linguistic audience other than the source language. We find that ChatGPT can provide meaningful feedback on translation issues, including common source survey language, inconsistent conceptualization, sensitivity and formality issues, and nonexistent concepts. In addition, we provide detailed information on the practicality of the approach, including accessing the necessary software, associated costs, and computational run times. Lastly, based on our findings, we propose avenues for future research that integrate AI into survey translation practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14472v1</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erica Ann Metheney, Lauren Yehle</dc:creator>
    </item>
    <item>
      <title>The Effects of Major League Baseball's Ban on Infield Shifts: A Quasi-Experimental Analysis</title>
      <link>https://arxiv.org/abs/2411.15075</link>
      <description>arXiv:2411.15075v1 Announce Type: cross 
Abstract: From 2020 to 2023, Major League Baseball changed rules affecting team composition, player positioning, and game time. Understanding the effects of these rules is crucial for leagues, teams, players, and other relevant parties to assess their impact and to advocate either for further changes or undoing previous ones. Panel data and quasi-experimental methods provide useful tools for causal inference in these settings. I demonstrate this potential by analyzing the effect of the 2023 shift ban at both the league-wide and player-specific levels. Using difference-in-differences analysis, I show that the policy increased batting average on balls in play and on-base percentage for left-handed batters by a modest amount (nine points). For individual players, synthetic control analyses identify several players whose offensive performance (on-base percentage, on-base plus slugging percentage, and weighted on-base average) improved substantially (over 70 points in several cases) because of the rule change, and other players with previously high shift rates for whom it had little effect. This article both estimates the impact of this specific rule change and demonstrates how these methods for causal inference are potentially valuable for sports analytics -- at the player, team, and league levels -- more broadly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15075v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lee Kennedy-Shaffer</dc:creator>
    </item>
    <item>
      <title>Rank-adaptive covariance testing with applications to genomics and neuroimaging</title>
      <link>https://arxiv.org/abs/2309.10284</link>
      <description>arXiv:2309.10284v2 Announce Type: replace 
Abstract: In biomedical studies, testing for differences in covariance offers scientific insights beyond mean differences, especially when differences are driven by complex joint behavior between features. However, when differences in joint behavior are weakly dispersed across many dimensions and arise from differences in low-rank structures within the data, as is often the case in genomics and neuroimaging, existing two-sample covariance testing methods may suffer from power loss. The Ky-Fan(k) norm, defined by the sum of the top Ky-Fan(k) singular values, is a simple and intuitive matrix norm able to capture signals caused by differences in low-rank structures between matrices, but its statistical properties in hypothesis testing have not been studied well. In this paper, we investigate the behavior of the Ky-Fan(k) norm in two-sample covariance testing. Ultimately, we propose a novel methodology, Rank-Adaptive Covariance Testing (RACT), which is able to leverage differences in low-rank structures found in the covariance matrices of two groups in order to maximize power. RACT uses permutation for statistical inference, ensuring an exact Type I error control. We validate RACT in simulation studies and evaluate its performance when testing for differences in gene expression networks between two types of lung cancer, as well as testing for covariance heterogeneity in diffusion tensor imaging (DTI) data taken on two different scanner types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10284v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David Veitch, Yinqiu He, Jun Young Park</dc:creator>
    </item>
    <item>
      <title>Robust Multi-Model Subset Selection</title>
      <link>https://arxiv.org/abs/2311.13202</link>
      <description>arXiv:2311.13202v3 Announce Type: replace 
Abstract: Outlying observations can be challenging to handle and adversely affect subsequent analyses, particularly, in complex high-dimensional datasets. Although outliers are not always undesired anomalies in the data and may possess valuable insights, only methods that are robust to outliers are able to accurately identify them and resist their influence. In this paper, we propose a method that generates an ensemble of sparse and diverse predictive models that are resistant to outliers. We show that the ensembles generally outperform single-model sparse and robust methods in high-dimensional prediction tasks. Cross-validation is used to tune model parameters to control levels of sparsity, diversity and resistance to outliers. We establish the finitesample breakdown point of the ensembles and the models that comprise them, and we develop a tailored computing algorithm to learn the ensembles by leveraging recent developments in L0 optimization. Our extensive numerical experiments on synthetic and artificially contaminated real datasets from bioinformatics and cheminformatics demonstrate the competitive advantage of our method over state-of-the-art single-model methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13202v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anthony-Alexander Christidis, Gabriela Cohen-Freue</dc:creator>
    </item>
    <item>
      <title>Ideal trials, target trials and actual randomized trials</title>
      <link>https://arxiv.org/abs/2405.10026</link>
      <description>arXiv:2405.10026v3 Announce Type: replace 
Abstract: Causal inference is the goal of randomized controlled trials and many observational studies. The first step in a formal approach to causal inference is to define the estimand of interest, and in both types of study this can be intuitively defined as the effect in an ideal trial: a hypothetical perfect randomized experiment (with representative sample, perfect adherence, etc.). The target trial framework is an increasingly popular approach to causal inference in observational studies, but clarity is lacking in how a target trial should be specified and, crucially, how it relates to the ideal trial. In this paper, we consider these questions and use an example from respiratory epidemiology to highlight challenges with an approach that is commonly seen in applications: to specify a target trial in a way that is closely aligned to the observational study (e.g. uses the same eligibility criteria, outcome measure, etc.). The main issue is that such a target trial generally deviates from the ideal trial. Thus, even if the target trial can be emulated perfectly apart from randomization, biases beyond baseline confounding are likely to remain, relative to the estimand of interest. Without consideration of the ideal trial, these biases may go unnoticed, mirroring the often-overlooked biases of actual trials. Therefore, we suggest that, in both actual trials and observational studies, specifying the ideal trial and how the target or actual trial differs from it is necessary to systematically assess all potential sources of biases, and therefore appropriately design analyses and interpret findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10026v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Margarita Moreno-Betancur, Rushani Wijesuriya, John B. Carlin</dc:creator>
    </item>
    <item>
      <title>HAL-based Plugin Estimation of the Causal Dose-Response Curve</title>
      <link>https://arxiv.org/abs/2406.05607</link>
      <description>arXiv:2406.05607v3 Announce Type: replace 
Abstract: Estimating and obtaining reliable inference for the marginally adjusted causal dose-response curve for continuous treatments without relying on parametric assumptions is a well-known statistical challenge. Parametric models risk introducing significant bias through model misspecification, compromising the accurate representation of the underlying data and dose-response relationship. On the other hand, nonparametric models face difficulties as the dose-response curve is not pathwise differentiable, preventing consistent estimation at standard rates. The Highly Adaptive Lasso (HAL) maximum likelihood estimator offers a promising approach to this issue. In this paper, we introduce a HAL-based plug-in estimator for the causal dose-response curve and assess its empirical performance against other estimators. Through comprehensive simulations, we evaluate the accuracy of the estimation and the quality of the inference, particularly in terms of coverage, using robust standard error estimators. Our results demonstrate the finite-sample effectiveness of the HAL-based estimator, utilizing an undersmoothed and smoothness-adaptive fit for the conditional outcome model. Additionally, the simulations reveal that the HAL-based estimator consistently outperforms existing methods for estimating the causal dose-response curve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05607v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junming Shi, Wenxin Zhang, Alan E. Hubbard, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>On estimation and order selection for multivariate extremes via clustering</title>
      <link>https://arxiv.org/abs/2406.14535</link>
      <description>arXiv:2406.14535v2 Announce Type: replace 
Abstract: We investigate the estimation of multivariate extreme models with a discrete spectral measure using spherical clustering techniques. The primary contribution involves devising a method for selecting the order, that is, the number of clusters. The method consistently identifies the true order, i.e., the number of spectral atoms, and enjoys intuitive implementation in practice. Specifically, we introduce an extra penalty term to the well-known simplified average silhouette width, which penalizes small cluster sizes and small dissimilarities between cluster centers. Consequently, we provide a consistent method for determining the order of a max-linear factor model, where a typical information-based approach is not viable. Our second contribution is a large-deviation-type analysis for estimating the discrete spectral measure through clustering methods, which serves as an assessment of the convergence quality of clustering-based estimation for multivariate extremes. Additionally, as a third contribution, we discuss how estimating the discrete measure can lead to parameter estimations of heavy-tailed factor models. We also present simulations and real-data studies that demonstrate order selection and factor model estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14535v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiyuan Deng, He Tang, Shuyang Bai</dc:creator>
    </item>
    <item>
      <title>Tail calibration of probabilistic forecasts</title>
      <link>https://arxiv.org/abs/2407.03167</link>
      <description>arXiv:2407.03167v2 Announce Type: replace 
Abstract: Probabilistic forecasts comprehensively describe the uncertainty in the unknown future outcome, making them essential for decision making and risk management. While several methods have been introduced to evaluate probabilistic forecasts, existing evaluation techniques are ill-suited to the evaluation of tail properties of such forecasts. However, these tail properties are often of particular interest to forecast users due to the severe impacts caused by extreme outcomes. In this work, we introduce a general notion of tail calibration for probabilistic forecasts, which allows forecasters to assess the reliability of their predictions for extreme outcomes. We study the relationships between tail calibration and standard notions of forecast calibration, and discuss connections to peaks-over-threshold models in extreme value theory. Diagnostic tools are introduced and applied in a case study on European precipitation forecasts</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03167v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sam Allen, Jonathan Koh, Johan Segers, Johanna Ziegel</dc:creator>
    </item>
    <item>
      <title>A Novel Unit Distribution Named As Median Based Unit Rayleigh (MBUR): Properties and Estimations</title>
      <link>https://arxiv.org/abs/2410.04132</link>
      <description>arXiv:2410.04132v2 Announce Type: replace 
Abstract: The importance of continuously emerging new distribution is a mandate to understand the world and environment surrounding us. In this paper, the author will discuss a new distribution defined on the interval (0,1) as regards the methodology of deducing its PDF, some of its properties and related functions. A simulation and real data analysis will be highlighted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04132v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohammed Attia</dc:creator>
    </item>
    <item>
      <title>Root cause discovery via permutations and Cholesky decomposition</title>
      <link>https://arxiv.org/abs/2410.12151</link>
      <description>arXiv:2410.12151v2 Announce Type: replace 
Abstract: This work is motivated by the following problem: Can we identify the disease-causing gene in a patient affected by a monogenic disorder? This problem is an instance of root cause discovery. In particular, we aim to identify the intervened variable in one interventional sample using a set of observational samples as reference. We consider a linear structural equation model where the causal ordering is unknown. We begin by examining a simple method that uses squared z-scores and characterize the conditions under which this method succeeds and fails, showing that it generally cannot identify the root cause. We then prove, without additional assumptions, that the root cause is identifiable even if the causal ordering is not. Two key ingredients of this identifiability result are the use of permutations and the Cholesky decomposition, which allow us to exploit an invariant property across different permutations to discover the root cause. Furthermore, we characterize permutations that yield the correct root cause and, based on this, propose a valid method for root cause discovery. We also adapt this approach to high-dimensional settings. Finally, we evaluate the performance of our methods through simulations and apply the high-dimensional method to discover disease-causing genes in the gene expression dataset that motivates this work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12151v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jinzhou Li, Benjamin B. Chu, Ines F. Scheller, Julien Gagneur, Marloes H. Maathuis</dc:creator>
    </item>
    <item>
      <title>An Anatomy of Event Studies: Hypothetical Experiments, Exact Decomposition, and Weighting Diagnostics</title>
      <link>https://arxiv.org/abs/2410.17399</link>
      <description>arXiv:2410.17399v2 Announce Type: replace 
Abstract: In recent decades, event studies have emerged as a central methodology in health and social research for evaluating the causal effects of staggered interventions. In this paper, we analyze event studies from experimental design principles for observational studies, with a focus on information borrowing across measurements. We develop robust weighting estimators that increasingly use more information across units and time periods, justified by increasingly stronger assumptions on the treatment assignment and potential outcomes mechanisms. As a particular case of this approach, we offer a novel decomposition of the classical dynamic two-way fixed effects (TWFE) regression estimator for event studies. Our decomposition is expressed in closed form and reveals in finite samples the hypothetical experiment that TWFE regression adjustments approximate. This decomposition offers insights into how standard regression estimators borrow information across different units and times, clarifying and supplementing the notion of forbidden comparison noted in the literature. The proposed approach enables the generalization of treatment effect estimates to a target population and offers new diagnostics for event studies, including covariate balance, sign reversal, effective sample size, and the contribution of each observation to the analysis. We also provide visualization tools for event studies and illustrate them in a case study of the impact of divorce reforms on female suicide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17399v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhu Shen, Ambarish Chattopadhyay, Yuzhou Lin, Jose R. Zubizarreta</dc:creator>
    </item>
    <item>
      <title>Bayesian Evidence Synthesis for Modeling SARS-CoV-2 Transmission</title>
      <link>https://arxiv.org/abs/2309.03122</link>
      <description>arXiv:2309.03122v2 Announce Type: replace-cross 
Abstract: The acute phase of the Covid-19 pandemic has made apparent the need for decision support based upon accurate epidemic modeling. This process is substantially hampered by under-reporting of cases and related data incompleteness issues. In this article we adopt the Bayesian paradigm and synthesize publicly available data via a discrete-time stochastic epidemic modeling framework. The models allow for estimating the total number of infections while accounting for the endemic phase of the pandemic. We assess the prediction of the infection rate utilizing mobility information, notably the principal components of the mobility data. We evaluate variational Bayes in this context and find that Hamiltonian Monte Carlo offers a robust inference alternative for such models. We elaborate upon vector analysis of the epidemic dynamics, thus enriching the traditional tools used for decision making. In particular, we show how certain 2-dimensional plots on the phase plane may yield intuitive information regarding the speed and the type of transmission dynamics. We investigate the potential of a two-stage analysis as a consequence of cutting feedback, for inference on certain functionals of the model parameters. Finally, we show that a point mass on critical parameters is overly restrictive and investigate informative priors as a suitable alternative.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.03122v2</guid>
      <category>stat.AP</category>
      <category>physics.soc-ph</category>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anastasios Apsemidis, Nikolaos Demiris</dc:creator>
    </item>
    <item>
      <title>Random Fourier Signature Features</title>
      <link>https://arxiv.org/abs/2311.12214</link>
      <description>arXiv:2311.12214v2 Announce Type: replace-cross 
Abstract: Tensor algebras give rise to one of the most powerful measures of similarity for sequences of arbitrary length called the signature kernel accompanied with attractive theoretical guarantees from stochastic analysis. Previous algorithms to compute the signature kernel scale quadratically in terms of the length and the number of the sequences. To mitigate this severe computational bottleneck, we develop a random Fourier feature-based acceleration of the signature kernel acting on the inherently non-Euclidean domain of sequences. We show uniform approximation guarantees for the proposed unbiased estimator of the signature kernel, while keeping its computation linear in the sequence length and number. In addition, combined with recent advances on tensor projections, we derive two even more scalable time series features with favourable concentration properties and computational complexity both in time and memory. Our empirical results show that the reduction in computational cost comes at a negligible price in terms of accuracy on moderate-sized datasets, and it enables one to scale to large datasets up to a million time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12214v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Csaba Toth, Harald Oberhauser, Zoltan Szabo</dc:creator>
    </item>
    <item>
      <title>Stable Survival Extrapolation via Transfer Learning</title>
      <link>https://arxiv.org/abs/2409.16044</link>
      <description>arXiv:2409.16044v2 Announce Type: replace-cross 
Abstract: The mean survival is the key ingredient of the decision process in several applications, notably in health economic evaluations. It is defined as the area under the complete survival curve, thus necessitating extrapolation of the observed data. This may be achieved in a more stable manner by borrowing long term evidence from registry and demographic data. Such borrowing can be seen as an implicit bias-variance trade-off in unseen data. In this article we employ a Bayesian mortality model and transfer its projections in order to construct the baseline population that acts as an anchor of the survival model. We then propose extrapolation methods based on flexible parametric polyhazard models which can naturally accommodate diverse shapes, including non-proportional hazards and crossing survival curves, while typically maintaining a natural interpretation. We estimate the mean survival and related estimands in three cases, namely breast cancer, cardiac arrhythmia and advanced melanoma. Specifically, we evaluate the survival disadvantage of triple-negative breast cancer cases, the efficacy of combining immunotherapy with mRNA cancer therapeutics for melanoma treatment and the suitability of implantable cardioverter defibrilators for cardiac arrhythmia. The latter is conducted in a competing risks context illustrating how working on the cause-specific hazard alone minimizes potential instability. The results suggest that the proposed approach offers a flexible, interpretable and robust approach when survival extrapolation is required.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16044v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anastasios Apsemidis, Nikolaos Demiris</dc:creator>
    </item>
    <item>
      <title>Joint Probability Estimation of Many Binary Outcomes via Localized Adversarial Lasso</title>
      <link>https://arxiv.org/abs/2410.15166</link>
      <description>arXiv:2410.15166v2 Announce Type: replace-cross 
Abstract: In this work we consider estimating the probability of many (possibly dependent) binary outcomes which is at the core of many applications, e.g., multi-level treatments in causal inference, demands for bundle of products, etc. Without further conditions, the probability distribution of an M dimensional binary vector is characterized by exponentially in M coefficients which can lead to a high-dimensional problem even without the presence of covariates. Understanding the (in)dependence structure allows us to substantially improve the estimation as it allows for an effective factorization of the probability distribution. In order to estimate the probability distribution of a M dimensional binary vector, we leverage a Bahadur representation that connects the sparsity of its coefficients with independence across the components. We propose to use regularized and adversarial regularized estimators to obtain an adaptive estimator with respect to the dependence structure which allows for rates of convergence to depend on this intrinsic (lower) dimension. These estimators are needed to handle several challenges within this setting, including estimating nuisance parameters, estimating covariates, and nonseparable moment conditions. Our main results consider the presence of (low dimensional) covariates for which we propose a locally penalized estimator. We provide pointwise rates of convergence addressing several issues in the theoretical analyses as we strive for making a computationally tractable formulation. We apply our results in the estimation of causal effects with multiple binary treatments and show how our estimators can improve the finite sample performance when compared with non-adaptive estimators that try to estimate all the probabilities directly. We also provide simulations that are consistent with our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15166v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Belloni, Yan Chen, Matthew Harding</dc:creator>
    </item>
  </channel>
</rss>
