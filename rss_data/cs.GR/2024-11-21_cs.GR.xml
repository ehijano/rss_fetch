<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Nov 2024 05:03:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Elastic Shape Registration of Surfaces in 3D Space with Gradient Descent and Dynamic Programming</title>
      <link>https://arxiv.org/abs/2411.12743</link>
      <description>arXiv:2411.12743v1 Announce Type: new 
Abstract: Algorithms based on gradient descent for computing the elastic shape registration of two simple surfaces in 3-dimensional space and therefore the elastic shape distance between them have been proposed by Kurtek, Jermyn, et al., and more recently by Riseth. Their algorithms are designed to minimize a distance function between the surfaces by rotating and reparametrizing one of the surfaces, the minimization for reparametrizing based on a gradient descent approach that may terminate at a local solution. On the other hand, Bernal and Lawrence have proposed a similar algorithm, the minimization for reparametrizing based on dynamic programming thus producing a partial not necessarily optimal elastic shape registration of the surfaces. Accordingly, Bernal and Lawrence have proposed to use the rotation and reparametrization computed with their algorithm as the initial solution to any algorithm based on a gradient descent approach for reparametrizing. Here we present results from doing exactly that. We also describe and justify the gradient descent approach that is used for reparametrizing one of the surfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12743v1</guid>
      <category>cs.GR</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.6028/NIST.TN.2310</arxiv:DOI>
      <arxiv:journal_reference>2024 NIST Technical Note 2310</arxiv:journal_reference>
      <dc:creator>Javier Bernal, Jim Lawrence</dc:creator>
    </item>
    <item>
      <title>Multilevel Skeletonization Using Local Separators</title>
      <link>https://arxiv.org/abs/2303.07210</link>
      <description>arXiv:2303.07210v1 Announce Type: cross 
Abstract: In this paper we give a new, efficient algorithm for computing curve skeletons, based on local separators. Our efficiency stems from a multilevel approach, where we solve small problems across levels of detail and combine these in order to quickly obtain a skeleton. We do this in a highly modular fashion, ensuring complete flexibility in adapting the algorithm for specific types of input or for otherwise targeting specific applications.
  Separator based skeletonization was first proposed by B{\ae}rentzen and Rotenberg in [ACM Tran. Graphics'21], showing high quality output at the cost of running times which become prohibitive for large inputs. Our new approach retains the high quality output, and applicability to any spatially embedded graph, while being orders of magnitude faster for all practical purposes.
  We test our skeletonization algorithm for efficiency and quality in practice, comparing it to local separator skeletonization on the University of Groningen Skeletonization Benchmark [Telea'16].</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07210v1</guid>
      <category>cs.CG</category>
      <category>cs.GR</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. Andreas B{\ae}rentzen, Rasmus Emil Christensen, Emil Toftegaard G{\ae}de, Eva Rotenberg</dc:creator>
    </item>
    <item>
      <title>Identity Preserving 3D Head Stylization with Multiview Score Distillation</title>
      <link>https://arxiv.org/abs/2411.13536</link>
      <description>arXiv:2411.13536v1 Announce Type: cross 
Abstract: 3D head stylization transforms realistic facial features into artistic representations, enhancing user engagement across gaming and virtual reality applications. While 3D-aware generators have made significant advancements, many 3D stylization methods primarily provide near-frontal views and struggle to preserve the unique identities of original subjects, often resulting in outputs that lack diversity and individuality. This paper addresses these challenges by leveraging the PanoHead model, synthesizing images from a comprehensive 360-degree perspective. We propose a novel framework that employs negative log-likelihood distillation (LD) to enhance identity preservation and improve stylization quality. By integrating multi-view grid score and mirror gradients within the 3D GAN architecture and introducing a score rank weighing technique, our approach achieves substantial qualitative and quantitative improvements. Our findings not only advance the state of 3D head stylization but also provide valuable insights into effective distillation processes between diffusion models and GANs, focusing on the critical issue of identity preservation. Please visit the https://three-bee.github.io/head_stylization for more visuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13536v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <category>cs.MM</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bahri Batuhan Bilecen, Ahmet Berke Gokmen, Furkan Guzelant, Aysegul Dundar</dc:creator>
    </item>
    <item>
      <title>GroomCap: High-Fidelity Prior-Free Hair Capture</title>
      <link>https://arxiv.org/abs/2409.00831</link>
      <description>arXiv:2409.00831v3 Announce Type: replace 
Abstract: Despite recent advances in multi-view hair reconstruction, achieving strand-level precision remains a significant challenge due to inherent limitations in existing capture pipelines. We introduce GroomCap, a novel multi-view hair capture method that reconstructs faithful and high-fidelity hair geometry without relying on external data priors. To address the limitations of conventional reconstruction algorithms, we propose a neural implicit representation for hair volume that encodes high-resolution 3D orientation and occupancy from input views. This implicit hair volume is trained with a new volumetric 3D orientation rendering algorithm, coupled with 2D orientation distribution supervision, to effectively prevent the loss of structural information caused by undesired orientation blending. We further propose a Gaussian-based hair optimization strategy to refine the traced hair strands with a novel chained Gaussian representation, utilizing direct photometric supervision from images. Our results demonstrate that GroomCap is able to capture high-quality hair geometries that are not only more precise and detailed than existing methods but also versatile enough for a range of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00831v3</guid>
      <category>cs.GR</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3687768</arxiv:DOI>
      <dc:creator>Yuxiao Zhou, Menglei Chai, Daoye Wang, Sebastian Winberg, Erroll Wood, Kripasindhu Sarkar, Markus Gross, Thabo Beeler</dc:creator>
    </item>
    <item>
      <title>HHAvatar: Gaussian Head Avatar with Dynamic Hairs</title>
      <link>https://arxiv.org/abs/2312.03029</link>
      <description>arXiv:2312.03029v3 Announce Type: replace-cross 
Abstract: Creating high-fidelity 3D head avatars has always been a research hotspot, but it remains a great challenge under lightweight sparse view setups. In this paper, we propose HHAvatar represented by controllable 3D Gaussians for high-fidelity head avatar with dynamic hair modeling. We first use 3D Gaussians to represent the appearance of the head, and then jointly optimize neutral 3D Gaussians and a fully learned MLP-based deformation field to capture complex expressions. The two parts benefit each other, thereby our method can model fine-grained dynamic details while ensuring expression accuracy. Furthermore, we devise a well-designed geometry-guided initialization strategy based on implicit SDF and Deep Marching Tetrahedra for the stability and convergence of the training procedure. To address the problem of dynamic hair modeling, we introduce a hybrid head model into our avatar representation based Gaussian Head Avatar and a training method that considers timing information and an occlusion perception module to model the non-rigid motion of hair. Experiments show that our approach outperforms other state-of-the-art sparse-view methods, achieving ultra high-fidelity rendering quality at 2K resolution even under exaggerated expressions and driving hairs reasonably with the motion of the head</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.03029v3</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zhanfeng Liao, Yuelang Xu, Zhe Li, Qijing Li, Boyao Zhou, Ruifeng Bai, Di Xu, Hongwen Zhang, Yebin Liu</dc:creator>
    </item>
  </channel>
</rss>
