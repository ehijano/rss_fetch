<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Jun 2025 04:01:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Causal Decomposition Analysis with Synergistic Interventions: A Triply-Robust Machine Learning Approach to Addressing Multiple Dimensions of Social Disparities</title>
      <link>https://arxiv.org/abs/2506.18994</link>
      <description>arXiv:2506.18994v1 Announce Type: new 
Abstract: Educational disparities are rooted in and perpetuate social inequalities across multiple dimensions such as race, socioeconomic status, and geography. To reduce disparities, most intervention strategies focus on a single domain and frequently evaluate their effectiveness by using causal decomposition analysis. However, a growing body of research suggests that single-domain interventions may be insufficient for individuals marginalized on multiple fronts. While interventions across multiple domains are increasingly proposed, there is limited guidance on appropriate methods for evaluating their effectiveness. To address this gap, we develop an extended causal decomposition analysis that simultaneously targets multiple causally ordered intervening factors, allowing for the assessment of their synergistic effects. These scenarios often involve challenges related to model misspecification due to complex interactions among group categories, intervening factors, and their confounders with the outcome. To mitigate these challenges, we introduce a triply robust estimator that leverages machine learning techniques to address potential model misspecification. We apply our method to a cohort of students from the High School Longitudinal Study, focusing on math achievement disparities between Black, Hispanic, and White high schoolers. Specifically, we examine how two sequential interventions - equalizing the proportion of students who attend high-performing schools and equalizing enrollment in Algebra I by 9th grade across racial groups - may reduce these disparities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18994v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Soojin Park, Su Yeon Kim, Xinyao Zheng, Chioun Lee</dc:creator>
    </item>
    <item>
      <title>Principal stratification with recurrent events truncated by a terminal event: A nested Bayesian nonparametric approach</title>
      <link>https://arxiv.org/abs/2506.19015</link>
      <description>arXiv:2506.19015v1 Announce Type: new 
Abstract: Recurrent events often serve as key endpoints in clinical studies but may be prematurely truncated by terminal events such as death, creating selection bias and complicating causal inference. To address this challenge, we propose novel causal estimands within the principal stratification framework, introducing a refined ``always-survivor'' stratum that defines survival until the final recurrent event rather than a fixed time point, yielding more stable and interpretable causal contrasts. We develop a flexible Bayesian nonparametric prior -- the enriched dependent Dirichlet process -- specifically designed for joint modeling of recurrent and terminal events, addressing a critical limitation where standard Dirichlet process priors create random partitions dominated by recurrent events, yielding poor predictive performance for terminal events. Our nested structure separates within-arm and cross-arm dependence through a dual-frailty framework, enabling transparent sensitivity analysis for non-identifiable parameters. Simulations are carried out to show that our method has superior performance compared to existing methods. We also illustrate the proposed Bayesian methods to infer the causal effect of intensive blood pressure control on recurrent cardiovascular events in a cardiovascular clinical trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19015v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuki Ohnishi, Michael O. Harhay, Fan Li</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach for unadjudicated events in cardiovascular disease cohort studies</title>
      <link>https://arxiv.org/abs/2506.19066</link>
      <description>arXiv:2506.19066v1 Announce Type: new 
Abstract: An important issue in joint modelling for outcomes and longitudinal risk factors in cohort studies is to have an accurate assessment of events. Events determined based on ICD-9 codes can be very inaccurate, in particular for cardiovascular disease (CVD) where ICD-9 codes may overestimate the frequency of CVD. Motivated by the lack of adjudicated events in the Established Populations for Epidemiologic Studies of the Elderly (EPESE) cohort, we develop methods that use a related cohort Atherosclerosis Risk in Communities (ARIC), with both ICD-9 code events and adjudicated events, to create a posterior predictive distribution of adjudicated events. The methods are based on the construction of flexible Bayesian joint models combined with a Bayesian additive regression trees to directly address the ICD-9 misclassification. We assessed the performance of our approach by simulation study and applied to ARIC data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19066v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mirajul Islam, Michael J. Daniels, Donald Lloyd-Jones, Juned Siddique</dc:creator>
    </item>
    <item>
      <title>Precision Matrix Regularization in Sufficient Dimension Reduction for Improved Quadratic Discriminant Classification</title>
      <link>https://arxiv.org/abs/2506.19192</link>
      <description>arXiv:2506.19192v1 Announce Type: new 
Abstract: Sufficient dimension reduction (SDR) methods, which often rely on class precision matrices, are widely used in supervised statistical classification problems. However, when class-specific sample sizes are small relative to the original feature-space dimension, precision matrix estimation becomes unstable and, as a result, increases the variability of the linear dimension reduction (LDR) matrix. Ultimately, this fact causes suboptimal supervised classification. To address this problem, we develop a multiclass and distribution-free SDR method, stabilized SDR (SSDR), that employs user-specified precision matrix shrinkage estimators to stabilize the LDR projection matrix and supervised classifier. We establish this technique with the theoretical guarantee of preserving all classification information under the quadratic discriminant analysis (QDA) decision rule. We evaluate multiple precision matrix shrinkage estimators within our proposed SSDR framework through Monte Carlo simulations and applications to real datasets. Our empirical results demonstrate the efficacy of the SSDR method, which generally improves classification accuracy and frequently outperforms several well-established competing SDR methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19192v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Derik T. Boonstra, Rakheon Kim, Dean M. Young</dc:creator>
    </item>
    <item>
      <title>gcor: A Python Implementation of Categorical Gini Correlation and Its Inference</title>
      <link>https://arxiv.org/abs/2506.19230</link>
      <description>arXiv:2506.19230v1 Announce Type: new 
Abstract: Categorical Gini Correlation (CGC), introduced by Dang et al. (2020), is a novel dependence measure designed to quantify the association between a numerical variable and a categorical variable. It has appealing properties compared to existing dependence measures, such as zero correlation mutually implying independence between the variables. It has also shown superior performance over existing methods when applied to feature screening for classification. This article presents a Python implementation for computing CGC, constructing confidence intervals, and performing independence tests based on it. Efficient algorithms have been implemented for all procedures, and they have been optimized using vectorization and parallelization to enhance computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19230v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sameera Hewage</dc:creator>
    </item>
    <item>
      <title>Multivariate Discrete Generalized Pareto Distributions: Theory, Simulation, and Applications to Dry spells</title>
      <link>https://arxiv.org/abs/2506.19361</link>
      <description>arXiv:2506.19361v1 Announce Type: new 
Abstract: This article extends the multivariate extreme value theory (MEVT) to discrete settings, focusing on the generalized Pareto distribution (GPD) as a foundational tool. The purpose of the study is to enhance the understanding of extreme discrete count data representation, particularly for discrete exceedances over thresholds, defining and using multivariate discrete Pareto distributions (MDGPD). Through theoretical results and illustrative examples, we outline the construction and properties of MDGPDs, providing practical insights into simulation techniques and data fitting approaches using recent likelihood-free inference methods. This framework broadens the toolkit for modeling extreme events, offering robust methodologies for analyzing multivariate discrete data with extreme values. To illustrate its practical relevance, we present an application of this method to drought analysis, addressing a growing concern in Europe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19361v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samira Aka, Marie Kratz, Philippe Naveau</dc:creator>
    </item>
    <item>
      <title>Clustering Tails in High Dimension</title>
      <link>https://arxiv.org/abs/2506.19414</link>
      <description>arXiv:2506.19414v1 Announce Type: new 
Abstract: One potential solution to combat the scarcity of tail observations in extreme value analysis is to integrate information from multiple datasets sharing similar tail properties, for instance, a common extreme value index. In other words, for a multivariate dataset, we intend to group dimensions into clusters first, before applying any pooling techniques. This paper addresses the clustering problem for a high dimensional dataset, according to their extreme value indices.
  We propose an iterative clustering procedure that sequentially partitions the variables into groups, ordered from the heaviest-tailed to the lightesttailed distributions. At each step, our method identifies and extracts a group of variables that share the highest extreme value index among the remaining ones. This approach differs fundamentally from conventional clustering methods such as using pre-estimated extreme value indices in a two-step clustering method.
  We show the consistency property of the proposed algorithm and demonstrate its finite-sample performance using a simulation study and a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19414v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liujun Chen, Marco Oesting, Chen Zhou</dc:creator>
    </item>
    <item>
      <title>Modeling the uncertainty on the covariance matrix for probabilistic forecast reconciliation</title>
      <link>https://arxiv.org/abs/2506.19554</link>
      <description>arXiv:2506.19554v1 Announce Type: new 
Abstract: In forecast reconciliation, the covariance matrix of the base forecasts errors plays a crucial role. Typically, this matrix is estimated, and then treated as known. In contrast, we propose a Bayesian reconciliation model that explicitly accounts for the uncertainty in the covariance matrix. We choose an Inverse-Wishart prior, which leads to a multivariate-t reconciled predictive distribution and allows a completely analytical derivation. Empirical experiments demonstrate that this approach improves the accuracy of the prediction intervals with respect to MinT, leading to more reliable probabilistic forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19554v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Carrara, Lorenzo Zambon, Dario Azzimonti, Giorgio Corani</dc:creator>
    </item>
    <item>
      <title>Bayesian community detection in assortative stochastic block model with unknown number of communities</title>
      <link>https://arxiv.org/abs/2506.19576</link>
      <description>arXiv:2506.19576v1 Announce Type: new 
Abstract: Structured data in the form of networks is increasingly common in a number of fields, including social sciences, biology, physics, computer science, and many others. A key task in network analysis is community detection, which typically consists of dividing the nodes into groups such that nodes within a group are strongly connected, while connections between groups are relatively scarcer. A generative model well-suited for the formation of such communities is the assortative stochastic block model, which prescribes a higher probability of a connection between nodes belonging to the same block rather than to different blocks. A recent line of work has utilized Bayesian nonparametric methods to recover communities in the SBM by placing a prior distribution on the number of blocks and estimating block assignments via collapsed Gibbs samplers. However, efficiently incorporating the assortativity constraint through the prior remains an open problem. In this work, we address this gap, aiming to study the effect of enforcing assortativity on Bayesian community detection and so identify under what scenario it pays its dividends in comparison with standard SBM. We illustrate our findings through an extensive simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19576v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martina Amongero, Pierpaolo De Blasi</dc:creator>
    </item>
    <item>
      <title>Simulation-Based Sensitivity Analysis in Optimal Treatment Regimes and Causal Decomposition with Individualized Interventions</title>
      <link>https://arxiv.org/abs/2506.19010</link>
      <description>arXiv:2506.19010v1 Announce Type: cross 
Abstract: Causal decomposition analysis aims to assess the effect of modifying risk factors on reducing social disparities in outcomes. Recently, this analysis has incorporated individual characteristics when modifying risk factors by utilizing optimal treatment regimes (OTRs). Since the newly defined individualized effects rely on the no omitted confounding assumption, developing sensitivity analyses to account for potential omitted confounding is essential. Moreover, OTRs and individualized effects are primarily based on binary risk factors, and no formal approach currently exists to benchmark the strength of omitted confounding using observed covariates for binary risk factors. To address this gap, we extend a simulation-based sensitivity analysis that simulates unmeasured confounders, addressing two sources of bias emerging from deriving OTRs and estimating individualized effects. Additionally, we propose a formal bounding strategy that benchmarks the strength of omitted confounding for binary risk factors. Using the High School Longitudinal Study 2009 (HSLS:09), we demonstrate this sensitivity analysis and benchmarking method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19010v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Soojin Park, Suyeon Kang, Chioun Lee</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Optimal Transport Maps: Recent Advances and Perspectives</title>
      <link>https://arxiv.org/abs/2506.19025</link>
      <description>arXiv:2506.19025v1 Announce Type: cross 
Abstract: In many applications of optimal transport (OT), the object of primary interest is the optimal transport map. This map rearranges mass from one probability distribution to another in the most efficient way possible by minimizing a specified cost. In this paper we review recent advances in estimating and developing limit theorems for the OT map, using samples from the underlying distributions. We also review parallel lines of work that establish similar results for special cases and variants of the basic OT setup. We conclude with a discussion of key directions for future research with the goal of providing practitioners with reliable inferential tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19025v1</guid>
      <category>math.ST</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sivaraman Balakrishnan, Tudor Manole, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Identifying Causally-Robust Mediators of Health Disparities: A Review and Simulation Studies With Directed Acyclic Graphs</title>
      <link>https://arxiv.org/abs/2506.19047</link>
      <description>arXiv:2506.19047v1 Announce Type: cross 
Abstract: Background Traditionally researchers have used linear approaches such as difference in coefficients DIC and Kitagawa Oaxaca Blinder KOB decomposition to identify risk factors or resources referred to as mediators underlying health disparities Recently causal decomposition analysis CDA has gained popularity by defining clear causal effects of interest and estimating them without modeling restrictions Methods We begin with a brief review of each method under the assumption of no unmeasured confounders followed by two realistic scenarios where unmeasured confounders affect first the relationship between intermediate confounders and the mediator and second the relationship between the mediator and the outcome For each scenario we generate simulated data apply all three methods compare estimates and interpret results using directed acyclic graphs Results The DIC approach performs well only when no intermediate confounders are present a condition unlikely in real world health disparities that arise from many factors over the life course The KOB decomposition is appropriate only when baseline covariates such as age need not be controlled When unmeasured confounding exists DIC yields biased estimates in both scenarios while both KOB and CDA yield biased estimates in the second scenario however CDA supplemented by sensitivity analysis can reveal how robust its estimates are to unmeasured confounding Conclusions We recommend against using DIC for investigating drivers of health disparities and instead advise applying CDA combined with sensitivity analysis as a robust strategy for identifying mediators of health disparities</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19047v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Soojin Park, Su Yeon Kim, Chioun Lee</dc:creator>
    </item>
    <item>
      <title>Identifying Macro Causal Effects in C-DMGs over DMGs</title>
      <link>https://arxiv.org/abs/2506.19650</link>
      <description>arXiv:2506.19650v1 Announce Type: cross 
Abstract: The do-calculus is a sound and complete tool for identifying causal effects in acyclic directed mixed graphs (ADMGs) induced by structural causal models (SCMs). However, in many real-world applications, especially in high-dimensional setting, constructing a fully specified ADMG is often infeasible. This limitation has led to growing interest in partially specified causal representations, particularly through cluster-directed mixed graphs (C-DMGs), which group variables into clusters and offer a more abstract yet practical view of causal dependencies. While these representations can include cycles, recent work has shown that the do-calculus remains sound and complete for identifying macro-level causal effects in C-DMGs over ADMGs under the assumption that all clusters size are greater than 1. Nevertheless, real-world systems often exhibit cyclic causal dynamics at the structural level. To account for this, input-output structural causal models (ioSCMs) have been introduced as a generalization of SCMs that allow for cycles. ioSCMs induce another type of graph structure known as a directed mixed graph (DMG). Analogous to the ADMG setting, one can define C-DMGs over DMGs as high-level representations of causal relations among clusters of variables. In this paper, we prove that, unlike in the ADMG setting, the do-calculus is unconditionally sound and complete for identifying macro causal effects in C-DMGs over DMGs. Furthermore, we show that the graphical criteria for non-identifiability of macro causal effects previously established C-DMGs over ADMGs naturally extends to a subset of C-DMGs over DMGs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19650v1</guid>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Ferreira, Charles K. Assaad</dc:creator>
    </item>
    <item>
      <title>Degree of Interference: A General Framework For Causal Inference Under Interference</title>
      <link>https://arxiv.org/abs/2210.17516</link>
      <description>arXiv:2210.17516v3 Announce Type: replace 
Abstract: One core assumption typically adopted for valid causal inference is that of no interference between experimental units, i.e., the outcome of an experimental unit is unaffected by the treatments assigned to other experimental units. This assumption can be violated in real-life experiments, which significantly complicates the task of causal inference. As the number of potential outcomes increases, it becomes challenging to disentangle direct treatment effects from ``spillover'' effects. Current methodologies are lacking, as they cannot handle arbitrary, unknown interference structures to permit inference on causal estimands. We present a general framework to address the limitations of existing approaches. Our framework is based on the new concept of the ``degree of interference'' (DoI). The DoI is a unit-level latent variable that captures the latent structure of interference. We also develop a data augmentation algorithm that adopts a blocked Gibbs sampler and Bayesian nonparametric methodology to perform inferences on the estimands under our framework. We illustrate the DoI concept and properties of our Bayesian methodology via extensive simulation studies and an analysis of a randomized experiment investigating the impact of a cash transfer program for which interference is a critical concern. Ultimately, our framework enables us to infer causal effects without strong structural assumptions on interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.17516v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuki Ohnishi, Bikram Karmakar, Arman Sabbaghi</dc:creator>
    </item>
    <item>
      <title>Estimating Complier Average Causal Effects with Mixtures of Experts</title>
      <link>https://arxiv.org/abs/2405.02779</link>
      <description>arXiv:2405.02779v2 Announce Type: replace 
Abstract: Treatment non-compliance, where individuals deviate from their assigned experimental conditions, frequently complicates the estimation of causal effects. To address this, we introduce a novel learning framework based on a mixture of experts architecture to estimate the Complier Average Causal Effect (CACE). Our framework provides a flexible alternative to classical instrumental variable methods by relaxing their strict monotonicity and exclusion restriction assumptions. We develop a principled, two-step procedure where each step is optimized with a dedicated Expectation-Maximization (EM) algorithm. Crucially, we provide formal proofs that the model's components are identifiable, ensuring the learning procedure is well-posed. The resulting CACE estimators are proven to be consistent and asymptotically normal. Extensive simulations demonstrate that our method achieves a substantially lower root mean squared error than traditional instrumental variable approaches when their assumptions fail, an advantage that persists even when our own mixture of experts are misspecified. We illustrate the framework's practical utility on data from a large-scale randomized trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02779v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Grolleau, C\'eline B\'eji, Rapha\"el Porcher, Fran\c{c}ois Petit</dc:creator>
    </item>
    <item>
      <title>Tree-based variational inference for Poisson log-normal models</title>
      <link>https://arxiv.org/abs/2406.17361</link>
      <description>arXiv:2406.17361v3 Announce Type: replace 
Abstract: When studying ecosystems, hierarchical trees are often used to organize entities based on proximity criteria, such as the taxonomy in microbiology, social classes in geography, or product types in retail businesses, offering valuable insights into entity relationships. Despite their significance, current count-data models do not leverage this structured information. In particular, the widely used Poisson log-normal (PLN) model, known for its ability to model interactions between entities from count data, lacks the possibility to incorporate such hierarchical tree structures, limiting its applicability in domains characterized by such complexities. To address this matter, we introduce the PLN-Tree model as an extension of the PLN model, specifically designed for modeling hierarchical count data. By integrating structured variational inference techniques, we propose an adapted training procedure and establish identifiability results, enhancing both theoretical foundations and practical interpretability. Experiments on synthetic datasets and human gut microbiome data highlight generative improvements when using PLN-Tree, demonstrating the practical interest of knowledge graphs like the taxonomy in microbiome modeling. Additionally, we present a proof-of-concept implication of the identifiability results by illustrating the practical benefits of using identifiable features for classification tasks, showcasing the versatility of the framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17361v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandre Chaussard (LPSM), Anna Bonnet (LPSM), Elisabeth Gassiat (LMO), Sylvain Le Corff (LPSM)</dc:creator>
    </item>
    <item>
      <title>The conditional saddlepoint approximation for fast and accurate large-scale hypothesis testing</title>
      <link>https://arxiv.org/abs/2407.08911</link>
      <description>arXiv:2407.08911v3 Announce Type: replace 
Abstract: Saddlepoint approximations (SPAs) for resampling-based procedures offer statistically accurate and computationally efficient inference, which is particularly critical in the analysis of large-scale, high-multiplicity data. Despite being introduced 70 years ago, SPAs for resampling-based procedures lack rigorous justification and have been underutilized in modern applications. We establish a theoretical foundation for the SPA in this context by developing a general result on its approximation accuracy for conditional tail probabilities of averages of conditionally independent summands. This result both justifies existing SPAs for classical procedures like the sign-flipping test and enables new SPAs for modern resampling methods, including those using black-box machine learning. Capitalizing on this result, we introduce the saddlepoint approximation-based conditional randomization test (spaCRT), a resampling-free conditional independence test that is both statistically accurate and computationally efficient. The method is especially well-suited for sparse, large-scale datasets such as single-cell CRISPR screens and genome-wide association studies involving rare diseases. We prove the validity of the spaCRT when paired with modern regression tools such as lasso and kernel ridge regression. Extensive analyses of simulated and real data show that the spaCRT controls Type-I error, achieves high power, and outperforms existing asymptotic and resampling-based alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08911v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziang Niu, Jyotishka Ray Choudhury, Eugene Katsevich</dc:creator>
    </item>
    <item>
      <title>Is checking for sequential positivity violations getting you down? Try sPoRT!</title>
      <link>https://arxiv.org/abs/2412.10245</link>
      <description>arXiv:2412.10245v2 Announce Type: replace 
Abstract: Background: Sequential positivity is often a necessary assumption for drawing causal inferences, such as through marginal structural modeling. Unfortunately, verification of this assumption can be challenging because it usually relies on multiple parametric propensity score models, unlikely all correctly specified. Therefore, we propose a new algorithm, called sequential Positivity Regression Tree (sPoRT), to overcome this issue and identify the subgroups found to be violating this assumption, allowing for insights about the nature of the violations and potential solutions.
  Methods: We present different versions of sPoRT based on either stratifying or pooling over time under static or dynamic treatment strategies. This methodological development was motivated by a real-life application of the impact of the timing of initiation of HIV treatment with and without smoothing over time, which we also use to demonstrate the method.
  Results: The illustration of sPoRT demonstrates its easy use and the interpretability of the results for applied epidemiologists. Furthermore, an R notebook showing how to use sPoRT in practice is available at github.com/ArthurChatton/sPoRT-notebook.
  Conclusions: The sPoRT algorithm provides interpretable subgroups violating the sequential positivity violation, allowing patterns and trends in the confounders to be easily identified. We finally provided practical implications and recommendations when positivity violations are identified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10245v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arthur Chatton, Michael Schomaker, Miguel-Angel Luque-Fernandez, Robert W. Platt, Mireille E. Schnitzer</dc:creator>
    </item>
    <item>
      <title>Confidence set for mixture order selection</title>
      <link>https://arxiv.org/abs/2503.18790</link>
      <description>arXiv:2503.18790v2 Announce Type: replace 
Abstract: A fundamental challenge in approximating an unknown density using finite Gaussian mixture models is selecting the number of mixture components, also known as order. Traditional approaches choose a single best model using information criteria. However, often models with different orders yield similar fits, leading to substantial model selection uncertainty and making it challenging to identify the optimal number of components. In this paper, we introduce the Model Selection Confidence Set (MSCS) for order selection in Gaussian mixtures - a set-valued estimator that, with a predefined confidence level, includes the true mixture order across repeated samples. Rather than selecting a single model, our MSCS identifies all plausible orders by determining whether each candidate model is at least as plausible as the best-selected one, using a screening based on a penalized likelihood ratio statistic. We provide theoretical guarantees for asymptotic coverage, and demonstrate its practical advantages through simulations and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18790v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Casa, Davide Ferrari</dc:creator>
    </item>
    <item>
      <title>Finding Distributions that Differ, with False Discovery Rate Control</title>
      <link>https://arxiv.org/abs/2505.13769</link>
      <description>arXiv:2505.13769v2 Announce Type: replace 
Abstract: We consider the problem of comparing a reference distribution with several other distributions. Given a sample from both the reference and the comparison groups, we aim to identify the comparison groups whose distributions differ from that of the reference group. Viewing this as a multiple testing problem, we introduce a methodology that provides exact, distribution-free control of the false discovery rate. To do so, we introduce the concept of batch conformal p-values and demonstrate that they satisfy positive regression dependence across the groups [Benjamini and Yekutieli, 2001], thereby enabling control of the false discovery rate through the Benjamini-Hochberg procedure. The proof of positive regression dependence introduces a novel technique for the inductive construction of rank vectors with almost sure dominance under exchangeability. We evaluate the performance of the proposed procedure through simulations, where, despite being distribution-free, in some cases they show performance comparable to methods with knowledge of the data-generating normal distribution; and further have more power than direct approaches based on conformal out-of-distribution detection. Further, we illustrate our methods on a Hepatitis C treatment dataset, where they can identify patient groups with large treatment effects; and on the Current Population Survey dataset, where they can identify sub-population with long work hours.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13769v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghoon Lee, Edgar Dobriban, Eric Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Safety-Driven Response Adaptive Randomisation: An Application in Non-inferiority Oncology Trials</title>
      <link>https://arxiv.org/abs/2506.08864</link>
      <description>arXiv:2506.08864v2 Announce Type: replace 
Abstract: The majority of response-adaptive randomisation (RAR) designs in the literature use efficacy data to dynamically allocate patients. Their applicability in settings where the efficacy measure is observable with a random delay, such as overall survival, remains challenging. This paper introduces a RAR design referred to as SAFER (Safety-Aware Flexible Elastic Randomisation) design, which uses early-emerging safety data to inform treatment allocation decisions in oncology trials. However, the design is applicable to a range of settings where it may be desirable to favour the arm demonstrating a superior safety profile. This is particularly relevant in non-inferiority trials, which aim to demonstrate an experimental treatment is not inferior to the standard of care, while offering advantages in terms of safety and tolerability. Consequently, an unavoidable and well-established trade-off arises for such designs: to balance the goals of preserving inferential efficiency for the primary non-inferiority outcome while incorporating safety considerations into the randomisation process through RAR. Our method, defines a randomisation procedure which prioritises the assignment of patients to better-tolerated arms and adjusts the allocation proportion according to the observed association between safety and efficacy endpoints. We illustrate our procedure through a comprehensive simulation study, inspired by the CAPP-IT Phase III oncology trial. Our results demonstrate that SAFER preserves statistical power even when efficacy and safety endpoints are weakly associated and offers power gains when a strong positive association is present. Moreover, the approach enables a faster/slower adaptation when efficacy and safety endpoints are temporally aligned/misaligned, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08864v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Vittoria Chiaruttini, Lukas Pin, Sofia S. Villar</dc:creator>
    </item>
    <item>
      <title>Learning Treatment Representations for Downstream Instrumental Variable Regression</title>
      <link>https://arxiv.org/abs/2506.02200</link>
      <description>arXiv:2506.02200v2 Announce Type: replace-cross 
Abstract: Traditional instrumental variable (IV) estimators face a fundamental constraint: they can only accommodate as many endogenous treatment variables as available instruments. This limitation becomes particularly challenging in settings where the treatment is presented in a high-dimensional and unstructured manner (e.g. descriptions of patient treatment pathways in a hospital). In such settings, researchers typically resort to applying unsupervised dimension reduction techniques to learn a low-dimensional treatment representation prior to implementing IV regression analysis. We show that such methods can suffer from substantial omitted variable bias due to implicit regularization in the representation learning step. We propose a novel approach to construct treatment representations by explicitly incorporating instrumental variables during the representation learning process. Our approach provides a framework for handling high-dimensional endogenous variables with limited instruments. We demonstrate both theoretically and empirically that fitting IV models on these instrument-informed representations ensures identification of directions that optimize outcome prediction. Our experiments show that our proposed methodology improves upon the conventional two-stage approaches that perform dimension reduction without incorporating instrument information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02200v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiangyi Lin, Hui Lan, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Double Machine Learning for Conditional Moment Restrictions: IV Regression, Proximal Causal Learning and Beyond</title>
      <link>https://arxiv.org/abs/2506.14950</link>
      <description>arXiv:2506.14950v2 Announce Type: replace-cross 
Abstract: Solving conditional moment restrictions (CMRs) is a key problem considered in statistics, causal inference, and econometrics, where the aim is to solve for a function of interest that satisfies some conditional moment equalities. Specifically, many techniques for causal inference, such as instrumental variable (IV) regression and proximal causal learning (PCL), are CMR problems. Most CMR estimators use a two-stage approach, where the first-stage estimation is directly plugged into the second stage to estimate the function of interest. However, naively plugging in the first-stage estimator can cause heavy bias in the second stage. This is particularly the case for recently proposed CMR estimators that use deep neural network (DNN) estimators for both stages, where regularisation and overfitting bias is present. We propose DML-CMR, a two-stage CMR estimator that provides an unbiased estimate with fast convergence rate guarantees. We derive a novel learning objective to reduce bias and develop the DML-CMR algorithm following the double/debiased machine learning (DML) framework. We show that our DML-CMR estimator can achieve the minimax optimal convergence rate of $O(N^{-1/2})$ under parameterisation and mild regularity conditions, where $N$ is the sample size. We apply DML-CMR to a range of problems using DNN estimators, including IV regression and proximal causal learning on real-world datasets, demonstrating state-of-the-art performance against existing CMR estimators and algorithms tailored to those problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14950v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daqian Shao, Ashkan Soleymani, Francesco Quinzan, Marta Kwiatkowska</dc:creator>
    </item>
  </channel>
</rss>
