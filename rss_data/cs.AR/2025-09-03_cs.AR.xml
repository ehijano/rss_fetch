<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Sep 2025 01:29:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>AGS: Accelerating 3D Gaussian Splatting SLAM via CODEC-Assisted Frame Covisibility Detection</title>
      <link>https://arxiv.org/abs/2509.00433</link>
      <description>arXiv:2509.00433v1 Announce Type: new 
Abstract: Simultaneous Localization and Mapping (SLAM) is a critical task that enables autonomous vehicles to construct maps and localize themselves in unknown environments. Recent breakthroughs combine SLAM with 3D Gaussian Splatting (3DGS) to achieve exceptional reconstruction fidelity. However, existing 3DGS-SLAM systems provide insufficient throughput due to the need for multiple training iterations per frame and the vast number of Gaussians.
  In this paper, we propose AGS, an algorithm-hardware co-design framework to boost the efficiency of 3DGS-SLAM based on the intuition that SLAM systems process frames in a streaming manner, where adjacent frames exhibit high similarity that can be utilized for acceleration. On the software level: 1) We propose a coarse-then-fine-grained pose tracking method with respect to the robot's movement. 2) We avoid redundant computations of Gaussians by sharing their contribution information across frames. On the hardware level, we propose a frame covisibility detection engine to extract intermediate data from the video CODEC. We also implement a pose tracking engine and a mapping engine with workload schedulers to efficiently deploy the AGS algorithm. Our evaluation shows that AGS achieves up to $17.12\times$, $6.71\times$, and $5.41\times$ speedups against the mobile and high-end GPUs, and a state-of-the-art 3DGS accelerator, GSCore.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00433v1</guid>
      <category>cs.AR</category>
      <category>cs.RO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Houshu He, Naifeng Jing, Li Jiang, Xiaoyao Liang, Zhuoran Song</dc:creator>
    </item>
    <item>
      <title>Bit Transition Reduction by Data Transmission Ordering in NoC-based DNN Accelerator</title>
      <link>https://arxiv.org/abs/2509.00500</link>
      <description>arXiv:2509.00500v1 Announce Type: new 
Abstract: As Deep Neural Networks (DNN) are becoming essential, Network-on-Chip (NoC)-based DNN accelerators gained increasing popularity. To save link power in NoC, many researchers focus on reducing the Bit Transition (BT). We propose '1'-bit count-based ordering method to reduce BT for DNN workloads. We provide a mathematical proof of the efficacy of proposed ordering. We evaluate our method through experiments without NoC and with NoC. Without NoC, our proposed ordering method achieves up to 20.38% BT reduction for floating-point-32 data and 55.71% for fixed-point-8 data, respectively. We propose two data ordering methods, affiliated-ordering and separated-ordering to process weight and input jointly or individually and apply them to run full DNNs in NoC-based DNN accelerator. We evaluate our approaches under various configurations, including different DNN models such as LeNet and DarkNet, various NoC sizes with different numbers of memory controllers, random weights and trained weights, and different data precision. Our approach efficiently reduces the link power by achieving up to 32.01% BT reduction for floating-point-32 data and 40.85% BT reduction for fixed-point-8 data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00500v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizhi Chen, Jingwei Li, Wenyao Zhu, Zhonghai Lu</dc:creator>
    </item>
    <item>
      <title>Real-Time Piano Note Frequency Detection Using FPGA and FFT Core</title>
      <link>https://arxiv.org/abs/2509.00589</link>
      <description>arXiv:2509.00589v1 Announce Type: new 
Abstract: Real-time frequency analysis of musical instruments, such as the piano, is an essential feature in areas like electronic tuners, music visualizers, and live sound monitoring. Traditional methods often rely on software-based digital signal processing (DSP), which may introduce latency and require significant computational power. In contrast, hardware platforms such as FPGAs (Field Programmable Gate Arrays) offer the ability to perform such analyses with greater speed and determinism due to their parallel processing capabilities. The primary objective of this project was to analyze analog audio signals from a digital piano using an FPGA-based real-time Fast Fourier Transform (FFT) system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00589v1</guid>
      <category>cs.AR</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shafayet M. Anik, D. G. Perera</dc:creator>
    </item>
    <item>
      <title>COMET: A Framework for Modeling Compound Operation Dataflows with Explicit Collectives</title>
      <link>https://arxiv.org/abs/2509.00599</link>
      <description>arXiv:2509.00599v1 Announce Type: new 
Abstract: Modern machine learning accelerators are designed to efficiently execute deep neural networks (DNNs) by optimizing data movement, memory hierarchy, and compute throughput. However, emerging DNN models such as large language models, state space models increasingly rely on compound operations-structured compositions of multiple basic operations-which introduce new challenges for dataflow optimization and minimizing off-chip memory traffic. Moreover, as model size continues to grow, deployment across spatially distributed compute clusters becomes essential, requiring frequent and complex collective communication. Existing dataflow optimization frameworks and performance models either focus on single operations or lack explicit modeling of collective communication cost, limiting their applicability to modern workloads.
  To address these limitations, we propose, a framework for modeling and optimizing dataflow for compound operations on machine learning accelerators. COMET introduces a novel representation that explicitly models collective communication across spatial clusters, along with latency and energy cost models that account for both GEMM and non-GEMM operation level dependencies within compound operations. We demonstrate COMET's capabilities to analyze and optimize dataflows for compound operations such as GEMM--Softmax, GEMM--LayerNorm, and self-attention, across both edge and cloud accelerator configurations. Our collective-aware modeling enables exploration of a broader mapping space, leading to improved performance and energy efficiency. Specifically, our optimized dataflows achieve up to 1.42$\times$ speedup for GEMM-Softmax, 3.46$\times$ for GEMM-LayerNorm and 1.82$\times$ for self-attention compared to unfused baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00599v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shubham Negi, Manik Singhal, Aayush Ankit, Sudeep Bhoja, Kaushik Roy</dc:creator>
    </item>
    <item>
      <title>On the Thermal Vulnerability of 3D-Stacked High-Bandwidth Memory Architectures</title>
      <link>https://arxiv.org/abs/2509.00633</link>
      <description>arXiv:2509.00633v1 Announce Type: new 
Abstract: 3D-stacked High Bandwidth Memory (HBM) architectures provide high-performance memory interactions to address the well-known performance challenge, namely the memory wall. However, these architectures are susceptible to thermal vulnerabilities due to the inherent vertical adjacency that occurs during the manufacturing process of HBM architectures. We anticipate that adversaries may exploit the intense vertical and lateral adjacency to design and develop thermal performance degradation attacks on the memory banks that host data/instructions from victim applications. In such attacks, the adversary manages to inject short and intense heat pulses from vertically and/or laterally adjacent memory banks, creating a convergent thermal wave that maximizes impact and delays the victim application from accessing its data/instructions. As the attacking application does not access any out-of-range memory locations, it can bypass both design-time security tests and the operating system's memory management policies. In other words, since the attack mimics legitimate workloads, it will be challenging to detect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00633v1</guid>
      <category>cs.AR</category>
      <category>cs.CE</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mehdi Elahi, Mohamed R. Elshamy, Abdel-Hameed A. Badawy, Ahmad Patooghy</dc:creator>
    </item>
    <item>
      <title>Low Power Approximate Multiplier Architecture for Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2509.00764</link>
      <description>arXiv:2509.00764v1 Announce Type: new 
Abstract: This paper proposes an low power approximate multiplier architecture for deep neural network (DNN) applications. A 4:2 compressor, introducing only a single combination error, is designed and integrated into an 8x8 unsigned multiplier. This integration significantly reduces the usage of exact compressors while preserving low error rates. The proposed multiplier is employed within a custom convolution layer and evaluated on neural network tasks, including image recognition and denoising. Hardware evaluation demonstrates that the proposed design achieves up to 30.24% energy savings compared to the best among existing multipliers. In image denoising, the custom approximate convolution layer achieves improved Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM) compared to other approximate designs. Additionally, when applied to handwritten digit recognition, the model maintains high classification accuracy. These results demonstrate that the proposed architecture offers a favorable balance between energy efficiency and computational precision, making it suitable for low-power AI hardware implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00764v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:journal_reference>29th International Symposium on VLSI Design and Test, 2025</arxiv:journal_reference>
      <dc:creator>Pragun Jaswal, L. Hemanth Krishna, B. Srinivasu</dc:creator>
    </item>
    <item>
      <title>Energy Efficient Exact and Approximate Systolic Array Architecture for Matrix Multiplication</title>
      <link>https://arxiv.org/abs/2509.00778</link>
      <description>arXiv:2509.00778v1 Announce Type: new 
Abstract: Deep Neural Networks (DNNs) require highly efficient matrix multiplication engines for complex computations. This paper presents a systolic array architecture incorporating novel exact and approximate processing elements (PEs), designed using energy-efficient positive partial product and negative partial product cells, termed as PPC and NPPC, respectively. The proposed 8-bit exact and approximate PE designs are employed in a 8x8 systolic array, which achieves a energy savings of 22% and 32%, respectively, compared to the existing design. To demonstrate their effectiveness, the proposed PEs are integrated into a systolic array (SA) for Discrete Cosine Transform (DCT) computation, achieving high output quality with a PSNR of 38.21,dB. Furthermore, in an edge detection application using convolution, the approximate PE achieves a PSNR of 30.45,dB. These results highlight the potential of the proposed design to deliver significant energy efficiency while maintaining competitive output quality, making it well-suited for error-resilient image and vision processing applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00778v1</guid>
      <category>cs.AR</category>
      <category>cs.CV</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Pragun Jaswal, L. Hemanth Krishna, B. Srinivasu</dc:creator>
    </item>
    <item>
      <title>GS-TG: 3D Gaussian Splatting Accelerator with Tile Grouping for Reducing Redundant Sorting while Preserving Rasterization Efficiency</title>
      <link>https://arxiv.org/abs/2509.00911</link>
      <description>arXiv:2509.00911v2 Announce Type: new 
Abstract: 3D Gaussian Splatting (3D-GS) has emerged as a promising alternative to neural radiance fields (NeRF) as it offers high speed as well as high image quality in novel view synthesis. Despite these advancements, 3D-GS still struggles to meet the frames per second (FPS) demands of real-time applications. In this paper, we introduce GS-TG, a tile-grouping-based accelerator that enhances 3D-GS rendering speed by reducing redundant sorting operations and preserving rasterization efficiency. GS-TG addresses a critical trade-off issue in 3D-GS rendering: increasing the tile size effectively reduces redundant sorting operations, but it concurrently increases unnecessary rasterization computations. So, during sorting of the proposed approach, GS-TG groups small tiles (for making large tiles) to share sorting operations across tiles within each group, significantly reducing redundant computations. During rasterization, a bitmask assigned to each Gaussian identifies relevant small tiles, to enable efficient sharing of sorting results. Consequently, GS-TG enables sorting to be performed as if a large tile size is used by grouping tiles during the sorting stage, while allowing rasterization to proceed with the original small tiles by using bitmasks in the rasterization stage. GS-TG is a lossless method requiring no retraining or fine-tuning and it can be seamlessly integrated with previous 3D-GS optimization techniques. Experimental results show that GS-TG achieves an average speed-up of 1.54 times over state-of-the-art 3D-GS accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00911v2</guid>
      <category>cs.AR</category>
      <category>cs.CV</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joongho Jo, Jongsun Park</dc:creator>
    </item>
    <item>
      <title>GeneTEK: Low-power, high-performance and scalable genome sequence matching in FPGAs</title>
      <link>https://arxiv.org/abs/2509.01020</link>
      <description>arXiv:2509.01020v1 Announce Type: new 
Abstract: The advent of next-generation sequencing (NGS) has revolutionized genomic research by enabling high-throughput data generation through parallel sequencing of a diverse range of organisms at significantly reduced costs. This breakthrough has unleashed a "Cambrian explosion" in genomic data volume and diversity. This volume of workloads places genomics among the top four big data challenges anticipated for this decade. In this context, pairwise sequence alignment represents a very time- and energy-consuming step in common bioinformatics pipelines. Speeding up this step requires the implementation of heuristic approaches, optimized algorithms, and/or hardware acceleration.
  Whereas state-of-the-art CPU and GPU implementations have demonstrated significant performance gains, recent field programmable gate array (FPGA) implementations have shown improved energy efficiency. However, the latter often suffer from limited scalability due to constraints on hardware resources when aligning longer sequences. In this work, we present a scalable and flexible FPGA-based accelerator template that implements Myers's algorithm using high-level synthesis and a worker-based architecture. GeneTEK, an instance of this accelerator template in a Xilinx Zynq UltraScale+ FPGA, outperforms state-of-the-art CPU and GPU implementations in both speed and energy efficiency, while overcoming scalability limitations of current FPGA approaches. Specifically, GeneTEK achieves at least a 19.4% increase in execution speed and up to 62x reduction in energy consumption compared to leading CPU and GPU solutions, while fitting comparison matrices up to 72% larger compared to previous FPGA solutions. These results reaffirm the potential of FPGAs as an energy-efficient platform for scalable genomic workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01020v1</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elena Espinosa, Rub\'en Rodr\'iguez \'Alvarez, Jos\'e Miranda, Rafael Larrosa, Miguel Pe\'on-Quir\'os, Oscar Plata, David Atienza</dc:creator>
    </item>
    <item>
      <title>LinkBo: An Adaptive Single-Wire, Low-Latency, and Fault-Tolerant Communications Interface for Variable-Distance Chip-to-Chip Systems</title>
      <link>https://arxiv.org/abs/2509.01339</link>
      <description>arXiv:2509.01339v1 Announce Type: new 
Abstract: Cost-effective embedded systems necessitate utilizing the single-wire communication protocol for inter-chip communication, thanks to its reduced pin count in comparison to the multi-wire I2C or SPI protocols. However, current single-wire protocols suffer from increased latency, restricted throughput, and lack of robustness. This paper presents LinkBo, an innovative single-wire protocol that offers reduced latency, enhanced throughput, and greater robustness with hardware-interrupt for variable-distance inter-chip communication. The LinkBo protocol-level guarantees that high-priority messages are delivered with an error detection feature in just 50.4 $\mu$s, surpassing current commercial options, 1-wire and UNI/O by at least 20X and 6.3X, respectively. In addition, we present the hardware architecture for this new protocol and its performance evaluation on a hardware platform consisting of two FPGAs. Our findings demonstrate that the protocol reliably supports wire lengths up to 15 meters with a data rate of 300 kbps, while reaching a maximum data rate of 7.5 Mbps over an 11 cm wire, providing reliable performance for varying inter-chip communication distances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01339v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Bochen Ye, Gustavo Naspolini, Kimmo Salo, Manil Dev Gomony</dc:creator>
    </item>
    <item>
      <title>Guidance and Control Neural Network Acceleration using Memristors</title>
      <link>https://arxiv.org/abs/2509.02369</link>
      <description>arXiv:2509.02369v1 Announce Type: new 
Abstract: In recent years, the space community has been exploring the possibilities of Artificial Intelligence (AI), specifically Artificial Neural Networks (ANNs), for a variety of on board applications. However, this development is limited by the restricted energy budget of smallsats and cubesats as well as radiation concerns plaguing modern chips. This necessitates research into neural network accelerators capable of meeting these requirements whilst satisfying the compute and performance needs of the application. This paper explores the use of Phase-Change Memory (PCM) and Resistive Random-Access Memory (RRAM) memristors for on-board in-memory computing AI acceleration in space applications. A guidance and control neural network (G\&amp;CNET) accelerated using memristors is simulated in a variety of scenarios and with both device types to evaluate the performance of memristor-based accelerators, considering device non-idealities such as noise and conductance drift. We show that the memristive accelerator is able to learn the expert actions, though challenges remain with the impact of noise on accuracy. We also show that re-training after degradation is able to restore performance to nominal levels. This study provides a foundation for future research into memristor-based AI accelerators for space, highlighting their potential and the need for further investigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02369v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.13885548</arxiv:DOI>
      <dc:creator>Zacharia A. Rudge, Dario Izzo, Moritz Fieback, Anteneh Gebregiorgis, Said Hamdioui, Dominik Dold</dc:creator>
    </item>
    <item>
      <title>BOLT: Bandwidth-Optimized Lightning-Fast Oblivious Map powered by Secure HBM Accelerators</title>
      <link>https://arxiv.org/abs/2509.01742</link>
      <description>arXiv:2509.01742v1 Announce Type: cross 
Abstract: While Trusted Execution Environments provide a strong foundation for secure cloud computing, they remain vulnerable to access pattern leakages. Oblivious Maps (OMAPs) mitigate this by fully hiding access patterns but suffer from high overhead due to randomized remapping and worst-case padding. We argue these costs are not fundamental. Modern accelerators featuring High-Bandwidth Memory (HBM) offer a new opportunity: Vaswani et al. [OSDI'18] point out that eavesdropping on HBM is difficult -- even for physical attackers -- as its memory channels are sealed together with processor cores inside the same physical package. Later, Hunt et al. [NSDI'20] show that, with proper isolation, HBM can be turned into an unobservable region where both data and memory traces are hidden. This motivates a rethink of OMAP design with HBM-backed solutions to finally overcome their traditional performance limits. Building on these insights, we present BOLT, a Bandwidth Optimized, Lightning-fast OMAP accelerator that, for the first time, achieves O(1) + O((log log N)^2) bandwidth overhead. BOLT introduces three key innovations: (i) a new OMAP algorithm that leverages isolated HBM as an unobservable cache to accelerate oblivious access to large host memory; (ii) a self-hosted architecture that offloads execution and memory control from the host to mitigate CPU-side leakage; and (iii) tailored algorithm-architecture co-designs that maximize resource efficiency. We implement a prototype BOLT on a Xilinx U55C FPGA. Evaluations show that BOLT achieves up to 279x and 480x speedups in initialization and query time, respectively, over state-of-the-art OMAPs, including an industry implementation from Facebook.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01742v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yitong Guo, Hongbo Chen, Haobin Hiroki Chen, Yukui Luo, XiaoFeng Wang, Chenghong Wang</dc:creator>
    </item>
    <item>
      <title>ASiM: Modeling and Analyzing Inference Accuracy of SRAM-Based Analog CiM Circuits</title>
      <link>https://arxiv.org/abs/2411.11022</link>
      <description>arXiv:2411.11022v4 Announce Type: replace 
Abstract: SRAM-based Analog Compute-in-Memory (ACiM) demonstrates promising energy efficiency for deep neural network (DNN) processing. Nevertheless, efforts to optimize efficiency frequently compromise accuracy, and this trade-off remains insufficiently studied due to the difficulty of performing full-system validation. Specifically, existing simulation tools rarely target SRAM-based ACiM and exhibit inconsistent accuracy predictions, highlighting the need for a standardized, SRAM CiM circuit-aware evaluation methodology. This paper presents ASiM, a simulation framework for evaluating inference accuracy in SRAM-based ACiM systems. ASiM captures critical effects in SRAM based analog compute in memory systems, such as ADC quantization, bit parallel encoding, and analog noise, which must be modeled with high fidelity due to their distinct behavior in charge domain architectures compared to other memory technologies. ASiM supports a wide range of modern DNN workloads, including CNN and Transformer-based models such as ViT, and scales to large-scale tasks like ImageNet classification. Our results indicate that bit-parallel encoding can improve energy efficiency with only modest accuracy degradation; however, even 1 LSB of analog noise can significantly impair inference performance, particularly in complex tasks such as ImageNet. To address this, we explore hybrid analog-digital execution and majority voting schemes, both of which enhance robustness without negating energy savings. ASiM bridges the gap between hardware design and inference performance, offering actionable insights for energy-efficient, high-accuracy ACiM deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11022v4</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TVLSI.2025.3605286</arxiv:DOI>
      <dc:creator>Wenlun Zhang, Shimpei Ando, Yung-Chin Chen, Kentaro Yoshioka</dc:creator>
    </item>
    <item>
      <title>Neural Signal Compression using RAMAN tinyML Accelerator for BCI Applications</title>
      <link>https://arxiv.org/abs/2504.06996</link>
      <description>arXiv:2504.06996v2 Announce Type: replace 
Abstract: High-quality, multi-channel neural recording is indispensable for neuroscience research and clinical applications. Large-scale brain recordings often produce vast amounts of data that must be wirelessly transmitted for subsequent offline analysis and decoding, especially in brain-computer interfaces (BCIs) utilizing high-density intracortical recordings with hundreds or thousands of electrodes. However, transmitting raw neural data presents significant challenges due to limited communication bandwidth and resultant excessive heating. To address this challenge, we propose a neural signal compression scheme utilizing Convolutional Autoencoders (CAEs), which achieves a compression ratio of up to 150 for compressing local field potentials (LFPs). The CAE encoder section is implemented on RAMAN, an energy-efficient tinyML accelerator designed for edge computing. RAMAN leverages sparsity in activation and weights through zero skipping, gating, and weight compression techniques. Additionally, we employ hardware-software co-optimization by pruning the CAE encoder model parameters using a hardware-aware balanced stochastic pruning strategy, resolving workload imbalance issues and eliminating indexing overhead to reduce parameter storage requirements by up to 32.4%. Post layout simulation shows that the RAMAN encoder can be implemented in a TSMC 65-nm CMOS process, occupying a core area of 0.0187 mm2 per channel. Operating at a clock frequency of 2 MHz and a supply voltage of 1.2 V, the estimated power consumption is 15.1 uW per channel for the proposed DS-CAE1 model. For functional validation, the RAMAN encoder was also deployed on an Efinix Ti60 FPGA, utilizing 37.3k LUTs and 8.6k flip-flops. The compressed neural data from RAMAN is reconstructed offline with SNDR of 22.6 dB and 27.4 dB, along with R2 scores of 0.81 and 0.94, respectively, evaluated on two monkey neural recordings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06996v2</guid>
      <category>cs.AR</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adithya Krishna, Sohan Debnath, Madhuvanthi Srivatsav, Andr\'e van Schaik, Mahesh Mehendale, Chetan Singh Thakur</dc:creator>
    </item>
    <item>
      <title>LP-Spec: Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization</title>
      <link>https://arxiv.org/abs/2508.07227</link>
      <description>arXiv:2508.07227v3 Announce Type: replace 
Abstract: LLM inference on mobile devices faces extraneous challenges due to limited memory bandwidth and computational resources. To address these issues, speculative inference and processing-in-memory (PIM) techniques have been explored at the algorithmic and hardware levels. However, speculative inference results in more compute-intensive GEMM operations, creating new design trade-offs for existing GEMV-accelerated PIM architectures. Furthermore, there exists a significant amount of redundant draft tokens in tree-based speculative inference, necessitating efficient token management schemes to minimize energy consumption. In this work, we present LP-Spec, an architecture-dataflow co-design leveraging hybrid LPDDR5 performance-enhanced PIM architecture with draft token pruning and dynamic workload scheduling to accelerate LLM speculative inference. A near-data memory controller is proposed to enable data reallocation between DRAM and PIM banks. Furthermore, a data allocation unit based on the hardware-aware draft token pruner is developed to minimize energy consumption and fully exploit parallel execution opportunities. Compared to end-to-end LLM inference on other mobile solutions such as mobile NPUs or GEMV-accelerated PIMs, our LP-Spec achieves 13.21x, 7.56x, and 99.87x improvements in performance, energy efficiency, and energy-delay-product (EDP). Compared with prior AttAcc PIM and RTX 3090 GPU, LP-Spec can obtain 12.83x and 415.31x EDP reduction benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07227v3</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyuan He, Zhantong Zhu, Yandong He, Tianyu Jia</dc:creator>
    </item>
    <item>
      <title>RIROS: A Parallel RTL Fault SImulation FRamework with TwO-Dimensional Parallelism and Unified Schedule</title>
      <link>https://arxiv.org/abs/2508.16376</link>
      <description>arXiv:2508.16376v2 Announce Type: replace 
Abstract: With the rapid development of safety-critical applications such as autonomous driving and embodied intelligence, the functional safety of the corresponding electronic chips becomes more critical. Ensuring chip functional safety requires performing a large number of time-consuming RTL fault simulations during the design phase, significantly increasing the verification cycle. To meet time-to-market demands while ensuring thorough chip verification, parallel acceleration of RTL fault simulation is necessary. Due to the dynamic nature of fault propagation paths and varying fault propagation capabilities, task loads in RTL fault simulation are highly imbalanced, making traditional singledimension parallel methods, such as structural-level parallelism, ineffective. Through an analysis of fault propagation paths and task loads, we identify two types of tasks in RTL fault simulation: tasks that are few in number but high in load, and tasks that are numerous but low in load. Based on this insight, we propose a two-dimensional parallel approach that combines structurallevel and fault-level parallelism to minimize bubbles in RTL fault simulation. Structural-level parallelism combining with workstealing mechanism is used to handle the numerous low-load tasks, while fault-level parallelism is applied to split the high-load tasks. Besides, we deviate from the traditional serial execution model of computation and global synchronization in RTL simulation by proposing a unified computation/global synchronization scheduling approach, which further eliminates bubbles. Finally, we implemented a parallel RTL fault simulation framework, RIROS. Experimental results show a performance improvement of 7.0 times and 11.0 times compared to the state-of-the-art RTL fault simulation and a commercial tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16376v2</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaping Tang, Jianan Mu, Zizhen Liu, Ge Yu, Tenghui Hua, Bin Sun, Silin Liu, Jing Ye, Huawei Li</dc:creator>
    </item>
    <item>
      <title>GPT-OSS-20B: A Comprehensive Deployment-Centric Analysis of OpenAI's Open-Weight Mixture of Experts Model</title>
      <link>https://arxiv.org/abs/2508.16700</link>
      <description>arXiv:2508.16700v2 Announce Type: replace 
Abstract: We present a single-GPU (H100, bf16) evaluation of GPT-OSS-20B (Mixture-of-Experts; 20.9B total, approx. 3.61B active) against dense baselines Qwen3-32B and Yi-34B across multiple dimensions. We measure true time-to-first-token (TTFT), full-decode throughput (TPOT), end-to-end latency percentiles, peak VRAM with past key values (PKV) held, and energy via a consistent nvidia-smi-based sampler. At a 2048-token context with 64-token decode, GPT-OSS-20B delivers higher decode throughput and tokens per Joule than dense baselines Qwen3-32B and Yi-34B, while substantially reducing peak VRAM and energy per 1000 generated tokens; its TTFT is higher due to MoE routing overhead. With only 17.3% of parameters active (3.61B of 20.9B), GPT-OSS-20B provides about 31.8% higher decode throughput and 25.8% lower energy per 1000 generated tokens than Qwen3-32B at 2048/64, while using 31.7% less peak VRAM. Normalized by active parameters, GPT-OSS-20B shows markedly stronger per-active-parameter efficiency (APE), underscoring MoE's deployment advantages. We do not evaluate accuracy; this is a deployment-focused study. We release code and consolidated results to enable replication and extension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16700v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deepak Kumar, Divakar Yadav, Yash Patel</dc:creator>
    </item>
    <item>
      <title>The Future of Memory: Limits and Opportunities</title>
      <link>https://arxiv.org/abs/2508.20425</link>
      <description>arXiv:2508.20425v2 Announce Type: replace 
Abstract: Memory latency, bandwidth, capacity, and energy increasingly limit performance. In this paper, we reconsider proposed system architectures that consist of huge (many-terabyte to petabyte scale) memories shared among large numbers of CPUs. We argue two practical engineering challenges, scaling and signaling, limit such designs. We propose the opposite approach. Rather than create large, shared, homogenous memories, systems explicitly break memory up into smaller slices more tightly coupled with compute elements. Leveraging advances in 2.5D/3D integration, this compute-memory node provisions private local memory, enabling accesses of node-exclusive data through micrometer-scale distances, and dramatically reduced access cost. In-package memory elements support shared state within a processor, providing far better bandwidth and energy-efficiency than DRAM, which is used as main memory for large working sets and cold data. Hardware making memory capacities and distances explicit allows software to efficiently compose this hierarchy, managing data placement and movement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20425v2</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Dayo, Shuhan Liu, Peijing Li, Philip Levis, Subhasish Mitra, Thierry Tambe, David Tennenhouse, H. -S. Philip Wong</dc:creator>
    </item>
  </channel>
</rss>
