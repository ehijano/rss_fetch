<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Jun 2025 03:04:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Energy-Oriented Computing Architecture Simulator for SNN Training</title>
      <link>https://arxiv.org/abs/2505.24137</link>
      <description>arXiv:2505.24137v1 Announce Type: new 
Abstract: With the growing demand for intelligent computing, neuromorphic computing, a paradigm that mimics the structure and functionality of the human brain, offers a promising approach to developing new high-efficiency intelligent computing systems. Spiking Neural Networks (SNNs), the foundation of neuromorphic computing, have garnered significant attention due to their unique potential in energy efficiency and biomimetic neural processing. However, current hardware development for efficient SNN training lags significantly. No systematic energy evaluation methods exist for SNN training tasks. Therefore, this paper proposes an Energy-Oriented Computing Architecture Simulator (EOCAS) for SNN training to identify the optimal architecture. EOCAS investigates the high sparsity of spike signals, unique hardware design representations, energy assessment, and computation patterns to support energy optimization in various architectures. Under the guidance of EOCAS, we implement the power-aimed optimal hardware architecture through Verilog HDL and achieve low energy consumption using Synopsys Design Compiler with TSMC-28nm technology library under typical parameters. Compared with several State-Of-The-Art (SOTA) DNN and SNN works, our hardware architecture outstands others in various criteria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24137v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhao Ma (Pengcheng Laboratory, Southern University of Science and Technology), Wanyi Jia (Pengcheng Laboratory, Southern University of Science and Technology), Yanyu Lin (Pengcheng Laboratory), Wenjie Lin (Pengcheng Laboratory), Xueke Zhu (Pengcheng Laboratory), Huihui Zhou (Pengcheng Laboratory), Fengwei An (Southern University of Science and Technology)</dc:creator>
    </item>
    <item>
      <title>Ramping Up Open-Source RISC-V Cores: Assessing the Energy Efficiency of Superscalar, Out-of-Order Execution</title>
      <link>https://arxiv.org/abs/2505.24363</link>
      <description>arXiv:2505.24363v1 Announce Type: new 
Abstract: Open-source RISC-V cores are increasingly demanded in domains like automotive and space, where achieving high instructions per cycle (IPC) through superscalar and out-of-order (OoO) execution is crucial. However, high-performance open-source RISC-V cores face adoption challenges: some (e.g. BOOM, Xiangshan) are developed in Chisel with limited support from industrial electronic design automation (EDA) tools. Others, like the XuanTie C910 core, use proprietary interfaces and protocols, including non-standard AXI protocol extensions, interrupts, and debug support.
  In this work, we present a modified version of the OoO C910 core to achieve full RISC-V standard compliance in its debug, interrupt, and memory interfaces. We also introduce CVA6S+, an enhanced version of the dual-issue, industry-supported open-source CVA6 core. CVA6S+ achieves 34.4% performance improvement over CVA6 core.
  We conduct a detailed performance, area, power, and energy analysis on the superscalar out-of-order C910, superscalar in-order CVA6S+ and vanilla, single-issue in-order CVA6, all implemented in a 22nm technology and integrated into Cheshire, an open-source modular SoC. We examine the performance and efficiency of different microarchitectures using the same ISA, SoC, and implementation with identical technology, tools, and methodologies. The area and performance rankings of CVA6, CVA6S+, and C910 follow expected trends: compared to the scalar CVA6, CVA6S+ shows an area increase of 6% and an IPC improvement of 34.4%, while C910 exhibits a 75% increase in area and a 119.5% improvement in IPC. However, efficiency analysis reveals that CVA6S+ leads in area efficiency (GOPS/mm2), while the C910 is highly competitive in energy efficiency (GOPS/W). This challenges the common belief that high performance in superscalar and out-of-order cores inherently comes at a significant cost in area and energy efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24363v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3719276.3725186</arxiv:DOI>
      <dc:creator>Zexin Fu, Riccardo Tedeschi, Gianmarco Ottavi, Nils Wistoff, C\'esar Fuguet, Davide Rossi, Luca Benini</dc:creator>
    </item>
    <item>
      <title>Bi-SamplerZ: A Hardware-Efficient Gaussian Sampler Architecture for Quantum-Resistant Falcon Signatures</title>
      <link>https://arxiv.org/abs/2505.24509</link>
      <description>arXiv:2505.24509v1 Announce Type: new 
Abstract: FALCON is a standardized quantum-resistant digital signature scheme that offers advantages over other schemes, but features more complex signature generation process. This paper presents Bi-Samplerz, a fully hardware-implemented, high-efficiency dual-path discrete Gaussian sampler designed to accelerate Falcon signature generation. Observing that the SamplerZ subroutine is consistently invoked in pairs during each signature generation, we propose a dual-datapath architecture capable of generating two sampling results simultaneously. To make the best use of coefficient correlation and the inherent properties of rejection sampling, we introduce an assistance mechanism that enables effective collaboration between the two datapaths, rather than simply duplicating the sampling process. Additionally, we incorporate several architectural optimizations over existing designs to further enhance speed, area efficiency, and resource utilization. Experimental results demonstrate that Bi-SamplerZ achieves the lowest sampling latency to date among existing designs, benefiting from fine-grained pipeline optimization and efficient control coordination. Compared with the state-of-the-art full hardware implementations, Bi-SamplerZ reduces the sampling cycle count by 54.1\% while incurring only a moderate increase in hardware resource consumption, thereby achieving the best-known area-time product (ATP) for fully hardware-based sampler designs. In addition, to facilitate comparison with existing works, we provide both ASIC and FPGA implementations. Together, these results highlight the suitability of Bi-SamplerZ as a high-performance sampling engine in standardized post-quantum cryptographic systems such as Falcon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24509v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Binke Zhao, Ghada Alsuhi, Hani Saleh, Baker Mohammad</dc:creator>
    </item>
    <item>
      <title>Chameleon: A MatMul-Free Temporal Convolutional Network Accelerator for End-to-End Few-Shot and Continual Learning from Sequential Data</title>
      <link>https://arxiv.org/abs/2505.24852</link>
      <description>arXiv:2505.24852v1 Announce Type: new 
Abstract: On-device learning at the edge enables low-latency, private personalization with improved long-term robustness and reduced maintenance costs. Yet, achieving scalable, low-power end-to-end on-chip learning, especially from real-world sequential data with a limited number of examples, is an open challenge. Indeed, accelerators supporting error backpropagation optimize for learning performance at the expense of inference efficiency, while simplified learning algorithms often fail to reach acceptable accuracy targets. In this work, we present Chameleon, leveraging three key contributions to solve these challenges. (i) A unified learning and inference architecture supports few-shot learning (FSL), continual learning (CL) and inference at only 0.5% area overhead to the inference logic. (ii) Long temporal dependencies are efficiently captured with temporal convolutional networks (TCNs), enabling the first demonstration of end-to-end on-chip FSL and CL on sequential data and inference on 16-kHz raw audio. (iii) A dual-mode, matrix-multiplication-free compute array allows either matching the power consumption of state-of-the-art inference-only keyword spotting (KWS) accelerators or enabling $4.3\times$ higher peak GOPS. Fabricated in 40-nm CMOS, Chameleon sets new accuracy records on Omniglot for end-to-end on-chip FSL (96.8%, 5-way 1-shot, 98.8%, 5-way 5-shot) and CL (82.2% final accuracy for learning 250 classes with 10 shots), while maintaining an inference accuracy of 93.3% on the 12-class Google Speech Commands dataset at an extreme-edge power budget of 3.1 $\mu$W.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24852v1</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Douwe den Blanken, Charlotte Frenkel</dc:creator>
    </item>
    <item>
      <title>Linear Layouts: Robust Code Generation of Efficient Tensor Computation Using $\mathbb{F}_2$</title>
      <link>https://arxiv.org/abs/2505.23819</link>
      <description>arXiv:2505.23819v1 Announce Type: cross 
Abstract: Efficient tensor computation is a cornerstone of modern deep learning (DL) workloads, yet existing approaches struggle to achieve flexible and performant design and implementation of tensor layouts -- mappings between logical tensors and hardware resources. The increasing complexity of DL algorithms and hardware demands a generic and systematic approach to handling tensor layouts. In this work, we introduce Linear Layouts, a novel approach that models tensor layouts using linear algebra over $\mathbb{F}_2$. By representing tensor layouts as binary matrices acting on the bits of the hardware representation, our approach enables a generic layout definition -- as opposed to the classical case-by-case approach -- and allows for generic layout-to-layout conversions, eliminating the quadratic explosion that plagues existing solutions. We integrate linear layouts with Triton and demonstrate their effectiveness in optimizing individual Triton operators as well as kernels written in Triton. We also show that linear layouts reduce engineering effort in the compiler backend while fixing several bugs in Triton's legacy layout system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23819v1</guid>
      <category>cs.PL</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keren Zhou, Mario Lezcano, Adam Goucher, Akhmed Rakhmati, Jeff Niu, Justin Lebar, Pawel Szczerbuk, Peter Bell, Phil Tillet, Thomas Raoux, Zahi Moudallal</dc:creator>
    </item>
    <item>
      <title>EmbAdvisor: Adaptive Cache Management for Sustainable LLM Serving</title>
      <link>https://arxiv.org/abs/2505.23970</link>
      <description>arXiv:2505.23970v1 Announce Type: cross 
Abstract: As large language models (LLMs) become widely used, their environmental impact$\unicode{x2014}$especially carbon emissions$\unicode{x2014}$has attracted more attention. Prior studies focus on compute-related carbon emissions. In this paper, we find that storage is another key contributor. LLM caching, which saves and reuses KV caches for repeated context, reduces operational carbon by avoiding redundant computation. However, this benefit comes at the cost of embodied carbon from high-capacity, high-speed SSDs. As LLMs scale, the embodied carbon of storage grows significantly.
  To address this tradeoff, we present EmbAdvisor, a carbon-aware caching framework that selects the optimal cache size for LLM serving. EmbAdvisor profiles different LLM tasks and uses an Integer Linear Programming (ILP) solver to select cache sizes that meet SLOs while minimizing total carbon emissions. Overall, EmbAdvisor reduces the average carbon emissions of a Llama-3 70B model by 9.5% under various carbon intensities compared to a non-adaptive cache scenario, and can save up to 31.2% when the carbon intensity is low.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23970v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuyang Tian, Desen Sun, Yi Ding, Sihang Liu</dc:creator>
    </item>
    <item>
      <title>CodeV-R1: Reasoning-Enhanced Verilog Generation</title>
      <link>https://arxiv.org/abs/2505.24183</link>
      <description>arXiv:2505.24183v1 Announce Type: cross 
Abstract: Large language models (LLMs) trained via reinforcement learning with verifiable reward (RLVR) have achieved breakthroughs on tasks with explicit, automatable verification, such as software programming and mathematical problems. Extending RLVR to electronic design automation (EDA), especially automatically generating hardware description languages (HDLs) like Verilog from natural-language (NL) specifications, however, poses three key challenges: the lack of automated and accurate verification environments, the scarcity of high-quality NL-code pairs, and the prohibitive computation cost of RLVR. To this end, we introduce CodeV-R1, an RLVR framework for training Verilog generation LLMs. First, we develop a rule-based testbench generator that performs robust equivalence checking against golden references. Second, we propose a round-trip data synthesis method that pairs open-source Verilog snippets with LLM-generated NL descriptions, verifies code-NL-code consistency via the generated testbench, and filters out inequivalent examples to yield a high-quality dataset. Third, we employ a two-stage "distill-then-RL" training pipeline: distillation for the cold start of reasoning abilities, followed by adaptive DAPO, our novel RLVR algorithm that can reduce training cost by adaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves 68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively, surpassing prior state-of-the-art by 12~20%, while matching or even exceeding the performance of 671B DeepSeek-R1. We will release our model, training pipeline, and dataset to facilitate research in EDA and LLM communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24183v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <category>cs.PL</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaoyu Zhu, Di Huang, Hanqi Lyu, Xiaoyun Zhang, Chongxiao Li, Wenxuan Shi, Yutong Wu, Jianan Mu, Jinghua Wang, Yang Zhao, Pengwei Jin, Shuyao Cheng, Shengwen Liang, Xishan Zhang, Rui Zhang, Zidong Du, Qi Guo, Xing Hu, Yunji Chen</dc:creator>
    </item>
    <item>
      <title>Minimizing Ray Tracing Memory Traffic through Quantized Structures and Ray Stream Tracing</title>
      <link>https://arxiv.org/abs/2505.24653</link>
      <description>arXiv:2505.24653v1 Announce Type: cross 
Abstract: Memory bandwidth constraints continue to be a significant limiting factor in ray tracing performance, particularly as scene complexity grows and computational capabilities outpace memory access speeds. This paper presents a memory-efficient ray tracing methodology that integrates compressed data structures with ray stream techniques to reduce memory traffic. The approach implements compressed BVH and triangle representations to minimize acceleration structure size in combination with ray stream tracing to reduce traversal stack memory traffic. The technique employs fixed-point arithmetic for intersection tests for prospective hardware with tailored integer operations. Despite using reduced precision, geometric holes are avoided by leveraging fixed-point arithmetic instead of encountering the floating-point rounding errors common in traditional approaches. Quantitative analysis demonstrates significant memory traffic reduction across various scene complexities and BVH configurations. The presented 8-wide BVH ray stream implementation reduces memory traffic to only 18% of traditional approaches by using 8-bit quantization for box and triangle coordinates and directly ray tracing these quantized structures. These reductions are especially beneficial for bandwidth-constrained hardware environments such as mobile devices. This integrated approach addresses both memory bandwidth limitations and numerical precision challenges inherent to modern ray tracing applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24653v1</guid>
      <category>cs.GR</category>
      <category>cs.AR</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moritz Grauer, Johannes Hanika, Carsten Dachsbacher</dc:creator>
    </item>
    <item>
      <title>Running Conventional Automatic Speech Recognition on Memristor Hardware: A Simulated Approach</title>
      <link>https://arxiv.org/abs/2505.24721</link>
      <description>arXiv:2505.24721v1 Announce Type: cross 
Abstract: Memristor-based hardware offers new possibilities for energy-efficient machine learning (ML) by providing analog in-memory matrix multiplication. Current hardware prototypes cannot fit large neural networks, and related literature covers only small ML models for tasks like MNIST or single word recognition. Simulation can be used to explore how hardware properties affect larger models, but existing software assumes simplified hardware. We propose a PyTorch-based library based on "Synaptogen" to simulate neural network execution with accurately captured memristor hardware properties. For the first time, we show how an ML system with millions of parameters would behave on memristor hardware, using a Conformer trained on the speech recognition task TED-LIUMv2 as example. With adjusted quantization-aware training, we limit the relative degradation in word error rate to 25% when using a 3-bit weight precision to execute linear operations via simulated analog computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24721v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick Rossenbach, Benedikt Hilmes, Leon Brackmann, Moritz Gunz, Ralf Schl\"uter</dc:creator>
    </item>
    <item>
      <title>TuRTLe: A Unified Evaluation of LLMs for RTL Generation</title>
      <link>https://arxiv.org/abs/2504.01986</link>
      <description>arXiv:2504.01986v2 Announce Type: replace 
Abstract: The rapid advancements in LLMs have driven the adoption of generative AI in various domains, including Electronic Design Automation (EDA). Unlike traditional software development, EDA presents unique challenges, as generated RTL code must not only be syntactically correct and functionally accurate but also synthesizable by hardware generators while meeting performance, power, and area constraints. These additional requirements introduce complexities that existing code-generation benchmarks often fail to capture, limiting their effectiveness in evaluating LLMs for RTL generation. To address this gap, we propose TuRTLe, a unified evaluation framework designed to systematically assess LLMs across key RTL generation tasks. TuRTLe integrates multiple existing benchmarks and automates the evaluation process, enabling a comprehensive assessment of LLM performance in syntax correctness, functional correctness, synthesis, PPA optimization, and exact line completion. Using this framework, we benchmark a diverse set of open LLMs and analyze their strengths and weaknesses in EDA-specific tasks. Our results show that reasoning-based models, such as DeepSeek R1, consistently outperform others across multiple evaluation criteria, but at the cost of increased computational overhead and inference latency. Additionally, base models are better suited in module completion tasks, while instruct-tuned models perform better in specification-to-RTL tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01986v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dario Garcia-Gasulla, Gokcen Kestor, Emanuele Parisi, Miquel Albert\'i-Binimelis, Cristian Gutierrez, Razine Moundir Ghorab, Orlando Montenegro, Bernat Homs, Miquel Moreto</dc:creator>
    </item>
    <item>
      <title>Efficient Implementation of RISC-V Vector Permutation Instructions</title>
      <link>https://arxiv.org/abs/2505.07112</link>
      <description>arXiv:2505.07112v2 Announce Type: replace 
Abstract: RISC-V CPUs leverage the RVV (RISC-V Vector) extension to accelerate data-parallel workloads. In addition to arithmetic operations, RVV includes powerful permutation instructions that enable flexible element rearrangement within vector registers --critical for optimizing performance in tasks such as matrix operations and cryptographic computations. However, the diverse control mechanisms of these instructions complicate their execution within a unified datapath while maintaining the fixed-latency requirement of cryptographic accelerators. To address this, we propose a unified microarchitecture capable of executing all RVV permutation instructions efficiently, regardless of their control information structure. This approach minimizes area and hardware costs while ensuring single-cycle execution for short vector machines (up to 256 bits) and enabling efficient pipelining for longer vectors. The proposed design is integrated into an open-source RISC-V vector processor and implemented at 7 nm using the OpenRoad physical synthesis flow. Experimental results validate the efficiency of our unified vector permutation unit, demonstrating that it only incurs 1.5% area overhead to the total vector processor. Furthermore, this area overhead decreases to near-0% as the minimum supported element width for vector permutations increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07112v2</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vasileios Titopoulos, George Alexakis, Chrysostomos Nicopoulos, Giorgos Dimitrakopoulos</dc:creator>
    </item>
    <item>
      <title>Low-Cost FlashAttention with Fused Exponential and Multiplication Hardware Operators</title>
      <link>https://arxiv.org/abs/2505.14314</link>
      <description>arXiv:2505.14314v2 Announce Type: replace 
Abstract: Attention mechanisms, particularly within Transformer architectures and large language models (LLMs), have revolutionized sequence modeling in machine learning and artificial intelligence applications. To compute attention for increasingly long sequences, specialized accelerators have been proposed to execute key attention steps directly in hardware. Among the various recently proposed architectures, those based on variants of the FlashAttention algorithm, originally designed for GPUs, stand out due to their optimized computation, tiling capabilities, and reduced memory traffic. In this work, we focus on optimizing the kernel of floating-point-based FlashAttention using new hardware operators that fuse the computation of exponentials and vector multiplications, e.g., e^x, V. The proposed ExpMul hardware operators significantly reduce the area and power costs of FlashAttention-based hardware accelerators. When implemented in a 28nm ASIC technology, they achieve improvements of 28.8% in area and 17.6% in power, on average, compared to state-of-the-art hardware architectures with separate exponentials and vector multiplications hardware operators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14314v2</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kosmas Alexandridis, Vasileios Titopoulos, Giorgos Dimitrakopoulos</dc:creator>
    </item>
    <item>
      <title>WaferLLM: Large Language Model Inference at Wafer Scale</title>
      <link>https://arxiv.org/abs/2502.04563</link>
      <description>arXiv:2502.04563v3 Announce Type: replace-cross 
Abstract: Emerging AI accelerators increasingly adopt wafer-scale manufacturing technologies, integrating hundreds of thousands of AI cores in a mesh architecture with large distributed on-chip memory (tens of GB in total) and ultra-high on-chip memory bandwidth (tens of PB/s). However, current LLM inference systems, optimized for shared memory architectures like GPUs, fail to exploit these accelerators fully.
  We introduce WaferLLM, the first wafer-scale LLM inference system. WaferLLM is guided by a novel PLMR model (pronounced as "Plummer") that captures the unique hardware characteristics of wafer-scale architectures. Leveraging this model, WaferLLM pioneers wafer-scale LLM parallelism, optimizing the utilization of hundreds of thousands of on-chip cores. It also introduces MeshGEMM and MeshGEMV, the first GEMM and GEMV implementations designed to scale effectively on wafer-scale accelerators.
  Evaluations show that WaferLLM achieves up to 200$\times$ higher accelerator utilization than state-of-the-art methods. Leveraging a wafer-scale accelerator (Cerebras WSE2), WaferLLM delivers GEMV operations 606$\times$ faster and 16$\times$ more energy-efficient than on an NVIDIA A100 GPU. For full LLM inference, WaferLLM achieves 10-20$\times$ speedups over A100 GPU clusters running SGLang and vLLM. These advantages are expected to grow as wafer-scale AI models, software, and hardware continue to mature. WaferLLM is open-sourced at https://github.com/MeshInfra/WaferLLM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04563v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Congjie He, Yeqi Huang, Pei Mu, Ziming Miao, Jilong Xue, Lingxiao Ma, Fan Yang, Luo Mai</dc:creator>
    </item>
  </channel>
</rss>
