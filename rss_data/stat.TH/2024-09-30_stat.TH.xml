<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Oct 2024 03:13:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Consistency of variational Bayesian inference for non-linear inverse problems of partial differential equations</title>
      <link>https://arxiv.org/abs/2409.18415</link>
      <description>arXiv:2409.18415v1 Announce Type: new 
Abstract: We consider non-linear Bayesian inverse problems of determining the parameter $f$. For the posterior distribution with a class of Gaussian process priors, we study the statistical performance of variational Bayesian inference to the posterior with variational sets consisting of Gaussian measures or a mean-field family. We propose certain conditions on the forward map $\mathcal{G}$, the variational set $\mathcal{Q}$ and the prior such that, as the number $N$ of measurements increases, the resulting variational posterior distributions contract to the ground truth $f_0$ generating the data, and derive a convergence rate with polynomial order or logarithmic order. As specific examples, we consider a collection of non-linear inverse problems, including the Darcy flow problem, the inverse potential problem for a subdiffusion equation, and the inverse medium scattering problem. Besides, we show that our convergence rates are minimax optimal for these inverse problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18415v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaokang Zu, Junxiong Jia, Deyu Meng</dc:creator>
    </item>
    <item>
      <title>Adaptive inference with random ellipsoids through Conformal Conditional Linear Expectation</title>
      <link>https://arxiv.org/abs/2409.18508</link>
      <description>arXiv:2409.18508v1 Announce Type: new 
Abstract: We propose a new conformity score  for conformal prediction, in a general multivariate regression framework. The underlying score function is based on a covariance analysis of the residuals and the input points. We give theoretical guarantees on the prediction set. This set consists in an explicit ellipsoid that has a reduced volume compared to a classic ball. Further, we also study the asymptotic properties of the ellipsoid. Finally, we illustrate the effectiveness of  all our results on an in-depth numerical study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18508v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iain Henderson (IMT), Adrien Mazoyer (IMT), Fabrice Gamboa (IMT)</dc:creator>
    </item>
    <item>
      <title>A Unified View on Learning Unnormalized Distributions via Noise-Contrastive Estimation</title>
      <link>https://arxiv.org/abs/2409.18209</link>
      <description>arXiv:2409.18209v1 Announce Type: cross 
Abstract: This paper studies a family of estimators based on noise-contrastive estimation (NCE) for learning unnormalized distributions. The main contribution of this work is to provide a unified perspective on various methods for learning unnormalized distributions, which have been independently proposed and studied in separate research communities, through the lens of NCE. This unified view offers new insights into existing estimators. Specifically, for exponential families, we establish the finite-sample convergence rates of the proposed estimators under a set of regularity assumptions, most of which are new.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18209v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Jon Ryu, Abhin Shah, Gregory W. Wornell</dc:creator>
    </item>
    <item>
      <title>Renewal Processes Represented as Doubly Stochastic Poisson Processes</title>
      <link>https://arxiv.org/abs/2409.18362</link>
      <description>arXiv:2409.18362v1 Announce Type: cross 
Abstract: This paper gives an elementary proof for the following theorem: a renewal process can be represented by a doubly-stochastic Poisson process (DSPP) if and only if the Laplace-Stieltjes transform of the inter-arrival times is of the following form: $$\phi(\theta)=\lambda\left[\lambda+\theta+k\int_0^\infty\left(1-e^{-\theta z}\right)\,dG(z)\right]^{-1},$$ for some positive real numbers $\lambda, k$, and some distribution function $G$ with $G(\infty)=1$. The intensity process $\Lambda(t)$ of the corresponding DSPP jumps between $\lambda$ and $0$, with the time spent at $\lambda$ being independent random variables that are exponentially distributed with mean $1/k$, and the time spent at $0$ being independent random variables with distribution function $G$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18362v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinlong Du, Harsha Honnappa</dc:creator>
    </item>
    <item>
      <title>Moment varieties of the inverse Gaussian and gamma distributions are nondefective</title>
      <link>https://arxiv.org/abs/2409.18421</link>
      <description>arXiv:2409.18421v1 Announce Type: cross 
Abstract: We show that the parameters of a $k$-mixture of inverse Gaussian or gamma distributions are algebraically identifiable from the first $3k-1$ moments, and rationally identifiable from the first $3k+2$ moments. Our proofs are based on Terracini's classification of defective surfaces, careful analysis of the intersection theory of moment varieties, and a recent result on sufficient conditions for rational identifiability of secant varieties by Massarenti--Mella.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18421v1</guid>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oskar Henriksson, Kristian Ranestad, Lisa Seccia, Teresa Yu</dc:creator>
    </item>
    <item>
      <title>WHOMP: Optimizing Randomized Controlled Trials via Wasserstein Homogeneity</title>
      <link>https://arxiv.org/abs/2409.18504</link>
      <description>arXiv:2409.18504v1 Announce Type: cross 
Abstract: We investigate methods for partitioning datasets into subgroups that maximize diversity within each subgroup while minimizing dissimilarity across subgroups. We introduce a novel partitioning method called the $\textit{Wasserstein Homogeneity Partition}$ (WHOMP), which optimally minimizes type I and type II errors that often result from imbalanced group splitting or partitioning, commonly referred to as accidental bias, in comparative and controlled trials. We conduct an analytical comparison of WHOMP against existing partitioning methods, such as random subsampling, covariate-adaptive randomization, rerandomization, and anti-clustering, demonstrating its advantages. Moreover, we characterize the optimal solutions to the WHOMP problem and reveal an inherent trade-off between the stability of subgroup means and variances among these solutions. Based on our theoretical insights, we design algorithms that not only obtain these optimal solutions but also equip practitioners with tools to select the desired trade-off. Finally, we validate the effectiveness of WHOMP through numerical experiments, highlighting its superiority over traditional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18504v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shizhou Xu, Thomas Strohmer</dc:creator>
    </item>
    <item>
      <title>Convergence of Diffusion Models Under the Manifold Hypothesis in High-Dimensions</title>
      <link>https://arxiv.org/abs/2409.18804</link>
      <description>arXiv:2409.18804v1 Announce Type: cross 
Abstract: Denoising Diffusion Probabilistic Models (DDPM) are powerful state-of-the-art methods used to generate synthetic data from high-dimensional data distributions and are widely used for image, audio and video generation as well as many more applications in science and beyond. The manifold hypothesis states that high-dimensional data often lie on lower-dimensional manifolds within the ambient space, and is widely believed to hold in provided examples. While recent results has provided invaluable insight into how diffusion models adapt to the manifold hypothesis, they do not capture the great empirical success of these models, making this a very fruitful research direction.
  In this work, we study DDPMs under the manifold hypothesis and prove that they achieve rates independent of the ambient dimension in terms of learning the score. In terms of sampling, we obtain rates independent of the ambient dimension w.r.t. the Kullback-Leibler divergence, and $O(\sqrt{D})$ w.r.t. the Wasserstein distance. We do this by developing a new framework connecting diffusion models to the well-studied theory of extrema of Gaussian Processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18804v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iskander Azangulov, George Deligiannidis, Judith Rousseau</dc:creator>
    </item>
    <item>
      <title>Probabilistic Analysis of Least Squares, Orthogonal Projection, and QR Factorization Algorithms Subject to Gaussian Noise</title>
      <link>https://arxiv.org/abs/2409.18905</link>
      <description>arXiv:2409.18905v1 Announce Type: cross 
Abstract: In this paper, we extend the work of Liesen et al. (2002), which analyzes how the condition number of an orthonormal matrix Q changes when a column is added ([Q, c]), particularly focusing on the perpendicularity of c to the span of Q. Their result, presented in Theorem 2.3 of Liesen et al. (2002), assumes exact arithmetic and orthonormality of Q, which is a strong assumption when applying these results to numerical methods such as QR factorization algorithms. In our work, we address this gap by deriving bounds on the condition number increase for a matrix B without assuming perfect orthonormality, even when a column is not perfectly orthogonal to the span of B. This framework allows us to analyze QR factorization methods where orthogonalization is imperfect and subject to Gaussian noise. We also provide results on the performance of orthogonal projection and least squares under Gaussian noise, further supporting the development of this theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18905v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ali Lotfi, Julien Langou, Mohammad Meysami</dc:creator>
    </item>
    <item>
      <title>Inference with Sequential Monte-Carlo Computation of $p$-values: Fast and Valid Approaches</title>
      <link>https://arxiv.org/abs/2409.18908</link>
      <description>arXiv:2409.18908v1 Announce Type: cross 
Abstract: Hypothesis tests calibrated by (re)sampling methods (such as permutation, rank and bootstrap tests) are useful tools for statistical analysis, at the computational cost of requiring Monte-Carlo sampling for calibration. It is common and almost universal practice to execute such tests with predetermined and large number of Monte-Carlo samples, and disregard any randomness from this sampling at the time of drawing and reporting inference. At best, this approach leads to computational inefficiency, and at worst to invalid inference. That being said, a number of approaches in the literature have been proposed to adaptively guide analysts in choosing the number of Monte-Carlo samples, by sequentially deciding when to stop collecting samples and draw inference. These works introduce varying competing notions of what constitutes "valid" inference, complicating the landscape for analysts seeking suitable methodology. Furthermore, the majority of these approaches solely guarantee a meaningful estimate of the testing outcome, not the $p$-value itself $\unicode{x2014}$ which is insufficient for many practical applications. In this paper, we survey the relevant literature, and build bridges between the scattered validity notions, highlighting some of their complementary roles. We also introduce a new practical methodology that provides an estimate of the $p$-value of the Monte-Carlo test, endowed with practically relevant validity guarantees. Moreover, our methodology is sequential, updating the $p$-value estimate after each new Monte-Carlo sample has been drawn, while retaining important validity guarantees regardless of the selected stopping time. We conclude this paper with a set of recommendations for the practitioner, both in terms of selection of methodology and manner of reporting results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18908v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ivo V. Stoepker, Rui M. Castro</dc:creator>
    </item>
    <item>
      <title>$O(d/T)$ Convergence Theory for Diffusion Probabilistic Models under Minimal Assumptions</title>
      <link>https://arxiv.org/abs/2409.18959</link>
      <description>arXiv:2409.18959v1 Announce Type: cross 
Abstract: Score-based diffusion models, which generate new data by learning to reverse a diffusion process that perturbs data from the target distribution into noise, have achieved remarkable success across various generative tasks. Despite their superior empirical performance, existing theoretical guarantees are often constrained by stringent assumptions or suboptimal convergence rates. In this paper, we establish a fast convergence theory for a popular SDE-based sampler under minimal assumptions. Our analysis shows that, provided $\ell_{2}$-accurate estimates of the score functions, the total variation distance between the target and generated distributions is upper bounded by $O(d/T)$ (ignoring logarithmic factors), where $d$ is the data dimensionality and $T$ is the number of steps. This result holds for any target distribution with finite first-order moment. To our knowledge, this improves upon existing convergence theory for both the SDE-based sampler and another ODE-based sampler, while imposing minimal assumptions on the target data distribution and score estimates. This is achieved through a novel set of analytical tools that provides a fine-grained characterization of how the error propagates at each step of the reverse process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18959v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gen Li, Yuling Yan</dc:creator>
    </item>
    <item>
      <title>Optimal Differentially Private PCA and Estimation for Spiked Covariance Matrices</title>
      <link>https://arxiv.org/abs/2401.03820</link>
      <description>arXiv:2401.03820v2 Announce Type: replace 
Abstract: Estimating a covariance matrix and its associated principal components is a fundamental problem in contemporary statistics. While optimal estimation procedures have been developed with well-understood properties, the increasing demand for privacy preservation introduces new complexities to this classical problem. In this paper, we study optimal differentially private Principal Component Analysis (PCA) and covariance estimation within the spiked covariance model. We precisely characterize the sensitivity of eigenvalues and eigenvectors under this model and establish the minimax rates of convergence for estimating both the principal components and covariance matrix. These rates hold up to logarithmic factors and encompass general Schatten norms, including spectral norm, Frobenius norm, and nuclear norm as special cases. We propose computationally efficient differentially private estimators and prove their minimax optimality for sub-Gaussian distributions, up to logarithmic factors. Additionally, matching minimax lower bounds are established. Notably, compared to the existing literature, our results accommodate a diverging rank, a broader range of signal strengths, and remain valid even when the sample size is much smaller than the dimension, provided the signal strength is sufficiently strong. Both simulation studies and real data experiments demonstrate the merits of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03820v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>T. Tony Cai, Dong Xia, Mengyue Zha</dc:creator>
    </item>
    <item>
      <title>Policy learning "without" overlap: Pessimism and generalized empirical Bernstein's inequality</title>
      <link>https://arxiv.org/abs/2212.09900</link>
      <description>arXiv:2212.09900v3 Announce Type: replace-cross 
Abstract: This paper studies offline policy learning, which aims at utilizing observations collected a priori (from either fixed or adaptively evolving behavior policies) to learn an optimal individualized decision rule that achieves the best overall outcomes for a given population. Existing policy learning methods rely on a uniform overlap assumption, i.e., the propensities of exploring all actions for all individual characteristics must be lower bounded. As one has no control over the data collection process, this assumption can be unrealistic in many situations, especially when the behavior policies are allowed to evolve over time with diminishing propensities for certain actions.
  In this paper, we propose Pessimistic Policy Learning (PPL), a new algorithm that optimizes lower confidence bounds (LCBs) -- instead of point estimates -- of the policy values. The LCBs are constructed using knowledge of the behavior policies for collecting the offline data. Without assuming any uniform overlap condition, we establish a data-dependent upper bound for the suboptimality of our algorithm, which only depends on (i) the overlap for the optimal policy, and (ii) the complexity of the policy class we optimize over. As an implication, for adaptively collected data, we ensure efficient policy learning as long as the propensities for optimal actions are lower bounded over time, while those for suboptimal ones are allowed to diminish arbitrarily fast. In our theoretical analysis, we develop a new self-normalized type concentration inequality for inverse-propensity-weighting estimators, generalizing the well-known empirical Bernstein's inequality to unbounded and non-i.i.d. data. We complement our theory with an efficient optimization algorithm via Majorization-Minimization and policy tree search, as well as extensive simulation studies and real-world applications that demonstrate the efficacy of PPL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.09900v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Jin, Zhimei Ren, Zhuoran Yang, Zhaoran Wang</dc:creator>
    </item>
    <item>
      <title>Multilevel Metamodels: A Novel Approach to Enhance Efficiency and Generalizability in Monte Carlo Simulation Studies</title>
      <link>https://arxiv.org/abs/2401.07294</link>
      <description>arXiv:2401.07294v3 Announce Type: replace-cross 
Abstract: Metamodels, or the regression analysis of Monte Carlo simulation results, provide a powerful tool to summarize simulation findings. However, an underutilized approach is the multilevel metamodel (MLMM) that accounts for the dependent data structure that arises from fitting multiple models to the same simulated data set. In this study, we articulate the theoretical rationale for the MLMM and illustrate how it can improve the interpretability of simulation results, better account for complex simulation designs, and provide new insights into the generalizability of simulation findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07294v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Gilbert, Luke Miratrix</dc:creator>
    </item>
    <item>
      <title>Coherent forecasting of NoGeAR(1) model</title>
      <link>https://arxiv.org/abs/2403.00304</link>
      <description>arXiv:2403.00304v2 Announce Type: replace-cross 
Abstract: This article focuses on the coherent forecasting of the recently introduced novel geometric AR(1) (NoGeAR(1)) model - an INAR model based on inflated - parameter binomial thinning approach. Various techniques are available to achieve h - step ahead coherent forecasts of count time series, like median and mode forecasting. However, there needs to be more body of literature addressing coherent forecasting in the context of overdispersed count time series. Here, we study the forecasting distribution corresponding to NoGeAR(1) process using the Monte Carlo (MC) approximation method. Accordingly, several forecasting measures are employed in the simulation study to facilitate a thorough comparison of the forecasting capability of NoGeAR(1) with other models. The methodology is also demonstrated using real-life data, specifically the data on CW{\ss} TeXpert downloads and Barbados COVID-19 data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00304v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Divya Kuttenchalil Andrews, N. Balakrishna</dc:creator>
    </item>
    <item>
      <title>The Star Geometry of Critic-Based Regularizer Learning</title>
      <link>https://arxiv.org/abs/2408.16852</link>
      <description>arXiv:2408.16852v2 Announce Type: replace-cross 
Abstract: Variational regularization is a classical technique to solve statistical inference tasks and inverse problems, with modern data-driven approaches parameterizing regularizers via deep neural networks showcasing impressive empirical performance. Recent works along these lines learn task-dependent regularizers. This is done by integrating information about the measurements and ground-truth data in an unsupervised, critic-based loss function, where the regularizer attributes low values to likely data and high values to unlikely data. However, there is little theory about the structure of regularizers learned via this process and how it relates to the two data distributions. To make progress on this challenge, we initiate a study of optimizing critic-based loss functions to learn regularizers over a particular family of regularizers: gauges (or Minkowski functionals) of star-shaped bodies. This family contains regularizers that are commonly employed in practice and shares properties with regularizers parameterized by deep neural networks. We specifically investigate critic-based losses derived from variational representations of statistical distances between probability measures. By leveraging tools from star geometry and dual Brunn-Minkowski theory, we illustrate how these losses can be interpreted as dual mixed volumes that depend on the data distribution. This allows us to derive exact expressions for the optimal regularizer in certain cases. Finally, we identify which neural network architectures give rise to such star body gauges and when do such regularizers have favorable properties for optimization. More broadly, this work highlights how the tools of star geometry can aid in understanding the geometry of unsupervised regularizer learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16852v2</guid>
      <category>cs.LG</category>
      <category>math.MG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oscar Leong, Eliza O'Reilly, Yong Sheng Soh</dc:creator>
    </item>
  </channel>
</rss>
