<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Nov 2024 05:02:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>GraphGrad: Efficient Estimation of Sparse Polynomial Representations for General State-Space Models</title>
      <link>https://arxiv.org/abs/2411.15637</link>
      <description>arXiv:2411.15637v1 Announce Type: new 
Abstract: State-space models (SSMs) are a powerful statistical tool for modelling time-varying systems via a latent state. In these models, the latent state is never directly observed. Instead, a sequence of observations related to the state is available. The state-space model is defined by the state dynamics and the observation model, both of which are described by parametric distributions. Estimation of parameters of these distributions is a very challenging, but essential, task for performing inference and prediction. Furthermore, it is typical that not all states of the system interact. We can therefore encode the interaction of the states via a graph, usually not fully connected. However, most parameter estimation methods do not take advantage of this feature. In this work, we propose GraphGrad, a fully automatic approach for obtaining sparse estimates of the state interactions of a non-linear state-space model via a polynomial approximation. This novel methodology unveils the latent structure of the data-generating process, allowing us to infer both the structure and value of a rich and efficient parameterisation of a general state-space model. Our method utilises a differentiable particle filter to optimise a Monte Carlo likelihood estimator. It also promotes sparsity in the estimated system through the use of suitable proximity updates, known to be more efficient and stable than subgradient methods. As shown in our paper, a number of well-known dynamical systems can be accurately represented and recovered by our method, providing basis for application to real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15637v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Cox, Emilie Chouznoux, Victor Elvira</dc:creator>
    </item>
    <item>
      <title>Enhancing Computational Efficiency in State-Space Models Using Rao-Blackwellization and 2-Step Approximation</title>
      <link>https://arxiv.org/abs/2411.16056</link>
      <description>arXiv:2411.16056v1 Announce Type: new 
Abstract: This paper explores a Bayesian self-organization method for state-space models, enabling simultaneous state and parameter estimation without repeated likelihood calculations. While efficient for low-dimensional models, high-dimensional cases like seasonal adjustment require many particles. Using Rao-Blackwellization and a 2-step approximation, the method reduces particle use and computation time while maintaining accuracy, as shown in Monte Carlo evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16056v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Genshiro Kitagawa (Tokyo University of Marine Science,Technology,The Institute of Statistical Mathmatics)</dc:creator>
    </item>
    <item>
      <title>Sampling with Adaptive Variance for Multimodal Distributions</title>
      <link>https://arxiv.org/abs/2411.15220</link>
      <description>arXiv:2411.15220v1 Announce Type: cross 
Abstract: We propose and analyze a class of adaptive sampling algorithms for multimodal distributions on a bounded domain, which share a structural resemblance to the classic overdamped Langevin dynamics. We first demonstrate that this class of linear dynamics with adaptive diffusion coefficients and vector fields can be interpreted and analyzed as weighted Wasserstein gradient flows of the Kullback--Leibler (KL) divergence between the current distribution and the target Gibbs distribution, which directly leads to the exponential convergence of both the KL and $\chi^2$ divergences, with rates depending on the weighted Wasserstein metric and the Gibbs potential. We then show that a derivative-free version of the dynamics can be used for sampling without gradient information of the Gibbs potential and that for Gibbs distributions with nonconvex potentials, this approach could achieve significantly faster convergence than the classical overdamped Langevin dynamics. A comparison of the mean transition times between local minima of a nonconvex potential further highlights the better efficiency of the derivative-free dynamics in sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15220v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bj\"orn Engquist, Kui Ren, Yunan Yang</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis on Interaction Effects of Policy-Augmented Bayesian Networks</title>
      <link>https://arxiv.org/abs/2411.15566</link>
      <description>arXiv:2411.15566v1 Announce Type: cross 
Abstract: Biomanufacturing plays an important role in supporting public health and the growth of the bioeconomy. Modeling and studying the interaction effects among various input variables is very critical for obtaining a scientific understanding and process specification in biomanufacturing. In this paper, we use the ShapleyOwen indices to measure the interaction effects for the policy-augmented Bayesian network (PABN) model, which characterizes the risk- and science-based understanding of production bioprocess mechanisms. In order to facilitate efficient interaction effect quantification, we propose a sampling-based simulation estimation framework. In addition, to further improve the computational efficiency, we develop a non-nested simulation algorithm with sequential sampling, which can dynamically allocate the simulation budget to the interactions with high uncertainty and therefore estimate the interaction effects more accurately under a total fixed budget setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15566v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junkai Zhao, Jun Luo, Wei Xie, Zixuan Bai</dc:creator>
    </item>
    <item>
      <title>Learning state and proposal dynamics in state-space models using differentiable particle filters and neural networks</title>
      <link>https://arxiv.org/abs/2411.15638</link>
      <description>arXiv:2411.15638v1 Announce Type: cross 
Abstract: State-space models are a popular statistical framework for analysing sequential data. Within this framework, particle filters are often used to perform inference on non-linear state-space models. We introduce a new method, StateMixNN, that uses a pair of neural networks to learn the proposal distribution and transition distribution of a particle filter. Both distributions are approximated using multivariate Gaussian mixtures. The component means and covariances of these mixtures are learnt as outputs of learned functions. Our method is trained targeting the log-likelihood, thereby requiring only the observation series, and combines the interpretability of state-space models with the flexibility and approximation power of artificial neural networks. The proposed method significantly improves recovery of the hidden state in comparison with the state-of-the-art, showing greater improvement in highly non-linear scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15638v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Cox, Santiago Segarra, Victor Elvira</dc:creator>
    </item>
    <item>
      <title>Posterior risk of modular and semi-modular Bayesian inference</title>
      <link>https://arxiv.org/abs/2301.10911</link>
      <description>arXiv:2301.10911v3 Announce Type: replace-cross 
Abstract: Modular Bayesian methods perform inference in models that are specified through a collection of coupled sub-models, known as modules. These modules often arise from modelling different data sources or from combining domain knowledge from different disciplines. ``Cutting feedback'' is a Bayesian inference method that ensures misspecification of one module does not affect inferences for parameters in other modules, and produces what is known as the cut posterior. However, choosing between the cut posterior and the standard Bayesian posterior is challenging. When misspecification is not severe, cutting feedback can greatly increase posterior uncertainty without a large reduction of estimation bias, leading to a bias-variance trade-off. This trade-off motivates semi-modular posteriors, which interpolate between standard and cut posteriors based on a tuning parameter. In this work, we provide the first precise formulation of the bias-variance trade-off that is present in cutting feedback, and we propose a new semi-modular posterior that takes advantage of it. Under general regularity conditions, we prove that this semi-modular posterior is more accurate than the cut posterior according to a notion of posterior risk. An important implication of this result is that point inferences made under the cut posterior are inadmissable. The new method is demonstrated in a number of examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.10911v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David T. Frazier, David J. Nott</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian Model Averaging for Linear Regression Models With Heavy-Tailed Errors</title>
      <link>https://arxiv.org/abs/2407.16366</link>
      <description>arXiv:2407.16366v2 Announce Type: replace-cross 
Abstract: Our goal is to develop a Bayesian model averaging technique in linear regression models that accommodates heavier tailed error densities than the normal distribution. Motivated by the use of the Huber loss function in the presence of outliers, the Bayesian Huberized lasso with hyperbolic errors has been proposed and recently implemented in the literature (Park and Casella (2008); Kawakami and Hashimoto (2023)). Since the Huberized lasso cannot enforce regression coefficients to be exactly zero, we propose a fully Bayesian variable selection approach with spike and slab priors to address sparsity more effectively. The shapes of the hyperbolic and the Student-t density functions are different. Furthermore, the tails of a hyperbolic distribution are less heavy compared to those of a Cauchy distribution. Thus, we propose a flexible regression model with an error distribution encompassing both the hyperbolic and the Student-t family of distributions, along with an unknown tail heaviness parameter, that is estimated based on the data. It is known that the limiting form of both the hyperbolic and the Student-t distributions is a normal distribution. We develop an efficient Gibbs sampler with Metropolis Hastings steps for posterior computation. Through simulation studies and analyses of real datasets, we show that our method is competitive with various state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16366v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shamriddha De, Joyee Ghosh</dc:creator>
    </item>
    <item>
      <title>Estimation of Spatiotemporal Poisson Processes with Some Missing Location Data</title>
      <link>https://arxiv.org/abs/2410.11103</link>
      <description>arXiv:2410.11103v3 Announce Type: replace-cross 
Abstract: We consider models for spatiotemporal Poisson processes with some missing location data. We discuss four models that make provision for missing location data, and their estimation. The corresponding code is available on GitHub as an extension of LASPATED at https://github.com/vguigues/LASPATED/Missing_Data. We tested our models using the process of emergency call arrivals to an emergency medical service where the emergency reports often omit the location of the emergency. We show the difference made by using models that make provision for missing location data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11103v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Guigues, Anton Kleywegt, Victor Hugo Nascimento, Lucas Lucas Rafael de Andrade</dc:creator>
    </item>
    <item>
      <title>Summarizing Bayesian Nonparametric Mixture Posterior -- Sliced Optimal Transport Metrics for Gaussian Mixtures</title>
      <link>https://arxiv.org/abs/2411.14674</link>
      <description>arXiv:2411.14674v2 Announce Type: replace-cross 
Abstract: Existing methods to summarize posterior inference for mixture models focus on identifying a point estimate of the implied random partition for clustering, with density estimation as a secondary goal (Wade and Ghahramani, 2018; Dahl et al., 2022). We propose a novel approach for summarizing posterior inference in nonparametric Bayesian mixture models, prioritizing density estimation of the mixing measure (or mixture) as an inference target. One of the key features is the model-agnostic nature of the approach, which remains valid under arbitrarily complex dependence structures in the underlying sampling model. Using a decision-theoretic framework, our method identifies a point estimate by minimizing posterior expected loss. A loss function is defined as a discrepancy between mixing measures. Estimating the mixing measure implies inference on the mixture density and the random partition. Exploiting the discrete nature of the mixing measure, we use a version of sliced Wasserstein distance. We introduce two specific variants for Gaussian mixtures. The first, mixed sliced Wasserstein, applies generalized geodesic projections on the product of the Euclidean space and the manifold of symmetric positive definite matrices. The second, sliced mixture Wasserstein, leverages the linearity of Gaussian mixture measures for efficient projection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14674v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen, Peter Mueller</dc:creator>
    </item>
  </channel>
</rss>
