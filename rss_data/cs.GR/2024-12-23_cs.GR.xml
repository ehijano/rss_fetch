<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Dec 2024 15:30:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>DreaMark: Rooting Watermark in Score Distillation Sampling Generated Neural Radiance Fields</title>
      <link>https://arxiv.org/abs/2412.15278</link>
      <description>arXiv:2412.15278v1 Announce Type: new 
Abstract: Recent advancements in text-to-3D generation can generate neural radiance fields (NeRFs) with score distillation sampling, enabling 3D asset creation without real-world data capture. With the rapid advancement in NeRF generation quality, protecting the copyright of the generated NeRF has become increasingly important. While prior works can watermark NeRFs in a post-generation way, they suffer from two vulnerabilities. First, a delay lies between NeRF generation and watermarking because the secret message is embedded into the NeRF model post-generation through fine-tuning. Second, generating a non-watermarked NeRF as an intermediate creates a potential vulnerability for theft. To address both issues, we propose Dreamark to embed a secret message by backdooring the NeRF during NeRF generation. In detail, we first pre-train a watermark decoder. Then, the Dreamark generates backdoored NeRFs in a way that the target secret message can be verified by the pre-trained watermark decoder on an arbitrary trigger viewport. We evaluate the generation quality and watermark robustness against image- and model-level attacks. Extensive experiments show that the watermarking process will not degrade the generation quality, and the watermark achieves 90+% accuracy among both image-level attacks (e.g., Gaussian noise) and model-level attacks (e.g., pruning attack).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15278v1</guid>
      <category>cs.GR</category>
      <category>cs.AI</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xingyu Zhu, Xiapu Luo, Xuetao Wei</dc:creator>
    </item>
    <item>
      <title>Robust and Feature-Preserving Offset Meshing</title>
      <link>https://arxiv.org/abs/2412.15564</link>
      <description>arXiv:2412.15564v1 Announce Type: new 
Abstract: We introduce a novel offset meshing approach that can robustly handle a 3D surface mesh with an arbitrary geometry and topology configurations, while nicely capturing the sharp features on the original input for both inward and outward offsets. Compared to the existing approaches focusing on constant-radius offset, to the best of our knowledge, we propose the first-ever solution for mitered offset that can well preserve sharp features. Our method is designed based on several core principals: 1) explicitly generating the offset vertices and triangles with feature-capturing energy and constraints; 2) prioritizing the generation of the offset geometry before establishing its connectivity, 3) employing exact algorithms in critical pipeline steps for robustness, balancing the use of floating-point computations for efficiency, 4) applying various conservative speed up strategies including early reject non-contributing computations to the final output. Our approach further uniquely supports variable offset distances on input surface elements, offering a wider range practical applications compared to conventional methods.
  We have evaluated our method on a subset of Thinkgi10K, containing models with diverse topological and geometric complexities created by practitioners in various fields. Our results demonstrate the superiority of our approach over current state-of-the-art methods in terms of element count, feature preservation, and non-uniform offset distances of the resulting offset mesh surfaces, marking a significant advancement in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15564v1</guid>
      <category>cs.GR</category>
      <category>cs.CG</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongyi Cao, Gang Xu, Renshu Gu, Jinlan Xu, Xiaoyu Zhang, Timon Rabczuk, Yuzhe Luo, Xifeng Gao</dc:creator>
    </item>
    <item>
      <title>3D Shape Tokenization</title>
      <link>https://arxiv.org/abs/2412.15618</link>
      <description>arXiv:2412.15618v1 Announce Type: cross 
Abstract: We introduce Shape Tokens, a 3D representation that is continuous, compact, and easy to incorporate into machine learning models. Shape Tokens act as conditioning vectors that represent shape information in a 3D flow-matching model. The flow-matching model is trained to approximate probability density functions corresponding to delta functions concentrated on the surfaces of shapes in 3D. By attaching Shape Tokens to various machine learning models, we can generate new shapes, convert images to 3D, align 3D shapes with text and images, and render shapes directly at variable, user specified, resolution. Moreover, Shape Tokens enable a systematic analysis of geometric properties such as normal, density, and deformation field. Across all tasks and experiments, utilizing Shape Tokens demonstrate strong performance compared to existing baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15618v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jen-Hao Rick Chang, Yuyang Wang, Miguel Angel Bautista Martin, Jiatao Gu, Josh Susskind, Oncel Tuzel</dc:creator>
    </item>
    <item>
      <title>AI-in-the-loop: The future of biomedical visual analytics applications in the era of AI</title>
      <link>https://arxiv.org/abs/2412.15876</link>
      <description>arXiv:2412.15876v1 Announce Type: cross 
Abstract: AI is the workhorse of modern data analytics and omnipresent across many sectors. Large Language Models and multi-modal foundation models are today capable of generating code, charts, visualizations, etc. How will these massive developments of AI in data analytics shape future data visualizations and visual analytics workflows? What is the potential of AI to reshape methodology and design of future visual analytics applications? What will be our role as visualization researchers in the future? What are opportunities, open challenges and threats in the context of an increasingly powerful AI? This Visualization Viewpoint discusses these questions in the special context of biomedical data analytics as an example of a domain in which critical decisions are taken based on complex and sensitive data, with high requirements on transparency, efficiency, and reliability. We map recent trends and developments in AI on the elements of interactive visualization and visual analytics workflows and highlight the potential of AI to transform biomedical visualization as a research field. Given that agency and responsibility have to remain with human experts, we argue that it is helpful to keep the focus on human-centered workflows, and to use visual analytics as a tool for integrating ``AI-in-the-loop''. This is in contrast to the more traditional term ``human-in-the-loop'', which focuses on incorporating human expertise into AI-based systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15876v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katja B\"uhler, Thomas H\"ollt, Thomas Schulz, Pere-Pau V\'azquez</dc:creator>
    </item>
    <item>
      <title>CCNDF: Curvature Constrained Neural Distance Fields from 3D LiDAR Sequences</title>
      <link>https://arxiv.org/abs/2412.15909</link>
      <description>arXiv:2412.15909v1 Announce Type: cross 
Abstract: Neural distance fields (NDF) have emerged as a powerful tool for addressing challenges in 3D computer vision and graphics downstream problems. While significant progress has been made to learn NDF from various kind of sensor data, a crucial aspect that demands attention is the supervision of neural fields during training as the ground-truth NDFs are not available for large-scale outdoor scenes. Previous works have utilized various forms of expected signed distance to guide model learning. Yet, these approaches often need to pay more attention to critical considerations of surface geometry and are limited to small-scale implementations. To this end, we propose a novel methodology leveraging second-order derivatives of the signed distance field for improved neural field learning. Our approach addresses limitations by accurately estimating signed distance, offering a more comprehensive understanding of underlying geometry. To assess the efficacy of our methodology, we conducted comparative evaluations against prevalent methods for mapping and localization tasks, which are primary application areas of NDF. Our results demonstrate the superiority of the proposed approach, highlighting its potential for advancing the capabilities of neural distance fields in computer vision and graphics applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15909v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-981-96-0969-7_14</arxiv:DOI>
      <dc:creator>Akshit Singh, Karan Bhakuni, Rajendra Nagar</dc:creator>
    </item>
    <item>
      <title>Immersive In Situ Visualizations for Monitoring Architectural-Scale Multiuser MR Experiences</title>
      <link>https://arxiv.org/abs/2412.15918</link>
      <description>arXiv:2412.15918v1 Announce Type: cross 
Abstract: Mixed reality (MR) environments provide great value in displaying 3D virtual content. Systems facilitating co-located multiuser MR (Co-MUMR) experiences allow multiple users to co-present in a shared immersive virtual environment with natural locomotion. They can be used to support a broad spectrum of applications such as immersive presentations, public exhibitions, psychological experiments, etc. However, based on our experiences in delivering Co-MUMR experiences in large architectures and our reflections, we noticed that the crucial challenge for hosts to ensure the quality of experience is their lack of insight into the real-time information regarding visitor engagement, device performance, and system events. This work facilitates the display of such information by introducing immersive in situ visualizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15918v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhongyuan Yu, Daniel Zeidler, Krishnan Chandran, Lars Engeln, Kelsang Mende, Matthew McGinity</dc:creator>
    </item>
    <item>
      <title>Shape Shifters: Does Body Shape Change the Perception of Small-Scale Crowd Motions?</title>
      <link>https://arxiv.org/abs/2412.16151</link>
      <description>arXiv:2412.16151v1 Announce Type: cross 
Abstract: The animation of realistic virtual avatars in crowd scenarios is an important element of immersive virtual environments. However, achieving this realism requires attention to multiple factors, such as their visual appearance and motion cues. We investigated how body shape diversity influences the perception of motion clones in virtual crowds. A physics-based model was used to simulate virtual avatars in a small-scale crowd of size twelve. Participants viewed side-by-side video clips of these virtual crowds: one featuring all unique motions (Baseline) and the other containing motion clones (i.e., the same motion used to animate two or more avatars in the crowd). We also varied the levels of body shape and motion diversity. Our findings revealed that body shape diversity did not influence participants' ratings of motion clone detection, and motion variety had a greater impact on their perception of the crowd. Further research is needed to investigate how other visual factors interact with motion in order to enhance the perception of virtual crowd realism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16151v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bharat Vyas, Carol O'Sullivan</dc:creator>
    </item>
    <item>
      <title>Thunderscapes: Simulating the Dynamics of Mesoscale Convective System</title>
      <link>https://arxiv.org/abs/2412.00703</link>
      <description>arXiv:2412.00703v3 Announce Type: replace-cross 
Abstract: A Mesoscale Convective System (MCS) is a collection of thunderstorms that function as a system, representing a widely discussed phenomenon in both the natural sciences and visual effects industries, and embodying the untamed forces of nature.In this paper, we present the first interactive, physically inspired mesoscale thunderstorms simulation model that integrates Grabowski-style cloud microphysics with atmospheric electrification processes. Our model simulates thunderclouds development and lightning flashes within a unified meteorological framework, providing a realistic and interactive approach for graphical applications. By incorporating key physical principles, it effectively links cloud formation, electrification, and lightning generation. The simulation also encompasses various thunderstorm types and their corresponding lightning activities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00703v3</guid>
      <category>physics.flu-dyn</category>
      <category>cs.GR</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianchen Hao</dc:creator>
    </item>
  </channel>
</rss>
