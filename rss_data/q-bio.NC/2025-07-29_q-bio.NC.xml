<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Jul 2025 04:00:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Make Silence Speak for Itself: a multi-modal learning analytic approach with neurophysiological data</title>
      <link>https://arxiv.org/abs/2507.21063</link>
      <description>arXiv:2507.21063v1 Announce Type: new 
Abstract: Background: Silence is a common phenomenon in classrooms, yet its implicit nature limits a clear understanding of students' underlying learning statuses. Aim: This study proposed a nuanced framework to classify classroom silence based on class events and student status, and examined neurophysiological markers to reveal similarities and differences in silent states across achievement groups. Sample: The study involved 54 middle school students during 34 math lessons, with simultaneous recordings of electroencephalogram (EEG), electrodermal activity (EDA), and heart rate signals, alongside video coding of classroom behaviors. Results: We found that high-achieving students showed no significant difference in mean EDA features between strategic silence (i.e., students choose silence deliberately) and active speaking during open questioning but exhibited higher EEG high-frequency relative power spectral density (RPSD) during strategic silence. In structural silence (i.e., students maintain silence following an external command) during directed questioning, they demonstrated significantly higher heart rates while listening to lectures compared to group activities, indicating heightened engagement. Both high- and medium-achieving students displayed elevated heart rates and EDA tonic components in structural silence during questioning compared to teaching. Furthermore, high-achieving students exhibited lower high-frequency RPSD during structural silence than strategic silence, a pattern not observed in other groups, highlighting group heterogeneity. Conclusions: The findings contribute to validating the complexity of silence, challenge its traditional association with passivity, and offer a novel classification framework along with preliminary empirical evidence to deepen the understanding of silent learning behaviors in classroom contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21063v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingxuan Gao, Jingjing Chen, Yun Long, Xiaomeng Xu, Yu Zhang</dc:creator>
    </item>
    <item>
      <title>Gender Similarities Dominate Mathematical Cognition at the Neural Level: A Japanese fMRI Study Using Advanced Wavelet Analysis and Generative AI</title>
      <link>https://arxiv.org/abs/2507.21140</link>
      <description>arXiv:2507.21140v1 Announce Type: new 
Abstract: Recent large scale behavioral studies suggest early emergence of gender differences in mathematical performance within months of school entry. However, these findings lack direct neural evidence and are constrained by cultural contexts. We conducted functional magnetic resonance imaging (fMRI) during mathematical tasks in Japanese participants (N = 156), employing an advanced wavelet time frequency analysis to examine dynamic brain processes rather than static activation patterns. Wavelet decomposition across four frequency bands (0.01-0.25 Hz) revealed that neural processing mechanisms underlying mathematical cognition are fundamentally similar between genders. Time frequency analysis demonstrated 89.1% similarity in dynamic activation patterns (p = 0.734, d = 0.05), with identical temporal sequences and frequency profiles during mathematical processing. Individual variation in neural dynamics exceeded group differences by 3.2:1 (p $&lt;$ 0.001). Machine learning classifiers achieved only 53.8% accuracy in distinguishing gender based neural patterns essentially at chance level even when analyzing sophisticated temporal spectral features. Cross frequency coupling analysis revealed similar network coordination patterns between genders, indicating shared fundamental cognitive architecture. These findings provide robust process level neural evidence that gender similarities dominate mathematical cognition, particularly in early developmental stages, challenging recent claims of inherent differences and demonstrating that dynamic brain analysis reveals neural mechanisms that static behavioral assessments cannot access.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21140v1</guid>
      <category>q-bio.NC</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Representations in vision and language converge in a shared, multidimensional space of perceived similarities</title>
      <link>https://arxiv.org/abs/2507.21871</link>
      <description>arXiv:2507.21871v1 Announce Type: new 
Abstract: Humans can effortlessly describe what they see, yet establishing a shared representational format between vision and language remains a significant challenge. Emerging evidence suggests that human brain representations in both vision and language are well predicted by semantic feature spaces obtained from large language models (LLMs). This raises the possibility that sensory systems converge in their inherent ability to transform their inputs onto shared, embedding-like representational space. However, it remains unclear how such a space manifests in human behaviour. To investigate this, sixty-three participants performed behavioural similarity judgements separately on 100 natural scene images and 100 corresponding sentence captions from the Natural Scenes Dataset. We found that visual and linguistic similarity judgements not only converge at the behavioural level but also predict a remarkably similar network of fMRI brain responses evoked by viewing the natural scene images. Furthermore, computational models trained to map images onto LLM-embeddings outperformed both category-trained and AlexNet controls in explaining the behavioural similarity structure. These findings demonstrate that human visual and linguistic similarity judgements are grounded in a shared, modality-agnostic representational structure that mirrors how the visual system encodes experience. The convergence between sensory and artificial systems suggests a common capacity of how conceptual representations are formed-not as arbitrary products of first order, modality-specific input, but as structured representations that reflect the stable, relational properties of the external world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21871v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katerina Marie Simkova, Adrien Doerig, Clayton Hickey, Ian Charest</dc:creator>
    </item>
    <item>
      <title>Hebbian Memory-Augmented Recurrent Networks: Engram Neurons in Deep Learning</title>
      <link>https://arxiv.org/abs/2507.21474</link>
      <description>arXiv:2507.21474v1 Announce Type: cross 
Abstract: Despite success across diverse tasks, current artificial recurrent network architectures rely primarily on implicit hidden-state memories, limiting their interpretability and ability to model long-range dependencies. In contrast, biological neural systems employ explicit, associative memory traces (i.e., engrams) strengthened through Hebbian synaptic plasticity and activated sparsely during recall. Motivated by these neurobiological insights, we introduce the Engram Neural Network (ENN), a novel recurrent architecture incorporating an explicit, differentiable memory matrix with Hebbian plasticity and sparse, attention-driven retrieval mechanisms. The ENN explicitly models memory formation and recall through dynamic Hebbian traces, improving transparency and interpretability compared to conventional RNN variants. We evaluate the ENN architecture on three canonical benchmarks: MNIST digit classification, CIFAR-10 image sequence modeling, and WikiText-103 language modeling. Our empirical results demonstrate that the ENN achieves accuracy and generalization performance broadly comparable to classical RNN, GRU, and LSTM architectures, with all models converging to similar accuracy and perplexity on the large-scale WikiText-103 task. At the same time, the ENN offers significant enhancements in interpretability through observable memory dynamics. Hebbian trace visualizations further reveal biologically plausible, structured memory formation processes, validating the potential of neuroscience-inspired mechanisms to inform the development of more interpretable and robust deep learning models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21474v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Szelogowski</dc:creator>
    </item>
    <item>
      <title>Signed Higher-Order Interactions for Brain Disorder Diagnosis via Multi-Channel Transformers</title>
      <link>https://arxiv.org/abs/2507.20205</link>
      <description>arXiv:2507.20205v2 Announce Type: replace 
Abstract: Accurately characterizing higher-order interactions of brain regions and extracting interpretable organizational patterns from Functional Magnetic Resonance Imaging data is crucial for brain disease diagnosis. Current graph-based deep learning models primarily focus on pairwise or triadic patterns while neglecting signed higher-order interactions, limiting comprehensive understanding of brain-wide communication. We propose HOI-Brain, a novel computational framework leveraging signed higher-order interactions and organizational patterns in fMRI data for brain disease diagnosis. First, we introduce a co-fluctuation measure based on Multiplication of Temporal Derivatives to detect higher-order interactions with temporal resolution. We then distinguish positive and negative synergistic interactions, encoding them in signed weighted simplicial complexes to reveal brain communication insights. Using Persistent Homology theory, we apply two filtration processes to these complexes to extract signed higher-dimensional neural organizations spatiotemporally. Finally, we propose a multi-channel brain Transformer to integrate heterogeneous topological features. Experiments on Alzheimer' s disease, Parkinson' s syndrome, and autism spectrum disorder datasets demonstrate our framework' s superiority, effectiveness, and interpretability. The identified key brain regions and higher-order patterns align with neuroscience literature, providing meaningful biological insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20205v2</guid>
      <category>q-bio.NC</category>
      <category>cs.GR</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dengyi Zhao, Zhiheng Zhou, Guiying Yan, Dongxiao Yu, Xingqin Qi</dc:creator>
    </item>
  </channel>
</rss>
