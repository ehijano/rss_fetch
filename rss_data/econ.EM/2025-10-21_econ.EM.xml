<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Oct 2025 01:54:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Prediction Intervals for Model Averaging</title>
      <link>https://arxiv.org/abs/2510.16224</link>
      <description>arXiv:2510.16224v1 Announce Type: new 
Abstract: A rich set of frequentist model averaging methods has been developed, but their applications have largely been limited to point prediction, as measuring prediction uncertainty in general settings remains an open problem. In this paper we propose prediction intervals for model averaging based on conformal inference. These intervals cover out-of-sample realizations of the outcome variable with a pre-specified probability, providing a way to assess predictive uncertainty beyond point prediction. The framework allows general model misspecification and applies to averaging across multiple models that can be nested, disjoint, overlapping, or any combination thereof, with weights that may depend on the estimation sample. We establish coverage guarantees under two sets of assumptions: exact finite-sample validity under exchangeability, relevant for cross-sectional data, and asymptotic validity under stationarity, relevant for time-series data. We first present a benchmark algorithm and then introduce a locally adaptive refinement and split-sample procedures that broaden applicability. The methods are illustrated with a cross-sectional application to real estate appraisal and a time-series application to equity premium forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16224v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhongjun Qu, Wendun Wang, Xiaomeng Zhang</dc:creator>
    </item>
    <item>
      <title>On the Asymptotics of the Minimax Linear Estimator</title>
      <link>https://arxiv.org/abs/2510.16661</link>
      <description>arXiv:2510.16661v1 Announce Type: new 
Abstract: Many causal estimands, such as average treatment effects under unconfoundedness, can be written as continuous linear functionals of an unknown regression function. We study a weighting estimator that sets weights by a minimax procedure: solving a convex optimization problem that trades off worst-case conditional bias against variance. Despite its growing use, general root-$n$ theory for this method has been limited. This paper fills that gap. Under regularity conditions, we show that the minimax linear estimator is root-$n$ consistent and asymptotically normal, and we derive its asymptotic variance. These results justify ignoring worst-case bias when forming large-sample confidence intervals and make inference less sensitive to the scaling of the function class. With a mild variance condition, the estimator attains the semiparametric efficiency bound, so an augmentation step commonly used in the literature is not needed to achieve first-order optimality. Evidence from simulations and three empirical applications, including job-training and minimum-wage policies, points to a simple rule: in designs satisfying our regularity conditions, standard-error confidence intervals suffice; otherwise, bias-aware intervals remain important.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16661v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Kong</dc:creator>
    </item>
    <item>
      <title>Causal Inference in High-Dimensional Generalized Linear Models with Binary Outcomes</title>
      <link>https://arxiv.org/abs/2510.16669</link>
      <description>arXiv:2510.16669v1 Announce Type: new 
Abstract: This paper proposes a debiased estimator for causal effects in high-dimensional generalized linear models with binary outcomes and general link functions. The estimator augments a regularized regression plug-in with weights computed from a convex optimization problem that approximately balances link-derivative-weighted covariates and controls variance; it does not rely on estimated propensity scores. Under standard conditions, the estimator is $\sqrt{n}$-consistent and asymptotically normal for dense linear contrasts and causal parameters. Simulation results show the superior performance of our approach in comparison to alternatives such as inverse propensity score estimators and double machine learning estimators in finite samples. In an application to the National Supported Work training data, our estimates and confidence intervals are close to the experimental benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16669v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Kong</dc:creator>
    </item>
    <item>
      <title>On Quantile Treatment Effects, Rank Similarity,and Variation of Instrumental Variables</title>
      <link>https://arxiv.org/abs/2510.16681</link>
      <description>arXiv:2510.16681v1 Announce Type: new 
Abstract: This paper develops a nonparametric framework to identify and estimate distributional treatment effects under nonseparable endogeneity. We begin by revisiting the widely adopted \emph{rank similarity} (RS) assumption and characterizing it by the relationship it imposes between observed and counterfactual potential outcome distributions. The characterization highlights the restrictiveness of RS, motivating a weaker identifying condition. Under this alternative, we construct identifying bounds on the distributional treatment effects of interest through a linear semi-infinite programming (SILP) formulation. Our identification strategy also clarifies how richer exogenous instrument variation, such as multi-valued or multiple instruments, can further tighten these bounds. Finally, exploiting the SILP's saddle-point structure and Karush-Kuhn-Tucker (KKT) conditions, we establish large-sample properties for the empirical SILP: consistency and asymptotic distribution results for the estimated bounds and associated solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16681v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sukjin Han, Haiqing Xu</dc:creator>
    </item>
    <item>
      <title>Local Overidentification and Efficiency Gains in Modern Causal Inference and Data Combination</title>
      <link>https://arxiv.org/abs/2510.16683</link>
      <description>arXiv:2510.16683v1 Announce Type: new 
Abstract: This paper studies nonparametric local (over-)identification, in the sense of Chen and Santos (2018), and the associated semiparametric efficiency in modern causal frameworks. We develop a unified approach that begins by translating structural models with latent variables into their induced statistical models of observables and then analyzes local overidentification through conditional moment restrictions. We apply this approach to three leading models: (i) the general treatment model under unconfoundedness, (ii) the negative control model, and (iii) the long-term causal inference model under unobserved confounding. The first design yields a locally just-identified statistical model, implying that all regular asymptotically linear estimators of the treatment effect share the same asymptotic variance, equal to the (trivial) semiparametric efficiency bound. In contrast, the latter two models involve nonparametric endogeneity and are naturally locally overidentified; consequently, some doubly robust orthogonal moment estimators of the average treatment effect are inefficient. Whereas existing work typically imposes strong conditions to restore just-identification before deriving the efficiency bound, we relax such assumptions and characterize the general efficiency bound, along with efficient estimators, in the overidentified models (ii) and (iii).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16683v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaohong Chen, Haitian Xie</dc:creator>
    </item>
    <item>
      <title>Equilibrium-Constrained Estimation of Recursive Logit Choice Models</title>
      <link>https://arxiv.org/abs/2510.16886</link>
      <description>arXiv:2510.16886v1 Announce Type: new 
Abstract: The recursive logit (RL) model provides a flexible framework for modeling sequential decision-making in transportation and choice networks, with important applications in route choice analysis, multiple discrete choice problems, and activity-based travel demand modeling. Despite its versatility, estimation of the RL model typically relies on nested fixed-point (NFXP) algorithms that are computationally expensive and prone to numerical instability. We propose a new approach that reformulates the maximum likelihood estimation problem as an optimization problem with equilibrium constraints, where both the structural parameters and the value functions are treated as decision variables. We further show that this formulation can be equivalently transformed into a conic optimization problem with exponential cones, enabling efficient solution using modern conic solvers such as MOSEK. Experiments on synthetic and real-world datasets demonstrate that our convex reformulation achieves accuracy comparable to traditional methods while offering significant improvements in computational stability and efficiency, thereby providing a practical and scalable alternative for recursive logit model estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16886v1</guid>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hung Tran, Tien Mai, Minh Hoang Ha</dc:creator>
    </item>
    <item>
      <title>Mixed LR-$C(\alpha)$-type tests for irregular hypotheses, general criterion functions and misspecified models</title>
      <link>https://arxiv.org/abs/2510.17070</link>
      <description>arXiv:2510.17070v1 Announce Type: new 
Abstract: This paper introduces a likelihood ratio (LR)-type test that possesses the robustness properties of \(C(\alpha)\)-type procedures in an extremum estimation setting.
  The test statistic is constructed by applying separate adjustments to the restricted and unrestricted criterion functions, and is shown to be asymptotically pivotal under minimal conditions. It features two main robustness properties. First, unlike standard LR-type statistics, its null asymptotic distribution remains chi-square even under model misspecification, where the information matrix equality fails. Second, it accommodates irregular hypotheses involving constrained parameter spaces, such as boundary parameters, relying solely on root-\(n\)-consistent estimators for nuisance parameters. When the model is correctly specified, no boundary constraints are present, and parameters are estimated by extremum estimators, the proposed test reduces to the standard LR-type statistic.
  Simulations with ARCH models, where volatility parameters are constrained to be nonnegative, and parametric survival regressions with potentially monotone increasing hazard functions, demonstrate that our test maintains accurate size and exhibits good power. An empirical application to a two-way error components model shows that the proposed test can provide more informative inference than the conventional \(t\)-test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17070v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean-Marie Dufour, Purevdorj Tuvaandorj</dc:creator>
    </item>
    <item>
      <title>From Reviews to Actionable Insights: An LLM-Based Approach for Attribute and Feature Extraction</title>
      <link>https://arxiv.org/abs/2510.16551</link>
      <description>arXiv:2510.16551v2 Announce Type: cross 
Abstract: This research proposes a systematic, large language model (LLM) approach for extracting product and service attributes, features, and associated sentiments from customer reviews. Grounded in marketing theory, the framework distinguishes perceptual attributes from actionable features, producing interpretable and managerially actionable insights. We apply the methodology to 20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a random subset of reviews. Model performance is assessed through agreement with human annotations and predictive validity for customer ratings. Results show high consistency between LLMs and human coders and strong predictive validity, confirming the reliability of the approach. Human coders required a median of six minutes per review, whereas the LLM processed each in two seconds, delivering comparable insights at a scale unattainable through manual coding. Managerially, the analysis identifies attributes and features that most strongly influence customer satisfaction and their associated sentiments, enabling firms to pinpoint "joy points," address "pain points," and design targeted interventions. We demonstrate how structured review data can power an actionable marketing dashboard that tracks sentiment over time and across stores, benchmarks performance, and highlights high-leverage features for improvement. Simulations indicate that enhancing sentiment for key service features could yield 1-2% average revenue gains per store.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16551v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Khaled Boughanmi, Kamel Jedidi, Nour Jedidi</dc:creator>
    </item>
    <item>
      <title>Estimating Treatment Effects under Recommender Interference: A Structured Neural Networks Approach</title>
      <link>https://arxiv.org/abs/2406.14380</link>
      <description>arXiv:2406.14380v4 Announce Type: replace 
Abstract: Recommender systems are essential for content-sharing platforms by curating personalized content. To improve recommender systems, platforms frequently rely on creator-side randomized experiments to evaluate algorithm updates. We show that commonly adopted difference-in-means estimators can lead to severely biased estimates due to recommender interference, where treated and control creators compete for exposure. This bias can result in incorrect business decisions. To address this, we propose a ``recommender choice model'' that explicitly represents the interference pathway. The approach combines a structural choice framework with neural networks to account for rich viewer-content heterogeneity. Building on this foundation, we develop a debiased estimator using the double machine learning (DML) framework to adjust for errors from nuisance component estimation. We show that the estimator is $\sqrt{n}$-consistent and asymptotically normal, and we extend the DML theory to handle correlated data, which arise in our context due to overlapped items. We validate our method with a large-scale field experiment on Weixin short-video platform, using a costly double-sided randomization design to obtain an interference-free ground truth. Our results show that the proposed estimator successfully recovers this ground truth, whereas benchmark estimators exhibit substantial bias, and in some cases, yield reversed signs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14380v4</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruohan Zhan, Shichao Han, Yuchen Hu, Zhenling Jiang</dc:creator>
    </item>
    <item>
      <title>Moments by Integrating the Moment-Generating Function</title>
      <link>https://arxiv.org/abs/2410.23587</link>
      <description>arXiv:2410.23587v4 Announce Type: replace 
Abstract: We introduce a novel method for obtaining a wide variety of moments of any random variable with a well-defined moment-generating function (MGF). We derive new expressions for fractional moments and fractional absolute moments, both central and non-central moments. The expressions are relatively simple integrals that involve the MGF, but do not require its derivatives. We label the new method CMGF because it uses a complex extension of the MGF and can be used to obtain complex moments. We illustrate the new method with three applications where the MGF is available in closed-form, while the corresponding densities and the derivatives of the MGF are either unavailable or very difficult to obtain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23587v4</guid>
      <category>econ.EM</category>
      <category>q-fin.CP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Reinhard Hansen, Chen Tong</dc:creator>
    </item>
    <item>
      <title>Global identification of dynamic panel models with interactive effects</title>
      <link>https://arxiv.org/abs/2504.14354</link>
      <description>arXiv:2504.14354v2 Announce Type: replace 
Abstract: We investigate the problem of global identification in dynamic panel models with interactive effects, under the large-N, fixed-T setting. While local identification, typically established via the Jacobian matrix, is well understood, global identification has remained a more elusive and challenging issue. It is commonly believed to be unachievable in this context. However, we demonstrate that the model is, in fact, globally identified. Our analysis also covers models with additive fixed effects, including cases with unit roots, where previous studies have reported non-identification based on certain estimators. We show that, even in these settings, global identification holds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14354v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jushan Bai, Pablo Mones</dc:creator>
    </item>
    <item>
      <title>Combine and conquer: model averaging for out-of-distribution forecasting</title>
      <link>https://arxiv.org/abs/2506.03693</link>
      <description>arXiv:2506.03693v2 Announce Type: replace 
Abstract: Travel behaviour modellers have an increasingly diverse set of models at their disposal, ranging from traditional econometric structures to models from mathematical psychology and data-driven approaches from machine learning. A key question arises as to how well these different models perform in prediction, especially when considering trips of different characteristics from those used in estimation, i.e. out-of-distribution prediction, and whether better predictions can be obtained by combining insights from the different models. Across two case studies, we show that while data-driven approaches excel in predicting mode choice for trips within the distance bands used in estimation, beyond that range, the picture is fuzzy. To leverage the relative advantages of the different model families and capitalise on the notion that multiple `weak' models can result in more robust models, we put forward the use of a model averaging approach that allocates weights to different model families as a function of the \emph{distance} between the characteristics of the trip for which predictions are made, and those used in model estimation - in our case, we use trip distance as the sole characteristic for this. Overall, we see that the model averaging approach gives larger weight to models with stronger behavioural or econometric underpinnings the more we move outside the interval of trip distances covered in estimation. Across both case studies, we show that our model averaging approach obtains improved performance both on the estimation and test data, and crucially also when predicting mode choices for trips of distances outside the range used in estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03693v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stephane Hess, Sander van Cranenburgh</dc:creator>
    </item>
    <item>
      <title>Selecting the Best Arm in One-Shot Multi-Arm RCTs: The Asymptotic Minimax-Regret Decision Framework for the Best-Population Selection Problem</title>
      <link>https://arxiv.org/abs/2509.03796</link>
      <description>arXiv:2509.03796v2 Announce Type: replace 
Abstract: We develop a frequentist decision-theoretic framework for selecting the best arm in one-shot, multi-arm randomized controlled trials (RCTs). Our approach characterizes the minimax-regret (MMR) optimal decision rule for any multivariate location family reward distribution with full support. We show that the MMR rule is deterministic, unique, and computationally tractable. We then specialize to the case of multivariate normal (MVN) rewards with an arbitrary covariance matrix, and establish the local asymptotic minimaxity of a plug-in version of the rule when only estimated means and covariances are available. This asymptotic MMR (AMMR) procedure maps a covariance-matrix estimate directly into decision boundaries, allowing straightforward implementation in practice. Our analysis highlights a sharp contrast between two-arm and multi-arm designs. With two arms, the "pick-the-winner" empirical success rule remains MMR-optimal, regardless of the arm-specific variances. By contrast, with three or more arms and heterogeneous variances, the empirical success rule is no longer optimal: the MMR decision boundaries become nonlinear and systematically penalize high-variance arms, requiring stronger evidence to select them. Our multi-arm AMMR framework offers a rigorous foundation that leads to practical criteria for comparing multiple policies simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03796v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joonhwi Joo</dc:creator>
    </item>
    <item>
      <title>Spatial and Temporal Boundaries in Difference-in-Differences: A Framework from Navier-Stokes Equation</title>
      <link>https://arxiv.org/abs/2510.11013</link>
      <description>arXiv:2510.11013v2 Announce Type: replace 
Abstract: This paper develops a unified framework for identifying spatial and temporal boundaries of treatment effects in difference-in-differences designs. Starting from fundamental fluid dynamics equations (Navier-Stokes), we derive conditions under which treatment effects decay exponentially in space and time, enabling researchers to calculate explicit boundaries beyond which effects become undetectable. The framework encompasses both linear (pure diffusion) and nonlinear (advection-diffusion with chemical reactions) regimes, with testable scope conditions based on dimensionless numbers from physics (P\'eclet and Reynolds numbers). We demonstrate the framework's diagnostic capability using air pollution from coal-fired power plants. Analyzing 791 ground-based PM$_{2.5}$ monitors and 189,564 satellite-based NO$_2$ grid cells in the Western United States over 2019-2021, we find striking regional heterogeneity: within 100 km of coal plants, both pollutants show positive spatial decay (PM$_{2.5}$: $\kappa_s = 0.00200$, $d^* = 1,153$ km; NO$_2$: $\kappa_s = 0.00112$, $d^* = 2,062$ km), validating the framework. Beyond 100 km, negative decay parameters correctly signal that urban sources dominate and diffusion assumptions fail. Ground-level PM$_{2.5}$ decays approximately twice as fast as satellite column NO$_2$, consistent with atmospheric transport physics. The framework successfully diagnoses its own validity in four of eight analyzed regions, providing researchers with physics-based tools to assess whether their spatial difference-in-differences setting satisfies diffusion assumptions before applying the estimator. Our results demonstrate that rigorous boundary detection requires both theoretical derivation from first principles and empirical validation of underlying physical assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11013v2</guid>
      <category>econ.EM</category>
      <category>econ.GN</category>
      <category>math.ST</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Nonparametric Identification and Estimation of Spatial Treatment Effect Boundaries: Evidence from 42 Million Pollution Observations</title>
      <link>https://arxiv.org/abs/2510.12289</link>
      <description>arXiv:2510.12289v2 Announce Type: replace 
Abstract: This paper develops a nonparametric framework for identifying and estimating spatial boundaries of treatment effects in settings with geographic spillovers. While atmospheric dispersion theory predicts exponential decay of pollution under idealized assumptions, these assumptions -- steady winds, homogeneous atmospheres, flat terrain -- are systematically violated in practice. I establish nonparametric identification of spatial boundaries under weak smoothness and monotonicity conditions, propose a kernel-based estimator with data-driven bandwidth selection, and derive asymptotic theory for inference. Using 42 million satellite observations of NO$_2$ concentrations near coal plants (2019-2021), I find that nonparametric kernel regression reduces prediction errors by 1.0 percentage point on average compared to parametric exponential decay assumptions, with largest improvements at policy-relevant distances: 2.8 percentage points at 10 km (near-source impacts) and 3.7 percentage points at 100 km (long-range transport). Parametric methods systematically underestimate near-source concentrations while overestimating long-range decay. The COVID-19 pandemic provides a natural experiment validating the framework's temporal sensitivity: NO$_2$ concentrations dropped 4.6\% in 2020, then recovered 5.7\% in 2021. These results demonstrate that flexible, data-driven spatial methods substantially outperform restrictive parametric assumptions in environmental policy applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12289v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Nonparametric Identification of Spatial Treatment Effect Boundaries: Evidence from Bank Branch Consolidation</title>
      <link>https://arxiv.org/abs/2510.13148</link>
      <description>arXiv:2510.13148v2 Announce Type: replace 
Abstract: I develop a nonparametric framework for identifying spatial boundaries of treatment effects without imposing parametric functional form restrictions. The method employs local linear regression with data-driven bandwidth selection to flexibly estimate spatial decay patterns and detect treatment effect boundaries. Monte Carlo simulations demonstrate that the nonparametric approach exhibits lower bias and correctly identifies the absence of boundaries when none exist, unlike parametric methods that may impose spurious spatial patterns. I apply this framework to bank branch openings during 2015--2020, matching 5,743 new branches to 5.9 million mortgage applications across 14,209 census tracts. The analysis reveals that branch proximity significantly affects loan application volume (8.5\% decline per 10 miles) but not approval rates, consistent with branches stimulating demand through local presence while credit decisions remain centralized. Examining branch survival during the digital transformation era (2010--2023), I find a non-monotonic relationship with area income: high-income areas experience more closures despite conventional wisdom. This counterintuitive pattern reflects strategic consolidation of redundant branches in over-banked wealthy urban areas rather than discrimination against poor neighborhoods. Controlling for branch density, urbanization, and competition, the direct income effect diminishes substantially, with branch density emerging as the primary determinant of survival. These findings demonstrate the necessity of flexible nonparametric methods for detecting complex spatial patterns that parametric models would miss, and challenge simplistic narratives about banking deserts by revealing the organizational complexity underlying spatial consolidation decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13148v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Dynamic Spatial Treatment Effect Boundaries: A Continuous Functional Framework from Navier-Stokes Equations</title>
      <link>https://arxiv.org/abs/2510.14409</link>
      <description>arXiv:2510.14409v2 Announce Type: replace 
Abstract: I develop a comprehensive theoretical framework for dynamic spatial treatment effect boundaries using continuous functional definitions grounded in Navier-Stokes partial differential equations. Rather than discrete treatment effect estimators, the framework characterizes treatment intensity as a continuous function $\tau(\mathbf{x}, t)$ over space-time, enabling rigorous analysis of propagation dynamics, boundary evolution, and cumulative exposure patterns. Building on exact self-similar solutions expressible through Kummer confluent hypergeometric and modified Bessel functions, I establish that treatment effects follow scaling laws $\tau(d, t) = t^{-\alpha} f(d/t^\beta)$ where exponents characterize diffusion mechanisms. Empirical validation using 42 million TROPOMI satellite observations of NO$_2$ pollution from U.S. coal-fired power plants demonstrates strong exponential spatial decay ($\kappa_s = 0.004$ per km, $R^2 = 0.35$) with detectable boundaries at 572 km. Monte Carlo simulations confirm superior performance over discrete parametric methods in boundary detection and false positive avoidance (94\% vs 27\% correct rejection). Regional heterogeneity analysis validates diagnostic capability: positive decay parameters within 100 km confirm coal plant dominance; negative parameters beyond 100 km correctly signal when urban sources dominate. The continuous functional perspective unifies spatial econometrics with mathematical physics, providing theoretically grounded methods for boundary detection, exposure quantification, and policy evaluation across environmental economics, banking, and healthcare applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14409v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Dynamic Spatial Treatment Effects as Continuous Functionals: Theory and Evidence from Healthcare Access</title>
      <link>https://arxiv.org/abs/2510.15324</link>
      <description>arXiv:2510.15324v2 Announce Type: replace 
Abstract: I develop a continuous functional framework for spatial treatment effects grounded in Navier-Stokes partial differential equations. Rather than discrete treatment parameters, the framework characterizes treatment intensity as continuous functions $\tau(\mathbf{x}, t)$ over space-time, enabling rigorous analysis of boundary evolution, spatial gradients, and cumulative exposure. Empirical validation using 32,520 U.S. ZIP codes demonstrates exponential spatial decay for healthcare access ($\kappa = 0.002837$ per km, $R^2 = 0.0129$) with detectable boundaries at 37.1 km. The framework successfully diagnoses when scope conditions hold: positive decay parameters validate diffusion assumptions near hospitals, while negative parameters correctly signal urban confounding effects. Heterogeneity analysis reveals 2-13 $\times$ stronger distance effects for elderly populations and substantial education gradients. Model selection strongly favors logarithmic decay over exponential ($\Delta \text{AIC} &gt; 10,000$), representing a middle ground between exponential and power-law decay. Applications span environmental economics, banking, and healthcare policy. The continuous functional framework provides predictive capability ($d^*(t) = \xi^* \sqrt{t}$), parameter sensitivity ($\partial d^*/\partial \nu$), and diagnostic tests unavailable in traditional difference-in-differences approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15324v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>A Double Machine Learning Approach to Combining Experimental and Observational Data</title>
      <link>https://arxiv.org/abs/2307.01449</link>
      <description>arXiv:2307.01449v4 Announce Type: replace-cross 
Abstract: Experimental and observational studies often lack validity due to untestable assumptions. We propose a double machine learning approach to combine experimental and observational studies, allowing practitioners to test for assumption violations and estimate treatment effects consistently. Our framework proposes a falsification test for external validity and ignorability under milder assumptions. We provide consistent treatment effect estimators even when one of the assumptions is violated. However, our no-free-lunch theorem highlights the necessity of accurately identifying the violated assumption for consistent treatment effect estimation. Through comparative analyses, we show our framework's superiority over existing data fusion methods. The practical utility of our approach is further exemplified by three real-world case studies, underscoring its potential for widespread application in empirical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.01449v4</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Harsh Parikh, Marco Morucci, Vittorio Orlandi, Sudeepa Roy, Cynthia Rudin, Alexander Volfovsky</dc:creator>
    </item>
    <item>
      <title>Robust Semiparametric Inference for Bayesian Additive Regression Trees</title>
      <link>https://arxiv.org/abs/2509.24634</link>
      <description>arXiv:2509.24634v2 Announce Type: replace-cross 
Abstract: We develop a semiparametric framework for inference on the mean response in missing-data settings using a corrected posterior distribution. Our approach is tailored to Bayesian Additive Regression Trees (BART), which is a powerful predictive method but whose nonsmoothness complicate asymptotic theory with multi-dimensional covariates. When using BART combined with Bayesian bootstrap weights, we establish a new Bernstein-von Mises theorem and show that the limit distribution generally contains a bias term. To address this, we introduce RoBART, a posterior bias-correction that robustifies BART for valid inference on the mean response. Monte Carlo studies support our theory, demonstrating reduced bias and improved coverage relative to existing procedures using BART.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24634v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Breunig, Ruixuan Liu, Zhengfei Yu</dc:creator>
    </item>
  </channel>
</rss>
