<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Sep 2024 03:14:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>MITHOS: Interactive Mixed Reality Training to Support Professional Socio-Emotional Interactions at Schools</title>
      <link>https://arxiv.org/abs/2409.12968</link>
      <description>arXiv:2409.12968v1 Announce Type: new 
Abstract: Teachers in challenging conflict situations often experience shame and self-blame, which relate to the feeling of incompetence but may externalise as anger. Sensing mixed signals fails the contingency rule for developing affect regulation and may result in confusion for students about their own emotions and hinder their emotion regulation. Therefore, being able to constructively regulate emotions not only benefits individual experience of emotions but also fosters effective interpersonal emotion regulation and influences how a situation is managed. MITHOS is a system aimed at training teachers' conflict resolution skills through realistic situative learning opportunities during classroom conflicts. In four stages, MITHOS supports teachers' socio-emotional self-awareness, perspective-taking and positive regard. It provides: a) a safe virtual environment to train free social interaction and receive natural social feedback from reciprocal student-agent reactions, b) spatial situational perspective taking through an avatar, c) individual virtual reflection guidance on emotional experiences through co-regulation processes, and d) expert feedback on professional behavioural strategies. This chapter presents the four stages and their implementation in a semi-automatic Wizard-of-Oz (WoZ) System. The WoZ system affords collecting data that are used for developing the fully automated hybrid (machine learning and model-based) system, and to validate the underlying psychological and conflict resolution models. We present results validating the approach in terms of scenario realism, as well as a systematic testing of the effects of external avatar similarity on antecedents of self-awareness with behavior similarity. The chapter contributes to a common methodology of conducting interdisciplinary research for human-centered and generalisable XR and presents a system designed to support it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12968v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lara Chehayeb, Chirag Bhuvaneshwara, Manuel Anglet, Bernhard Hilpert, Ann-Kristin Meyer, Dimitra Tsovaltzi, Patrick Gebhard, Antje Biermann, Sinah Auchtor, Nils Lauinger, Julia Knopf, Andreas Kaiser, Fabian Kersting, Gregor Mehlmann, Florian Lingenfelser, Elisabeth Andr\'e</dc:creator>
    </item>
    <item>
      <title>Quality of Mobile Apps for Psychological Skills Training in Sport: a MARS-based Study</title>
      <link>https://arxiv.org/abs/2409.12970</link>
      <description>arXiv:2409.12970v1 Announce Type: new 
Abstract: Over the last decade, there has been a significant increase in the development of mobile applications to deliver various services in sports, including psychological skills training (PST) for athletes. While there are numerous PST-related apps available, little attention has been given to their objective quality. This study aimed to assess the current offerings of PST apps in sports, rate their quality, and provide recommendations for future app development. A scoping review of PST-related apps available on the Apple App Store was conducted, resulting in the retention of 19 apps. The apps used different media types to develop the PST. Of the 19 apps, videos were used by 8 (42%), audios by 7 (37%), articles by 3 (16%), assessment by 4 (21%), ebook by 1 (5%), and both cognitive tasks and personalized journals by 2 (10%). Overall, the app quality measured through the Mobile App Rating Scale (MARS) failed to meet acceptable standards, with a mean rating of 2.78 and only 6 of the apps receiving a score that met the acceptable standards. The findings highlight the need for improvement in the development of PST apps to enhance their quality and usability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12970v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>R. Bonetti, B. Rod, D. Hauw</dc:creator>
    </item>
    <item>
      <title>Exploring Higher Education Competencies through Spreadsheet Self-Assessment and Time</title>
      <link>https://arxiv.org/abs/2409.12974</link>
      <description>arXiv:2409.12974v1 Announce Type: new 
Abstract: The present paper aims to explore higher education students' spreadsheet competencies and reliability through self-assessment and real-world problem-solving practices. Digital natives alleged skills and competences allowed us to hypothesize that students perform better in Excel than on paper, but the findings cannot confirm this hypothesis. However, our results indicate that students tend to inaccurately assess their spreadsheet competencies compared to their actual performance in both paper-based and Excel tasks. It has also be found that students need at least twice as much time to achieve the same high scores in the digital environment as they do on paper. The results violated the widely accepted assumption that digital native students do not need computer science education, since they are born with it. This study highlights the importance of accurate self-assessment in digital skill development and time management within higher education contexts, particularly in technology-driven disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12974v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the EuSpRIG 2024 Conference "Spreadsheet Productivity &amp; Risks" ISBN : 978-1-905404-59-9</arxiv:journal_reference>
      <dc:creator>Maria Csernoch, Judit T. Kiss, Viktor Tak\'acs, Domici\'an M\'at\'e</dc:creator>
    </item>
    <item>
      <title>Subject integration with spreadsheets -- Ignoring education is the greatest risk ever</title>
      <link>https://arxiv.org/abs/2409.12975</link>
      <description>arXiv:2409.12975v1 Announce Type: new 
Abstract: Within the framework of Technological Pedagogical and Content Knowledge, subject integration is one possible solution for the introduction of meaningful digitalization and digitization in schools. This process incorporates that any school subject can be taught with digital support, informatics (computer) classes can be contextualized, and the gap between 'serious informatics' and 'digital literacy' can be minimized. The present paper details how three traditional Grade 3 tasks can be solved in spreadsheets, what skills, competencies, and computer science knowledge of both teachers and students can be developed. The solutions also reveal that analysing, understanding, planning, and discussing tasks is as important as the activity in the spreadsheets, which process plays a crucial role in the preparation of students for their future jobs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12975v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M\'aria Csernoch, \'Ad\'am Gul\'acsi, J\'ulia Csernoch</dc:creator>
    </item>
    <item>
      <title>Can we only use guideline instead of shot in prompt?</title>
      <link>https://arxiv.org/abs/2409.12979</link>
      <description>arXiv:2409.12979v1 Announce Type: new 
Abstract: Currently, prompting techniques can be mainly divided into two categories:1)shot method implicitly inspires the model to answer the question by mimicing the steps in the given example, e.g., the few-shot CoT. 2) Guideline method explicitly instructs the model to reason by following guidelines, which contains succinct and concise task-specific knowledge. Shot method is prone to difficulties in terms of selection of shots type, the number of shots, and the design of the reasoning steps, so a question arises: can we only use guideline instead of shot in the prompt? To this end, we propose the FGT framework to automatically learn task-specific guidelines from dataset consisting of Feedback, Guideline, and Tree-gather agents. First, the feedback agent is designed to evaluate the outcomes, both right and wrong, of each Q&amp;A to gather insights guiding more effective optimization strategies. Next, the guideline agent is tasked with deriving guidelines from each piece of feedback and storing them in local memory. Lastly, the tree-gather agent aggregates all guidelines hierarchically through a tree structure, ultimately obtaining all unduplicated guidelines from a global perspective. In addition, we induce the model to generate intermediate processes to ensure the reasoning consistent with the guidelines. Experimental results demonstrate that our approach achieves superior performance across multiple tasks, thereby highlighting the effectiveness of using the guidelines in prompt.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12979v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaxiang Chen, Song Wang, Zhucong Li, Wayne Xiong, Lizhen Qu, Zenglin Xu, Yuan Qi</dc:creator>
    </item>
    <item>
      <title>Across-Game Engagement Modelling via Few-Shot Learning</title>
      <link>https://arxiv.org/abs/2409.13002</link>
      <description>arXiv:2409.13002v1 Announce Type: new 
Abstract: Domain generalisation involves learning artificial intelligence (AI) models that can maintain high performance across diverse domains within a specific task. In video games, for instance, such AI models can supposedly learn to detect player actions across different games. Despite recent advancements in AI, domain generalisation for modelling the users' experience remains largely unexplored. While video games present unique challenges and opportunities for the analysis of user experience -- due to their dynamic and rich contextual nature -- modelling such experiences is limited by generally small datasets. As a result, conventional modelling methods often struggle to bridge the domain gap between users and games due to their reliance on large labelled training data and assumptions of common distributions of user experience. In this paper, we tackle this challenge by introducing a framework that decomposes the general domain-agnostic modelling of user experience into several domain-specific and game-dependent tasks that can be solved via few-shot learning. We test our framework on a variation of the publicly available GameVibe corpus, designed specifically to test a model's ability to predict user engagement across different first-person shooter games. Our findings demonstrate the superior performance of few-shot learners over traditional modelling methods and thus showcase the potential of few-shot learning for robust experience modelling in video games and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13002v1</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kosmas Pinitas, Konstantinos Makantasis, Georgios N. Yannakakis</dc:creator>
    </item>
    <item>
      <title>Choosing Between an LLM versus Search for Learning: A HigherEd Student Perspective</title>
      <link>https://arxiv.org/abs/2409.13051</link>
      <description>arXiv:2409.13051v1 Announce Type: new 
Abstract: Large language models (LLMs) are rapidly changing learning processes, as they are readily available to students and quickly complete or augment several learning-related activities with non-trivial performance. Such major shifts in learning dynamic have previously occurred when search engines and Wikipedia were introduced, and they augmented or traditional information consumption sources such as libraries and books for university students. We investigate the possibility of the next shift: the use of LLMs to find and digest information in the context of learning and how they relate to existing technologies such as the search engine. We conducted a study where students were asked to learn new topics using a search engine and an LLM in a within-subjects counterbalanced design. We used that study as a contextual grounding for a post-experience follow-up interview where we elicited student reflections, preferences, pain points, and general outlook of an LLM (ChatGPT) over a search engine (Google).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13051v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rahul R. Divekar, Sophia Guerra, Lisette Gonzalez, Natasha Boos</dc:creator>
    </item>
    <item>
      <title>Mixed Reality Tele-ultrasound over 750 km: a Clinical Study</title>
      <link>https://arxiv.org/abs/2409.13058</link>
      <description>arXiv:2409.13058v1 Announce Type: new 
Abstract: Ultrasound is a hand-held, low-cost, non-invasive medical imaging modality which plays a vital role in diagnosing various diseases. Despite this, many rural and remote communities do not have access to ultrasound scans due to the lack of local experts trained to perform them. To address this challenge, we built a mixed reality and haptics-based tele-ultrasound system to enable an expert to precisely guide a novice remotely in carrying out an ultrasound exam. The precision and flexibility of our solution makes it more practical than existing tele-ultrasound solutions. We tested the system in Skidegate on the islands of Haida Gwaii, BC, Canada, with the experts positioned 754 km away at the University of British Columbia, Vancouver, Canada. We performed 11 scans with 10 novices and 2 experts. The experts were tasked with acquiring 5 target images and measurements in the epigastric region. The novices of various backgrounds and ages were all inexperienced in mixed reality and were not required to have prior ultrasound experience. The captured images were evaluated by two radiologists who were not present for the tests. These results are discussed along with new insights into the human computer interaction in such a system. We show that human teleoperation is feasible and can achieve high performance for completing remote ultrasound procedures, even at a large distance and with completely novice followers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13058v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Yeung, David Black, Patrick B. Chen, Victoria Lessoway, Janice Reid, Sergio Rangel-Suarez, Silvia D. Chang, Septimiu E. Salcudean</dc:creator>
    </item>
    <item>
      <title>Usage of Virtual Reality in Combating Social Anxiety Disorders in Non-native English Speakers: A Survey</title>
      <link>https://arxiv.org/abs/2409.13085</link>
      <description>arXiv:2409.13085v1 Announce Type: new 
Abstract: Social Anxiety Disorder (SAD) is a common yet underestimated mental health disorder. While non-native English speaker (NNES) students face public speaking, they are more likely to suffer some public speaking anxiety (PSA) due to linguistic and sociocultural differences \cite{cite1}. Virtual Reality (VR) technology has already benefitted social-emotional training. The core objective is to summarise the benefits and limitations of using VR technology to help NNES students practice and improve their public speaking skills. This is not a comprehensive survey of the literature. Instead, the selected papers are intended to reflect the current knowledge across various broad topics. Virtual Reality, Social Anxiety Disorder, Public Speaking Anxiety, English as a Second Language, and Non native English speakers are the keywords used for searching mainly in the Academic Search Complete (ASC) database. Compared with native English speaker (NES) students, NNES students have the potential to achieve better results when using VR technology for PSA social-emotional training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13085v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyi Zhang, Ayesha Khalid</dc:creator>
    </item>
    <item>
      <title>Traceable Text: Deepening Reading of AI-Generated Summaries with Phrase-Level Provenance Links</title>
      <link>https://arxiv.org/abs/2409.13099</link>
      <description>arXiv:2409.13099v1 Announce Type: new 
Abstract: As AI-generated summaries proliferate, how can we help people understand the veracity of those summaries? In this short paper, we design a simple interaction primitive, traceable text, to support critical examination of generated summaries and the source texts they were derived from. In a traceable text, passages of a generated summary link to passages of the source text that informed them. A traceable text can be generated with a straightforward prompt chaining approach, and optionally adjusted by human authors depending on application. In a usability study, we examined the impact of traceable texts on reading and understanding patient medical records. Traceable text helped readers answer questions about the content of the source text more quickly and markedly improved correctness of answers in cases where there were hallucinations in the summaries. When asked to read a text of personal importance with traceable text, readers employed traceable text as an understanding aid and as an index into the source note.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13099v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hita Kambhamettu, Jamie Flores, Andrew Head</dc:creator>
    </item>
    <item>
      <title>A Cognitive Approach to Improving Binary Reverse Engineering with Immersive Virtual Reality</title>
      <link>https://arxiv.org/abs/2409.13100</link>
      <description>arXiv:2409.13100v1 Announce Type: new 
Abstract: Through its affordances, immersive virtual reality (VR) offers a means to apply embodied and external cognition from the physical realm to solving analytical problems that are typically only conceptual. We present an example of executing a structured analysis following the tenets of cognitive systems engineering to derive immersive affordances applicable to a difficult analytical problem, in our case, reverse engineering (RE) binary programs. We conducted a basic cognitive task analysis of the problem to reveal features of its cognitive model and their associated fundamental cognitive phenomena, and then we mapped those concepts to immersive affordances associated with those concepts. We implemented a subset of those affordances in a VR system facilitating discovery of features of a binary program. Feedback from RE practitioners drove the initial development of the system and we are preparing for a formal effectiveness study to inform the direction of future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13100v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dennis G. Brown, Julian Bauer, Luke Wittbrodt, Samuel Mulder</dc:creator>
    </item>
    <item>
      <title>Visualizationary: Automating Design Feedback for Visualization Designers using LLMs</title>
      <link>https://arxiv.org/abs/2409.13109</link>
      <description>arXiv:2409.13109v1 Announce Type: new 
Abstract: Interactive visualization editors empower people to author visualizations without writing code, but do not guide them in the art and craft of effective visual communication. In this paper, we explore the potential for using an off-the-shelf Large Language Model (LLM) to provide actionable and customized feedback to visualization designers. Our implementation, called VISUALIZATIONARY, showcases how ChatGPT can be used in this manner using two components: a preamble of visualization design guidelines and a suite of perceptual filters extracting salient metrics from a visualization image. We present findings from a longitudinal user study involving 13 visualization designers - 6 novices, 4 intermediate ones, and 3 experts - authoring a new visualization from scratch over the course of several days. Our results indicate that providing guidance in natural language using an LLM can aid even seasoned designers in refining their visualizations. All supplemental materials accompanying this paper are available at https://osf.io/v7hu8.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13109v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sungbok Shin, Sanghyun Hong, Niklas Elmqvist</dc:creator>
    </item>
    <item>
      <title>Designing an Intervention Tool for End-User Algorithm Audits in Personalized Recommendation Systems</title>
      <link>https://arxiv.org/abs/2409.13176</link>
      <description>arXiv:2409.13176v1 Announce Type: new 
Abstract: As algorithms increasingly shape user experiences on personalized recommendation platforms, there is a growing need for tools that empower end users to audit these algorithms for potential bias and harms. This paper introduces a novel intervention tool, MapMyFeed, designed to support everyday user audits. The tool addresses key challenges associated with user-driven algorithm audits, such as low algorithm literacy, unstructured audit paths, and the presence of noise. MapMyFeed assists users by offering guiding prompts, tracking audit paths via a browser extension, and visualizing audit results through a live dashboard. The tool will not only foster users' algorithmic literacy and awareness but also enhance more transparent and fair recommendation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13176v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qunfang Wu, Lu Xian</dc:creator>
    </item>
    <item>
      <title>V-Hands: Touchscreen-based Hand Tracking for Remote Whiteboard Interaction</title>
      <link>https://arxiv.org/abs/2409.13347</link>
      <description>arXiv:2409.13347v1 Announce Type: new 
Abstract: In whiteboard-based remote communication, the seamless integration of drawn content and hand-screen interactions is essential for an immersive user experience. Previous methods either require bulky device setups for capturing hand gestures or fail to accurately track the hand poses from capacitive images. In this paper, we present a real-time method for precise tracking 3D poses of both hands from capacitive video frames. To this end, we develop a deep neural network to identify hands and infer hand joint positions from capacitive frames, and then recover 3D hand poses from the hand-joint positions via a constrained inverse kinematic solver. Additionally, we design a device setup for capturing high-quality hand-screen interaction data and obtained a more accurate synchronized capacitive video and hand pose dataset. Our method improves the accuracy and stability of 3D hand tracking for capacitive frames while maintaining a compact device setup for remote communication. We validate our scheme design and its superior performance on 3D hand pose tracking and demonstrate the effectiveness of our method in whiteboard-based remote communication. Our code, model, and dataset are available at https://V-Hands.github.io.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13347v1</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinshuang Liu, Yizhong Zhang, Xin Tong</dc:creator>
    </item>
    <item>
      <title>MapCraft: Dissecting and Designing Custom Geo-Infographics</title>
      <link>https://arxiv.org/abs/2409.13424</link>
      <description>arXiv:2409.13424v1 Announce Type: new 
Abstract: Geographic infographics are increasingly utilized across various domains to convey spatially relevant information effectively. However, creating these infographics typically requires substantial expertise in design and visualization, as well as proficiency with specialized tools, which can deter many potential creators. To address this barrier, our research analyzed and categorized 118 geographic infographics and sketches designed by 8 experts, leading to the development of a structured design space encompassing four critical dimensions: basic map representations, encoding channels, label design and placement, and highlighting techniques. Based on this design space, we developed a web-based authoring tool that allows users to explore and apply these dimensions interactively. The tool's effectiveness was evaluated through a user study involving 12 participants without prior design experience. Participants were first required manually to create geographic infographics using provided datasets, then utilize our authoring tool to recreate and refine their initial drafts. We also conducted pre- and post-use assessments of the participants' knowledge of geographic infographic design. The findings revealed significant improvements in understanding and applying information encoding channels, highlighting techniques, and label design and placement strategies. These results demonstrate the tool's dual capacity to assist users in creating geographics while educating them on key visualization strategies. Our tool, therefore, empowers a broader audience, including those with limited design and visualization backgrounds, to effectively create and utilize geo-infographics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13424v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Zhang, Yifan Xu, Kaiwen Li, Lingyun Yu, Yu Liu</dc:creator>
    </item>
    <item>
      <title>Sportoonizer: Augmenting Sports Highlights' Narration and Visual Impact via Automatic Manga B-Roll Generation</title>
      <link>https://arxiv.org/abs/2409.13443</link>
      <description>arXiv:2409.13443v1 Announce Type: new 
Abstract: Sports highlights are becoming increasingly popular on video-sharing platforms. Yet, crafting sport highlight videos is challenging, which requires producing engaging narratives from different angles, and conforming to different platform affordances with constantly changing audiences. Many content creators therefore create derivative work of the original sports video through manga styles to enhance its expressiveness. But manually creating and inserting tailored manga-style content can still be time-consuming. We introduce Sportoonizer, a system embedding the pipeline for automatic generation of manga-style animations for highlights in sports videos and insertion into original videos. It seamlessly merges dynamic manga sequences with live-action footage, enriching the visual tapestry and deepening narrative scope. By leveraging genAIs, Sportoonizer crafts compelling storylines encapsulating the intensity of sports moments and athletes' personal journeys. Our evaluation study demonstrates that integrating manga B-rolls significantly enhances viewer engagement, visual interest, and emotional connection towards athletes' stories in the viewing experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13443v1</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siying Hu, Xiangzhe Yuan, Jiajun Wang, Piaohong Wang, Jian Ma, Zhiyang Wu, Qian Wan, Zhicong Lu</dc:creator>
    </item>
    <item>
      <title>ChainBuddy: An AI Agent System for Generating LLM Pipelines</title>
      <link>https://arxiv.org/abs/2409.13588</link>
      <description>arXiv:2409.13588v1 Announce Type: new 
Abstract: As large language models (LLMs) advance, their potential applications have grown significantly. However, it remains difficult to evaluate LLM behavior on user-specific tasks and craft effective pipelines to do so. Many users struggle with where to start, often referred to as the "blank page" problem. ChainBuddy, an AI assistant for generating evaluative LLM pipelines built into the ChainForge platform, aims to tackle this issue. ChainBuddy offers a straightforward and user-friendly way to plan and evaluate LLM behavior, making the process less daunting and more accessible across a wide range of possible tasks and use cases. We report a within-subjects user study comparing ChainBuddy to the baseline interface. We find that when using AI assistance, participants reported a less demanding workload and felt more confident setting up evaluation pipelines of LLM behavior. We derive insights for the future of interfaces that assist users in the open-ended evaluation of AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13588v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyue Zhang, Ian Arawjo</dc:creator>
    </item>
    <item>
      <title>BoilerTAI: A Platform for Enhancing Instruction Using Generative AI in Educational Forums</title>
      <link>https://arxiv.org/abs/2409.13196</link>
      <description>arXiv:2409.13196v1 Announce Type: cross 
Abstract: Contribution: This Full paper in the Research Category track describes a practical, scalable platform that seamlessly integrates Generative AI (GenAI) with online educational forums, offering a novel approach to augment the instructional capabilities of staff. The platform empowers instructional staff to efficiently manage, refine, and approve responses by facilitating interaction between student posts and a Large Language Model (LLM). This contribution enhances the efficiency and effectiveness of instructional support and significantly improves the quality and speed of responses provided to students, thereby enriching the overall learning experience.
  Background: Grounded in Vygotsky's socio-cultural theory and the concept of the More Knowledgeable Other (MKO), the study examines how GenAI can act as an auxiliary MKO to enrich educational dialogue between students and instructors.
  Research Question: How effective is GenAI in reducing the workload of instructional staff when used to pre-answer student questions posted on educational discussion forums?
  Methodology: Using a mixed-methods approach in large introductory programming courses, human Teaching Assistants (AI-TAs) employed an AI-assisted platform to pre-answer student queries. We analyzed efficiency indicators like the frequency of modifications to AI-generated responses and gathered qualitative feedback from AI-TAs.
  Findings: The findings indicate no significant difference in student reception to responses generated by AI-TAs compared to those provided by human instructors. This suggests that GenAI can effectively meet educational needs when adequately managed. Moreover, AI-TAs experienced a reduction in the cognitive load required for responding to queries, pointing to GenAI's potential to enhance instructional efficiency without compromising the quality of education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13196v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anvit Sinha, Shruti Goyal, Zachary Sy, Rhianna Kuperus, Ethan Dickey, Andres Bejarano</dc:creator>
    </item>
    <item>
      <title>A User Study on Contrastive Explanations for Multi-Effector Temporal Planning with Non-Stationary Costs</title>
      <link>https://arxiv.org/abs/2409.13427</link>
      <description>arXiv:2409.13427v1 Announce Type: cross 
Abstract: In this paper, we adopt constrastive explanations within an end-user application for temporal planning of smart homes. In this application, users have requirements on the execution of appliance tasks, pay for energy according to dynamic energy tariffs, have access to high-capacity battery storage, and are able to sell energy to the grid. The concurrent scheduling of devices makes this a multi-effector planning problem, while the dynamic tariffs yield costs that are non-stationary (alternatively, costs that are stationary but depend on exogenous events). These characteristics are such that the planning problems are generally not supported by existing PDDL-based planners, so we instead design a custom domain-dependent planner that scales to reasonable appliance numbers and time horizons. We conduct a controlled user study with 128 participants using an online crowd-sourcing platform based on two user stories. Our results indicate that users provided with contrastive questions and explanations have higher levels of satisfaction, tend to gain improved understanding, and rate the helpfulness more favourably with the recommended AI schedule compared to those without access to these features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13427v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaowei Liu, Kevin McAreavey, Weiru Liu</dc:creator>
    </item>
    <item>
      <title>Dermatologist-like explainable AI enhances melanoma diagnosis accuracy: eye-tracking study</title>
      <link>https://arxiv.org/abs/2409.13476</link>
      <description>arXiv:2409.13476v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) systems have substantially improved dermatologists' diagnostic accuracy for melanoma, with explainable AI (XAI) systems further enhancing clinicians' confidence and trust in AI-driven decisions. Despite these advancements, there remains a critical need for objective evaluation of how dermatologists engage with both AI and XAI tools. In this study, 76 dermatologists participated in a reader study, diagnosing 16 dermoscopic images of melanomas and nevi using an XAI system that provides detailed, domain-specific explanations. Eye-tracking technology was employed to assess their interactions. Diagnostic performance was compared with that of a standard AI system lacking explanatory features. Our findings reveal that XAI systems improved balanced diagnostic accuracy by 2.8 percentage points relative to standard AI. Moreover, diagnostic disagreements with AI/XAI systems and complex lesions were associated with elevated cognitive load, as evidenced by increased ocular fixations. These insights have significant implications for clinical practice, the design of AI tools for visual tasks, and the broader development of XAI in medical diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13476v1</guid>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tirtha Chanda, Sarah Haggenmueller, Tabea-Clara Bucher, Tim Holland-Letz, Harald Kittler, Philipp Tschandl, Markus V. Heppt, Carola Berking, Jochen S. Utikal, Bastian Schilling, Claudia Buerger, Cristian Navarrete-Dechent, Matthias Goebeler, Jakob Nikolas Kather, Carolin V. Schneider, Benjamin Durani, Hendrike Durani, Martin Jansen, Juliane Wacker, Joerg Wacker, Reader Study Consortium, Titus J. Brinker</dc:creator>
    </item>
    <item>
      <title>'Since Lawyers are Males..': Examining Implicit Gender Bias in Hindi Language Generation by LLMs</title>
      <link>https://arxiv.org/abs/2409.13484</link>
      <description>arXiv:2409.13484v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly being used to generate text across various languages, for tasks such as translation, customer support, and education. Despite these advancements, LLMs show notable gender biases in English, which become even more pronounced when generating content in relatively underrepresented languages like Hindi. This study explores implicit gender biases in Hindi text generation and compares them to those in English. We developed Hindi datasets inspired by WinoBias to examine stereotypical patterns in responses from models like GPT-4o and Claude-3 sonnet. Our results reveal a significant gender bias of 87.8% in Hindi, compared to 33.4% in English GPT-4o generation, with Hindi responses frequently relying on gender stereotypes related to occupations, power hierarchies, and social class. This research underscores the variation in gender biases across languages and provides considerations for navigating these biases in generative AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13484v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ishika Joshi, Ishita Gupta, Adrita Dey, Tapan Parikh</dc:creator>
    </item>
    <item>
      <title>Sketching With Your Voice: "Non-Phonorealistic" Rendering of Sounds via Vocal Imitation</title>
      <link>https://arxiv.org/abs/2409.13507</link>
      <description>arXiv:2409.13507v1 Announce Type: cross 
Abstract: We present a method for automatically producing human-like vocal imitations of sounds: the equivalent of "sketching," but for auditory rather than visual representation. Starting with a simulated model of the human vocal tract, we first try generating vocal imitations by tuning the model's control parameters to make the synthesized vocalization match the target sound in terms of perceptually-salient auditory features. Then, to better match human intuitions, we apply a cognitive theory of communication to take into account how human speakers reason strategically about their listeners. Finally, we show through several experiments and user studies that when we add this type of communicative reasoning to our method, it aligns with human intuitions better than matching auditory features alone does. This observation has broad implications for the study of depiction in computer graphics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13507v1</guid>
      <category>cs.GR</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3680528.3687679</arxiv:DOI>
      <arxiv:journal_reference>SIGGRAPH Asia 2024</arxiv:journal_reference>
      <dc:creator>Matthew Caren, Kartik Chandra, Joshua B. Tenenbaum, Jonathan Ragan-Kelley, Karima Ma</dc:creator>
    </item>
    <item>
      <title>A Survey on Moral Foundation Theory and Pre-Trained Language Models: Current Advances and Challenges</title>
      <link>https://arxiv.org/abs/2409.13521</link>
      <description>arXiv:2409.13521v1 Announce Type: cross 
Abstract: Moral values have deep roots in early civilizations, codified within norms and laws that regulated societal order and the common good. They play a crucial role in understanding the psychological basis of human behavior and cultural orientation. The Moral Foundation Theory (MFT) is a well-established framework that identifies the core moral foundations underlying the manner in which different cultures shape individual and social lives. Recent advancements in natural language processing, particularly Pre-trained Language Models (PLMs), have enabled the extraction and analysis of moral dimensions from textual data. This survey presents a comprehensive review of MFT-informed PLMs, providing an analysis of moral tendencies in PLMs and their application in the context of the MFT. We also review relevant datasets and lexicons and discuss trends, limitations, and future directions. By providing a structured overview of the intersection between PLMs and MFT, this work bridges moral psychology insights within the realm of PLMs, paving the way for further research and development in creating morally aware AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13521v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Zangari, Candida M. Greco, Davide Picca, Andrea Tagarelli</dc:creator>
    </item>
    <item>
      <title>Toward Improving Binary Program Comprehension via Embodied Immersion: A Survey</title>
      <link>https://arxiv.org/abs/2404.17051</link>
      <description>arXiv:2404.17051v2 Announce Type: replace 
Abstract: Binary program comprehension is critical for many use cases but is difficult, suffering from compounded uncertainty and lack of full automation. We seek methods to improve the effectiveness of the human-machine joint cognitive system performing binary PC. We survey three research areas to perform an indirect cognitive task analysis: cognitive models of the PC process, related elements of cognitive theory, and applicable affordances of virtual reality. Based on common elements in these areas, we identify three overarching themes: enhancing abductive iteration, augmenting working memory, and supporting information organization. These themes spotlight several affordances of VR to exploit in future studies of immersive tools for binary PC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17051v2</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dennis Brown, Emily Mulder, Samuel Mulder</dc:creator>
    </item>
    <item>
      <title>Beyond Recommendations: From Backward to Forward AI Support of Pilots' Decision-Making Process</title>
      <link>https://arxiv.org/abs/2406.08959</link>
      <description>arXiv:2406.08959v3 Announce Type: replace 
Abstract: AI is anticipated to enhance human decision-making in high-stakes domains like aviation, but adoption is often hindered by challenges such as inappropriate reliance and poor alignment with users' decision-making. Recent research suggests that a core underlying issue is the recommendation-centric design of many AI systems, i.e., they give end-to-end recommendations and ignore the rest of the decision-making process. Alternative support paradigms are rare, and it remains unclear how the few that do exist compare to recommendation-centric support. In this work, we aimed to empirically compare recommendation-centric support to an alternative paradigm, continuous support, in the context of diversions in aviation. We conducted a mixed-methods study with 32 professional pilots in a realistic setting. To ensure the quality of our study scenarios, we conducted a focus group with four additional pilots prior to the study. We found that continuous support can support pilots' decision-making in a forward direction, allowing them to think more beyond the limits of the system and make faster decisions when combined with recommendations, though the forward support can be disrupted. Participants' statements further suggest a shift in design goal away from providing recommendations, to supporting quick information gathering. Our results show ways to design more helpful and effective AI decision support that goes beyond end-to-end recommendations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08959v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3687024</arxiv:DOI>
      <dc:creator>Zelun Tony Zhang, Sebastian S. Feger, Lucas Dullenkopf, Rulu Liao, Lukas S\"usslin, Yuanting Liu, Andreas Butz</dc:creator>
    </item>
    <item>
      <title>ArticulatePro: A Comparative Study on a Proactive and Non-Proactive Assistant in a Climate Data Exploration Task</title>
      <link>https://arxiv.org/abs/2409.10797</link>
      <description>arXiv:2409.10797v2 Announce Type: replace 
Abstract: Recent advances in Natural Language Interfaces (NLIs) and Large Language Models (LLMs) have transformed our approach to NLP tasks, shifting the focus towards a more Pragmatics-based approach. This shift enables more natural interactions between humans and voice assistants, which have been historically difficult to achieve. Pragmatics involves understanding how users often talk out of turn, interrupt one another, or provide relevant information without being explicitly asked (maxim of quantity). To explore this, we developed a digital assistant that continuously listens to conversations and proactively generates relevant visualizations during data exploration tasks. In a within-subject study, participants interacted with both proactive and non-proactive versions of a voice assistant while exploring the Hawaii Climate Data Portal (HCDP). Results suggest that the proactive assistant enhanced user engagement and facilitated quicker insights. Our study highlights the potential of Pragmatic, proactive AI in NLIs and identifies key challenges in its implementation, offering insights for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10797v2</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roderick Tabalba, Christopher J. Lee, Giorgio Tran, Nurit Kirshenbaum, Jason Leigh</dc:creator>
    </item>
    <item>
      <title>OSINT Clinic: Co-designing AI-Augmented Collaborative OSINT Investigations for Vulnerability Assessment</title>
      <link>https://arxiv.org/abs/2409.11672</link>
      <description>arXiv:2409.11672v2 Announce Type: replace 
Abstract: Small businesses need vulnerability assessments to identify and mitigate cyber risks. Cybersecurity clinics provide a solution by offering students hands-on experience while delivering free vulnerability assessments to local organizations. To scale this model, we propose an Open Source Intelligence (OSINT) clinic where students conduct assessments using only publicly available data. We enhance the quality of investigations in the OSINT clinic by addressing the technical and collaborative challenges. Over the duration of the 2023-24 academic year, we conducted a three-phase co-design study with six students. Our study identified key challenges in the OSINT investigations and explored how generative AI could address these performance gaps. We developed design ideas for effective AI integration based on the use of AI probes and collaboration platform features. A pilot with three small businesses highlighted both the practical benefits of AI in streamlining investigations, and limitations, including privacy concerns and difficulty in monitoring progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11672v2</guid>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anirban Mukhopadhyay, Kurt Luther</dc:creator>
    </item>
    <item>
      <title>From Cash to Cashless: UPI's Impact on Spending Behavior Among Indian Users and Prototyping Financially Responsible Interfaces</title>
      <link>https://arxiv.org/abs/2401.09937</link>
      <description>arXiv:2401.09937v3 Announce Type: replace-cross 
Abstract: Unified Payments Interface (UPI) is a groundbreaking innovation making waves in digital payment systems in India. It has revolutionised financial transactions by offering enhanced convenience and security. While previous research has primarily focused on the macroeconomic effects of digital payments, our study examines UPI's impact on individual spending behavior. Through a survey of 276 respondents and 20 follow-up interviews, we found that approximately 75% of participants reported increased spending due to UPI. Many attributed this to UPI's intangible nature, which reduced feelings of guilt typically associated with spending. Additionally, participants provided suggestions to improve the user experience of existing UPI applications. Utilizing this feedback, we developed a high-fidelity prototype based on a popular UPI app in India and conducted usability testing with 34 participants. The insights gathered from this testing shaped the final prototype and its features. This study offers valuable design recommendations for UPI app developers and other stakeholders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09937v3</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harshal Dev, Raj Gupta, Sahiti Dharmavaram, Dhruv Kumar</dc:creator>
    </item>
    <item>
      <title>Unveiling Thoughts: A Review of Advancements in EEG Brain Signal Decoding into Text</title>
      <link>https://arxiv.org/abs/2405.00726</link>
      <description>arXiv:2405.00726v2 Announce Type: replace-cross 
Abstract: The conversion of brain activity into text using electroencephalography (EEG) has gained significant traction in recent years. Many researchers are working to develop new models to decode EEG signals into text form. Although this area has shown promising developments, it still faces numerous challenges that necessitate further improvement. It's important to outline this area's recent developments and future research directions. In this review article, we thoroughly summarize the progress in EEG-to-text conversion. Firstly, we talk about how EEG-to-text technology has grown and what problems we still face. Secondly, we discuss existing techniques used in this field. This includes methods for collecting EEG data, the steps to process these signals, and the development of systems capable of translating these signals into coherent text. We conclude with potential future research directions, emphasizing the need for enhanced accuracy, reduced system constraints, and the exploration of novel applications across varied sectors. By addressing these aspects, this review aims to contribute to developing more accessible and effective Brain-Computer Interface (BCI) technology for a broader user base.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00726v2</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TCDS.2024.3462452</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Cognitive and Developmental Systems (2024)</arxiv:journal_reference>
      <dc:creator>Saydul Akbar Murad, Nick Rahimi</dc:creator>
    </item>
    <item>
      <title>SECURE: Benchmarking Large Language Models for Cybersecurity Advisory</title>
      <link>https://arxiv.org/abs/2405.20441</link>
      <description>arXiv:2405.20441v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated potential in cybersecurity applications but have also caused lower confidence due to problems like hallucinations and a lack of truthfulness. Existing benchmarks provide general evaluations but do not sufficiently address the practical and applied aspects of LLM performance in cybersecurity-specific tasks. To address this gap, we introduce the SECURE (Security Extraction, Understanding \&amp; Reasoning Evaluation), a benchmark designed to assess LLMs performance in realistic cybersecurity scenarios. SECURE includes six datasets focussed on the Industrial Control System sector to evaluate knowledge extraction, understanding, and reasoning based on industry-standard sources. Our study evaluates seven state-of-the-art models on these tasks, providing insights into their strengths and weaknesses in cybersecurity contexts, and offer recommendations for improving LLMs reliability as cyber advisory tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20441v3</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dipkamal Bhusal, Md Tanvirul Alam, Le Nguyen, Ashim Mahara, Zachary Lightcap, Rodney Frazier, Romy Fieblinger, Grace Long Torales, Benjamin A. Blakely, Nidhi Rastogi</dc:creator>
    </item>
    <item>
      <title>Unsupervised Reward-Driven Image Segmentation in Automated Scanning Transmission Electron Microscopy Experiments</title>
      <link>https://arxiv.org/abs/2409.12462</link>
      <description>arXiv:2409.12462v2 Announce Type: replace-cross 
Abstract: Automated experiments in scanning transmission electron microscopy (STEM) require rapid image segmentation to optimize data representation for human interpretation, decision-making, site-selective spectroscopies, and atomic manipulation. Currently, segmentation tasks are typically performed using supervised machine learning methods, which require human-labeled data and are sensitive to out-of-distribution drift effects caused by changes in resolution, sampling, or beam shape. Here, we operationalize and benchmark a recently proposed reward-driven optimization workflow for on-the fly image analysis in STEM. This unsupervised approach is much more robust, as it does not rely on human labels and is fully explainable. The explanatory feedback can help the human to verify the decision making and potentially tune the model by selecting the position along the Pareto frontier of reward functions. We establish the timing and effectiveness of this method, demonstrating its capability for real-time performance in high-throughput and dynamic automated STEM experiments. The reward driven approach allows to construct explainable robust analysis workflows and can be generalized to a broad range of image analysis tasks in electron and scanning probe microscopy and chemical imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12462v2</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kamyar Barakati, Utkarsh Pratiush, Austin C. Houston, Gerd Duscher, Sergei V. Kalinin</dc:creator>
    </item>
  </channel>
</rss>
