<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Sep 2024 04:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer</title>
      <link>https://arxiv.org/abs/2408.16978</link>
      <description>arXiv:2408.16978v1 Announce Type: new 
Abstract: Large Language Models (LLMs) with long context capabilities are integral to complex tasks in natural language processing and computational biology, such as text generation and protein sequence analysis. However, training LLMs directly on extremely long contexts demands considerable GPU resources and increased memory, leading to higher costs and greater complexity. Alternative approaches that introduce long context capabilities via downstream finetuning or adaptations impose significant design limitations. In this paper, we propose Fully Pipelined Distributed Transformer (FPDT) for efficiently training long-context LLMs with extreme hardware efficiency. For GPT and Llama models, we achieve a 16x increase in sequence length that can be trained on the same hardware compared to current state-of-the-art solutions. With our dedicated sequence chunk pipeline design, we can now train 8B LLM with 2 million sequence length on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed FPDT is agnostic to existing training techniques and is proven to work efficiently across different LLM models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16978v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinghan Yao, Sam Ade Jacobs, Masahiro Tanaka, Olatunji Ruwase, Aamir Shafi, Hari Subramoni, Dhabaleswar K. Panda</dc:creator>
    </item>
    <item>
      <title>Application-Driven Exascale: The JUPITER Benchmark Suite</title>
      <link>https://arxiv.org/abs/2408.17211</link>
      <description>arXiv:2408.17211v1 Announce Type: new 
Abstract: Benchmarks are essential in the design of modern HPC installations, as they define key aspects of system components. Beyond synthetic workloads, it is crucial to include real applications that represent user requirements into benchmark suites, to guarantee high usability and widespread adoption of a new system. Given the significant investments in leadership-class supercomputers of the exascale era, this is even more important and necessitates alignment with a vision of Open Science and reproducibility. In this work, we present the JUPITER Benchmark Suite, which incorporates 16 applications from various domains. It was designed for and used in the procurement of JUPITER, the first European exascale supercomputer. We identify requirements and challenges and outline the project and software infrastructure setup. We provide descriptions and scalability studies of selected applications and a set of key takeaways. The JUPITER Benchmark Suite is released as open source software with this work at https://github.com/FZJ-JSC/jubench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17211v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas Herten, Sebastian Achilles, Damian Alvarez, Jayesh Badwaik, Eric Behle, Mathis Bode, Thomas Breuer, Daniel Caviedes-Voulli\`eme, Mehdi Cherti, Adel Dabah, Salem El Sayed, Wolfgang Frings, Ana Gonzalez-Nicolas, Eric B. Gregory, Kaveh Haghighi Mood, Thorsten Hater, Jenia Jitsev, Chelsea Maria John, Jan H. Meinke, Catrin I. Meyer, Pavel Mezentsev, Jan-Oliver Mirus, Stepan Nassyr, Carolin Penke, Manoel R\"ommer, Ujjwal Sinha, Benedikt von St. Vieth, Olaf Stein, Estela Suarez, Dennis Willsch, Ilya Zhukov</dc:creator>
    </item>
    <item>
      <title>Machine Learning-Based Research on the Adaptability of Adolescents to Online Education</title>
      <link>https://arxiv.org/abs/2408.16849</link>
      <description>arXiv:2408.16849v1 Announce Type: cross 
Abstract: With the rapid advancement of internet technology, the adaptability of adolescents to online learning has emerged as a focal point of interest within the educational sphere. However, the academic community's efforts to develop predictive models for adolescent online learning adaptability require further refinement and expansion. Utilizing data from the "Chinese Adolescent Online Education Survey" spanning the years 2014 to 2016, this study implements five machine learning algorithms - logistic regression, K-nearest neighbors, random forest, XGBoost, and CatBoost - to analyze the factors influencing adolescent online learning adaptability and to determine the model best suited for prediction. The research reveals that the duration of courses, the financial status of the family, and age are the primary factors affecting students' adaptability in online learning environments. Additionally, age significantly impacts students' adaptive capacities. Among the predictive models, the random forest, XGBoost, and CatBoost algorithms demonstrate superior forecasting capabilities, with the random forest model being particularly adept at capturing the characteristics of students' adaptability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16849v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingwei Wang, Sitong Liu</dc:creator>
    </item>
    <item>
      <title>Modular population protocols</title>
      <link>https://arxiv.org/abs/2111.11983</link>
      <description>arXiv:2111.11983v3 Announce Type: replace 
Abstract: Population protocols are a model of distributed computation intended for the study of networks of independent computing agents with dynamic communication structure. Each agent has a finite number of states, and communication opportunities occur nondeterministically, allowing the agents involved to change their states based on each other's states. Population protocols are often studied in terms of reaching a consensus on whether the input configuration satisfied some predicate.
  A desirable property of a computation model is modularity, the ability to combine existing simpler computations in a straightforward way. In the present paper we present a more general notion of functionality implemented by a population protocol in terms of multisets of inputs and outputs. This notion allows to design multiphase protocols as combinations of independently defined phases. The additional generality also increases the range of behaviours that can be captured in applications (e.g. maintaining the role distribution in a fleet of servers).
  We show that composition of protocols can be performed in a uniform mechanical way, and that the expressive power is essentially semilinear, similar to the predicate expressive power in the original population protocol setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.11983v3</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Michael Raskin</dc:creator>
    </item>
    <item>
      <title>A Modular Approach to Construct Signature-Free BRB Algorithms under a Message Adversary</title>
      <link>https://arxiv.org/abs/2204.13388</link>
      <description>arXiv:2204.13388v3 Announce Type: replace 
Abstract: This paper explores how reliable broadcast can be implemented without signatures when facing a dual adversary that can both corrupt processes and remove messages. More precisely, we consider an asynchronous $n$-process message-passing system in which up to $t_b$ processes are Byzantine and where, at the network level, for each message broadcast by a correct process, an adversary can prevent up to $t_m$ processes from receiving it (the integer $t_m$ defines the power of the message adversary). So, unlike previous works, this work considers that not only can computing entities be faulty (Byzantine processes), but, in addition, that the network can also lose messages. To this end, the paper adopts a modular strategy and first introduces a new basic communication abstraction denoted $k2\ell$-cast, which simplifies quorum engineering, and studies its properties in this new adversarial context. Then, the paper deconstructs existing signature-free Byzantine-tolerant asynchronous broadcast algorithms and, with the help of the $k2\ell$-cast communication abstraction, reconstructs versions of them that tolerate both Byzantine processes and message adversaries. Interestingly, these reconstructed algorithms are also more efficient than the Byzantine-tolerant-only algorithms from which they originate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.13388v3</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.OPODIS.2022.1</arxiv:DOI>
      <dc:creator>Timoth\'e Albouy (IRISA, WIDE), Davide Frey (Inria, WIDE), Michel Raynal (IRISA, WIDE), Fran\c{c}ois Ta\"iani (IRISA, WIDE)</dc:creator>
    </item>
    <item>
      <title>Iterative Thresholding and Projection Algorithms and Model-Based Deep Neural Networks for Sparse LQR Control Design</title>
      <link>https://arxiv.org/abs/2212.02929</link>
      <description>arXiv:2212.02929v2 Announce Type: replace 
Abstract: In this paper, we consider an LQR design problem for distributed control systems. For large-scale distributed systems, finding a solution might be computationally demanding due to communications among agents. To this aim, we deal with LQR minimization problem with a regularization for sparse feedback matrix, which can lead to achieve the reduction of the communication links in the distributed control systems. For this work, we introduce simple but efficient iterative algorithms -- Iterative Shrinkage Thresholding Algorithm (ISTA) and Iterative Sparse Projection Algorithm (ISPA). They can give us a trade-off solution between LQR cost and sparsity level on feedback matrix. Moreover, in order to improve the speed of the proposed algorithms, we design deep neural network models based on the proposed iterative algorithms. Numerical experiments demonstrate that our algorithms can outperform the previous methods using the Alternating Direction Method of Multiplier (ADMM) [2] and the Gradient Support Pursuit (GraSP) [3], and their deep neural network models can improve the performance of the proposed algorithms in convergence speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.02929v2</guid>
      <category>cs.DC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Myung Cho</dc:creator>
    </item>
    <item>
      <title>Hyperparameter Optimization as a Service on INFN Cloud</title>
      <link>https://arxiv.org/abs/2301.05522</link>
      <description>arXiv:2301.05522v3 Announce Type: replace 
Abstract: The simplest and often most effective way of parallelizing the training of complex machine learning models is to execute several training instances on multiple machines, scanning the hyperparameter space to optimize the underlying statistical model and the learning procedure. Often, such a meta-learning procedure is limited by the ability of accessing securely a common database organizing the knowledge of the previous and ongoing trials. Exploiting opportunistic GPUs provided in different environments represents a further challenge when designing such optimization campaigns. In this contribution, we discuss how a set of REST APIs can be used to access a dedicated service based on INFN Cloud to monitor and coordinate multiple training instances, with gradient-less optimization techniques, via simple HTTP requests. The service, called Hopaas (Hyperparameter OPtimization As A Service), is made of a web interface and sets of APIs implemented with a FastAPI backend running through Uvicorn and NGINX in a virtual instance of INFN Cloud. The optimization algorithms are currently based on Bayesian techniques as provided by Optuna. A Python frontend is also made available for quick prototyping. We present applications to hyperparameter optimization campaigns performed by combining private, INFN Cloud, and CINECA resources. Such multi-node multi-site optimization studies have given a significant boost to the development of a set of parameterizations for the ultra-fast simulation of the LHCb experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.05522v3</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Barbetti, Lucio Anderlini</dc:creator>
    </item>
    <item>
      <title>What Cannot Be Implemented on Weak Memory?</title>
      <link>https://arxiv.org/abs/2405.16611</link>
      <description>arXiv:2405.16611v2 Announce Type: replace 
Abstract: We present a general methodology for establishing the impossibility of implementing certain concurrent objects on different (weak) memory models. The key idea behind our approach lies in characterizing memory models by their mergeability properties, identifying restrictions under which independent memory traces can be merged into a single valid memory trace. In turn, we show that the mergeability properties of the underlying memory model entail similar mergeability requirements on the specifications of objects that can be implemented on that memory model. We demonstrate the applicability of our approach to establish the impossibility of implementing standard distributed objects with different restrictions on memory traces on three memory models: strictly consistent memory, total store order, and release-acquire. These impossibility results allow us to identify tight and almost tight bounds for some objects, as well as new separation results between weak memory models, and between well-studied objects based on their implementability on weak memory models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16611v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Armando Casta\~neda, Gregory Chockler, Brijesh Dongol, Ori Lahav</dc:creator>
    </item>
    <item>
      <title>Elevator: Self-* and Persistent Hub Sampling Service in Unstructured Peer-to-Peer Networks</title>
      <link>https://arxiv.org/abs/2406.07946</link>
      <description>arXiv:2406.07946v2 Announce Type: replace 
Abstract: We present Elevator, a novel algorithm for hub samplingin peer-to-peer networks, enabling the construction of overlays with atopology between a random graph and a star network, and networksthat have both hubs and are resilient to failures. Our approach emergesfrom principles of preferential attachment, forming hubs spontaneously,offering an innovative solution for decentralized networks that can benefituse cases requiring a network with both low diameter and resilience tofailures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07946v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamed Amine Legheraba (NPA), Maria Potop-Butucaru (NPA), S\'ebastien Tixeuil (IUF, NPA)</dc:creator>
    </item>
    <item>
      <title>Simple, strict, proper, happy: A study of reachability in temporal graphs</title>
      <link>https://arxiv.org/abs/2208.01720</link>
      <description>arXiv:2208.01720v4 Announce Type: replace-cross 
Abstract: Dynamic networks are a complex subject. Not only do they inherit the complexity of static networks (as a particular case); they are also sensitive to definitional subtleties that are a frequent source of confusion and incomparability of results in the literature.
  In this paper, we take a step back and examine three such aspects in more details, exploring their impact in a systematic way; namely, whether the temporal paths are required to be \emph{strict} (i.e., the times along a path must increasing, not just be non-decreasing), whether the time labeling is \emph{proper} (two adjacent edges cannot be present at the same time) and whether the time labeling is \emph{simple} (an edge can have only one presence time). In particular, we investigate how different combinations of these features impact the expressivity of the graph in terms of reachability.
  Our results imply a hierarchy of expressivity for the resulting settings, shedding light on the loss of generality that one is making when considering either combination. Some settings are more general than expected; in particular, proper temporal graphs turn out to be as expressive as general temporal graphs where non-strict paths are allowed. Also, we show that the simplest setting, that of \emph{happy} temporal graphs (i.e., both proper and simple) remains expressive enough to emulate the reachability of general temporal graphs in a certain (restricted but useful) sense. Furthermore, this setting is advocated as a target of choice for proving negative results. We illustrates this by strengthening two known results to happy graphs (namely, the inexistence of sparse spanners, and the hardness of computing temporal components). Overall, we hope that this article can be seen as a guide for choosing between different settings of temporal graphs, while being aware of the way these choices affect generality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.01720v4</guid>
      <category>cs.DM</category>
      <category>cs.DC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arnaud Casteigts, Timoth\'ee Corsini, Writika Sarkar</dc:creator>
    </item>
    <item>
      <title>FedAgg: Adaptive Federated Learning with Aggregated Gradients</title>
      <link>https://arxiv.org/abs/2303.15799</link>
      <description>arXiv:2303.15799v5 Announce Type: replace-cross 
Abstract: Federated Learning (FL) has emerged as a crucial distributed training paradigm, enabling discrete devices to collaboratively train a shared model under the coordination of a central server, while leveraging their locally stored private data. Nonetheless, the non-independent-and-identically-distributed (Non-IID) data generated on heterogeneous clients and the incessant information exchange among participants may significantly impede training efficacy, retard the model convergence rate and increase the risk of privacy leakage. To alleviate the divergence between the local and average model parameters and obtain a fast model convergence rate, we propose an adaptive FEDerated learning algorithm called FedAgg by refining the conventional stochastic gradient descent (SGD) methodology with an AGgregated Gradient term at each local training epoch and adaptively adjusting the learning rate based on a penalty term that quantifies the local model deviation. To tackle the challenge of information exchange among clients during local training and design a decentralized adaptive learning rate for each client, we introduce two mean-field terms to approximate the average local parameters and gradients over time. Through rigorous theoretical analysis, we demonstrate the existence and convergence of the mean-field terms and provide a robust upper bound on the convergence of our proposed algorithm. The extensive experimental results on real-world datasets substantiate the superiority of our framework in comparison with existing state-of-the-art FL strategies for enhancing model performance and accelerating convergence rate under IID and Non-IID datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.15799v5</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenhao Yuan, Xuehe Wang</dc:creator>
    </item>
    <item>
      <title>Etalon: Holistic Performance Evaluation Framework for LLM Inference Systems</title>
      <link>https://arxiv.org/abs/2407.07000</link>
      <description>arXiv:2407.07000v2 Announce Type: replace-cross 
Abstract: Serving large language models (LLMs) in production can incur substantial costs, which has prompted recent advances in inference system optimizations. Today, these systems are evaluated against conventional latency and throughput metrics (eg. TTFT, TBT, Normalised Latency and TPOT). However, these metrics fail to fully capture the nuances of LLM inference, leading to an incomplete assessment of user-facing performance crucial for real-time applications such as chat and translation. In this paper, we first identify the pitfalls of current performance metrics in evaluating LLM inference systems. We then propose Etalon, a comprehensive performance evaluation framework that includes fluidity-index -- a novel metric designed to reflect the intricacies of the LLM inference process and its impact on real-time user experience. Finally, we evaluate various existing open-source platforms and model-as-a-service offerings using Etalon, discussing their strengths and weaknesses. Etalon is available at https://github.com/project-etalon/etalon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07000v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amey Agrawal, Anmol Agarwal, Nitin Kedia, Jayashree Mohan, Souvik Kundu, Nipun Kwatra, Ramachandran Ramjee, Alexey Tumanov</dc:creator>
    </item>
  </channel>
</rss>
