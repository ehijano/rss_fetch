<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 Oct 2025 04:00:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Call-Center Staff Scheduling Considering Performance Evolution under Emotional Stress</title>
      <link>https://arxiv.org/abs/2510.16406</link>
      <description>arXiv:2510.16406v1 Announce Type: new 
Abstract: Emotional stress often has a significant effect on the working performance of staff, but this effect is commonly neglected in existing staff scheduling methods. We study a call-center staff scheduling problem, which considers the evolution of work performance of staff under emotional stress. First, we present an emotional stress driven model that estimates the working performance of call-center employees based on not only skill levels but also emotional states. On the basis of the model, we formulate a combined short-term and long-term call-center staff scheduling problem aiming at maximizing the customer service level, which depends on the working performance of employees. We then propose a memetic optimization algorithm combining global mutation and neighborhood search assisted by deep reinforcement learning to efficiently solve this problem. Experimental results on real-world problem instances of bank call-center staff scheduling demonstrate the performance advantages of the proposed method over selected popular staff scheduling methods. By explicitly modeling and incorporating emotional stress, our method reflects a more realistic understanding and utilization of human behavior in staff scheduling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16406v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujun Zheng, Xinya Chen, Xueqin Lu, Weiguo Sheng, Shengyong Chen</dc:creator>
    </item>
    <item>
      <title>Bombardier Beetle Optimizer: A Novel Bio-Inspired Algorithm for Global Optimization</title>
      <link>https://arxiv.org/abs/2510.17005</link>
      <description>arXiv:2510.17005v1 Announce Type: new 
Abstract: In this paper, a novel bio-inspired optimization algorithm is proposed, called Bombardier Beetle Optimizer (BBO). This type of species is very intelligent, which has an ability to defense and escape from predators. The principles of the former one is inspired by the defense mechanism of Bombardier Beetle against the predators, which the Bombardier Beetle triggers a toxic chemical spray when it feels threatened. This reaction occurs in a specialized reaction chamber inside its abdomen and includes a well regulated enzymatic mechanism, which comprises hot water vapor, oxygen, and irritating substances like p-benzoquinones. In addition, the proposed BBO simulates also the escape mechanism of Bombardier Beetle from predator, which it has the ability to calculate its distance from predator and it can fly away. The BBO is tested with optimizing Congress on Evolutionary Computation (CEC 2017) test bed suites. Moreover, it is compared against well-known metaheuristic optimization algorithms includes Chernobyl Disaster Optimizer (CDO), Grey Wolf Optimizer (GWO), Particle Swarm Optimization (PSO), Bermuda Triangle Optimizer (BTO), Sperm Swarm Optimization (SSO) and Gravitational Search Algorithm (GSA). The outcomes of this paper prove the BBO's efficiency in which outperforms the other algorithms in terms of convergence rate and quality of results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17005v1</guid>
      <category>cs.NE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hisham A. Shehadeh, Mohd Yamani Idna Idris, Iqbal H. Jebril</dc:creator>
    </item>
    <item>
      <title>ReLACE: A Resource-Efficient Low-Latency Cortical Acceleration Engine</title>
      <link>https://arxiv.org/abs/2510.17392</link>
      <description>arXiv:2510.17392v1 Announce Type: new 
Abstract: We present a Cortical Neural Pool (CNP) architecture featuring a high-speed, resource-efficient CORDIC-based Hodgkin Huxley (RCHH) neuron model. Unlike shared CORDIC-based DNN approaches, the proposed neuron leverages modular and performance-optimised CORDIC stages with a latency-area trade-off. The FPGA implementation of the RCHH neuron shows 24.5% LUT reduction and 35.2% improved speed, compared to SoTA designs, with 70% better normalised root mean square error (NRMSE). Furthermore, the CNP exhibits 2.85x higher throughput (12.69 GOPS) compared to a functionally equivalent CORDIC-based DNN engine, with only a 0.35% accuracy drop compared to the DNN counterpart on the MNIST dataset. The overall results indicate that the design shows biologically accurate, low-resource spiking neural network implementations for resource-constrained edge AI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17392v1</guid>
      <category>cs.NE</category>
      <category>cs.AR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sonu Kumar, Arjun S. Nair, Bhawna Chaudhary, Mukul Lokhande, Santosh Kumar Vishvakarma</dc:creator>
    </item>
    <item>
      <title>A Multi-Threading Kernel for Enabling Neuromorphic Edge Applications</title>
      <link>https://arxiv.org/abs/2510.17745</link>
      <description>arXiv:2510.17745v1 Announce Type: new 
Abstract: Spiking Neural Networks (SNNs) have sparse, event driven processing that can leverage neuromorphic applications. In this work, we introduce a multi-threading kernel that enables neuromorphic applications running at the edge, meaning they process sensory input directly and without any up-link to or dependency on a cloud service. The kernel shows speed-up gains over single thread processing by a factor of four on moderately sized SNNs and 1.7X on a Synfire network. Furthermore, it load-balances all cores available on multi-core processors, such as ARM, which run today's mobile devices and is up to 70% more energy efficient compared to statical core assignment. The present work can enable the development of edge applications that have low Size, Weight, and Power (SWaP), and can prototype the integration of neuromorphic chips.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17745v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars Niedermeier (Niedermeier Consulting, Zurich, ZH, Switzerland, Department of Cognitive Sciences, University of California, Irvine, CA, USA), Vyom Shah (Department of Computer Science, University of California, Irvine, CA, USA), Jeffrey L. Krichmar (Department of Computer Science, University of California, Irvine, CA, USA, Department of Cognitive Sciences, University of California, Irvine, CA, USA)</dc:creator>
    </item>
    <item>
      <title>Direct Simplified Symbolic Analysis (DSSA) Tool</title>
      <link>https://arxiv.org/abs/2510.15901</link>
      <description>arXiv:2510.15901v1 Announce Type: cross 
Abstract: This paper introduces Direct Simplified Symbolic Analysis (DSSA), a new method for simplifying analog circuits. Unlike traditional matrix- or graph-based techniques that are often slow and memory-intensive, DSSA treats the task as a modeling problem and directly extracts the most significant transfer function terms. By combining Monte Carlo simulation with a genetic algorithm, it minimizes error between simplified symbolic and exact numeric expressions. Tests on five circuits in MATLAB show strong performance, with only 0.64 dB average and 1.36 dB maximum variation in dc-gain, along with a 6.8% average pole/zero error. These results highlight DSSA as an efficient and accurate tool for symbolic circuit analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15901v1</guid>
      <category>cs.OH</category>
      <category>cs.AR</category>
      <category>cs.NE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3934/jdg.2023023</arxiv:DOI>
      <arxiv:journal_reference>Journal of Dynamics and Games, 2024, 11(3), 232-248</arxiv:journal_reference>
      <dc:creator>Mohammad Shokouhifar, Hossein Yazdanjouei, Gerhard-Wilhelm Weber</dc:creator>
    </item>
    <item>
      <title>Spiking Neural Network for Cross-Market Portfolio Optimization in Financial Markets: A Neuromorphic Computing Approach</title>
      <link>https://arxiv.org/abs/2510.15921</link>
      <description>arXiv:2510.15921v1 Announce Type: cross 
Abstract: Cross-market portfolio optimization has become increasingly complex with the globalization of financial markets and the growth of high-frequency, multi-dimensional datasets. Traditional artificial neural networks, while effective in certain portfolio management tasks, often incur substantial computational overhead and lack the temporal processing capabilities required for large-scale, multi-market data. This study investigates the application of Spiking Neural Networks (SNNs) for cross-market portfolio optimization, leveraging neuromorphic computing principles to process equity data from both the Indian (Nifty 500) and US (S&amp;P 500) markets. A five-year dataset comprising approximately 1,250 trading days of daily stock prices was systematically collected via the Yahoo Finance API. The proposed framework integrates Leaky Integrate-andFire neuron dynamics with adaptive thresholding, spike-timingdependent plasticity, and lateral inhibition to enable event-driven processing of financial time series. Dimensionality reduction is achieved through hierarchical clustering, while populationbased spike encoding and multiple decoding strategies support robust portfolio construction under realistic trading constraints, including cardinality limits, transaction costs, and adaptive risk aversion. Experimental evaluation demonstrates that the SNN-based framework delivers superior risk-adjusted returns and reduced volatility compared to ANN benchmarks, while substantially improving computational efficiency. These findings highlight the promise of neuromorphic computation for scalable, efficient, and robust portfolio optimization across global financial markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15921v1</guid>
      <category>q-fin.PM</category>
      <category>cs.NE</category>
      <category>q-fin.CP</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Amarendra Mohan (IIT Kharagpur), Ameer Tamoor Khan (University of Copenhagen), Shuai Li (University of Oulu), Xinwei Cao (Jiangnan University), Zhibin Li (Chengdu University of Information Technology)</dc:creator>
    </item>
    <item>
      <title>Impl\'ementation Efficiente de Fonctions de Convolution sur FPGA \`a l'Aide de Blocs Param\'etrables et d'Approximations Polynomiales</title>
      <link>https://arxiv.org/abs/2510.15930</link>
      <description>arXiv:2510.15930v1 Announce Type: cross 
Abstract: Implementing convolutional neural networks (CNNs) on field-programmable gate arrays (FPGAs) has emerged as a promising alternative to GPUs, offering lower latency, greater power efficiency and greater flexibility. However, this development remains complex due to the hardware knowledge required and the long synthesis, placement and routing stages, which slow down design cycles and prevent rapid exploration of network configurations, making resource optimisation under severe constraints particularly challenging. This paper proposes a library of configurable convolution Blocks designed to optimize FPGA implementation and adapt to available resources. It also presents a methodological framework for developing mathematical models that predict FPGA resources utilization. The approach is validated by analyzing the correlation between the parameters, followed by error metrics. The results show that the designed blocks enable adaptation of convolution layers to hardware constraints, and that the models accurately predict resource consumption, providing a useful tool for FPGA selection and optimized CNN deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15930v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippe Magalh\~aes (LabHC), Virginie Fresse (LabHC), Beno\^it Suffran (LabHC), Olivier Alata (LabHC)</dc:creator>
    </item>
    <item>
      <title>Neuronal Group Communication for Efficient Neural representation</title>
      <link>https://arxiv.org/abs/2510.16851</link>
      <description>arXiv:2510.16851v1 Announce Type: cross 
Abstract: The ever-increasing scale of modern neural networks has brought unprecedented performance alongside daunting challenges in efficiency and interpretability. This paper addresses the core question of how to build large neural systems that learn efficient, modular, and interpretable representations. We propose Neuronal Group Communication (NGC), a theory-driven framework that reimagines a neural network as a dynamical system of interacting neuronal groups rather than a monolithic collection of neural weights. Instead of treating each weight as an independent trainable parameter, NGC treats weights as transient interactions between embedding-like neuronal states, with neural computation unfolding through iterative communication among groups of neurons. This low-rank, modular representation yields compact models: groups of neurons exchange low-dimensional signals, enabling intra-group specialization and inter-group information sharing while dramatically reducing redundant parameters. By drawing on dynamical systems theory, we introduce a neuronal stability metric (analogous to Lyapunov stability) that quantifies the contraction of neuron activations toward stable patterns during sequence processing. Using this metric, we reveal that emergent reasoning capabilities correspond to an external driving force or ``potential'', which nudges the neural dynamics away from trivial trajectories while preserving stability. Empirically, we instantiate NGC in large language models (LLMs) and demonstrate improved performance on complex reasoning benchmarks under moderate compression. NGC consistently outperforms standard low-rank approximations and cross-layer basis-sharing methods at comparable compression rates. We conclude by discussing the broader implications of NGC, including how structured neuronal group dynamics might relate to generalization in high-dimensional learning systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16851v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengqi Pei, Qingming Huang, Shuhui Wang</dc:creator>
    </item>
    <item>
      <title>Application-oriented automatic hyperparameter optimization for spiking neural network prototyping</title>
      <link>https://arxiv.org/abs/2502.12172</link>
      <description>arXiv:2502.12172v3 Announce Type: replace 
Abstract: Hyperparameter optimization (HPO) is of paramount importance in the development of high-performance, specialized artificial intelligence (AI) models, ranging from well-established machine learning (ML) solutions to the deep learning (DL) domain and the field of spiking neural networks (SNNs). The latter introduce further complexity due to the neuronal computational units and their additional hyperparameters, whose inadequate setting can dramatically impact the final model performance. At the cost of possible reduced generalization capabilities, the most suitable strategy to fully disclose the power of SNNs is to adopt an application-oriented approach and perform extensive HPO experiments. To facilitate these operations, automatic pipelines are fundamental, and their configuration is crucial. In this document, the Neural Network Intelligence (NNI) toolkit is used as reference framework to present one such solution, with a use case example providing evidence of the corresponding results. In addition, a summary of published works employing the presented pipeline is reported as a potential source of insights into application-oriented HPO experiments for SNN prototyping.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12172v3</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vittorio Fra</dc:creator>
    </item>
    <item>
      <title>Improvement of Spiking Neural Network with Bit Planes and Color Models</title>
      <link>https://arxiv.org/abs/2410.08229</link>
      <description>arXiv:2410.08229v4 Announce Type: replace-cross 
Abstract: Spiking neural network (SNN) has emerged as a promising paradigm in computational neuroscience and artificial intelligence, offering advantages such as low energy consumption and small memory footprint. However, their practical adoption is constrained by several challenges, prominently among them being performance optimization. In this study, we present a novel approach to enhance the performance of SNN for images through a new coding method that exploits bit plane representation. Our proposed technique is designed to improve the accuracy of SNN without increasing model size. Also, we investigate the impacts of color models of the proposed coding process. Through extensive experimental validation, we demonstrate the effectiveness of our coding strategy in achieving performance gain across multiple datasets. To the best of our knowledge, this is the first research that considers bit planes and color models in the context of SNN. By leveraging the unique characteristics of bit planes, we hope to unlock new potentials in SNNs performance, potentially paving the way for more efficient and effective SNNs models in future researches and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08229v4</guid>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <category>eess.IV</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nhan T. Luu, Duong T. Luu, Nam N. Pham, Thang C. Truong</dc:creator>
    </item>
    <item>
      <title>What should a neuron aim for? Designing local objective functions based on information theory</title>
      <link>https://arxiv.org/abs/2412.02482</link>
      <description>arXiv:2412.02482v5 Announce Type: replace-cross 
Abstract: In modern deep neural networks, the learning dynamics of the individual neurons is often obscure, as the networks are trained via global optimization. Conversely, biological systems build on self-organized, local learning, achieving robustness and efficiency with limited global information. We here show how self-organization between individual artificial neurons can be achieved by designing abstract bio-inspired local learning goals. These goals are parameterized using a recent extension of information theory, Partial Information Decomposition (PID), which decomposes the information that a set of information sources holds about an outcome into unique, redundant and synergistic contributions. Our framework enables neurons to locally shape the integration of information from various input classes, i.e. feedforward, feedback, and lateral, by selecting which of the three inputs should contribute uniquely, redundantly or synergistically to the output. This selection is expressed as a weighted sum of PID terms, which, for a given problem, can be directly derived from intuitive reasoning or via numerical optimization, offering a window into understanding task-relevant local information processing. Achieving neuron-level interpretability while enabling strong performance using local learning, our work advances a principled information-theoretic foundation for local learning strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02482v5</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>math.IT</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas C. Schneider, Valentin Neuhaus, David A. Ehrlich, Abdullah Makkeh, Alexander S. Ecker, Viola Priesemann, Michael Wibral</dc:creator>
    </item>
    <item>
      <title>Graph Coloring for Multi-Task Learning</title>
      <link>https://arxiv.org/abs/2509.16959</link>
      <description>arXiv:2509.16959v3 Announce Type: replace-cross 
Abstract: When different objectives conflict with each other in multi-task learning, gradients begin to interfere and slow convergence, thereby potentially reducing the final model's performance. To address this, we introduce SON-GOKU, a scheduler that computes gradient interference, constructs an interference graph, and then applies greedy graph-coloring to partition tasks into groups that align well with each other. At each training step, only one group (color class) of tasks are activated, and the grouping partition is constantly recomputed as task relationships evolve throughout training. By ensuring that each mini-batch contains only tasks that pull the model in the same direction, our method improves the effectiveness of any underlying multi-task learning optimizer without additional tuning. Since tasks within these groups will update in compatible directions, multi-task learning will improve model performance rather than impede it. Empirical results on six different datasets show that this interference-aware graph-coloring approach consistently outperforms baselines and state-of-the-art multi-task optimizers. We provide extensive theory showing why grouping and sequential updates improve multi-task learning, with guarantees on descent, convergence, and accurately identifying what tasks conflict or align.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16959v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>stat.ML</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Santosh Patapati</dc:creator>
    </item>
  </channel>
</rss>
