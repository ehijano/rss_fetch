<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 11 Nov 2024 05:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Fast and interpretable electricity consumption scenario generation for individual consumers</title>
      <link>https://arxiv.org/abs/2411.05014</link>
      <description>arXiv:2411.05014v1 Announce Type: new 
Abstract: To enable the transition from fossil fuels towards renewable energy, the low-voltage grid needs to be reinforced at a faster pace and on a larger scale than was historically the case. To efficiently plan reinforcements, one needs to estimate the currents and voltages throughout the grid, which are unknown but can be calculated from the grid layout and the electricity consumption time series of each consumer. However, for many consumers, these time series are unknown and have to be estimated from the available consumer information. We refer to this task as scenario generation. The state-of-the-art approach that generates electricity consumption scenarios is complex, resulting in a computationally expensive procedure with only limited interpretability. To alleviate these drawbacks, we propose a fast and interpretable scenario generation technique based on predictive clustering trees (PCTs) that does not compromise accuracy. In our experiments on three datasets from different locations, we found that our proposed approach generates time series that are at least as accurate as the state-of-the-art while being at least 7 times faster in training and prediction. Moreover, the interpretability of the PCT allows domain experts to gain insight into their data while simultaneously building trust in the predictions of the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05014v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.ML</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Soenen, A. Yurtman, T. Becker, K. Vanthournout, H. Blockeel</dc:creator>
    </item>
    <item>
      <title>Bootstrap Pettitt test for detecting change point in hydroclimatological data: a case study for Itaipu hydroelectric plant in Brazil</title>
      <link>https://arxiv.org/abs/2411.05233</link>
      <description>arXiv:2411.05233v1 Announce Type: new 
Abstract: The Pettitt test has been widely used in climate change and hydrological analyzes. However, studies evidence difficulties of this test in detecting change points, especially in small samples. This study presents a bootstrap application of the Pettitt test, which is numerically compared with the classical Pettitt test by an extensive Monte Carlo simulation study. The proposed test outperforms the classical test in all simulated scenarios. An application of the tests is conducted in the historical series of naturalized flows of the Itaipu Hydroelectric plant in Brazil, where several studies have shown a change point in the 70s. When the series is split into shorter series, to simulate small sample actual situations, the proposed test is more powerful than the classical Pettitt test to detect the change point. The proposed test can be an important tool to detect abrupt changes in water availability, supporting hydroclimatological resources decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05233v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/02626667.2019.1632461</arxiv:DOI>
      <arxiv:journal_reference>Hydrological Sciences Journal, Volume 64, 2019</arxiv:journal_reference>
      <dc:creator>Luiza Chiarelli Conte, D\'ebora Missio Bayer, F\'abio M. Bayer</dc:creator>
    </item>
    <item>
      <title>Increasing power and robustness in screening trials by testing stored specimens in the control arm</title>
      <link>https://arxiv.org/abs/2411.05580</link>
      <description>arXiv:2411.05580v1 Announce Type: new 
Abstract: Background: Screening trials require large sample sizes and long time-horizons to demonstrate mortality reductions. We recently proposed increasing statistical power by testing stored control-arm specimens, called the Intended Effect (IE) design. To evaluate feasibility of the IE design, the US National Cancer Institute (NCI) is collecting blood specimens in the control-arm of the NCI Vanguard Multicancer Detection pilot feasibility trial. However, key assumptions of the IE design require more investigation and relaxation. Methods: We relax the IE design to (1) reduce costs by testing only a stratified sample of control-arm specimens by incorporating inverse-probability sampling weights, (2) correct for potential loss-of-signal in stored control-arm specimens, and (3) correct for non-compliance with control-arm specimen collections. We also examine sensitivity to unintended effects of screening. Results: In simulations, testing all primary-outcome control-arm specimens and a 50% sample of the rest maintains nearly all the power of the IE while only testing half the control-arm specimens. Power remains increased from the IE analysis (versus the standard analysis) even if unintended effects exist. The IE design is robust to some loss-of-signal scenarios, but otherwise requires retest-positive fractions that correct bias at a small loss of power. The IE can be biased and lose power under control-arm non-compliance scenarios, but corrections correct bias and can increase power. Conclusions: The IE design can be made more cost-efficient and robust to loss-of-signal. Unintended effects will not typically reduce the power gain over the standard trial design. Non-compliance with control-arm specimen collections can cause bias and loss of power that can be mitigated by corrections. Although promising, practical experience with the IE design in screening trials is necessary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05580v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hormuzd A. Katki, Li C. Cheung</dc:creator>
    </item>
    <item>
      <title>Cast vote records: A database of ballots from the 2020 U.S. Election</title>
      <link>https://arxiv.org/abs/2411.05020</link>
      <description>arXiv:2411.05020v1 Announce Type: cross 
Abstract: Ballots are the core records of elections. Electronic records of actual ballots cast (cast vote records) are available to the public in some jurisdictions. However, they have been released in a variety of formats and have not been independently evaluated. Here we introduce a database of cast vote records from the 2020 U.S. general election. We downloaded publicly available unstandardized cast vote records, standardized them into a multi-state database, and extensively compared their totals to certified election results. Our release includes vote records for President, Governor, U.S. Senate and House, and state upper and lower chambers -- covering 42.7 million voters in 20 states who voted for more than 2,204 candidates. This database serves as a uniquely granular administrative dataset for studying voting behavior and election administration. Using this data, we show that in battleground states, 1.9 percent of solid Republicans (as defined by their congressional and state legislative voting) in our database split their ticket for Joe Biden, while 1.2 percent of solid Democrats split their ticket for Donald Trump.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05020v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shiro Kuriwaki (Yale University, Institution for Social and Policy Studies), Mason Reece (Massachusetts Institute of Technology, Department of Political Science), Samuel Baltz (Massachusetts Institute of Technology, Department of Political Science), Aleksandra Conevska (Harvard University, Department of Government), Joseph R. Loffredo (Massachusetts Institute of Technology, Department of Political Science), Can Mutlu (Harvard University, Department of Government), Taran Samarth (Yale University, Institution for Social and Policy Studies), Kevin E. Acevedo Jetter (Massachusetts Institute of Technology, Department of Political Science), Zachary Djanogly Garai (Massachusetts Institute of Technology, Department of Political Science), Kate Murray (Massachusetts Institute of Technology, Department of Political Science), Shigeo Hirano (Columbia University, Department of Political Science), Jeffrey B. Lewis (University of California Los Angeles, Department of Political Science), James M. Snyder Jr. (Harvard University, Department of Government), Charles H. Stewart III (Massachusetts Institute of Technology, Department of Political Science)</dc:creator>
    </item>
    <item>
      <title>Anticipatory Understanding of Resilient Agriculture to Climate</title>
      <link>https://arxiv.org/abs/2411.05219</link>
      <description>arXiv:2411.05219v1 Announce Type: cross 
Abstract: With billions of people facing moderate or severe food insecurity, the resilience of the global food supply will be of increasing concern due to the effects of climate change and geopolitical events. In this paper we describe a framework to better identify food security hotspots using a combination of remote sensing, deep learning, crop yield modeling, and causal modeling of the food distribution system. While we feel that the methods are adaptable to other regions of the world, we focus our analysis on the wheat breadbasket of northern India, which supplies a large percentage of the world's population. We present a quantitative analysis of deep learning domain adaptation methods for wheat farm identification based on curated remote sensing data from France. We model climate change impacts on crop yields using the existing crop yield modeling tool WOFOST and we identify key drivers of crop simulation error using a longitudinal penalized functional regression. A description of a system dynamics model of the food distribution system in India is also presented, along with results of food insecurity identification based on seeding this model with the predicted crop yields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05219v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David Willmes, Nick Krall, James Tanis, Zachary Terner, Fernando Tavares, Joe Haberlin III, Matt Crichton, Alexander Schlichting</dc:creator>
    </item>
    <item>
      <title>Pruning the Path to Optimal Care: Identifying Systematically Suboptimal Medical Decision-Making with Inverse Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2411.05237</link>
      <description>arXiv:2411.05237v1 Announce Type: cross 
Abstract: In aims to uncover insights into medical decision-making embedded within observational data from clinical settings, we present a novel application of Inverse Reinforcement Learning (IRL) that identifies suboptimal clinician actions based on the actions of their peers. This approach centers two stages of IRL with an intermediate step to prune trajectories displaying behavior that deviates significantly from the consensus. This enables us to effectively identify clinical priorities and values from ICU data containing both optimal and suboptimal clinician decisions. We observe that the benefits of removing suboptimal actions vary by disease and differentially impact certain demographic groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05237v1</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Inko Bovenzi, Adi Carmel, Michael Hu, Rebecca M. Hurwitz, Fiona McBride, Leo Benac, Jos\'e Roberto Tello Ayala, Finale Doshi-Velez</dc:creator>
    </item>
    <item>
      <title>Caliper Synthetic Matching: Generalized Radius Matching with Local Synthetic Controls</title>
      <link>https://arxiv.org/abs/2411.05246</link>
      <description>arXiv:2411.05246v1 Announce Type: cross 
Abstract: Matching promises transparent causal inferences for observational data, making it an intuitive approach for many applications. In practice, however, standard matching methods often perform poorly compared to modern approaches such as response-surface modeling and optimizing balancing weights. We propose Caliper Synthetic Matching (CSM) to address these challenges while preserving simple and transparent matches and match diagnostics. CSM extends Coarsened Exact Matching by incorporating general distance metrics, adaptive calipers, and locally constructed synthetic controls. We show that CSM can be viewed as a monotonic imbalance bounding matching method, so that it inherits the usual bounds on imbalance and bias enjoyed by MIB methods. We further provide a bound on a measure of joint covariate imbalance. Using a simulation study, we illustrate how CSM can even outperform modern matching methods in certain settings, and finally illustrate its use in an empirical example. Overall, we find CSM allows for many of the benefits of matching while avoiding some of the costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05246v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Che, Xiang Meng, Luke Miratrix</dc:creator>
    </item>
    <item>
      <title>Post-Hoc Robustness Enhancement in Graph Neural Networks with Conditional Random Fields</title>
      <link>https://arxiv.org/abs/2411.05399</link>
      <description>arXiv:2411.05399v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs), which are nowadays the benchmark approach in graph representation learning, have been shown to be vulnerable to adversarial attacks, raising concerns about their real-world applicability. While existing defense techniques primarily concentrate on the training phase of GNNs, involving adjustments to message passing architectures or pre-processing methods, there is a noticeable gap in methods focusing on increasing robustness during inference. In this context, this study introduces RobustCRF, a post-hoc approach aiming to enhance the robustness of GNNs at the inference stage. Our proposed method, founded on statistical relational learning using a Conditional Random Field, is model-agnostic and does not require prior knowledge about the underlying model architecture. We validate the efficacy of this approach across various models, leveraging benchmark node classification datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05399v1</guid>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yassine Abbahaddou, Sofiane Ennadir, Johannes F. Lutzeyer, Fragkiskos D. Malliaros, Michalis Vazirgiannis</dc:creator>
    </item>
    <item>
      <title>Cities Reconceptualized: Unveiling Hidden Uniform Urban Shape through Commute Flow Modeling in Major US Cities</title>
      <link>https://arxiv.org/abs/2411.05455</link>
      <description>arXiv:2411.05455v1 Announce Type: cross 
Abstract: Urban development is shaped by historical, geographical, and economic factors, presenting challenges for planners in understanding urban form. This study models commute flows across multiple U.S. cities, uncovering consistent patterns in urban population distributions and commuting behaviors. By embedding urban locations to reflect mobility networks, we observe that population distributions across redefined urban spaces tend to approximate log-normal distributions, in contrast to the often irregular distributions found in geographical space. This divergence suggests that natural and historical constraints shape spatial population patterns, while, under ideal conditions, urban organization may naturally align with log-normal distribution. A theoretical model using preferential attachment and random walks supports the emergence of this distribution in urban settings. These findings reveal a fundamental organizing principle in urban systems that, while not always visible geographically, consistently governs population flows and distributions. This insight into the underlying urban structure can inform planners seeking to design efficient, resilient cities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05455v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Margarita Mishina, Mingyi He, Venu Garikapati, Stanislav Sobolevsky</dc:creator>
    </item>
    <item>
      <title>Mitigating Consequences of Prestige in Citations of Publications</title>
      <link>https://arxiv.org/abs/2411.05584</link>
      <description>arXiv:2411.05584v1 Announce Type: cross 
Abstract: For many public research organizations, funding creation of science and maximizing scientific output is of central interest. Typically, when evaluating scientific production for funding, citations are utilized as a proxy, although these are severely influenced by factors beyond scientific impact. This study aims to mitigate the consequences of the Matthew effect in citations, where prominent authors and prestigious journals receive more citations regardless of the scientific content of the publications. To this end, the study presents an approach to predicting citations of papers based solely on observable characteristics available at the submission stage of a double-blind peer-review process. Combining classical linear models, generalized linear models and utilizing large-scale data sets on biomedical papers based on the PubMed database, the results demonstrate that it is possible to make fairly accurate predictions of citations using only observable characteristics of papers excluding information on authors and journals, thereby mitigating the Matthew effect. Thus, the outcomes have important implications for the field of scientometrics, providing a more objective method for citation prediction by relying on pre-publication variables that are immune to manipulation by authors and journals, thereby enhancing the objectivity of the evaluation process. Our approach is thus important for government agencies responsible for funding the creation of high-quality scientific content rather than perpetuating prestige.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05584v1</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Balzer, Adhen Benlahlou</dc:creator>
    </item>
    <item>
      <title>Torus Probabilistic Principal Component Analysis</title>
      <link>https://arxiv.org/abs/2008.10725</link>
      <description>arXiv:2008.10725v2 Announce Type: replace 
Abstract: Analyzing data in non-Euclidean spaces, such as bioinformatics, biology, and geology, where variables represent directions or angles, poses unique challenges. This type of data is known as circular data in univariate cases and can be termed spherical or toroidal in multivariate contexts. In this paper, we introduce a novel extension of Probabilistic Principal Component Analysis (PPCA) designed for toroidal (or torus) data, termed Torus Probabilistic PCA (TPPCA). We provide detailed algorithms for implementing TPPCA and demonstrate its applicability to torus data. To assess the efficacy of TPPCA, we perform comparative analyses using a simulation study and three real datasets. Our findings highlight the advantages and limitations of TPPCA in handling torus data. Furthermore, we propose statistical tests based on likelihood ratio statistics to determine the optimal number of components, enhancing the practical utility of TPPCA for real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2008.10725v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anahita Nodehi, Mousa Golalizadeh, Mehdi Maadooliat, Claudio Agostinelli</dc:creator>
    </item>
    <item>
      <title>TCKAN:A Novel Integrated Network Model for Predicting Mortality Risk in Sepsis Patients</title>
      <link>https://arxiv.org/abs/2407.06560</link>
      <description>arXiv:2407.06560v2 Announce Type: replace 
Abstract: Sepsis poses a major global health threat, accounting for millions of deaths annually and significant economic costs. Accurately predicting the risk of mortality in sepsis patients enables early identification, promotes the efficient allocation of medical resources, and facilitates timely interventions, thereby improving patient outcomes. Current methods typically utilize only one type of data--either constant, temporal, or ICD codes. This study introduces a novel approach, the Time-Constant Kolmogorov-Arnold Network (TCKAN), which uniquely integrates temporal data, constant data, and ICD codes within a single predictive model. Unlike existing methods that typically rely on one type of data, TCKAN leverages a multi-modal data integration strategy, resulting in superior predictive accuracy and robustness in identifying high-risk sepsis patients. Validated against the MIMIC-III and MIMIC-IV datasets, TCKAN surpasses existing machine learning and deep learning methods in accuracy, sensitivity, and specificity. Notably, TCKAN achieved AUCs of 87.76% and 88.07%, demonstrating superior capability in identifying high-risk patients. Additionally, TCKAN effectively combats the prevalent issue of data imbalance in clinical settings, improving the detection of patients at elevated risk of mortality and facilitating timely interventions. These results confirm the model's effectiveness and its potential to transform patient management and treatment optimization in clinical practice. Although the TCKAN model has already incorporated temporal, constant, and ICD code data, future research could include more diverse medical data types, such as imaging and laboratory test results, to achieve a more comprehensive data integration and further improve predictive accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06560v2</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fanglin Dong</dc:creator>
    </item>
    <item>
      <title>Multi-time small-area estimation of oil and gas production capacity by Bayesian multilevel modeling</title>
      <link>https://arxiv.org/abs/2408.11167</link>
      <description>arXiv:2408.11167v4 Announce Type: replace 
Abstract: This paper presents a Bayesian multilevel modeling approach for estimating well-level oil and gas production capacities across small geographic areas over multiple time periods. Focusing on a basin, which is a geologically and economically distinct drilling region, we model the production level of wells grouped by area and time, using priors as regulators of inferences. Our model accounts for area-level and time-level variations as well as well-level variations, incorporating lateral length, water usage, and sand usage. The Maidenhead Coordinate System is used to define uniform (small) geographic areas, many of which contain only a small number of wells in a given time period. The Bayesian small-area model is first built and checked, using data from the Bakken region, covering from 21 February 2012 to 12 June 2024. The model is expanded to accommodate temporal dynamics by introducing time-effect components, allowing for the analysis of production trends over times. We explore the impact of technological advancements by modeling water-sand intensity as a proxy for production efficiency. The Bayesian multilevel modeling approach provides a robust and flexible tool for modeling oil or/and gas production at area and time levels, informing the energy production prediction with uncertainties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11167v4</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroaki Minato</dc:creator>
    </item>
    <item>
      <title>Pinpointing Important Genetic Variants via A Feature-level Filter and Group Knockoffs</title>
      <link>https://arxiv.org/abs/2408.12618</link>
      <description>arXiv:2408.12618v2 Announce Type: replace 
Abstract: Identifying variants that carry substantial information on the trait of interest remains a core topic in genetic studies. In analyzing the EADB-UKBB dataset to identify genetic variants associated with Alzheimer's disease (AD), however, we recognize that both existing marginal association tests and conditional independence tests using knockoffs suffer either power loss or lack of informativeness, especially when strong correlations exist among variants. To address the limitations of existing knockoff filters, we propose a new feature-versus-group (FVG) filter that is more powerful and precise in identifying important features from a set of strongly correlated features using group knockoffs. In extensive simulation studies, the FVG filter controls the expected proportion of false discoveries and identifies important features with enhanced power and greater precision. Applying the proposed method to the EADB-UKBB dataset, we discover important variants from 84 loci (same as the most powerful group knockoff filter) with catching sets of substantially smaller size and higher purity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12618v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaqi Gu, Zhaomeng Chen, Zihuai He</dc:creator>
    </item>
    <item>
      <title>Generating Synthetic Functional Data for Privacy-Preserving GPS Trajectories</title>
      <link>https://arxiv.org/abs/2410.12514</link>
      <description>arXiv:2410.12514v2 Announce Type: replace 
Abstract: This research presents FDASynthesis, a novel algorithm designed to generate synthetic GPS trajectory data while preserving privacy. After pre-processing the input GPS data, human mobility traces are modeled as multidimensional curves using Functional Data Analysis (FDA). Then, the synthesis process identifies the K-nearest trajectories and averages their Square-Root Velocity Functions (SRVFs) to generate synthetic data. This results in synthetic trajectories that maintain the utility of the original data while ensuring privacy. Although applied for human mobility research, FDASynthesis is highly adaptable to different types of functional data, offering a scalable solution in various application domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12514v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arianna Burzacchi, Lise Bellanger, Klervi Le Gall, Aymeric Stamm, Simone Vantini</dc:creator>
    </item>
    <item>
      <title>Meta-models for transfer learning in source localisation</title>
      <link>https://arxiv.org/abs/2305.08657</link>
      <description>arXiv:2305.08657v2 Announce Type: replace-cross 
Abstract: In practice, non-destructive testing (NDT) procedures tend to consider experiments (and their respective models) as distinct, conducted in isolation and associated with independent data. In contrast, this work looks to capture the interdependencies between acoustic emission (AE) experiments (as meta-models) and then use the resulting functions to predict the model hyperparameters for previously unobserved systems. We utilise a Bayesian multilevel approach (similar to deep Gaussian Processes) where a higher level meta-model captures the inter-task relationships. Our key contribution is how knowledge of the experimental campaign can be encoded between tasks as well as within tasks. We present an example of AE time-of-arrival mapping for source localisation, to illustrate how multilevel models naturally lend themselves to representing aggregate systems in engineering. We constrain the meta-model based on domain knowledge, then use the inter-task functions for transfer learning, predicting hyperparameters for models of previously unobserved experiments (for a specific design).</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.08657v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lawrence A. Bull, Matthew R. Jones, Elizabeth J. Cross, Andrew Duncan, Mark Girolami</dc:creator>
    </item>
    <item>
      <title>A constructive approach to selective risk control</title>
      <link>https://arxiv.org/abs/2401.16651</link>
      <description>arXiv:2401.16651v2 Announce Type: replace-cross 
Abstract: Many modern applications require using data to select the statistical tasks and make valid inference after selection. In this article, we provide a unifying approach to control for a class of selective risks. Our method is motivated by a reformulation of the celebrated Benjamini-Hochberg (BH) procedure for multiple hypothesis testing as the fixed point iteration of the Benjamini-Yekutieli (BY) procedure for constructing post-selection confidence intervals. Building on this observation, we propose a constructive approach to control extra-selection risk (where selection is made after decision) by iterating decision strategies that control the post-selection risk (where decision is made after selection). We show that many previous methods and results are special cases of this general framework, and we further extend this approach to problems with multiple selective risks. Our development leads to two surprising results about the BH procedure: (1) in the context of one-sided location testing, the BH procedure not only controls the false discovery rate at the null but also at other locations for free; (2) in the context of permutation tests, the BH procedure with exact permutation p-values can be well approximated by a procedure which only requires a total number of permutations that is almost linear in the total number of hypotheses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16651v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zijun Gao, Wenjie Hu, Qingyuan Zhao</dc:creator>
    </item>
    <item>
      <title>Unmasking the Role of Remote Sensors in Comfort, Energy and Demand Response</title>
      <link>https://arxiv.org/abs/2404.15368</link>
      <description>arXiv:2404.15368v2 Announce Type: replace-cross 
Abstract: In single-zone multi-node systems (SZMRSs), temperature controls rely on a single probe near the thermostat, resulting in temperature discrepancies that cause thermal discomfort and energy waste. Augmenting smart thermostats (STs) with per-room sensors has gained acceptance by major ST manufacturers. This paper leverages additional sensory information to empirically characterize the services provided by buildings, including thermal comfort, energy efficiency, and demand response (DR). Utilizing room-level time-series data from 1,000 houses, metadata from 110,000 houses across the United States, and data from two real-world testbeds, we examine the limitations of SZMNSs and explore the potential of remote sensors. We discovered that comfortable DR durations (CDRDs) for rooms are typically 70% longer or 40% shorter than for the room with the thermostat. When averaging, rooms at the control temperature's bounds are typically deviated around -3{\deg}F to 2.5{\deg}F from the average. Moreover, in 95% of houses, we identified rooms experiencing notably higher solar gains compared to the rest of the rooms, while 85% and 70% of houses demonstrated lower heat input and poor insulation, respectively. Lastly, it became evident that the consumption of cooling energy escalates with the increase in the number of sensors, whereas heating usage experiences fluctuations ranging from -19% to +25%. This study serves as a benchmark for assessing the thermal comfort and DR services in the existing housing stock, while also highlighting the energy efficiency impacts of sensing technologies. Our approach sets the stage for more granular, precise control strategies of SZMNSs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15368v2</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 11 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1017/dce.2024.25</arxiv:DOI>
      <arxiv:journal_reference>Data-Centric Engineering, 5, e28 (2024)</arxiv:journal_reference>
      <dc:creator>Ozan Baris Mulayim, Edson Severnini, Mario Berg\'es</dc:creator>
    </item>
  </channel>
</rss>
