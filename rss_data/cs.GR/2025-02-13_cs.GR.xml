<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Feb 2025 02:49:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Interactive Holographic Visualization for 3D Facial Avatar</title>
      <link>https://arxiv.org/abs/2502.08085</link>
      <description>arXiv:2502.08085v1 Announce Type: new 
Abstract: Traditional methods for visualizing dynamic human expressions, particularly in medical training, often rely on flat-screen displays or static mannequins, which have proven inefficient for realistic simulation. In response, we propose a platform that leverages a 3D interactive facial avatar capable of displaying non-verbal feedback, including pain signals. This avatar is projected onto a stereoscopic, view-dependent 3D display, offering a more immersive and realistic simulated patient experience for pain assessment practice. However, there is no existing solution that dynamically predicts and projects interactive 3D facial avatars in real-time. To overcome this, we emphasize the need for a 3D display projection system that can project the facial avatar holographically, allowing users to interact with the avatar from any viewpoint. By incorporating 3D Gaussian Splatting (3DGS) and real-time view-dependent calibration, we significantly improve the training environment for accurate pain recognition and assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08085v1</guid>
      <category>cs.GR</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tri Tung Nguyen Nguyen, Fujii Yasuyuki, Dinh Tuan Tran, Joo-Ho Lee</dc:creator>
    </item>
    <item>
      <title>Machine Learning-Driven Volumetric Cloud Rendering: Procedural Shader Optimization and Dynamic Lighting in Unreal Engine for Realistic Atmospheric Simulation</title>
      <link>https://arxiv.org/abs/2502.08107</link>
      <description>arXiv:2502.08107v1 Announce Type: new 
Abstract: This study advances real-time volumetric cloud rendering in Computer Graphics (CG) by developing a specialized shader in Unreal Engine (UE), focusing on realistic cloud modeling and lighting. By leveraging ray-casting-based lighting algorithms, this work demonstrates the practical application of a dual-layered procedural noise model, eliminating the need for conventional two-dimensional (2D) weather textures. The shader allows for procedurally configured skies with a defined parameter set, offering flexibility for both artistic expression and realistic simulation. Empirical results reveal that the shader achieves an average rendering time of 35ms per frame while maintaining high visual accuracy and scene realism. Visual fidelity assessments indicate a 15% improvement in cloud realism over traditional 2D techniques, particularly in dynamic lighting scenarios. This research contributes to CG by bridging technical and aesthetic elements, enhancing real-time visual storytelling and immersion within gigital media environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08107v1</guid>
      <category>cs.GR</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shruti Singh, Shantanu Kumar</dc:creator>
    </item>
    <item>
      <title>BEAM: Bridging Physically-based Rendering and Gaussian Modeling for Relightable Volumetric Video</title>
      <link>https://arxiv.org/abs/2502.08297</link>
      <description>arXiv:2502.08297v1 Announce Type: new 
Abstract: Volumetric video enables immersive experiences by capturing dynamic 3D scenes, enabling diverse applications for virtual reality, education, and telepresence. However, traditional methods struggle with fixed lighting conditions, while neural approaches face trade-offs in efficiency, quality, or adaptability for relightable scenarios. To address these limitations, we present BEAM, a novel pipeline that bridges 4D Gaussian representations with physically-based rendering (PBR) to produce high-quality, relightable volumetric videos from multi-view RGB footage. BEAM recovers detailed geometry and PBR properties via a series of available Gaussian-based techniques. It first combines Gaussian-based performance tracking with geometry-aware rasterization in a coarse-to-fine optimization framework to recover spatially and temporally consistent geometries. We further enhance Gaussian attributes by incorporating PBR properties step by step. We generate roughness via a multi-view-conditioned diffusion model, and then derive AO and base color using a 2D-to-3D strategy, incorporating a tailored Gaussian-based ray tracer for efficient visibility computation. Once recovered, these dynamic, relightable assets integrate seamlessly into traditional CG pipelines, supporting real-time rendering with deferred shading and offline rendering with ray tracing. By offering realistic, lifelike visualizations under diverse lighting conditions, BEAM opens new possibilities for interactive entertainment, storytelling, and creative visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08297v1</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Hong, Yize Wu, Zhehao Shen, Chengcheng Guo, Yuheng Jiang, Yingliang Zhang, Jingyi Yu, Lan Xu</dc:creator>
    </item>
    <item>
      <title>Movie Weaver: Tuning-Free Multi-Concept Video Personalization with Anchored Prompts</title>
      <link>https://arxiv.org/abs/2502.07802</link>
      <description>arXiv:2502.07802v1 Announce Type: cross 
Abstract: Video personalization, which generates customized videos using reference images, has gained significant attention. However, prior methods typically focus on single-concept personalization, limiting broader applications that require multi-concept integration. Attempts to extend these models to multiple concepts often lead to identity blending, which results in composite characters with fused attributes from multiple sources. This challenge arises due to the lack of a mechanism to link each concept with its specific reference image. We address this with anchored prompts, which embed image anchors as unique tokens within text prompts, guiding accurate referencing during generation. Additionally, we introduce concept embeddings to encode the order of reference images. Our approach, Movie Weaver, seamlessly weaves multiple concepts-including face, body, and animal images-into one video, allowing flexible combinations in a single model. The evaluation shows that Movie Weaver outperforms existing methods for multi-concept video personalization in identity preservation and overall quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07802v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Feng Liang, Haoyu Ma, Zecheng He, Tingbo Hou, Ji Hou, Kunpeng Li, Xiaoliang Dai, Felix Juefei-Xu, Samaneh Azadi, Animesh Sinha, Peizhao Zhang, Peter Vajda, Diana Marculescu</dc:creator>
    </item>
    <item>
      <title>"You'll Be Alice Adventuring in Wonderland!" Processes, Challenges, and Opportunities of Creating Animated Virtual Reality Stories</title>
      <link>https://arxiv.org/abs/2502.08513</link>
      <description>arXiv:2502.08513v1 Announce Type: cross 
Abstract: Animated virtual reality (VR) stories, combining the presence of VR and the artistry of computer animation, offer a compelling way to deliver messages and evoke emotions. Motivated by the growing demand for immersive narrative experiences, more creators are creating animated VR stories. However, a holistic understanding of their creation processes and challenges involved in crafting these stories is still limited. Based on semi-structured interviews with 21 animated VR story creators, we identify ten common stages in their end-to-end creation processes, ranging from idea generation to evaluation, which form diverse workflows that are story-driven or visual-driven. Additionally, we highlight nine unique issues that arise during the creation process, such as a lack of reference material for multi-element plots, the absence of specific functionalities for story integration, and inadequate support for audience evaluation. We compare the creation of animated VR stories to general XR applications and distill several future research opportunities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08513v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <category>cs.MM</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3714257</arxiv:DOI>
      <dc:creator>Lin-Ping Yuan, Feilin Han, Liwenhan Xie, Junjie Zhang, Jian Zhao, Huamin Qu</dc:creator>
    </item>
    <item>
      <title>Multi-directional Backlighting Compressive Light Field Displays</title>
      <link>https://arxiv.org/abs/2502.07413</link>
      <description>arXiv:2502.07413v2 Announce Type: replace-cross 
Abstract: We propose a compressive light field display of a wide viewing angle with a multi-directional backlight. Displayed layer images of sub-viewing zones are synchronized with the multi-directional backlight. Viewers can perceive a three-dimensional scene with a large viewing angle based on the persistence of vision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07413v2</guid>
      <category>physics.optics</category>
      <category>cs.GR</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Gao, Sheng Xu, Yun Ye, Enguo Chen</dc:creator>
    </item>
  </channel>
</rss>
