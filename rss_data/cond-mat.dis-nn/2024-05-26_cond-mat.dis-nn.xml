<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 May 2024 04:07:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 27 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Hybrid scaling theory of localization transition in a non-Hermitian disorder Aubry-Andr\'{e} model</title>
      <link>https://arxiv.org/abs/2405.15220</link>
      <description>arXiv:2405.15220v1 Announce Type: new 
Abstract: In this paper, we study the critical behaviors in the non-Hermtian disorder Aubry-Andr\'{e} (DAA) model, and we assume the non-Hermiticity is introduced by the nonreciprocal hopping. We employ the localization length $\xi$, the inverse participation ratio ($\rm IPR$), and the real part of the energy gap between the first excited state and the ground state $\Delta E$ as the character quantities to describe the critical properties of the localization transition. By preforming the scaling analysis, the critical exponents of the non-Hermitian Anderson model and the non-Hermitian DAA model are obtained, and these critical exponents are different from their Hermitian counterparts, indicating the Hermitian and non-Hermitian disorder and DAA models belong to different universe classes. The critical exponents of non-Hermitian DAA model are remarkably different from both the pure non-Hermitian AA model and the non-Hermitian Anderson model, showing that disorder is a independent relevant direction at the non-Hermitian AA model. We further propose a hybrid scaling theory to describe the critical behavior in the overlapping critical region constituted by the critical regions of non-Hermitian DAA model and the non-Hermitian Anderson localization transition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15220v1</guid>
      <category>cond-mat.dis-nn</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue-Mei Sun, Xin-Yu Wang, Zi-Kang Wang, Liang-Jun Zhai</dc:creator>
    </item>
    <item>
      <title>Slow measurement-only dynamics of entanglement in Pauli subsystem codes</title>
      <link>https://arxiv.org/abs/2405.14927</link>
      <description>arXiv:2405.14927v1 Announce Type: cross 
Abstract: We study the non-unitary dynamics of a class of quantum circuits based on stochastically measuring check operators of subsystem quantum error-correcting codes, such as the Bacon-Shor code and its various generalizations. Our focus is on how properties of the underlying code are imprinted onto the measurement-only dynamics. We find that in a large class of codes with nonlocal stabilizer generators, at late times there is generically a nonlocal contribution to the subsystem entanglement entropy which scales with the subsystem size. The nonlocal stabilizer generators can also induce slow dynamics, since depending on the rate of competing measurements the associated degrees of freedom can take exponentially long (in system size) to purify (disentangle from the environment when starting from a mixed state) and to scramble (become entangled with the rest of the system when starting from a product state). Concretely, we consider circuits for which the nonlocal stabilizer generators of the underlying subsystem code take the form of subsystem symmetries. We present a systematic study of the phase diagrams and relevant time scales in two and three spatial dimensions for both Calderbank-Shor-Steane (CSS) and non-CSS codes, focusing in particular on the link between slow measurement-only dynamics and the geometry of the subsystem symmetry. A key finding of our work is that slowly purifying or scrambling degrees of freedom appear to emerge only in codes whose subsystem symmetries are nonlocally {\it generated}, a strict subset of those whose symmetries are simply nonlocal. We comment on the link between our results on subsystem codes and the phenomenon of Hilbert-space fragmentation in light of their shared algebraic structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14927v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.str-el</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benedikt Placke, S. A. Parameswaran</dc:creator>
    </item>
    <item>
      <title>Local and nonlocal stochastic control of quantum chaos: Measurement- and control-induced criticality</title>
      <link>https://arxiv.org/abs/2405.14936</link>
      <description>arXiv:2405.14936v1 Announce Type: cross 
Abstract: We theoretically study the topology of the phase diagram of a family of quantum models inspired by the classical Bernoulli map under stochastic control. The quantum models inherit a control-induced phase transition from the classical model and also manifest an entanglement phase transition intrinsic to the quantum setting. This measurement-induced phase transition has been shown in various settings to either coincide or split off from the control transition, but a systematic understanding of the necessary and sufficient conditions for the two transitions to coincide in this case has so far been lacking. In this work, we generalize the control map to allow for either local or global control action. While this does not affect the classical aspects of the control transition that is described by a random walk, it significantly influences the quantum dynamics, leading to the universality class of the measurement-induced transition being dependent on the locality of the control operation. In the presence of a global control map, the two transitions coincide and the control-induced phase transition dominates the measurement-induced phase transition. Contrarily, the two transitions split in the presence of the local control map or additional projective measurements and generically take on distinct universality classes. For local control, the measurement-induced phase transition recovers the Haar logarithmic conformal field theory universality class found in feedback-free models. However, for global control, a novel universality class with correlation length exponent $\nu \approx 0.7$ emerges from the interplay of control and projective measurements. This work provides a more refined understanding of the relationship between the control- and measurement-induced phase transitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14936v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>nlin.CD</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Haining Pan, Sriram Ganeshan, Thomas Iadecola, Justin H. Wilson, J. H. Pixley</dc:creator>
    </item>
    <item>
      <title>Fast, accurate training and sampling of Restricted Boltzmann Machines</title>
      <link>https://arxiv.org/abs/2405.15376</link>
      <description>arXiv:2405.15376v1 Announce Type: cross 
Abstract: Thanks to their simple architecture, Restricted Boltzmann Machines (RBMs) are powerful tools for modeling complex systems and extracting interpretable insights from data. However, training RBMs, as other energy-based models, on highly structured data poses a major challenge, as effective training relies on mixing the Markov chain Monte Carlo simulations used to estimate the gradient. This process is often hindered by multiple second-order phase transitions and the associated critical slowdown. In this paper, we present an innovative method in which the principal directions of the dataset are integrated into a low-rank RBM through a convex optimization procedure. This approach enables efficient sampling of the equilibrium measure via a static Monte Carlo process. By starting the standard training process with a model that already accurately represents the main modes of the data, we bypass the initial phase transitions. Our results show that this strategy successfully trains RBMs to capture the full diversity of data in datasets where previous methods fail. Furthermore, we use the training trajectories to propose a new sampling method, {\em parallel trajectory tempering}, which allows us to sample the equilibrium measure of the trained model much faster than previous optimized MCMC approaches and a better estimation of the log-likelihood. We illustrate the success of the training method on several highly structured datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15376v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas B\'ereux, Aur\'elien Decelle, Cyril Furtlehner, Lorenzo Rosset, Beatriz Seoane</dc:creator>
    </item>
    <item>
      <title>Fundamental limits of weak learnability in high-dimensional multi-index models</title>
      <link>https://arxiv.org/abs/2405.15480</link>
      <description>arXiv:2405.15480v1 Announce Type: cross 
Abstract: Multi-index models -- functions which only depend on the covariates through a non-linear transformation of their projection on a subspace -- are a useful benchmark for investigating feature learning with neural networks. This paper examines the theoretical boundaries of learnability in this hypothesis class, focusing particularly on the minimum sample complexity required for weakly recovering their low-dimensional structure with first-order iterative algorithms, in the high-dimensional regime where the number of samples is $n=\alpha d$ is proportional to the covariate dimension $d$. Our findings unfold in three parts: (i) first, we identify under which conditions a \textit{trivial subspace} can be learned with a single step of a first-order algorithm for any $\alpha\!&gt;\!0$; (ii) second, in the case where the trivial subspace is empty, we provide necessary and sufficient conditions for the existence of an {\it easy subspace} consisting of directions that can be learned only above a certain sample complexity $\alpha\!&gt;\!\alpha_c$. The critical threshold $\alpha_{c}$ marks the presence of a computational phase transition, in the sense that no efficient iterative algorithm can succeed for $\alpha\!&lt;\!\alpha_c$. In a limited but interesting set of really hard directions -- akin to the parity problem -- $\alpha_c$ is found to diverge. Finally, (iii) we demonstrate that interactions between different directions can result in an intricate hierarchical learning phenomenon, where some directions can be learned sequentially when coupled to easier ones. Our analytical approach is built on the optimality of approximate message-passing algorithms among first-order iterative methods, delineating the fundamental learnability limit across a broad spectrum of algorithms, including neural networks trained with gradient descent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15480v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.CC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emanuele Troiani, Yatin Dandi, Leonardo Defilippis, Lenka Zdeborov\'a, Bruno Loureiro, Florent Krzakala</dc:creator>
    </item>
    <item>
      <title>A generalized neural tangent kernel for surrogate gradient learning</title>
      <link>https://arxiv.org/abs/2405.15539</link>
      <description>arXiv:2405.15539v1 Announce Type: cross 
Abstract: State-of-the-art neural network training methods depend on the gradient of the network function. Therefore, they cannot be applied to networks whose activation functions do not have useful derivatives, such as binary and discrete-time spiking neural networks. To overcome this problem, the activation function's derivative is commonly substituted with a surrogate derivative, giving rise to surrogate gradient learning (SGL). This method works well in practice but lacks theoretical foundation. The neural tangent kernel (NTK) has proven successful in the analysis of gradient descent. Here, we provide a generalization of the NTK, which we call the surrogate gradient NTK, that enables the analysis of SGL. First, we study a naive extension of the NTK to activation functions with jumps, demonstrating that gradient descent for such activation functions is also ill-posed in the infinite-width limit. To address this problem, we generalize the NTK to gradient descent with surrogate derivatives, i.e., SGL. We carefully define this generalization and expand the existing key theorems on the NTK with mathematical rigor. Further, we illustrate our findings with numerical experiments. Finally, we numerically compare SGL in networks with sign activation function and finite width to kernel regression with the surrogate gradient NTK; the results confirm that the surrogate gradient NTK provides a good characterization of SGL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15539v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Eilers, Raoul-Martin Memmesheimer, Sven Goedeke</dc:creator>
    </item>
    <item>
      <title>Super-diffusive transport in two-dimensional Fermionic wires</title>
      <link>https://arxiv.org/abs/2405.15560</link>
      <description>arXiv:2405.15560v1 Announce Type: cross 
Abstract: We consider a two-dimensional model of a Fermionic wire in contact with reservoirs along its two opposite edges. With the reservoirs biased around a Fermi level, $E$, we study the scaling of the conductance of the wire with its length, $L$ as the width of the wire $W\rightarrow\infty$. The wire is disordered along the direction of the transport so the conductance is expected to exponentially decay with the length of the wire. However, we show that our model shows a super-diffusive scaling ($1/L^{1/2}$) of the conductance within $|E|&lt;E_c$. This behavior is attributed to the presence of eigenstates of diverging localization length as $W\rightarrow\infty$. At $|E|=E_c$, the conductance behavior is sensitive to the disorder and scales sub-diffusively as $1/L^{3/2}$, and $1/L^{5/2}$ for zero and nonzero expectation value of the disorder. Furthermore, at this Fermi level and at certain points in the parameter space of the wire, the behavior of the conductance is also sensitive to the sign of the expectation value of the disorder. At these points we find $1/L^{7/4}$ for zero expectation value of the disorder and $1/L$, $1/L^{3}$ for different signs of the expectation value of the disorder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15560v1</guid>
      <category>cond-mat.mes-hall</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>quant-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junaid Majeed Bhat</dc:creator>
    </item>
    <item>
      <title>Dimension-free deterministic equivalents for random feature regression</title>
      <link>https://arxiv.org/abs/2405.15699</link>
      <description>arXiv:2405.15699v1 Announce Type: cross 
Abstract: In this work we investigate the generalization performance of random feature ridge regression (RFRR). Our main contribution is a general deterministic equivalent for the test error of RFRR. Specifically, under a certain concentration property, we show that the test error is well approximated by a closed-form expression that only depends on the feature map eigenvalues. Notably, our approximation guarantee is non-asymptotic, multiplicative, and independent of the feature map dimension -- allowing for infinite-dimensional features. We expect this deterministic equivalent to hold broadly beyond our theoretical analysis, and we empirically validate its predictions on various real and synthetic datasets. As an application, we derive sharp excess error rates under standard power-law assumptions of the spectrum and target decay. In particular, we provide a tight result for the smallest number of features achieving optimal minimax error rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15699v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonardo Defilippis, Bruno Loureiro, Theodor Misiakiewicz</dc:creator>
    </item>
    <item>
      <title>Infinite Limits of Multi-head Transformer Dynamics</title>
      <link>https://arxiv.org/abs/2405.15712</link>
      <description>arXiv:2405.15712v1 Announce Type: cross 
Abstract: In this work, we analyze various scaling limits of the training dynamics of transformer models in the feature learning regime. We identify the set of parameterizations that admit well-defined infinite width and depth limits, allowing the attention layers to update throughout training--a relevant notion of feature learning in these models. We then use tools from dynamical mean field theory (DMFT) to analyze various infinite limits (infinite key/query dimension, infinite heads, and infinite depth) which have different statistical descriptions depending on which infinite limit is taken and how attention layers are scaled. We provide numerical evidence of convergence to the limits and discuss how the parameterization qualitatively influences learned features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15712v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Blake Bordelon, Hamza Tahir Chaudhry, Cengiz Pehlevan</dc:creator>
    </item>
    <item>
      <title>Logarithmic critical slowing down in complex systems: from statics to dynamics</title>
      <link>https://arxiv.org/abs/2403.07565</link>
      <description>arXiv:2403.07565v4 Announce Type: replace 
Abstract: We consider second-order phase transitions in which the order parameter is a replicated overlap matrix. We focus on a tricritical point that occurs in a variety of mean-field models and that, more generically, describes higher order liquid-liquid or liquid-glass transitions. We show that the static replicated theory implies slowing down with a logarithmic decay in time. The dynamical equations turn out to be those predicted by schematic Mode Coupling Theory for supercooled viscous liquids at a $A_3$ singularity, where the parameter exponent is $\lambda=1$. We obtain a quantitative expression for the parameter $\mu$ of the logarithmic decay in terms of cumulants of the overlap, which are physically observable in experiments or numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07565v4</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cond-mat.soft</category>
      <category>cond-mat.stat-mech</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevB.109.174211</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. B 109, 174211(2024)</arxiv:journal_reference>
      <dc:creator>Luca Leuzzi, Tommaso Rizzo</dc:creator>
    </item>
    <item>
      <title>Furutsu-Novikov--like cross-correlation--response relations for systems driven by shot noise</title>
      <link>https://arxiv.org/abs/2405.13508</link>
      <description>arXiv:2405.13508v2 Announce Type: replace 
Abstract: We consider a dynamic system that is driven by an intensity-modulated Poisson process with intensity $\Lambda(t)=\lambda(t)+\varepsilon\nu(t)$. We derive an exact relation between the input-output cross-correlation in the spontaneous state ($\varepsilon=0$) and the linear response to the modulation ($\varepsilon&gt;0$). This can be regarded as a variant of the Furutsu-Novikov theorem for the case of shot noise. As we show, the relation is still valid in the presence of additional independent noise. Furthermore, we derive an extension to Cox-process input, i.e. to colored shot noise. We discuss applications to particle detection and to neuroscience. Using the new relation, we obtain a fluctuation-response-relation for a leaky integrate-and-fire neuron. We also show how the new relation can be used in a remote control problem in a recurrent neural network. The relations are numerically tested for both stationary and non-stationary dynamics. Lastly, extensions to marked Poisson processes and to higher-order statistics are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13508v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>physics.bio-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakob Stubenrauch, Benjamin Lindner</dc:creator>
    </item>
    <item>
      <title>Generating-functional analysis of random Lotka-Volterra systems: A step-by-step guide</title>
      <link>https://arxiv.org/abs/2405.14289</link>
      <description>arXiv:2405.14289v2 Announce Type: replace 
Abstract: This paper provides what is hopefully a self-contained set of notes describing the detailed steps of a generating-functional analysis of systems of generalised Lotka-Volterra equations with random interaction coefficients. Nothing in these notes is original, instead the generating-functional method (also known as the Martin-Siggia-Rose-DeDominic-Janssen formalism) and the resulting dynamic mean field theories have been used for the study of disordered systems and spin glasses for decades. But it is hard to find unifying sources which would allow a beginner to learn step-by-step how these methods can be used. My aim is to provide such a source. Most of the calculations are specific to generalised Lotka-Volterra systems, but much can be transferred to disordered systems in more general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14289v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>q-bio.PE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias Galla</dc:creator>
    </item>
    <item>
      <title>Variational Quantum Algorithms for the Allocation of Resources in a Cloud/Edge Architecture</title>
      <link>https://arxiv.org/abs/2401.14339</link>
      <description>arXiv:2401.14339v2 Announce Type: replace-cross 
Abstract: Modern Cloud/Edge architectures need to orchestrate multiple layers of heterogeneous computing nodes, including pervasive sensors/actuators, distributed Edge/Fog nodes, centralized data centers and quantum devices. The optimal assignment and scheduling of computation on the different nodes is a very difficult problem, with NP-hard complexity. In this paper, we explore the possibility of solving this problem with Variational Quantum Algorithms, which can become a viable alternative to classical algorithms in the near future. In particular, we compare the performances, in terms of success probability, of two algorithms, i.e., Quantum Approximate Optimization Algorithm (QAOA) and Variational Quantum Eigensolver (VQE). The simulation experiments, performed for a set of simple problems, %CM230124 that involve a Cloud and two Edge nodes, show that the VQE algorithm ensures better performances when it is equipped with appropriate circuit \textit{ansatzes} that are able to restrict the search space. Moreover, experiments executed on real quantum hardware show that the execution time, when increasing the size of the problem, grows much more slowly than the trend obtained with classical computation, which is known to be exponential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14339v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.other</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TQE.2024.3398410</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Quantum Engineering,2024</arxiv:journal_reference>
      <dc:creator>Carlo Mastroianni, Francesco Plastina, Jacopo Settino, Andrea Vinci</dc:creator>
    </item>
    <item>
      <title>Mean-field theory of first-order quantum superconductor-insulator transition</title>
      <link>https://arxiv.org/abs/2405.08571</link>
      <description>arXiv:2405.08571v2 Announce Type: replace-cross 
Abstract: Recent experimental studies on strongly disordered indium oxide films have revealed an unusual first-order quantum phase transition between the superconducting and insulating states (SIT). This transition is characterized by a discontinuous jump from non-zero to zero values of superfluid stiffness at the critical point, contradicting the conventional ``scaling scenario'' typically associated with SIT. In this paper, we present a theoretical framework for understanding this first-order transition. Our approach is based on the concept of competition between two fundamentally distinct ground states that arise from electron pairs initially localized by strong disorder: the superconducting state and the Coulomb glass insulator. These ground states are distinguished by two crucially different order parameters, suggesting a natural expectation of a discontinuous transition between them at $T=0$. This transition occurs when the magnitudes of the superconducting gap $\Delta$ and the Coulomb gap $E_C$ become comparable. Additionally, we extend our analysis to low non-zero temperatures and provide a mean-field ``phase diagram'' in the plane of $(T/\Delta,E_C/\Delta)$. Our results reveal the existence of a natural upper bound for the kinetic inductance of strongly disordered superconductors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08571v2</guid>
      <category>cond-mat.supr-con</category>
      <category>cond-mat.dis-nn</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Igor Poboiko, Mikhail Feigel'man</dc:creator>
    </item>
    <item>
      <title>Glassy dynamics in deep neural networks: A structural comparison</title>
      <link>https://arxiv.org/abs/2405.13098</link>
      <description>arXiv:2405.13098v2 Announce Type: replace-cross 
Abstract: Deep Neural Networks (DNNs) share important similarities with structural glasses. Both have many degrees of freedom, and their dynamics are governed by a high-dimensional, non-convex landscape representing either the loss or energy, respectively. Furthermore, both experience gradient descent dynamics subject to noise. In this work we investigate, by performing quantitative measurements on realistic networks trained on the MNIST and CIFAR-10 datasets, the extent to which this qualitative similarity gives rise to glass-like dynamics in neural networks. We demonstrate the existence of a Topology Trivialisation Transition as well as the previously studied under-to-overparameterised transition analogous to jamming. By training DNNs with overdamped Langevin dynamics in the resulting disordered phases, we do not observe diverging relaxation times at non-zero temperature, nor do we observe any caging effects, in contrast to glass phenomenology. However, the weight overlap function follows a power law in time, with an exponent of approximately -0.5, in agreement with the Mode-Coupling Theory of structural glasses. In addition, the DNN dynamics obey a form of time-temperature superposition. Finally, dynamic heterogeneity and ageing are observed at low temperatures. These results highlight important and surprising points of both difference and agreement between the behaviour of DNNs and structural glasses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13098v2</guid>
      <category>physics.comp-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Kerr Winter, Liesbeth M. C. Janssen</dc:creator>
    </item>
  </channel>
</rss>
