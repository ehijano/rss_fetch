<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Aug 2025 01:31:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Exact and Conservative Inference for the Average Treatment Effect in Stratified Experiments with Binary Outcomes</title>
      <link>https://arxiv.org/abs/2508.03834</link>
      <description>arXiv:2508.03834v1 Announce Type: new 
Abstract: We extend methods for finite-sample inference about the average treatment effect (ATE) in randomized experiments with binary outcomes to accommodate stratification (blocking). We present three valid methods that differ in their computational and statistical efficiency. The first method constructs conservative, Bonferroni-adjusted confidence intervals separately for the mean response in the treatment and control groups in each stratum, then takes appropriate weighted differences of their endpoints to find a confidence interval for the ATE. The second method inverts permutation tests for the overall ATE, maximizing the $P$-value over all ways a given ATE can be attained. The third method applies permutation tests for the ATE in separate strata, then combines those tests to form a confidence interval for the overall ATE. We compare the statistical and computational performance of the methods using simulations and a case study. The second approach is most efficient statistically in the simulations, but a naive implementation requires O(\Pi_{k=1}^{K} n_{k}^{4}) permutation tests, the highest computational burden among the three methods. That computational burden can be reduced to O(\sum_{k=1}^K n_k \times\Pi_{k=1}^{K} n_{k}^{2}) if all strata are balanced and to O(\Pi_{k=1}^{K} n_{k}^{3}) otherwise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03834v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiaxun Li, Jacob Spertus, Philip B. Stark</dc:creator>
    </item>
    <item>
      <title>The Regression Discontinuity Design in Medical Science</title>
      <link>https://arxiv.org/abs/2508.03878</link>
      <description>arXiv:2508.03878v1 Announce Type: new 
Abstract: This article provides an introduction to the Regression Discontinuity (RD) design, and its application to empirical research in the medical sciences. While the main focus of this article is on causal interpretation, key concepts of estimation and inference are also briefly mentioned. A running medical empirical example is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03878v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rocio Titiunik</dc:creator>
    </item>
    <item>
      <title>Bayesian Design of Experiments in the Presence of Nuisance Parameters</title>
      <link>https://arxiv.org/abs/2508.03948</link>
      <description>arXiv:2508.03948v1 Announce Type: new 
Abstract: Design of experiments has traditionally relied on the frequentist hypothesis testing framework where the optimal size of the experiment is specified as the minimum sample size that guarantees a required level of power. Sample size determination may be performed analytically when the test statistic has a known asymptotic sampling distribution and, therefore, the power function is available in analytic form. Bayesian methods have gained popularity in all stages of discovery, namely, design, analysis and decision making. Bayesian decision procedures rely on posterior summaries whose sampling distributions are commonly estimated via Monte Carlo simulations. In the design of scientific studies, the Bayesian approach incorporates uncertainty about the design value(s) instead of conditioning on a single value of the model parameter(s). Accounting for uncertainties in the design value(s) is particularly critical when the model includes nuisance parameters. In this manuscript, we propose methodology that utilizes the large-sample properties of the posterior distribution together with Bayesian additive regression trees (BART) to efficiently obtain the optimal sample size and decision criteria in fixed and adaptive designs. We introduce a fully Bayesian procedure that incorporates the uncertainty associated with the model parameters including the nuisance parameters at the design stage. The proposed approach significantly reduces the computational burden associated with Bayesian design and enables the wide adoption of Bayesian operating characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03948v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shirin Golchi, Luke Hagar</dc:creator>
    </item>
    <item>
      <title>The signal is not flushed away: Inferring the effective reproduction number from wastewater data in small populations</title>
      <link>https://arxiv.org/abs/2508.03959</link>
      <description>arXiv:2508.03959v1 Announce Type: new 
Abstract: The effective reproduction number is an important descriptor of an infectious disease epidemic. In small populations, ideally we would estimate the effective reproduction number using a Markov Jump Process (MJP) model of the spread of infectious disease, but in practice this is computationally challenging. We propose a computationally tractable approximation to an MJP which tracks only latent and infectious individuals, the EI model, an MJP where the time-varying immigration rate into the E compartment is equal to the product of the proportion of susceptibles in the population and the transmission rate. We use an analogue of the central limit theorem for MJPs to approximate transition densities as normal, which makes Bayesian computation tractable. Using simulated pathogen RNA concentrations collected from wastewater data, we demonstrate the advantages of our stochastic model over its deterministic counterpart for the purpose of estimating effective reproduction number dynamics, and compare against a state of the art method. We apply our new model to inference of changes in the effective reproduction number of SARS-CoV-2 in several college campus communities that were put under wastewater pathogen surveillance in 2022.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03959v1</guid>
      <category>stat.ME</category>
      <category>q-bio.PE</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isaac H. Goldstein, Daniel M. Parker, Sunny Jiang, Aiswarya Rani Pappu, Volodymyr M. Minin</dc:creator>
    </item>
    <item>
      <title>The benefit of dose-exposure-response modeling in the estimation of dose-response relationship and dose optimization: some theoretical and simulation evidence</title>
      <link>https://arxiv.org/abs/2508.04186</link>
      <description>arXiv:2508.04186v1 Announce Type: new 
Abstract: In randomized dose-finding trials, although drug exposure data form a part of key information for dose selection, the evaluation of the dose-response (DR) relationship often mainly uses DR data. We examine the benefit of dose-exposure-response (DER) modeling by sequentially modeling the dose-exposure (DE) and exposure-response (ER) relationships in parameter estimation and prediction, compared with direct DR modeling without PK data. We consider ER modeling approaches with control function (CF) that adjust for unobserved confounders in the ER relationship using randomization as an instrumental variable (IV). With both analytical derivation and a simulation study, we show that when the DE and ER models are linear, although the DER approach is moderately more efficient than the DR approach, with adjustment using CF, it has no efficiency gain (but also no loss). However, with some common ER models representing sigmoid curves, generally DER approaches with and without CF adjustment are more efficient than the DR approach. For response prediction at a given dose, the efficiency also depends on the dose level. Our simulation quantifies the benefit in multiple scenarios with different models and parameter settings. Our method can be used easily to assess the performance of randomized dose-finding trial designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04186v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jixian Wang, Zhiwei Zhang, Ram Tiwari</dc:creator>
    </item>
    <item>
      <title>Robust estimation of causal dose-response relationship using exposure data with dose as an instrumental variable</title>
      <link>https://arxiv.org/abs/2508.04215</link>
      <description>arXiv:2508.04215v1 Announce Type: new 
Abstract: An accurate estimation of the dose-response relationship is important to determine the optimal dose. For this purpose, a dose finding trial in which subjects are randomized to a few fixed dose levels is the most commonly used design. Often, the estimation uses response data only, although drug exposure data are often obtained during the trial. The use of exposure data to improve this estimation is difficult, as exposure-response relationships are typically subject to confounding bias even in a randomized trial. We propose a robust approach to estimate the dose-response relationship without assuming a true exposure-response model, using dose as an instrumental variable. Our approach combines the control variable approach in causal inference with unobserved confounding factors and the ANCOVA adjustment of randomized trials. The approach presented uses working models for dose-exposure-response data, but they are robust to model misspecification and remain consistent when the working models are far from correct. The asymptotic properties of the proposed approach are also examined. A simulation study is performed to evaluate the performance of the proposed approach. For illustration, the approach is used to a Car-T trial with randomized doses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04215v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jixian Wang, Zhiwei Zhang, Ram Tiwari</dc:creator>
    </item>
    <item>
      <title>Generative Flexible Latent Structure Regression (GFLSR) model</title>
      <link>https://arxiv.org/abs/2508.04393</link>
      <description>arXiv:2508.04393v1 Announce Type: new 
Abstract: Latent structure methods, specifically linear continuous latent structure methods, are a type of fundamental statistical learning strategy. They are widely used for dimension reduction, regression and prediction, in the fields of chemometrics, economics, social science and etc. However, due to the lack of model inference, generative form, and unidentifiable parameters, most of these methods are always used as an algorithm, instead of a model. This paper proposed a Generative Flexible Latent Structure Regression (GFLSR) model structure to address this problem. Moreover, we show that most linear continuous latent variable methods can be represented under the proposed framework. The recursive structure allows potential model inference and residual analysis. Then, the traditional Partial Least Squares (PLS) is focused; we show that the PLS can be specialised in the proposed model structure, named Generative-PLS. With a model structure, we analyse the convergence of the parameters and the latent variables. Under additional distribution assumptions, we show that the proposed model structure can lead to model inference without solving the probabilistic model. Additionally, we proposed a novel bootstrap algorithm that enables uncertainty on parameters and on prediction for new datasets. A simulation study and a Real-world dataset are used to verify the proposed Generative-PLS model structure. Although the traditional PLS is a special case, this proposed GFLSRM structure leads to a potential inference structure for all the linear continuous latent variable methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04393v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Clara Grazian, Qian Jin, Pierre Lafaye De Micheaux</dc:creator>
    </item>
    <item>
      <title>Bias in Meta-Analytic Modeling of Surrogate Endpoints in Cancer Screening Trials</title>
      <link>https://arxiv.org/abs/2508.04633</link>
      <description>arXiv:2508.04633v1 Announce Type: new 
Abstract: In meta-analytic modeling, the functional relationship between a primary and surrogate endpoint is estimated using summary data from a set of completed clinical trials. Parameters in the meta-analytic model are used to assess the quality of the proposed surrogate. Recently, meta-analytic models have been employed to evaluate whether late-stage cancer incidence can serve as a surrogate for cancer mortality in cancer screening trials. A major challenge in meta-analytic models is that uncertainty of trial-level estimates affects the evaluation of surrogacy, since each trial provides only estimates of the primary and surrogate endpoints rather than their true parameter values. In this work, we show via simulation and theory that trial-level estimate uncertainty may bias the results of meta-analytic models towards positive findings of the quality of the surrogate. We focus on cancer screening trials and the late stage incidence surrogate. We reassess correlations between primary and surrogate endpoints in Ovarian cancer screening trials. Our findings indicate that completed trials provide limited information regarding quality of the late-stage incidence surrogate. These results support restricting meta-analytic regression usage to settings where trial-level estimate uncertainty is incorporated into the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04633v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>James P. Long, Abhishikta Roy, Ehsan Irajizad, Kim-Anh Do, Yu Shen</dc:creator>
    </item>
    <item>
      <title>Accept-Reject Lasso</title>
      <link>https://arxiv.org/abs/2508.04646</link>
      <description>arXiv:2508.04646v1 Announce Type: new 
Abstract: The Lasso method is known to exhibit instability in the presence of highly correlated features, often leading to an arbitrary selection of predictors. This issue manifests itself in two primary error types: the erroneous omission of features that lack a true substitutable relationship (falsely redundant features) and the inclusion of features with a true substitutable relationship (truly redundant features). Although most existing methods address only one of these challenges, we introduce the Accept-Reject Lasso (ARL), a novel approach that resolves this dilemma. ARL operationalizes an Accept-Reject framework through a fine-grained analysis of feature selection across data subsets. This framework is designed to partition the output of an ensemble method into beneficial and detrimental components through fine-grained analysis. The fundamental challenge for Lasso is that inter-variable correlation obscures the true sources of information. ARL tackles this by first using clustering to identify distinct subset structures within the data. It then analyzes Lasso's behavior across these subsets to differentiate between true and spurious correlations. For truly correlated features, which induce multicollinearity, ARL tends to select a single representative feature and reject the rest to ensure model stability. Conversely, for features linked by spurious correlations, which may vanish in certain subsets, ARL accepts those that Lasso might have incorrectly omitted. The distinct patterns arising from true versus spurious correlations create a divisible separation. By setting an appropriate threshold, our framework can effectively distinguish between these two phenomena, thereby maximizing the inclusion of informative variables while minimizing the introduction of detrimental ones. We illustrate the efficacy of the proposed method through extensive simulation and real-data experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04646v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanxin Liu, Yunqi Zhang</dc:creator>
    </item>
    <item>
      <title>Stochastic Taylor expansion via Poisson point processes</title>
      <link>https://arxiv.org/abs/2508.04703</link>
      <description>arXiv:2508.04703v1 Announce Type: new 
Abstract: We generalize Taylor's theorem by introducing a stochastic formulation based on an underlying Poisson point process model. We utilize this approach to propose a novel non-linear regression framework and perform statistical inference of the model parameters. Theoretical properties of the proposed estimator are also proven, including its convergence, uniformly almost surely, to the true function. The theory is presented for the univariate and multivariate cases, and we exemplify the proposed methodology using several examples via simulations and an application to stock market data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04703v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weichao Wu, Athanasios C. Micheas</dc:creator>
    </item>
    <item>
      <title>Bayesian MI-LASSO for Variable Selection on Multiply-Imputed Data</title>
      <link>https://arxiv.org/abs/2211.00114</link>
      <description>arXiv:2211.00114v2 Announce Type: replace 
Abstract: Multiple imputation is widely used for handling missing data in real-world applications. For variable selection on multiply-imputed datasets, however, if selection is performed on each imputed dataset separately, it can result in different sets of selected variables across datasets. MI-LASSO, one of the most commonly used approaches to this problem, regards the same variable across all separate imputed datasets as a group variable and exploits the group LASSO to yield a consistent variable selection across all the multiply-imputed datasets. In this paper, we extend MI-LASSO to a Bayesian framework and propose four Bayesian MI-LASSO models for variable selection on multiply-imputed data, including three shrinkage prior-based and one Spike-Slab prior-based methods. To further support robust variable selection, we develop a four-step projection predictive variable selection procedure that avoids ad hoc thresholding and facilitates valid post-selection inference. Simulation studies showed that the Bayesian MI-LASSO outperformed MI-LASSO and other alternative approaches, achieving higher specificity and lower mean squared error across a range of settings. We further demonstrated these methods via a case study using a multiply-imputed dataset from the University of Michigan Dioxin Exposure Study. The R package BMIselect is available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.00114v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jungang Zou, Sijian Wang, Qixuan Chen</dc:creator>
    </item>
    <item>
      <title>Producing treatment hierarchies in network meta-analysis using probabilistic models and treatment-choice criteria</title>
      <link>https://arxiv.org/abs/2406.10612</link>
      <description>arXiv:2406.10612v2 Announce Type: replace 
Abstract: A key output of network meta-analysis (NMA) is the relative ranking of treatments; nevertheless, it has attracted substantial criticism. Existing ranking methods often lack clear interpretability and fail to adequately account for uncertainty, over-emphasizing small differences in treatment effects. We propose a novel framework to estimate treatment hierarchies in NMA using a probabilistic model, focusing on a clinically relevant treatment-choice criterion (TCC). Initially, we formulate a mathematical expression to define a TCC based on smallest worthwhile differences (SWD), converting NMA relative treatment effects into treatment preference format. This data is then synthesized using a probabilistic ranking model, assigning each treatment a latent 'ability' parameter, representing its propensity to yield clinically important and beneficial true treatment effects relative to the rest of the treatments in the network. Parameter estimation relies on the maximum likelihood theory, with standard errors derived asymptotically from Fisher's information matrix. To facilitate the use of our methods, we launched the R package mtrank. We applied our method to two clinical datasets: one comparing 18 antidepressants for major depression and another comparing 6 antihypertensives for the incidence of diabetes. Our approach provided robust, interpretable treatment hierarchies that account for a concrete TCC. We further examined the agreement between the proposed method and existing ranking metrics in 153 published networks, concluding that the degree of agreement depends on the precision of the NMA estimates. Our framework offers a valuable alternative for NMA treatment ranking, mitigating over-interpretation of minor differences. This enables more reliable and clinically meaningful treatment hierarchies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10612v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Theodoros Evrenoglou, Adriani Nikolakopoulou, Guido Schwarzer, Gerta R\"ucker, Anna Chaimani</dc:creator>
    </item>
    <item>
      <title>Analyzing zero-inflated clustered longitudinal ordinal outcomes using GEE-type models with an application to dental fluorosis studies</title>
      <link>https://arxiv.org/abs/2412.11348</link>
      <description>arXiv:2412.11348v4 Announce Type: replace 
Abstract: Motivated by the Iowa Fluoride Study (IFS), which tracked fluoride intake and dental outcomes from childhood to young adulthood (ages 9, 13, 17, and 23), we analyze dental fluorosis - a condition caused by excessive fluoride exposure during enamel formation. In this context, fluorosis scores across tooth surfaces present as zero-inflated, clustered, and longitudinal ordinal outcomes, prompting the development of a unified modeling framework. Leveraging generalized estimating equations (GEEs), we construct separate models for the presence and severity of fluorosis and propose a combined model that links these components though shared covariates. To improve estimation efficiency and borrowing strength across timepoints, we incorporate James-Stein shrinkage estimators. We compare several working correlation structures, including a data-driven jackknifed structure, and perform model selection via rank aggregation. Simulation studies validate the finite-sample performance of the proposed models, and a bootstrap-based power analysis further confirms the validity of the testing procedure. In our analysis of the IFS data, early-life total daily fluoride intake, average home water fluoride concentration, and specific teeth and zones emerge as significant risk factors for dental fluorosis. Maxillary lateral incisors and zones closer to the gum show protective effects across different ages. These findings reveal novel age-specific associations between early-life exposures and the progression of dental fluorosis through early adulthood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11348v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shoumi Sarkar, Anish Mukherjee, Jeremy T. Gaskins, Steven Levy, Peihua Qiu, Somnath Datta</dc:creator>
    </item>
    <item>
      <title>Assessing Heterogeneity of Treatment Effects</title>
      <link>https://arxiv.org/abs/2306.15048</link>
      <description>arXiv:2306.15048v4 Announce Type: replace-cross 
Abstract: Heterogeneous treatment effects are of major interest in economics. For example, a poverty reduction measure would be best evaluated by its effects on those who would be poor in the absence of the treatment, or by the share among the poor who would increase their earnings because of the treatment. While these quantities are not identified, we derive nonparametrically sharp bounds using only the marginal distributions of the control and treated outcomes. Applications to microfinance and welfare reform demonstrate their utility even when the average treatment effects are not significant and when economic theory makes opposite predictions between heterogeneous individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15048v4</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tetsuya Kaji, Jianfei Cao</dc:creator>
    </item>
    <item>
      <title>Survey Data Integration for Distribution Function Estimation</title>
      <link>https://arxiv.org/abs/2409.14284</link>
      <description>arXiv:2409.14284v3 Announce Type: replace-cross 
Abstract: Integration of probabilistic and non-probabilistic samples for the estimation of finite population totals (or means) has recently received considerable attention in the field of survey sampling; yet, to the best of our knowledge, this framework has not been extended to cumulative distribution function (CDF) estimation. To address this gap, we propose a novel CDF estimator that integrates data from probability samples with data from, potentially big, nonprobability samples. Assuming that a set of shared covariates are observed in both, while the response variable is observed only in the latter, the proposed estimator uses a survey-weighted empirical CDF of regression residuals trained on the convenience sample to estimate the CDF of the response variable. Under some assumptions, we derive the asymptotic bias and variance of our CDF estimator and show that it is asymptotically unbiased for the finite population CDF if ignorability holds. Our empirical results imply that the proposed CDF estimator is robust to model misspecification under ignorability, and robust to ignorability under model misspecification; when both assumptions are violated, our residual-based CDF estimator still outperforms its `plug-in' mass imputation and naive siblings, albeit with noted decreases in efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14284v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Flood, Sayed Mostafa</dc:creator>
    </item>
    <item>
      <title>Deep Discrete Encoders: Identifiable Deep Generative Models for Rich Data with Discrete Latent Layers</title>
      <link>https://arxiv.org/abs/2501.01414</link>
      <description>arXiv:2501.01414v2 Announce Type: replace-cross 
Abstract: In the era of generative AI, deep generative models (DGMs) with latent representations have gained tremendous popularity. Despite their impressive empirical performance, the statistical properties of these models remain underexplored. DGMs are often overparametrized, non-identifiable, and uninterpretable black boxes, raising serious concerns when deploying them in high-stakes applications. Motivated by this, we propose interpretable deep generative models for rich data types with discrete latent layers, called Deep Discrete Encoders (DDEs). A DDE is a directed graphical model with multiple binary latent layers. Theoretically, we propose transparent identifiability conditions for DDEs, which imply progressively smaller sizes of the latent layers as they go deeper. Identifiability ensures consistent parameter estimation and inspires an interpretable design of the deep architecture. Computationally, we propose a scalable estimation pipeline of a layerwise nonlinear spectral initialization followed by a penalized stochastic approximation EM algorithm. This procedure can efficiently estimate models with exponentially many latent components. Extensive simulation studies for high-dimensional data and deep architectures validate our theoretical results and demonstrate the excellent performance of our algorithms. We apply DDEs to three diverse real datasets with different data types to perform hierarchical topic modeling, image representation learning, and response time modeling in educational testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01414v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seunghyun Lee, Yuqi Gu</dc:creator>
    </item>
    <item>
      <title>A Time-Scaled ETAS Model for Earthquake Forecasting</title>
      <link>https://arxiv.org/abs/2505.24412</link>
      <description>arXiv:2505.24412v2 Announce Type: replace-cross 
Abstract: The Himalayan region, including Nepal, is prone to frequent and large earthquakes. Accurate forecasting of these earthquakes is crucial for minimizing loss of life and damage to infrastructure. In this study, we propose various time-scaled Epidemic Type Aftershock Sequence (ETAS) models to forecast earthquakes in Nepal. The ETAS model is a statistical model that describes the temporal and spatial patterns of aftershocks following a main shock. A dataset of earthquake occurrences in Nepal from 2000 to 2020 was collected, and this data was used to fit the models showcased in this article. Our results show that the time-scaled ETAS model is able to accurately forecast earthquake occurrences in Nepal, and could be a useful tool for earthquake early warning systems in the region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24412v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 07 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-981-96-7556-2</arxiv:DOI>
      <arxiv:journal_reference>Data-Centric Approaches to Industrial Decisions: Technology, Digitisation nad Business Decisions, Asset Analytics, Performance and Safety Management (2025) 97-114; ISBN: 978-981-96-7555-5</arxiv:journal_reference>
      <dc:creator>Agniva Das, Muralidharan K</dc:creator>
    </item>
  </channel>
</rss>
