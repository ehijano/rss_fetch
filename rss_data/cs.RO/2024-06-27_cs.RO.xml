<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.RO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.RO</link>
    <description>cs.RO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.RO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Jun 2024 01:40:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Human-centered In-building Embodied Delivery Benchmark</title>
      <link>https://arxiv.org/abs/2406.17898</link>
      <description>arXiv:2406.17898v1 Announce Type: new 
Abstract: Recently, the concept of embodied intelligence has been widely accepted and popularized, leading people to naturally consider the potential for commercialization in this field. In this work, we propose a specific commercial scenario simulation, human-centered in-building embodied delivery. Furthermore, for this scenario, we have developed a brand-new virtual environment system from scratch, constructing a multi-level connected building space modeled after a polar research station. This environment also includes autonomous human characters and robots with grasping and mobility capabilities, as well as a large number of interactive items. Based on this environment, we have built a delivery dataset containing 13k language instructions to guide robots in providing services. We simulate human behavior through human characters and sample their various needs in daily life. Finally, we proposed a method centered around a large multimodal model to serve as the baseline system for this dataset. Compared to past embodied data work, our work focuses on a virtual environment centered around human-robot interaction for commercial scenarios. We believe this will bring new perspectives and exploration angles to the embodied community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17898v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhuoqun Xu, Yang Liu, Xiaoqi Li, Jiyao Zhang, Hao Dong</dc:creator>
    </item>
    <item>
      <title>SonicSense: Object Perception from In-Hand Acoustic Vibration</title>
      <link>https://arxiv.org/abs/2406.17932</link>
      <description>arXiv:2406.17932v1 Announce Type: new 
Abstract: We introduce SonicSense, a holistic design of hardware and software to enable rich robot object perception through in-hand acoustic vibration sensing. While previous studies have shown promising results with acoustic sensing for object perception, current solutions are constrained to a handful of objects with simple geometries and homogeneous materials, single-finger sensing, and mixing training and testing on the same objects. SonicSense enables container inventory status differentiation, heterogeneous material prediction, 3D shape reconstruction, and object re-identification from a diverse set of 83 real-world objects. Our system employs a simple but effective heuristic exploration policy to interact with the objects as well as end-to-end learning-based algorithms to fuse vibration signals to infer object properties. Our framework underscores the significance of in-hand acoustic vibration sensing in advancing robot tactile perception.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17932v1</guid>
      <category>cs.RO</category>
      <category>cs.MM</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaxun Liu, Boyuan Chen</dc:creator>
    </item>
    <item>
      <title>Continuous Execution of High-Level Collaborative Tasks for Heterogeneous Robot Teams</title>
      <link>https://arxiv.org/abs/2406.18019</link>
      <description>arXiv:2406.18019v1 Announce Type: new 
Abstract: We propose a control synthesis framework for a heterogeneous multi-robot system to satisfy collaborative tasks, where actions may take varying duration of time to complete. We encode tasks using the discrete logic LTL^\psi, which uses the concept of bindings to interleave robot actions and express information about relationship between specific task requirements and robot assignments. We present a synthesis approach to automatically generate a teaming assignment and corresponding discrete behavior that is correct-by-construction for continuous execution, while also implementing synchronization policies to ensure collaborative portions of the task are satisfied. We demonstrate our approach on a physical multi-robot system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18019v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amy Fang, Tenny Yin, Jiawei Lin, Hadas Kress-Gazit</dc:creator>
    </item>
    <item>
      <title>Open-vocabulary Mobile Manipulation in Unseen Dynamic Environments with 3D Semantic Maps</title>
      <link>https://arxiv.org/abs/2406.18115</link>
      <description>arXiv:2406.18115v1 Announce Type: new 
Abstract: Open-Vocabulary Mobile Manipulation (OVMM) is a crucial capability for autonomous robots, especially when faced with the challenges posed by unknown and dynamic environments. This task requires robots to explore and build a semantic understanding of their surroundings, generate feasible plans to achieve manipulation goals, adapt to environmental changes, and comprehend natural language instructions from humans. To address these challenges, we propose a novel framework that leverages the zero-shot detection and grounded recognition capabilities of pretraining visual-language models (VLMs) combined with dense 3D entity reconstruction to build 3D semantic maps. Additionally, we utilize large language models (LLMs) for spatial region abstraction and online planning, incorporating human instructions and spatial semantic context. We have built a 10-DoF mobile manipulation robotic platform JSR-1 and demonstrated in real-world robot experiments that our proposed framework can effectively capture spatial semantics and process natural language user instructions for zero-shot OVMM tasks under dynamic environment settings, with an overall navigation and task success rate of 80.95% and 73.33% over 105 episodes, and better SFT and SPL by 157.18% and 19.53% respectively compared to the baseline. Furthermore, the framework is capable of replanning towards the next most probable candidate location based on the spatial semantic context derived from the 3D semantic map when initial plans fail, keeping an average success rate of 76.67%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18115v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dicong Qiu, Wenzong Ma, Zhenfu Pan, Hui Xiong, Junwei Liang</dc:creator>
    </item>
    <item>
      <title>B-TMS: Bayesian Traversable Terrain Modeling and Segmentation Across 3D LiDAR Scans and Maps for Enhanced Off-Road Navigation</title>
      <link>https://arxiv.org/abs/2406.18138</link>
      <description>arXiv:2406.18138v1 Announce Type: new 
Abstract: Recognizing traversable terrain from 3D point cloud data is critical, as it directly impacts the performance of autonomous navigation in off-road environments. However, existing segmentation algorithms often struggle with challenges related to changes in data distribution, environmental specificity, and sensor variations. Moreover, when encountering sunken areas, their performance is frequently compromised, and they may even fail to recognize them. To address these challenges, we introduce B-TMS, a novel approach that performs map-wise terrain modeling and segmentation by utilizing Bayesian generalized kernel (BGK) within the graph structure known as the tri-grid field (TGF). Our experiments encompass various data distributions, ranging from single scans to partial maps, utilizing both public datasets representing urban scenes and off-road environments, and our own dataset acquired from extremely bumpy terrains. Our results demonstrate notable contributions, particularly in terms of robustness to data distribution variations, adaptability to diverse environmental conditions, and resilience against the challenges associated with parameter changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18138v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minho Oh, Gunhee Shin, Seoyeon Jang, Seungjae Lee, Dongkyu Lee, Wonho Song, Byeongho Yu, Hyungtae Lim, Jaeyoung Lee, Hyun Myung</dc:creator>
    </item>
    <item>
      <title>3D-MVP: 3D Multiview Pretraining for Robotic Manipulation</title>
      <link>https://arxiv.org/abs/2406.18158</link>
      <description>arXiv:2406.18158v1 Announce Type: new 
Abstract: Recent works have shown that visual pretraining on egocentric datasets using masked autoencoders (MAE) can improve generalization for downstream robotics tasks. However, these approaches pretrain only on 2D images, while many robotics applications require 3D scene understanding. In this work, we propose 3D-MVP, a novel approach for 3D multi-view pretraining using masked autoencoders. We leverage Robotic View Transformer (RVT), which uses a multi-view transformer to understand the 3D scene and predict gripper pose actions. We split RVT's multi-view transformer into visual encoder and action decoder, and pretrain its visual encoder using masked autoencoding on large-scale 3D datasets such as Objaverse. We evaluate 3D-MVP on a suite of virtual robot manipulation tasks and demonstrate improved performance over baselines. We also show promising results on a real robot platform with minimal finetuning. Our results suggest that 3D-aware pretraining is a promising approach to improve sample efficiency and generalization of vision-based robotic manipulation policies. We will release code and pretrained models for 3D-MVP to facilitate future research. Project site: https://jasonqsy.github.io/3DMVP</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18158v1</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengyi Qian, Kaichun Mo, Valts Blukis, David F. Fouhey, Dieter Fox, Ankit Goyal</dc:creator>
    </item>
    <item>
      <title>Multimodal Reaching-Position Prediction for ADL Support Using Neural Networks</title>
      <link>https://arxiv.org/abs/2406.18162</link>
      <description>arXiv:2406.18162v1 Announce Type: new 
Abstract: This study aimed to develop daily living support robots for patients with hemiplegia and the elderly. To support the daily living activities using robots in ordinary households without imposing physical and mental burdens on users, the system must detect the actions of the user and move appropriately according to their motions.
  We propose a reaching-position prediction scheme that targets the motion of lifting the upper arm, which is burdensome for patients with hemiplegia and the elderly in daily living activities.
  For this motion, it is difficult to obtain effective features to create a prediction model in environments where large-scale sensor system installation is not feasible and the motion time is short.
  We performed motion-collection experiments, revealed the features of the target motion and built a prediction model using the multimodal motion features and deep learning.
  The proposed model achieved an accuracy of 93 \% macro average and F1-score of 0.69 for a 9-class classification prediction at 35\% of the motion completion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18162v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yutaka Takase, Kimitoshi Yamazaki</dc:creator>
    </item>
    <item>
      <title>Advancing Robotic Surgery: Affordable Kinesthetic and Tactile Feedback Solutions for Endotrainers</title>
      <link>https://arxiv.org/abs/2406.18229</link>
      <description>arXiv:2406.18229v1 Announce Type: new 
Abstract: The proliferation of robot-assisted minimally invasive surgery highlights the need for advanced training tools such as cost-effective robotic endotrainers. Current surgical robots often lack haptic feedback, which is crucial for providing surgeons with a real-time sense of touch. This absence can impact the surgeon's ability to perform delicate operations effectively. To enhance surgical training and address this deficiency, we have integrated a cost-effective haptic feedback system into a robotic endotrainer. This system incorporates both kinesthetic (force) and tactile feedback, improving the fidelity of surgical simulations and enabling more precise control during operations. Our system incorporates an innovative, cost-effective Force/Torque sensor utilizing optoelectronic technology, specifically designed to accurately detect forces and moments exerted on surgical tools with a 95% accuracy, providing essential kinesthetic feedback. Additionally, we implemented a tactile feedback mechanism that informs the surgeon of the gripping forces between the tool's tip and the tissue. This dual feedback system enhances the fidelity of training simulations and the execution of robotic surgeries, promoting broader adoption and safer practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18229v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bharath Rajiv Nair, Aravinthkumar T., B. Vinod</dc:creator>
    </item>
    <item>
      <title>LLCoach: Generating Robot Soccer Plans using Multi-Role Large Language Models</title>
      <link>https://arxiv.org/abs/2406.18285</link>
      <description>arXiv:2406.18285v1 Announce Type: new 
Abstract: The deployment of robots into human scenarios necessitates advanced planning strategies, particularly when we ask robots to operate in dynamic, unstructured environments. RoboCup offers the chance to deploy robots in one of those scenarios, a human-shaped game represented by a soccer match. In such scenarios, robots must operate using predefined behaviors that can fail in unpredictable conditions. This paper introduces a novel application of Large Language Models (LLMs) to address the challenge of generating actionable plans in such settings, specifically within the context of the RoboCup Standard Platform League (SPL) competitions where robots are required to autonomously execute soccer strategies that emerge from the interactions of individual agents. In particular, we propose a multi-role approach leveraging the capabilities of LLMs to generate and refine plans for a robotic soccer team. The potential of the proposed method is demonstrated through an experimental evaluation,carried out simulating multiple matches where robots with AI-generated plans play against robots running human-built code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18285v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michele Brienza, Emanuele Musumeci, Vincenzo Suriani, Daniele Affinita, Andrea Pennisi, Daniele Nardi, Domenico Daniele Bloisi</dc:creator>
    </item>
    <item>
      <title>Robotic Exploration through Semantic Topometric Mapping</title>
      <link>https://arxiv.org/abs/2406.18381</link>
      <description>arXiv:2406.18381v1 Announce Type: new 
Abstract: In this article, we introduce a novel strategy for robotic exploration in unknown environments using a semantic topometric map. As it will be presented, the semantic topometric map is generated by segmenting the grid map of the currently explored parts of the environment into regions, such as intersections, pathways, dead-ends, and unexplored frontiers, which constitute the structural semantics of an environment. The proposed exploration strategy leverages metric information of the frontier, such as distance and angle to the frontier, similar to existing frameworks, with the key difference being the additional utilization of structural semantic information, such as properties of the intersections leading to frontiers. The algorithm for generating semantic topometric mapping utilized by the proposed method is lightweight, resulting in the method's online execution being both rapid and computationally efficient. Moreover, the proposed framework can be applied to both structured and unstructured indoor and outdoor environments, which enhances the versatility of the proposed exploration algorithm. We validate our exploration strategy and demonstrate the utility of structural semantics in exploration in two complex indoor environments by utilizing a Turtlebot3 as the robotic agent. Compared to traditional frontier-based methods, our findings indicate that the proposed approach leads to faster exploration and requires less computation time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18381v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Scott Fredriksson, Akshit Saradagi, George Nikolakopoulos</dc:creator>
    </item>
    <item>
      <title>SAM: Semi-Active Mechanism for Extensible Continuum Manipulator and Real-time Hysteresis Compensation Control Algorithm</title>
      <link>https://arxiv.org/abs/2406.18388</link>
      <description>arXiv:2406.18388v2 Announce Type: new 
Abstract: Cable-Driven Continuum Manipulators (CDCMs) enable scar-free procedures via natural orifices and improve target lesion accessibility through curved paths. However, CDCMs face limitations in workspace and control accuracy due to non-linear cable effects causing hysteresis. This paper introduces an extensible CDCM with a Semi-active Mechanism (SAM) to expand the workspace via translational motion without additional mechanical elements or actuation. We collect a hysteresis dataset using 8 fiducial markers and RGBD sensing. Based on this dataset, we develop a real-time hysteresis compensation control algorithm using the trained Temporal Convolutional Network (TCN) with a 1ms time latency, effectively estimating the manipulator's hysteresis behavior. Performance validation through random trajectory tracking tests and box pointing tasks shows the proposed controller significantly reduces hysteresis by up to 69.5% in joint space and approximately 26% in the box pointing task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18388v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junhyun Park, Seonghyeok Jang, Myeongbo Park, Hyojae Park, Jeonghyeon Yoon, Minho Hwang</dc:creator>
    </item>
    <item>
      <title>Sensorless model-based tension control for a cable-driven exosuit</title>
      <link>https://arxiv.org/abs/2406.18412</link>
      <description>arXiv:2406.18412v1 Announce Type: new 
Abstract: Cable-driven exosuits have the potential to support individuals with motor disabilities across the continuum of care. When supporting a limb with a cable, force sensors are often used to measure tension. However, force sensors add cost, complexity, and distal components. This paper presents a design and control approach to remove the force sensor from an upper limb cable-driven exosuit. A mechanical design for the exosuit was developed to maximize passive transparency. Then, a data-driven friction identification was conducted on a mannequin test bench to design a model-based tension controller. Seventeen healthy participants raised and lowered their right arms to evaluate tension tracking, movement quality, and muscular effort. Questionnaires on discomfort, physical exertion, and fatigue were collected. The proposed strategy allowed tracking the desired assistive torque with an RMSE of 0.71 Nm (18%) at 50% gravity support. During the raising phase, the EMG signals of the anterior deltoid, trapezius, and pectoralis major were reduced on average compared to the no-suit condition by 30%, 38%, and 38%, respectively. The posterior deltoid activity was increased by 32% during lowering. Position tracking was not significantly altered, whereas movement smoothness significantly decreased. This work demonstrates the feasibility and effectiveness of removing the force sensor from a cable-driven exosuit. A significant increase in discomfort in the lower neck and right shoulder indicated that the ergonomics of the suit could be improved. Overall this work paves the way towards simpler and more affordable exosuits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18412v1</guid>
      <category>cs.RO</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Elena Bardi, Adrian Esser, Peter Wolf, Marta Gandolla, Emilia Ambrosini, Alessandra Pedrocchi, Robert Riener</dc:creator>
    </item>
    <item>
      <title>Optimal Multi-Robot Communication-Aware Trajectory Planning by Constraining the Fiedler Value</title>
      <link>https://arxiv.org/abs/2406.18452</link>
      <description>arXiv:2406.18452v1 Announce Type: new 
Abstract: The paper present a novel approach for the solution of the Multi-Robot Communication-Aware Trajectory Planning, which builds on a general optimisation framework where the changes in robots positions are used as decision variable, and linear constraints on the trajectories of the robots are introduced to ensure communication performance and collision avoidance. The Fiedler value is adopted as communication performance metric. The validity of the method in computing both feasible and optimal trajectories for the robots is demonstrated both in simulation and experimentally. Results show that the constraint on the Fiedler value ensures that the robot network fulfils its objective while maintaining communication connectivity at all times. Further, the paper shows that the introduction of approximations for the constraints enables a significant improvement in the computational time of the solution, which remain very close to the optimal solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18452v1</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeppe Heini Mikkelsen, Roberto Galeazzi, Matteo Fumagalli</dc:creator>
    </item>
    <item>
      <title>ET tu, CLIP? Addressing Common Object Errors for Unseen Environments</title>
      <link>https://arxiv.org/abs/2406.17876</link>
      <description>arXiv:2406.17876v1 Announce Type: cross 
Abstract: We introduce a simple method that employs pre-trained CLIP encoders to enhance model generalization in the ALFRED task. In contrast to previous literature where CLIP replaces the visual encoder, we suggest using CLIP as an additional module through an auxiliary object detection objective. We validate our method on the recently proposed Episodic Transformer architecture and demonstrate that incorporating CLIP improves task performance on the unseen validation set. Additionally, our analysis results support that CLIP especially helps with leveraging object descriptions, detecting small objects, and interpreting rare words.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17876v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Conference on Computer Vision and Pattern Recognition (CVPR 2022) - Embodied AI Workshop</arxiv:journal_reference>
      <dc:creator>Ye Won Byun, Cathy Jiao, Shahriar Noroozizadeh, Jimin Sun, Rosa Vitiello</dc:creator>
    </item>
    <item>
      <title>Multimodal foundation world models for generalist embodied agents</title>
      <link>https://arxiv.org/abs/2406.18043</link>
      <description>arXiv:2406.18043v1 Announce Type: cross 
Abstract: Learning generalist embodied agents, able to solve multitudes of tasks in different domains is a long-standing problem. Reinforcement learning (RL) is hard to scale up as it requires a complex reward design for each task. In contrast, language can specify tasks in a more natural way. Current foundation vision-language models (VLMs) generally require fine-tuning or other adaptations to be functional, due to the significant domain gap. However, the lack of multimodal data in such domains represents an obstacle toward developing foundation models for embodied applications. In this work, we overcome these problems by presenting multimodal foundation world models, able to connect and align the representation of foundation VLMs with the latent space of generative world models for RL, without any language annotations. The resulting agent learning framework, GenRL, allows one to specify tasks through vision and/or language prompts, ground them in the embodied domain's dynamics, and learns the corresponding behaviors in imagination. As assessed through large-scale multi-task benchmarking, GenRL exhibits strong multi-task generalization performance in several locomotion and manipulation domains. Furthermore, by introducing a data-free RL strategy, it lays the groundwork for foundation model-based RL for generalist embodied agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18043v1</guid>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Aaron Courville, Sai Rajeswar</dc:creator>
    </item>
    <item>
      <title>PlaMo: Plan and Move in Rich 3D Physical Environments</title>
      <link>https://arxiv.org/abs/2406.18237</link>
      <description>arXiv:2406.18237v1 Announce Type: cross 
Abstract: Controlling humanoids in complex physically simulated worlds is a long-standing challenge with numerous applications in gaming, simulation, and visual content creation. In our setup, given a rich and complex 3D scene, the user provides a list of instructions composed of target locations and locomotion types. To solve this task we present PlaMo, a scene-aware path planner and a robust physics-based controller. The path planner produces a sequence of motion paths, considering the various limitations the scene imposes on the motion, such as location, height, and speed. Complementing the planner, our control policy generates rich and realistic physical motion adhering to the plan. We demonstrate how the combination of both modules enables traversing complex landscapes in diverse forms while responding to real-time changes in the environment. Video: https://youtu.be/wWlqSQlRZ9M .</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18237v1</guid>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Assaf Hallak, Gal Dalal, Chen Tessler, Kelly Guo, Shie Mannor, Gal Chechik</dc:creator>
    </item>
    <item>
      <title>Mental Modeling of Reinforcement Learning Agents by Language Models</title>
      <link>https://arxiv.org/abs/2406.18505</link>
      <description>arXiv:2406.18505v1 Announce Type: cross 
Abstract: Can emergent language models faithfully model the intelligence of decision-making agents? Though modern language models exhibit already some reasoning ability, and theoretically can potentially express any probable distribution over tokens, it remains underexplored how the world knowledge these pretrained models have memorized can be utilized to comprehend an agent's behaviour in the physical world. This study empirically examines, for the first time, how well large language models (LLMs) can build a mental model of agents, termed agent mental modelling, by reasoning about an agent's behaviour and its effect on states from agent interaction history. This research may unveil the potential of leveraging LLMs for elucidating RL agent behaviour, addressing a key challenge in eXplainable reinforcement learning (XRL). To this end, we propose specific evaluation metrics and test them on selected RL task datasets of varying complexity, reporting findings on agent mental model establishment. Our results disclose that LLMs are not yet capable of fully mental modelling agents through inference alone without further innovations. This work thus provides new insights into the capabilities and limitations of modern LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18505v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenhao Lu, Xufeng Zhao, Josua Spisak, Jae Hee Lee, Stefan Wermter</dc:creator>
    </item>
    <item>
      <title>Factor Graph-Based Planning as Inference for Autonomous Vehicle Racing</title>
      <link>https://arxiv.org/abs/2203.03224</link>
      <description>arXiv:2203.03224v5 Announce Type: replace 
Abstract: Factor graph, as a bipartite graphical model, offers a structured representation by revealing local connections among graph nodes. This study explores the utilization of factor graphs in modeling the autonomous racecar planning problem, presenting an alternate perspective to the traditional optimization-based formulation. We model the planning problem as a probabilistic inference over a factor graph, with factor nodes capturing the joint distribution of motion objectives. By leveraging the duality between optimization and inference, a fast solution to the maximum a posteriori estimation of the factor graph is obtained via least-squares optimization. The localized design thinking inherent in this formulation ensures that motion objectives depend on a small subset of variables. We exploit the locality feature of the factor graph structure to integrate the minimum curvature path and local planning computations into a unified algorithm. This diverges from the conventional separation of global and local planning modules, where curvature minimization occurs at the global level. The evaluation of the proposed framework demonstrated superior performance for cumulative curvature and average speed across the racetrack. Furthermore, the results highlight the computational efficiency of our approach. While acknowledging the structural design advantages and computational efficiency of the proposed methodology, we also address its limitations and outline potential directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.03224v5</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/OJITS.2024.3418956</arxiv:DOI>
      <dc:creator>Salman Bari, Xiagong Wang, Ahmad Schoha Haidari, Dirk Wollherr</dc:creator>
    </item>
    <item>
      <title>Visual Odometry with Neuromorphic Resonator Networks</title>
      <link>https://arxiv.org/abs/2209.02000</link>
      <description>arXiv:2209.02000v3 Announce Type: replace 
Abstract: Visual Odometry (VO) is a method to estimate self-motion of a mobile robot using visual sensors. Unlike odometry based on integrating differential measurements that can accumulate errors, such as inertial sensors or wheel encoders, visual odometry is not compromised by drift. However, image-based VO is computationally demanding, limiting its application in use cases with low-latency, -memory, and -energy requirements. Neuromorphic hardware offers low-power solutions to many vision and AI problems, but designing such solutions is complicated and often has to be assembled from scratch. Here we propose to use Vector Symbolic Architecture (VSA) as an abstraction layer to design algorithms compatible with neuromorphic hardware. Building from a VSA model for scene analysis, described in our companion paper, we present a modular neuromorphic algorithm that achieves state-of-the-art performance on two-dimensional VO tasks. Specifically, the proposed algorithm stores and updates a working memory of the presented visual environment. Based on this working memory, a resonator network estimates the changing location and orientation of the camera. We experimentally validate the neuromorphic VSA-based approach to VO with two benchmarks: one based on an event camera dataset and the other in a dynamic scene with a robotic task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.02000v3</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1038/s42256-024-00846-2</arxiv:DOI>
      <arxiv:journal_reference>Nature Machine Intelligence 6 (2024)</arxiv:journal_reference>
      <dc:creator>Alpha Renner, Lazar Supic, Andreea Danielescu, Giacomo Indiveri, E. Paxon Frady, Friedrich T. Sommer, Yulia Sandamirskaya</dc:creator>
    </item>
    <item>
      <title>DMCA: Dense Multi-agent Navigation using Attention and Communication</title>
      <link>https://arxiv.org/abs/2209.06415</link>
      <description>arXiv:2209.06415v3 Announce Type: replace 
Abstract: In decentralized multi-robot navigation, ensuring safe and efficient movement with limited environmental awareness remains a challenge. While robots traditionally navigate based on local observations, this approach falters in complex environments. A possible solution is to enhance understanding of the world through inter-agent communication, but mere information broadcasting falls short in efficiency. In this work, we address this problem by simultaneously learning decentralized multi-robot collision avoidance and selective inter-agent communication. We use a multi-head self-attention mechanism that encodes observable information from neighboring robots into a concise and fixed-length observation vector, thereby handling varying numbers of neighbors. Our method focuses on improving navigation performance through selective communication. We cast the communication selection as a link prediction problem, where the network determines the necessity of establishing a communication link with a specific neighbor based on the observable state information. The communicated information enhances the neighbor's observation and aids in selecting an appropriate navigation plan. By training the network end-to-end, we concurrently learn the optimal weights for the observation encoder, communication selection, and navigation components. We showcase the benefits of our approach by achieving safe and efficient navigation among multiple robots, even in dense and challenging environments. Comparative evaluations against various learning-based and model-based baselines demonstrate our superior navigation performance, resulting in an impressive improvement of up to 24% in success rate within complex evaluation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.06415v3</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Senthil Hariharan Arul, Amrit Singh Bedi, Dinesh Manocha</dc:creator>
    </item>
    <item>
      <title>The Brain-Inspired Cooperative Shared Control for Brain-Machine Interface</title>
      <link>https://arxiv.org/abs/2210.09531</link>
      <description>arXiv:2210.09531v2 Announce Type: replace 
Abstract: In the practical application of brain-machine interface technology, the problem often faced is the low information content and high noise of the neural signals collected by the electrode and the difficulty of decoding by the decoder, which makes it difficult for the robotic to obtain stable instructions to complete the task. The idea based on the principle of cooperative shared control can be achieved by extracting general motor commands from brain activity, while the fine details of the movement can be hosted to the robot for completion, or the brain can have complete control. This study proposes a brain-machine interface shared control system based on spiking neural networks for robotic arm movement control and wheeled robots wheel speed control and steering, respectively. The former can reliably control the robotic arm to move to the destination position, while the latter controls the wheeled robots for object tracking and map generation. The results show that the shared control based on brain-inspired intelligence can perform some typical tasks in complex environments and positively improve the fluency and ease of use of brain-machine interaction, and also demonstrate the potential of this control method in clinical applications of brain-machine interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.09531v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shengjie Zheng, Ling Liu, Junjie Yang, Lang Qian, Gang Gao, Xin Chen, Wenqi Jin, Chunshan Deng, Xiaojian Li</dc:creator>
    </item>
    <item>
      <title>Multimodal and Force-Matched Imitation Learning with a See-Through Visuotactile Sensor</title>
      <link>https://arxiv.org/abs/2311.01248</link>
      <description>arXiv:2311.01248v3 Announce Type: replace 
Abstract: Contact-rich tasks continue to present a variety of challenges for robotic manipulation. In this work, we leverage a multimodal visuotactile sensor within the framework of imitation learning (IL) to perform contact rich tasks that involve relative motion (slipping/sliding) between the end-effector and object. We introduce two algorithmic contributions, tactile force matching and learned mode switching, as complimentary methods for improving IL. Tactile force matching enhances kinesthetic teaching by reading approximate forces during the demonstration and generating an adapted robot trajectory that recreates the recorded forces. Learned mode switching uses IL to couple visual and tactile sensor modes with the learned motion policy, simplifying the transition from reaching to contacting. We perform robotic manipulation experiments on four door opening tasks with a variety of observation and method configurations to study the utility of our proposed improvements and multimodal visuotactile sensing. Our results show that the inclusion of force matching raises average policy success rates by 62.5%, visuotactile mode switching by 30.3%, and visuotactile data as a policy input by 42.5%, emphasizing the value of see-through tactile sensing for IL, both for data collection to allow force matching, and for policy execution to allow accurate task feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01248v3</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Trevor Ablett, Oliver Limoyo, Adam Sigal, Affan Jilani, Jonathan Kelly, Kaleem Siddiqi, Francois Hogan, Gregory Dudek</dc:creator>
    </item>
    <item>
      <title>Design and Control Co-Optimization for Automated Design Iteration of Dexterous Anthropomorphic Soft Robotic Hands</title>
      <link>https://arxiv.org/abs/2403.09933</link>
      <description>arXiv:2403.09933v2 Announce Type: replace 
Abstract: We automate soft robotic hand design iteration by co-optimizing design and control policy for dexterous manipulation skills in simulation. Our design iteration pipeline combines genetic algorithms and policy transfer to learn control policies for nearly 400 hand designs, testing grasp quality under external force disturbances. We validate the optimized designs in the real world through teleoperation of pickup and reorient manipulation tasks. Our real world evaluation, from over 900 teleoperated tasks, shows that the trend in design performance in simulation resembles that of the real world. Furthermore, we show that optimized hand designs from our approach outperform existing soft robot hands from prior work in the real world. The results highlight the usefulness of simulation in guiding parameter choices for anthropomorphic soft robotic hand systems, and the effectiveness of our automated design iteration approach, despite the sim-to-real gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09933v2</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>IEEE-RAS International Conference on Soft Robotics (RoboSoft) 2024</arxiv:journal_reference>
      <dc:creator>Pragna Mannam, Xingyu Liu, Ding Zhao, Jean Oh, Nancy Pollard</dc:creator>
    </item>
    <item>
      <title>Comparison of two Cooperative Maneuver Planning Approaches at a Real-World T-Junction</title>
      <link>https://arxiv.org/abs/2403.16478</link>
      <description>arXiv:2403.16478v2 Announce Type: replace 
Abstract: Connected automated driving promises a significant improvement of traffic efficiency and safety on highways and in urban areas. Cooperative maneuver planning may facilitate active guidance of connected automated vehicles at intersections. Research in automatic intersection management put forth a large body of works that mostly employ rule-based or optimization-based approaches primarily in fully automated simulated environments. In this work, we compare two cooperative planning approaches for unsignalized intersections that are capable of handling mixed traffic, i.e., the road being shared by automated vehicles and regular vehicles driven by humans. The first approach is a cooperative planner that selects the most efficient out of multiple possible maneuvers based on a scene prediction trained on real driving data. The second cooperative planning approach is based on graph-based reinforcement learning, which conquers the lack of ground truth data for cooperative maneuvers. We thoroughly evaluate both cooperative planners in a realistic high-fidelity simulation with fully automated traffic and mixed traffic. The simulative experiments show that cooperative maneuver planning leads to less delay due to interaction and a reduced number of stops. Furthermore, we present results from real-world experiments with three prototype automated vehicles at a T-junction in public traffic, in which both planning modules demonstrate their ability to perform efficient cooperative maneuvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16478v2</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marvin Klimke, Max Bastian Mertens, Benjamin V\"olz, Michael Buchholz</dc:creator>
    </item>
    <item>
      <title>On Convex Data-Driven Inverse Optimal Control for Nonlinear, Non-stationary and Stochastic Systems</title>
      <link>https://arxiv.org/abs/2306.13928</link>
      <description>arXiv:2306.13928v2 Announce Type: replace-cross 
Abstract: This paper is concerned with a finite-horizon inverse control problem, which has the goal of reconstructing, from observations, the possibly non-convex and non-stationary cost driving the actions of an agent. In this context, we present a result enabling cost reconstruction by solving an optimization problem that is convex even when the agent cost is not and when the underlying dynamics is nonlinear, non-stationary and stochastic. To obtain this result, we also study a finite-horizon forward control problem that has randomized policies as decision variables. We turn our findings into algorithmic procedures and show the effectiveness of our approach via in-silico and hardware validations. All experiments confirm the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13928v2</guid>
      <category>math.OC</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>math.DS</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Emiland Garrabe, Hozefa Jesawada, Carmen Del Vecchio, Giovanni Russo</dc:creator>
    </item>
    <item>
      <title>ODIN: A Single Model for 2D and 3D Segmentation</title>
      <link>https://arxiv.org/abs/2401.02416</link>
      <description>arXiv:2401.02416v3 Announce Type: replace-cross 
Abstract: State-of-the-art models on contemporary 3D segmentation benchmarks like ScanNet consume and label dataset-provided 3D point clouds, obtained through post processing of sensed multiview RGB-D images. They are typically trained in-domain, forego large-scale 2D pre-training and outperform alternatives that featurize the posed RGB-D multiview images instead. The gap in performance between methods that consume posed images versus post-processed 3D point clouds has fueled the belief that 2D and 3D perception require distinct model architectures. In this paper, we challenge this view and propose ODIN (Omni-Dimensional INstance segmentation), a model that can segment and label both 2D RGB images and 3D point clouds, using a transformer architecture that alternates between 2D within-view and 3D cross-view information fusion. Our model differentiates 2D and 3D feature operations through the positional encodings of the tokens involved, which capture pixel coordinates for 2D patch tokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation benchmarks, and competitive performance on ScanNet, S3DIS and COCO. It outperforms all previous works by a wide margin when the sensed 3D point cloud is used in place of the point cloud sampled from 3D mesh. When used as the 3D perception engine in an instructable embodied agent architecture, it sets a new state-of-the-art on the TEACh action-from-dialogue benchmark. Our code and checkpoints can be found at the project website (https://odin-seg.github.io).</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.02416v3</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ayush Jain, Pushkal Katara, Nikolaos Gkanatsios, Adam W. Harley, Gabriel Sarch, Kriti Aggarwal, Vishrav Chaudhary, Katerina Fragkiadaki</dc:creator>
    </item>
  </channel>
</rss>
