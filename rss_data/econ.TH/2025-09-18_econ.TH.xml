<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.TH</link>
    <description>econ.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Sep 2025 04:05:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Friend or Foe: Delegating to an AI Whose Alignment is Unknown</title>
      <link>https://arxiv.org/abs/2509.14396</link>
      <description>arXiv:2509.14396v1 Announce Type: new 
Abstract: AI systems have the potential to improve decision-making, but decision makers face the risk that the AI may be misaligned with their objectives. We study this problem in the context of a treatment decision, where a designer decides which patient attributes to reveal to an AI before receiving a prediction of the patient's need for treatment. Providing the AI with more information increases the benefits of an aligned AI but also amplifies the harm from a misaligned one. We characterize how the designer should select attributes to balance these competing forces, depending on their beliefs about the AI's reliability. We show that the designer should optimally disclose attributes that identify \emph{rare} segments of the population in which the need for treatment is high, and pool the remaining patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14396v1</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Drew Fudenberg, Annie Liang</dc:creator>
    </item>
    <item>
      <title>Outside options and risk attitude</title>
      <link>https://arxiv.org/abs/2509.14732</link>
      <description>arXiv:2509.14732v1 Announce Type: new 
Abstract: We uncover a close link between outside options and risk attitude: when a decision-maker gains access to an outside option, her behaviour becomes less risk-averse, and conversely, any observed decrease of risk-aversion can be explained by an outside option having been made available. We characterise the comparative statics of risk-aversion, delineating how effective risk attitude (i.e. actual choice among risky prospects) varies with the outside option and with the decision-maker's 'true' risk attitude. We prove that outside options are special: among transformations of a decision problem, those that amount to adding an outside option are the only ones that always reduce risk-aversion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14732v1</guid>
      <category>econ.TH</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregorio Curello, Ludvig Sinander, Mark Whitmeyer</dc:creator>
    </item>
    <item>
      <title>An Implementation Relaxation Approach to Principal-Agent Problems</title>
      <link>https://arxiv.org/abs/2509.14766</link>
      <description>arXiv:2509.14766v1 Announce Type: new 
Abstract: The first-order approach (FOA) is the standard tool for solving principal-agent problems, replacing the incentive compatibility (IC) constraint with its first-order condition to obtain a relaxed problem. We show that FOA is not a valid relaxation when the support of the outcome distribution shifts with the agent's effort, as in well-studied additive-noise models. In such cases, the optimal effort may occur at a kink point that the first-order condition cannot capture, causing FOA to miss optimal contracts, including widely adopted bonus schemes. Motivated by this limitation, we introduce the Implementation Relaxation Approach (IRA), which relaxes the set of agent actions and payoffs that feasible contracts can induce, rather than directly relaxing IC. IRA accommodates non-differentiable optima and is straightforward to apply across settings, particularly for deriving optimality conditions for simple contracts. Using IRA, we derive an optimality condition for quota-bonus contracts that is more general, encompassing a broader range of scenarios than FOA-based conditions, including those established in the literature under fixed-support assumptions. This also fills a gap where the optimality of quota-bonus contracts in shifting-support settings has been examined only under endogenous assumptions, and it highlights the broader applicability of IRA as a methodological tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14766v1</guid>
      <category>econ.TH</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hang Jiang</dc:creator>
    </item>
    <item>
      <title>Invariant Modeling for Joint Distributions</title>
      <link>https://arxiv.org/abs/2509.15165</link>
      <description>arXiv:2509.15165v1 Announce Type: new 
Abstract: A common theme underlying many problems in statistics and economics involves the determination of a systematic method of selecting a joint distribution consistent with a specified list of categorical marginals, some of which have an ordinal structure. We propose guidance in narrowing down the set of possible methods by introducing Invariant Aggregation (IA), a natural property that requires merging adjacent categories in one marginal not to alter the joint distribution over unaffected values. We prove that a model satisfies IA if and only if it is a copula model. This characterization ensures i) robustness against data manipulation and survey design, and ii) allows seamless incorporation of new variables. Our results provide both theoretical clarity and practical safeguards for inference under marginal constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15165v1</guid>
      <category>econ.TH</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Christopher P. Chambers, Yusufcan Masatlioglu, Ruodu Wang</dc:creator>
    </item>
    <item>
      <title>Optimal Algorithms for Bandit Learning in Matching Markets</title>
      <link>https://arxiv.org/abs/2509.14466</link>
      <description>arXiv:2509.14466v1 Announce Type: cross 
Abstract: We study the problem of pure exploration in matching markets under uncertain preferences, where the goal is to identify a stable matching with confidence parameter $\delta$ and minimal sample complexity. Agents learn preferences via stochastic rewards, with expected values indicating preferences. This finds use in labor market platforms like Upwork, where firms and freelancers must be matched quickly despite noisy observations and no prior knowledge, in a stable manner that prevents dissatisfaction. We consider markets with unique stable matching and establish information-theoretic lower bounds on sample complexity for (1) one-sided learning, where one side of the market knows its true preferences, and (2) two-sided learning, where both sides are uncertain. We propose a computationally efficient algorithm and prove that it asymptotically ($\delta\to 0$) matches the lower bound to a constant for one-sided learning. Using the insights from the lower bound, we extend our algorithm to the two-sided learning setting and provide experimental results showing that it closely matches the lower bound on sample complexity. Finally, using a system of ODEs, we characterize the idealized fluid path that our algorithm chases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14466v1</guid>
      <category>cs.GT</category>
      <category>cs.SY</category>
      <category>econ.TH</category>
      <category>eess.SY</category>
      <category>math.DS</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tejas Pagare, Agniv Bandyopadhyay, Sandeep Juneja</dc:creator>
    </item>
    <item>
      <title>Emergent Alignment via Competition</title>
      <link>https://arxiv.org/abs/2509.15090</link>
      <description>arXiv:2509.15090v1 Announce Type: cross 
Abstract: Aligning AI systems with human values remains a fundamental challenge, but does our inability to create perfectly aligned models preclude obtaining the benefits of alignment? We study a strategic setting where a human user interacts with multiple differently misaligned AI agents, none of which are individually well-aligned. Our key insight is that when the users utility lies approximately within the convex hull of the agents utilities, a condition that becomes easier to satisfy as model diversity increases, strategic competition can yield outcomes comparable to interacting with a perfectly aligned model. We model this as a multi-leader Stackelberg game, extending Bayesian persuasion to multi-round conversations between differently informed parties, and prove three results: (1) when perfect alignment would allow the user to learn her Bayes-optimal action, she can also do so in all equilibria under the convex hull condition (2) under weaker assumptions requiring only approximate utility learning, a non-strategic user employing quantal response achieves near-optimal utility in all equilibria and (3) when the user selects the best single AI after an evaluation period, equilibrium guarantees remain near-optimal without further distributional assumptions. We complement the theory with two sets of experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15090v1</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Natalie Collina, Surbhi Goel, Aaron Roth, Emily Ryu, Mirah Shi</dc:creator>
    </item>
    <item>
      <title>Diversity in Choice as Majorization</title>
      <link>https://arxiv.org/abs/2407.17589</link>
      <description>arXiv:2407.17589v2 Announce Type: replace 
Abstract: We propose a framework that uses majorization to model diversity and representativeness in school admissions. We generalize the standard notion of majorization to accommodate arbitrary distributional targets, such as a student body that reflects the population served by the school. Building on this framework, we introduce and axiomatically characterize the $r$-targeting Schur choice rule, which balances diversity and priority in admissions. We show that this rule is optimal: any alternative rule must either leave seats unfilled, reduce diversity, or admit lower-priority students. The rule satisfies path independence (and substitutability), which guarantees desirable outcomes in matching markets. Our work contributes to the ongoing discourse on market design by providing a new and flexible framework for improving diversity and representation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17589v2</guid>
      <category>econ.TH</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Echenique, Teddy Mekonnen, M. Bumin Yenmez</dc:creator>
    </item>
    <item>
      <title>Privacy-Aware Sequential Learning</title>
      <link>https://arxiv.org/abs/2502.19525</link>
      <description>arXiv:2502.19525v4 Announce Type: replace 
Abstract: In settings like vaccination registries, individuals act after observing others, and the resulting public records can expose private information. We study privacy-preserving sequential learning, where agents add endogenous noise to their reported actions to conceal private signals. Efficient social learning relies on information flow, seemingly in conflict with privacy. Surprisingly, with continuous signals and a fixed privacy budget $(\varepsilon)$, the optimal randomization strategy balances privacy and accuracy, accelerating learning to $\Theta_{\varepsilon}(\log n)$, faster than the nonprivate $\Theta(\sqrt{\log n})$ rate. In the nonprivate baseline, the expected time to the first correct action and the number of incorrect actions diverge; under privacy with sufficiently small $\varepsilon$, both are finite. Privacy helps because, under the false state, agents more often receive signals contradicting the majority; randomization then asymmetrically amplifies the log-likelihood ratio, enhancing aggregation. In heterogeneous populations, an order-optimal $\Theta(\sqrt{n})$ rate is achievable when a subset of agents have low privacy budgets. With binary signals, however, privacy reduces informativeness and impairs learning relative to the nonprivate baseline, though the dependence on $\varepsilon$ is nonmonotone. Our results show how privacy reshapes information dynamics and inform the design of platforms and policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19525v4</guid>
      <category>econ.TH</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxin Liu, M. Amin Rahimian</dc:creator>
    </item>
    <item>
      <title>Equity in strategic exchange</title>
      <link>https://arxiv.org/abs/2504.05678</link>
      <description>arXiv:2504.05678v5 Announce Type: replace 
Abstract: New fairness notions aligned with the merit principle are proposed for designing exchange rules. We show that for an obviously strategy-proof, efficient and individually rational rule, (i) an agent receives her favorite object when others unanimously perceive her object the best, if and only if preferences are single-peaked, and (ii) an upper bound on fairness attainable is that, if two agents' objects are considered the best by all agents partitioned evenly into two groups, it is guaranteed that one, not both, gets her favorite object. This indicates an unambiguous trade-off between incentives and fairness in the design of exchange rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05678v5</guid>
      <category>econ.TH</category>
      <pubDate>Fri, 19 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Liu, Huaxia Zeng</dc:creator>
    </item>
  </channel>
</rss>
