<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Dec 2024 04:25:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Conditional Density Estimation with Neural Networks for Bias Correction of Multivariate Climate Model Data</title>
      <link>https://arxiv.org/abs/2411.18799</link>
      <description>arXiv:2411.18799v1 Announce Type: new 
Abstract: Global Climate Models (GCMs) are numerical models that simulate complex physical processes within the Earth's climate system, and are essential for understanding and predicting climate change. However, GCMs suffer from systemic biases due to assumptions about and simplifications made to the underlying physical processes. GCM output therefore needs to be bias corrected before it can be used for future climate projections. Most common bias correction methods, however, cannot preserve spatial, temporal, or inter-variable dependencies. We propose a new bias correction method based on conditional density estimation for the simultaneous bias correction of daily precipitation and maximum temperature data obtained from gridded GCM spatial fields. The Vecchia approximation is employed to preserve dependencies in the data, and conditional density estimation is carried out using semi-parametric quantile regression. Illustration on historical data from 1951-2014 over two 5 x 5 spatial grids in the US indicate that our method can preserve key marginal and joint distribution properties of precipitation and maximum temperature, and predictions obtained using our approach are better calibrated compared to predictions using asynchronous quantile mapping and canonical correlation analysis, two commonly used alternative bias correction approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18799v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Reetam Majumder, Shiqi Fang, Arumugam Sankarasubramanian, Emily C. Hector, Brian J. Reich</dc:creator>
    </item>
    <item>
      <title>ZIPG-SK: A Novel Knockoff-Based Approach for Variable Selection in Multi-Source Count Data</title>
      <link>https://arxiv.org/abs/2411.18986</link>
      <description>arXiv:2411.18986v1 Announce Type: new 
Abstract: The rapid development of sequencing technology has generated complex, highly skewed, and zero-inflated multi-source count data. This has posed significant challenges in variable selection, which is crucial for uncovering shared disease mechanisms, such as tumor development and metabolic dysregulation. In this study, we propose a novel variable selection method called Zero-Inflated Poisson-Gamma based Simultaneous knockoff (ZIPG-SK) for multi-source count data. To address the highly skewed and zero-inflated properties of count data, we introduce a Gaussian copula based on the ZIPG distribution for constructing knockoffs, while also incorporating the information of covariates. This method successfully detects common features related to the results in multi-source data while controlling the false discovery rate (FDR). Additionally, our proposed method effectively combines e-values to enhance power. Extensive simulations demonstrate the superiority of our method over Simultaneous Knockoff and other existing methods in processing count data, as it improves power across different scenarios. Finally, we validated the method by applying it to two real-world multi-source datasets: colorectal cancer (CRC) and type 2 diabetes (T2D). The identified variable characteristics are consistent with existing studies and provided additional insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18986v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shan Tang, Shanjun Mao, Shourong Ma, Falong Tan</dc:creator>
    </item>
    <item>
      <title>Scaling Methods To Estimate Macroscopic Fundamental Diagrams in Urban Networks with Sparse Sensor Coverage</title>
      <link>https://arxiv.org/abs/2411.19721</link>
      <description>arXiv:2411.19721v1 Announce Type: new 
Abstract: Accurately estimating traffic variables across unequipped portions of a network remains a significant challenge due to the limited coverage of sensor-equipped links, such as loop detectors and probe vehicles. A common approach is to apply uniform scaling, treating unequipped links as equivalent to equipped ones. This study introduces a novel framework to improve traffic variable estimation by integrating statistical scaling methods with geospatial imputation techniques. Two main approaches are proposed: (1) Statistical Scaling, which includes hierarchical and non-hierarchical network approaches, and (2) Geospatial Imputation, based on variogram modeling. The hierarchical scaling method categorizes the network into several levels according to spatial and functional characteristics, applying tailored scaling factors to each category. In contrast, the non-hierarchical method uses a uniform scaling factor across all links, ignoring network heterogeneity. The variogram-based geospatial imputation leverages spatial correlations to estimate traffic variables for unequipped links, capturing spatial dependencies in urban road networks. Validation results indicate that the hierarchical scaling approach provides the most accurate estimates, achieving reliable performance even with as low as 5% uniform detector coverage. Although the variogram-based method yields strong results, it is slightly less effective than the hierarchical scaling approach but outperforms the non-hierarchical method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19721v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nandan Maiti, Manon Seppecher, Ludovic Leclercq</dc:creator>
    </item>
    <item>
      <title>Temporal Models for Demographic and Global Health Outcomes in Multiple Populations: Introducing the Normal-with-Optional-Shrinkage Data Model Class</title>
      <link>https://arxiv.org/abs/2411.18646</link>
      <description>arXiv:2411.18646v1 Announce Type: cross 
Abstract: Statistical models are used to produce estimates of demographic and global health indicators in populations with limited data. Such models integrate multiple data sources to produce estimates and forecasts with uncertainty based on model assumptions. Model assumptions can be divided into assumptions that describe latent trends in the indicator of interest versus assumptions on the data generating process of the observed data, conditional on the latent process value. Focusing on the latter, we introduce a class of data models that can be used to combine data from multiple sources with various reporting issues. The proposed data model accounts for sampling errors and differences in observational uncertainty based on survey characteristics. In addition, the data model employs horseshoe priors to produce estimates that are robust to outlying observations. We refer to the data model class as the normal-with-optional-shrinkage (NOS) set up. We illustrate the use of the NOS data model for the estimation of modern contraceptive use and other family planning indicators at the national level for countries globally, using survey data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18646v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Leontine Alkema, Herbert Susmann, Evan Ray</dc:creator>
    </item>
    <item>
      <title>A Bayesian semi-parametric approach to causal mediation for longitudinal mediators and time-to-event outcomes with application to a cardiovascular disease cohort study</title>
      <link>https://arxiv.org/abs/2411.18739</link>
      <description>arXiv:2411.18739v1 Announce Type: cross 
Abstract: Causal mediation analysis of observational data is an important tool for investigating the potential causal effects of medications on disease-related risk factors, and on time-to-death (or disease progression) through these risk factors. However, when analyzing data from a cohort study, such analyses are complicated by the longitudinal structure of the risk factors and the presence of time-varying confounders. Leveraging data from the Atherosclerosis Risk in Communities (ARIC) cohort study, we develop a causal mediation approach, using (semi-parametric) Bayesian Additive Regression Tree (BART) models for the longitudinal and survival data. Our framework allows for time-varying exposures, confounders, and mediators, all of which can either be continuous or binary. We also identify and estimate direct and indirect causal effects in the presence of a competing event. We apply our methods to assess how medication, prescribed to target cardiovascular disease (CVD) risk factors, affects the time-to-CVD death.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18739v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saurabh Bhandari, Michael J. Daniels, Maria Josefsson, Donald M. Lloyd-Jones, Juned Siddique</dc:creator>
    </item>
    <item>
      <title>Difference-in-differences Design with Outcomes Missing Not at Random</title>
      <link>https://arxiv.org/abs/2411.18772</link>
      <description>arXiv:2411.18772v1 Announce Type: cross 
Abstract: This paper addresses one of the most prevalent problems encountered by political scientists working with difference-in-differences (DID) design: missingness in panel data. A common practice for handling missing data, known as complete case analysis, is to drop cases with any missing values over time. A more principled approach involves using nonparametric bounds on causal effects or applying inverse probability weighting based on baseline covariates. Yet, these methods are general remedies that often under-utilize the assumptions already imposed on panel structure for causal identification. In this paper, I outline the pitfalls of complete case analysis and propose an alternative identification strategy based on principal strata. To be specific, I impose parallel trends assumption within each latent group that shares the same missingness pattern (e.g., always-respondents, if-treated-respondents) and leverage missingness rates over time to estimate the proportions of these groups. Building on this, I tailor Lee bounds, a well-known nonparametric bounds under selection bias, to partially identify the causal effect within the DID design. Unlike complete case analysis, the proposed method does not require independence between treatment selection and missingness patterns, nor does it assume homogeneous effects across these patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18772v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sooahn Shin</dc:creator>
    </item>
    <item>
      <title>Penetrance Estimation in Family-based Studies with the penetrance R package</title>
      <link>https://arxiv.org/abs/2411.18816</link>
      <description>arXiv:2411.18816v1 Announce Type: cross 
Abstract: Reliable methods for penetrance estimation are critical to improving clinical decision making and risk assessment for hereditary cancer syndromes. Penetrance is defined as the proportion of individuals who carry a genetic variant (i.e., genotype) that causes a trait and show symptoms of that trait, such as cancer (i.e., phenotype). We introduce penetrance, an open-source R package, to estimate age-specific penetrance from pedigree data. The package employs a Bayesian estimation approach, allowing for the incorporation of prior knowledge through the specification of priors for the parameters of the carrier distribution. It also includes options to impute missing ages during the estimation process, addressing incomplete age data in pedigree datasets. Our software provides a flexible and user-friendly tool for researchers to estimate penetrance in complex family-based studies, facilitating improved genetic risk assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18816v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Kubista, Danielle Braun, Giovanni Parmigiani</dc:creator>
    </item>
    <item>
      <title>Improved order selection method for hidden Markov models: a case study with movement data</title>
      <link>https://arxiv.org/abs/2411.18826</link>
      <description>arXiv:2411.18826v1 Announce Type: cross 
Abstract: Hidden Markov models (HMMs) are a versatile statistical framework commonly used in ecology to characterize behavioural patterns from animal movement data. In HMMs, the observed data depend on a finite number of underlying hidden states, generally interpreted as the animal's unobserved behaviour. The number of states is a crucial parameter, controlling the trade-off between ecological interpretability of behaviours (fewer states) and the goodness of fit of the model (more states). Selecting the number of states, commonly referred to as order selection, is notoriously challenging. Common model selection metrics, such as AIC and BIC, often perform poorly in determining the number of states, particularly when models are misspecified. Building on existing methods for HMMs and mixture models, we propose a double penalized likelihood maximum estimate (DPMLE) for the simultaneous estimation of the number of states and parameters of non-stationary HMMs. The DPMLE differs from traditional information criteria by using two penalty functions on the stationary probabilities and state-dependent parameters. For non-stationary HMMs, forward and backward probabilities are used to approximate stationary probabilities. Using a simulation study that includes scenarios with additional complexity in the data, we compare the performance of our method with that of AIC and BIC. We also illustrate how the DPMLE differs from AIC and BIC using narwhal (Monodon monoceros) movement data. The proposed method outperformed AIC and BIC in identifying the correct number of states under model misspecification. Furthermore, its capacity to handle non-stationary dynamics allowed for more realistic modeling of complex movement data, offering deeper insights into narwhal behaviour. Our method is a powerful tool for order selection in non-stationary HMMs, with potential applications extending beyond the field of ecology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18826v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fanny Dupont, Marianne Marcoux, Nigel Hussey, Marie Auger-M\'eth\'e</dc:creator>
    </item>
    <item>
      <title>A Unified Bayesian Framework for Mortality Model Selection</title>
      <link>https://arxiv.org/abs/2411.19176</link>
      <description>arXiv:2411.19176v1 Announce Type: cross 
Abstract: In recent years, a wide range of mortality models has been proposed to address the diverse factors influencing mortality rates, which has highlighted the need to perform model selection. Traditional mortality model selection methods, such as AIC and BIC, often require fitting multiple models independently and ranking them based on these criteria. This process can fail to account for uncertainties in model selection, which can lead to overly optimistic prediction interval, and it disregards the potential insights from combining models. To address these limitations, we propose a novel Bayesian model selection framework that integrates model selection and parameter estimation into the same process. This requires creating a model building framework that will give rise to different models by choosing different parametric forms for each term. Inference is performed using the reversible jump Markov chain Monte Carlo algorithm, which is devised to allow for transition between models of different dimensions, as is the case for the models considered here. We develop modelling frameworks for data stratified by age and period and for data stratified by age, period and product. Our results are presented in two case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19176v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Diana, Jackie Wong Siaw Tze, Aniketh Pittea</dc:creator>
    </item>
    <item>
      <title>Annealed variational mixtures for disease subtyping and biomarker discovery</title>
      <link>https://arxiv.org/abs/2411.19262</link>
      <description>arXiv:2411.19262v1 Announce Type: cross 
Abstract: Cluster analyses of high-dimensional data are often hampered by the presence of large numbers of variables that do not provide relevant information, as well as the perennial issue of choosing an appropriate number of clusters. These challenges are frequently encountered when analysing `omics datasets, such as in molecular precision medicine, where a key goal is to identify disease subtypes and the biomarkers that define them. Here we introduce an annealed variational Bayes algorithm for fitting high-dimensional mixture models while performing variable selection. Our algorithm is scalable and computationally efficient, and we provide an open source Python implementation, VBVarSel. In a range of simulated and real biomedical examples, we show that VBVarSel outperforms the current state of the art, and demonstrate its use for cancer subtyping and biomarker discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19262v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emma Prevot, Rory Toogood, Filippo Pagani, Paul D. W. Kirk</dc:creator>
    </item>
    <item>
      <title>Random Effects Misspecification and its Consequences for Prediction in Generalized Linear Mixed Models</title>
      <link>https://arxiv.org/abs/2411.19384</link>
      <description>arXiv:2411.19384v1 Announce Type: cross 
Abstract: When fitting generalized linear mixed models (GLMMs), one important decision to make relates to the choice of the random effects distribution. As the random effects are unobserved, misspecification of this distribution is a real possibility. In this article, we investigate the consequences of random effects misspecification for point prediction and prediction inference in GLMMs, a topic on which there is considerably less research compared to consequences for parameter estimation and inference. We use theory, simulation, and a real application to explore the effect of using the common normality assumption for the random effects distribution when the correct specification is a mixture of normal distributions, focusing on the impacts on point prediction, mean squared prediction errors (MSEPs), and prediction intervals. We found that the optimal shrinkage is different under the two random effect distributions, so is impacted by misspecification. The unconditional MSEPs for the random effects are almost always larger under the misspecified normal random effects distribution, especially when cluster sizes are small. Results for the MSEPs conditional on the random effects are more complicated, but they remain generally larger under the misspecified distribution when the true random effect is close to the mean of one of the component distributions in the true mixture distribution. Results for prediction intervals indicate that overall coverage probability is not greatly impacted by misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19384v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quan Vu, Francis K. C. Hui, Samuel Muller, A. H. Welsh</dc:creator>
    </item>
    <item>
      <title>Bayesian Hierarchical Modeling for Predicting Spatially Correlated Curves in Irregular Domains: A Case Study on PM10 Pollution</title>
      <link>https://arxiv.org/abs/2411.19425</link>
      <description>arXiv:2411.19425v1 Announce Type: cross 
Abstract: This study presents a Bayesian hierarchical model for analyzing spatially correlated functional data and handling irregularly spaced observations. The model uses Bernstein polynomial (BP) bases combined with autoregressive random effects, allowing for nuanced modeling of spatial correlations between sites and dependencies of observations within curves. Moreover, the proposed procedure introduces a distinct structure for the random effect component compared to previous works. Simulation studies conducted under various challenging scenarios verify the model's robustness, demonstrating its capacity to accurately recover spatially dependent curves and predict observations at unmonitored locations. The model's performance is further supported by its application to real-world data, specifically PM$_{10}$ particulate matter measurements from a monitoring network in Mexico City. This application is of practical importance, as particles can penetrate the respiratory system and aggravate various health conditions. The model effectively predicts concentrations at unmonitored sites, with uncertainty estimates that reflect spatial variability across the domain. This new methodology provides a flexible framework for the FDA in spatial contexts and addresses challenges in analyzing irregular domains with potential applications in environmental monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19425v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alvaro Alexander Burbano Moreno, Ronaldo Dias</dc:creator>
    </item>
    <item>
      <title>Multimodal Whole Slide Foundation Model for Pathology</title>
      <link>https://arxiv.org/abs/2411.19666</link>
      <description>arXiv:2411.19666v1 Announce Type: cross 
Abstract: The field of computational pathology has been transformed with recent advances in foundation models that encode histopathology region-of-interests (ROIs) into versatile and transferable feature representations via self-supervised learning (SSL). However, translating these advancements to address complex clinical challenges at the patient and slide level remains constrained by limited clinical data in disease-specific cohorts, especially for rare clinical conditions. We propose TITAN, a multimodal whole slide foundation model pretrained using 335,645 WSIs via visual self-supervised learning and vision-language alignment with corresponding pathology reports and 423,122 synthetic captions generated from a multimodal generative AI copilot for pathology. Without any finetuning or requiring clinical labels, TITAN can extract general-purpose slide representations and generate pathology reports that generalize to resource-limited clinical scenarios such as rare disease retrieval and cancer prognosis. We evaluate TITAN on diverse clinical tasks and find that TITAN outperforms both ROI and slide foundation models across machine learning settings such as linear probing, few-shot and zero-shot classification, rare cancer retrieval and cross-modal retrieval, and pathology report generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19666v1</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tong Ding, Sophia J. Wagner, Andrew H. Song, Richard J. Chen, Ming Y. Lu, Andrew Zhang, Anurag J. Vaidya, Guillaume Jaume, Muhammad Shaban, Ahrong Kim, Drew F. K. Williamson, Bowen Chen, Cristina Almagro-Perez, Paul Doucet, Sharifa Sahai, Chengkuan Chen, Daisuke Komura, Akihiro Kawabe, Shumpei Ishikawa, Georg Gerber, Tingying Peng, Long Phi Le, Faisal Mahmood</dc:creator>
    </item>
    <item>
      <title>Musical composition and 2D cellular automata based on music intervals</title>
      <link>https://arxiv.org/abs/2411.19844</link>
      <description>arXiv:2411.19844v1 Announce Type: cross 
Abstract: This study is a theoretical approach for exploring the applicability of a 2D cellular automaton based on melodic and harmonic intervals in random arrays of musical notes. The aim of this study was to explore alternatives uses for a cellular automaton in the musical context for better understanding the musical creativity. We used the complex systems and humanities approaches as a framework for capturing the essence of creating music based on rules of music theory. Findings suggested that such rules matter for generating large-scale patterns of organized notes. Therefore, our formulation provides a novel approach for understanding and replicating aspects of the musical creativity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19844v1</guid>
      <category>cs.SD</category>
      <category>cs.CY</category>
      <category>eess.AS</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Igor Lugo, Martha G. Alatriste-Contreras</dc:creator>
    </item>
    <item>
      <title>Thompson, Ulam, or Gauss? Multi-criteria recommendations for posterior probability computation methods in Bayesian response-adaptive trials</title>
      <link>https://arxiv.org/abs/2411.19871</link>
      <description>arXiv:2411.19871v1 Announce Type: cross 
Abstract: To implement a Bayesian response-adaptive trial it is necessary to evaluate a sequence of posterior probabilities. This sequence is often approximated by simulation due to the unavailability of closed-form formulae to compute it exactly. Approximating these probabilities by simulation can be computationally expensive and impact the accuracy or the range of scenarios that may be explored. An alternative approximation method based on Gaussian distributions can be faster but its accuracy is not guaranteed. The literature lacks practical recommendations for selecting approximation methods and comparing their properties, particularly considering trade-offs between computational speed and accuracy. In this paper, we focus on the case where the trial has a binary endpoint with Beta priors. We first outline an efficient way to compute the posterior probabilities exactly for any number of treatment arms. Then, using exact probability computations, we show how to benchmark calculation methods based on considerations of computational speed, patient benefit, and inferential accuracy. This is done through a range of simulations in the two-armed case, as well as an analysis of the three-armed Established Status Epilepticus Treatment Trial. Finally, we provide practical guidance for which calculation method is most appropriate in different settings, and how to choose the number of simulations if the simulation-based approximation method is used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19871v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Kaddaj, Lukas Pin, Stef Baas, Edwin Y. N. Tang, David S. Robertson, Sof\'ia S. Villar</dc:creator>
    </item>
    <item>
      <title>Tomographic reconstruction of a disease transmission landscape via GPS recorded random paths</title>
      <link>https://arxiv.org/abs/2404.04455</link>
      <description>arXiv:2404.04455v3 Announce Type: replace 
Abstract: Identifying areas in a landscape where individuals have a higher likelihood of disease infection is key to managing diseases. Unlike conventional methods relying on ecological assumptions, we perform a novel epidemiological tomography for the estimation of landscape propensity to disease infection, using GPS animal tracks in a manner analogous to tomographic techniques in positron emission tomography (PET). Treating tracking data as random Radon transforms, we analyze Cervid movements in a game preserve, paired with antibody levels for epizootic hemorrhagic disease virus (EHDV) -- a vector-borne disease transmitted by biting midges. After discretizing the field and building the regression matrix of the time spent by each deer (row) at each point of the lattice (column), we model the binary response (infected or not) as a binomial linear inverse problem where spatial coherence is enforced with a total variation regularization. The smoothness of the reconstructed propensity map is selected by the quantile universal threshold. To address limitations of small sample sizes and evaluate significance of our estimates, we quantify uncertainty using a bootstrap-based data augmentation procedure. Our method outperforms alternative ones when using simulated and real data. This tomographic framework is novel, with no established statistical methods tailored for such data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04455v3</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jairo Diaz-Rodriguez, Juan Pablo Gomez, Jeremy P. Orange, Nathan D. Burkett-Cadena, Samantha M. Wisely, Jason K. Blackburn, Sylvain Sardy</dc:creator>
    </item>
    <item>
      <title>Using iterated local alignment to aggregate trajectory data into a traffic flow map</title>
      <link>https://arxiv.org/abs/2406.17500</link>
      <description>arXiv:2406.17500v3 Announce Type: replace 
Abstract: Vehicle trajectories, with their detailed geolocations, are a promising data source to compute traffic flow maps which facilitate the understanding of traffic flows at scales ranging from the city/regional level to the road level. The trade-off is that trajectory data are prone to measurement noise. While this is negligible for large-scale flow aggregation, it poses substantial obstacles for small-scale aggregation. To overcome these obstacles, we introduce innovative local alignment algorithms, where we infer road segments to serve as local reference segments, and proceed to align nearby road segments to them. We then deploy these algorithms in an iterative workflow to compute locally aligned flow maps. By applying this workflow to synthetic and empirical trajectories, we verify that our locally aligned flow maps provide high levels of accuracy and spatial resolution of flow aggregation at multiple scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17500v3</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tarn Duong</dc:creator>
    </item>
    <item>
      <title>A Probabilistic Framework for Estimating the Modal Age at Death</title>
      <link>https://arxiv.org/abs/2411.09800</link>
      <description>arXiv:2411.09800v3 Announce Type: replace 
Abstract: The modal age at death is a critical measure for understanding longevity and mortality patterns. However, existing methods primarily focus on point estimates, overlooking the inherent variability and uncertainty in mortality data. This study addresses this gap by introducing a probabilistic framework for estimating the probability distribution of the modal age at death. Using a multinomial model for age-specific death counts and leveraging a Gaussian approximation, our methodology captures variability while aligning with the categorical nature of mortality data. Application to mortality data from six countries (1960-2020) reinforces the framework's effectiveness in revealing gender differences, temporal trends, and variability across populations. By quantifying uncertainty and improving robustness to data fluctuations, this approach offers valuable insights for demographic research and policy planning</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09800v3</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Silvio C. Patricio</dc:creator>
    </item>
    <item>
      <title>On Consistency of Signature Using Lasso</title>
      <link>https://arxiv.org/abs/2305.10413</link>
      <description>arXiv:2305.10413v4 Announce Type: replace-cross 
Abstract: Signatures are iterated path integrals of continuous and discrete-time processes, and their universal nonlinearity linearizes the problem of feature selection in time series data analysis. This paper studies the consistency of signature using Lasso regression, both theoretically and numerically. We establish conditions under which the Lasso regression is consistent both asymptotically and in finite sample. Furthermore, we show that the Lasso regression is more consistent with the It\^o signature for time series and processes that are closer to the Brownian motion and with weaker inter-dimensional correlations, while it is more consistent with the Stratonovich signature for mean-reverting time series and processes. We demonstrate that signature can be applied to learn nonlinear functions and option prices with high accuracy, and the performance depends on properties of the underlying process and the choice of the signature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.10413v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xin Guo, Binnan Wang, Ruixun Zhang, Chaoyi Zhao</dc:creator>
    </item>
    <item>
      <title>Asymptotics for non-degenerate multivariate $U$-statistics with estimated nuisance parameters under the null and local alternative hypotheses</title>
      <link>https://arxiv.org/abs/2401.11272</link>
      <description>arXiv:2401.11272v3 Announce Type: replace-cross 
Abstract: The large-sample behavior of non-degenerate multivariate $U$-statistics of arbitrary degree is investigated under the assumption that their kernel depends on parameters that can be estimated consistently. Mild regularity conditions are provided which guarantee that once properly normalized, such statistics are asymptotically multivariate Gaussian both under the null hypothesis and sequences of local alternatives. The work of Randles (1982, Ann. Statist.) is extended in three ways: the data and the kernel values can be multivariate rather than univariate, the limiting behavior under local alternatives is studied for the first time, and the effect of knowing some of the nuisance parameters is quantified. These results can be applied to a broad range of goodness-of-fit testing contexts, as shown in two specific examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11272v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of Multivariate Analysis (2025)</arxiv:journal_reference>
      <dc:creator>Alain Desgagn\'e, Christian Genest, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Assessing biomedical knowledge robustness in large language models by query-efficient sampling attacks</title>
      <link>https://arxiv.org/abs/2402.10527</link>
      <description>arXiv:2402.10527v3 Announce Type: replace-cross 
Abstract: The increasing depth of parametric domain knowledge in large language models (LLMs) is fueling their rapid deployment in real-world applications. Understanding model vulnerabilities in high-stakes and knowledge-intensive tasks is essential for quantifying the trustworthiness of model predictions and regulating their use. The recent discovery of named entities as adversarial examples (i.e. adversarial entities) in natural language processing tasks raises questions about their potential impact on the knowledge robustness of pre-trained and finetuned LLMs in high-stakes and specialized domains. We examined the use of type-consistent entity substitution as a template for collecting adversarial entities for billion-parameter LLMs with biomedical knowledge. To this end, we developed an embedding-space attack based on powerscaled distance-weighted sampling to assess the robustness of their biomedical knowledge with a low query budget and controllable coverage. Our method has favorable query efficiency and scaling over alternative approaches based on random sampling and blackbox gradient-guided search, which we demonstrated for adversarial distractor generation in biomedical question answering. Subsequent failure mode analysis uncovered two regimes of adversarial entities on the attack surface with distinct characteristics and we showed that entity substitution attacks can manipulate token-wise Shapley value explanations, which become deceptive in this setting. Our approach complements standard evaluations for high-capacity models and the results highlight the brittleness of domain knowledge in LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10527v3</guid>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>R. Patrick Xian, Alex J. Lee, Satvik Lolla, Vincent Wang, Qiming Cui, Russell Ro, Reza Abbasi-Asl</dc:creator>
    </item>
    <item>
      <title>mmWave Radar for Sit-to-Stand Analysis: A Comparative Study with Wearables and Kinect</title>
      <link>https://arxiv.org/abs/2411.14656</link>
      <description>arXiv:2411.14656v2 Announce Type: replace-cross 
Abstract: This study explores a novel approach for analyzing Sit-to-Stand (STS) movements using millimeter-wave (mmWave) radar technology. The goal is to develop a non-contact sensing, privacy-preserving, and all-day operational method for healthcare applications, including fall risk assessment. We used a 60GHz mmWave radar system to collect radar point cloud data, capturing STS motions from 45 participants. By employing a deep learning pose estimation model, we learned the human skeleton from Kinect built-in body tracking and applied Inverse Kinematics (IK) to calculate joint angles, segment STS motions, and extract commonly used features in fall risk assessment. Radar extracted features were then compared with those obtained from Kinect and wearable sensors. The results demonstrated the effectiveness of mmWave radar in capturing general motion patterns and large joint movements (e.g., trunk). Additionally, the study highlights the advantages and disadvantages of individual sensors and suggests the potential of integrated sensor technologies to improve the accuracy and reliability of motion analysis in clinical and biomedical research settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14656v2</guid>
      <category>eess.SP</category>
      <category>cs.ET</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuting Hu, Peggy Ackun, Xiang Zhang, Siyang Cao, Jennifer Barton, Melvin G. Hector, Mindy J. Fain, Nima Toosizadeh</dc:creator>
    </item>
  </channel>
</rss>
