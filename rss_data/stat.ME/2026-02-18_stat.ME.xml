<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Feb 2026 02:33:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Natural direct effects of vaccines and post-vaccination behaviour</title>
      <link>https://arxiv.org/abs/2602.15095</link>
      <description>arXiv:2602.15095v1 Announce Type: new 
Abstract: Knowledge of the protection afforded by vaccines might, in some circumstances, modify a vaccinated individual's behaviour, potentially increasing exposure to pathogens and hindering effectiveness. Although vaccine studies typically do not explicitly account for this possibility in their analyses, we argue that natural direct effects might represent appropriate causal estimands when an objective is to quantify the effect of vaccination on disease while blocking its influence on behaviour. There are, however, complications of a practical nature for the estimation of natural direct effects in this context. Here, we discuss some of these issues, including exposure-outcome and mediator-outcome confounding by healthcare seeking behaviour, and possible approaches to facilitate estimates of these effects. This work highlights the importance of data collection on behaviour, of assessing whether vaccination induces riskier behaviour, and of understanding the potential effects of interventions on vaccination that could turn off vaccine's influence on behaviour.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15095v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bronner P. Gon\c{c}alves, Piero L. Olliaro, Sheena G. Sullivan, Benjamin J. Cowling</dc:creator>
    </item>
    <item>
      <title>bayesics: Core Statistical Methods via Bayesian Inference in R</title>
      <link>https://arxiv.org/abs/2602.15150</link>
      <description>arXiv:2602.15150v1 Announce Type: new 
Abstract: Bayesian statistics is an integral part of contemporary applied science. bayesics provides a single framework, unified in syntax and output, for performing the most commonly used statistical procedures, ranging from one- and two-sample inference to general mediation analysis. bayesics leans hard away from the requirement that users be familiar with sampling algorithms by using closed-form solutions whenever possible, and automatically selecting the number of posterior samples required for accurate inference when such solutions are not possible. bayesics} focuses on providing key inferential quantities: point estimates, credible intervals, probability of direction, region of practical equivalance (ROPE), and, when applicable, Bayes factors. While algorithmic assessment is not required in bayesics, model assessment is still critical; towards that, bayesics provides diagnostic plots for parametric inference, including Bayesian p-values. Finally, bayesics provides extensions to models implemented in alternative R packages and, in the case of mediation analysis, correction to existing implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15150v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel K. Sewell, Alan T. Arakkal</dc:creator>
    </item>
    <item>
      <title>Sample size and power determination for assessing overall SNP effects in joint modeling of longitudinal and time-to-event data</title>
      <link>https://arxiv.org/abs/2602.15247</link>
      <description>arXiv:2602.15247v1 Announce Type: new 
Abstract: Longitudinal biomarkers are frequently collected in clinical studies due to their strong association with time-to-event outcomes. While considerable progress has been made in methods for jointly modeling longitudinal and survival data, comparatively little attention has been paid to statistical design considerations, particularly sample size and power calculations, in genetic studies. Yet, appropriate sample size estimation is essential for ensuring adequate power and valid inference. Genetic variants may influence event risk through both direct effects and indirect effects mediated by longitudinal biomarkers. In this paper, we derive a closed-form sample size formula for testing the overall effect of a single nucleotide polymorphism within a joint modeling framework. Simulation studies demonstrate that the proposed formula yields accurate and robust performance in finite samples. We illustrate the practical utility of our method using data from the Diabetes Control and Complications Trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15247v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Bian, Shelley B. Bull</dc:creator>
    </item>
    <item>
      <title>Structural grouping of extreme value models via graph fused lasso</title>
      <link>https://arxiv.org/abs/2602.15291</link>
      <description>arXiv:2602.15291v1 Announce Type: new 
Abstract: The generalized Pareto distribution (GPD) is a fundamental model for analyzing the tail behavior of a distribution. In particular, the shape parameter of the GPD characterizes the extremal properties of the distribution. As described in this paper, we propose a method for grouping shape parameters in the GPD for clustered data via graph fused lasso. The proposed method simultaneously estimates the model parameters and identifies which clusters can be grouped together. We establish the asymptotic theory of the proposed estimator and demonstrate that its variance is lower than that of the cluster-wise estimator. This variance reduction not only enhances estimation stability but also provides a principled basis for identifying homogeneity and heterogeneity among clusters in terms of their tail behavior. We assess the performance of the proposed estimator through Monte Carlo simulations. As an illustrative example, our method is applied to rainfall data from 996 clustered sites across Japan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15291v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuma Yoshida, Koki Momoki, Shuichi Kawano</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Joint Tail Risk in Paired Biomarkers via Archimedean Copulas with Restricted Jeffreys Priors</title>
      <link>https://arxiv.org/abs/2602.15319</link>
      <description>arXiv:2602.15319v1 Announce Type: new 
Abstract: We propose a Bayesian copula-based framework to quantify clinically interpretable joint tail risks from paired continuous biomarkers. After converting each biomarker margin to rank-based pseudo-observations, we model dependence using one-parameter Archimedean copulas and focus on three probability-scale summaries at tail level $\alpha$: the lower-tail joint risk $R_L(\theta)=C_\theta(\alpha,\alpha)$, the upper-tail joint risk $R_U(\theta)=2\alpha-1+C_\theta(1-\alpha,1-\alpha)$, and the conditional lower-tail risk $R_C(\theta)=R_L(\theta)/\alpha$. Uncertainty is quantified via a restricted Jeffreys prior on the copula parameter and grid-based posterior approximation, which induces an exact posterior for each tail-risk functional. In simulations from Clayton and Gumbel copulas across multiple dependence strengths, posterior credible intervals achieve near-nominal coverage for $R_L$, $R_U$, and $R_C$. We then analyze NHANES 2017--2018 fasting glucose (GLU) and HbA1c (GHB) ($n=2887$) at $\alpha=0.05$, obtaining tight posterior credible intervals for both the dependence parameter and induced tail risks. The results reveal markedly elevated extremal co-movement relative to independence; under the Gumbel model, the posterior mean joint upper-tail risk is $R_U(\alpha)=0.0286$, approximately $11.46\times$ the independence benchmark $\alpha^2=0.0025$. Overall, the proposed approach provides a principled, dependence-aware method for reporting joint and conditional extremal-risk summaries with Bayesian uncertainty quantification in biomedical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15319v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agnideep Aich, Md. Monzur Murshed, Sameera Hewage, Ashit Baran Aich</dc:creator>
    </item>
    <item>
      <title>Joint Modeling of Longitudinal EHR Data with Shared Random Effects for Informative Visiting and Observation Processes</title>
      <link>https://arxiv.org/abs/2602.15374</link>
      <description>arXiv:2602.15374v1 Announce Type: new 
Abstract: Longitudinal electronic health record (EHR) data offer opportunities to study biomarker trajectories; however, association estimates-the primary inferential target-from standard models designed for regular observation times may be biased by a two-stage hierarchical missingness mechanism. The first stage is the visiting process (informative presence), where encounters occur at irregular times driven by patient health status; the second is the observation process (informative observation), where biomarkers are selectively measured during visits. To address these mechanisms, we propose a unified semiparametric joint modeling framework that simultaneously characterizes the visiting, biomarker observation, and longitudinal outcome processes. Central to this framework is a shared subject-specific Gaussian latent variable that captures unmeasured frailty and induces dependence across all components. We develop a three-stage estimation procedure and establish the consistency and asymptotic normality of our estimators. We also introduce a sequential procedure that imputes missing biomarkers prior to adjusting for irregular visiting and examine its performance. Simulation results demonstrate that our method yields unbiased estimates under this mechanism, whereas existing approaches can be substantially biased; notably, methods adjusting only for irregular visiting may exhibit even greater bias than those ignoring both mechanisms. We apply our framework to data from the All of Us Research Program to investigate associations between neighborhood-level socioeconomic status indicators and six blood-based biomarker trajectories, providing a robust tool for outpatient settings where irregular monitoring and selective measurement are prevalent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15374v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cheng-Han Yang, Xu Shi, Bhramar Mukherjee</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametrics for Gene-Gene and Gene-Environment Interactions in Case-Control Studies: A Synthesis and Extension</title>
      <link>https://arxiv.org/abs/2602.15387</link>
      <description>arXiv:2602.15387v1 Announce Type: new 
Abstract: Gene-gene and gene-environment interactions are widely believed to play significant roles in explaining the variability of complex traits. While substantial research exists in this area, a comprehensive statistical framework that addresses multiple sources of uncertainty simultaneously remains lacking. In this article, we synthesize and propose extension of a novel class of Bayesian nonparametric approaches that account for interactions among genes, loci, and environmental factors while accommodating uncertainty about population substructure. Our contribution is threefold: (1) We provide a unified exposition of hierarchical Bayesian models driven by Dirichlet processes for genetic interactions, clarifying their conceptual advantages over traditional regression approaches; (2) We shed light on new computational strategies that combine transformation-based MCMC with parallel processing for scalable inference; and (3) We present enhanced hypothesis testing procedures for identifying disease-predisposing loci.Through applications to myocardial infarction data, we demonstrate how these methods offer biological insights not readily obtainable from standard approaches. Our synthesis highlights the advantages of Bayesian nonparametric thinking in genetic epidemiology while providing practical guidance for implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15387v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Durba Bhattacharya, Sourabh Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Space-filling lattice designs for computer experiments</title>
      <link>https://arxiv.org/abs/2602.15390</link>
      <description>arXiv:2602.15390v1 Announce Type: new 
Abstract: This paper investigates the construction of space-filling designs for computer experiments. The space-filling property is characterized by the covering and separation radii of a design, which are integrated through the unified criterion of quasi-uniformity. We focus on a special class of designs, known as quasi-Monte Carlo (QMC) lattice point sets, and propose two construction algorithms. The first algorithm generates rank-1 lattice point sets as an approximation of quasi-uniform Kronecker sequences, where the generating vector is determined explicitly. As a byproduct of our analysis, we prove that this explicit point set achieves an isotropic discrepancy of $O(N^{-1/d})$. The second algorithm utilizes Korobov lattice point sets, employing the Lenstra--Lenstra--Lov\'{a}sz (LLL) basis reduction algorithm to identify the generating vector that ensures quasi-uniformity. Numerical experiments are provided to validate our theoretical claims regarding quasi-uniformity. Furthermore, we conduct empirical comparisons between various QMC point sets in the context of Gaussian process regression, showcasing the efficacy of the proposed designs for computer experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15390v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.NT</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naoki Sakai, Takashi Goda</dc:creator>
    </item>
    <item>
      <title>Confidence Distributions for FIC scores</title>
      <link>https://arxiv.org/abs/2602.15496</link>
      <description>arXiv:2602.15496v1 Announce Type: new 
Abstract: When using the Focused Information Criterion (FIC) for assessing and ranking candidate models with respect to how well they do for a given estimation task, it is customary to produce a so-called FIC plot. This plot has the different point estimates along the y-axis and the root-FIC scores on the x-axis, these being the estimated root-mean-square scores. In this paper we address the estimation uncertainty involved in each of the points of such a FIC plot. This needs careful assessment of each of the estimators from the candidate models, taking also modelling bias into account, along with the relative precision of the associated estimated mean squared error quantities. We use confidence distributions for these endeavours. This leads to fruitful CD-FIC plots, helping the statistician to judge to what extent the seemingly best models really are better than other models, etc. These efforts also lead to two further developments. The first is a new tool for model selection, which we call the quantile FIC, which helps overcome certain difficulties associated with the usual FIC procedures, related to somewhat arbitrary schemes for handling estimated squared biases. A particular case is the median-FIC. The second development is to form model averaged estimators with fruitful weights determined by the relative sizes of the median- and quantile-FIC scores. And Mrs. Jones is pregnant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15496v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>C\'eline Cunen, Nils Lid Hjort</dc:creator>
    </item>
    <item>
      <title>Fixed-Horizon Self-Normalized Inference for Adaptive Experiments via Martingale AIPW/DML with Logged Propensities</title>
      <link>https://arxiv.org/abs/2602.15559</link>
      <description>arXiv:2602.15559v1 Announce Type: new 
Abstract: Adaptive randomized experiments update treatment probabilities as data accrue, but still require an end-of-study interval for the average treatment effect (ATE) at a prespecified horizon. Under adaptive assignment, propensities can keep changing, so the predictable quadratic variation of AIPW/DML score increments may remain random. When no deterministic variance limit exists, Wald statistics normalized by a single long-run variance target can be conditionally miscalibrated given the realized variance regime. We assume no interference, sequential randomization, i.i.d. arrivals, and executed overlap on a prespecified scored set, and we require two auditable pipeline conditions: the platform logs the executed randomization probability for each unit, and the nuisance regressions used to score unit $t$ are constructed predictably from past data only. These conditions make the centered AIPW/DML scores an exact martingale difference sequence. Using self-normalized martingale limit theory, we show that the Studentized statistic, with variance estimated by realized quadratic variation, is asymptotically N(0,1) at the prespecified horizon, even without variance stabilization. Simulations validate the theory and highlight when standard fixed-variance Wald reporting fails.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15559v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Saco</dc:creator>
    </item>
    <item>
      <title>Scenario Approach with Post-Design Certification of User-Specified Properties</title>
      <link>https://arxiv.org/abs/2602.15568</link>
      <description>arXiv:2602.15568v1 Announce Type: new 
Abstract: The scenario approach is an established data-driven design framework that comes equipped with a powerful theory linking design complexity to generalization properties. In this approach, data are simultaneously used both for design and for certifying the design's reliability, without resorting to a separate test dataset. This paper takes a step further by guaranteeing additional properties, useful in post-design usage but not considered during the design phase. To this end, we introduce a two-level framework of appropriateness: baseline appropriateness, which guides the design process, and post-design appropriateness, which serves as a criterion for a posteriori evaluation. We provide distribution-free upper bounds on the risk of failing to meet the post-design appropriateness; these bounds are computable without using any additional test data. Under additional assumptions, lower bounds are also derived. As part of an effort to demonstrate the usefulness of the proposed methodology, the paper presents two practical examples in H2 and pole-placement problems. Moreover, a method is provided to infer comprehensive distributional knowledge of relevant performance indexes from the available dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15568v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.ML</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Algo Car\`e, Marco C. Campi, Simone Garatti</dc:creator>
    </item>
    <item>
      <title>Leicester's Tale: Another Perspective on the EPL 2015/16 Through Expected Goals (xG) Modelling</title>
      <link>https://arxiv.org/abs/2602.15673</link>
      <description>arXiv:2602.15673v1 Announce Type: new 
Abstract: Probabilistic modeling is an effective tool for evaluating team performance and predicting outcomes in sports. However, an important question that hasn't been fully explored is whether these models can reliably reflect actual performance while assigning meaningful probabilities to rare results that differ greatly from expectations. In this study, we create an inference-based probabilistic framework built on expected goals (xG). This framework converts shot-level event data into season-level simulations of points, rankings, and outcome probabilities. Using the English Premier League 2015/16 season as a data, we demonstrate that the framework captures the overall structure of the league table. It correctly identifies the top-four contenders and relegation candidates while explaining a significant portion of the variance in final points and ranks. In a full-season evaluation, the model assigns a low probability to extreme outcomes, particularly Leicester City's historic title win, which stands out as a statistical anomaly. We then look at the ex ante inferential and early-diagnostic role of xG by only using mid-season information. With first-half data, we simulate the rest of the season and show that teams with stronger mid-season xG profiles tend to earn more points in the second half, even after considering their current league position. In this mid-season assessment, Leicester City ranks among the top teams by xG and is given a small but noteworthy chance of winning the league. This suggests that their ultimate success was unlikely but not entirely detached from their actual performance. Our analysis indicates that expected goals models work best as probabilistic baselines for analysis and early-warning diagnostics, rather than as certain predictors of rare season outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15673v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sheikh Badar Ud Din Tahir, Leonardo Egidi, Nicola Torelli</dc:creator>
    </item>
    <item>
      <title>Safe hypotheses testing with application to order restricted inference</title>
      <link>https://arxiv.org/abs/2602.15679</link>
      <description>arXiv:2602.15679v1 Announce Type: new 
Abstract: Hypothesis tests under order restrictions arise in a wide range of scientific applications. By exploiting inequality constraints, such tests can achieve substantial gains in power and interpretability. However, these gains come at a cost: when the imposed constraints are misspecified, the resulting inferences may be misleading or even invalid, and Type III errors may occur, i.e., the null hypothesis may be rejected when neither the null nor the alternative is true. To address this problem, this paper introduces safe tests. Heuristically, a safe test is a testing procedure that is asymptotically free of Type III errors. The proposed test is accompanied by a certificate of validity, a pre--test that assesses whether the original hypotheses are consistent with the data, thereby ensuring that the null hypothesis is rejected only when warranted, enabling principled inference without risk of systematic error. Although the development in this paper focus on testing problems in order--restricted inference, the underlying ideas are more broadly applicable. The proposed methodology is evaluated through simulation studies and the analysis of well--known illustrative data examples, demonstrating strong protection against Type III errors while maintaining power comparable to standard procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15679v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ori Davidov</dc:creator>
    </item>
    <item>
      <title>Generalised Exponential Kernels for Nonparametric Density Estimation</title>
      <link>https://arxiv.org/abs/2602.15731</link>
      <description>arXiv:2602.15731v1 Announce Type: new 
Abstract: This paper introduces a novel kernel density estimator (KDE) based on the generalised exponential (GE) distribution, designed specifically for positive continuous data. The proposed GE KDE offers a mathematically tractable form that avoids the use of special functions, for instance, distinguishing it from the widely used gamma KDE, which relies on the gamma function. Despite its simpler form, the GE KDE maintains similar flexibility and shape characteristics, aligning with distributions such as the gamma, which are known for their effectiveness in modelling positive data. We derive the asymptotic bias and variance of the proposed kernel density estimator, and formally demonstrate the order of magnitude of the remaining terms in these expressions. We also propose a second GE KDE, for which we are able to show that it achieves the optimal mean integrated squared error, something that is difficult to establish for the former. Through numerical experiments involving simulated and real data sets, we show that GE KDEs can be an important alternative and competitive to existing KDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15731v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura M. Craig, Wagner Barreto-Souza</dc:creator>
    </item>
    <item>
      <title>Generative Bayesian Inference with GANs</title>
      <link>https://arxiv.org/abs/2208.12113</link>
      <description>arXiv:2208.12113v3 Announce Type: replace 
Abstract: In the absence of explicit or tractable likelihoods, Bayesians often resort to approximate Bayesian computation (ABC) for inference. Our work bridges ABC with deep neural implicit samplers based on generative adversarial networks (GANs) and adversarial variational Bayes. Both ABC and GANs compare aspects of observed and fake data to simulate from posteriors and likelihoods, respectively. We develop a Bayesian GAN (B-GAN) sampler that directly targets the posterior by solving an adversarial optimization problem. B-GAN is driven by a deterministic mapping learned on the ABC reference by conditional GANs. Once the mapping has been trained, iid posterior samples are obtained by filtering noise at a negligible additional cost. We propose two post-processing local refinements using (1) data-driven proposals with importance reweighting, and (2) variational Bayes. We support our findings with frequentist-Bayesian results, showing that the typical total variation distance between the true and approximate posteriors converges to zero for certain neural network generators and discriminators. Our findings on simulated data show highly competitive performance relative to some of the most recent likelihood-free posterior simulators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.12113v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuexi Wang, Veronika Ro\v{c}kov\'a</dc:creator>
    </item>
    <item>
      <title>Transportability of aggregate trial results to an external environment in causally interpretable meta-analysis</title>
      <link>https://arxiv.org/abs/2408.04854</link>
      <description>arXiv:2408.04854v2 Announce Type: replace 
Abstract: In evidence synthesis, multilevel modeling approaches (MMAs) are commonly employed to combine aggregate data (AD) and individual participant data (IPD). These approaches rely on an aggregate outcome model that is ideally obtained by integrating the prespecified individual- level outcome model over the covariate distribution observed in each eligible study. In non- linear settings, such an integration may however be analytically intractable and requires ap- proximations. In this paper, we propose a novel method for incorporating AD into causal meta-analysis of IPD studies that can overcome this challenge. Rather than relying on an ag- gregate outcome model that is difficult to be correctly formulated, we propose modeling the trial membership as a function of baseline covariates. This model allows one to estimate the individual-level outcome model in each AD study by leveraging IPD available in other trials, and then to transport the treatment effects estimated from both AD and IPD trials to an external target population, even when only aggregate covariate data are available for that population. Unlike previous proposals, we do not require pseudo-IPD to be generated from the aggregate data, which helps minimize bias due to incomplete information on the covariate distribution in each AD trial and in the target population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04854v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tran Trong Khoi Le, Marie-Felicia B\'eclin, Sivem Afach, Tat-Thang Vo</dc:creator>
    </item>
    <item>
      <title>On the Validity of Isotropic Covariance Functions for Set-indexed Random Fields</title>
      <link>https://arxiv.org/abs/2502.15146</link>
      <description>arXiv:2502.15146v3 Announce Type: replace 
Abstract: Distances between sets arise naturally when modeling stochastic dependence on collections of spatial supports, including settings with point-referenced and areal observations. However, commonly used constructions of distances on sets, including those derived from the Hausdorff distance, generally fail to be conditionally negative definite, precluding their use in isotropic covariance models. We propose the ball--Hausdorff distance, defined as the Hausdorff distance between the minimum enclosing balls of bounded sets in a metric space. For length spaces, we derive an explicit representation of this distance in terms of the associated centers and radii. We show that the ball--Hausdorff distance is conditionally negative definite whenever the underlying metric is conditionally negative definite. By Schoenberg's theorem, this implies an isometric embedding into a Hilbert space and guarantees the validity of broad classes of isotropic covariance functions, including the Mat\'ern and powered exponential families, for set-indexed random fields. The construction reduces dependence between sets to low-dimensional geometric summaries, leading to substantial simplifications in covariance evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15146v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas da Cunha Godoy, Marcos Oliveira Prates, Fernando Andr\'es Quintana, Jun Yan</dc:creator>
    </item>
    <item>
      <title>Boosting prediction with data missing not at random</title>
      <link>https://arxiv.org/abs/2502.21276</link>
      <description>arXiv:2502.21276v3 Announce Type: replace 
Abstract: Boosting has emerged as a useful machine learning technique over the past three decades, attracting increased attention. Most advancements in this area, however, have primarily focused on numerical implementation procedures, often lacking rigorous theoretical justifications. Moreover, these approaches are generally designed for datasets with fully observed data, and their validity can be compromised by the presence of missing observations. In this paper, we employ semiparametric estimation approaches to develop boosting prediction methods for data with missing responses. We explore two strategies for adjusting the loss functions to account for missingness effects. The proposed methods are implemented using a functional gradient descent algorithm, and their theoretical properties, including algorithm convergence and estimator consistency, are rigorously established. Numerical studies demonstrate that the proposed methods perform well in finite sample settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21276v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/10618600.2025.2541012</arxiv:DOI>
      <dc:creator>Yuan Bian, Grace Y. Yi, Wenqing He</dc:creator>
    </item>
    <item>
      <title>Quantile Vector Autoregression without Crossing</title>
      <link>https://arxiv.org/abs/2601.04663</link>
      <description>arXiv:2601.04663v2 Announce Type: replace 
Abstract: This paper considers estimation and model selection of quantile vector autoregression (QVAR). Conventional quantile regression often yields undesirable crossing quantile curves, violating the monotonicity of quantiles. To address this issue, we propose a simplex quantile vector autoregression (SQVAR) framework, which transforms the autoregressive (AR) structure of the original QVAR model into a simplex, ensuring that the estimated quantile curves remain monotonic across all quantile levels. In addition, we impose the smoothly clipped absolute deviation (SCAD) penalty on the SQVAR model to mitigate the explosive nature of the parameter space. We further develop a Bayesian information criterion (BIC)-based procedure for selecting the optimal penalty parameter and introduce new frameworks for impulse response analysis of QVAR models. Finally, we establish asymptotic properties of the proposed method, including the convergence rate and asymptotic normality of the estimator, the consistency of AR order selection, and the validity of the BIC-based penalty selection. For illustration, we apply the proposed method to U.S. financial market data, highlighting the usefulness of our SQVAR method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04663v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomohiro Ando, Tadao Hoshino, Ruey Tsay</dc:creator>
    </item>
    <item>
      <title>Robust $M$-Estimation of Scatter Matrices via Precision Structure Shrinkage</title>
      <link>https://arxiv.org/abs/2601.11099</link>
      <description>arXiv:2601.11099v2 Announce Type: replace 
Abstract: Maronna's and Tyler's $M$-estimators are among the most widely used robust estimators for scatter matrices. However, when the dimension of observations is relatively high, their performance can substantially deteriorate in certain situations, particularly in the presence of clustered outliers. To address this issue, we propose an estimator that shrinks the estimated precision matrix toward the identity matrix. We derive a sufficient condition for its existence, discuss its statistical interpretation, and establish upper and lower bounds for its additive finite sample breakdown point. Numerical experiments confirm the robustness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11099v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soma Nikai, Yuichi Goto, Koji Tsukuda</dc:creator>
    </item>
    <item>
      <title>Leave-One-Out Neighborhood Smoothing for Graphons: Berry-Esseen Bounds, Confidence Intervals, and Honest Tuning</title>
      <link>https://arxiv.org/abs/2602.02319</link>
      <description>arXiv:2602.02319v2 Announce Type: replace 
Abstract: Neighborhood smoothing methods achieve minimax-optimal rates for estimating edge probabilities under graphon models, but their use for statistical inference has remained limited. The main obstacle is that classical neighborhood smoothers select data-driven neighborhoods and average edges using the same adjacency matrix, inducing complex dependencies that invalidate standard concentration and normal approximation arguments.
  We introduce a leave-one-out modification of neighborhood smoothing for undirected simple graphs. When estimating a single entry P_ij, the neighborhood of node i is constructed from an adjacency matrix in which the jth row and column are set to zero, thereby decoupling neighborhood selection from the edges being averaged. We show that this construction restores conditional independence of the centered summands, enabling the use of classical probabilistic tools for inference.
  Under piecewise Lipschitz graphon assumptions and logarithmic degree growth, we derive variance-adaptive concentration inequalities based on Bousquet's inequality and establish Berry-Esseen bounds with explicit rates for the normalized estimation error. These results yield both finite-sample and asymptotic confidence intervals for individual edge probabilities. The same leave-one-out structure also supports an honest cross-validation scheme for tuning parameter selection, for which we prove an oracle inequality. The proposed estimator retains the optimal row-wise mean-squared error rates of classical neighborhood smoothing while providing valid entrywise uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02319v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Behzad Aalipur, Rachel Kilby</dc:creator>
    </item>
    <item>
      <title>Robust Design in the Presence of Aleatoric and Epistemic Uncertainty</title>
      <link>https://arxiv.org/abs/2602.13380</link>
      <description>arXiv:2602.13380v2 Announce Type: replace 
Abstract: This paper proposes strategies for designing a system whose computational model is subject to aleatory and epistemic uncertainty. Aleatory variables, which are caused by randomness in physical parameters, are draws from a possibly unknown distribution; whereas epistemic variables, which are caused by ignorance in the value of fixed parameters, are free to take any value in a bounded set. Chance-constrained formulations enforcing the system requirements at a finite number of realizations of the uncertain parameters are proposed. These formulations trade off a lower objective value against a reduced robustness by eliminating an optimally chosen subset of such realizations. Risk-aware designs are obtained by accounting for the severity of the requirement violations resulting from this elimination process. Furthermore, we propose a computationally efficient design approach in which the training dataset is sequentially updated according to the results of high-fidelity reliability analyses of suboptimal designs. Robustness is evaluated by using Monte Carlo analysis and Robust Scenario Theory, with the latter approach accounting for the infinitely many values that the epistemic variables can take.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13380v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luis G. Crespo</dc:creator>
    </item>
    <item>
      <title>The empirical distribution of sequential LS factors in Multi-level Dynamic Factor Models</title>
      <link>https://arxiv.org/abs/2602.14813</link>
      <description>arXiv:2602.14813v2 Announce Type: replace 
Abstract: The research question we answer in this paper is whether the asymptotic distribution derived by Bai (2003) for Principal Components (PC) factors in dynamic factor models (DFMs) can approximate the empirical distribution of the sequential Least Squares (SLS) estimator of global and group-specific factors in multi-level dynamic factor models (ML-DFMs). Monte Carlo experiments confirm that under general forms of the idiosyncratic covariance matrix, the finite-sample distribution of SLS global and group-specific factors can be well approximated using the asymptotic distribution of PC factors. We also analyse the performance of alternative estimators of the asymptotic mean squared error (MSE) of the SLS factors and show that the MSE estimator that allows for idiosyncratic cross-sectional correlation and accounts for estimation uncertainty of factor loadings is best.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14813v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gian Pietro Bellocca, Ignacio Garr\'on, Vladimir Rodr\'iguez-Caballero, Esther Ruiz</dc:creator>
    </item>
    <item>
      <title>Quasi-Bayes properties of a recursive procedure for mixtures</title>
      <link>https://arxiv.org/abs/1902.10708</link>
      <description>arXiv:1902.10708v2 Announce Type: replace-cross 
Abstract: Bayesian methods are often optimal, yet increasing pressure for fast computations, especially with streaming data, brings renewed interest in faster, possibly sub-optimal, solutions. The extent to which these algorithms approximate Bayesian solutions is a question of interest, but often unanswered. We propose a methodology to address this question in predictive settings, when the algorithm can be reinterpreted as a probabilistic predictive rule. We specifically develop the proposed methodology for a recursive procedure for online learning in nonparametric mixture models, often refereed to as Newton's algorithm. This algorithm is simple and fast; however, its approximation properties are unclear. By reinterpreting it as a predictive rule, we can show that it underlies a statistical model which is, asymptotically, a Bayesian, exchangeable mixture model. In this sense, the recursive rule provides a quasi-Bayes solution. While the algorithm only offers a point estimate, our clean statistical formulation allows us to provide the asymptotic posterior distribution and asymptotic credible intervals for the mixing distribution. Moreover, it gives insights for tuning the parameters, as we illustrate in simulation studies, and paves the way to extensions in various directions. Beyond mixture models, our approach can be applied to other predictive algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:1902.10708v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sandra Fortini, Sonia Petrone</dc:creator>
    </item>
    <item>
      <title>IGC-Net for conditional average potential outcome estimation over time</title>
      <link>https://arxiv.org/abs/2405.21012</link>
      <description>arXiv:2405.21012v4 Announce Type: replace-cross 
Abstract: Estimating potential outcomes for treatments over time based on observational data is important for personalized decision-making in medicine. However, many existing methods for this task fail to properly adjust for time-varying confounding and thus yield biased estimates. There are only a few neural methods with proper adjustments, but these have inherent limitations (e.g., division by propensity scores that are often close to zero), which result in poor performance. As a remedy, we introduce the iterative G-computation network (IGC-Net). Our IGC-Net is a novel, neural end-to-end model which adjusts for time-varying confounding in order to estimate conditional average potential outcomes (CAPOs) over time. Specifically, our IGC-Net is the first neural model to perform fully regression-based iterative G-computation for CAPOs in the time-varying setting. We evaluate the effectiveness of our IGC-Net across various experiments. In sum, this work represents a significant step towards personalized decision-making from electronic health records.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.21012v4</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantin Hess, Dennis Frauen, Valentyn Melnychuk, Stefan Feuerriegel</dc:creator>
    </item>
    <item>
      <title>Score-based change point detection via tracking the best of infinitely many experts</title>
      <link>https://arxiv.org/abs/2408.14073</link>
      <description>arXiv:2408.14073v2 Announce Type: replace-cross 
Abstract: We propose an algorithm for nonparametric online change point detection based on sequential score function estimation and the tracking the best expert approach. The core of the procedure is a version of the fixed share forecaster tailored to the case of infinite number of experts and quadratic loss functions. The algorithm shows promising results in numerical experiments on artificial and real-world data sets. Its performance is supported by rigorous high-probability bounds describing behaviour of the test statistic in the pre-change and post-change regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14073v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Markovich, Nikita Puchkin</dc:creator>
    </item>
    <item>
      <title>Counterfactual Survival Q-learning via Buckley-James Boosting, with Applications to ACTG 175 and CALGB 8923</title>
      <link>https://arxiv.org/abs/2508.11060</link>
      <description>arXiv:2508.11060v2 Announce Type: replace-cross 
Abstract: We propose a Buckley James (BJ) Boost Q learning framework for estimating optimal dynamic treatment regimes from right censored survival outcomes in longitudinal randomized clinical trials, motivated by the clinical need to support patient specific treatment decisions when follow up is incomplete and covariate effects may be nonlinear. The method combines accelerated failure time modeling with iterative boosting using flexible base learners, including componentwise least squares and regression trees, within a counterfactual Q learning framework. By modeling conditional survival time directly, BJ Boost Q learning avoids the proportional hazards assumption, yields clinically interpretable time scale contrasts, and enables estimation of stage specific Q functions and individualized decision rules under standard potential outcomes assumptions. In contrast to Cox based Q learning, which relies on hazard modeling and can be sensitive to nonproportional hazards and model misspecification, our approach provides a robust and flexible alternative for regime learning. Simulation studies and analyses of the ACTG175 HIV trial and the CALGB 8923 two stage leukemia trial show that BJ Boost Q learning improves treatment decision accuracy and produces more stable within participant counterfactual contrasts, particularly in multistage settings where estimation error and bias can compound across stages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11060v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeongjin Lee, Jong-Min Kim</dc:creator>
    </item>
    <item>
      <title>Theoretical guarantees for change localization using conformal p-values</title>
      <link>https://arxiv.org/abs/2510.08749</link>
      <description>arXiv:2510.08749v2 Announce Type: replace-cross 
Abstract: Changepoint localization aims to provide confidence sets for a changepoint (if one exists). Existing methods either relying on strong parametric assumptions or providing only asymptotic guarantees or focusing on a particular kind of change(e.g., change in the mean) rather than the entire distributional change. A method (possibly the first) to achieve distribution-free changepoint localization with finite-sample validity was recently introduced by \cite{dandapanthula2025conformal}. However, while they proved finite sample coverage, there was no analysis of set size. In this work, we provide rigorous theoretical guarantees for their algorithm. We also show the consistency of a point estimator for change, and derive its convergence rate without distributional assumptions. Along that line, we also construct a distribution-free consistent test to assess whether a particular time point is a changepoint or not. Thus, our work provides unified distribution-free guarantees for changepoint detection, localization, and testing. In addition, we present various finite sample and asymptotic properties of the conformal $p$-value in the distribution change setup, which provides a theoretical foundation for many applications of the conformal $p$-value. As an application of these properties, we construct distribution-free consistent tests for exchangeability against distribution-change alternatives and a new, computationally tractable method of optimizing the powers of conformal tests. We run detailed simulation studies to corroborate the performance of our methods and theoretical results. Together, our contributions offer a comprehensive and theoretically principled approach to distribution-free changepoint inference, broadening both the scope and credibility of conformal methods in modern changepoint analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08749v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Swapnaneel Bhattacharyya, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>A Code Smell Refactoring Approach using GNNs</title>
      <link>https://arxiv.org/abs/2511.12069</link>
      <description>arXiv:2511.12069v2 Announce Type: replace-cross 
Abstract: Code smell is a great challenge in software refactoring, which indicates latent design or implementation flaws that may degrade the software maintainability and evolution. Over the past decades, a variety of refactoring approaches have been proposed, which can be broadly classified into metrics-based, rule-based, and machine learning-based approaches. Recent years, deep learning-based approaches have also attracted widespread attention. However, existing techniques exhibit various limitations. Metrics- and rule-based approaches rely heavily on manually defined heuristics and thresholds, whereas deep learning-based approaches are often constrained by dataset availability and model design. In this study, we proposed a graph-based deep learning approach for code smell refactoring. Specifically, we designed two types of input graphs (class-level and method-level) and employed both graph classification and node classification tasks to address the refactoring of three representative code smells: long method, large class, and feature envy. In our experiment, we propose a semi-automated dataset generation approach that could generate a large-scale dataset with minimal manual effort. We implemented the proposed approach with three classical GNN (graph neural network) architectures: GCN, GraphSAGE, and GAT, and evaluated its performance against both traditional and state-of-the-art deep learning approaches. The results demonstrate that proposed approach achieves superior refactoring performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12069v2</guid>
      <category>cs.SE</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>HanYu Zhang, Tomoji Kishi</dc:creator>
    </item>
    <item>
      <title>Energy-Efficient Routing Protocol in Vehicular Opportunistic Networks: A Dynamic Cluster-based Routing Using Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2511.19026</link>
      <description>arXiv:2511.19026v4 Announce Type: replace-cross 
Abstract: Opportunistic Networks (OppNets) employ the Store-Carry-Forward (SCF) paradigm to maintain communication during intermittent connectivity. However, routing performance suffers due to dynamic topology changes, unpredictable contact patterns, and resource constraints including limited energy and buffer capacity. These challenges compromise delivery reliability, increase latency, and reduce node longevity in highly dynamic environments. This paper proposes Cluster-based Routing using Deep Reinforcement Learning (CR-DRL), an adaptive routing approach that integrates an Actor-Critic learning framework with a heuristic function. CR-DRL enables real-time optimal relay selection and dynamic cluster overlap adjustment to maintain connectivity while minimizing redundant transmissions and enhancing routing efficiency. Simulation results demonstrate significant improvements over state-of-the-art baselines. CR-DRL extends node lifetimes by up to 21%, overall energy use is reduced by 17%, and nodes remain active for 15% longer. Communication performance also improves, with up to 10% higher delivery ratio, 28.5% lower delay, 7% higher throughput, and data requiring 30% fewer transmission steps across the network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19026v4</guid>
      <category>cs.NI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TITS.2026.3661465</arxiv:DOI>
      <dc:creator>Meisam Sharifi Sani, Saeid Iranmanesh, Raad Raad, Faisel Tubbal</dc:creator>
    </item>
    <item>
      <title>Difference-in-Differences in the Presence of Unknown Interference</title>
      <link>https://arxiv.org/abs/2512.21176</link>
      <description>arXiv:2512.21176v2 Announce Type: replace-cross 
Abstract: The stable unit treatment value (SUTVA) is a crucial assumption in the Difference-in-Differences (DiD) research design. It rules out hidden versions of treatment and any sort of interference and spillover effects across units. Even if this is a strong assumption, it has not received much attention from DiD practitioners and, in many cases, it is not even explicitly stated as an assumption, especially the no-interference assumption. In this technical note, we investigate what the DiD estimand identifies in the presence of unknown interference. We show that the DiD estimand identifies a contrast of causal effects, but it is not informative on any of these causal effects separately, without invoking further assumptions. Then, we explore different sets of assumptions under which the DiD estimand becomes informative about specific causal effects. We illustrate these results by revisiting the seminal paper on minimum wages and employment by Card and Krueger (1994).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21176v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabrizia Mealli, Javier Viviens</dc:creator>
    </item>
    <item>
      <title>Perfect Clustering for Sparse Directed Stochastic Block Models</title>
      <link>https://arxiv.org/abs/2601.16427</link>
      <description>arXiv:2601.16427v2 Announce Type: replace-cross 
Abstract: Exact recovery in stochastic block models (SBMs) is well understood in undirected settings, but remains considerably less developed for directed and sparse networks, particularly when the number of communities diverges. Spectral methods for directed SBMs often lack stability in asymmetric, low-degree regimes, and existing non-spectral approaches focus primarily on undirected or dense settings.
  We propose a fully non-spectral, two-stage procedure for community detection in sparse directed SBMs with potentially growing numbers of communities. The method first estimates the directed probability matrix using a neighborhood-smoothing scheme tailored to the asymmetric setting, and then applies $K$-means clustering to the estimated rows, thereby avoiding the limitations of eigen- or singular value decompositions in sparse, asymmetric networks. Our main theoretical contribution is a uniform row-wise concentration bound for the smoothed estimator, obtained through new arguments that control asymmetric neighborhoods and separate in- and out-degree effects. These results imply the exact recovery of all community labels with probability tending to one, under mild sparsity and separation conditions that allow both $\gamma_n \to 0$ and $K_n \to \infty$.
  Simulation studies, including highly directed, sparse, and non-symmetric block structures, demonstrate that the proposed procedure performs reliably in regimes where directed spectral and score-based methods deteriorate. To the best of our knowledge, this provides the first exact recovery guarantee for this class of non-spectral, neighborhood-smoothing methods in the sparse, directed setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16427v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Behzad Aalipur, Yichen Qin</dc:creator>
    </item>
    <item>
      <title>Amortised and provably-robust simulation-based inference</title>
      <link>https://arxiv.org/abs/2602.11325</link>
      <description>arXiv:2602.11325v2 Announce Type: replace-cross 
Abstract: Complex simulator-based models are now routinely used to perform inference across the sciences and engineering, but existing inference methods are often unable to account for outliers and other extreme values in data which occur due to faulty measurement instruments or human error. In this paper, we introduce a novel approach to simulation-based inference grounded in generalised Bayesian inference and a neural approximation of a weighted score-matching loss. This leads to a method that is both amortised and provably robust to outliers, a combination not achieved by existing approaches. Furthermore, through a carefully chosen conditional density model, we demonstrate that inference can be further simplified and performed without the need for Markov chain Monte Carlo sampling, thereby offering significant computational advantages, with complexity that is only a small fraction of that of current state-of-the-art approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11325v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ayush Bharti, Charita Dellaporta, Yuga Hikida, Fran\c{c}ois-Xavier Briol</dc:creator>
    </item>
  </channel>
</rss>
