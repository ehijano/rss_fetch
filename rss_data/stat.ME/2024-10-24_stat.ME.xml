<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Oct 2024 04:00:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Using Platt's scaling for calibration after undersampling -- limitations and how to address them</title>
      <link>https://arxiv.org/abs/2410.18144</link>
      <description>arXiv:2410.18144v1 Announce Type: new 
Abstract: When modelling data where the response is dichotomous and highly imbalanced, response-based sampling where a subset of the majority class is retained (i.e., undersampling) is often used to create more balanced training datasets prior to modelling. However, the models fit to this undersampled data, which we refer to as base models, generate predictions that are severely biased. There are several calibration methods that can be used to combat this bias, one of which is Platt's scaling. Here, a logistic regression model is used to model the relationship between the base model's original predictions and the response. Despite its popularity for calibrating models after undersampling, Platt's scaling was not designed for this purpose. Our work presents what we believe is the first detailed study focused on the validity of using Platt's scaling to calibrate models after undersampling. We show analytically, as well as via a simulation study and a case study, that Platt's scaling should not be used for calibration after undersampling without critical thought. If Platt's scaling would have been able to successfully calibrate the base model had it been trained on the entire dataset (i.e., without undersampling), then Platt's scaling might be appropriate for calibration after undersampling. If this is not the case, we recommend a modified version of Platt's scaling that fits a logistic generalized additive model to the logit of the base model's predictions, as it is both theoretically motivated and performed well across the settings considered in our study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18144v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Phelps, Daniel J. Lizotte, Douglas G. Woolford</dc:creator>
    </item>
    <item>
      <title>Detecting Spatial Outliers: the Role of the Local Influence Function</title>
      <link>https://arxiv.org/abs/2410.18261</link>
      <description>arXiv:2410.18261v1 Announce Type: new 
Abstract: In the analysis of large spatial datasets, identifying and treating spatial outliers is essential for accurately interpreting geographical phenomena. While spatial correlation measures, particularly Local Indicators of Spatial Association (LISA), are widely used to detect spatial patterns, the presence of abnormal observations frequently distorts the landscape and conceals critical spatial relationships. These outliers can significantly impact analysis due to the inherent spatial dependencies present in the data. Traditional influence function (IF) methodologies, commonly used in statistical analysis to measure the impact of individual observations, are not directly applicable in the spatial context because the influence of an observation is determined not only by its own value but also by its spatial location, its connections with neighboring regions, and the values of those neighboring observations. In this paper, we introduce a local version of the influence function (LIF) that accounts for these spatial dependencies. Through the analysis of both simulated and real-world datasets, we demonstrate how the LIF provides a more nuanced and accurate detection of spatial outliers compared to traditional LISA measures and local impact assessments, improving our understanding of spatial patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18261v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giuseppe Arbia, Vincenzo Nardelli</dc:creator>
    </item>
    <item>
      <title>Robust function-on-function interaction regression</title>
      <link>https://arxiv.org/abs/2410.18338</link>
      <description>arXiv:2410.18338v1 Announce Type: new 
Abstract: A function-on-function regression model with quadratic and interaction effects of the covariates provides a more flexible model. Despite several attempts to estimate the model's parameters, almost all existing estimation strategies are non-robust against outliers. Outliers in the quadratic and interaction effects may deteriorate the model structure more severely than their effects in the main effect. We propose a robust estimation strategy based on the robust functional principal component decomposition of the function-valued variables and $\tau$-estimator. The performance of the proposed method relies on the truncation parameters in the robust functional principal component decomposition of the function-valued variables. A robust Bayesian information criterion is used to determine the optimum truncation constants. A forward stepwise variable selection procedure is employed to determine relevant main, quadratic, and interaction effects to address a possible model misspecification. The finite-sample performance of the proposed method is investigated via a series of Monte-Carlo experiments. The proposed method's asymptotic consistency and influence function are also studied in the supplement, and its empirical performance is further investigated using a U.S. COVID-19 dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18338v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ufuk Beyaztas, Han Lin Shang, Abhijit Mandal</dc:creator>
    </item>
    <item>
      <title>Doubly protected estimation for survival outcomes utilizing external controls for randomized clinical trials</title>
      <link>https://arxiv.org/abs/2410.18409</link>
      <description>arXiv:2410.18409v1 Announce Type: new 
Abstract: Censored survival data are common in clinical trials, but small control groups can pose challenges, particularly in rare diseases or where balanced randomization is impractical. Recent approaches leverage external controls from historical studies or real-world data to strengthen treatment evaluation for survival outcomes. However, using external controls directly may introduce biases due to data heterogeneity. We propose a doubly protected estimator for the treatment-specific restricted mean survival time difference that is more efficient than trial-only estimators and mitigates biases from external data. Our method adjusts for covariate shifts via doubly robust estimation and addresses outcome drift using the DR-Learner for selective borrowing. The approach incorporates machine learning to approximate survival curves and detect outcome drifts without strict parametric assumptions, borrowing only comparable external controls. Extensive simulation studies and a real-data application evaluating the efficacy of Galcanezumab in mitigating migraine headaches have been conducted to illustrate the effectiveness of our proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18409v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyin Gao, Shu Yang, Mingyang Shan, Wenyu Wendy Ye, Ilya Lipkovich, Douglas Faries</dc:creator>
    </item>
    <item>
      <title>Studentized Tests of Independence: Random-Lifter approach</title>
      <link>https://arxiv.org/abs/2410.18437</link>
      <description>arXiv:2410.18437v1 Announce Type: new 
Abstract: The exploration of associations between random objects with complex geometric structures has catalyzed the development of various novel statistical tests encompassing distance-based and kernel-based statistics. These methods have various strengths and limitations. One problem is that their test statistics tend to converge to asymptotic null distributions involving second-order Wiener chaos, which are hard to compute and need approximation or permutation techniques that use much computing power to build rejection regions. In this work, we take an entirely different and novel strategy by using the so-called ``Random-Lifter''. This method is engineered to yield test statistics with the standard normal limit under null distributions without the need for sample splitting. In other words, we set our sights on having simple limiting distributions and finding the proper statistics through reverse engineering. We use the Central Limit Theorems (CLTs) for degenerate U-statistics derived from our novel association measures to do this. As a result, the asymptotic distributions of our proposed tests are straightforward to compute. Our test statistics also have the minimax property. We further substantiate that our method maintains competitive power against existing methods with minimal adjustments to constant factors. Both numerical simulations and real-data analysis corroborate the efficacy of the Random-Lifter method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18437v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhe Gao, Roulin Wang, Xueqin Wang, Heping Zhang</dc:creator>
    </item>
    <item>
      <title>Inferring Latent Graphs from Stationary Signals Using a Graphical Autoregressive Model</title>
      <link>https://arxiv.org/abs/2410.18445</link>
      <description>arXiv:2410.18445v1 Announce Type: new 
Abstract: Graphs are an intuitive way to represent relationships between variables in fields such as finance and neuroscience. However, these graphs often need to be inferred from data. In this paper, we propose a novel framework to infer a latent graph by treating the observed multidimensional data as graph-referenced stationary signals. Specifically, we introduce the graphical autoregressive model (GAR), where the inverse covariance matrix of the observed signals is expressed as a second-order polynomial of the normalized graph Laplacian of the latent graph. The GAR model extends the autoregressive model from time series analysis to general undirected graphs, offering a new approach to graph inference. To estimate the latent graph, we develop a three-step procedure based on penalized maximum likelihood, supported by theoretical analysis and numerical experiments. Simulation studies and an application to S&amp;P 500 stock price data show that the GAR model can outperform Gaussian graphical models when it fits the observed data well. Our results suggest that the GAR model offers a promising new direction for inferring latent graphs across diverse applications. Codes and example scripts are available at https://github.com/jed-harwood/SGM .</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18445v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jedidiah Harwood, Debashis Paul, Jie Peng</dc:creator>
    </item>
    <item>
      <title>Evolving Voices Based on Temporal Poisson Factorisation</title>
      <link>https://arxiv.org/abs/2410.18486</link>
      <description>arXiv:2410.18486v1 Announce Type: new 
Abstract: The world is evolving and so is the vocabulary used to discuss topics in speech. Analysing political speech data from more than 30 years requires the use of flexible topic models to uncover the latent topics and their change in prevalence over time as well as the change in the vocabulary of the topics. We propose the temporal Poisson factorisation (TPF) model as an extension to the Poisson factorisation model to model sparse count data matrices obtained based on the bag-of-words assumption from text documents with time stamps. We discuss and empirically compare different model specifications for the time-varying latent variables consisting either of a flexible auto-regressive structure of order one or a random walk. Estimation is based on variational inference where we consider a combination of coordinate ascent updates with automatic differentiation using batching of documents. Suitable variational families are proposed to ease inference. We compare results obtained using independent univariate variational distributions for the time-varying latent variables to those obtained with a multivariate variant. We discuss in detail the results of the TPF model when analysing speeches from 18 sessions in the U.S. Senate (1981-2016).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18486v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan V\'avra (Vienna University of Economics and Business, Paris-Lodron University of Salzburg), Bettina Gr\"un (Vienna University of Economics and Business), Paul Hofmarcher (Paris-Lodron University of Salzburg)</dc:creator>
    </item>
    <item>
      <title>Latent Functional PARAFAC for modeling multidimensional longitudinal data</title>
      <link>https://arxiv.org/abs/2410.18696</link>
      <description>arXiv:2410.18696v1 Announce Type: new 
Abstract: In numerous settings, it is increasingly common to deal with longitudinal data organized as high-dimensional multi-dimensional arrays, also known as tensors. Within this framework, the time-continuous property of longitudinal data often implies a smooth functional structure on one of the tensor modes. To help researchers investigate such data, we introduce a new tensor decomposition approach based on the CANDECOMP/PARAFAC decomposition. Our approach allows for representing a high-dimensional functional tensor as a low-dimensional set of functions and feature matrices. Furthermore, to capture the underlying randomness of the statistical setting more efficiently, we introduce a probabilistic latent model in the decomposition. A covariance-based block-relaxation algorithm is derived to obtain estimates of model parameters. Thanks to the covariance formulation of the solving procedure and thanks to the probabilistic modeling, the method can be used in sparse and irregular sampling schemes, making it applicable in numerous settings. We apply our approach to help characterize multiple neurocognitive scores observed over time in the Alzheimer's Disease Neuroimaging Initiative (ADNI) study. Finally, intensive simulations show a notable advantage of our method in reconstructing tensors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18696v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Sort, Laurent Le Brusquet, Arthur Tenenhaus</dc:creator>
    </item>
    <item>
      <title>Response Surface Designs for Crossed and Nested Multi-Stratum Structures</title>
      <link>https://arxiv.org/abs/2410.18734</link>
      <description>arXiv:2410.18734v1 Announce Type: new 
Abstract: Response surface designs are usually described as being run under complete randomization of the treatment combinations to the experimental units. In practice, however, it is often necessary or beneficial to run them under some kind of restriction to the randomization, leading to multi-stratum designs. In particular, some factors are often hard to set, so they cannot have their levels reset for each experimental unit. This paper presents a general solution to designing response surface experiments in any multi-stratum structure made up of crossing and/or nesting of unit factors. A stratum-by-stratum approach to constructing designs using compound optimal design criteria is used and illustrated. It is shown that good designs can be found even for large experiments in complex structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18734v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luzia A. Trinca, Steven G. Gilmour</dc:creator>
    </item>
    <item>
      <title>Adaptive partition Factor Analysis</title>
      <link>https://arxiv.org/abs/2410.18939</link>
      <description>arXiv:2410.18939v1 Announce Type: new 
Abstract: Factor Analysis has traditionally been utilized across diverse disciplines to extrapolate latent traits that influence the behavior of multivariate observed variables. Historically, the focus has been on analyzing data from a single study, neglecting the potential study-specific variations present in data from multiple studies. Multi-study factor analysis has emerged as a recent methodological advancement that addresses this gap by distinguishing between latent traits shared across studies and study-specific components arising from artifactual or population-specific sources of variation. In this paper, we extend the current methodologies by introducing novel shrinkage priors for the latent factors, thereby accommodating a broader spectrum of scenarios -- from the absence of study-specific latent factors to models in which factors pertain only to small subgroups nested within or shared between the studies. For the proposed construction we provide conditions for identifiability of factor loadings and guidelines to perform straightforward posterior computation via Gibbs sampling. Through comprehensive simulation studies, we demonstrate that our proposed method exhibits competing performance across a variety of scenarios compared to existing methods, yet providing richer insights. The practical benefits of our approach are further illustrated through applications to bird species co-occurrence data and ovarian cancer gene expression data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18939v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Bortolato, Antonio Canale</dc:creator>
    </item>
    <item>
      <title>Saddlepoint Monte Carlo and its Application to Exact Ecological Inference</title>
      <link>https://arxiv.org/abs/2410.18243</link>
      <description>arXiv:2410.18243v1 Announce Type: cross 
Abstract: Assuming X is a random vector and A a non-invertible matrix, one sometimes need to perform inference while only having access to samples of Y = AX. The corresponding likelihood is typically intractable. One may still be able to perform exact Bayesian inference using a pseudo-marginal sampler, but this requires an unbiased estimator of the intractable likelihood.
  We propose saddlepoint Monte Carlo, a method for obtaining an unbiased estimate of the density of Y with very low variance, for any model belonging to an exponential family. Our method relies on importance sampling of the characteristic function, with insights brought by the standard saddlepoint approximation scheme with exponential tilting. We show that saddlepoint Monte Carlo makes it possible to perform exact inference on particularly challenging problems and datasets. We focus on the ecological inference problem, where one observes only aggregates at a fine level. We present in particular a study of the carryover of votes between the two rounds of various French elections, using the finest available data (number of votes for each candidate in about 60,000 polling stations over most of the French territory).
  We show that existing, popular approximate methods for ecological inference can lead to substantial bias, which saddlepoint Monte Carlo is immune from. We also present original results for the 2024 legislative elections on political centre-to-left and left-to-centre conversion rates when the far-right is present in the second round. Finally, we discuss other exciting applications for saddlepoint Monte Carlo, such as dealing with aggregate data in privacy or inverse problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18243v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Th\'eo Voldoire, Nicolas Chopin, Guillaume Rateau, Robin J. Ryder</dc:creator>
    </item>
    <item>
      <title>Stabilizing black-box model selection with the inflated argmax</title>
      <link>https://arxiv.org/abs/2410.18268</link>
      <description>arXiv:2410.18268v1 Announce Type: cross 
Abstract: Model selection is the process of choosing from a class of candidate models given data. For instance, methods such as the LASSO and sparse identification of nonlinear dynamics (SINDy) formulate model selection as finding a sparse solution to a linear system of equations determined by training data. However, absent strong assumptions, such methods are highly unstable: if a single data point is removed from the training set, a different model may be selected. This paper presents a new approach to stabilizing model selection that leverages a combination of bagging and an "inflated" argmax operation. Our method selects a small collection of models that all fit the data, and it is stable in that, with high probability, the removal of any training point will result in a collection of selected models that overlaps with the original collection. In addition to developing theoretical guarantees, we illustrate this method in (a) a simulation in which strongly correlated covariates make standard LASSO model selection highly unstable and (b) a Lotka-Volterra model selection problem focused on identifying how competition in an ecosystem influences species' abundances. In both settings, the proposed method yields stable and compact collections of selected models, outperforming a variety of benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18268v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Melissa Adrian, Jake A. Soloff, Rebecca Willett</dc:creator>
    </item>
    <item>
      <title>Forecasting Australian fertility by age, region, and birthplace</title>
      <link>https://arxiv.org/abs/2410.18435</link>
      <description>arXiv:2410.18435v1 Announce Type: cross 
Abstract: Fertility differentials by urban-rural residence and nativity of women in Australia significantly impact population composition at sub-national levels. We aim to provide consistent fertility forecasts for Australian women characterized by age, region, and birthplace. Age-specific fertility rates at the national and sub-national levels obtained from census data between 1981-2011 are jointly modeled and forecast by the grouped functional time series method. Forecasts for women of each region and birthplace are reconciled following the chosen hierarchies to ensure that results at various disaggregation levels consistently sum up to the respective national total. Coupling the region of residence disaggregation structure with the trace minimization reconciliation method produces the most accurate point and interval forecasts. In addition, age-specific fertility rates disaggregated by the birthplace of women show significant heterogeneity that supports the application of the grouped forecasting method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18435v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yang Yang, Han Lin Shang, James Raymer</dc:creator>
    </item>
    <item>
      <title>Learning to Explore with Lagrangians for Bandits under Unknown Linear Constraints</title>
      <link>https://arxiv.org/abs/2410.18844</link>
      <description>arXiv:2410.18844v1 Announce Type: cross 
Abstract: Pure exploration in bandits models multiple real-world problems, such as tuning hyper-parameters or conducting user studies, where different safety, resource, and fairness constraints on the decision space naturally appear. We study these problems as pure exploration in multi-armed bandits with unknown linear constraints, where the aim is to identify an $r$$\textit{-good feasible policy}$. First, we propose a Lagrangian relaxation of the sample complexity lower bound for pure exploration under constraints. We show how this lower bound evolves with the sequential estimation of constraints. Second, we leverage the Lagrangian lower bound and the properties of convex optimisation to propose two computationally efficient extensions of Track-and-Stop and Gamified Explorer, namely LATS and LAGEX. To this end, we propose a constraint-adaptive stopping rule, and while tracking the lower bound, use pessimistic estimate of the feasible set at each step. We show that these algorithms achieve asymptotically optimal sample complexity upper bounds up to constraint-dependent constants. Finally, we conduct numerical experiments with different reward distributions and constraints that validate efficient performance of LAGEX and LATS with respect to baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18844v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Udvas Das, Debabrota Basu</dc:creator>
    </item>
    <item>
      <title>Distance and Kernel-Based Measures for Global and Local Two-Sample Conditional Distribution Testing</title>
      <link>https://arxiv.org/abs/2210.08149</link>
      <description>arXiv:2210.08149v2 Announce Type: replace 
Abstract: Testing the equality of two conditional distributions is crucial in various modern applications, including transfer learning and causal inference. Despite its importance, this fundamental problem has received surprisingly little attention in the literature. This work aims to present a unified framework based on distance and kernel methods for both global and local two-sample conditional distribution testing. To this end, we introduce distance and kernel-based measures that characterize the homogeneity of two conditional distributions. Drawing from the concept of conditional U-statistics, we propose consistent estimators for these measures. Theoretically, we derive the convergence rates and the asymptotic distributions of the estimators under both the null and alternative hypotheses. Utilizing these measures, along with a local bootstrap approach, we develop global and local tests that can detect discrepancies between two conditional distributions at global and local levels, respectively. Our tests demonstrate reliable performance through simulations and real data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.08149v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian Yan, Zhuoxi Li, Xianyang Zhang</dc:creator>
    </item>
    <item>
      <title>Multi-objective optimisation using expected quantile improvement for decision making in disease outbreaks</title>
      <link>https://arxiv.org/abs/2401.12031</link>
      <description>arXiv:2401.12031v2 Announce Type: replace 
Abstract: Optimization under uncertainty is important in many applications, particularly to inform policy and decision making in areas such as public health. A key source of uncertainty arises from the incorporation of environmental variables as inputs into computational models or simulators. Such variables represent uncontrollable features of the optimization problem and reliable decision making must account for the uncertainty they propagate to the simulator outputs. Often, multiple, competing objectives are defined from these outputs such that the final optimal decision is a compromise between different goals.
  Here, we present emulation-based optimization methodology for such problems that extends expected quantile improvement (EQI) to address multi-objective optimization. Focusing on the practically important case of two objectives, we use a sequential design strategy to identify the Pareto front of optimal solutions. Uncertainty from the environmental variables is integrated out using Monte Carlo samples from the simulator. Interrogation of the expected output from the simulator is facilitated by use of (Gaussian process) emulators. The methodology is demonstrated on an optimization problem from public health involving the dispersion of anthrax spores across a spatial terrain. Environmental variables include meteorological features that impact the dispersion, and the methodology identifies the Pareto front even when there is considerable input uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12031v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daria Semochkina, Alexander I. J. Forrester, David C Woods</dc:creator>
    </item>
    <item>
      <title>Classification Using Global and Local Mahalanobis Distances</title>
      <link>https://arxiv.org/abs/2402.08283</link>
      <description>arXiv:2402.08283v2 Announce Type: replace 
Abstract: We propose a novel semiparametric classifier based on Mahalanobis distances of an observation from the competing classes. Our tool is a generalized additive model with the logistic link function that uses these distances as features to estimate the posterior probabilities of different classes. While popular parametric classifiers like linear and quadratic discriminant analyses are mainly motivated by the normality of the underlying distributions, the proposed classifier is more flexible and free from such parametric modeling assumptions. Since the densities of elliptic distributions are functions of Mahalanobis distances, this classifier works well when the competing classes are (nearly) elliptic. In such cases, it often outperforms popular nonparametric classifiers, especially when the sample size is small compared to the dimension of the data. To cope with non-elliptic and possibly multimodal distributions, we propose a local version of the Mahalanobis distance. Subsequently, we propose another classifier based on a generalized additive model that uses the local Mahalanobis distances as features. This nonparametric classifier usually performs like the Mahalanobis distance based semiparametric classifier when the underlying distributions are elliptic, but outperforms it for several non-elliptic and multimodal distributions. We also investigate the behaviour of these two classifiers in high dimension, low sample size situations. A thorough numerical study involving several simulated and real datasets demonstrate the usefulness of the proposed classifiers in comparison to many state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08283v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annesha Ghosh, Anil K. Ghosh, Rita SahaRay, Soham Sarkar</dc:creator>
    </item>
    <item>
      <title>Robust Estimation and Inference for Categorical Data</title>
      <link>https://arxiv.org/abs/2403.11954</link>
      <description>arXiv:2403.11954v2 Announce Type: replace 
Abstract: While there is a rich literature on robust methodologies for contamination in continuously distributed data, contamination in categorical data is largely overlooked. This is regrettable because many datasets are categorical and oftentimes suffer from contamination. Examples include inattentive responding and bot responses in questionnaires or zero-inflated count data. We propose a novel class of contamination-robust estimators of models for categorical data, coined $C$-estimators (``$C$" for categorical). We show that the countable and possibly finite sample space of categorical data results in non-standard theoretical properties. Notably, in contrast to classic robustness theory, $C$-estimators can be simultaneously robust \textit{and} fully efficient at the postulated model. In addition, a certain particularly robust specification fails to be asymptotically Gaussian at the postulated model, but is asymptotically Gaussian in the presence of contamination. We furthermore propose a diagnostic test to identify categorical outliers and demonstrate the enhanced robustness of $C$-estimators in a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11954v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Welz</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Covariate-Adjusted and Interpretable Generalized Factor Model with Application to Testing Fairness</title>
      <link>https://arxiv.org/abs/2404.16745</link>
      <description>arXiv:2404.16745v2 Announce Type: replace 
Abstract: Latent variable models are popularly used to measure latent factors (e.g., abilities and personalities) from large-scale assessment data. Beyond understanding these latent factors, the covariate effect on responses controlling for latent factors is also of great scientific interest and has wide applications, such as evaluating the fairness of educational testing, where the covariate effect reflects whether a test question is biased toward certain individual characteristics (e.g., gender and race), taking into account their latent abilities. However, the large sample sizes and test lengths pose challenges to developing efficient methods and drawing valid inferences. Moreover, to accommodate the commonly encountered discrete responses, nonlinear latent factor models are often assumed, adding further complexity. To address these challenges, we consider a covariate-adjusted generalized factor model and develop novel and interpretable conditions to address the identifiability issue. Based on the identifiability conditions, we propose a joint maximum likelihood estimation method and establish estimation consistency and asymptotic normality results for the covariate effects. Furthermore, we derive estimation and inference results for latent factors and the factor loadings. We illustrate the finite sample performance of the proposed method through extensive numerical studies and an educational assessment dataset from the Programme for International Student Assessment (PISA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16745v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Ouyang, Chengyu Cui, Kean Ming Tan, Gongjun Xu</dc:creator>
    </item>
    <item>
      <title>Nowcasting in triple-system estimation</title>
      <link>https://arxiv.org/abs/2406.17637</link>
      <description>arXiv:2406.17637v2 Announce Type: replace 
Abstract: Multiple systems estimation uses samples that each cover part of a population to obtain a total population size estimate. Ideally, all the available samples are used, but if some samples are available (much) later, one may use only the samples that are available early. Under some regularity conditions, including sample independence, two samples is enough to obtain an asymptotically unbiased population size estimate. However, the assumption of sample independence may be unrealistic, especially when samples are derived from administrative sources. The sample independence assumption can be relaxed when three or more samples are used, which is therefore generally recommended. This may be a problem if the third sample is available much later than the first two samples. Therefore, in this paper we propose a new approach that deals with this issue by utilising older samples, using the so-called expectation maximisation algorithm. This leads to a population size nowcast estimate that is asymptotically unbiased under more relaxed assumptions than the estimate based on two samples. The resulting nowcasting model is applied to the problem of estimating the number of homeless people in The Netherlands, which leads to reasonably accurate nowcast estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17637v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daan B. Zult, Peter G. M. van der Heijden, Bart F. M. Bakker</dc:creator>
    </item>
    <item>
      <title>Robust Estimation of Polychoric Correlation</title>
      <link>https://arxiv.org/abs/2407.18835</link>
      <description>arXiv:2407.18835v2 Announce Type: replace 
Abstract: Polychoric correlation is often an important building block in the analysis of rating data, particularly for structural equation models. However, the commonly employed maximum likelihood (ML) estimator is highly susceptible to misspecification of the polychoric correlation model, for instance through violations of latent normality assumptions. We propose a novel estimator that is designed to be robust to partial misspecification of the polychoric model, that is, the model is only misspecified for an unknown fraction of observations, for instance (but not limited to) careless respondents. In contrast to existing literature, our estimator makes no assumption on the type or degree of model misspecification. It furthermore generalizes ML estimation and is consistent as well as asymptotically normally distributed. We demonstrate the robustness and practical usefulness of our estimator in simulation studies and an empirical application on a Big Five administration. In the latter, the polychoric correlation estimates of our estimator and ML differ substantially, which, after further inspection, is likely due to the presence of careless respondents that the estimator helps identify.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18835v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Welz, Patrick Mair, Andreas Alfons</dc:creator>
    </item>
    <item>
      <title>Estimating risk factors for pathogenic dose accrual from longitudinal data</title>
      <link>https://arxiv.org/abs/2407.20051</link>
      <description>arXiv:2407.20051v2 Announce Type: replace 
Abstract: Estimating risk factors for incidence of a disease is crucial for understanding its etiology. For diseases caused by enteric pathogens, off-the-shelf statistical model-based approaches do not consider the biological mechanisms through which infection occurs and thus can only be used to make comparatively weak statements about association between risk factors and incidence. Building off of established work in quantitative microbiological risk assessment, we propose a new approach to determining the association between risk factors and dose accrual rates. Our more mechanistic approach achieves a higher degree of biological plausibility, incorporates currently-ignored sources of variability, and provides regression parameters that are easily interpretable as the dose accrual rate ratio due to changes in the risk factors under study. We also describe a method for leveraging information across multiple pathogens. The proposed methods are available as an R package at \url{https://github.com/dksewell/dare}. Our simulation study shows unacceptable coverage rates from generalized linear models, while the proposed approach empirically maintains the nominal rate even when the model is misspecified. Finally, we demonstrated our proposed approach by applying our method to infant data obtained through the PATHOME study (\url{https://reporter.nih.gov/project-details/10227256}), discovering the impact of various environmental factors on infant enteric infections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20051v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel K. Sewell, Kelly K. Baker</dc:creator>
    </item>
    <item>
      <title>Choice of the hypothesis matrix for using the Anova-type-statistic</title>
      <link>https://arxiv.org/abs/2409.12592</link>
      <description>arXiv:2409.12592v2 Announce Type: replace 
Abstract: Initially developed in Brunner et al. (1997), the Anova-type-statistic (ATS) is one of the most used quadratic forms for testing multivariate hypotheses for a variety of different parameter vectors $\boldsymbol{\theta}\in\mathbb{R}^d$. Such tests can be based on several versions of ATS and in most settings, they are preferable over those based on other quadratic forms, as for example the Wald-type-statistic (WTS). However, the same null hypothesis $\boldsymbol{H}\boldsymbol{\theta}=\boldsymbol{y}$ can be expressed by a multitude of hypothesis matrices $\boldsymbol{H}\in\mathbb{R}^{m\times d}$ and corresponding vectors $\boldsymbol{y}\in\mathbb{R}^m$, which leads to different values of the test statistic, as it can be seen in simple examples. Since this can entail distinct test decisions, it remains to investigate under which conditions tests using different hypothesis matrices coincide. Here, the dimensions of the different hypothesis matrices can be substantially different, which has exceptional potential to save computation effort.
  In this manuscript, we show that for the Anova-type-statistic and some versions thereof, it is possible for each hypothesis $\boldsymbol{H}\boldsymbol{\theta}=\boldsymbol{y}$ to construct a companion matrix $\boldsymbol{L}$ with a minimal number of rows, which not only tests the same hypothesis but also always yields the same test decisions. This allows a substantial reduction of computation time, which is investigated in several conducted simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12592v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paavo Sattler, Manuel Rosenbaum</dc:creator>
    </item>
    <item>
      <title>Fractional binomial regression model for count data with excess zeros</title>
      <link>https://arxiv.org/abs/2410.08488</link>
      <description>arXiv:2410.08488v2 Announce Type: replace 
Abstract: This paper proposes a new generalized linear model with fractional binomial distribution.
  Zero-inflated Poisson/negative binomial distributions are used for count data that has many zeros. To analyze the association of such a count variable with covariates, zero-inflated Poisson/negative binomial regression models are widely used. In this work, we develop a regression model with the fractional binomial distribution that can serve as an additional tool for modeling the count response variable with covariates. Data analysis results show that on some occasions, our model outperforms the existing zero-inflated regression models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08488v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Chloe Breece, Jeonghwa Lee</dc:creator>
    </item>
    <item>
      <title>Fair comparisons of causal parameters with many treatments and positivity violations</title>
      <link>https://arxiv.org/abs/2410.13522</link>
      <description>arXiv:2410.13522v2 Announce Type: replace 
Abstract: Comparing outcomes across treatments is essential in medicine and public policy. To do so, researchers typically estimate a set of parameters, possibly counterfactual, with each targeting a different treatment. Treatment-specific means (TSMs) are commonly used, but their identification requires a positivity assumption -- that every subject has a non-zero probability of receiving each treatment. This assumption is often implausible, especially when treatment can take many values. Causal parameters based on dynamic stochastic interventions can be robust to positivity violations. However, comparing these parameters may be unfair because they may depend on outcomes under non-target treatments. To address this, and clarify when fair comparisons are possible, we propose a fairness criterion: if the conditional TSM for one treatment is greater than that for another, then the corresponding causal parameter should also be greater. We derive two intuitive properties equivalent to this criterion and show that only a mild positivity assumption is needed to identify fair parameters. We then provide examples that satisfy this criterion and are identifiable under the milder positivity assumption. These parameters are non-smooth, making standard nonparametric efficiency theory inapplicable, so we propose smooth approximations of them. We then develop doubly robust-style estimators that attain parametric convergence rates under nonparametric conditions. We illustrate our methods with an analysis of dialysis providers in New York State.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13522v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec McClean, Yiting Li, Sunjae Bae, Mara A. McAdams-DeMarco, Iv\'an D\'iaz, Wenbo Wu</dc:creator>
    </item>
    <item>
      <title>Simplified vine copula models: state of science and affairs</title>
      <link>https://arxiv.org/abs/2410.16806</link>
      <description>arXiv:2410.16806v2 Announce Type: replace 
Abstract: Vine copula models have become highly popular practical tools for modeling multivariate dependencies. To maintain tractability, a commonly employed simplifying assumption is that conditional copulas remain unchanged by the conditioning variables. This assumption has sparked a somewhat polarizing debate within the copula community. The fact that much of this dispute occurs outside the public record has placed the field in an unfortunate position, impeding scientific progress. In this article, I will review what we know about the flexibility and limitations of simplified vine copula models, explore the broader implications, and offer my own, hopefully reconciling, perspective on the issue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16806v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Nagler</dc:creator>
    </item>
    <item>
      <title>Tail-adaptive Bayesian shrinkage</title>
      <link>https://arxiv.org/abs/2007.02192</link>
      <description>arXiv:2007.02192v5 Announce Type: replace-cross 
Abstract: Robust Bayesian methods for high-dimensional regression problems under diverse sparse regimes are studied. Traditional shrinkage priors are primarily designed to detect a handful of signals from tens of thousands of predictors in the so-called ultra-sparsity domain. However, they may not perform desirably when the degree of sparsity is moderate. In this paper, we propose a robust sparse estimation method under diverse sparsity regimes, which has a tail-adaptive shrinkage property. In this property, the tail-heaviness of the prior adjusts adaptively, becoming larger or smaller as the sparsity level increases or decreases, respectively, to accommodate more or fewer signals, a posteriori. We propose a global-local-tail (GLT) Gaussian mixture distribution that ensures this property. We examine the role of the tail-index of the prior in relation to the underlying sparsity level and demonstrate that the GLT posterior contracts at the minimax optimal rate for sparse normal mean models. We apply both the GLT prior and the Horseshoe prior to a real data problem and simulation examples. Our findings indicate that the varying tail rule based on the GLT prior offers advantages over a fixed tail rule based on the Horseshoe prior in diverse sparsity regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.02192v5</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Se Yoon Lee, Peng Zhao, Debdeep Pati, Bani K. Mallick</dc:creator>
    </item>
    <item>
      <title>Data Augmentation of Multivariate Sensor Time Series using Autoregressive Models and Application to Failure Prognostics</title>
      <link>https://arxiv.org/abs/2410.16419</link>
      <description>arXiv:2410.16419v2 Announce Type: replace-cross 
Abstract: This work presents a novel data augmentation solution for non-stationary multivariate time series and its application to failure prognostics. The method extends previous work from the authors which is based on time-varying autoregressive processes. It can be employed to extract key information from a limited number of samples and generate new synthetic samples in a way that potentially improves the performance of PHM solutions. This is especially valuable in situations of data scarcity which are very usual in PHM, especially for failure prognostics. The proposed approach is tested based on the CMAPSS dataset, commonly employed for prognostics experiments and benchmarks. An AutoML approach from PHM literature is employed for automating the design of the prognostics solution. The empirical evaluation provides evidence that the proposed method can substantially improve the performance of PHM solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16419v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Douglas Baptista de Souza, Bruno Paes Leao</dc:creator>
    </item>
  </channel>
</rss>
