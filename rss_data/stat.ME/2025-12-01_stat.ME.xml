<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Dec 2025 03:43:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A simple and powerful test of vaccine waning</title>
      <link>https://arxiv.org/abs/2511.21836</link>
      <description>arXiv:2511.21836v1 Announce Type: new 
Abstract: Determining whether vaccine efficacy wanes is important for individual and public decision making. Yet, quantification of waning is a subtle task. The classical approaches cannot be interpreted as measures of declining efficacy unless we impose unreasonable assumptions. Recently, formal causal estimands designed to quantify vaccine waning have been proposed. These estimands can be bounded under weaker assumptions, but the bounds are often too wide to make claims about the presence of vaccine waning. We propose an alternative approach: a formal test to determine whether a treatment effect is constant over time. This test not only gives a considerable power gain compared to existing approaches but is also valid under plausible assumptions that are expected to hold in vaccine trials. We illustrate the increase in power through real and simulated examples, using three different approaches to compute the test statistics. Two of these approaches are based solely on summary data, accessible from existing clinical trials. Beyond our test, we also give new results that bound the waning effect. We use our methods to reanalyze data from a randomized controlled trial of the BNT162b2 COVID-19 vaccine. While prior analysis did not establish waning, our test rejects the null hypothesis of no waning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21836v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gell\'ert Per\'enyi, Matias Janvin, Mats J. Stensrud</dc:creator>
    </item>
    <item>
      <title>A Non-Bipartite Matching Framework for Difference-in-Differences with General Treatment Types</title>
      <link>https://arxiv.org/abs/2511.21973</link>
      <description>arXiv:2511.21973v1 Announce Type: new 
Abstract: Difference-in-differences (DID) is one of the most widely used causal inference frameworks in observational studies. However, most existing DID methods are designed for binary treatments and cannot be readily applied to non-binary treatment settings. Although recent work has begun to extend DID to non-binary (e.g., continuous) treatments, these approaches typically require strong additional assumptions, including parametric outcome models or the presence of idealized comparison units with (nearly) static treatment levels over time (commonly called ``stayers'' or ``quasi-stayers''). In this technical note, we introduce a new non-bipartite matching framework for DID that naturally accommodates general treatment types (e.g., binary, ordinal, or continuous). Our framework makes three main contributions. First, we develop an optimal non-bipartite matching design for DID that jointly balances baseline covariates across comparable units (reducing bias) and maximizes contrasts in treatment trajectories over time (improving efficiency). Second, we establish a post-matching randomization condition, the design-based counterpart to the traditional parallel-trends assumption, which enables valid design-based inference. Third, we introduce the sample average DID ratio, a finite-population-valid and fully nonparametric causal estimand applicable to arbitrary treatment types. Our design-based approach that preserves the full treatment-dose information, avoids parametric assumptions, does not rely on the existence of stayers or quasi-stayers, and operates entirely within a finite-population framework, without appealing to hypothetical super-populations or outcome distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21973v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyu Heng, Yuan Huang, Hyunseung Kang</dc:creator>
    </item>
    <item>
      <title>Design-based nested instrumental variable analysis</title>
      <link>https://arxiv.org/abs/2511.21992</link>
      <description>arXiv:2511.21992v1 Announce Type: new 
Abstract: Two binary instrumental variables (IVs) are nested if individuals who comply under one binary IV also comply under the other. This situation often arises when the two IVs represent different intensities of encouragement or discouragement to take the treatment--one stronger than the other. In a nested IV structure, treatment effects can be identified for two latent subgroups: always-compliers and switchers. Always-compliers are individuals who comply even under the weaker IV, while switchers are those who do not comply under the weaker IV but do under the stronger IV. We introduce a novel pair-of-pairs nested IV design, where each matched stratum consists of four units organized in two pairs. Under this design, we develop design-based inference for estimating the always-complier sample average treatment effect (ACO-SATE) and switcher sample average treatment effect (SW-SATE). In a nested IV analysis, IV assignment is randomized within each IV pair; however, whether a study unit receives the weaker or stronger IV may not be randomized. To address this complication, we then propose a novel partly biased randomization scheme and study design-based inference under this new scheme. Using extensive simulation studies, we demonstrate the validity of the proposed method and assess its power under different scenarios. Applying the nested IV framework, we estimated that 52.2% (95% CI: 50.4%-53.9%) of participants enrolled at the Henry Ford Health System in the Prostate, Lung, Colorectal, and Ovarian Cancer Screening Trial were always-compliers, while 26.7% (95% CI: 24.5%-28.9%) were switchers. Among always-compliers, flexible sigmoidoscopy was associated with a trend toward a decreased colorectal cancer rate. No effect was detected among switchers. This offers a richer interpretation of why no increase in the intention-to-treat effect was observed after 1997, even though the compliance rate rose.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21992v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhe Chen, Xinran Li, Michael O. Harhay, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Univariate-Guided Sparse Regression for Biobank-Scale High-Dimensional -omics Data</title>
      <link>https://arxiv.org/abs/2511.22049</link>
      <description>arXiv:2511.22049v1 Announce Type: new 
Abstract: We present a scalable framework for computing polygenic risk scores (PRS) in high-dimensional genomic settings using the recently introduced Univariate-Guided Sparse Regression (uniLasso). UniLasso is a two-stage penalized regression procedure that leverages univariate coefficients and magnitudes to stabilize feature selection and enhance interpretability. Building on its theoretical and empirical advantages, we adapt uniLasso for application to the UK Biobank, a population-based repository comprising over one million genetic variants measured on hundreds of thousands of individuals from the United Kingdom. We further extend the framework to incorporate external summary statistics to increase predictive accuracy. Our results demonstrate that the adapted uniLasso attains predictive performance comparable to standard Lasso while selecting substantially fewer variants, yielding sparser and more interpretable models. Moreover, it exhibits superior performance in estimating PRS relative to its competitors, such as PRS-CS. Integrating external scores further improves prediction while maintaining sparsity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22049v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Richland, Tuomo Kiiskinen, William Wang, Sophia Lu, Balasubramanian Narasimhan, Manuel Rivas, Robert Tibshirani</dc:creator>
    </item>
    <item>
      <title>The Bayes Factor Reversal Paradox</title>
      <link>https://arxiv.org/abs/2511.22152</link>
      <description>arXiv:2511.22152v1 Announce Type: new 
Abstract: In 1957, Lindley published "A statistical paradox" in Biometrika, revealing a fundamental conflict between frequentist and Bayesian inference as sample size approaches infinity. We present a new paradox of a different kind: a conflict within Bayesian inference itself. In the normal model with known variance, we prove that for any two-sided statistically significant result at the 0.05 level there exist prior variances such that the Bayes factor indicates evidence for the alternative with one choice while indicating evidence for the null with another. Thus, the same data, testing the same hypothesis, can yield opposite conclusions depending solely on prior choice. This answers Robert's 2016 call to investigate the impact of the prior scale on Bayes factors and formalises his concern that this choice involves arbitrariness to a high degree. Unlike the Jeffreys-Lindley paradox, which requires sample size approaching infinity, the paradox we identify occurs with realistic sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22152v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miodrag M. Lovric</dc:creator>
    </item>
    <item>
      <title>Overall marginalized models for longitudinal zero-inflated count data</title>
      <link>https://arxiv.org/abs/2511.22223</link>
      <description>arXiv:2511.22223v1 Announce Type: new 
Abstract: To analyze longitudinal zero-inflated count data, we extend existing models by introducing marginalized zero-inflated Poisson (MZIP) models with random effects, which explicitly capture the marginal effect of covariates and address limitations of previous methods. These models provide a clearer interpretation of the overall mean effect of covariates on zero-inflated count data. To further accommodate overdispersion, we develop marginalized zero-inflated negative binomial (MZINB) models. Both models incorporate subject-specific heterogeneity through a flexible random effects covariance structure. Simulation studies are conducted to evaluate the performance of the MZIP and MZINB models, comparing their inference under both homogeneous and heterogeneous random effects. Finally, we illustrate the applicability of the proposed models through an analysis of systemic lupus erythematosus data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22223v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keunbaik Lee, Eun Jin Jang, Dipak Dey</dc:creator>
    </item>
    <item>
      <title>Diagnostic Checking for Wasserstein Autoregression</title>
      <link>https://arxiv.org/abs/2511.22274</link>
      <description>arXiv:2511.22274v1 Announce Type: new 
Abstract: Wasserstein autoregression provides a robust framework for modeling serial dependence among probability distributions, with wide-ranging applications in economics, finance, and climate science. In this paper, we develop portmanteau-type diagnostic tests for assessing the adequacy of Wasserstein autoregressive models. By defining autocorrelation functions for model errors and residuals in the Wasserstein space, we construct two related tests: one analogous to the classical McLeod type test, and the other based on the sample-splitting approach of Davis and Fernandes(2025). We establish that, under mild regularity conditions, the corresponding test statistics converge in distribution to chi-square limits. Simulation studies and empirical applications demonstrate that the proposed tests effectively detect model mis-specification, offering a principled and reliable diagnostic tool for distributional time series analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22274v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenxiao Dai, Feiyu Jiang, Dong Li, Xiaofeng Shao</dc:creator>
    </item>
    <item>
      <title>Investigating new, signature-based, spatial autoregressive models for functional covariates</title>
      <link>https://arxiv.org/abs/2511.22414</link>
      <description>arXiv:2511.22414v1 Announce Type: new 
Abstract: We developed two new alternatives to signature-based, spatial autoregressive models. In a simulation study, we found that the new models performed at least as well as existing approaches but presented shorter computation times. We then used the new models to analyze the premature mortality rate and the mortality rate for people aged 65 and over.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22414v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Camille Fr\'event</dc:creator>
    </item>
    <item>
      <title>A signature-based spatial scan statistic for functional data</title>
      <link>https://arxiv.org/abs/2511.22432</link>
      <description>arXiv:2511.22432v1 Announce Type: new 
Abstract: We have developed a new signature-based spatial scan statistic for functional data (SigFSS). This scan statistic can be applied to both univariate and multivariate functional data. In a simulation study, SigFSS almost always performed better than the literature approaches and yielded more precise clusters in geographic terms. Lastly, we used SigFSS to search for spatial clusters of abnormally high or abnormally low mortality rates in mainland France.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22432v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Camille Fr\'event</dc:creator>
    </item>
    <item>
      <title>Improving Spatio-temporal Gaussian Process Modeling with Vecchia Approximation: A Low-Cost Sensor-Driven Approach to Urban Environmental Monitoring</title>
      <link>https://arxiv.org/abs/2511.22500</link>
      <description>arXiv:2511.22500v1 Announce Type: new 
Abstract: This paper explores Vecchia likelihood approximation for modeling physical phenomena sensed by mobile and fixed low-cost sensors in urban environments. A three-level hierarchical model is proposed to simultaneously accounts for the physical process of interest and measurement errors inherent in low-cost sensors. Several innovative configurations of Vecchia's approximation are investigated, including variations in ordering strategies, distance definitions, and sensor-specific conditioning. These configurations are evaluated for approximating the likelihood of a spatio-temporal Gaussian process, using simulated data based on real mobile sensor trajectories across Nantes, France. Our findings highlight the effectiveness of the min-max distance algorithm for ordering, reaffirming existing literature. Additionally, we demonstrate the utility of a random ordering approach that doesn't require prior definition of a spatio-temporal distance. These two ordering configurations achieved, on average, 102\% better results in log Kullback-Leibler divergence compared with four other ordering schemes studied. Results are supplemented with Asymptotic Relative Efficiency analysis, offering practical recommendations for optimizing parameter estimation. The proposed model and preferred Vecchia configuration are applied to real-world air quality data collected using mobile and fixed low-cost sensors. This application underscores the model's practical value for pollution mapping and prediction in environmental monitoring. This study advances the use of Vecchia's approximation for addressing computational challenges of Gaussian models in large-scale spatio-temporal datasets from environmental monitoring with low-cost sensor networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22500v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yacine Mohamed Idir, Olivier Orfila, Patrice Chatellier, Vincent Judalet</dc:creator>
    </item>
    <item>
      <title>Design-based theory for causal inference</title>
      <link>https://arxiv.org/abs/2511.22518</link>
      <description>arXiv:2511.22518v1 Announce Type: new 
Abstract: Causal inference, as a major research area in statistics and data science, plays a central role across diverse fields such as medicine, economics, education, and the social sciences. Design-based causal inference begins with randomized experiments and emphasizes conducting statistical inference by leveraging the known randomization mechanism, thereby enabling identification and estimation of causal effects under weak model dependence. Grounded in the seminal works of Fisher and Neyman, this paradigm has evolved to include various design strategies, such as stratified randomization and rerandomization, and analytical methods including Fisher randomization tests, Neyman-style asymptotic inference, and regression adjustment. In recent years, with the emergence of complex settings involving high-dimensional data, individual noncompliance, and network interference, design-based causal inference has witnessed remarkable theoretical and methodological advances. This paper provides a systematic review of recent progress in this field, focusing on covariate-balanced randomization designs, design-based statistical inference methods, and their extensions to high-dimensional, noncompliance, and network interference scenarios. It concludes with a comprehensive perspective on future directions for the theoretical development and practical applications of causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22518v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xin Lu, Wanjia Fu, Hongzi Li, Haoyang Yu, Honghao Zhang, Ke Zhu, Hanzhong Liu</dc:creator>
    </item>
    <item>
      <title>Bayes Factor Hypothesis Testing in Meta-Analyses: Practical Advantages and Methodological Considerations</title>
      <link>https://arxiv.org/abs/2511.22535</link>
      <description>arXiv:2511.22535v1 Announce Type: new 
Abstract: Bayesian hypothesis testing via Bayes factors offers a principled alternative to classical p-value methods in meta-analysis, particularly suited to its cumulative and sequential nature. Unlike p-values, Bayes factors allow for quantifying support both for and against the existence of an effect, facilitate ongoing evidence monitoring, and maintain coherent long-run behavior as additional studies are incorporated. Recent theoretical developments further show how Bayes factors can flexibly control Type I error rates through connections to e-value theory. Despite these advantages, their use remains limited in the meta-analytic literature. This paper provides a critical overview of their theoretical properties, methodological considerations, such as prior sensitivity, and practical advantages for evidence synthesis. Two illustrative applications are provided: one on statistical learning in individuals with language impairments, and another on seroma incidence following post-operative exercise in breast cancer patients. New tools supporting these methods are available in the open-source R package BFpack.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22535v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Joris Mulder, Robbie C. M. van Aert</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametric Marked Hawkes Processes for Earthquake Modeling</title>
      <link>https://arxiv.org/abs/2511.22538</link>
      <description>arXiv:2511.22538v1 Announce Type: new 
Abstract: The Hawkes process is a versatile stochastic model for point patterns that exhibit self-excitation, that is, the property that an event occurrence increases the rate of occurrence for some period of time in the future. We present a Bayesian nonparametric modeling approach for temporal marked Hawkes processes. Our focus is on point process modeling of earthquake occurrences, where the mark variable is given by earthquake magnitude. We develop a nonparametric prior model for the marked Hawkes process excitation function, using a representation with basis components for the time lag and the mark, and basis weights defined through a gamma process prior. We elaborate the model with a nonparametric prior for time-dependent background intensity functions, thus enabling a fully nonparametric approach to modeling the ground process intensity of marked Hawkes processes. The model construction balances computationally tractable inference with flexible forms for marked Hawkes process functionals, including mark-dependent offspring densities. The posterior simulation method provides full inference, without any approximations to the Hawkes process likelihood. In the context of the application, the modeling approach enables estimation of aftershock densities that vary with the magnitude of the main shock, thus significantly expanding the inferential scope of existing self-exciting point process models for earthquake occurrences. We investigate different aspects of the methodology through study of model properties, and with inference results based on synthetic marked point patterns. The practical utility of modeling magnitude-dependent aftershock dynamics is demonstrated with analysis of earthquakes that occurred in Japan from 1885 through 1980.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22538v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyotae Kim, Athanasios Kottas</dc:creator>
    </item>
    <item>
      <title>Mapping Urban Air Quality from Mobile Sensors Using Spatio-Temporal Geostatistics</title>
      <link>https://arxiv.org/abs/2511.22544</link>
      <description>arXiv:2511.22544v1 Announce Type: new 
Abstract: With the advancement of technology and the arrival of miniaturized environmental sensors that offer greater performance, the idea of building mobile network sensing for air quality has quickly emerged to increase our knowledge of air pollution in urban environments. However, with these new techniques, the difficulty of building mathematical models capable of aggregating all these data sources in order to provide precise mapping of air quality arises. In this context, we explore the spatio-temporal geostatistics methods as a solution for such a problem and evaluate three different methods: Simple Kriging (SK) in residuals, Ordinary Kriging (OK), and Kriging with External Drift (KED). On average, geostatistical models showed 26.57% improvement in the Root Mean Squared Error (RMSE) compared to the standard Inverse Distance Weighting (IDW) technique in interpolating scenarios (27.94% for KED, 26.05% for OK, and 25.71% for SK). The results showed less significant scores in extrapolating scenarios (a 12.22% decrease in the RMSE for geostatisical models compared to IDW). We conclude that univariable geostatistics is suitable for interpolating this type of data but is less appropriate for an extrapolation of non-sampled places since it does not create any information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22544v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yacine Mohamed Idir, Olivier Orfila, Vincent Judalet, Benoit Sagot, Patrice Chatellier</dc:creator>
    </item>
    <item>
      <title>A Framework for Initial Transient Detection and Statistical Assessment of Convergence in CFD Simulations</title>
      <link>https://arxiv.org/abs/2511.22618</link>
      <description>arXiv:2511.22618v1 Announce Type: new 
Abstract: Time series data often contain initial transient periods before reaching a stable state, posing challenges in analysis and interpretation. In this paper, we propose a novel approach to detect and estimate the end of the initial transient in time series data. Our method leverages the reversal mean standard error (RMSE) as a metric for assessing the stability of the data. Additionally, we employ fractional filtering techniques to enhance the detection accuracy by filtering out noise and capturing essential features of the underlying dynamics.
  Combining with autocorrelation-corrected confidence intervals we provide a robust framework to automate transient detection and convergence assessment. The method ensures statistical rigor by accounting for autocorrelation effects, validated through simulations with varying time steps. Results demonstrate independence from numerical parameters (e.g., time step size, under-relaxation factors), offering a reliable tool for steady-state analysis. The framework is lightweight, generalizable, and mitigates inflated false positives in autocorrelated datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22618v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonardo Scandurra, Pavlos Alexias, Eugene de Villiers</dc:creator>
    </item>
    <item>
      <title>High dimensional Mean Test for Temporal Dependent Data</title>
      <link>https://arxiv.org/abs/2511.22762</link>
      <description>arXiv:2511.22762v1 Announce Type: new 
Abstract: This paper proposes a novel test method for high-dimensional mean testing regard for the temporal dependent data. Comparison to existing methods, we establish the asymptotic normality of the test statistic without relying on restrictive assumptions, such as Gaussian distribution or M-dependence. Importantly, our theoretical framework holds potential for extension to other high-dimensional problems involving temporal dependent data. Additionally, our method offers significantly reduced computational complexity, making it more practical for large-scale applications. Simulation studies further demonstrate the computational advantages and performance improvements of our test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22762v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuchen Hu, Xiaoyi Wang, Long Feng</dc:creator>
    </item>
    <item>
      <title>Gaussian approximations for fast Bayesian inference of partially observed branching processes with applications to epidemiology</title>
      <link>https://arxiv.org/abs/2511.22833</link>
      <description>arXiv:2511.22833v1 Announce Type: new 
Abstract: We consider the problem of inference for the states and parameters of a continuous-time multitype branching process from partially observed time series data. Exact inference for this class of models, typically using sequential Monte Carlo, can be computationally challenging when the populations that are being modelled grow exponentially or the time series is long. Instead, we derive a Gaussian approximation for the transition function of the process that leads to a Kalman filtering algorithm that runs in a time independent of the population sizes. We also develop a hybrid approach for when populations are smaller and the approximation is less applicable. We investigate the performance of our approximation and algorithms to both a simple and a complex epidemic model, finding good adherence to the true posterior distributions in both cases with large computational speed-ups in most cases. We also apply our method to a COVID-19 dataset with time dependent parameters where exact methods are intractable due to the population sizes involved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22833v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Angus Lewis, Antonio Parrella, John Maclean, Andrew J. Black</dc:creator>
    </item>
    <item>
      <title>Constrained Gaussian Random Fields with Continuous Linear Boundary Restrictions for Physics-informed Modeling of States</title>
      <link>https://arxiv.org/abs/2511.22868</link>
      <description>arXiv:2511.22868v1 Announce Type: new 
Abstract: Boundary constraints in physical, environmental and engineering models restrict smooth states such as temperature to follow known physical laws at the edges of their spatio-temporal domain. Examples include fixed-state or fixed-derivative (insulated) boundary conditions, and constraints that relate the state and the derivatives, such as in models of heat transfer. Despite their flexibility as prior models over system states, Gaussian random fields do not in general enable exact enforcement of such constraints. This work develops a new general framework for constructing linearly boundary-constrained Gaussian random fields from unconstrained Gaussian random fields over multi-dimensional, convex domains. This new class of models provides flexible priors for modeling smooth states with known physical mechanisms acting at the domain boundaries. Simulation studies illustrate how such physics-informed probability models yield improved predictive performance and more realistic uncertainty quantification in applications including probabilistic numerics, data-driven discovery of dynamical systems, and boundary-constrained state estimation, as compared to unconstrained alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22868v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Ma, Oksana A. Chkrebtii, Stephen R. Niezgoda</dc:creator>
    </item>
    <item>
      <title>Joint Bayesian Inference of Parameter and Discretization Error Uncertainties in ODE Models</title>
      <link>https://arxiv.org/abs/2511.23010</link>
      <description>arXiv:2511.23010v1 Announce Type: new 
Abstract: We address the problem of Bayesian inference for parameters in ordinary differential equation (ODE) models based on observational data. Conventional approaches in this setting typically rely on numerical solvers such as the Euler or Runge-Kutta methods. However, these methods generally do not account for the discretization error induced by discretizing the ODE model. We propose a Bayesian inference framework for ODE models that explicitly quantifies discretization errors. Our method models discretization error as a random variable and performs Bayesian inference on both ODE parameters and variances of the randomized discretization errors, referred to as the discretization error variance. A key idea of our approach is the introduction of a Markov prior on the temporal evolution of the discretization error variances, enabling the inference problem to be formulated as a state-space model. Furthermore, we propose a specific form of the Markov prior that arises naturally from standard discretization error analysis. This prior depends on the step size in the numerical solver, and we discuss its asymptotic property in the limit as the step size approaches zero. Numerical experiments illustrate that the proposed method can simultaneously quantify uncertainties in both the ODE parameters and the discretization errors, and can produce posterior distributions over the parameters with broader support by accounting for discretization error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23010v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shoji Toyota, Yuto Miyatake</dc:creator>
    </item>
    <item>
      <title>A General Bayesian Nonparametric Approach for Estimating Population-Level and Conditional Causal Effects</title>
      <link>https://arxiv.org/abs/2511.23085</link>
      <description>arXiv:2511.23085v1 Announce Type: new 
Abstract: We propose a Bayesian nonparametric (BNP) approach to causal inference using observational data consisting of outcome, treatment, and a set of confounders. The conditional distribution of the outcome given treatment and confounders is modeled flexibly using a dependent nonparametric mixture model, in which both the atoms and the weights vary with the confounders. The proposed BNP model is well suited for causal inference problems, as it does not rely on parametric assumptions about how the conditional distribution depends on the confounders. In particular, the model effectively adjusts for confounding and improves the modeling of treatment effect heterogeneity, leading to more accurate estimation of both the average treatment effect (ATE) and heterogeneous treatment effects (HTE). Posterior inference under the proposed model is computationally efficient due to the use of data augmentation. Extensive evaluations demonstrate that the proposed model offers competitive or superior performance compared to a wide range of recent methods spanning various statistical approaches, including Bayesian additive regression tree (BART) models, which are well known for their strong empirical performance. More importantly, the model provides fully probabilistic inference on quantities of interest that other methods cannot easily provide, using their posterior distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23085v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongseok Hur, Joonhyuk Jung, Juhee Lee</dc:creator>
    </item>
    <item>
      <title>Inference for quantile-parametrized families via CDF confidence bands</title>
      <link>https://arxiv.org/abs/2511.23086</link>
      <description>arXiv:2511.23086v1 Announce Type: new 
Abstract: Quantile-based distribution families are an important subclass of parametric families, capable of exhibiting a wide range of behaviors using very few parameters. These parametric models present significant challenges for classical methods, since the CDF and density do not have a closed-form expression. Furthermore, approximate maximum likelihood estimation and related procedures may yield non-$\sqrt{n}$ and non-normal asymptotics over regions of the parameter space, making bootstrap and resampling techniques unreliable. We develop a novel inference framework that constructs confidence sets by inverting distribution-free confidence bands for the empirical CDF through the known quantile function. Our proposed inference procedure provides a principled and assumption-lean alternative in this setting, requiring no distributional assumptions beyond the parametric model specification and avoiding the computational and theoretical difficulties associated with likelihood-based methods for these complex parametric families. We demonstrate our framework on Tukey Lambda and generalized Lambda distributions, evaluate its performance through simulation studies, and illustrate its practical utility with an application to both a small-sample dataset (Twin Study) and a large-sample dataset (Spanish household incomes).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23086v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Srijan Chattopadhyay, Siddhaarth Sarkar, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Machine learning for violence prediction: a systematic review and critical appraisal</title>
      <link>https://arxiv.org/abs/2511.23118</link>
      <description>arXiv:2511.23118v1 Announce Type: new 
Abstract: Purpose To conduct a systematic review of machine learning models for predicting violent behaviour by synthesising and appraising their validity, usefulness, and performance.
  Methods We systematically searched nine bibliographic databases and Google Scholar up to September 2025 for development and/or validation studies on machine learning methods for predicting all forms of violent behaviour. We synthesised the results by summarising discrimination and calibration performance statistics and evaluated study quality by examining risk of bias and clinical utility.
  Results We identified 38 studies reporting the development and validation of 40 models. Most studies reported Area Under the Curve (AUC) as the discrimination statistic with a range of 0.68-0.99. Only eight studies reported calibration performance, and three studies reported external validation. 31 studies had a high risk of bias, mainly in the analysis domain, and three studies had low risk of bias. The overall clinical utility of violence prediction models is poor, as indicated by risks of overfitting due to small samples, lack of transparent reporting, and low generalisability.
  Conclusion Although black box machine learning models currently have limited applicability in clinical settings, they may show promise for identifying high-risk individuals. We recommend five key considerations for violence prediction modelling: (i) ensuring methodological quality (e.g. following guidelines) and interdisciplinary collaborations; (ii) using black box algorithms only for highly complex data; (iii) incorporating dynamic predictions to allow for risk monitoring; (iv) developing more trustworthy algorithms using explainable methods; and (v) applying causal machine learning approaches where appropriate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23118v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefaniya Kozhevnikova, Denis Yukhnenko, Giulio Scola, Seena Fazel</dc:creator>
    </item>
    <item>
      <title>Goodness-of-fit testing for the error distribution in functional linear models</title>
      <link>https://arxiv.org/abs/2511.23137</link>
      <description>arXiv:2511.23137v1 Announce Type: new 
Abstract: We consider the error distribution in functional linear models with scalar response and functional covariate. Different asymptotic expansions of the empirical distribution function and the empirical characteristic function based on estimated residuals under different model assumptions are discussed. The results are applied for simple and composite goodness-of-fit testing for the error distribution, in particular testing for normal distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23137v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Natalie Neumeyer, Leonie Selk</dc:creator>
    </item>
    <item>
      <title>The Bayesian optimal two-stage design for clinical phase II trials based on Bayes factors</title>
      <link>https://arxiv.org/abs/2511.23144</link>
      <description>arXiv:2511.23144v1 Announce Type: new 
Abstract: Sequential trial design is an important statistical approach to increase the efficiency of clinical trials. Bayesian sequential trial design relies primarily on conducting a Monte Carlo simulation under the hypotheses of interest and investigating the resulting design characteristics via Monte Carlo estimates. This approach has several drawbacks, namely that replicating the calibration of a Bayesian design requires repeating a possibly complex Monte Carlo simulation. Furthermore, Monte Carlo standard errors are required to judge the reliability of the simulation. All of this is due to a lack of closed-form or numerical approaches to calibrate a Bayesian design which uses Bayes factors. In this paper, we propose the Bayesian optimal two-stage design for clinical phase II trials based on Bayes factors. The optimal two-stage Bayes factor design is a sequential clinical trial design that is built on the idea of trinomial tree branching, a method we propose to correct the resulting design characteristics for introducing a single interim analysis. We build upon this idea to invent a calibration algorithm which yields the optimal Bayesian design that minimizes the expected sample size under the null hypothesis. Examples show that our design recovers Simon's two-stage optimal design as a special case, improves upon non-sequential Bayesian design based on Bayes factors, and can be calibrated quickly, as it makes use only of standard numerical techniques instead of time-consuming Monte Carlo simulations. Furthermore, the design allows to ensure a minimum probability on compelling evidence in favour of the null hypothesis, which is not possible with other designs. As the idea of trinomial tree branching is neither dependent on the endpoint, nor on the use of Bayes factors, the design can therefore be generalized to other settings, too.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23144v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Riko Kelter, Samuel Pawel</dc:creator>
    </item>
    <item>
      <title>A Design-Based Matching Framework for Staggered Adoption with Time-Varying Confounding</title>
      <link>https://arxiv.org/abs/2511.23208</link>
      <description>arXiv:2511.23208v1 Announce Type: new 
Abstract: Causal inference in longitudinal datasets has long been challenging due to dynamic treatment adoption and confounding by time-varying covariates. Prior work either fails to account for heterogeneity across treatment adoption cohorts and treatment timings or relies on modeling assumptions. In this paper, we develop a novel design-based framework for inference on group- and time-specific treatment effects in panel data with staggered treatment adoption. We establish identification results for causal effects under this structure and introduce corresponding estimators, together with a block bootstrap procedure for estimating the covariance matrix and testing the homogeneity of group-time treatment effects. To implement the framework in practice, we propose the Reverse-Time Nested Matching algorithm, which constructs matched strata by pairing units from different adoption cohorts in a way that ensures comparability of covariate histories at each treatment time. Applying the algorithm to the Netflix-IPTV dataset, we find that while Netflix subscription does not significantly affect total IPTV viewing time, it does negatively affect VoD usage. We also provide statistical evidence that the causal effects of Netflix subscription may vary even within the same treatment cohort or across the same outcome and event times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23208v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suehyun Kim, Kwonsang Lee</dc:creator>
    </item>
    <item>
      <title>Comparing Variable Selection and Model Averaging Methods for Logistic Regression</title>
      <link>https://arxiv.org/abs/2511.23216</link>
      <description>arXiv:2511.23216v1 Announce Type: new 
Abstract: Model uncertainty is a central challenge in statistical models for binary outcomes such as logistic regression, arising when it is unclear which predictors should be included in the model. Many methods have been proposed to address this issue for logistic regression, but their relative performance under realistic conditions remains poorly understood. We therefore conducted a preregistered, simulation-based comparison of 28 established methods for variable selection and inference under model uncertainty, using 11 empirical datasets spanning a range of sample sizes and numbers of predictors, in cases both with and without separation. We found that Bayesian model averaging methods based on g-priors, particularly with g = max(n, p^2), show the strongest overall performance when separation is absent. When separation occurs, penalized likelihood approaches, especially the LASSO, provide the most stable results, while Bayesian model averaging with the local empirical Bayes (EB-local) prior is competitive in both situations. These findings offer practical guidance for applied researchers on how to effectively address model uncertainty in logistic regression in modern empirical and machine learning research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23216v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikola Sekulovski, Franti\v{s}ek Barto\v{s}, Don van den Bergh, Giuseppe Arena, Henrik R. Godmann, Vipasha Goyal, Julius M. Pfadt, Maarten Marsman, Adrian E. Raftery</dc:creator>
    </item>
    <item>
      <title>Conjugate Generalised Bayesian Inference for Discrete Doubly Intractable Problems</title>
      <link>https://arxiv.org/abs/2511.23275</link>
      <description>arXiv:2511.23275v1 Announce Type: new 
Abstract: Doubly intractable problems occur when both the likelihood and the posterior are available only in unnormalised form, with computationally intractable normalisation constants. Bayesian inference then typically requires direct approximation of the posterior through specialised and typically expensive MCMC methods. In this paper, we provide a computationally efficient alternative in the form of a novel generalised Bayesian posterior that allows for conjugate inference within the class of exponential family models for discrete data. We derive theoretical guarantees to characterise the asymptotic behaviour of the generalised posterior, supporting its use for inference. The method is evaluated on a range of challenging intractable exponential family models, including the Conway-Maxwell-Poisson graphical model of multivariate count data, autoregressive discrete time series models, and Markov random fields such as the Ising and Potts models. The computational gains are significant; in our experiments, the method is between 10 and 6000 times faster than state-of-the-art Bayesian computational methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23275v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Laplante, Matias Altamirano, Jeremias Knoblauch, Andrew Duncan, Fran\c{c}ois-Xavier Briol</dc:creator>
    </item>
    <item>
      <title>Getting it right: Methods for risk ratios and risk differences cluster randomized trials with a small number of clusters</title>
      <link>https://arxiv.org/abs/2511.23419</link>
      <description>arXiv:2511.23419v1 Announce Type: new 
Abstract: Most cluster randomized trials (CRTs) randomize fewer than 30-40 clusters in total. When performing inference for such ``small'' CRTs, it is important to use methods that appropriately account for the small sample size. When the generalized estimating equations (GEE) approach is used for analysis of ``small'' CRTs, the robust variance estimator from GEE is biased downward and therefore bias-corrected standard errors should be used. Moreover, in order to avoid inflated Type I error, an appropriate bias-corrected standard error should be paired with the t- rather than Z-statistic when making inference about a single-parameter intervention effect. Although several bias-correction methods (including Kauermann and Carroll (KC), Mancl and DeRouen (MD), Morel, Bokossa, and Neerchal (MBN), and the average of KC and MD (AVG)) have been evaluated for inference for odds ratios, their finite-sample behavior in ``small'' CRTs with few clusters has not been thoroughly investigated for risk ratios and risk differences. The current article aims to fill the gap by including analysis via binomial, Poisson and Gaussian models and for a broad spectrum of scenarios. Analysis is via binomial and Poisson models (using log and identity link for risk and differences measures, respectively). We additionally explore the use of Gaussian models with identity link for risk differences and adopt the "modified" approach for analysis with misspecified Poisson and Gaussian models. We consider a broad spectrum of scenarios including for rare outcomes, small cluster sizes, high intracluster correlations (ICCs), and high coefficients of variation (CVs) of cluster size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23419v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shifeng Sun, Xueqi Wang, Zhuoran Hou, Elizabeth L. Turner</dc:creator>
    </item>
    <item>
      <title>Consensus Tree Estimation with False Discovery Rate Control via Partially Ordered Sets</title>
      <link>https://arxiv.org/abs/2511.23433</link>
      <description>arXiv:2511.23433v1 Announce Type: new 
Abstract: Connected acyclic graphs (trees) are data objects that hierarchically organize categories. Collections of trees arise in a diverse variety of fields, including evolutionary biology, public health, machine learning, social sciences and anatomy. Summarizing a collection of trees by a single representative is challenging, in part due to the dimension of both the sample and parameter space. We frame consensus tree estimation as a structured feature-selection problem, where leaves and edges are the features. We introduce a partial order on leaf-labeled trees, use it to define true and false discoveries for a candidate summary tree, and develop an estimation algorithm that controls the false discovery rate at a nominal level for a broad class of non-parametric generative models. Furthermore, using the partial order structure, we assess the stability of each feature in a selected tree. Importantly, our method accommodates unequal leaf sets and non-binary trees, allowing the estimator to reflect uncertainty by collapsing poorly supported structure instead of forcing full resolution. We apply the method to study the archaeal origin of eukaryotic cells and to quantify uncertainty in deep branching orders. While consensus tree construction has historically been viewed as an estimation task, reframing it as feature selection over a partially ordered set allows us to obtain the first estimator with finite-sample and model-free guarantees. More generally, our approach provides a foundation for integrating tools from multiple testing into tree estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23433v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Alejandra Valdez Cabrera, Amy D Willis, Armeen Taeb</dc:creator>
    </item>
    <item>
      <title>The $L$-test: Increasing the Linear Model $F$-test's Power Under Sparsity Without Sacrificing Validity</title>
      <link>https://arxiv.org/abs/2511.23466</link>
      <description>arXiv:2511.23466v1 Announce Type: new 
Abstract: We introduce a new procedure for testing the significance of a set of regression coefficients in a Gaussian linear model with $n \geq d$. Our method, the $L$-test, provides the same statistical validity guarantee as the classical $F$-test, while attaining higher power when the nuisance coefficients are sparse. Although the $L$-test requires Monte Carlo sampling, each sample's runtime is dominated by simple matrix-vector multiplications so that the overall test remains computationally efficient. Furthermore, we provide a Monte-Carlo-free variant that can be used for particularly large-scale multiple testing applications. We give intuition for the power of our approach, validate its advantages through extensive simulations, and illustrate its practical utility in both single- and multiple-testing contexts with an application to an HIV drug resistance dataset. In the concluding remarks, we also discuss how our methodology can be applied to a more general class of parametric models that admit asymptotically Gaussian estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23466v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danielle Paulson, Souhardya Sengupta, Lucas Janson</dc:creator>
    </item>
    <item>
      <title>Survey-Based Estimation of Probe Group Sizes in the Network Scale-up Method: A Case Study from Jordan</title>
      <link>https://arxiv.org/abs/2511.21938</link>
      <description>arXiv:2511.21938v1 Announce Type: cross 
Abstract: Estimating the size of marginalized populations is a persistent challenge in survey statistics and public health, especially where stigma and legal restrictions exclude such groups from census and administrative data. Migrant domestic workers in Jordan represent one such population. We employ the Network Scale-up Method using the direct probe group method, estimating probe group sizes from survey respondents' own membership rather than relying on external counts. Using data from a nationally representative household survey in Jordan, we combine the direct probe group method with Bayesian logistic mixed-effects models to stabilize small-area estimates at the Governorate level. We validate the method against census data, demonstrating that direct probe group estimates yield reliable inference and provide a practical alternative where known probe group sizes are unavailable. Our results highlight regional variation in social network size and connectivity to migrant domestic workers. We argue that the direct probe group method is more likely to satisfy the conditions required for unbiased estimation than relying on official record sizes. This work provides the first systematic validation of the direct probe group method in a small-area setting and offers guidance for adapting the Network Scale-up Method to surveys with limited sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21938v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ian Laga</dc:creator>
    </item>
    <item>
      <title>A Sensitivity Approach to Causal Inference Under Limited Overlap</title>
      <link>https://arxiv.org/abs/2511.22003</link>
      <description>arXiv:2511.22003v1 Announce Type: cross 
Abstract: Limited overlap between treated and control groups is a key challenge in observational analysis. Standard approaches like trimming importance weights can reduce variance but introduce a fundamental bias. We propose a sensitivity framework for contextualizing findings under limited overlap, where we assess how irregular the outcome function has to be in order for the main finding to be invalidated. Our approach is based on worst-case confidence bounds on the bias introduced by standard trimming practices, under explicit assumptions necessary to extrapolate counterfactual estimates from regions of overlap to those without. Empirically, we demonstrate how our sensitivity framework protects against spurious findings by quantifying uncertainty in regions with limited overlap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22003v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuanzhe Ma, Hongseok Namkoong</dc:creator>
    </item>
    <item>
      <title>A two-parameter, minimal-data model to predict dengue cases: the 2022-2023 outbreak in Florida, USA</title>
      <link>https://arxiv.org/abs/2511.22040</link>
      <description>arXiv:2511.22040v1 Announce Type: cross 
Abstract: Reliable and timely dengue predictions provide actionable lead time for targeted vector control and clinical preparedness, reducing preventable diseases and health-system costs in at-risk communities. Dengue forecasting often relies on site-specific covariates and entomological data, limiting generalizability in data-sparse settings. We propose a data-parsimonious (DP) framework based on the incidence versus cumulative cases (ICC) curve, extending it from a basic SIR to a two-population SEIR model for dengue. Our DP model uses only the target season's incidence time series and estimates only two parameters, reducing noise and computational burden. A Bayesian extension quantifies the case reporting and fitting uncertainty to produce calibrated predictive intervals. We evaluated the performance of the DP model in the 2022-2023 outbreaks in Florida, where standardized clinical tests and reporting support accurate case determination. The DP framework demonstrates competitive predictive performance at substantially lower computational cost than more elaborate models, making it suitable for dengue early detection where dense surveillance or long historical records are unavailable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22040v1</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saman Hosseini, Lee W. Cohnstaedt, Caterina Scoglio</dc:creator>
    </item>
    <item>
      <title>Statistics of Extremes for the Insurance Industry</title>
      <link>https://arxiv.org/abs/2511.22272</link>
      <description>arXiv:2511.22272v1 Announce Type: cross 
Abstract: We provide a survey of how techniques developed for the modelling of extremes naturally matter in insurance, and how they need to and can be adapted for the insurance applications. Topics covered include truncation, tempering, censoring and regression techniques. The discussed techniques are illustrated on concrete data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22272v1</guid>
      <category>q-fin.RM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hansjoerg Albrecher, Jan Beirlant</dc:creator>
    </item>
    <item>
      <title>What the Jeffreys-Lindley Paradox Really Is: Correcting a Persistent Misconception</title>
      <link>https://arxiv.org/abs/2511.22816</link>
      <description>arXiv:2511.22816v1 Announce Type: cross 
Abstract: The Jeffreys-Lindley paradox stands as the most profound divergence between frequentist and Bayesian approaches to hypothesis testing. Yet despite more than six decades of discussion, this paradox remains frequently misunderstood--even in the pages of leading statistical journals. In a 1993 paper published in Statistica Sinica, Robert characterized the Jeffreys-Lindley paradox as "the fact that a point null hypothesis will always be accepted when the variance of a conjugate prior goes to infinity." This characterization, however, describes a different phenomenon entirely-what we term Bartlett's Anomaly-rather than the Jeffreys-Lindley paradox as originally formulated. The paradox, as presented by Lindley (1957), concerns what happens as sample size increases without bound while holding the significance level fixed, not what happens as prior variance diverges. This distinction is not merely terminological: the two phenomena have different mathematical structures, different implications, and require different solutions. The present paper aims to clarify this confusion, demonstrating through Lindley's own equations that he was concerned exclusively with sample size asymptotics. We show that even Jeffreys himself underestimated the practical frequency of the paradox. Finally, we argue that the only genuine resolution lies in abandoning point null hypotheses in favor of interval nulls, a paradigm shift that eliminates the paradox and restores harmony between Bayesian and frequentist inference. Submitted to Statistica Sinica.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22816v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Miodrag M. Lovric</dc:creator>
    </item>
    <item>
      <title>A Trainable Centrality Framework for Modern Data</title>
      <link>https://arxiv.org/abs/2511.22959</link>
      <description>arXiv:2511.22959v1 Announce Type: cross 
Abstract: Measuring how central or typical a data point is underpins robust estimation, ranking, and outlier detection, but classical depth notions become expensive and unstable in high dimensions and are hard to extend beyond Euclidean data. We introduce Fused Unified centrality Score Estimation (FUSE), a neural centrality framework that operates on top of arbitrary representations. FUSE combines a global head, trained from pairwise distance-based comparisons to learn an anchor-free centrality score, with a local head, trained by denoising score matching to approximate a smoothed log-density potential. A single parameter between 0 and 1 interpolates between these calibrated signals, yielding depth-like centrality from different views via one forward pass. Across synthetic distributions, real images, time series, and text data, and standard outlier detection benchmarks, FUSE recovers meaningful classical ordering, reveals multi-scale geometric structures, and attains competitive performance with strong classical baselines while remaining simple and efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22959v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minh Duc Vu, Mingshuo Liu, Doudou Zhou</dc:creator>
    </item>
    <item>
      <title>A PLS-Integrated LASSO Method with Application in Index Tracking</title>
      <link>https://arxiv.org/abs/2511.23205</link>
      <description>arXiv:2511.23205v1 Announce Type: cross 
Abstract: In traditional multivariate data analysis, dimension reduction and regression have been treated as distinct endeavors. Established techniques such as principal component regression (PCR) and partial least squares (PLS) regression traditionally compute latent components as intermediary steps -- although with different underlying criteria -- before proceeding with the regression analysis. In this paper, we introduce an innovative regression methodology named PLS-integrated Lasso (PLS-Lasso) that integrates the concept of dimension reduction directly into the regression process. We present two distinct formulations for PLS-Lasso, denoted as PLS-Lasso-v1 and PLS-Lasso-v2, along with clear and effective algorithms that ensure convergence to global optima. PLS-Lasso-v1 and PLS-Lasso-v2 are compared with Lasso on the task of financial index tracking and show promising results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23205v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiqin Tang, Yining Dong, S. Joe Qin</dc:creator>
    </item>
    <item>
      <title>Finite- and Large- Sample Inference for Model and Coefficients in High-dimensional Linear Regression with Repro Samples</title>
      <link>https://arxiv.org/abs/2209.09299</link>
      <description>arXiv:2209.09299v4 Announce Type: replace 
Abstract: In this paper, we present a novel and effective inference approach to conduct both finite- and large-sample inference for high-dimensional linear regression models. This approach is developed under the so-called repro samples framework, in which we conduct statistical inference by creating and studying the behavior of artificial samples that are obtained by mimicking the sampling mechanism of the data. We construct confidence sets for (a) the true model corresponding to the nonzero coefficients, (b) a single or any collection of regression coefficients, and (c) both the model and regression coefficients jointly. To facilitate the constructions of these confidence sets and overcome computational difficulties of searching all possible models, we use an innovative Fisher inversion technique to construct a model candidate set that includes the true sparse model with the probability close to 1 for models with both Gaussian and non-Gaussian errors. The proposed approach fills in two major gaps in the high-dimensional regression literature: (1) lack of effective approaches to addressing model selection uncertainty and providing valid inference for the underlying true model; (2) lack of effective inference approaches to guaranteeing finite-sample performance. We provide both finite-sample and asymptotic results to theoretically guarantee the performance of the proposed methods. In addition, our numerical results demonstrate that the proposed methods are valid and achieve better coverage with smaller confidence sets than the current state-of-the-art approaches, such as debiasing and bootstrap approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.09299v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peng Wang, Min-Ge Xie, Linjun Zhang</dc:creator>
    </item>
    <item>
      <title>A robust and powerful method for assessing replicability of high dimensional data</title>
      <link>https://arxiv.org/abs/2310.09701</link>
      <description>arXiv:2310.09701v3 Announce Type: replace 
Abstract: Identifying signals that replicate across multiple studies is essential for establishing robust scientific evidence, yet existing methods for high-dimensional replicability analysis either rely on restrictive modeling assumptions, are limited to two-study settings, or lack statistical power. We propose a general empirical Bayes framework for multi-study replicability analysis that jointly models summary-level $p$-values while explicitly accounting for between-study heterogeneity. Within each study, non-null $p$-value densities are estimated nonparametrically under monotonicity constraints, enabling flexible and tuning-free inference. For two studies, we develop a local false discovery rate (Lfdr) statistic for the composite null of non-replicability and establish identifiability, consistency, and a cubic-rate convergence of the nonparametric MLE, along with minimax optimality. Extending replicability analysis to $n$ studies typically requires estimating $2^n$ latent configurations, which is computationally infeasible. To address this challenge, we introduce a scalable pairwise rejection strategy that decomposes the exponentially large composite null into disjoint components, yielding linear complexity in the number of studies. We prove asymptotic FDR control under mild regularity conditions and show that Lfdr-based thresholding is power-optimal. Extensive simulations demonstrate that our method provides substantial power gains while maintaining valid FDR control, outperforming state-of-the-art alternatives across a wide range of scenarios. Applying our framework to East Asian- and European-ancestry genome-wide association studies of type 2 diabetes reveals replicable genetic associations that competing approaches fail to detect, illustrating the method's practical utility in large-scale biomedical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09701v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haochen Lei, Yan Li, Hongyuan Cao</dc:creator>
    </item>
    <item>
      <title>Modeling asymmetry in multi-way contingency tables with ordinal categories via f-divergence</title>
      <link>https://arxiv.org/abs/2405.12157</link>
      <description>arXiv:2405.12157v3 Announce Type: replace 
Abstract: This study introduces a novel model that effectively captures asymmetric structures in multivariate contingency tables with ordinal categories. Leveraging the principle of maximum entropy, our approach employs f-divergence to provide a rational model under the presence of a ``prior guess.'' Inspired by the constraints used in the derivation of multivariate normal distributions, we demonstrate that the proposed model minimizes f-divergence from complete symmetry under specific constraints. The proposed model encompasses existing asymmetry models as special cases while offering remarkably high interpretability. By modifying divergence measures included in f-divergence, the model provides the flexibility to adapt to specific probabilistic structures of interest. Furthermore, we established theorems that show that a complete symmetry model can be decomposed into two or more models, each imposing less restrictive parameter constraints. We also investigated the properties of the goodness-of-fit statistics with an emphasis on the likelihood ratio and Wald test statistics. Extensive Monte Carlo simulations confirmed the nominal size, high power, and robustness of the choice of f-divergence. Finally, an application to real-world data highlights the practical utility of the proposed model for analyzing asymmetric structures in ordinal contingency tables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12157v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hisaya Okahara, Kouji Tahata</dc:creator>
    </item>
    <item>
      <title>A Generalized Tangent Approximation based Variational Inference Framework for Strongly Super-Gaussian Likelihoods</title>
      <link>https://arxiv.org/abs/2504.05431</link>
      <description>arXiv:2504.05431v2 Announce Type: replace 
Abstract: Variational inference, as an alternative to Markov chain Monte Carlo sampling, has played a transformative role in enabling scalable computation for complex Bayesian models. Nevertheless, existing approaches often depend on either rigid model-specific formulations or stochastic black-box optimization routines. Tangent approximation is a principled class of structured variational methods that exploits the geometry of the underlying probability model. However, its utility has largely been confined to logistic regression and related modeling regimes. In this article, we propose a novel variational framework based on tangent transformation for a broad class of probability models characterized by strongly super-Gaussian likelihoods. Our method leverages convex duality to construct tangent minorants of the log-likelihood, thereby inducing conjugacy with Gaussian priors over model parameters in an otherwise intractable setup. Under mild assumptions on the data-generating mechanism, we establish algorithmic convergence guarantees, a contribution that stands in contrast to the limited theoretical assurances typically available for black-box variational methods. Additionally, we derive near-minimax optimal bounds for the variational risk. Superior performance of our proposed methodology is illustrated on simulated and real-data scenarios that challenge state-of-the-art variational algorithms in terms of scalability and their ability to consistently capture complex underlying data structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05431v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Somjit Roy, Pritam Dey, Debdeep Pati, Bani K. Mallick</dc:creator>
    </item>
    <item>
      <title>Joint Modelling of Line and Point Data on Metric Graphs</title>
      <link>https://arxiv.org/abs/2505.01175</link>
      <description>arXiv:2505.01175v2 Announce Type: replace 
Abstract: Metric graphs are useful tools for describing spatial domains like road and river networks, where spatial dependence act along the network. We take advantage of recent developments for such Gaussian Random Fields (GRFs), and consider joint spatial modelling of observations with different spatial supports. Motivated by an application to traffic state modelling in Trondheim, Norway, we consider line-referenced data, which can be described by an integral of the GRF along a line segment on the metric graph, and point-referenced data. Through a simulation study inspired by the application, we investigate the number of replicates that are needed to estimate parameters and to predict unobserved locations. The former is assessed using bias and variability, and the latter is assessed through root mean square error (RMSE), continuous rank probability scores (CRPSs), and coverage. Joint modelling is contrasted with a simplified approach that treat line-referenced observations as point-referenced observations. The results suggest joint modelling leads to strong improvements. The application to Trondheim, Norway, combines point-referenced induction loop data and line-referenced public transportation data. To ensure positive speeds, we use a non-linear link function, which requires integrals of non-linear combinations of the linear predictor. This is made computationally feasible by a combination of the R packages inlabru and MetricGraph, and new code for processing geographical line data to work with existing graph representations and fmesher methods for dealing with line support in inlabru on objects from MetricGraph. We fit the model to two datasets where we expect different spatial dependency and compare the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01175v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spasta.2025.100946</arxiv:DOI>
      <dc:creator>Karina Lilleborge, Sara Martino, Geir-Arne Fuglstad, Finn Lindgren, Rikke Ingebrigtsen</dc:creator>
    </item>
    <item>
      <title>Principal stratification with recurrent events truncated by a terminal event: A nested Bayesian nonparametric approach</title>
      <link>https://arxiv.org/abs/2506.19015</link>
      <description>arXiv:2506.19015v2 Announce Type: replace 
Abstract: Recurrent events often serve as key endpoints in clinical studies but may be prematurely truncated by terminal events such as death, creating selection bias and complicating causal inference. To address this challenge, we propose novel causal estimands within the principal stratification framework, introducing a refined always-survivor stratum that defines survival until the final recurrent event rather than a fixed time point, yielding more stable causal contrasts. We develop a flexible Bayesian nonparametric prior, the enriched dependent Dirichlet process, specifically designed for joint modeling of recurrent and terminal events, addressing a limitation where standard Dirichlet process priors create random partitions dominated by recurrent events, yielding poor predictive performance for terminal events. Our nested structure separates within-arm and cross-arm dependence through a dual-frailty framework, enabling transparent sensitivity analysis for non-identifiable parameters. Simulations are carried out to show that our method has superior performance compared to existing methods. We also illustrate the proposed Bayesian methods to infer the causal effect of intensive blood pressure control on recurrent cardiovascular events in a cardiovascular clinical trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19015v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuki Ohnishi, Michael O. Harhay, Fan Li</dc:creator>
    </item>
    <item>
      <title>Choosing Better NLDR Layouts by Evaluating the Model in the High-dimensional Data Space</title>
      <link>https://arxiv.org/abs/2506.22051</link>
      <description>arXiv:2506.22051v3 Announce Type: replace 
Abstract: Nonlinear dimension reduction (NLDR) techniques such as tSNE, and UMAP provide a low-dimensional representation of high-dimensional data ($p\text{-}D$) by applying a nonlinear transformation. NLDR often exaggerates random patterns. But NLDR views have an important role in data analysis because, if done well, they provide a concise visual (and conceptual) summary of $p\text{-}D$ distributions. The NLDR methods and hyper-parameter choices can create wildly different representations, making it difficult to decide which is best, or whether any or all are accurate or misleading. To help assess the NLDR and decide on which, if any, is the most reasonable representation of the structure(s) present in the $p\text{-}D$ data, we have developed an algorithm to show the $2\text{-}D$ NLDR model in the $p\text{-}D$ space, viewed with a tour, a movie of linear projections. From this, one can see if the model fits everywhere, or better in some subspaces, or completely mismatches the data. Also, we can see how different methods may have similar summaries or quirks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22051v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jayani P. Gamage, Dianne Cook, Paul Harrison, Michael Lydeamore, Thiyanga S. Talagala</dc:creator>
    </item>
    <item>
      <title>Robust fuzzy clustering with cellwise outliers</title>
      <link>https://arxiv.org/abs/2508.03310</link>
      <description>arXiv:2508.03310v2 Announce Type: replace 
Abstract: In a data matrix, it can be distinguished between observations, each represented by a full row vector for an individual, and cells, which correspond to single entries of that matrix. Recent developments in robust statistics have introduced the cellwise contamination paradigm, which assumes contamination on cells rather than on entire observations. This approach becomes particularly relevant as the number of variables increases. Indeed, discarding or downweighting entire observations because of a few anomalous values in them, as done by traditional (casewise) robust methods, can result in substantial information loss, since the non-contaminated (or reliable) cells can still be highly informative. This philosophy can also be considered in fuzzy clustering, by assuming that reliable cells within an observation may still provide useful information for determining fuzzy memberships. A robust fuzzy clustering proposal is thus introduced in this work, combining the advantages of dealing with outlying cells and simultaneously controlling the degrees of fuzziness of observation assignments. The cluster-specific relationships among variables, detected by the fuzzy clustering approach, are also key to better identifying outlying cells. The strengths of the proposed methodology are illustrated through a simulation study and two real-world applications. The effects of the model's tuning parameters are explored, and some guidance for users on how to set them suitably is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03310v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giorgia Zaccaria, Lorenzo Benzakour, Luis A. Garc\'ia-Escudero, Francesca Greselin, Agust\'in Mayo-\'Iscar</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian Calibration of Computer Models</title>
      <link>https://arxiv.org/abs/2509.22597</link>
      <description>arXiv:2509.22597v3 Announce Type: replace 
Abstract: Calibration of computer models is a key step in making inferences, predictions, and decisions for complex science and engineering systems. We formulate and analyze a nonparametric Bayesian methodology for computer model calibration. This paper presents a number of key results including; establishment of a unique nonparametric Bayesian posterior corresponding to a chosen prior with an explicit formula for the corresponding conditional density; a maximum entropy property of the posterior corresponding to the uniform prior; the almost everywhere continuity of the density of the nonparametric posterior; and a comprehensive convergence and asymptotic analysis of an estimator based on a form of importance sampling. We illustrate the problem and results using several examples, including a simple experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22597v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haiyi Shi, Lei Yang, Jiarui Chi, Troy Butler, Haonan Wang, Derek Bingham, Don Estep</dc:creator>
    </item>
    <item>
      <title>Zeroes and Extrema of Functions via Random Measures</title>
      <link>https://arxiv.org/abs/2511.10293</link>
      <description>arXiv:2511.10293v3 Announce Type: replace 
Abstract: We present methods that provide all zeroes and extrema of a function that do not require differentiation. Using point process theory, we are able to describe the locations of zeroes or maxima, their number, as well as their distribution over a given window of observation. The algorithms in order to accomplish the theoretical development are also provided, and they are exemplified using many illustrative examples, for real and complex functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10293v3</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Athanasios Christou Micheas</dc:creator>
    </item>
    <item>
      <title>Two-stage Estimation for Causal Inference Involving a Semi-continuous Exposure</title>
      <link>https://arxiv.org/abs/2511.20985</link>
      <description>arXiv:2511.20985v2 Announce Type: replace 
Abstract: Methods for causal inference are well developed for binary and continuous exposures, but in many settings, the exposure has a substantial mass at zero-such exposures are called semi-continuous. We propose a general causal framework for such semi-continuous exposures, together with a novel two-stage estimation strategy. A two-part propensity structure is introduced for the semi-continuous exposure, with one component for exposure status (exposed vs unexposed) and another for the exposure level among those exposed, and incorporates both into a marginal structural model that disentangles the effects of exposure status and dose. The two-stage procedure sequentially targets the causal dose-response among exposed individuals and the causal effect of exposure status at a reference dose, allowing flexibility in the choice of propensity score methods in the second stage. We establish consistency and asymptotic normality for the resulting estimators, and characterise their limiting values under misspecification of the propensity score models. Simulation studies evaluate finite sample performance and robustness, and an application to a study of prenatal alcohol exposure and child cognition demonstrates how the proposed methods can be used to address a range of scientific questions about both exposure status and exposure intensity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20985v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoya Wang, Richard J. Cook, Yeying Zhu, Tugba Akkaya-Hocagil, R. Colin Carter, Sandra W. Jacobson, Joseph L. Jacobson, Louise M. Ryan</dc:creator>
    </item>
    <item>
      <title>Information-Theoretic Limits and Strong Consistency on Binary Non-uniform Hypergraph Stochastic Block Models</title>
      <link>https://arxiv.org/abs/2306.06845</link>
      <description>arXiv:2306.06845v3 Announce Type: replace-cross 
Abstract: We investigate the unsupervised node classification problem on random hypergraphs under the non-uniform Hypergraph Stochastic Block Model (HSBM) with two equal-sized communities. In this model, edges appear independently with probabilities depending only on the labels of their vertices. We identify the threshold for strong consistency, expressed in terms of the Generalized Hellinger distance. Below this threshold, strong consistency is impossible, and we derive the Information-Theoretic (IT) lower bound on the expected mismatch ratio. Above the threshold, the parameter space is typically divided into two disjoint regions. When only the aggregated adjacency matrices are accessible, while one-stage algorithms accomplish strong consistency with high probability in the region far from the threshold, they fail in the region closer to the threshold. We propose a new refinement algorithm which, in conjunction with the initial estimation, provably achieves strong consistency throughout the entire region above the threshold, and attains the IT lower bound when below the threshold, proving its optimality. This novel refinement algorithm applies the power iteration method to a weighted adjacency matrix, where the weights are determined by hyperedge sizes and the initial label estimate. Unlike the constant degree regime where a subset selection of uniform layers is necessary to enhance clustering accuracy, in the scenario with diverging degrees, each uniform layer contributes non-negatively to clustering accuracy. Therefore, aggregating information across all uniform layers yields better performance than using any single layer alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06845v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hai-Xiao Wang</dc:creator>
    </item>
    <item>
      <title>Totally Concave Regression</title>
      <link>https://arxiv.org/abs/2501.04360</link>
      <description>arXiv:2501.04360v3 Announce Type: replace-cross 
Abstract: Shape constraints in nonparametric regression provide a powerful framework for estimating regression functions under realistic assumptions without tuning parameters. However, most existing methods$\unicode{x2013}$except additive models$\unicode{x2013}$impose too weak restrictions, often leading to overfitting in high dimensions. Conversely, additive models can be too rigid, failing to capture covariate interactions. This paper introduces a novel multivariate shape-constrained regression approach based on total concavity, originally studied by T. Popoviciu. Our method allows interactions while mitigating the curse of dimensionality, with convergence rates that depend only logarithmically on the number of covariates. We characterize and compute the least squares estimator over totally concave functions, derive theoretical guarantees, and demonstrate its practical effectiveness through empirical studies on real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04360v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dohyeong Ki, Adityanand Guntuboyina</dc:creator>
    </item>
    <item>
      <title>PLRD: Partially Linear Regression Discontinuity Inference</title>
      <link>https://arxiv.org/abs/2503.09907</link>
      <description>arXiv:2503.09907v2 Announce Type: replace-cross 
Abstract: Regression discontinuity designs have become one of the most popular research designs in empirical economics. We argue, however, that widely used approaches to building confidence intervals in regression discontinuity designs exhibit suboptimal behavior in practice: In a simulation study calibrated to high-profile applications of regression discontinuity designs, existing methods either have systematic under-coverage or have wider-than-necessary intervals. We propose a new approach, partially linear regression discontinuity inference (PLRD), and find it to address shortcomings of existing methods: Throughout our experiments, confidence intervals built using PLRD are both valid and short. We also provide large-sample guarantees for PLRD under smoothness assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09907v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Ghosh, Guido Imbens, Stefan Wager</dc:creator>
    </item>
    <item>
      <title>TabPFN: One Model to Rule Them All?</title>
      <link>https://arxiv.org/abs/2505.20003</link>
      <description>arXiv:2505.20003v2 Announce Type: replace-cross 
Abstract: Hollmann et al. (Nature 637 (2025) 319-326) recently introduced TabPFN, a transformer-based deep learning model for regression and classification on tabular data, which they claim "outperforms all previous methods on datasets with up to 10,000 samples by a wide margin, using substantially less training time." Furthermore, they have called TabPFN a "foundation model" for tabular data, as it can support "data generation, density estimation, learning reusable embeddings and fine-tuning". In this paper, we provide a tailored explanation of how TabPFN works for a statistics audience, by emphasizing its interpretation as approximate Bayesian inference. We then explore the significance of TabPFN to the field of statistics: We show that an out-of-the-box application of TabPFN can sometimes outperform specialized state-of-the-art methods for semi-supervised parameter estimation, prediction under covariate shift, and heterogeneous treatment effect estimation. As a partial explanation for the predictive effectiveness of TabPFN, we show that it can simultaneously adapt to both nonparametric structure and parametric structure, for instance, sometimes outperforming LASSO even when assumptions are correctly specified. All experiments can be reproduced using the code provided at https://github.com/qinglong-tian/tabpfn_study (https://github.com/qinglong-tian/tabpfn_study).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20003v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiong Zhang, Yan Shuo Tan, Qinglong Tian, Pengfei Li</dc:creator>
    </item>
    <item>
      <title>Network Dynamics and Spatial Shifts in Civilian Targeting: A Stochastic Block Model Analysis of the Colombian Armed Conflict</title>
      <link>https://arxiv.org/abs/2508.09051</link>
      <description>arXiv:2508.09051v2 Announce Type: replace-cross 
Abstract: In this article, we explore how the escalating victimization of civilians during civil wars is mirrored in the fragmented distribution of territorial control, focusing on the Colombian armed conflict. Through an exhaustive characterization of the topology of bipartite and projected networks of municipalities, we describe changes in territorial configurations across different periods between 1978 and 2007. By employing stochastic block models for count data, we show that, during periods dominated by a small set of actors, the networks adopt a centralized node periphery structure, whereas during times of widespread conflict, areas of influence overlap in complex ways. Our findings also suggest the existence of cohesive municipal communities shaped by both geographic proximity and affinities between armed structures, as well as internally dispersed groups with a high likelihood of interaction. As the spatial distribution shifts toward a more fragmented arrangement, the average interaction intensity between communities predicted by the stochastic block model approaches that within communities, indicating a weakening of modular structure and increased inter community connectivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09051v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Natalia Perdomo-Londo\~no, Juan Sosa, Emma J. Camargo-D\'iaz</dc:creator>
    </item>
    <item>
      <title>Big Wins, Small Net Gains: Direct and Spillover Effects of First Industry Entries in Puerto Rico</title>
      <link>https://arxiv.org/abs/2511.19469</link>
      <description>arXiv:2511.19469v2 Announce Type: replace-cross 
Abstract: I study how first sizable industry entries reshape local and neighboring labor markets in Puerto Rico. Using over a decade of quarterly municipality--industry data (2014Q1--2025Q1), I identify ``first sizable entries'' as large, persistent jumps in establishments, covered employment, and wage bill, and treat these as shocks to local industry presence at the municipio--industry level. Methodologically, I combine staggered-adoption difference-in-differences estimators that are robust to heterogeneous treatment timing with an imputation-based event-study approach, and I use a doubly robust difference-in-differences framework that explicitly allows for interference through pre-specified exposure mappings on a contiguity graph. The estimates show large and persistent direct gains in covered employment and wage bill in the treated municipality--industry cells over 0--16 quarters. Same-industry neighbors experience sizable short-run gains that reverse over the medium run, while within-municipality cross-industry and neighbor all-industries spillovers are small and imprecisely estimated. Once these spillovers are taken into account and spatially robust inference and sensitivity checks are applied, the net regional 0--16 quarter effect on covered employment is positive but modest in magnitude and estimated with considerable uncertainty. The results imply that first sizable entries generate substantial local gains where they occur, but much smaller and less precisely measured net employment gains for the broader regional economy, highlighting the importance of accounting for spatial spillovers when evaluating place-based policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19469v2</guid>
      <category>econ.GN</category>
      <category>econ.EM</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jorge A. Arroyo</dc:creator>
    </item>
  </channel>
</rss>
