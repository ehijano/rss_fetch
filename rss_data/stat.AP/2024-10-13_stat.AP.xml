<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 14 Oct 2024 04:01:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A multi-year CONUS-wide analysis of lightning strikes to wind turbines</title>
      <link>https://arxiv.org/abs/2410.08444</link>
      <description>arXiv:2410.08444v1 Announce Type: new 
Abstract: Lightning strikes to wind turbines (WTs) pose significant hazards and operational costs to the renewable wind industry. These strikes fall into two categories: downward cloud-to-ground (CG) strokes and upward discharges, which can be self-initiated or triggered by a nearby flash. The incidence of each type of strike depends on several factors, including the electrical structure of the thunderstorm and turbine height. The strike rates of CG strokes and triggered upward lightning can be normalized by the amount of local CG activity, where the constant of proportionality carries units of area and is often termed the collection area. This paper introduces a statistical analysis technique that uses lightning locating system (LLS) data to estimate the collection areas for downward and triggered upward lightning strikes to WTs. The technique includes a normalization method that addresses the confounding factor of neighboring WTs. This analysis method is applied to seven years of data from the National Lightning Detection Network$^\textrm{TM}$ and the US Wind Turbine Database to investigate the dependence of collection areas on blade tip height and peak current. The results are compared against estimates of collection areas derived from counts of LLS-detected CG strokes close to WTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08444v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Said, Eric Grimit, Martin Murphy</dc:creator>
    </item>
    <item>
      <title>Online stochastic generators using Slepian bases for regional bivariate wind speed ensembles from ERA5</title>
      <link>https://arxiv.org/abs/2410.08945</link>
      <description>arXiv:2410.08945v1 Announce Type: new 
Abstract: Reanalysis data, such as ERA5, provide a comprehensive and detailed representation of the Earth's system by assimilating observations into climate models. While crucial for climate research, they pose significant challenges in terms of generation, storage, and management. For 3-hourly bivariate wind speed ensembles from ERA5, which face these challenges, this paper proposes an online stochastic generator (OSG) applicable to any global region, offering fast stochastic approximations while storing only model parameters. A key innovation is the incorporation of the online updating, which allows data to sequentially enter the model in blocks of time and contribute to parameter updates. This approach reduces storage demands during modeling by eliminating the need to store and analyze the entire dataset, and enables near real-time emulations that complement the generation of reanalysis data. The Slepian concentration technique supports the efficiency of the proposed OSG by representing the data in a lower-dimensional space spanned by data-independent Slepian bases optimally concentrated within the specified region. We demonstrate the flexibility and efficiency of the OSG through two case studies requiring long and short blocks, specified for the Arabian-Peninsula region (ARP). For both cases, the OSG performs well across several statistical metrics and is comparable to the SG trained on the full dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08945v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Song, Zubair Khalid, Marc G. Genton</dc:creator>
    </item>
    <item>
      <title>Ensemble Kalman Inversion for Geothermal Reservoir Modelling</title>
      <link>https://arxiv.org/abs/2410.09017</link>
      <description>arXiv:2410.09017v1 Announce Type: new 
Abstract: Numerical models of geothermal reservoirs typically depend on hundreds or thousands of unknown parameters, which must be estimated using sparse, noisy data. However, these models capture complex physical processes, which frequently results in long run-times and simulation failures, making the process of estimating the unknown parameters a challenging task. Conventional techniques for parameter estimation and uncertainty quantification, such as Markov chain Monte Carlo (MCMC), can require tens of thousands of simulations to provide accurate results and are therefore challenging to apply in this context. In this paper, we study the ensemble Kalman inversion (EKI) algorithm as an alternative technique for approximate parameter estimation and uncertainty quantification for geothermal reservoir models. EKI possesses several characteristics that make it well-suited to a geothermal setting; it is derivative-free, parallelisable, robust to simulation failures, and requires far fewer simulations than conventional uncertainty quantification techniques such as MCMC. We illustrate the use of EKI in a reservoir modelling context using a combination of synthetic and real-world case studies. Through these case studies, we also demonstrate how EKI can be paired with flexible parametrisation techniques capable of accurately representing prior knowledge of the characteristics of a reservoir and adhering to geological constraints, and how the algorithm can be made robust to simulation failures. Our results demonstrate that EKI provides a reliable and efficient means of obtaining accurate parameter estimates for large-scale, two-phase geothermal reservoir models, with appropriate characterisation of uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09017v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex de Beer, Elvar K Bjarkason, Michael Gravatt, Ruanui Nicholson, John P O'Sullivan, Oliver J Maclaren</dc:creator>
    </item>
    <item>
      <title>EarthquakeNPP: Benchmark Datasets for Earthquake Forecasting with Neural Point Processes</title>
      <link>https://arxiv.org/abs/2410.08226</link>
      <description>arXiv:2410.08226v1 Announce Type: cross 
Abstract: Classical point process models, such as the epidemic-type aftershock sequence (ETAS) model, have been widely used for forecasting the event times and locations of earthquakes for decades. Recent advances have led to Neural Point Processes (NPPs), which promise greater flexibility and improvements over classical models. However, the currently-used benchmark dataset for NPPs does not represent an up-to-date challenge in the seismological community since it lacks a key earthquake sequence from the region and improperly splits training and testing data. Furthermore, initial earthquake forecast benchmarking lacks a comparison to state-of-the-art earthquake forecasting models typically used by the seismological community. To address these gaps, we introduce EarthquakeNPP: a collection of benchmark datasets to facilitate testing of NPPs on earthquake data, accompanied by a credible implementation of the ETAS model. The datasets cover a range of small to large target regions within California, dating from 1971 to 2021, and include different methodologies for dataset generation. In a benchmarking experiment, we compare three spatio-temporal NPPs against ETAS and find that none outperform ETAS in either spatial or temporal log-likelihood. These results indicate that current NPP implementations are not yet suitable for practical earthquake forecasting. However, EarthquakeNPP will serve as a platform for collaboration between the seismology and machine learning communities with the goal of improving earthquake predictability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08226v1</guid>
      <category>physics.geo-ph</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Stockman, Daniel Lawson, Maximilian Werner</dc:creator>
    </item>
    <item>
      <title>Forecasting mortality associated emergency department crowding</title>
      <link>https://arxiv.org/abs/2410.08247</link>
      <description>arXiv:2410.08247v1 Announce Type: cross 
Abstract: Emergency department (ED) crowding is a global public health issue that has been repeatedly associated with increased mortality. Predicting future service demand would enable preventative measures aiming to eliminate crowding along with it's detrimental effects. Recent findings in our ED indicate that occupancy ratios exceeding 90% are associated with increased 10-day mortality. In this paper, we aim to predict these crisis periods using retrospective data from a large Nordic ED with a LightGBM model. We provide predictions for the whole ED and individually for it's different operational sections. We demonstrate that afternoon crowding can be predicted at 11 a.m. with an AUC of 0.82 (95% CI 0.78-0.86) and at 8 a.m. with an AUC up to 0.79 (95% CI 0.75-0.83). Consequently we show that forecasting mortality-associated crowding using anonymous administrative data is feasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08247v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jalmari Nevanlinna, Anna Eidst{\o}, Jari Yl\"a-Mattila, Teemu Koivistoinen, Niku Oksala, Juho Kanniainen, Ari Palom\"aki, Antti Roine</dc:creator>
    </item>
    <item>
      <title>A scientific review on advances in statistical methods for crossover design</title>
      <link>https://arxiv.org/abs/2410.08441</link>
      <description>arXiv:2410.08441v1 Announce Type: cross 
Abstract: A comprehensive review of the literature on crossover design is needed to highlight its evolution, applications, and methodological advancements across various fields. Given its widespread use in clinical trials and other research domains, understanding this design's challenges, assumptions, and innovations is essential for optimizing its implementation and ensuring accurate, unbiased results. This article extensively reviews the history and statistical inference methods for crossover designs. A primary focus is given to the AB-BA design as it is the most widely used design in literature. Extension from two periods to higher-order designs is discussed, and a general inference procedure for continuous response is studied. Analysis of multivariate and categorical responses is also reviewed in this context. A bunch of open problems in this area are shortlisted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08441v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Salil Koner</dc:creator>
    </item>
    <item>
      <title>Causal machine learning for predicting treatment outcomes</title>
      <link>https://arxiv.org/abs/2410.08770</link>
      <description>arXiv:2410.08770v1 Announce Type: cross 
Abstract: Causal machine learning (ML) offers flexible, data-driven methods for predicting treatment outcomes including efficacy and toxicity, thereby supporting the assessment and safety of drugs. A key benefit of causal ML is that it allows for estimating individualized treatment effects, so that clinical decision-making can be personalized to individual patient profiles. Causal ML can be used in combination with both clinical trial data and real-world data, such as clinical registries and electronic health records, but caution is needed to avoid biased or incorrect predictions. In this Perspective, we discuss the benefits of causal ML (relative to traditional statistical or ML approaches) and outline the key components and steps. Finally, we provide recommendations for the reliable use of causal ML and effective translation into the clinic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.08770v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1038/s41591-024-02902-1</arxiv:DOI>
      <arxiv:journal_reference>Nature Medicine, vol. 30, pp. 958-968 (2024)</arxiv:journal_reference>
      <dc:creator>Stefan Feuerriegel, Dennis Frauen, Valentyn Melnychuk, Jonas Schweisthal, Konstantin Hess, Alicia Curth, Stefan Bauer, Niki Kilbertus, Isaac S. Kohane, Mihaela van der Schaar</dc:creator>
    </item>
    <item>
      <title>Variance reduction combining pre-experiment and in-experiment data</title>
      <link>https://arxiv.org/abs/2410.09027</link>
      <description>arXiv:2410.09027v1 Announce Type: cross 
Abstract: Online controlled experiments (A/B testing) are essential in data-driven decision-making for many companies. Increasing the sensitivity of these experiments, particularly with a fixed sample size, relies on reducing the variance of the estimator for the average treatment effect (ATE). Existing methods like CUPED and CUPAC use pre-experiment data to reduce variance, but their effectiveness depends on the correlation between the pre-experiment data and the outcome. In contrast, in-experiment data is often more strongly correlated with the outcome and thus more informative. In this paper, we introduce a novel method that combines both pre-experiment and in-experiment data to achieve greater variance reduction than CUPED and CUPAC, without introducing bias or additional computation complexity. We also establish asymptotic theory and provide consistent variance estimators for our method. Applying this method to multiple online experiments at Etsy, we reach substantial variance reduction over CUPAC with the inclusion of only a few in-experiment covariates. These results highlight the potential of our approach to significantly improve experiment sensitivity and accelerate decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09027v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhexiao Lin, Pablo Crespo</dc:creator>
    </item>
    <item>
      <title>Towards a Health-Based Power Grid Optimization in the Artificial Intelligence Era</title>
      <link>https://arxiv.org/abs/2410.09029</link>
      <description>arXiv:2410.09029v1 Announce Type: cross 
Abstract: The electric power sector is one of the largest contributors to greenhouse gas emissions in the world. In recent years, there has been an unprecedented increase in electricity demand driven by the so-called Artificial Intelligence (AI) revolution. Although AI has and will continue to have a transformative impact, its environmental and health impacts are often overlooked. The standard approach to power grid optimization aims to minimize CO$_2$ emissions. In this paper, we propose a new holistic paradigm. Our proposed optimization directly targets the minimization of adverse health outcomes under energy efficiency and emission constraints. We show the first example of an optimal fuel mix allocation problem aiming to minimize the average number of adverse health effects resulting from exposure to hazardous air pollutants with constraints on the average and marginal emissions. We argue that this new health-based power grid optimization is essential to promote truly sustainable technological advances that align both with global climate goals and public health priorities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09029v1</guid>
      <category>eess.SP</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claudio Battiloro, Gianluca Guidi, Falco J. Bargagli-Stoffi, Francesca Dominici</dc:creator>
    </item>
    <item>
      <title>Latent Space-Based Likelihood Estimation Using a Single Observation for Bayesian Updating of a Nonlinear Hysteretic Model</title>
      <link>https://arxiv.org/abs/2404.03871</link>
      <description>arXiv:2404.03871v2 Announce Type: replace 
Abstract: This study presents a novel approach to quantifying uncertainties in Bayesian model updating, which is effective in sparse or single observations. Conventional uncertainty quantification metrics such as the Euclidean and Bhattacharyya distance-based metrics are potential in scenarios with ample observations. However, their validation is limited in situations with insufficient data, particularly for nonlinear responses like post-yield behavior. Our method addresses this challenge by using the latent space of a Variational Auto-encoder (VAE), a generative model that enables nonparametric likelihood evaluation. This approach is valuable in updating model parameters based on nonlinear seismic responses of structure, wherein data scarcity is a common challenge. Our numerical experiments confirm the ability of the proposed method to accurately update parameters and quantify uncertainties using limited observations. Additionally, these numerical experiments reveal a tendency for increased information about nonlinear behavior to result in decreased uncertainty in terms of estimations. This study provides a robust tool for quantifying uncertainty in scenarios characterized by considerable uncertainty, thereby expanding the applicability of Bayesian updating methods in data-constrained environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03871v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1061/AJRUA6.RUENG-1305</arxiv:DOI>
      <arxiv:journal_reference>ASCE-ASME Journal of Risk and Uncertainty in Engineering Systems, Part A: Civil Engineering, Vol. 10, No. 4, pp. 04024072, 2024</arxiv:journal_reference>
      <dc:creator>Sangwon Lee, Taro Yaoyama, Yuma Matsumoto, Takenori Hida, Tatsuya Itoi</dc:creator>
    </item>
    <item>
      <title>Probabilistic Inversion Modeling of Gas Emissions: A Gradient-Based MCMC Estimation of Gaussian Plume Parameters</title>
      <link>https://arxiv.org/abs/2408.01298</link>
      <description>arXiv:2408.01298v2 Announce Type: replace 
Abstract: In response to global concerns regarding air quality and the environmental impact of greenhouse gas emissions, detecting and quantifying sources of emissions has become critical. To understand this impact and target mitigations effectively, methods for accurate quantification of greenhouse gas emissions are required. In this paper, we focus on the inversion of concentration measurements to estimate source location and emission rate. In practice, such methods often rely on atmospheric stability class-based Gaussian plume dispersion models. However, incorrectly identifying the atmospheric stability class can lead to significant bias in estimates of source characteristics. We present a robust approach that reduces this bias by jointly estimating the horizontal and vertical dispersion parameters of the Gaussian plume model, together with source location and emission rate, atmospheric background concentration, and sensor measurement error variance. Uncertainty in parameter estimation is quantified through probabilistic inversion using gradient-based MCMC methods. A simulation study is performed to assess the inversion methodology. We then focus on inference for the published Chilbolton dataset which contains controlled methane releases and demonstrates the practical benefits of estimating dispersion parameters in source inversion problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01298v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Newman, Christopher Nemeth, Matthew Jones, Philip Jonathan</dc:creator>
    </item>
    <item>
      <title>Data-driven Characterization of Near-Surface Velocity in the San Francisco Bay Area: A Stationary and Spatially Varying Approach</title>
      <link>https://arxiv.org/abs/2409.18856</link>
      <description>arXiv:2409.18856v2 Announce Type: replace 
Abstract: This study presents the development of two new sedimentary velocity models for the San Francisco Bay Area (SFBA) to improve the near-surface representation of shear-wave velocity ($V_S$) for large-scale, broadband numerical simulations, with the ultimate goal of enhancing the representation of the sedimentary layers in the Bay Area community velocity model. The first velocity model is stationary and is based solely on $V_{S30}$; the second velocity model is spatially varying and has location-specific adjustments. They were developed using a dataset of 200 measured $V_S$ profiles. Both models were formulated within a hierarchical Bayesian framework, using a parameterization that ensures robust scaling. The spatially varying model includes a slope adjustment term modeled as a Gaussian process to capture site-specific effects based on location. Residual analysis shows that both models are unbiased for $V_S$ values up to 1000 m/sec. Along-depth variability models were also developed using within-profile residuals. The proposed models show higher $V_S$ in the San Jose area and Livermore Valley compared to the USGS Bay Area community velocity model by a factor of two or more in some cases. Goodness-of-fit (GOF) comparisons using one-dimensional linear site-response analysis at selected sites demonstrate that the proposed models outperform the USGS model in capturing near-surface amplification across a broad frequency range. Incorporating along-depth variability further improves the GOF scores by reducing over-amplification at high frequencies. These results underscore the importance of integrating data-driven models of the shallow crust, like the ones presented here, in coarser regional community velocity models to enhance regional seismic hazard assessments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18856v2</guid>
      <category>stat.AP</category>
      <category>physics.geo-ph</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grigorios Lavrentiadis, Elnaz Seylabi, Feiruo Xia, Hesam Tehrani, Domniki Asimaki, David McCallen</dc:creator>
    </item>
    <item>
      <title>On the application of Gaussian graphical models to paired data problems</title>
      <link>https://arxiv.org/abs/2307.14160</link>
      <description>arXiv:2307.14160v2 Announce Type: replace-cross 
Abstract: Gaussian graphical models are nowadays commonly applied to the comparison of groups sharing the same variables, by jointy learning their independence structures. We consider the case where there are exactly two dependent groups and the association structure is represented by a family of coloured Gaussian graphical models suited to deal with paired data problems. To learn the two dependent graphs, together with their across-graph association structure, we implement a fused graphical lasso penalty. We carry out a comprehensive analysis of this approach, with special attention to the role played by some relevant submodel classes. In this way, we provide a broad set of tools for the application of Gaussian graphical models to paired data problems. These include results useful for the specification of penalty values in order to obtain a path of lasso solutions and an ADMM algorithm that solves the fused graphical lasso optimization problem. Finally, we present an application of our method to cancer genomics where it is of interest to compare cancer cells with a control sample from histologically normal tissues adjacent to the tumor. All the methods described in this article are implemented in the $\texttt{R}$ package $\texttt{pdglasso}$ availabe at: https://github.com/savranciati/pdglasso.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.14160v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saverio Ranciati, Alberto Roverato</dc:creator>
    </item>
    <item>
      <title>A Generalized Difference-in-Differences Estimator for Randomized Stepped-Wedge and Observational Staggered Adoption Settings</title>
      <link>https://arxiv.org/abs/2405.08730</link>
      <description>arXiv:2405.08730v3 Announce Type: replace-cross 
Abstract: Staggered treatment adoption arises in the evaluation of policy impact and implementation in many settings, including both randomized stepped-wedge trials and non-randomized quasi-experiments with panel data. In both settings, getting an interpretable, unbiased effect estimate requires careful consideration of the target estimand and possible treatment effect heterogeneities. This paper proposes a novel non-parametric approach to this estimation for either setting. By constructing an estimator using weighted averages of two-by-two difference-in-differences comparisons as building blocks, the investigator can target the desired estimand for any assumed treatment effect heterogeneities. This provides desirable bias and interpretation properties while using the comparisons efficiently to mitigate the loss of precision, without requiring correct variance specification. The methods are demonstrated for both a randomized stepped-wedge trial on the impact of novel tuberculosis diagnostic tools and an observational staggered adoption study on the effects of COVID-19 vaccine financial incentive lotteries in U.S. states; these are compared to analyses using previous methods. A full algorithm with R code is provided to implement this method and to compare against existing methods. The proposed method allows for high flexibility and clear targeting of desired effects, providing one solution to the bias-variance-generalizability tradeoff.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08730v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lee Kennedy-Shaffer</dc:creator>
    </item>
    <item>
      <title>Anatomy of Elite and Mass Polarization in Social Networks</title>
      <link>https://arxiv.org/abs/2406.12525</link>
      <description>arXiv:2406.12525v2 Announce Type: replace-cross 
Abstract: In the political arena of social platforms, opposing factions of varying sizes show asymmetrical patterns, and elites and masses within these groups have divergent motivations and influence,challenging simplistic views of polarization. Yet, existing methods for quantifying polarization reduce division to a single value, assuming uniform distribution of polarization online. While this approach can confirm the observed increase in political polarization in many societies, it overlooks complexities that could explain this phenomenon. Notably, opposing groups can have unequal impacts on polarization, and the literature shows division between elites and the masses is a critical factor to consider.
  We propose a method to decompose existing polarization measures in order to quantify the role of groups, determined by these distinct hierarchies, in the total polarization value. We applied this method to polarized topics in the Finnish Twittersphere surrounding the 2019 and 2023parliamentary elections. Our analysis reveals two key insights: 1) The impact of opposing groups on observed polarization is rarely balanced, and 2) while elites strongly contribute to structural polarization and consistently display greater alignment across various topics, the masses have also recently experienced a surge in issue alignment, a stronger form of polarization.
  Our findings suggest that the masses may not be as immune to an increasingly polarized environment as previously thought. This research provides a more nuanced understanding of polarization dynamics, offering potential insights into its underlying mechanisms and evolution</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12525v2</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Salloum, Ted Hsuan Yun Chen, Mikko Kivel\"a</dc:creator>
    </item>
    <item>
      <title>Annotation aggregation of multi-label ecological datasets via Bayesian modeling</title>
      <link>https://arxiv.org/abs/2406.15844</link>
      <description>arXiv:2406.15844v2 Announce Type: replace-cross 
Abstract: Ecological and conservation studies monitoring bird communities typically rely on species classification based on bird vocalizations. Historically, this has been based on expert volunteers going into the field and making lists of the bird species that they observe. Recently, machine learning algorithms have emerged that can accurately classify bird species based on audio recordings of their vocalizations. Such algorithms crucially rely on training data that are labeled by experts. Automated classification is challenging when multiple species are vocalizing simultaneously, there is background noise, and/or the bird is far from the microphone. In continuously monitoring different locations, the size of the audio data become immense and it is only possible for human experts to label a tiny proportion of the available data. In addition, experts can vary in their accuracy and breadth of knowledge about different species. This article focuses on the important problem of combining sparse expert annotations to improve bird species classification while providing uncertainty quantification. We additionally are interested in providing expert performance scores to increase their engagement and encourage improvements. We propose a Bayesian hierarchical modeling approach and evaluate this approach on a new community science platform developed in Finland.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15844v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoxuan Wang, Patrik Lauha, David B. Dunson</dc:creator>
    </item>
  </channel>
</rss>
