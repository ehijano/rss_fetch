<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Mar 2024 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 20 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Answer Set Programming for Flexible Payroll Management</title>
      <link>https://arxiv.org/abs/2403.12823</link>
      <description>arXiv:2403.12823v1 Announce Type: new 
Abstract: Payroll management is a critical business task that is subject to a large number of rules, which vary widely between companies, sectors, and countries. Moreover, the rules are often complex and change regularly. Therefore, payroll management systems must be flexible in design. In this paper, we suggest an approach based on a flexible Answer Set Programming (ASP) model and an easy-to-read tabular representation based on the Decision Model and Notation (DMN) standard. It allows HR consultants to represent complex rules without the need for a software engineer, and to ultimately design payroll systems for a variety of different scenarios. We show how the multi-shot solving capabilities of the clingo ASP system can be used to reach the performance that is necessary to handle real-world instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12823v1</guid>
      <category>cs.LO</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Callewaert, Joost Vennekens</dc:creator>
    </item>
    <item>
      <title>Routing and Scheduling in Answer Set Programming applied to Multi-Agent Path Finding: Preliminary Report</title>
      <link>https://arxiv.org/abs/2403.12153</link>
      <description>arXiv:2403.12153v1 Announce Type: cross 
Abstract: We present alternative approaches to routing and scheduling in Answer Set Programming (ASP), and explore them in the context of Multi-agent Path Finding. The idea is to capture the flow of time in terms of partial orders rather than time steps attached to actions and fluents. This also abolishes the need for fixed upper bounds on the length of plans. The trade-off for this avoidance is that (parts of) temporal trajectories must be acyclic, since multiple occurrences of the same action or fluent cannot be distinguished anymore. While this approach provides an interesting alternative for modeling routing, it is without alternative for scheduling since fine-grained timings cannot be represented in ASP in a feasible way. This is different for partial orders that can be efficiently handled by external means such as acyclicity and difference constraints. We formally elaborate upon this idea and present several resulting ASP encodings. Finally, we demonstrate their effectiveness via an empirical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12153v1</guid>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <category>cs.SC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roland Kaminski, Torsten Schaub, Tran Cao Son, Ji\v{r}\'i \v{S}vancara, Philipp Wanko</dc:creator>
    </item>
    <item>
      <title>An Upper Bound on the Weisfeiler-Leman Dimension</title>
      <link>https://arxiv.org/abs/2403.12581</link>
      <description>arXiv:2403.12581v1 Announce Type: cross 
Abstract: The Weisfeiler-Leman (WL) dimension is a standard measure in descriptive complexity theory for the structural complexity of a graph. We prove that the WL-dimension of a graph on $n$ vertices is at most $3/20 \cdot n + o(n)= 0.15 \cdot n + o(n)$. The proof develops various techniques to analyze the structure of coherent configurations.
  This includes sufficient conditions under which a fiber can be restored up to isomorphism if it is removed, a recursive proof exploiting a degree reduction and treewidth bounds, as well as an analysis of interspaces involving small fibers.
  As a base case, we also analyze the dimension of coherent configurations with small fiber size and thereby graphs with small color class size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12581v1</guid>
      <category>cs.DM</category>
      <category>cs.LO</category>
      <category>math.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Schneider, Pascal Schweitzer</dc:creator>
    </item>
    <item>
      <title>Enhancing Formal Theorem Proving: A Comprehensive Dataset for Training AI Models on Coq Code</title>
      <link>https://arxiv.org/abs/2403.12627</link>
      <description>arXiv:2403.12627v1 Announce Type: cross 
Abstract: In the realm of formal theorem proving, the Coq proof assistant stands out for its rigorous approach to verifying mathematical assertions and software correctness. Despite the advances in artificial intelligence and machine learning, the specialized nature of Coq syntax and semantics poses unique challenges for Large Language Models (LLMs). Addressing this gap, we present a comprehensive dataset specifically designed to enhance LLMs' proficiency in interpreting and generating Coq code. This dataset, derived from a collection of over 10,000 Coq source files, encompasses a wide array of propositions, proofs, and definitions, enriched with metadata including source references and licensing information. Our primary aim is to facilitate the development of LLMs capable of generating syntactically correct and semantically meaningful Coq constructs, thereby advancing the frontier of automated theorem proving. Initial experiments with this dataset have showcased its significant potential; models trained on this data exhibited enhanced accuracy in Coq code generation. Notably, a particular experiment revealed that a fine-tuned LLM was capable of generating 141 valid proofs for a basic lemma, highlighting the dataset's utility in facilitating the discovery of diverse and valid proof strategies. This paper discusses the dataset's composition, the methodology behind its creation, and the implications of our findings for the future of machine learning in formal verification. The dataset is accessible for further research and exploration: https://huggingface.co/datasets/florath/coq-facts-props-proofs-gen0-v1</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12627v1</guid>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas Florath</dc:creator>
    </item>
    <item>
      <title>Small Scale Reflection for the Working Lean User</title>
      <link>https://arxiv.org/abs/2403.12733</link>
      <description>arXiv:2403.12733v1 Announce Type: cross 
Abstract: We present the design and implementation of the Small Scale Reflection proof methodology and tactic language (a.k.a. SSR) for the Lean 4 proof assistant. Like its Coq predecessor SSReflect, our Lean 4 implementation, dubbed LeanSSR, provides powerful rewriting principles and means for effective management of hypotheses in the proof context. Unlike SSReflect for Coq, LeanSSR does not require explicit switching between the logical and symbolic representation of a goal, allowing for even more concise proof scripts that seamlessly combine deduction steps with proofs by computation.
  In this paper, we first provide a gentle introduction to the principles of structuring mechanised proofs using LeanSSR. Next, we show how the native support for metaprogramming in Lean 4 makes it possible to develop LeanSSR entirely within the proof assistant, greatly improving the overall experience of both tactic implementers and proof engineers. Finally, we demonstrate the utility of LeanSSR by conducting two case studies: (a) porting a collection of Coq lemmas about sequences from the widely used Mathematical Components library and (b) reimplementing proofs in the finite set library of Lean's mathlib4. Both case studies show significant reduction in proof sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12733v1</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <category>cs.MS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Vladimir Gladshtein, George P\^irlea, Ilya Sergey</dc:creator>
    </item>
    <item>
      <title>Regularization in Spider-Style Strategy Discovery and Schedule Construction</title>
      <link>https://arxiv.org/abs/2403.12869</link>
      <description>arXiv:2403.12869v1 Announce Type: cross 
Abstract: To achieve the best performance, automatic theorem provers often rely on schedules of diverse proving strategies to be tried out (either sequentially or in parallel) on a given problem. In this paper, we report on a large-scale experiment with discovering strategies for the Vampire prover, targeting the FOF fragment of the TPTP library and constructing a schedule for it, based on the ideas of Andrei Voronkov's system Spider. We examine the process from various angles, discuss the difficulty (or ease) of obtaining a strong Vampire schedule for the CASC competition, and establish how well a schedule can be expected to generalize to unseen problems and what factors influence this property.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12869v1</guid>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filip B\'artek, Karel Chvalovsk\'y, Martin Suda</dc:creator>
    </item>
    <item>
      <title>SCL(FOL) Revisited</title>
      <link>https://arxiv.org/abs/2302.05954</link>
      <description>arXiv:2302.05954v2 Announce Type: replace 
Abstract: This paper presents an up-to-date and refined version of the SCL calculus for first-order logic without equality. The refinement mainly consists of the following two parts: First, we incorporate a stronger notion of regularity into SCL(FOL). Our regularity definition is adapted from the SCL(T) calculus. This adapted definition guarantees non-redundant clause learning during a run of SCL. However, in contrast to the original presentation, it does not require exhaustive propagation. Second, we introduce trail and model bounding to achieve termination guarantees. In previous versions, no termination guarantees about SCL were achieved. Last, we give rigorous proofs for soundness, completeness and clause learning guarantees of SCL(FOL) and put SCL(FOL) into context of existing first-order calculi.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.05954v2</guid>
      <category>cs.LO</category>
      <category>cs.SC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bromberger, Simon Schwarz, Christoph Weidenbach</dc:creator>
    </item>
    <item>
      <title>Completions of Kleene's second model</title>
      <link>https://arxiv.org/abs/2312.14656</link>
      <description>arXiv:2312.14656v2 Announce Type: replace 
Abstract: We investigate completions of partial combinatory algebras (pcas), in particular of Kleene's second model $\mathcal{K}_2$ and generalizations thereof. We consider weak and strong notions of embeddability and completion that have been studied before. By a result of Klop it is known that not every pca has a strong completion. The study of completions of $\mathcal{K}_2$ has as corollaries that weak and strong embeddings are different, and that every countable pca has a weak completion. We then consider generalizations of $\mathcal{K}_2$ for larger cardinals, and use these to show that it is consistent that every pca has a weak completion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14656v2</guid>
      <category>cs.LO</category>
      <category>math.LO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastiaan A. Terwijn</dc:creator>
    </item>
    <item>
      <title>Analyzing Robustness of Angluin's L$^*$ Algorithm in Presence of Noise</title>
      <link>https://arxiv.org/abs/2306.08266</link>
      <description>arXiv:2306.08266v5 Announce Type: replace-cross 
Abstract: Angluin's L$^*$ algorithm learns the minimal deterministic finite automaton (DFA) of a regular language using membership and equivalence queries. Its probabilistic approximatively correct (PAC) version substitutes an equivalence query by numerous random membership queries to get a high level confidence to the answer. Thus it can be applied to any kind of device and may be viewed as an algorithm for synthesizing an automaton abstracting the behavior of the device based on observations. Here we are interested on how Angluin's PAC learning algorithm behaves for devices which are obtained from a DFA by introducing some noise. More precisely we study whether Angluin's algorithm reduces the noise and produces a DFA closer to the original one than the noisy device. We propose several ways to introduce the noise: (1) the noisy device inverts the classification of words w.r.t. the DFA with a small probability, (2) the noisy device modifies with a small probability the letters of the word before asking its classification w.r.t. the DFA, (3) the noisy device combines the classification of a word w.r.t. the DFA and its classification w.r.t. a counter automaton, and (4) the noisy DFA is obtained by a random process from two DFA such that the language of the first one is included in the second one. Then when a word is accepted (resp. rejected) by the first (resp. second) one, it is also accepted (resp. rejected) and in the remaining cases, it is accepted with probability 0.5. Our main experimental contributions consist in showing that: (1) Angluin's algorithm behaves well whenever the noisy device is produced by a random process, (2) but poorly with a structured noise, and, that (3) is able to eliminate pathological behaviours specified in a regular way. Theoretically, we show that randomness almost surely yields systems with non-recursively enumerable languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.08266v5</guid>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lina Ye, Igor Khmelnitsky, Serge Haddad, Beno\^it Barbot, Benedikt Bollig, Martin Leucker, Daniel Neider, Rajarshi Roy</dc:creator>
    </item>
    <item>
      <title>Safe Planning through Incremental Decomposition of Signal Temporal Logic Specifications</title>
      <link>https://arxiv.org/abs/2403.10554</link>
      <description>arXiv:2403.10554v2 Announce Type: replace-cross 
Abstract: Trajectory planning is a critical process that enables autonomous systems to safely navigate complex environments. Signal temporal logic (STL) specifications are an effective way to encode complex temporally extended objectives for trajectory planning in cyber-physical systems (CPS). However, planning from these specifications using existing techniques scale exponentially with the number of nested operators and the horizon of specification. Additionally, performance is exacerbated at runtime due to limited computational budgets and compounding modeling errors. Decomposing a complex specification into smaller subtasks and incrementally planning for them can remedy these issues. In this work, we present a way to decompose STL requirements temporally to improve planning efficiency and performance. The key insight in our work is to encode all specifications as a set of reachability and invariance constraints and scheduling these constraints sequentially at runtime. Our proposed technique outperforms the state-of-the-art trajectory synthesis techniques for both linear and non linear dynamical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10554v2</guid>
      <category>eess.SY</category>
      <category>cs.LO</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Parv Kapoor, Eunsuk Kang, Romulo Meira-Goes</dc:creator>
    </item>
  </channel>
</rss>
