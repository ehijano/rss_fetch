<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Nov 2024 03:48:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>AI-Driven Feedback Loops in Digital Technologies: Psychological Impacts on User Behaviour and Well-Being</title>
      <link>https://arxiv.org/abs/2411.09706</link>
      <description>arXiv:2411.09706v1 Announce Type: new 
Abstract: The rapid spread of digital technologies has produced data-driven feedback loops, wearable devices, social media networks, and mobile applications that shape user behavior, motivation, and mental well-being. While these systems encourage self-improvement and the development of healthier habits through real-time feedback, they also create psychological risks such as technostress, addiction, and loss of autonomy. The present study also aims to investigate the positive and negative psychological consequences of feedback mechanisms on users' behaviour and well-being. Employing a descriptive survey method, the study collected data from 200 purposely selected users to assess changes in behaviour, motivation, and mental well-being related to health, social, and lifestyle applications. Results indicate that while feedback mechanisms facilitate goal attainment and social interconnection through streaks and badges, among other components, they also enhance anxiety, mental weariness, and loss of productivity due to actions that are considered feedback-seeking. Furthermore, test subjects reported that their actions are unconsciously shaped by app feedback, often at the expense of personal autonomy, while real-time feedback minimally influences professional or social interactions. The study shows that data-driven feedback loops deliver not only motivational benefits but also psychological challenges. To mitigate these risks, users should establish boundaries regarding their use of technology to prevent burnout and addiction, while developers need to refine feedback mechanisms to reduce cognitive load and foster more inclusive participation. Future research should focus on designing feedback mechanisms that promote well-being without compromising individual freedom or increasing social comparison.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09706v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Anthonette Adanyin</dc:creator>
    </item>
    <item>
      <title>The role of broadband connectivity in achieving Sustainable Development Goals (SDGs)</title>
      <link>https://arxiv.org/abs/2411.09708</link>
      <description>arXiv:2411.09708v1 Announce Type: new 
Abstract: Broadband connectivity is a tool for catalyzing socio-economic development and reducing the societal inequalities. Recent studies have investigated the supporting role of broadband in addressing Sustainable Development Goals (SDGs). Relationally, emerging ultra-dense broadband networks such as 5/6G have been linked to increased power consumption and more carbon footprint. With SDGs recognized as interdependent and addressing one should not jeopardize the achievement of the other, there is need for sustainability research. Despite the need to narrow the digital divide and address the SDGs by 2030, it is surprising that limited comprehensive studies exist on broadband sustainability. To this end, we review 113 peer reviewed journals focusing on six key areas (SDGs addressed, application areas, country income, technology, methodology and spatial focus). We further discuss our findings before making four key recommendations on broadband sustainability research to fast-track SDG achievement by 2030 especially for developing economies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09708v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Osoro B. Ogutu, Edward J. Oughton</dc:creator>
    </item>
    <item>
      <title>An IoT Based Smart Waste Management System for the Municipality or City Corporations</title>
      <link>https://arxiv.org/abs/2411.09710</link>
      <description>arXiv:2411.09710v1 Announce Type: new 
Abstract: The population of the urban areas is increasing daily, and this migration is causing serious environmental pollution. A larger population is creating pressure on the municipality's waste management and the city corporations of developing countries such as Bangladesh, further threatening human health. New generation technologies, such as the Internet of Things (IoT)-based waste management systems, can help improve this serious issue. IoT-enabled smart dustbins and mobile applications-based interactive management can effectively solve this problem. In this article, we combine these two technologies to offer an acceptable solution to this problem. The proposed waste management model enables smart dustbins to communicate with waste collectors or waste control centers whenever it is necessary. Additionally, city dwellers can use mobile applications to report their observations in their neighborhoods. As a result, both sensors and humans are involved directly in the development loop. We have conducted a detailed survey to study the acceptance of such a system in the community and received some encouraging results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09710v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laboni Paul, Rahul Deb Mohalder, Kazi Masudul Alam</dc:creator>
    </item>
    <item>
      <title>Environmental Burden of United States Data Centers in the Artificial Intelligence Era</title>
      <link>https://arxiv.org/abs/2411.09786</link>
      <description>arXiv:2411.09786v1 Announce Type: new 
Abstract: The rapid proliferation of data centers in the US - driven partly by the adoption of artificial intelligence - has set off alarm bells about the industry's environmental impact. We compiled detailed information on 2,132 US data centers operating between September 2023 and August 2024 and determined their electricity consumption, electricity sources, and attributable CO$_{2}$e emissions. Our findings reveal that data centers accounted for more than 4% of total US electricity consumption - with 56% derived from fossil fuels - generating more than 105 million tons of CO$_{2}$e (2.18% of US emissions in 2023). Data centers' carbon intensity - the amount of CO$_{2}$e emitted per unit of electricity consumed - exceeded the US average by 48%. Our data pipeline and visualization tools can be used to assess current and future environmental impacts of data centers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09786v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gianluca Guidi, Francesca Dominici, Jonathan Gilmour, Kevin Butler, Eric Bell, Scott Delaney, Falco J. Bargagli-Stoffi</dc:creator>
    </item>
    <item>
      <title>Hollywood's misrepresentation of death: A comparison of overall and by-gender mortality causes in film and the real world</title>
      <link>https://arxiv.org/abs/2411.10040</link>
      <description>arXiv:2411.10040v1 Announce Type: new 
Abstract: The common phrase 'representation matters' asserts that media has a measurable and important impact on civic society's perception of self and others. The representation of health in media, in particular, may reflect and perpetuate a society's disease burden. Here, for the top 10 major causes of death in the United States, we examine how cinematic representation of overall and by-gender mortality diverges from reality. Using crowd-sourced data on film deaths from Cinemorgue Wiki, we employ natural language processing (NLP) techniques to analyze shifts in representation of deaths in movies versus the 2021 National Vital Statistic Survey (NVSS) top ten mortality causes. Overall, movies strongly overrepresent suicide and, to a lesser degree, accidents. In terms of gender, movies overrepresent men and underrepresent women for nearly every major mortality cause, including heart disease and cerebrovascular disease. The two exceptions for which women are overrepresented are suicide and accidents. We discuss the implications of under- and over-representing causes of death overall and by gender, as well as areas of future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10040v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Calla Beauregard, Christopher M. Danforth, Peter Sheridan Dodds</dc:creator>
    </item>
    <item>
      <title>First Steps towards K-12 Computer Science Education in Portugal -- Experience Report</title>
      <link>https://arxiv.org/abs/2411.10142</link>
      <description>arXiv:2411.10142v1 Announce Type: new 
Abstract: Computer scientists Jeannette Wing and Simon Peyton Jones have catalyzed a pivotal discussion on the need to introduce computing in K-12 mandatory education. In Wing's own words, computing 'represents a universally applicable attitude and skill set everyone, not just computer scientists, would be eager to learn and use.'' The crux of this educational endeavor lies in its execution. This paper reports on the efforts of the ENSICO association to implement such aims in Portugal. Starting with pilot projects in a few schools in 2020, it is currently working with 4500 students, 35 schools and 100 school teachers. The main aim is to gain enough experience and knowledge to eventually define a comprehensive syllabus for teaching computing as a mandatory subject throughout the basic and secondary levels of the Portuguese educational system. A structured framework for integrating computational thinking into K-12 education is proposed, with a particular emphasis on mathematical modeling and the functional programming paradigm. This approach is chosen for its potential to promote analytical and problem-solving skills of computational thinking aligned with the core background on maths and science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10142v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fernando Luis Neves, Jose Nuno Oliveira</dc:creator>
    </item>
    <item>
      <title>Virtual Reality in Teacher Education: Insights from Pre-Service Teachers in Resource-limited Regions</title>
      <link>https://arxiv.org/abs/2411.10225</link>
      <description>arXiv:2411.10225v2 Announce Type: new 
Abstract: This study explores the perceptions, challenges, and opportunities associated with using Virtual Reality (VR) as a tool in teacher education among pre-service teachers in a resource-limited setting. Utilizing a qualitative case study design, the study draws on the experiences and reflections of 36 Ghanaian pre-service teachers who engaged with VR in a facilitated lesson for the first time. Findings reveal that initial exposure to VR generated a positive perception, with participants highlighting VR's potential as an engaging and interactive tool that can support experiential learning. Notably, many participants saw the VR-facilitated lesson as a promising alternative to synchronous online learning, particularly for its ability to simulate in-person presentations. They believe VR's immersive capabilities could enhance both teacher preparation and learner engagement in ways that traditional teaching often does not, especially noting that VR has the potential of addressing expensive educational field trips. Despite these promising perceptions, participants identified key challenges, including limited infrastructure, unreliable internet connectivity, and insufficient access to VR equipment as perceived challenges that might hinder the integration of VR in a resource-limited region like Ghana. These findings offer significant implications for educational policymakers and institutions aiming to leverage VR to enhance teacher training and professional development in similar contexts to consider addressing the perceived challenges for successful VR integration in education. We recommend further empirical research be conducted involving pre-service teachers use of VR in their classrooms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10225v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Nyaaba, Bismark Nyaaba Akanzire, Macharious Nabang</dc:creator>
    </item>
    <item>
      <title>AI-Driven Human-Autonomy Teaming in Tactical Operations: Proposed Framework, Challenges, and Future Directions</title>
      <link>https://arxiv.org/abs/2411.09788</link>
      <description>arXiv:2411.09788v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) techniques, particularly machine learning techniques, are rapidly transforming tactical operations by augmenting human decision-making capabilities. This paper explores AI-driven Human-Autonomy Teaming (HAT) as a transformative approach, focusing on how it empowers human decision-making in complex environments. While trust and explainability continue to pose significant challenges, our exploration focuses on the potential of AI-driven HAT to transform tactical operations. By improving situational awareness and supporting more informed decision-making, AI-driven HAT can enhance the effectiveness and safety of such operations. To this end, we propose a comprehensive framework that addresses the key components of AI-driven HAT, including trust and transparency, optimal function allocation between humans and AI, situational awareness, and ethical considerations. The proposed framework can serve as a foundation for future research and development in the field. By identifying and discussing critical research challenges and knowledge gaps in this framework, our work aims to guide the advancement of AI-driven HAT for optimizing tactical operations. We emphasize the importance of developing scalable and ethical AI-driven HAT systems that ensure seamless human-machine collaboration, prioritize ethical considerations, enhance model transparency through Explainable AI (XAI) techniques, and effectively manage the cognitive load of human operators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09788v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Desta Haileselassie Hagos, Hassan El Alami, Danda B. Rawat</dc:creator>
    </item>
    <item>
      <title>InvestESG: A multi-agent reinforcement learning benchmark for studying climate investment as a social dilemma</title>
      <link>https://arxiv.org/abs/2411.09856</link>
      <description>arXiv:2411.09856v1 Announce Type: cross 
Abstract: InvestESG is a novel multi-agent reinforcement learning (MARL) benchmark designed to study the impact of Environmental, Social, and Governance (ESG) disclosure mandates on corporate climate investments. Supported by both PyTorch and GPU-accelerated JAX framework, the benchmark models an intertemporal social dilemma where companies balance short-term profit losses from climate mitigation efforts and long-term benefits from reducing climate risk, while ESG-conscious investors attempt to influence corporate behavior through their investment decisions. Companies allocate capital across mitigation, greenwashing, and resilience, with varying strategies influencing climate outcomes and investor preferences. Our experiments show that without ESG-conscious investors with sufficient capital, corporate mitigation efforts remain limited under the disclosure mandate. However, when a critical mass of investors prioritizes ESG, corporate cooperation increases, which in turn reduces climate risks and enhances long-term financial stability. Additionally, providing more information about global climate risks encourages companies to invest more in mitigation, even without investor involvement. Our findings align with empirical research using real-world data, highlighting MARL's potential to inform policy by providing insights into large-scale socio-economic challenges through efficient testing of alternative policy and market designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09856v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoxuan Hou, Jiayi Yuan, Joel Z. Leibo, Natasha Jaques</dc:creator>
    </item>
    <item>
      <title>Understanding The Effect Of Temperature On Alignment With Human Opinions</title>
      <link>https://arxiv.org/abs/2411.10080</link>
      <description>arXiv:2411.10080v1 Announce Type: cross 
Abstract: With the increasing capabilities of LLMs, recent studies focus on understanding whose opinions are represented by them and how to effectively extract aligned opinion distributions. We conducted an empirical analysis of three straightforward methods for obtaining distributions and evaluated the results across a variety of metrics. Our findings suggest that sampling and log-probability approaches with simple parameter adjustments can return better aligned outputs in subjective tasks compared to direct prompting. Yet, assuming models reflect human opinions may be limiting, highlighting the need for further research on how human subjectivity affects model uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10080v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maja Pavlovic, Massimo Poesio</dc:creator>
    </item>
    <item>
      <title>Static network structure cannot stabilize cooperation among Large Language Model agents</title>
      <link>https://arxiv.org/abs/2411.10294</link>
      <description>arXiv:2411.10294v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used to model human social behavior, with recent research exploring their ability to simulate social dynamics. Here, we test whether LLMs mirror human behavior in social dilemmas, where individual and collective interests conflict. Humans generally cooperate more than expected in laboratory settings, showing less cooperation in well-mixed populations but more in fixed networks. In contrast, LLMs tend to exhibit greater cooperation in well-mixed settings. This raises a key question: Are LLMs about to emulate human behavior in cooperative dilemmas on networks? In this study, we examine networked interactions where agents repeatedly engage in the Prisoner's Dilemma within both well-mixed and structured network configurations, aiming to identify parallels in cooperative behavior between LLMs and humans. Our findings indicate critical distinctions: while humans tend to cooperate more within structured networks, LLMs display increased cooperation mainly in well-mixed environments, with limited adjustment to networked contexts. Notably, LLM cooperation also varies across model types, illustrating the complexities of replicating human-like social adaptability in artificial agents. These results highlight a crucial gap: LLMs struggle to emulate the nuanced, adaptive social strategies humans deploy in fixed networks. Unlike human participants, LLMs do not alter their cooperative behavior in response to network structures or evolving social contexts, missing the reciprocity norms that humans adaptively employ. This limitation points to a fundamental need in future LLM design -- to integrate a deeper comprehension of social norms, enabling more authentic modeling of human-like cooperation and adaptability in networked environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10294v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jin Han, Balaraju Battu, Ivan Romi\'c, Talal Rahwan, Petter Holme</dc:creator>
    </item>
    <item>
      <title>Exploring the Future Metaverse: Research Models for User Experience, Business Readiness, and National Competitiveness</title>
      <link>https://arxiv.org/abs/2411.10408</link>
      <description>arXiv:2411.10408v1 Announce Type: cross 
Abstract: This systematic literature review paper explores perspectives on the ideal metaverse from user experience, business, and national levels, considering both academic and industry viewpoints. The study examines the metaverse as a sociotechnical imaginary, enabled collectively by virtual reality (VR), augmented reality (AR), and mixed reality (MR) technologies. Through a systematic literature review, n=144 records were included and by employing grounded theory for analysis of data, we developed three research models, which can guide researchers in examining the metaverse as a sociotechnical future of information technology. Designers can apply the metaverse user experience maturity model to develop more user-friendly services, while business strategists can use the metaverse business readiness model to assess their firms' current state and prepare for transformation. Additionally, policymakers and policy analysts can utilize the metaverse national competitiveness model to track their countries' competitiveness during this paradigm shift. The synthesis of the results also led to the development of practical assessment tools derived from these models that can guide researchers</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10408v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amir Reza Asadi, Shiva Ghasemi</dc:creator>
    </item>
    <item>
      <title>The Silicon Ceiling: Auditing GPT's Race and Gender Biases in Hiring</title>
      <link>https://arxiv.org/abs/2405.04412</link>
      <description>arXiv:2405.04412v3 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly being introduced in workplace settings, with the goals of improving efficiency and fairness. However, concerns have arisen regarding these models' potential to reflect or exacerbate social biases and stereotypes. This study explores the potential impact of LLMs on hiring practices. To do so, we conduct an AI audit of race and gender biases in one commonly-used LLM, OpenAI's GPT-3.5, taking inspiration from the history of traditional offline resume audits. We conduct two studies using names with varied race and gender connotations: resume assessment (Study 1) and resume generation (Study 2). In Study 1, we ask GPT to score resumes with 32 different names (4 names for each combination of the 2 gender and 4 racial groups) and two anonymous options across 10 occupations and 3 evaluation tasks (overall rating, willingness to interview, and hireability). We find that the model reflects some biases based on stereotypes. In Study 2, we prompt GPT to create resumes (10 for each name) for fictitious job candidates. When generating resumes, GPT reveals underlying biases; women's resumes had occupations with less experience, while Asian and Hispanic resumes had immigrant markers, such as non-native English and non-U.S. education and work experiences. Our findings contribute to a growing body of literature on LLM biases, particularly in workplace contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04412v3</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3689904.3694699</arxiv:DOI>
      <dc:creator>Lena Armstrong, Abbey Liu, Stephen MacNeil, Dana\"e Metaxa</dc:creator>
    </item>
    <item>
      <title>GPT-4V Cannot Generate Radiology Reports Yet</title>
      <link>https://arxiv.org/abs/2407.12176</link>
      <description>arXiv:2407.12176v4 Announce Type: replace 
Abstract: GPT-4V's purported strong multimodal abilities raise interests in using it to automate radiology report writing, but there lacks thorough evaluations. In this work, we perform a systematic evaluation of GPT-4V in generating radiology reports on two chest X-ray report datasets: MIMIC-CXR and IU X-Ray. We attempt to directly generate reports using GPT-4V through different prompting strategies and find that it fails terribly in both lexical metrics and clinical efficacy metrics. To understand the low performance, we decompose the task into two steps: 1) the medical image reasoning step of predicting medical condition labels from images; and 2) the report synthesis step of generating reports from (groundtruth) conditions. We show that GPT-4V's performance in image reasoning is consistently low across different prompts. In fact, the distributions of model-predicted labels remain constant regardless of which groundtruth conditions are present on the image, suggesting that the model is not interpreting chest X-rays meaningfully. Even when given groundtruth conditions in report synthesis, its generated reports are less correct and less natural-sounding than a finetuned LLaMA-2. Altogether, our findings cast doubt on the viability of using GPT-4V in a radiology workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12176v4</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuyang Jiang, Chacha Chen, Dang Nguyen, Benjamin M. Mervak, Chenhao Tan</dc:creator>
    </item>
    <item>
      <title>Catalog of General Ethical Requirements for AI Certification</title>
      <link>https://arxiv.org/abs/2408.12289</link>
      <description>arXiv:2408.12289v2 Announce Type: replace 
Abstract: This whitepaper offers normative and practical guidance for developers of artificial intelligence (AI) systems to achieve "Trustworthy AI". In it, we present overall ethical requirements and six ethical principles with value-specific recommendations for tools to implement these principles into technology. Our value-specific recommendations address the principles of fairness, privacy and data protection, safety and robustness, sustainability, transparency and explainability and truthfulness. For each principle, we also present examples of criteria for risk assessment and categorization of AI systems and applications in line with the categories of the European Union (EU) AI Act. Our work is aimed at stakeholders who can take it as a potential blueprint to fulfill minimum ethical requirements for trustworthy AI and AI Certification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12289v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nicholas Kluge Corr\^ea, Julia Maria M\"onig</dc:creator>
    </item>
    <item>
      <title>Disclosure of AI-Generated News Increases Engagement but Does Not Reduce Aversion, Despite Positive Quality Ratings</title>
      <link>https://arxiv.org/abs/2409.03500</link>
      <description>arXiv:2409.03500v2 Announce Type: replace 
Abstract: The advancement of artificial intelligence (AI) has led to its application in many areas, including news media. The integration of AI in journalism presents both opportunities and risks for democracy, making it crucial to understand public reception of and engagement with AI-generated news, as it may directly influence political knowledge and trust. This preregistered study investigates (i) the perceived quality of AI-assisted and AI-generated versus human-generated news articles, (ii) whether disclosure of AI's involvement in generating these news articles influences engagement with them, and (iii) whether such awareness affects the willingness to read AI-generated articles in the future. We employed a between-subjects survey experiment with 599 participants from the German-speaking part of Switzerland, who evaluated the credibility, readability, and expertise of news articles. These articles were either written by journalists (control group), rewritten by AI (AI-assisted group), or entirely generated by AI (AI-generated group). Our results indicate that all news articles, regardless of whether they were written by journalists or AI, were perceived to be of equal quality. When participants in the treatment groups were subsequently made aware of AI's involvement in generating the articles, they expressed a higher willingness to engage with (i.e., continue reading) the articles than participants in the control group. However, they were not more willing to read AI-generated news in the future. These results suggest that aversion to AI usage in news media is not primarily rooted in a perceived lack of quality, and that by disclosing using AI, journalists could attract more immediate engagement with their content, at least in the short term.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03500v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabrizio Gilardi, Sabrina Di Lorenzo, Juri Ezzaini, Beryl Santa, Benjamin Streiff, Eric Zurfluh, Emma Hoes</dc:creator>
    </item>
    <item>
      <title>Monitoring Human Dependence On AI Systems With Reliance Drills</title>
      <link>https://arxiv.org/abs/2409.14055</link>
      <description>arXiv:2409.14055v4 Announce Type: replace 
Abstract: AI systems are assisting humans with increasingly diverse intellectual tasks but are still prone to mistakes. Humans are over-reliant on this assistance if they trust AI-generated advice, even though they would make a better decision on their own. To identify such instances of over-reliance, this paper proposes the reliance drill: an exercise that tests whether a human can recognise mistakes in AI-generated advice. Our paper examines the reasons why an organisation might choose to implement reliance drills and the doubts they may have about doing so. As an example, we consider the benefits and risks that could arise when using these drills to detect over-reliance on AI in healthcare professionals. We conclude by arguing that reliance drills should become a standard risk management practice for ensuring humans remain appropriately involved in the oversight of AI-assisted decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14055v4</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rosco Hunter, Richard Moulange, Jamie Bernardi, Merlin Stein</dc:creator>
    </item>
    <item>
      <title>Risk Sources and Risk Management Measures in Support of Standards for General-Purpose AI Systems</title>
      <link>https://arxiv.org/abs/2410.23472</link>
      <description>arXiv:2410.23472v2 Announce Type: replace 
Abstract: There is an urgent need to identify both short and long-term risks from newly emerging types of Artificial Intelligence (AI), as well as available risk management measures. In response, and to support global efforts in regulating AI and writing safety standards, we compile an extensive catalog of risk sources and risk management measures for general-purpose AI (GPAI) systems, complete with descriptions and supporting examples where relevant. This work involves identifying technical, operational, and societal risks across model development, training, and deployment stages, as well as surveying established and experimental methods for managing these risks. To the best of our knowledge, this paper is the first of its kind to provide extensive documentation of both GPAI risk sources and risk management measures that are descriptive, self-contained and neutral with respect to any existing regulatory framework. This work intends to help AI providers, standards experts, researchers, policymakers, and regulators in identifying and mitigating systemic risks from GPAI systems. For this reason, the catalog is released under a public domain license for ease of direct use by stakeholders in AI governance and standards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23472v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rokas Gipi\v{s}kis, Ayrton San Joaquin, Ze Shen Chin, Adrian Regenfu{\ss}, Ariel Gil, Koen Holtman</dc:creator>
    </item>
    <item>
      <title>Temporal Patterns of Multiple Long-Term Conditions in Individuals with Intellectual Disability Living in Wales: An Unsupervised Clustering Approach to Disease Trajectories</title>
      <link>https://arxiv.org/abs/2411.08894</link>
      <description>arXiv:2411.08894v2 Announce Type: replace 
Abstract: Identifying and understanding the co-occurrence of multiple long-term conditions (MLTC) in individuals with intellectual disabilities (ID) is vital for effective healthcare management. These individuals often face earlier onset and higher prevalence of MLTCs, yet specific co-occurrence patterns remain unexplored. This study applies an unsupervised approach to characterise MLTC clusters based on shared disease trajectories using electronic health records (EHRs) from 13069 individuals with ID in Wales (2000-2021). Disease associations and temporal directionality were assessed, followed by spectral clustering to group shared trajectories. The population consisted of 52.3% males and 47.7% females, with an average of 4.5 conditions per patient. Males under 45 formed a single cluster dominated by neurological conditions (32.4%), while males above 45 had three clusters, the largest characterised circulatory (51.8%). Females under 45 formed one cluster with digestive conditions (24.6%) as most prevalent, while those aged 45 and older showed two clusters: one dominated by circulatory (34.1%), and the other by digestive (25.9%) and musculoskeletal (21.9%) system conditions. Mental illness, epilepsy, and reflux were common across groups. These clusters offer insights into disease progression in individuals with ID, informing targeted interventions and personalised healthcare strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08894v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rania Kousovista, Georgina Cosma, Emeka Abakasanga, Ashley Akbari, Francesco Zaccardi, Gyuchan Thomas Jun, Reza Kiani, Satheesh Gangadharan</dc:creator>
    </item>
    <item>
      <title>Provocation: Who benefits from "inclusion" in Generative AI?</title>
      <link>https://arxiv.org/abs/2411.09102</link>
      <description>arXiv:2411.09102v2 Announce Type: replace 
Abstract: The demands for accurate and representative generative AI systems means there is an increased demand on participatory evaluation structures. While these participatory structures are paramount to to ensure non-dominant values, knowledge and material culture are also reflected in AI models and the media they generate, we argue that dominant structures of community participation in AI development and evaluation are not explicit enough about the benefits and harms that members of socially marginalized groups may experience as a result of their participation. Without explicit interrogation of these benefits by AI developers, as a community we may remain blind to the immensity of systemic change that is needed as well. To support this provocation, we present a speculative case study, developed from our own collective experiences as AI researchers. We use this speculative context to itemize the barriers that need to be overcome in order for the proposed benefits to marginalized communities to be realized, and harms mitigated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09102v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samantha Dalal, Siobhan Mackenzie Hall, Nari Johnson</dc:creator>
    </item>
    <item>
      <title>ShaRP: A Novel Feature Importance Framework for Ranking</title>
      <link>https://arxiv.org/abs/2401.16744</link>
      <description>arXiv:2401.16744v3 Announce Type: replace-cross 
Abstract: Algorithmic decisions in critical domains such as hiring, college admissions, and lending are often based on rankings. Because of the impact these decisions have on individuals, organizations, and population groups, there is a need to understand them: to help individuals improve their position in a ranking, design better ranking procedures, and check whether a procedure is legally compliant. In this paper, we present ShaRP - Shapley for Rankings and Preferences - a framework that explains the contributions of features to different aspects of a ranked outcome and is based on Shapley values. Using ShaRP, we show that even when the scoring function used by an algorithmic ranker is known and linear, the feature weights do not correspond to their Shapley value contribution. The contributions instead depend on the feature distributions and the subtle local interactions between the scoring features.
  ShaRP builds on the Quantitative Input Influence framework to compute the contributions of features for multiple - ranking specific - Quantities of Interest, including score, rank, pair-wise preference, and top-k. We show the results of an extensive experimental validation of ShaRP using real and synthetic datasets. We demonstrate that feature importance can be computed efficiently, and that ShaRP compares favorably to several prior local feature importance methods, in terms of both generality and quality of explanations. Among our results, we highlight a case study on the CS Rankings dataset. Contrary to expectation, we find that a strong track record in Systems research is much more important than AI research for placing a CS department among the top-10%.
  ShaRP is available as an open-source library at https://github.com/DataResponsibly/ShaRP and is already used in teaching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16744v3</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Venetia Pliatsika, Joao Fonseca, Kateryna Akhynko, Ivan Shevchenko, Julia Stoyanovich</dc:creator>
    </item>
    <item>
      <title>Networking Systems for Video Anomaly Detection: A Tutorial and Survey</title>
      <link>https://arxiv.org/abs/2405.10347</link>
      <description>arXiv:2405.10347v2 Announce Type: replace-cross 
Abstract: The increasing utilization of surveillance cameras in smart cities, coupled with the surge of online video applications, has heightened concerns regarding public security and privacy protection, which propelled automated Video Anomaly Detection (VAD) into a fundamental research task within the Artificial Intelligence (AI) community. With the advancements in deep learning and edge computing, VAD has made significant progress and advances synergized with emerging applications in smart cities and video internet, which has moved beyond the conventional research scope of algorithm engineering to deployable Networking Systems for VAD (NSVAD), a practical hotspot for intersection exploration in the AI, IoVT, and computing fields. In this article, we delineate the foundational assumptions, learning frameworks, and applicable scenarios of various deep learning-driven VAD routes, offering an exhaustive tutorial for novices in NSVAD. This article elucidates core concepts by reviewing recent advances and typical solutions and aggregating available research resources accessible at https://github.com/fdjingliu/NSVAD. Additionally, we showcase our latest NSVAD research in industrial IoT and smart cities, along with an end-cloud collaborative architecture for deployable NSVAD. Lastly, this article projects future development trends and discusses how the integration of AI and computing technologies can address existing research challenges and promote open opportunities, serving as an insightful guide for prospective researchers and engineers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10347v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Liu, Yang Liu, Jieyu Lin, Jielin Li, Liang Cao, Peng Sun, Bo Hu, Liang Song, Azzedine Boukerche, Victor C. M. Leung</dc:creator>
    </item>
    <item>
      <title>What is Fair? Defining Fairness in Machine Learning for Health</title>
      <link>https://arxiv.org/abs/2406.09307</link>
      <description>arXiv:2406.09307v4 Announce Type: replace-cross 
Abstract: Ensuring that machine learning (ML) models are safe, effective, and equitable across all patient groups is essential for clinical decision-making and for preventing the reinforcement of existing health disparities. This review examines notions of fairness used in ML for health, including a review of why ML models can be unfair and how fairness has been quantified in a wide range of real-world examples. We provide an overview of commonly used fairness metrics and supplement our discussion with a case-study of an openly available electronic health record (EHR) dataset. We also discuss the outlook for future research, highlighting current challenges and opportunities in defining fairness in health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09307v4</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <pubDate>Mon, 18 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jianhui Gao, Benson Chou, Zachary R. McCaw, Hilary Thurston, Paul Varghese, Chuan Hong, Jessica Gronsbell</dc:creator>
    </item>
  </channel>
</rss>
