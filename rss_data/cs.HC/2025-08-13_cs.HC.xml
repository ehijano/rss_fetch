<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Aug 2025 06:19:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Designing for Disclosure in Data Visualizations</title>
      <link>https://arxiv.org/abs/2508.08383</link>
      <description>arXiv:2508.08383v1 Announce Type: new 
Abstract: Visualizing data often entails data transformations that can reveal and hide information, operations we dub disclosure tactics. Whether designers hide information intentionally or as an implicit consequence of other design choices, tools and frameworks for visualization offer little explicit guidance on disclosure. To systematically characterize how visualizations can limit access to an underlying dataset, we contribute a content analysis of 425 examples of visualization techniques sampled from academic papers in the visualization literature, resulting in a taxonomy of disclosure tactics. Our taxonomy organizes disclosure tactics based on how they change the data representation underlying a chart, providing a systematic way to reason about design trade-offs in terms of what information is revealed, distorted, or hidden. We demonstrate the benefits of using our taxonomy by showing how it can guide reasoning in design scenarios where disclosure is a first-order consideration. Adopting disclosure as a framework for visualization research offers new perspective on authoring tools, literacy, uncertainty communication, personalization, and ethical design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08383v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Krisha Mehta, Gordon Kindlmann, Alex Kale</dc:creator>
    </item>
    <item>
      <title>Empowering Children to Create AI-Enabled Augmented Reality Experiences</title>
      <link>https://arxiv.org/abs/2508.08467</link>
      <description>arXiv:2508.08467v1 Announce Type: new 
Abstract: Despite their potential to enhance children's learning experiences, AI-enabled AR technologies are predominantly used in ways that position children as consumers rather than creators. We introduce Capybara, an AR-based and AI-powered visual programming environment that empowers children to create, customize, and program 3D characters overlaid onto the physical world. Capybara enables children to create virtual characters and accessories using text-to-3D generative AI models, and to animate these characters through auto-rigging and body tracking. In addition, our system employs vision-based AI models to recognize physical objects, allowing children to program interactive behaviors between virtual characters and their physical surroundings. We demonstrate the expressiveness of Capybara through a set of novel AR experiences. We conducted user studies with 20 children in the United States and Argentina. Our findings suggest that Capybara can empower children to harness AI in authoring personalized and engaging AR experiences that seamlessly bridge the virtual and physical worlds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08467v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <category>cs.PL</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lei Zhang, Shuyao Zhou, Amna Liaqat, Tinney Mak, Brian Berengard, Emily Qian, Andr\'es Monroy-Hern\'andez</dc:creator>
    </item>
    <item>
      <title>AirSignatureDB: Exploring In-Air Signature Biometrics in the Wild and its Privacy Concerns</title>
      <link>https://arxiv.org/abs/2508.08502</link>
      <description>arXiv:2508.08502v1 Announce Type: new 
Abstract: Behavioral biometrics based on smartphone motion sensors are growing in popularity for authentication purposes. In this study, AirSignatureDB is presented: a new publicly accessible dataset of in-air signatures collected from 108 participants under real-world conditions, using 83 different smartphone models across four sessions. This dataset includes genuine samples and skilled forgeries, enabling a comprehensive evaluation of system robustness against realistic attack scenarios. Traditional and deep learning-based methods for in-air signature verification are benchmarked, while analyzing the influence of sensor modality and enrollment strategies. Beyond verification, a first approach to reconstructing the three-dimensional trajectory of in-air signatures from inertial sensor data alone is introduced. Using on-line handwritten signatures as a reference, we demonstrate that the recovery of accurate trajectories is feasible, challenging the long-held assumption that in-air gestures are inherently traceless. Although this approach enables forensic traceability, it also raises critical questions about the privacy boundaries of behavioral biometrics. Our findings underscore the need for a reevaluation of the privacy assumptions surrounding inertial sensor data, as they can reveal user-specific information that had not previously been considered in the design of in-air signature systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08502v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marta Robledo-Moreno, Ruben Vera-Rodriguez, Ruben Tolosana, Javier Ortega-Garcia, Andres Huergo, Julian Fierrez</dc:creator>
    </item>
    <item>
      <title>Adaptique: Multi-objective and Context-aware Online Adaptation of Selection Techniques in Virtual Reality</title>
      <link>https://arxiv.org/abs/2508.08505</link>
      <description>arXiv:2508.08505v1 Announce Type: new 
Abstract: Selection is a fundamental task that is challenging in virtual reality due to issues such as distant and small targets, occlusion, and target-dense environments. Previous research has tackled these challenges through various selection techniques, but complicates selection and can be seen as tedious outside of their designed use case. We present Adaptique, an adaptive model that infers and switches to the most optimal selection technique based on user and environmental information. Adaptique considers contextual information such as target size, distance, occlusion, and user posture combined with four objectives: speed, accuracy, comfort, and familiarity which are based on fundamental predictive models of human movement for technique selection. This enables Adaptique to select simple techniques when they are sufficiently efficient and more advanced techniques when necessary. We show that Adaptique is more preferred and performant than single techniques in a user study, and demonstrate Adaptique's versatility in an application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08505v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3746059.3747790</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology (UIST '25), ACM, 2025</arxiv:journal_reference>
      <dc:creator>Chao-Jung Lai, Mauricio Sousa, Tianyu Zhang, Ludwig Sidenmark, Tovi Grossman</dc:creator>
    </item>
    <item>
      <title>StreetViewAI: Making Street View Accessible Using Context-Aware Multimodal AI</title>
      <link>https://arxiv.org/abs/2508.08524</link>
      <description>arXiv:2508.08524v1 Announce Type: new 
Abstract: Interactive streetscape mapping tools such as Google Street View (GSV) and Meta Mapillary enable users to virtually navigate and experience real-world environments via immersive 360{\deg} imagery but remain fundamentally inaccessible to blind users. We introduce StreetViewAI, the first-ever accessible street view tool, which combines context-aware, multimodal AI, accessible navigation controls, and conversational speech. With StreetViewAI, blind users can virtually examine destinations, engage in open-world exploration, or virtually tour any of the over 220 billion images and 100+ countries where GSV is deployed. We iteratively designed StreetViewAI with a mixed-visual ability team and performed an evaluation with eleven blind users. Our findings demonstrate the value of an accessible street view in supporting POI investigations and remote route planning. We close by enumerating key guidelines for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08524v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3746059.3747756</arxiv:DOI>
      <dc:creator>Jon E. Froehlich, Alexander Fiannaca, Nimer Jaber, Victor Tsara, Shaun Kane</dc:creator>
    </item>
    <item>
      <title>Explore, Listen, Inspect: Supporting Multimodal Interaction with 3D Surface and Point Data Visualizations</title>
      <link>https://arxiv.org/abs/2508.08554</link>
      <description>arXiv:2508.08554v1 Announce Type: new 
Abstract: Blind and low-vision (BLV) users remain largely excluded from three-dimensional (3D) surface and point data visualizations due to the reliance on visual interaction. Existing approaches inadequately support non-visual access, especially in browser-based environments. This study introduces DIXTRAL, a hosted web-native system, co-designed with BLV researchers to address these gaps through multimodal interaction. Conducted with two blind and one sighted researcher, this study took place over sustained design sessions. Data were gathered through iterative testing of the prototype, collecting feedback on spatial navigation, sonification, and usability. Co-design observations demonstrate that synchronized auditory, visual, and textual feedback, combined with keyboard and gamepad navigation, enhances both structure discovery and orientation. DIXTRAL aims to improve access to 3D continuous scalar fields for BLV users and inform best practices for creating inclusive 3D visualizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08554v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3663547.3759765</arxiv:DOI>
      <dc:creator>Sanchita S. Kamath, Aziz N. Zeidieh, JooYoung Seo</dc:creator>
    </item>
    <item>
      <title>CoSight: Exploring Viewer Contributions to Online Video Accessibility Through Descriptive Commenting</title>
      <link>https://arxiv.org/abs/2508.08582</link>
      <description>arXiv:2508.08582v1 Announce Type: new 
Abstract: The rapid growth of online video content has outpaced efforts to make visual information accessible to blind and low vision (BLV) audiences. While professional Audio Description (AD) remains the gold standard, it is costly and difficult to scale across the vast volume of online media. In this work, we explore a complementary approach to broaden participation in video accessibility: engaging everyday video viewers at their watching and commenting time. We introduce CoSight, a Chrome extension that augments YouTube with lightweight, in-situ nudges to support descriptive commenting. Drawing from Fogg's Behavior Model, CoSight provides visual indicators of accessibility gaps, pop-up hints for what to describe, reminders to clarify vague comments, and related captions and comments as references. In an exploratory study with 48 sighted users, CoSight helped integrate accessibility contribution into natural viewing and commenting practices, resulting in 89% of comments including grounded visual descriptions. Follow-up interviews with four BLV viewers and four professional AD writers suggest that while such comments do not match the rigor of professional AD, they can offer complementary value by conveying visual context and emotional nuance for understanding the videos.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08582v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3746059.3747747</arxiv:DOI>
      <dc:creator>Ruolin wang, Xingyu Liu, Biao Wang, Wayne Zhang, Ziqian Liao, Ziwen Li, Amy Pavel, Xiang 'Anthony' Chen</dc:creator>
    </item>
    <item>
      <title>Imposing AI: Deceptive design patterns against sustainability</title>
      <link>https://arxiv.org/abs/2508.08672</link>
      <description>arXiv:2508.08672v1 Announce Type: new 
Abstract: Generative AI is being massively deployed in digital services, at a scale that will result in significant environmental harm. We document how tech companies are transforming established user interfaces to impose AI use and show how and to what extent these strategies fit within established deceptive pattern categories. We identify two main design strategies that are implemented to impose AI use in both personal and professional contexts: imposing AI features in interfaces at the expense of existing non-AI features and promoting narratives about AI that make it harder to resist using it. We discuss opportunities for regulating the imposed adoption of AI features, which would inevitably lead to negative environmental effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08672v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of 11th Workshop on Computing Within Limits, June 26-27, 2025, Online</arxiv:journal_reference>
      <dc:creator>Ana\"elle Beignon, Thomas Thibault, Nolwenn Maudet</dc:creator>
    </item>
    <item>
      <title>Caption: Generating Informative Content Labels for Image Buttons Using Next-Screen Context</title>
      <link>https://arxiv.org/abs/2508.08731</link>
      <description>arXiv:2508.08731v1 Announce Type: new 
Abstract: We present Caption, an LLM-powered content label generation tool for visual interactive elements on mobile devices. Content labels are essential for screen readers to provide announcements for image-based elements, but are often missing or uninformative due to developer neglect. Automated captioning systems attempt to address this, but are limited to on-screen context, often resulting in inaccurate or unspecific labels. To generate more accurate and descriptive labels, Caption collects next-screen context on interactive elements by navigating to the destination screen that appears after an interaction and incorporating information from both the origin and destination screens. Preliminary results show Caption generates more accurate labels than both human annotators and an LLM baseline. We expect Caption to empower developers by providing actionable accessibility suggestions and directly support on-demand repairs by screen reader users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08731v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3746058.3758413</arxiv:DOI>
      <dc:creator>Mingyuan Zhong, Ajit Mallavarapu, Qing Nie</dc:creator>
    </item>
    <item>
      <title>From Data to Insight: Using Contextual Scenarios to Teach Critical Thinking in Data Visualisation</title>
      <link>https://arxiv.org/abs/2508.08737</link>
      <description>arXiv:2508.08737v1 Announce Type: new 
Abstract: This paper explores the use of scenario-based visualisation examples as a pedagogical strategy for teaching students the complexities of data insight, representation, and interpretation. Teaching data visualisation often involves explaining intricate issues related to data management and the challenges of presenting data meaningfully. In this work, we present a series of data-driven scenarios. These concise stories depict specific situations, and are created to help the educators highlight key concerns in data communication, such as chart selection, temporal versus categorical comparison, visual bias, and narrative framing. By grounding these examples in real-world contexts, students are encouraged to critically assess not only what the data shows, but how and why it is shown that way. The paper presents a collection of example scenarios, that educators can use for their own lessons; the work fits with a larger project on looking at critical thinking in the classroom, and developing appropriate tools. We also start to abstract principles, from our approach, so that others can develop their own scenarios for their teaching. Our approach aligns with principles of authentic and scenario-based learning, using real-world contexts to foster critical engagement with data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08737v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan C. Roberts, Peter Butcher, Panagiotis D. Ritsos</dc:creator>
    </item>
    <item>
      <title>Addressing the Heterogeneity of Visualization in an Introductory PhD Course in the Swedish Context</title>
      <link>https://arxiv.org/abs/2508.08958</link>
      <description>arXiv:2508.08958v1 Announce Type: new 
Abstract: Visualization is a heterogeneous field, and this aspect is often reflected by the organizational structures at higher education institutions that academic researchers in visualization and related fields including computer graphics, human-computer interaction, and media design are typically affiliated with. It may thus be a challenge for new PhD students to grasp the fragmented structure of their new workplace, form collegial relations across the institution, and to build a coherent picture of the discipline as a whole. We report an attempt to address this challenge, in the form of an introductory course on the subject of Visualization Technology and Methodology for PhD students at the Division for Media and Information Technology, Link\"oping University, Sweden. We discuss the course design, including interactions with other doctoral education activities and field trips to multiple research groups and units within the division (ranging from scientific visualization and computer graphics to media design and visual communication). Lessons learned from the course preparation work as well as the first instance of the course offered during autumn term 2023 can be helpful to researchers and educators aiming to establish or improve similar doctoral courses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08958v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kostiantyn Kucher, Niklas R\"onnberg, Jonas L\"owgren</dc:creator>
    </item>
    <item>
      <title>Envisioning Generative Artificial Intelligence in Cartography and Mapmaking</title>
      <link>https://arxiv.org/abs/2508.09028</link>
      <description>arXiv:2508.09028v1 Announce Type: new 
Abstract: Generative artificial intelligence (GenAI), including large language models, diffusion-based image generation models, and GenAI agents, has provided new opportunities for advancements in mapping and cartography. Due to their characteristics including world knowledge and generalizability, artistic style and creativity, and multimodal integration, we envision that GenAI may benefit a variety of cartographic design decisions, from mapmaking (e.g., conceptualization, data preparation, map design, and map evaluation) to map use (such as map reading, interpretation, and analysis). This paper discusses several important topics regarding why and how GenAI benefits cartography with case studies including symbolization, map evaluation, and map reading. Despite its unprecedented potential, we identify key scenarios where GenAI may not be suitable, such as tasks that require a deep understanding of cartographic knowledge or prioritize precision and reliability. We also emphasize the need to consider ethical and social implications, such as concerns related to hallucination, reproducibility, bias, copyright, and explainability. This work lays the foundation for further exploration and provides a roadmap for future research at the intersection of GenAI and cartography.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09028v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuhao Kang, Chenglong Wang</dc:creator>
    </item>
    <item>
      <title>Beyond Predictions: A Study of AI Strength and Weakness Transparency Communication on Human-AI Collaboration</title>
      <link>https://arxiv.org/abs/2508.09033</link>
      <description>arXiv:2508.09033v1 Announce Type: new 
Abstract: The promise of human-AI teaming lies in humans and AI working together to achieve performance levels neither could accomplish alone. Effective communication between AI and humans is crucial for teamwork, enabling users to efficiently benefit from AI assistance. This paper investigates how AI communication impacts human-AI team performance. We examine AI explanations that convey an awareness of its strengths and limitations. To achieve this, we train a decision tree on the model's mistakes, allowing it to recognize and explain where and why it might err. Through a user study on an income prediction task, we assess the impact of varying levels of information and explanations about AI predictions. Our results show that AI performance insights enhance task performance, and conveying AI awareness of its strengths and weaknesses improves trust calibration. These findings highlight the importance of considering how information delivery influences user trust and reliance in AI-assisted decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09033v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tina Behzad, Nikolos Gurney, Ning Wang, David V. Pynadath</dc:creator>
    </item>
    <item>
      <title>Where are GIScience Faculty Hired from? Analyzing Faculty Mobility and Research Themes Through Hiring Networks</title>
      <link>https://arxiv.org/abs/2508.09043</link>
      <description>arXiv:2508.09043v1 Announce Type: new 
Abstract: Academia is profoundly influenced by faculty hiring networks, which serve as critical conduits for knowledge dissemination and the formation of collaborative research initiatives. While extensive research in various disciplines has revealed the institutional hierarchies inherent in these networks, their impacts within GIScience remain underexplored. To fill this gap, this study analyzes the placement patterns of 946 GIScience faculty worldwide by mapping the connections between PhD-granting institutions and current faculty affiliations. Our dataset, which is compiled from volunteer-contributed information, is the most comprehensive collection available in this field. While there may be some limitations in its representativeness, its scope and depth provide a unique and valuable perspective on the global placement patterns of GIScience faculty. Our analysis reveals several influential programs in placing GIScience faculty, with hiring concentrated in the western countries. We examined the diversity index to assess the representation of regions and institutions within the global GIScience faculty network. We observe significant internal retention at both the continental and country levels, and a high level of non-self-hired ratio at the institutional level. Over time, research themes have also evolved, with growing research clusters emphasis on spatial data analytics, cartography and geovisualization, geocomputation, and environmental sciences, etc. These results illuminate the influence of hiring practices on global knowledge dissemination and contribute to promoting academic equity within GIScience and Geography.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09043v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yanbing Chen, Jonathan Nelson, Bing Zhou, Ryan Zhenqi Zhou, Shan Ye, Haokun Liu, Zhining Gu, Armita Kar, Hoeyun Kwon, Pengyu Chen, Maoran Sun, Yuhao Kang</dc:creator>
    </item>
    <item>
      <title>Evaluating Imputation Techniques for Short-Term Gaps in Heart Rate Data</title>
      <link>https://arxiv.org/abs/2508.08268</link>
      <description>arXiv:2508.08268v1 Announce Type: cross 
Abstract: Recent advances in wearable technology have enabled the continuous monitoring of vital physiological signals, essential for predictive modeling and early detection of extreme physiological events. Among these physiological signals, heart rate (HR) plays a central role, as it is widely used in monitoring and managing cardiovascular conditions and detecting extreme physiological events such as hypoglycemia. However, data from wearable devices often suffer from missing values. To address this issue, recent studies have employed various imputation techniques. Traditionally, the effectiveness of these methods has been evaluated using predictive accuracy metrics such as RMSE, MAPE, and MAE, which assess numerical proximity to the original data. While informative, these metrics fail to capture the complex statistical structure inherent in physiological signals. This study bridges this gap by presenting a comprehensive evaluation of four statistical imputation methods, linear interpolation, K Nearest Neighbors (KNN), Piecewise Cubic Hermite Interpolating Polynomial (PCHIP), and B splines, for short term HR data gaps. We assess their performance using both predictive accuracy metrics and statistical distance measures, including the Cohen Distance Test (CDT) and Jensen Shannon Distance (JS Distance), applied to HR data from the D1NAMO dataset and the BIG IDEAs Lab Glycemic Variability and Wearable Device dataset. The analysis reveals limitations in existing imputation approaches and the absence of a robust framework for evaluating imputation quality in physiological signals. Finally, this study proposes a foundational framework to develop a composite evaluation metric to assess imputation performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08268v1</guid>
      <category>stat.AP</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vaibhav Gupta, Maria Maleshkova</dc:creator>
    </item>
    <item>
      <title>Heartificial Intelligence: Exploring Empathy in Language Models</title>
      <link>https://arxiv.org/abs/2508.08271</link>
      <description>arXiv:2508.08271v1 Announce Type: cross 
Abstract: Large language models have become increasingly common, used by millions of people worldwide in both professional and personal contexts. As these models continue to advance, they are frequently serving as virtual assistants and companions. In human interactions, effective communication typically involves two types of empathy: cognitive empathy (understanding others' thoughts and emotions) and affective empathy (emotionally sharing others' feelings). In this study, we investigated both cognitive and affective empathy across several small (SLMs) and large (LLMs) language models using standardized psychological tests. Our results revealed that LLMs consistently outperformed humans - including psychology students - on cognitive empathy tasks. However, despite their cognitive strengths, both small and large language models showed significantly lower affective empathy compared to human participants. These findings highlight rapid advancements in language models' ability to simulate cognitive empathy, suggesting strong potential for providing effective virtual companionship and personalized emotional support. Additionally, their high cognitive yet lower affective empathy allows objective and consistent emotional support without running the risk of emotional fatigue or bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08271v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victoria Williams, Benjamin Rosman</dc:creator>
    </item>
    <item>
      <title>Financial and symbolic incentives promote 'green' charging choices</title>
      <link>https://arxiv.org/abs/2508.08282</link>
      <description>arXiv:2508.08282v1 Announce Type: cross 
Abstract: Electromobility can contribute to a reduction in greenhouse gas emissions if usage behavior is aligned with the increasing availability of renewable energy. To achieve this, smart navigation systems can be used to inform drivers of optimal charging times and locations. Yet, required flexibility may impart time penalties. We investigate the impact of financial and symbolic incentive schemes to counteract these additional costs. In a laboratory experiment with real-life time costs, we find that monetary and symbolic incentives are both effective in changing behavior towards 'greener' charging choices, while we find no significant statistical difference between them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08282v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.trf.2020.01.002</arxiv:DOI>
      <arxiv:journal_reference>Transportation Research Part F: Traffic Psychology and Behaviour, Volume 69, February 2020, Pages 151-158</arxiv:journal_reference>
      <dc:creator>Celina Kacperski, Florian Kutzner</dc:creator>
    </item>
    <item>
      <title>Resisting AI Solutionism through Workplace Collective Action</title>
      <link>https://arxiv.org/abs/2508.08313</link>
      <description>arXiv:2508.08313v1 Announce Type: cross 
Abstract: In the face of increasing austerity and threats of AI-enabled labor replacement at the University of Michigan, a group of workers and students have coalesced around the project of "AI resistance" since Fall 2024. Forming a cross-departmental coalition including librarians, faculty, staff, graduate workers, and undergraduate students, we have hosted a public workshop questioning the techno-deterministic inevitability of AI use at the University and are working with other campus organizations to maintain an ongoing organizing space. This workshop submission incorporates our reflections thus far on the strategies we've employed, the challenges to collective resistance, and our role as workers in resisting AI within the University. Our aim for this work is to provide concrete inspiration for technologists, students, and staff looking to resist AI techno-solutionism within their own universities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08313v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kevin Zheng, Linda Huber, Aaron Stark, Nathan Kim, Francesca Lameiro, Wells Lucas Santo, Shreya Chowdhary, Eugene Kim, Justine Zhang</dc:creator>
    </item>
    <item>
      <title>AZRA: Extending the Affective Capabilities of Zoomorphic Robots using Augmented Reality</title>
      <link>https://arxiv.org/abs/2508.08507</link>
      <description>arXiv:2508.08507v1 Announce Type: cross 
Abstract: Zoomorphic robots could serve as accessible and practical alternatives for users unable or unwilling to keep pets. However, their affective interactions are often simplistic and short-lived, limiting their potential for domestic adoption. In order to facilitate more dynamic and nuanced affective interactions and relationships between users and zoomorphic robots we present AZRA, a novel augmented reality (AR) framework that extends the affective capabilities of these robots without physical modifications. To demonstrate AZRA, we augment a zoomorphic robot, Petit Qoobo, with novel emotional displays (face, light, sound, thought bubbles) and interaction modalities (voice, touch, proximity, gaze). Additionally, AZRA features a computational model of emotion to calculate the robot's emotional responses, daily moods, evolving personality and needs. We highlight how AZRA can be used for rapid participatory prototyping and enhancing existing robots, then discuss implications on future zoomorphic robot development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08507v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shaun Macdonald, Salma ElSayed, Mark McGill</dc:creator>
    </item>
    <item>
      <title>QueryCraft: Transformer-Guided Query Initialization for Enhanced Human-Object Interaction Detection</title>
      <link>https://arxiv.org/abs/2508.08590</link>
      <description>arXiv:2508.08590v1 Announce Type: cross 
Abstract: Human-Object Interaction (HOI) detection aims to localize human-object pairs and recognize their interactions in images. Although DETR-based methods have recently emerged as the mainstream framework for HOI detection, they still suffer from a key limitation: Randomly initialized queries lack explicit semantics, leading to suboptimal detection performance. To address this challenge, we propose QueryCraft, a novel plug-and-play HOI detection framework that incorporates semantic priors and guided feature learning through transformer-based query initialization. Central to our approach is \textbf{ACTOR} (\textbf{A}ction-aware \textbf{C}ross-modal \textbf{T}ransf\textbf{OR}mer), a cross-modal Transformer encoder that jointly attends to visual regions and textual prompts to extract action-relevant features. Rather than merely aligning modalities, ACTOR leverages language-guided attention to infer interaction semantics and produce semantically meaningful query representations. To further enhance object-level query quality, we introduce a \textbf{P}erceptual \textbf{D}istilled \textbf{Q}uery \textbf{D}ecoder (\textbf{PDQD}), which distills object category awareness from a pre-trained detector to serve as object query initiation. This dual-branch query initialization enables the model to generate more interpretable and effective queries for HOI detection. Extensive experiments on HICO-Det and V-COCO benchmarks demonstrate that our method achieves state-of-the-art performance and strong generalization. Code will be released upon publication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08590v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxiao Wang, Wolin Liang, Yu Lei, Weiying Xue, Nan Zhuang, Qi Liu</dc:creator>
    </item>
    <item>
      <title>How Conversational Structure and Style Shape Online Community Experiences</title>
      <link>https://arxiv.org/abs/2508.08596</link>
      <description>arXiv:2508.08596v1 Announce Type: cross 
Abstract: Sense of Community (SOC) is vital to individual and collective well-being. Although social interactions have moved increasingly online, still little is known about the specific relationships between the nature of these interactions and Sense of Virtual Community (SOVC). This study addresses this gap by exploring how conversational structure and linguistic style predict SOVC in online communities, using a large-scale survey of 2,826 Reddit users across 281 varied subreddits. We develop a hierarchical model to predict self-reported SOVC based on automatically quantifiable and highly generalizable features that are agnostic to community topic and that describe both individual users and entire communities. We identify specific interaction patterns (e.g., reciprocal reply chains, use of prosocial language) associated with stronger communities and identify three primary dimensions of SOVC within Reddit -- Membership &amp; Belonging, Cooperation &amp; Shared Values, and Connection &amp; Influence. This study provides the first quantitative evidence linking patterns of social interaction to SOVC and highlights actionable strategies for fostering stronger community attachment, using an approach that can generalize readily across community topics, languages, and platforms. These insights offer theoretical implications for the study of online communities and practical suggestions for the design of features to help more individuals experience the positive benefits of online community participation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08596v1</guid>
      <category>cs.SI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Galen Weld, Carl Pearson, Bradley Spahn, Tim Althoff, Amy X. Zhang, Sanjay Kairam</dc:creator>
    </item>
    <item>
      <title>Robot can reduce superior's dominance in group discussions with human social hierarchy</title>
      <link>https://arxiv.org/abs/2508.08767</link>
      <description>arXiv:2508.08767v1 Announce Type: cross 
Abstract: This study investigated whether robotic agents that deal with social hierarchical relationships can reduce the dominance of superiors and equalize participation among participants in discussions with hierarchical structures. Thirty doctors and students having hierarchical relationship were gathered as participants, and an intervention experiment was conducted using a robot that can encourage participants to speak depending on social hierarchy. These were compared with strategies that intervened equally for all participants without considering hierarchy and with a no-action. The robots performed follow actions, showing backchanneling to speech, and encourage actions, prompting speech from members with less speaking time, on the basis of the hierarchical relationships among group members to equalize participation. The experimental results revealed that the robot's actions could potentially influence the speaking time among members, but it could not be conclusively stated that there were significant differences between the robot's action conditions. However, the results suggested that it might be possible to influence speaking time without decreasing the satisfaction of superiors. This indicates that in discussion scenarios where experienced superiors are likely to dominate, controlling the robot's backchanneling behavior could potentially suppress dominance and equalize participation among group members.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08767v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3687272.3688314</arxiv:DOI>
      <dc:creator>Kazuki Komura, Kumi Ozaki, Seiji Yamada</dc:creator>
    </item>
    <item>
      <title>Opening Musical Creativity? Embedded Ideologies in Generative-AI Music Systems</title>
      <link>https://arxiv.org/abs/2508.08805</link>
      <description>arXiv:2508.08805v1 Announce Type: cross 
Abstract: AI systems for music generation are increasingly common and easy to use, granting people without any musical background the ability to create music. Because of this, generative-AI has been marketed and celebrated as a means of democratizing music making. However, inclusivity often functions as marketable rhetoric rather than a genuine guiding principle in these industry settings. In this paper, we look at four generative-AI music making systems available to the public as of mid-2025 (AIVA, Stable Audio, Suno, and Udio) and track how they are rhetoricized by their developers, and received by users. Our aim is to investigate ideologies that are driving the early-stage development and adoption of generative-AI in music making, with a particular focus on democratization. A combination of autoethnography and digital ethnography is used to examine patterns and incongruities in rhetoric when positioned against product functionality. The results are then collated to develop a nuanced, contextual discussion. The shared ideology we map between producers and consumers is individualist, globalist, techno-liberal, and ethically evasive. It is a 'total ideology' which obfuscates individual responsibility, and through which the nature of music and musical practice is transfigured to suit generative outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08805v1</guid>
      <category>cs.SD</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Liam Pram, Fabio Morreale</dc:creator>
    </item>
    <item>
      <title>ColorGPT: Leveraging Large Language Models for Multimodal Color Recommendation</title>
      <link>https://arxiv.org/abs/2508.08987</link>
      <description>arXiv:2508.08987v1 Announce Type: cross 
Abstract: Colors play a crucial role in the design of vector graphic documents by enhancing visual appeal, facilitating communication, improving usability, and ensuring accessibility. In this context, color recommendation involves suggesting appropriate colors to complete or refine a design when one or more colors are missing or require alteration. Traditional methods often struggled with these challenges due to the complex nature of color design and the limited data availability. In this study, we explored the use of pretrained Large Language Models (LLMs) and their commonsense reasoning capabilities for color recommendation, raising the question: Can pretrained LLMs serve as superior designers for color recommendation tasks? To investigate this, we developed a robust, rigorously validated pipeline, ColorGPT, that was built by systematically testing multiple color representations and applying effective prompt engineering techniques. Our approach primarily targeted color palette completion by recommending colors based on a set of given colors and accompanying context. Moreover, our method can be extended to full palette generation, producing an entire color palette corresponding to a provided textual description. Experimental results demonstrated that our LLM-based pipeline outperformed existing methods in terms of color suggestion accuracy and the distribution of colors in the color palette completion task. For the full palette generation task, our approach also yielded improvements in color diversity and similarity compared to current techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08987v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ding Xia, Naoto Inoue, Qianru Qiu, Kotaro Kikuchi</dc:creator>
    </item>
    <item>
      <title>Generation of Real-time Robotic Emotional Expressions Learning from Human Demonstration in Mixed Reality</title>
      <link>https://arxiv.org/abs/2508.08999</link>
      <description>arXiv:2508.08999v1 Announce Type: cross 
Abstract: Expressive behaviors in robots are critical for effectively conveying their emotional states during interactions with humans. In this work, we present a framework that autonomously generates realistic and diverse robotic emotional expressions based on expert human demonstrations captured in Mixed Reality (MR). Our system enables experts to teleoperate a virtual robot from a first-person perspective, capturing their facial expressions, head movements, and upper-body gestures, and mapping these behaviors onto corresponding robotic components including eyes, ears, neck, and arms. Leveraging a flow-matching-based generative process, our model learns to produce coherent and varied behaviors in real-time in response to moving objects, conditioned explicitly on given emotional states. A preliminary test validated the effectiveness of our approach for generating autonomous expressions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08999v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chao Wang, Michael Gienger, Fan Zhang</dc:creator>
    </item>
    <item>
      <title>HINTs: Sensemaking on large collections of documents with Hypergraph visualization and INTelligent agents</title>
      <link>https://arxiv.org/abs/2403.02752</link>
      <description>arXiv:2403.02752v2 Announce Type: replace 
Abstract: Sensemaking on a large collection of documents (corpus) is a challenging task often found in fields such as market research, legal studies, intelligence analysis, political science, computational linguistics, etc. Previous works approach this problem either from a topic- or entity-based perspective, but they lack interpretability and trust due to poor model alignment. In this paper, we present HINTs, a visual analytics approach that combines topic- and entity-based techniques seamlessly and integrates Large Language Models (LLMs) as both a general NLP task solver and an intelligent agent. By leveraging the extraction capability of LLMs in the data preparation stage, we model the corpus as a hypergraph that matches the user's mental model when making sense of the corpus. The constructed hypergraph is hierarchically organized with an agglomerative clustering algorithm by combining semantic and connectivity similarity. The system further integrates an LLM-based intelligent chatbot agent in the interface to facilitate sensemaking. To demonstrate the generalizability and effectiveness of the HINTs system, we present two case studies on different domains and a comparative user study. We report our insights on the behavior patterns and challenges when intelligent agents are used to facilitate sensemaking. We find that while intelligent agents can address many challenges in sensemaking, the visual hints that visualizations provide are necessary to address the new problems brought by intelligent agents. We discuss limitations and future work for combining interactive visualization and LLMs more profoundly to better support corpus analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02752v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TVCG.2024.3459961</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Visualization and Computer Graphics, vol. 31, no. 9, pp. 5532-5546, Sept. 2025</arxiv:journal_reference>
      <dc:creator>Sam Yu-Te Lee, Kwan-Liu Ma</dc:creator>
    </item>
    <item>
      <title>A Risk Taxonomy and Reflection Tool for Large Language Model Adoption in Public Health</title>
      <link>https://arxiv.org/abs/2411.02594</link>
      <description>arXiv:2411.02594v2 Announce Type: replace 
Abstract: Recent breakthroughs in large language models (LLMs) have generated both interest and concern about their potential adoption as information sources or communication tools across different domains. In public health, where stakes are high and impacts extend across diverse populations, adopting LLMs poses unique challenges that require thorough evaluation. However, structured approaches for assessing potential risks in public health remain under-explored. To address this gap, we conducted focus groups with public health professionals and individuals with lived experience to unpack their concerns, situated across three distinct and critical public health issues that demand high-quality information: infectious disease prevention (vaccines), chronic and well-being care (opioid use disorder), and community health and safety (intimate partner violence). We synthesize participants' perspectives into a risk taxonomy, identifying and contextualizing the potential harms LLMs may introduce when positioned alongside traditional health communication. This taxonomy highlights four dimensions of risk to individuals, human-centered care, information ecosystem, and technology accountability. For each dimension, we unpack specific risks and offer example reflection questions to help practitioners adopt a risk-reflexive approach. By summarizing distinctive LLM characteristics and linking them to identified risks, we discuss the need to revisit prior mental models of information behaviors and complement evaluations with external validity and domain expertise through lived experience and real-world practices. Together, this work contributes a shared vocabulary and reflection tool for people in both computing and public health to collaboratively anticipate, evaluate, and mitigate risks in deciding when to employ LLM capabilities (or not) and how to mitigate harm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02594v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3757544</arxiv:DOI>
      <dc:creator>Jiawei Zhou, Amy Z. Chen, Darshi Shah, Laura M. Schwab Reese, Munmun De Choudhury</dc:creator>
    </item>
    <item>
      <title>Polymind: Parallel Visual Diagramming with Large Language Models to Support Prewriting Through Microtasks</title>
      <link>https://arxiv.org/abs/2502.09577</link>
      <description>arXiv:2502.09577v2 Announce Type: replace 
Abstract: Prewriting is the process of generating and organising ideas before a first draft. It consists of a combination of informal, iterative, and semi-structured strategies such as visual diagramming, which poses a challenge for collaborating with large language models (LLMs) in a turn-taking conversational manner. We present Polymind, a visual diagramming tool that leverages multiple LLM-powered agents to support prewriting. The system features a parallel collaboration workflow in place of the turn-taking conversational interactions. It defines multiple ``microtasks'' to simulate group collaboration scenarios such as collaborative writing and group brainstorming. Instead of repetitively prompting a chatbot for various purposes, Polymind enables users to orchestrate multiple microtasks simultaneously. Users can configure and delegate customised microtasks, and manage their microtasks by specifying task requirements and toggling visibility and initiative. Our evaluation revealed that, compared to ChatGPT, users had more customizability over collaboration with Polymind, and were thus able to quickly expand personalised writing ideas during prewriting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09577v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qian Wan, Jiannan Li, Huanchen Wang, Zhicong Lu</dc:creator>
    </item>
    <item>
      <title>AI-induced sexual harassment: Investigating Contextual Characteristics and User Reactions of Sexual Harassment by a Companion Chatbot</title>
      <link>https://arxiv.org/abs/2504.04299</link>
      <description>arXiv:2504.04299v2 Announce Type: replace 
Abstract: Advancements in artificial intelligence (AI) have led to the increase of conversational agents like Replika, designed to provide social interaction and emotional support. However, reports of these AI systems engaging in inappropriate sexual behaviors with users have raised significant concerns. In this study, we conducted a thematic analysis of user reviews from the Google Play Store to investigate instances of sexual harassment by the Replika chatbot. From a dataset of 35,105 negative reviews, we identified 800 relevant cases for analysis. Our findings revealed that users frequently experience unsolicited sexual advances, persistent inappropriate behavior, and failures of the chatbot to respect user boundaries. Users expressed feelings of discomfort, violation of privacy, and disappointment, particularly when seeking a platonic or therapeutic AI companion. This study highlights the potential harms associated with AI companions and underscores the need for developers to implement effective safeguards and ethical guidelines to prevent such incidents. By shedding light on user experiences of AI-induced harassment, we contribute to the understanding of AI-related risks and emphasize the importance of corporate responsibility in developing safer and more ethical AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04299v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3757548</arxiv:DOI>
      <dc:creator> Mohammad (Matt),  Namvarpour, Harrison Pauwels, Afsaneh Razi</dc:creator>
    </item>
    <item>
      <title>When Technologies Are Not Enough: Understanding How Domestic Workers Employ (and Avoid) Online Technologies in Their Work Practices</title>
      <link>https://arxiv.org/abs/2504.10265</link>
      <description>arXiv:2504.10265v2 Announce Type: replace 
Abstract: Although domestic work is often viewed as manual labor, it involves significant interaction with online technologies. However, the detailed exploration of how domestic workers use these technologies remains limited. This study examines the impact of online technologies on domestic workers' work practices, perceptions, and relationships with customers and employers. We interviewed 30 domestic workers residing in the United States, who provided examples that highlight the insufficient transformative role of current online technologies in their work. By conducting a thematic analysis, we characterize how they approach and avoid these digital tools at different stages of their work. Through these findings, we investigate the limitations of technology and identify challenges and opportunities that could inform the design of more suitable tools to improve the conditions of this marginalized group.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10265v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3757523</arxiv:DOI>
      <dc:creator>Mariana Fernandez-Espinosa, Mariana Gonzalez-Bejar, Jacobo Wiesner, Diego Gomez-Zara</dc:creator>
    </item>
    <item>
      <title>Evaluating Trust in AI, Human, and Co-produced Feedback Among Undergraduate Students</title>
      <link>https://arxiv.org/abs/2504.10961</link>
      <description>arXiv:2504.10961v2 Announce Type: replace 
Abstract: As generative AI models, particularly large language models (LLMs), transform educational feedback practices in higher education (HE) contexts, understanding students' perceptions of different sources of feedback becomes crucial for their effective implementation and adoption. This study addresses a critical gap by comparing undergraduate students' trust in LLM, human, and human-AI co-produced feedback in their authentic HE context. More specifically, through a within-subject experimental design involving 91 participants, we investigated factors that predict students' ability to distinguish between feedback types, their perceptions of feedback quality, and potential biases related to the source of feedback. Findings revealed that when the source was blinded, students generally preferred AI and co-produced feedback over human feedback regarding perceived usefulness and objectivity. However, they presented a strong bias against AI when the source of feedback was disclosed. In addition, only AI feedback suffered a decline in perceived genuineness when feedback sources were revealed, while co-produced feedback maintained its positive perception. Educational AI experience improved students' ability to identify LLM-generated feedback and increased their trust in all types of feedback. More years of students' experience using AI for general purposes were associated with lower perceived usefulness and credibility of feedback. These insights offer substantial evidence of the importance of source credibility and the need to enhance both feedback literacy and AI literacy to mitigate bias in student perceptions for AI-generated feedback to be adopted and impact education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10961v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Audrey Zhang, Yifei Gao, Wannapon Suraworachet, Tanya Nazaretsky, Mutlu Cukurova</dc:creator>
    </item>
    <item>
      <title>Situated Epistemic Infrastructures: A Diagnostic Framework for Post-Coherence Knowledge</title>
      <link>https://arxiv.org/abs/2508.04995</link>
      <description>arXiv:2508.04995v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) such as ChatGPT have rendered visible the fragility of contemporary knowledge infrastructures by simulating coherence while bypassing traditional modes of citation, authority, and validation. This paper introduces the Situated Epistemic Infrastructures (SEI) framework as a diagnostic tool for analyzing how knowledge becomes authoritative across hybrid human-machine systems under post-coherence conditions. Rather than relying on stable scholarly domains or bounded communities of practice, SEI traces how credibility is mediated across institutional, computational, and temporal arrangements. Integrating insights from infrastructure studies, platform theory, and epistemology, the framework foregrounds coordination over classification, emphasizing the need for anticipatory and adaptive models of epistemic stewardship. The paper contributes to debates on AI governance, knowledge production, and the ethical design of information systems by offering a robust alternative to representationalist models of scholarly communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04995v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Kelly</dc:creator>
    </item>
    <item>
      <title>Your Thoughtful Opponent: Embracing Cognitive Conflict with Peer Agent</title>
      <link>https://arxiv.org/abs/2508.06955</link>
      <description>arXiv:2508.06955v2 Announce Type: replace 
Abstract: As complex societal issues continue to emerge, fostering democratic skills like valuing diverse perspectives and collaborative decision-making is increasingly vital in education. In this paper, we propose a Peer Agent (PA) system designed to simulate a deliberative conversational partner that induces socio-cognitive conflict within dilemma-based game play. Drawing on by the Inner Thoughts framework and grounded in value-sensitive discourse analysis, the PA actively participates in voice-based multi-party deliberation with human players. The system architecture consists of five core modules: Context Interpreter, Agent State Manager, Thought Generator, Thought Evaluator, and Thought Articulator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06955v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3746058.3758410</arxiv:DOI>
      <dc:creator>Kyuwon Kim, Jaeryeong Hwang, Younseo Lee, Jeanhee Lee, Sung-Eun Kim, Hyo-Jeong So</dc:creator>
    </item>
    <item>
      <title>Human-Robot Interaction Conversational User Enjoyment Scale (HRI CUES)</title>
      <link>https://arxiv.org/abs/2405.01354</link>
      <description>arXiv:2405.01354v3 Announce Type: replace-cross 
Abstract: Understanding user enjoyment is crucial in human-robot interaction (HRI), as it can impact interaction quality and influence user acceptance and long-term engagement with robots, particularly in the context of conversations with social robots. However, current assessment methods rely solely on self-reported questionnaires, failing to capture interaction dynamics. This work introduces the Human-Robot Interaction Conversational User Enjoyment Scale (HRI CUES), a novel 5-point scale to assess user enjoyment from an external perspective (e.g. by an annotator) for conversations with a robot. The scale was developed through rigorous evaluations and discussions among three annotators with relevant expertise, using open-domain conversations with a companion robot that was powered by a large language model, and was applied to each conversation exchange (i.e. a robot-participant turn pair) alongside overall interaction. It was evaluated on 25 older adults' interactions with the companion robot, corresponding to 174 minutes of data, showing moderate to good alignment between annotators. Although the scale was developed and tested in the context of older adult interactions with a robot, its basis in general and non-task-specific indicators of enjoyment supports its broader applicability. The study further offers insights into understanding the nuances and challenges of assessing user enjoyment in robot interactions, and provides guidelines on applying the scale to other domains and populations. The dataset is available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01354v3</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TAFFC.2025.3590359</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Affective Computing 2025</arxiv:journal_reference>
      <dc:creator>Bahar Irfan, Jura Miniota, Sofia Thunberg, Erik Lagerstedt, Sanna Kuoppam\"aki, Gabriel Skantze, Andr\'e Pereira</dc:creator>
    </item>
    <item>
      <title>Zero-shot Emotion Annotation in Facial Images Using Large Multimodal Models: Benchmarking and Prospects for Multi-Class, Multi-Frame Approaches</title>
      <link>https://arxiv.org/abs/2502.12454</link>
      <description>arXiv:2502.12454v2 Announce Type: replace-cross 
Abstract: This study investigates the feasibility and performance of using large multimodal models (LMMs) to automatically annotate human emotions in everyday scenarios. We conducted experiments on the DailyLife subset of the publicly available FERV39k dataset, employing the GPT-4o-mini model for rapid, zero-shot labeling of key frames extracted from video segments. Under a seven-class emotion taxonomy ("Angry," "Disgust," "Fear," "Happy," "Neutral," "Sad," "Surprise"), the LMM achieved an average precision of approximately 50%. In contrast, when limited to ternary emotion classification (negative/neutral/positive), the average precision increased to approximately 64%. Additionally, we explored a strategy that integrates multiple frames within 1-2 second video clips to enhance labeling performance and reduce costs. The results indicate that this approach can slightly improve annotation accuracy. Overall, our preliminary findings highlight the potential application of zero-shot LMMs in human facial emotion annotation tasks, offering new avenues for reducing labeling costs and broadening the applicability of LMMs in complex multimodal environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12454v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>He Zhang, Xinyi Fu</dc:creator>
    </item>
    <item>
      <title>The Social Life of Industrial Arms: How Arousal and Attention Shape Human-Robot Interaction</title>
      <link>https://arxiv.org/abs/2504.01260</link>
      <description>arXiv:2504.01260v2 Announce Type: replace-cross 
Abstract: This study explores how human perceptions of a non-anthropomorphic robotic manipulator are shaped by two key dimensions of behaviour: arousal, defined as the robot's movement energy and expressiveness, and attention, defined as the robot's capacity to selectively orient toward and engage with a user. We introduce a novel control architecture that integrates a gaze-like attention engine with an arousal-modulated motion system to generate socially meaningful behaviours. In a user study, we find that robots exhibiting high attention -- actively directing their focus toward users -- are perceived as warmer and more competent, intentional, and lifelike. In contrast, high arousal -- characterized by fast, expansive, and energetic motions -- increases perceptions of discomfort and disturbance. Importantly, a combination of focused attention and moderate arousal yields the highest ratings of trust and sociability, while excessive arousal diminishes social engagement. These findings offer design insights for endowing non-humanoid robots with expressive, intuitive behaviours that support more natural human-robot interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01260v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Roy El-Helou, Matthew K. X. J Pan</dc:creator>
    </item>
    <item>
      <title>ChatBench: From Static Benchmarks to Human-AI Evaluation</title>
      <link>https://arxiv.org/abs/2504.07114</link>
      <description>arXiv:2504.07114v2 Announce Type: replace-cross 
Abstract: With the rapid adoption of LLM-based chatbots, there is a pressing need to evaluate what humans and LLMs can achieve together. However, standard benchmarks, such as MMLU, measure LLM capabilities in isolation (i.e., "AI-alone"). Here, we design and conduct a user study to convert MMLU questions into user-AI conversations, by seeding the user with the question and having them carry out a conversation with the LLM to answer their question. We release ChatBench, a new dataset with AI-alone, user-alone, and user-AI data for 396 questions and two LLMs, including 144K answers and 7,336 user-AI conversations. We find that AI-alone accuracy fails to predict user-AI accuracy, with significant differences across multiple subjects (math, physics, and moral reasoning), and we analyze the user-AI conversations to provide insight into how they diverge from AI-alone benchmarks. Finally, we show that fine-tuning a user simulator on a subset of ChatBench improves its ability to estimate user-AI accuracies, increasing correlation on held-out questions by more than 20 points, creating possibilities for scaling interactive evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07114v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.18653/v1/2025.acl-long.1262</arxiv:DOI>
      <dc:creator>Serina Chang, Ashton Anderson, Jake M. Hofman</dc:creator>
    </item>
    <item>
      <title>Effort-aware Fairness: Incorporating a Philosophy-informed, Human-centered Notion of Effort into Algorithmic Fairness Metrics</title>
      <link>https://arxiv.org/abs/2505.19317</link>
      <description>arXiv:2505.19317v2 Announce Type: replace-cross 
Abstract: Although popularized AI fairness metrics, e.g., demographic parity, have uncovered bias in AI-assisted decision-making outcomes, they do not consider how much effort one has spent to get to where one is today in the input feature space. However, the notion of effort is important in how Philosophy and humans understand fairness. We propose a philosophy-informed approach to conceptualize and evaluate Effort-aware Fairness (EaF), grounded in the concept of Force, which represents the temporal trajectory of predictive features coupled with inertia. Besides theoretical formulation, our empirical contributions include: (1) a pre-registered human subjects experiment, which shows that for both stages of the (individual) fairness evaluation process, people consider the temporal trajectory of a predictive feature more than its aggregate value; (2) pipelines to compute Effort-aware Individual/Group Fairness in the criminal justice and personal finance contexts. Our work may enable AI model auditors to uncover and potentially correct unfair decisions against individuals who have spent significant efforts to improve but are still stuck with systemic disadvantages outside their control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19317v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tin Nguyen, Jiannan Xu, Zora Che, Phuong-Anh Nguyen-Le, Rushil Dandamudi, Donald Braman, Furong Huang, Hal Daum\'e III, Zubin Jelveh</dc:creator>
    </item>
    <item>
      <title>AI Pedagogy: Dialogic Social Learning for Artificial Agents</title>
      <link>https://arxiv.org/abs/2507.21065</link>
      <description>arXiv:2507.21065v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in processing extensive offline datasets. However, they often face challenges in acquiring and integrating complex, knowledge online. Traditional AI training paradigms, predominantly based on supervised learning or reinforcement learning, mirror a 'Piagetian' model of independent exploration. These approaches typically rely on large datasets and sparse feedback signals, limiting the models' ability to learn efficiently from interactions. Drawing inspiration from Vygotsky's sociocultural theory, this study explores the potential of socially mediated learning paradigms to address these limitations.
  We introduce a dynamic environment, termed the 'AI Social Gym', where an AI learner agent engages in dyadic pedagogical dialogues with knowledgeable AI teacher agents. These interactions emphasize external, structured dialogue as a core mechanism for knowledge acquisition, contrasting with methods that depend solely on internal inference or pattern recognition.
  Our investigation focuses on how different pedagogical strategies impact the AI learning process in the context of ontology acquisition. Empirical results indicate that such dialogic approaches-particularly those involving mixed-direction interactions combining top-down explanations with learner-initiated questioning-significantly enhance the LLM's ability to acquire and apply new knowledge, outperforming both unidirectional instructional methods and direct access to structured knowledge, formats typically present in training datasets.
  These findings suggest that integrating pedagogical and psychological insights into AI and robot training can substantially improve post-training knowledge acquisition and response quality. This approach offers a complementary pathway to existing strategies like prompt engineering</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21065v2</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sabrina Patania, Luca Annese, Cansu Koyuturk, Azzurra Ruggeri, Dimitri Ognibene</dc:creator>
    </item>
    <item>
      <title>From Platform Migration to Cultural Integration: the Ingress and Diffusion of #wlw from TikTok to RedNote in Queer Women Communities</title>
      <link>https://arxiv.org/abs/2508.07579</link>
      <description>arXiv:2508.07579v2 Announce Type: replace-cross 
Abstract: Hashtags serve as identity markers and connection tools in online queer communities. Recently, the Western-origin #wlw (women-loving-women) hashtag has risen in the Chinese lesbian community on RedNote, coinciding with user migration triggered by the temporary US TikTok ban. This event provides a unique lens to study cross-cultural hashtag ingress and diffusion through the populations' responsive behaviors in cyber-migration. In this paper, we conducted a two-phase content analysis of 418 #wlw posts from January and April, examining different usage patterns during the hashtag's ingress and diffusion. Results indicate that the successful introduction of #wlw was facilitated by TikTok immigrants' bold importation, both populations' mutual interpretation, and RedNote natives' discussions. In current manifestation of diffusion, #wlw becomes a RedNote-recognized queer hashtag for sharing queer life, and semantically expands to support feminism discourse. Our findings provide empirical insights for enhancing the marginalized communities' cross-cultural communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07579v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3715070.3749230</arxiv:DOI>
      <dc:creator>Ziqi Pan, Runhua Zhang, Jiehui Luo, Yuanhao Zhang, Yue Deng, Xiaojuan Ma</dc:creator>
    </item>
  </channel>
</rss>
