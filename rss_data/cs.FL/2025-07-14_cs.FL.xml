<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.FL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.FL</link>
    <description>cs.FL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.FL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Jul 2025 02:16:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Family of Sequences Generalizing the Thue Morse and Rudin Shapiro Sequences</title>
      <link>https://arxiv.org/abs/2505.20547</link>
      <description>arXiv:2505.20547v3 Announce Type: replace 
Abstract: For $m \ge 1,$ let $P_m =1^m,$ the binary string of $m$ ones. Further define the infinite sequence $s_m$ by $s_{m,n} = 1$ iff the number of (possibly overlapping) occurrences of $P_m$ in the binary representation of $n$ is odd, $n \ge 0.$ For $m=1,2$ respectively $s_m$ is the Thue-Morse and Rudin-Shapiro sequences. This paper shows that for each $m\ge 1,$ (i) $s_m$ is automatic; (ii) the minimal, DFA (deterministic finite automata) accepting $s_m$ has $2m$ states; (iii) it suffices to use prefixes of length $2^{m-1}$ to distinguish all sequences in the 2-kernel of $s_m$; and (iv) the characteristic function of the length $2^{m-1}$ prefix of the 2-kernel sequences of $s_m$ can be formulated using the Vile and Jacobsthal sequences. The proofs are based on a correspondence between binary strings under concatenation and integers under addition and multiplication. Both Mathematica and Walnut are employed for exploratory analysis of patterns. The paper presents results about maximal runs, palindromes, order of squares, and borders in $s_m,$ generalizing similar results for $s_1$ and $s_2.$ In the conclusion we suggest that families of automatic sequences is a fruitful concept and a useful group of sequences extending automatic sequences similar to the regular and synchronized sequences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20547v3</guid>
      <category>cs.FL</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Russell Jay Hendel</dc:creator>
    </item>
    <item>
      <title>Comparing Spoken Languages using Paninian System of Sounds and Finite State Machines</title>
      <link>https://arxiv.org/abs/2301.12463</link>
      <description>arXiv:2301.12463v3 Announce Type: replace-cross 
Abstract: The study of spoken languages comprises phonology, morphology, and grammar. The languages can be classified as root languages, inflectional languages, and stem languages. In addition, languages continually change over time and space by picking isoglosses, as speakers move from region to/through region. All these factors lead to the formation of vocabulary, which has commonality/similarity across languages as well as distinct and subtle differences among them. Comparison of vocabularies across languages and detailed analysis has led to the hypothesis of language families. In particular, in the view of Western linguists, Vedic Sanskrit is a daughter language, part of the Indo-Iranian branch of the Indo-European Language family, and Dravidian Languages belong to an entirely different family. These and such conclusions are reexamined in this paper. Based on our study and analysis, we propose an Ecosystem Model for Linguistic Development with Sanskrit at the core, in place of the widely accepted family tree model. To that end, we leverage the Paninian system of sounds to construct a phonetic map. Then we represent words across languages as state transitions on the phonetic map and construct corresponding Morphological Finite Automata (MFA) that accept groups of words. Regardless of whether the contribution of this paper is significant or minor, it is an important step in challenging policy-driven research that has plagued this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.12463v3</guid>
      <category>cs.CL</category>
      <category>cs.FL</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shreekanth M Prabhu, Abhisek Midya</dc:creator>
    </item>
    <item>
      <title>Sampling from Your Language Model One Byte at a Time</title>
      <link>https://arxiv.org/abs/2506.14123</link>
      <description>arXiv:2506.14123v2 Announce Type: replace-cross 
Abstract: Tokenization is used almost universally by modern language models, enabling efficient text representation using multi-byte or multi-character tokens. However, prior work has shown that tokenization can introduce distortion into the model's generations, an issue known as the Prompt Boundary Problem (PBP). For example, users are often advised not to end their prompts with a space because it prevents the model from including the space as part of the next token. While this heuristic is effective in English, the underlying PBP continues to affect languages such as Chinese as well as code generation, where tokens often do not line up with word and syntactic boundaries. In this work, we present an inference-time method to convert any autoregressive LM with a BPE tokenizer into a character-level or byte-level LM. Our method efficiently solves the PBP and is also able to unify the vocabularies of language models with different tokenizers, allowing one to ensemble LMs with different tokenizers at inference time or transfer the post-training from one model to another using proxy-tuning. We demonstrate in experiments that the ensemble and proxy-tuned models outperform their constituents on downstream evals. Code is available at https://github.com/SewoongLab/byte-sampler .</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.14123v2</guid>
      <category>cs.CL</category>
      <category>cs.FL</category>
      <category>cs.LG</category>
      <pubDate>Mon, 14 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Hayase, Alisa Liu, Noah A. Smith, Sewoong Oh</dc:creator>
    </item>
  </channel>
</rss>
