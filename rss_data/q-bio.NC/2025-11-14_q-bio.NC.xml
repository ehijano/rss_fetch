<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Nov 2025 05:01:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Brian Intensify: An Adaptive Machine Learning Framework for Auditory EEG Stimulation and Cognitive Enhancement in FXS</title>
      <link>https://arxiv.org/abs/2511.09765</link>
      <description>arXiv:2511.09765v1 Announce Type: new 
Abstract: Neurodevelopmental disorders such as Fragile X Syndrome (FXS) and Autism Spectrum Disorder (ASD) are characterized by disrupted cortical oscillatory activity, particularly in the alpha and gamma frequency bands. These abnormalities are linked to deficits in attention, sensory processing, and cognitive function. In this work, we present an adaptive machine learning-based brain-computer interface (BCI) system designed to modulate neural oscillations through frequency-specific auditory stimulation to enhance cognitive readiness in individuals with FXS. EEG data were recorded from 38 participants using a 128-channel system under a stimulation paradigm consisting of a 30-second baseline (no stimulus) followed by 60-second auditory entrainment episodes at 7Hz, 9Hz, 11Hz, and 13Hz. A comprehensive analysis of power spectral features (Alpha, Gamma, Delta, Theta, Beta) and cross-frequency coupling metrics (Alpha-Gamma, Alpha-Beta, etc.) was conducted. The results identified Peak Alpha Power, Peak Gamma Power, and Alpha Power per second per channel as the most discriminative biomarkers. The 13Hz stimulation condition consistently elicited a significant increase in Alpha activity and suppression of Gamma activity, aligning with our optimization objective. A supervised machine learning framework was developed to predict EEG responses and dynamically adjust stimulation parameters, enabling real-time, subject-specific adaptation. This work establishes a novel EEG-driven optimization framework for cognitive neuromodulation, providing a foundational model for next-generation AI-integrated BCI systems aimed at personalized neurorehabilitation in FXS and related disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09765v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zag ElSayed, Grace Westerkamp, Jack Yanchen Liu, Ernest Pedapati</dc:creator>
    </item>
    <item>
      <title>Imaging the Topology of Dynamic Brain Connectivity</title>
      <link>https://arxiv.org/abs/2511.09949</link>
      <description>arXiv:2511.09949v1 Announce Type: new 
Abstract: Functional brain connectivity changes dynamically over time, making its representation challenging for learning on non-Euclidean data. We present a framework that encodes dynamic functional connectivity as an image representation of evolving network topology. Persistent graph homology summarizes global organization across scales, yielding Wasserstein distance-preserving embeddings stable under resolution changes. Stacking these embeddings forms a topological image that captures temporal reconfiguration of brain networks. This design enables convolutional architectures and transfer learning from pretrained foundational models to operate effectively under limited and imbalanced data. Applied to early Alzheimer's detection, the approach achieves clinically meaningful accuracy, establishing a principled foundation for imaging dynamic brain topology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.09949v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peilin He, Tananun Songdechakraiwut</dc:creator>
    </item>
    <item>
      <title>Representation learning in cerebellum-like structures</title>
      <link>https://arxiv.org/abs/2511.10261</link>
      <description>arXiv:2511.10261v1 Announce Type: new 
Abstract: Animals use past experiences to adapt future behavior. To enable this rapid learning, vertebrates and invertebrates have evolved analogous neural structures like the vertebrate cerebellum or insect mushroom body. A defining feature of these circuits is a large expansion layer, which re-codes sensory inputs to improve pattern separation, a prerequisite to learn non-overlapping associations between relevant sensorimotor inputs and adaptive changes in behavior. However, classical models of associative learning treat expansion layers as static, assuming that associations are learned through plasticity at the output synapses. Here, we review emerging evidence that also highlights the importance of plasticity within the expansion layer for associative learning. Because the underlying plasticity mechanisms and principles of this representation learning are only emerging, we systematically compare experimental data from two well-studied circuits for expansion coding -- the cerebellum granule layer and the mushroom body calyx. The data indicate remarkably similar interneuron circuits, dendritic morphology and plasticity mechanisms between both systems that hint at more general principles for representation learning. Moreover, the data show strong overlap with recent theoretical advances that consider interneuron circuits and dendritic computations for representation learning. However, they also hint at an interesting interaction of stimulus-induced, non-associative and reinforced, associative mechanisms of plasticity that is not well understood in current theories of representation learning. Therefore, studying expansion layer plasticity will be important to elucidate the mechanisms and full potential of representation learning for behavioral adaptation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10261v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Rudelt, Fabian Mikulasch, Viola Priesemann, Andr\'e Ferreira Castro</dc:creator>
    </item>
    <item>
      <title>Theoretical Analysis of Resource-Induced Phase Transitions in Estimation Strategies</title>
      <link>https://arxiv.org/abs/2511.10184</link>
      <description>arXiv:2511.10184v1 Announce Type: cross 
Abstract: Organisms adapt to volatile environments by integrating sensory information with internal memory, yet their information processing is constrained by resource limitations. Such limitations can fundamentally alter optimal estimation strategies in biological systems. For example, recent experiments suggest that organisms exhibit nonmonotonic phase transitions between memoryless and memory-based estimation strategies depending on sensory reliability. However, an analytical understanding of these resource-induced phase transitions is still missing. This Letter presents an analytical characterization of resource-induced phase transitions in optimal estimation strategies. Our result identifies the conditions under which resource limitations alter estimation strategies and analytically reveals the mechanism underlying the emergence of discontinuous, nonmonotonic, and scaling behaviors. These results provide a theoretical foundation for understanding how limited resources shape information processing in biological systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10184v1</guid>
      <category>physics.bio-ph</category>
      <category>math.OC</category>
      <category>q-bio.NC</category>
      <category>q-bio.SC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takehiro Tottori, Tetsuya J. Kobayashi</dc:creator>
    </item>
    <item>
      <title>Shaping manifolds in equivariant recurrent neural networks</title>
      <link>https://arxiv.org/abs/2511.04802</link>
      <description>arXiv:2511.04802v2 Announce Type: replace 
Abstract: Recordings of increasingly large neural populations have revealed that the firing of individual neurons is highly coordinated. When viewed in the space of all possible patterns, the collective activity forms non-linear structures called neural manifolds. Because such structures are observed even at rest or during sleep, an important hypothesis is that activity manifolds may correspond to continuous attractors shaped by recurrent connectivity between neurons. Classical models of recurrent networks have shown that continuous attractors can be generated by specific symmetries in the connectivity. Although a variety of attractor network models have been studied, general principles linking network connectivity and the geometry of attractors remain to be formulated. Here, we address this question by using group representation theory to formalize the relationship between the symmetries in recurrent connectivity and the resulting fixed-point manifolds. We start by revisiting the classical ring model, a continuous attractor network generating a circular manifold. Interpreting its connectivity as a circular convolution, we draw a parallel with feed-forward CNNs. Building on principles of geometric deep learning, we then generalize this architecture to a broad range of symmetries using group representation theory. Specifically, we introduce a new class of equivariant RNNs, where the connectivity is based on group convolution. Using the group Fourier transform, we reduce such networks to low-rank models, giving us a low-dimensional description that can be fully analyzed to determine the symmetry, dimensionality and stability of fixed-point manifolds. Our results underline the importance of stability considerations: for a connectivity with a given symmetry, depending on parameters, several manifolds with different symmetry subgroups can coexist, some stable and others consisting of saddle points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04802v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 14 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arianna Di Bernardo, Adrian Valente, Francesca Mastrogiuseppe, Srdjan Ostojic</dc:creator>
    </item>
  </channel>
</rss>
