<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Sep 2025 01:29:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Three Distributional Approaches for PM10 Assessment in Northern Italy</title>
      <link>https://arxiv.org/abs/2509.13886</link>
      <description>arXiv:2509.13886v1 Announce Type: new 
Abstract: We propose three spatial methods for estimating the full probability distribution of PM10 concentrations, with the ultimate goal of assessing air quality in Northern Italy. Moving beyond spatial averages and simple indicators, we adopt a distributional perspective to capture the complex variability of pollutant concentrations across space. The first proposed approach predicts class-based compositions via Fixed Rank Kriging; the second estimates multiple, non-crossing quantiles through a spatial regression with differential regularization; the third directly reconstructs full probability densities leveraging on both Fixed Rank Kriging and multiple quantiles spatial regression within a Simplicial Principal Component Analysis framework. These approaches are applied to daily PM10 measurements, collected from 2018 to 2022 in Northern Italy, to estimate spatially continuous distributions and to identify regions at risk of regulatory exceedance. The three approaches exhibit localized differences, revealing how modeling assumptions may influence the prediction of fine-scale pollutant concentration patterns. Nevertheless, they consistently agree on the broader spatial patterns of pollution. This general agreement supports the robustness of a distributional approach, which offers a comprehensive and policy-relevant framework for assessing air quality and regulatory exceedance risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13886v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marco F. De Sanctis, Andrea Gilardi, Giacomo Milan, Laura M. Sangalli, Francesca Ieva, Piercesare Secchi</dc:creator>
    </item>
    <item>
      <title>PoPStat-COVID19: Leveraging Population Pyramids to Quantify Demographic Vulnerability to COVID-19</title>
      <link>https://arxiv.org/abs/2509.14213</link>
      <description>arXiv:2509.14213v1 Announce Type: new 
Abstract: Understanding how population age structure shapes COVID-19 burden is crucial for pandemic preparedness, yet common summary measures such as median age ignore key distributional features like skewness, bimodality, and the proportional weight of high-risk cohorts. We extend the PoPStat framework, originally devised to link entire population pyramids with cause-specific mortality by applying it to COVID-19. Using 2019 United Nations World Population Prospects age-sex distributions together with cumulative cases and deaths per million recorded up to 5 May 2023 by Our World in Data, we calculate PoPDivergence (the Kullback-Leibler divergence from an optimised reference pyramid) for 180+ countries and derive PoPStat-COVID19 as the Pearson correlation between that divergence and log-transformed incidence or mortality. Optimisation selects Malta's old-skewed pyramid as the reference, yielding strong negative correlations for cases (r=-0.86, p&lt;0.001, R^2=0.74) and deaths (r=-0.82, p&lt;0.001, R^2=0.67). Sensitivity tests across twenty additional, similarly old-skewed references confirm that these associations are robust to reference choice. Benchmarking against eight standard indicators like gross domestic product per capita, Gini index, Human Development Index, life expectancy at birth, median age, population density, Socio-demographic Index, and Universal Health Coverage Index shows that PoPStat-COVID19 surpasses GDP per capita, median age, population density, and several other traditional measures, and outperforms every comparator for fatality burden. PoPStat-COVID19 therefore provides a concise, distribution-aware scalar for quantifying demographic vulnerability to COVID-19.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14213v1</guid>
      <category>stat.AP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Buddhi Wijenayake, Athulya Ratnayake, Lelumi Edirisinghe, Uditha Wijeratne, Tharaka Fonseka, Roshan Godaliyadda, Samath Dharmaratne, Parakrama Ekanayake, Vijitha Herath, Insoha Alwis, Supun Manathunga</dc:creator>
    </item>
    <item>
      <title>Landcover classification and change detection using remote sensing and machine learning: a case study of Western Fiji</title>
      <link>https://arxiv.org/abs/2509.13388</link>
      <description>arXiv:2509.13388v1 Announce Type: cross 
Abstract: As a developing country, Fiji is facing rapid urbanisation, which is visible in the massive development projects that include housing, roads, and civil works. In this study, we present machine learning and remote sensing frameworks to compare land use and land cover change from 2013 to 2024 in Nadi, Fiji. The ultimate goal of this study is to provide technical support in land cover/land use modelling and change detection. We used Landsat-8 satellite image for the study region and created our training dataset with labels for supervised machine learning. We used Google Earth Engine and unsupervised machine learning via k-means clustering to generate the land cover map. We used convolutional neural networks to classify the selected regions' land cover types. We present a visualisation of change detection, highlighting urban area changes over time to monitor changes in the map.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13388v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yadvendra Gurjar, Ruoni Wan, Ehsan Farahbakhsh, Rohitash Chandra</dc:creator>
    </item>
    <item>
      <title>Sample Size Calculations for the Development of Risk Prediction Models that Account for Performance Variability</title>
      <link>https://arxiv.org/abs/2509.14028</link>
      <description>arXiv:2509.14028v1 Announce Type: cross 
Abstract: Existing approaches to sample size calculations for developing clinical prediction models have focused on ensuring that the expected value of a chosen performance measure meets a pre-specified target. For example, to limit model-overfitting, the sample size is commonly chosen such that the expected calibration slope (CS) is 0.9, close to 1 for a perfectly calibrated model. In practice, due to sampling variability, model performance can vary considerably across different development samples of the recommended size. If this variability is high, the probability of obtaining a model with performance close to the target for a given measure may be unacceptably low. To address this, we propose an adapted approach to sample size calculations that explicitly incorporates performance variability by targeting the probability of acceptable performance (PrAP). For example, in the context of calibration, we may define a model as acceptably calibrated if CS falls in a pre-defined range, e.g. between 0.85 and 1.15. Then we choose the required sample size to ensure that PrAP(CS)=80%. For binary outcomes we implemented our approach for CS within a simulation-based framework via the R package `samplesizedev'. Additionally, for CS specifically, we have proposed an equivalent analytical calculation which is computationally efficient. While we focused on CS, the simulation-based framework is flexible and can be easily extended to accommodate other performance measures and types of outcomes. When adhering to existing recommendations, we found that performance variability increased substantially as the number of predictors, p, decreased. Consequently, PrAP(CS) was often low. For example, with 5 predictors, PrAP(CS) was around 50%. Our adapted approach resulted in considerably larger sample sizes, especially for p&lt;10. Applying shrinkage tends to improve PrAP(CS).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14028v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Menelaos Pavlou, Rumana Z. Omar, Gareth Ambler</dc:creator>
    </item>
    <item>
      <title>Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework</title>
      <link>https://arxiv.org/abs/2509.14167</link>
      <description>arXiv:2509.14167v1 Announce Type: cross 
Abstract: Many critical healthcare decisions are challenged by the inability to measure key underlying parameters. Glaucoma, a leading cause of irreversible blindness driven by elevated intraocular pressure (IOP), provides a stark example. The primary determinant of IOP, a tissue property called trabecular meshwork permeability, cannot be measured in vivo, forcing clinicians to depend on indirect surrogates. This clinical challenge is compounded by a broader computational one: developing predictive models for such ill-posed inverse problems is hindered by a lack of ground-truth data and prohibitive cost of large-scale, high-fidelity simulations. We address both challenges with an end-to-end framework to noninvasively estimate unmeasurable variables from sparse, routine data. Our approach combines a multi-stage artificial intelligence architecture to functionally separate the problem; a novel data generation strategy we term PCDS that obviates the need for hundreds of thousands of costly simulations, reducing the effective computational time from years to hours; and a Bayesian engine to quantify predictive uncertainty. Our framework deconstructs a single IOP measurement into its fundamental components from routine inputs only, yielding estimates for the unmeasurable tissue permeability and a patient's outflow facility. Our noninvasively estimated outflow facility achieved excellent agreement with state-of-the-art tonography with precision comparable to direct physical instruments. Furthermore, the newly derived permeability biomarker demonstrates high accuracy in stratifying clinical cohorts by disease risk, highlighting its diagnostic potential. More broadly, our framework establishes a generalizable blueprint for solving similar inverse problems in other data-scarce, computationally-intensive domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14167v1</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Rezwan Jaher, Abul Mukid Mohammad Mukaddes, A. B. M. Abdul Malek</dc:creator>
    </item>
    <item>
      <title>Index Date Imputation For Survival Outcomes for Externally Controlled Trials</title>
      <link>https://arxiv.org/abs/2509.14183</link>
      <description>arXiv:2509.14183v1 Announce Type: cross 
Abstract: Externally controlled trials (ECTs) compare outcomes between a single-arm trial and external controls drawn from sources such as historical trials, registries, or observational studies. In survival analysis, a major challenge arises when the time origin (index date) differs across groups, for example, when treatment initiation occurs after a delay in the single-arm trial but is undefined in the external controls. This misalignment can bias treatment effect estimates and distort causal interpretation. We propose a novel statistical method, Index Date Imputation (IDI), that imputes comparable index dates for external control patients using the estimated distribution of treatment initiation times from the single-arm cohort. To address population-level confounding, IDI is combined with propensity score weighting or matching, yielding balanced and temporally aligned cohorts for survival comparison. We detail diagnostics for covariate balance and truncation bias, and evaluate performance via extensive simulations. Applying IDI to a randomized oncology trial, we demonstrate that the method recovers the known treatment effect despite artificial index date misalignment. IDI provides a principled framework for time-to-event analyses in ECTs and is broadly applicable in oncology and rare disease settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14183v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Q. Le Coent, G. L. Rosner, M-C. Wang, C. Hu</dc:creator>
    </item>
    <item>
      <title>Bayesian Image-on-Image Regression via Deep Kernel Learning based Gaussian Processes</title>
      <link>https://arxiv.org/abs/2311.05649</link>
      <description>arXiv:2311.05649v2 Announce Type: replace 
Abstract: In neuroimaging studies, it becomes increasingly important to study associations between different imaging modalities using image-on-image regression (IIR), which faces challenges in interpretation, statistical inference, and prediction. Our motivating problem is how to predict task-evoked fMRI activity using resting-state fMRI data in the Human Connectome Project (HCP). The main difficulty lies in effectively combining different types of imaging predictors with varying resolutions and spatial domains in IIR. To address these issues, we develop Bayesian Image-on-image Regression via Deep Kernel Learning Gaussian Processes (BIRD-GP) and develop efficient posterior computation methods through Stein variational gradient descent. We demonstrate the advantages of BIRD-GP over state-of-the-art IIR methods using simulations. For HCP data analysis using BIRD-GP, we combine the voxel-wise fALFF maps and region-wise connectivity matrices to predict fMRI contrast maps for language and social recognition tasks. We show that fALFF is less predictive than the connectivity matrix for both tasks, but combining both yields improved results. Angular Gyrus Right emerges as the most predictable region for the language task (75.9% predictable voxels), while Superior Parietal Gyrus Right tops for the social recognition task (48.9% predictable voxels). Additionally, we identify features from the resting-state fMRI data that are important for task fMRI prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05649v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guoxuan Ma, Bangyao Zhao, Hasan Abu-Amara, Jian Kang</dc:creator>
    </item>
    <item>
      <title>Random time-shift approximation enables hierarchical Bayesian inference of mechanistic within-host viral dynamics models on large datasets</title>
      <link>https://arxiv.org/abs/2507.02884</link>
      <description>arXiv:2507.02884v2 Announce Type: replace 
Abstract: Mechanistic mathematical models of within-host viral dynamics are tools for understanding how a virus' biology and its interaction with the immune system shape the infectivity of a host. The biology of the process is encoded by the structure and parameters of the model that can be inferred statistically by fitting to viral load data. The main drawback of mechanistic models is that this inference is computationally expensive because the model must be repeatedly solved. This limits the size of the datasets that can be considered or the complexity of the models fitted. In this paper we develop a much cheaper inference method by implementing a novel approximation of the model dynamics that uses a combination of random and deterministic processes. This approximation also properly accounts for process noise early in the infection when cell and virion numbers are small, which is important for the viral dynamics but often overlooked. Our method runs on a consumer laptop and is fast enough to facilitate a full hierarchical Bayesian treatment of the problem with sharing of information to allow for individual level parameter differences. We apply our method to simulated datasets and a reanalysis of COVID-19 monitoring data in an National Basketball Association cohort of 163 individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02884v2</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dylan J. Morris, Lauren Kennedy, Andrew J. Black</dc:creator>
    </item>
    <item>
      <title>Statistical Consequences of Fat Tails: Real World Preasymptotics, Epistemology, and Applications</title>
      <link>https://arxiv.org/abs/2001.10488</link>
      <description>arXiv:2001.10488v4 Announce Type: replace-cross 
Abstract: (The third edition corrects minor typos and adds 3 chapters synthesized from published papers plus an appendix on maximum entropy distributions.) The monograph investigates the misapplication of conventional statistical techniques to fat tailed distributions and looks for remedies, when possible.
  Switching from thin tailed to fat tailed distributions requires more than "changing the color of the dress". Traditional asymptotics deal mainly with either n=1 or $n=\infty$, and the real world is in between, under of the "laws of the medium numbers" --which vary widely across specific distributions. Both the law of large numbers and the generalized central limit mechanisms operate in highly idiosyncratic ways outside the standard Gaussian or Levy-Stable basins of convergence.
  A few examples:
  + The sample mean is rarely in line with the population mean, with effect on "naive empiricism", but can be sometimes be estimated via parametric methods.
  + The "empirical distribution" is rarely empirical.
  + Parameter uncertainty has compounding effects on statistical metrics.
  + Dimension reduction (principal components) fails.
  + Inequality estimators (GINI or quantile contributions) are not additive and produce wrong results.
  + Many "biases" found in psychology become entirely rational under more sophisticated probability distributions
  + Most of the failures of financial economics, econometrics, and behavioral economics can be attributed to using the wrong distributions.
  This book, the first volume of the Technical Incerto, weaves a narrative around published journal articles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2001.10488v4</guid>
      <category>stat.OT</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nassim Nicholas Taleb</dc:creator>
    </item>
    <item>
      <title>Design and Analysis of Switchback Experiments</title>
      <link>https://arxiv.org/abs/2009.00148</link>
      <description>arXiv:2009.00148v4 Announce Type: replace-cross 
Abstract: Switchback experiments, where a firm sequentially exposes an experimental unit to random treatments, are among the most prevalent designs used in the technology sector, with applications ranging from ride-hailing platforms to online marketplaces. Although practitioners have widely adopted this technique, the derivation of the optimal design has been elusive, hindering practitioners from drawing valid causal conclusions with enough statistical power. We address this limitation by deriving the optimal design of switchback experiments under a range of different assumptions on the order of the carryover effect -- the length of time a treatment persists in impacting the outcome. We cast the optimal experimental design problem as a minimax discrete optimization problem, identify the worst-case adversarial strategy, establish structural results, and solve the reduced problem via a continuous relaxation. For switchback experiments conducted under the optimal design, we provide two approaches for performing inference. The first provides exact randomization based p-values, and the second uses a new finite population central limit theorem to conduct conservative hypothesis tests and build confidence intervals. We further provide theoretical results when the order of the carryover effect is misspecified and provide a data-driven procedure to identify the order of the carryover effect. We conduct extensive simulations to study the numerical performance and empirical properties of our results, and conclude with practical suggestions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2009.00148v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iavor Bojinov, David Simchi-Levi, Jinglong Zhao</dc:creator>
    </item>
    <item>
      <title>Optimal Data Splitting for Holdout Cross-Validation in Large Covariance Matrix Estimation</title>
      <link>https://arxiv.org/abs/2503.15186</link>
      <description>arXiv:2503.15186v2 Announce Type: replace-cross 
Abstract: Cross-validation is a statistical tool that can be used to improve large covariance matrix estimation. Although its efficiency is observed in practical applications and a convergence result towards the error of the non linear shrinkage is available in the high-dimensional regime, formal proofs that take into account the finite sample size effects are currently lacking. To carry on analytical analysis, we focus on the holdout method, a single iteration of cross-validation, rather than the traditional $k$-fold approach. We derive a closed-form expression for the expected estimation error when the population matrix follows a white inverse Wishart distribution, and we observe the optimal train-test split scales as the square root of the matrix dimension. For general population matrices, we connected the error to the variance of eigenvalues distribution, but approximations are necessary. In this framework and in the high-dimensional asymptotic regime, both the holdout and $k$-fold cross-validation methods converge to the optimal estimator when the train-test ratio scales with the square root of the matrix dimension which is coherent with the existing theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15186v2</guid>
      <category>math.ST</category>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lamia Lamrani, Christian Bongiorno, Marc Potters</dc:creator>
    </item>
    <item>
      <title>Continuous Temporal Learning of Probability Distributions via Neural ODEs with Applications in Continuous Glucose Monitoring Data</title>
      <link>https://arxiv.org/abs/2505.08698</link>
      <description>arXiv:2505.08698v3 Announce Type: replace-cross 
Abstract: Modeling the dynamics of probability distributions from time-dependent data samples is a fundamental problem in many fields, including digital health. The goal is to analyze how the distribution of a biomarker, such as glucose, changes over time and how these changes may reflect the progression of chronic diseases such as diabetes. We introduce a probabilistic model based on a Gaussian mixture that captures the evolution of a continuous-time stochastic process. Our approach combines a nonparametric estimate of the distribution, obtained with Maximum Mean Discrepancy (MMD), and a Neural Ordinary Differential Equation (Neural ODE) that governs the temporal evolution of the mixture weights. The model is highly interpretable, detects subtle distribution shifts, and remains computationally efficient. We illustrate the broad utility of our approach in a 26-week clinical trial that treats all continuous glucose monitoring (CGM) time series as the primary outcome. This method enables rigorous longitudinal comparisons between the treatment and control arms and yields characterizations that conventional summary-based clinical trials analytical methods typically do not capture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08698v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Antonio \'Alvarez-L\'opez, Marcos Matabuena</dc:creator>
    </item>
  </channel>
</rss>
