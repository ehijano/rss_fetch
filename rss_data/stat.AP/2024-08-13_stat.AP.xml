<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 14 Aug 2024 01:40:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Predictive maintenance solution for industrial systems -- an unsupervised approach based on log periodic power law</title>
      <link>https://arxiv.org/abs/2408.05231</link>
      <description>arXiv:2408.05231v1 Announce Type: new 
Abstract: A new unsupervised predictive maintenance analysis method based on the renormalization group approach used to discover critical behavior in complex systems has been proposed. The algorithm analyzes univariate time series and detects critical points based on a newly proposed theorem that identifies critical points using a Log Periodic Power Law function fits. Application of a new algorithm for predictive maintenance analysis of industrial data collected from reciprocating compressor systems is presented. Based on the knowledge of the dynamics of the analyzed compressor system, the proposed algorithm predicts valve and piston rod seal failures well in advance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05231v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bogdan {\L}obodzi\'nski</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Case Study on the Performance of Machine Learning Methods on the Classification of Solar Panel Electroluminescence Images</title>
      <link>https://arxiv.org/abs/2408.06229</link>
      <description>arXiv:2408.06229v1 Announce Type: new 
Abstract: Photovoltaics (PV) are widely used to harvest solar energy, an important form of renewable energy. Photovoltaic arrays consist of multiple solar panels constructed from solar cells. Solar cells in the field are vulnerable to various defects, and electroluminescence (EL) imaging provides effective and non-destructive diagnostics to detect those defects. We use multiple traditional machine learning and modern deep learning models to classify EL solar cell images into different functional/defective categories. Because of the asymmetry in the number of functional vs. defective cells, an imbalanced label problem arises in the EL image data. The current literature lacks insights on which methods and metrics to use for model training and prediction. In this paper, we comprehensively compare different machine learning and deep learning methods under different performance metrics on the classification of solar cell EL images from monocrystalline and polycrystalline modules. We provide a comprehensive discussion on different metrics. Our results provide insights and guidelines for practitioners in selecting prediction methods and performance metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06229v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Song, Kennedy Odongo, Francis G. Pascual, Yili Hong</dc:creator>
    </item>
    <item>
      <title>Bootstrap Matching: a robust and efficient correction for non-random A/B test, and its applications</title>
      <link>https://arxiv.org/abs/2408.05297</link>
      <description>arXiv:2408.05297v1 Announce Type: cross 
Abstract: A/B testing, a widely used form of Randomized Controlled Trial (RCT), is a fundamental tool in business data analysis and experimental design. However, despite its intent to maintain randomness, A/B testing often faces challenges that compromise this randomness, leading to significant limitations in practice. In this study, we introduce Bootstrap Matching, an innovative approach that integrates Bootstrap resampling, Matching techniques, and high-dimensional hypothesis testing to address the shortcomings of A/B tests when true randomization is not achieved. Unlike traditional methods such as Difference-in-Differences (DID) and Propensity Score Matching (PSM), Bootstrap Matching is tailored for large-scale datasets, offering enhanced robustness and computational efficiency. We illustrate the effectiveness of this methodology through a real-world application in online advertising and further discuss its potential applications in digital marketing, empirical economics, clinical trials, and high-dimensional bioinformatics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05297v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihao Zheng, Carol Liu</dc:creator>
    </item>
    <item>
      <title>Metacognitive Myopia in Large Language Models</title>
      <link>https://arxiv.org/abs/2408.05568</link>
      <description>arXiv:2408.05568v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) exhibit potentially harmful biases that reinforce culturally inherent stereotypes, cloud moral judgments, or amplify positive evaluations of majority groups. Previous explanations mainly attributed bias in LLMs to human annotators and the selection of training data. Consequently, they have typically been addressed with bottom-up approaches such as reinforcement learning or debiasing corpora. However, these methods only treat the effects of LLM biases by indirectly influencing the model architecture, but do not address the underlying causes in the computational process. Here, we propose metacognitive myopia as a cognitive-ecological framework that can account for a conglomerate of established and emerging LLM biases and provide a lever to address problems in powerful but vulnerable tools. Our theoretical framework posits that a lack of the two components of metacognition, monitoring and control, causes five symptoms of metacognitive myopia in LLMs: integration of invalid tokens and embeddings, susceptibility to redundant information, neglect of base rates in conditional computation, decision rules based on frequency, and inappropriate higher-order statistical inference for nested data structures. As a result, LLMs produce erroneous output that reaches into the daily high-stakes decisions of humans. By introducing metacognitive regulatory processes into LLMs, engineers and scientists can develop precise remedies for the underlying causes of these biases. Our theory sheds new light on flawed human-machine interactions and raises ethical concerns regarding the increasing, imprudent implementation of LLMs in organizational structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05568v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Scholten, Tobias R. Rebholz, Mandy H\"utter</dc:creator>
    </item>
    <item>
      <title>More Skin, More Likes! Measuring Child Exposure and User Engagement on TikTok</title>
      <link>https://arxiv.org/abs/2408.05622</link>
      <description>arXiv:2408.05622v1 Announce Type: cross 
Abstract: Sharenting, the practice of parents sharing content about their children on social media platforms, has become increasingly common, raising concerns about children's privacy and safety online. This study investigates children's exposure on TikTok, offering a detailed examination of the platform's content and associated comments. Analyzing 432,178 comments across 5,896 videos from 115 user accounts featuring children, we categorize content into Family, Fashion, and Sports. Our analysis highlights potential risks, such as inappropriate comments or contact offers, with a focus on appearance-based comments. Notably, 21% of comments relate to visual appearance. Additionally, 19.57% of videos depict children in revealing clothing, such as swimwear or bare midriffs, attracting significantly more appearance-based comments and likes than videos featuring fully clothed children, although this trend does not extend to downloads. These findings underscore the need for heightened awareness and protective measures to safeguard children's privacy and well-being in the digital age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05622v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Miriam Schirmer, Angelina Voggenreiter, J\"urgen Pfeffer</dc:creator>
    </item>
    <item>
      <title>Bank Cost Efficiency and Credit Market Structure Under a Volatile Exchange Rate</title>
      <link>https://arxiv.org/abs/2408.05688</link>
      <description>arXiv:2408.05688v1 Announce Type: cross 
Abstract: We study the impact of exchange rate volatility on cost efficiency and market structure in a cross-section of banks that have non-trivial exposures to foreign currency (FX) operations. We use unique data on quarterly revaluations of FX assets and liabilities (Revals) that Russian banks were reporting between 2004 Q1 and 2020 Q2. {\it First}, we document that Revals constitute the largest part of the banks' total costs, 26.5\% on average, with considerable variation across banks. {\it Second}, we find that stochastic estimates of cost efficiency are both severely downward biased -- by 30\% on average -- and generally not rank preserving when Revals are ignored, except for the tails, as our nonparametric copulas reveal. To ensure generalizability to other emerging market economies, we suggest a two-stage approach that does not rely on Revals but is able to shrink the downward bias in cost efficiency estimates by two-thirds. {\it Third}, we show that Revals are triggered by the mismatch in the banks' FX operations, which, in turn, is driven by household FX deposits and the instability of Ruble's exchange rate. {\it Fourth}, we find that the failure to account for Revals leads to the erroneous conclusion that the credit market is inefficient, which is driven by the upper quartile of the banks' distribution by total assets. Revals have considerable negative implications for financial stability which can be attenuated by the cross-border diversification of bank assets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05688v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jbankfin.2024.107285</arxiv:DOI>
      <dc:creator>Mikhail Mamonov, Christopher Parmeter, Artem Prokhorov</dc:creator>
    </item>
    <item>
      <title>Quantification of the Self-Excited Emotion Dynamics in Online Interactions</title>
      <link>https://arxiv.org/abs/2408.05700</link>
      <description>arXiv:2408.05700v1 Announce Type: cross 
Abstract: Emotions are essential for guiding human behavior, particularly in social interactions. In modern societies, a growing share of human interactions are taking place online which has been shown to amplify and distort the expression and perception of emotions. However, the entanglement across different emotions is not fully understood. We use a multivariate Hawkes self-excited point process to model and calibrate the temporal expressions of six basic emotions in YouTube live chats. This allows us to understand interdependencies among emotions, but also to disentangle the influence from the video content and social interactions with peers. Positive emotions are found to be more contagious, while negative emotions tend to leave a longer-lasting impression on users' memories. Furthermore, we quantify the endogeneity of online emotion dynamics and find that peer interactions drive user emotional expressions 3-5 times more than passive content consumption. This underscores the powerful incentives of social interactions and the potential risk of emotional manipulation through the use of modern chatbots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05700v1</guid>
      <category>cs.SI</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> Yishan (Ivy),  Luo, Didier Sornette, Sandro Claudio Lera</dc:creator>
    </item>
    <item>
      <title>Addressing Outcome Reporting Bias in Meta-analysis: A Selection Model Perspective</title>
      <link>https://arxiv.org/abs/2408.05747</link>
      <description>arXiv:2408.05747v1 Announce Type: cross 
Abstract: Outcome Reporting Bias (ORB) poses significant threats to the validity of meta-analytic findings. It occurs when researchers selectively report outcomes based on the significance or direction of results, potentially leading to distorted treatment effect estimates. Despite its critical implications, ORB remains an under-recognized issue, with few comprehensive adjustment methods available. The goal of this research is to investigate ORB-adjustment techniques through a selection model lens, thereby extending some of the existing methodological approaches available in the literature. To gain a better insight into the effects of ORB in meta-analysis of clinical trials, specifically in the presence of heterogeneity, and to assess the effectiveness of ORB-adjustment techniques, we apply the methodology to real clinical data affected by ORB and conduct a simulation study focusing on treatment effect estimation with a secondary interest in heterogeneity quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05747v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandra Gaia Saracini, Leonhard Held</dc:creator>
    </item>
    <item>
      <title>Quantifying uncertainty in area and regression coefficient estimation from remote sensing maps</title>
      <link>https://arxiv.org/abs/2407.13659</link>
      <description>arXiv:2407.13659v2 Announce Type: replace 
Abstract: Remote sensing map products are used to obtain estimates of environmental quantities, such as deforested area or the effect of conservation zones on deforestation. However, the quality of map products varies, and - because maps are outputs of complex machine learning algorithms that take in a variety of remotely sensed variables as inputs - errors are difficult to characterize. Without capturing the biases that may be present, naive calculations of population-level estimates from such maps are statistically invalid. In this paper, we compare several uncertainty quantification methods - stratification, Olofsson area estimation method, and prediction-powered inference - that combine a small amount of randomly sampled ground truth data with large-scale remote sensing map products to generate statistically valid estimates. Applying these methods across four remote sensing use cases in area and regression coefficient estimation, we find that they result in estimates that are more reliable than naively using the map product as if it were 100% accurate and have lower uncertainty than using only the ground truth and ignoring the map product. Prediction-powered inference uses ground truth data to correct for bias in the map product estimate and (unlike stratification) does not require us to choose a map product before sampling. This is the first work to (1) apply prediction-powered inference to remote sensing estimation tasks, and (2) perform uncertainty quantification on remote sensing regression coefficients without assumptions on the structure of map product errors. To improve the utility of machine learning-generated remote sensing maps for downstream applications, we recommend that map producers provide a holdout ground truth dataset to be used for calibration in uncertainty quantification alongside their maps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13659v2</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>eess.SP</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kerri Lu, Stephen Bates, Sherrie Wang</dc:creator>
    </item>
    <item>
      <title>Regularized Adjusted Plus-Minus Models for Evaluating and Scouting Football (Soccer) Players using Possession Sequences</title>
      <link>https://arxiv.org/abs/2407.17832</link>
      <description>arXiv:2407.17832v2 Announce Type: replace 
Abstract: This paper presents a novel framework for evaluating players in association football (soccer). Our method uses possession sequences, i.e. sequences of consecutive on-ball actions, for deriving estimates for player strengths. On the surface, the methodology is similar to classical adjusted plus-minus rating models using mainly regularized regression techniques. However, by analyzing possessions, our framework is able to distinguish on-ball and off-ball contributions of players to the game. From a methodological viewpoint, the framework explores four different penalization schemes, which exploit football-specific structures such as the grouping of players into position groups as well as into common strength groups. These four models lead to four ways to rate players by considering the respective estimate of each model corresponding to the player. The ratings are used to analyze the 2017/18 season of the Spanish La Liga. We compare similarities as well as particular use cases of each of the penalized models and provide guidance for practitioners when using the individual model specifications. Finally, we conclude our analysis by providing a domain-specific statistical evaluation framework, which highlights the potential of the penalized regression approaches for evaluating players.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17832v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Bajons, Kurt Hornik</dc:creator>
    </item>
    <item>
      <title>The Impacts of the Gender Imbalance on the Marriage Market: Evidence from World War II in Japan</title>
      <link>https://arxiv.org/abs/2102.00687</link>
      <description>arXiv:2102.00687v3 Announce Type: replace-cross 
Abstract: This study uses the unprecedented changes in the sex ratio due to the losses of men during World War II to identify the impacts of the gender imbalance on marriage market outcomes in Japan. Using newly digitized census-based historical statistics, we find evidence that men have a stronger bargaining position in the marriage market than women do. Under the conditions of relative male scarcity, women are less likely to marry. Although the entry of younger cohorts with a natural gender balance into the marriage market attenuated its magnitude, this tendency persisted until the mid-1950s. Widowed women facing male scarcity are particularly unable to remarry. Our results suggest that reinstating military pensions in the early 1950s further reduced their incentive to remarry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2102.00687v3</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kota Ogasawara, Erika Igarashi</dc:creator>
    </item>
    <item>
      <title>Feature Selection in High-dimensional Spaces Using Graph-Based Methods</title>
      <link>https://arxiv.org/abs/2108.12682</link>
      <description>arXiv:2108.12682v3 Announce Type: replace-cross 
Abstract: High-dimensional feature selection is a central problem in a variety of application domains such as machine learning, image analysis, and genomics. In this paper, we propose graph-based tests as a useful basis for feature selection. We describe an algorithm for selecting informative features in high-dimensional data, where each observation comes from one of $K$ different distributions. Our algorithm can be applied in a completely nonparametric setup without any distributional assumptions on the data, and it aims at outputting those features in the data, that contribute the most to the overall distributional variation. At the heart of our method is the recursive application of distribution-free graph-based tests on subsets of the feature set, located at different depths of a hierarchical clustering tree constructed from the data. Our algorithm recovers all truly contributing features with high probability, while ensuring optimal control on false-discovery. We show the superior performance of our method over other existing ones through synthetic data, and demonstrate the utility of this method on several real-life datasets from the domains of climate change and biology, wherein our algorithm is not only able to detect known features expected to be associated with the underlying process, but also discovers novel targets that can be subsequently studied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.12682v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Swarnadip Ghosh, Somabha Mukherjee, Divyansh Agarwal, Yichen He, Mingzhi Song, Xuejiao Pei</dc:creator>
    </item>
    <item>
      <title>Spectral Clustering for Crowdsourcing with Inherently Distinct Task Types</title>
      <link>https://arxiv.org/abs/2302.07393</link>
      <description>arXiv:2302.07393v2 Announce Type: replace-cross 
Abstract: The Dawid-Skene model is the most widely assumed model in the analysis of crowdsourcing algorithms that estimate ground-truth labels from noisy worker responses. In this work, we are motivated by crowdsourcing applications where workers have distinct skill sets and their accuracy additionally depends on a task's type. While weighted majority vote (WMV) with a single weight vector for each worker achieves the optimal label estimation error in the Dawid-Skene model, we show that different weights for different types are necessary for a multi-type model. Focusing on the case where there are two types of tasks, we propose a spectral method to partition tasks into two groups that cluster tasks by type. Our analysis reveals that task types can be perfectly recovered if the number of workers $n$ scales logarithmically with the number of tasks $d$. Any algorithm designed for the Dawid-Skene model can then be applied independently to each type to infer the labels. Numerical experiments show how clustering tasks by type before estimating ground-truth labels enhances the performance of crowdsourcing algorithms in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.07393v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saptarshi Mandal, Seo Taek Kong, Dimitrios Katselis, R. Srikant</dc:creator>
    </item>
    <item>
      <title>Cooperative Abnormal Node Detection with Adversary Resistance</title>
      <link>https://arxiv.org/abs/2311.16661</link>
      <description>arXiv:2311.16661v2 Announce Type: replace-cross 
Abstract: This paper presents a novel probabilistic detection scheme called Cooperative Statistical Detection~(CSD) for abnormal node detection while defending against adversarial attacks in cluster-tree wireless sensor networks. The CSD performs a two-phase process: 1) designing a likelihood ratio test~(LRT) for a non-root node at its children from the perspective of packet loss; 2) making an overall decision at the root node based on the aggregated detection data of the nodes over tree branches. In most adversarial scenarios, malicious children knowing the detection policy can generate falsified data to protect the abnormal parent from being detected. To resolve this issue, a mechanism is presented in the CSD to remove untrustworthy information. Through theoretical analysis, we show that the LRT-based method achieves the perfect detection. Furthermore, the optimal removal threshold is derived for falsifications with uncertain strategies and guarantees perfect detection of the CSD. As our simulation results shown, the CSD approach is robust to falsifications and can rapidly reach $99\%$ detection accuracy, even in existing adversarial scenarios, which outperforms the state-of-the-art technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16661v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingying Huangfu, Tian Bai</dc:creator>
    </item>
    <item>
      <title>Benchmarking M6 Competitors: An Analysis of Financial Metrics and Discussion of Incentives</title>
      <link>https://arxiv.org/abs/2406.19105</link>
      <description>arXiv:2406.19105v2 Announce Type: replace-cross 
Abstract: The M6 Competition assessed the performance of competitors using a ranked probability score and an information ratio (IR). While these metrics do well at picking the winners in the competition, crucial questions remain for investors with longer-term incentives. To address these questions, we compare the competitors' performance to a number of conventional (long-only) and alternative indices using standard industry metrics. We apply factor models to measure the competitors' value-adds above industry-standard benchmarks and find that competitors with more extreme performance are less dependent on the benchmarks. We also uncover that most competitors could not generate significant out-performance compared to randomly selected long-only and long-short portfolios but did generate out-performance compared to short-only portfolios. We further introduce two new strategies by picking the competitors with the best (Superstars) and worst (Superlosers) recent performance and show that it is challenging to identify skill amongst investment managers. We also discuss the incentives of winning the competition compared to professional investors, where investors wish to maximize fees over an extended period of time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19105v2</guid>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.34990.11847/1</arxiv:DOI>
      <dc:creator>Matthew J. Schneider, Rufus Rankin, Prabir Burman, Alexander Aue</dc:creator>
    </item>
    <item>
      <title>AI-Powered Energy Algorithmic Trading: Integrating Hidden Markov Models with Neural Networks</title>
      <link>https://arxiv.org/abs/2407.19858</link>
      <description>arXiv:2407.19858v3 Announce Type: replace-cross 
Abstract: In quantitative finance, machine learning methods are essential for alpha generation. This study introduces a new approach that combines Hidden Markov Models (HMM) and neural networks, integrated with Black-Litterman portfolio optimization. During the COVID period (2019-2022), this dual-model approach achieved a 83% return with a Sharpe ratio of 0.77. It incorporates two risk models to enhance risk management, showing efficiency during volatile periods. The methodology was implemented on the QuantConnect platform, which was chosen for its robust framework and experimental reproducibility. The system, which predicts future price movements, includes a three-year warm-up to ensure proper algorithm function. It targets highly liquid, large-cap energy stocks to ensure stable and predictable performance while also considering broker payments. The dual-model alpha system utilizes log returns to select the optimal state based on the historical performance. It combines state predictions with neural network outputs, which are based on historical data, to generate trading signals. This study examined the architecture of the trading system, data pre-processing, training, and performance. The full code and backtesting data are available under the QuantConnect terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19858v3</guid>
      <category>q-fin.PM</category>
      <category>cs.LG</category>
      <category>q-fin.GN</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tiago Monteiro</dc:creator>
    </item>
  </channel>
</rss>
