<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Dec 2025 02:35:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Evaluating A/B Testing Methodologies via Sample Splitting: Theory and Practice</title>
      <link>https://arxiv.org/abs/2512.03366</link>
      <description>arXiv:2512.03366v1 Announce Type: new 
Abstract: We develop a theoretical framework for sample splitting in A/B testing environments, where data for each test are partitioned into two splits to measure methodological performance when the true impacts of tests are unobserved. We show that sample-split estimators are generally biased for full-sample performance but consistently estimate sample-split analogues of it. We derive their asymptotic distributions, construct valid confidence intervals, and characterize the bias-variance trade-offs underlying sample-split design choices. We validate our theoretical results through simulations and provide implementation guidance for A/B testing products seeking to evaluate new estimators and decision rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03366v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Kessler, James McQueen, Miikka Rokkanen</dc:creator>
    </item>
    <item>
      <title>Estimation of Panel Data Models with Nonlinear Factor Structure</title>
      <link>https://arxiv.org/abs/2512.03693</link>
      <description>arXiv:2512.03693v1 Announce Type: new 
Abstract: Panel data models with unobserved heterogeneity in the form of interactive effects standardly assume that the time effects - or "common factors" - enter linearly. This assumption is unnatural in the sense that it pertains to the unobserved component of the model, and there is rarely any reason to believe that this component takes on a particular functional form. This is in stark contrast to the relationship between the observables, which can often be credibly argued to be linear. Linearity in the factors has persevered mainly because it is convenient, and that it is better than standard fixed effects. The present paper relaxes this assumption. It does so by combining the common correlated effects (CCE) approach to standard interactive effects with the method of sieves. The new estimator - abbreviated "SCCE" - retains many of the advantages of CCE, including its computational simplicity, and good small-sample and asymptotic properties, but is applicable under a much broader class of factor structures that includes the linear one as a special case. This makes it well-suited for a wide range of empirical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03693v1</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christina Maschmann, Joakim Westerlund</dc:creator>
    </item>
    <item>
      <title>Learning from crises: A new class of time-varying parameter VARs with observable adaptation</title>
      <link>https://arxiv.org/abs/2512.03763</link>
      <description>arXiv:2512.03763v1 Announce Type: new 
Abstract: We revisit macroeconomic time-varying parameter vector autoregressions (TVP-VARs), whose persistent coefficients may adapt too slowly to large, abrupt shifts such as those during major crises. We explore the performance of an adaptively-varying parameter (AVP) VAR that incorporates deterministic adjustments driven by observable exogenous variables, replacing latent state innovations with linear combinations of macroeconomic and financial indicators. This reformulation collapses the state equation into the measurement equation, enabling simple linear estimation of the model. Simulations show that adaptive parameters are substantially more parsimonious than conventional TVPs, effectively disciplining parameter dynamics without sacrificing flexibility. Using macroeconomic datasets for both the U.S. and the euro area, we demonstrate that AVP-VAR consistently improves out-of-sample forecasts, especially during periods of heightened volatility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.03763v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nicolas Hardy, Dimitris Korobilis</dc:creator>
    </item>
    <item>
      <title>Large Language Models: An Applied Econometric Framework</title>
      <link>https://arxiv.org/abs/2412.07031</link>
      <description>arXiv:2412.07031v3 Announce Type: replace 
Abstract: Large language models (LLMs) enable researchers to analyze text at unprecedented scale and minimal cost. Researchers can now revisit old questions and tackle novel ones with rich data. We provide an econometric framework for realizing this potential in two empirical uses. For prediction problems -- forecasting outcomes from text -- valid conclusions require ``no training leakage'' between the LLM's training data and the researcher's sample, which can be enforced through careful model choice and research design. For estimation problems -- automating the measurement of economic concepts for downstream analysis -- valid downstream inference requires combining LLM outputs with a small validation sample to deliver consistent and precise estimates. Absent a validation sample, researchers cannot assess possible errors in LLM outputs, and consequently seemingly innocuous choices (which model, which prompt) can produce dramatically different parameter estimates. When used appropriately, LLMs are powerful tools that can expand the frontier of empirical economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07031v3</guid>
      <category>econ.EM</category>
      <category>cs.AI</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jens Ludwig, Sendhil Mullainathan, Ashesh Rambachan</dc:creator>
    </item>
    <item>
      <title>Market Sensitivities and Growth Differentials Across Australian Housing Markets</title>
      <link>https://arxiv.org/abs/2512.01139</link>
      <description>arXiv:2512.01139v2 Announce Type: replace 
Abstract: Australian house prices have risen strongly since the mid-1990s, but growth has been highly uneven across regions. Raw growth figures obscure whether these differences reflect persistent structural trends or cyclical fluctuations. We address this by estimating a three-factor model in levels for regional repeat-sales log price indexes over 1995-2024. The model decomposes each regional index into a national Market factor, two stationary spreads (Mining and Lifestyle) that capture mean-reverting geographic cycles, and a city-specific residual. The Mining spread, proxied by a Perth-Sydney index differential, reflects resource-driven oscillations in relative performance; the Lifestyle spread captures amenity-driven coastal and regional cycles. The Market loading isolates each region's fundamental sensitivity, beta, to national growth, so that a city's growth under an assumed national change is calculated from its beta once mean-reverting spreads are netted out. Comparing realised paths to these factor-implied trajectories indicates when a city is historically elevated or depressed, and attributes the gap to Mining or Lifestyle spreads.
  Expanding-window ARIMAX estimation reveals that Market betas are stable across major shocks (the mining boom, the Global Financial Crisis, and COVID-19), while Mining and Lifestyle behave as stationary spreads that widen forecast funnels without overturning the cross-sectional ranking implied by beta. Melbourne amplifies national growth, Sydney tracks the national trend closely, and regional areas dampen it. The framework thus provides a simple, factor-based tool for interpreting regional growth differentials and their persistence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.01139v2</guid>
      <category>econ.EM</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Willem P Sijp</dc:creator>
    </item>
    <item>
      <title>Why is the estimation of metaorder impact with public market data so challenging?</title>
      <link>https://arxiv.org/abs/2501.17096</link>
      <description>arXiv:2501.17096v2 Announce Type: replace-cross 
Abstract: Estimating market impact and transaction costs of large trades (metaorders) is a very important topic in finance. However, using models of price and trade based on public market data provide average price trajectories which are qualitatively different from what is observed during real metaorder executions: the price increases linearly, rather than in a concave way, during the execution and the amount of reversion after its end is very limited. We claim that this is a generic phenomenon due to the fact that even sophisticated statistical models are unable to correctly describe the origin of the autocorrelation of the order flow. We propose a modified Transient Impact Model which provides more realistic trajectories by assuming that only a fraction of the metaorder trading triggers market order flow. Interestingly, in our model there is a critical condition on the kernels of the price and order flow equations in which market impact becomes permanent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17096v2</guid>
      <category>q-fin.TR</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 04 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel Naviglio, Giacomo Bormetti, Francesco Campigli, German Rodikov, Fabrizio Lillo</dc:creator>
    </item>
  </channel>
</rss>
