<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Jan 2025 02:32:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Learnable Sparsification of Die-to-Die Communication via Spike-Based Encoding</title>
      <link>https://arxiv.org/abs/2501.08645</link>
      <description>arXiv:2501.08645v1 Announce Type: new 
Abstract: Efficient communication is central to both biological and artificial intelligence (AI) systems. In biological brains, the challenge of long-range communication across regions is addressed through sparse, spike-based signaling, minimizing energy consumption and latency. In contrast, modern AI workloads, which keep scaling ever larger across distributed compute systems, are increasingly constrained by bandwidth limitations, creating bottlenecks that hinder scalability and energy efficiency. Inspired by the brain's efficient communication strategies, we propose SNAP, a hybrid neural network architecture combining spiking neural networks (SNNs) and artificial neural networks (ANNs) to address these challenges. SNAP integrates SNNs at bandwidth-constrained regions, such as chip boundaries, where spike-based encoding reduces data transfer overhead. Within each chip, dense ANN computations are maintained to preserve high throughput, accuracy, and robustness.
  Historically, SNNs have faced difficulties scaling up, with limitations in task-specific performance and reliance on specialized hardware to exploit sparsity. SNAP overcomes these barriers through an algorithm-architecture co-design leveraging learnable sparsity for die-to-die communication while limiting spiking layers to specific network partitions. This composable design integrates spike-based and non-spiking pathways, making it adaptable to diverse deep learning workloads. Our evaluations on language processing and computer vision tasks demonstrate up to 5.3x energy efficiency improvements and 15.2x reductions in inference latency, outperforming both traditional SNNs and non-spiking models. We find that as model resources scale, SNAP's improvement margins grow. By addressing the critical bottleneck of inter-chip communication, SNAP offers a scalable, biologically inspired pathway to more efficient AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08645v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Nardone, Ruijie Zhu, Joseph Callenes, Mohammed E. Elbtity, Ramtin Zand, Jason Eshraghian</dc:creator>
    </item>
    <item>
      <title>Karatsuba Matrix Multiplication and its Efficient Custom Hardware Implementations</title>
      <link>https://arxiv.org/abs/2501.08889</link>
      <description>arXiv:2501.08889v1 Announce Type: new 
Abstract: While the Karatsuba algorithm reduces the complexity of large integer multiplication, the extra additions required minimize its benefits for smaller integers of more commonly-used bitwidths. In this work, we propose the extension of the scalar Karatsuba multiplication algorithm to matrix multiplication, showing how this maintains the reduction in multiplication complexity of the original Karatsuba algorithm while reducing the complexity of the extra additions. Furthermore, we propose new matrix multiplication hardware architectures for efficiently exploiting this extension of the Karatsuba algorithm in custom hardware. We show that the proposed algorithm and hardware architectures can provide real area or execution time improvements for integer matrix multiplication compared to scalar Karatsuba or conventional matrix multiplication algorithms, while also supporting implementation through proven systolic array and conventional multiplier architectures at the core. We provide a complexity analysis of the algorithm and architectures and evaluate the proposed designs both in isolation and in an end-to-end deep learning accelerator system compared to baseline designs and prior state-of-the-art works implemented on the same type of compute platform, demonstrating their ability to increase the performance-per-area of matrix multiplication hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08889v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TC.2025.3525606</arxiv:DOI>
      <dc:creator>Trevor E. Pogue, Nicola Nicolici</dc:creator>
    </item>
    <item>
      <title>Octopus: Scalable Low-Cost CXL Memory Pooling</title>
      <link>https://arxiv.org/abs/2501.09020</link>
      <description>arXiv:2501.09020v1 Announce Type: new 
Abstract: Compute Express Link (CXL) is widely-supported interconnect standard that promises to enable memory disaggregation in data centers. CXL allows for memory pooling, which can be used to create a shared memory space across multiple servers. However, CXL does not specify how to actually build a memory pool. Existing proposals for CXL memory pools are expensive, as they require CXL switches or large multi-headed devices. In this paper, we propose a new design for CXL memory pools that is cost-effective. We call these designs Octopus topologies. Our design uses small CXL devices that can be made cheaply and offer fast access latencies. Specifically, we propose asymmetric CXL topologies where hosts connect to different sets of CXL devices. This enables pooling and sharing memory across multiple hosts even as each individual CXL device is only connected to a small number of hosts. Importantly, this uses hardware that is readily available today. We also show the trade-off in terms of CXL pod size and cost overhead per host. Octopus improves the Pareto frontier defined by prior policies, e.g., offering to connect 3x as many hosts at 17% lower cost per host.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09020v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel S. Berger, Yuhong Zhong, Pantea Zardoshti, Shuwei Teng, Fiodar Kazhamiaka, Rodrigo Fonseca</dc:creator>
    </item>
    <item>
      <title>A Modern Primer on Processing in Memory</title>
      <link>https://arxiv.org/abs/2012.03112</link>
      <description>arXiv:2012.03112v4 Announce Type: replace 
Abstract: This paper discusses recent research that aims to enable computation close to data, an approach we broadly call processing-in-memory (PIM). PIM places computation mechanisms in or near where the data is stored (i.e., inside memory chips or modules, in the logic layer of 3D-stacked memory, in the memory controllers, in storage devices or chips), so that data movement between the computation units and memory/storage units is reduced or eliminated. While the general idea of PIM is not new, we discuss motivating trends in applications as well as memory circuits and technology that greatly exacerbate the need for enabling it in modern computing systems. We examine at least two promising new approaches to designing PIM systems to accelerate important data-intensive applications: (1) processing-using-memory, which exploits fundamental analog operational principles of memory chips to perform massively-parallel operations in-situ in memory, (2) processing-near-memory, which exploits different logic and memory integration technologies (e.g., 3D-stacked memory technology) to place computation logic close to memory circuitry, and thereby enable high-bandwidth, low-energy, and low-latency access to data. In both approaches, we describe and tackle relevant cross-layer research, design, and adoption challenges in devices, architecture, systems, compilers, programming models, and applications. Our focus is on the development of PIM designs that can be adopted in real computing platforms at low cost. We conclude by discussing work on solving key challenges to the practical adoption of PIM. We believe that the shift from a processor-centric to a memory-centric mindset (and infrastructure) remains the largest adoption challenge for PIM, which, once overcome, can unleash a fundamentally energy-efficient, high-performance, and sustainable new way of designing, using, and programming computing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2012.03112v4</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Onur Mutlu, Saugata Ghose, Juan G\'omez-Luna, Rachata Ausavarungnirun, Mohammad Sadrosadati, Geraldo F. Oliveira</dc:creator>
    </item>
    <item>
      <title>LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System</title>
      <link>https://arxiv.org/abs/2412.20166</link>
      <description>arXiv:2412.20166v2 Announce Type: replace 
Abstract: The expansion of large language models (LLMs) with hundreds of billions of parameters presents significant challenges to computational resources, particularly data movement and memory bandwidth. Long-context LLMs, which process sequences of tens of thousands of tokens, further increase the demand on the memory system as the complexity in attention layers and key-value cache sizes is proportional to the context length. Processing-in-Memory (PIM) maximizes memory bandwidth by moving compute to the data and can address the memory bandwidth challenges; however, PIM is not necessarily scalable to accelerate long-context LLM because of limited per-module memory capacity and the inflexibility of fixed-functional unit PIM architecture and static memory management. In this work, we propose LoL-PIM which is a multi-node PIM architecture that accelerates long context LLM through hardware-software co-design. In particular, we propose how pipeline parallelism can be exploited across a multi-PIM module while a direct PIM access (DPA) controller (or DMA for PIM) is proposed that enables dynamic PIM memory management and results in efficient PIM utilization across a diverse range of context length. We developed an MLIR-based compiler for LoL-PIM extending a commercial PIM-based compiler where the software modifications were implemented and evaluated, while the hardware changes were modeled in the simulator. Our evaluations demonstrate that LoL-PIM significantly improves throughput and reduces latency for long-context LLM inference, outperforming both multi-GPU and GPU-PIM systems (up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient deployment of LLMs in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20166v2</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hyucksung Kwon, Kyungmo Koo, Janghyeon Kim, Woongkyu Lee, Minjae Lee, Hyungdeok Lee, Yousub Jung, Jaehan Park, Yosub Song, Byeongsu Yang, Haerang Choi, Guhyun Kim, Jongsoon Won, Woojae Shin, Changhyun Kim, Gyeongcheol Shin, Yongkee Kwon, Ilkon Kim, Euicheol Lim, John Kim, Jungwook Choi</dc:creator>
    </item>
  </channel>
</rss>
