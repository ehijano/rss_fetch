<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Feb 2025 02:49:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Turing Test for Artificial Nets devoted to model Human Vision</title>
      <link>https://arxiv.org/abs/2502.00721</link>
      <description>arXiv:2502.00721v1 Announce Type: new 
Abstract: In this 2022 work we argued that, despite claims about successful modeling of the visual brain using artificial nets, the problem is far from being solved (even for low-level vision). Examples of open issues include: where should we read from ANNs in order to reproduce human behavior?, this ad-hoc read-out is considered part of the brain model or not?, should we use artificial psychophysics or artificial physiology?, in the case of ANNs, artificial experiments should literally match the experiments done with humans?. There is a clear need of rigorous procedures for experimental tests for ANNs devoted to model the visual brain, and more generally, to understand ANNs devoted to generic vision tasks. Following our experience in using low-level facts from Quantitative Visual Neuroscience in computer vision, in this work we presented the idea of developing a low-level dataset compiling the basic spatio-temporal and chromatic facts that are known to happen in the retina-V1 pathway, and they are not currently available in existing databases such as BrainScore. In our results we checked the behavior of three recently proposed models with similar architecture: (1) A parametric model tuned via Maximum Differentiation [Malo &amp; Simoncelli SPIE 15, Martinez et al. PLOS 18, Martinez et al. Front. Neurosci. 19], (2) A non-parametric model called PerceptNet tuned to maximize the correlation with human opinion on subjective distortions [Hepburn et al. IEEE ICIP 19], and (3) A model with the same encoder as PerceptNet, but tuned for image segmentation (published as Hernandez-Camara et al. Patt.Recogn.Lett. 23). Results on 10 compelling psycho/physio visual facts show that the first model is the one with closer behavior to the humans in terms of receptive fields, but more interestingly, on the nonlinear behavior when facing complex spatio-chromatic patterns of a range of luminances and contrasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00721v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jorge Vila-Tom\'as, Pablo Hern\'andez-C\'amara, Qiang Li, Valero Laparra, Jes\'us Malo</dc:creator>
    </item>
    <item>
      <title>Probabilistic adaptation of language comprehension for individual speakers: Evidence from neural oscillations</title>
      <link>https://arxiv.org/abs/2502.01299</link>
      <description>arXiv:2502.01299v1 Announce Type: new 
Abstract: Listeners adapt language comprehension based on their mental representations of speakers, but how these representations are dynamically updated remains unclear. We investigated whether listeners probabilistically adapt their comprehension based on the likelihood of speakers producing stereotype-incongruent utterances. Our findings reveal two potential mechanisms: a speaker-general mechanism that adjusts overall expectations about speaker-content relationships, and a speaker-specific mechanism that updates individual speaker models. In two EEG experiments, participants heard speakers make stereotype-congruent or incongruent utterances, with incongruency base rate manipulated between blocks. In Experiment 1, speaker incongruency modulated both high-beta (21-30 Hz) and theta (4-6 Hz) oscillations: incongruent utterances decreased oscillatory power in low base rate condition but increased it in high base rate condition. The theta effect varied with listeners' openness trait: less open participants showed theta increases to speaker-incongruencies, suggesting maintenance of speaker-specific information, while more open participants showed theta decreases, indicating flexible model updating. In Experiment 2, we dissociated base rate from the target speaker by manipulating the overall base rate using an alternative non-target speaker. Only the high-beta effect persisted, showing power decrease for speaker-incongruencies in low base rate condition but no effect in high base rate condition. The high-beta oscillations might reflect the speaker-general adjustment, while theta oscillations may index the speaker-specific model updating. These findings provide evidence for how language processing is shaped by social cognition in real time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01299v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CL</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanlin Wu, Xiaohui Rao, Zhenguang G. Cai</dc:creator>
    </item>
    <item>
      <title>Constructing Fundamentals for the Theory of Proportions and Symbolic Allusions Applied Interdisciplinarily</title>
      <link>https://arxiv.org/abs/2502.00780</link>
      <description>arXiv:2502.00780v1 Announce Type: cross 
Abstract: The Theory of Proportions and Symbolic Allusions applied Interdisciplinary (TPASAI) is a framework that integrates mathematics, linguistics, psychology, and game theory to uncover hidden patterns and proportions in reality. Its central idea is that numerical encoding of symbols, dates, and language can reveal recurring structures and connections that reflect universal principles. By applying fractal analysis, the theory identifies patterns across different scales, offering a unifying perspective on the structure of the world. One key aspect of TPASAI is symbolic analysis, which allows for the reinterpretation of traumatic experiences in psychotherapy. For example, assigning numerical values to elements like fingers, dates, or words can help individuals uncover meaningful associations between personal experiences and collective symbols. This approach encourages cognitive flexibility and provides a therapeutic avenue for recontextualizing emotions. The theory also incorporates principles of game theory, which frame reality as a system of symbolic "codes" governed by rules that can be understood and strategically used. This perspective is especially useful for psychological conditions like obsessive-compulsive disorder (OCD), enabling patients to approach their obsessions as decipherable patterns rather than rigid constraints. TPASAI has practical applications in psychology, education, and technology. In education, it aids in teaching mathematical and linguistic concepts by exploring connections between symbolic representations and real-world events. In technology, the methodology can be employed in ciphering and natural language processing. The innovation of TPASAI lies in its ability to merge the structured rigor of mathematics with the interpretative flexibility of symbolic analysis, offering a deeper understanding of events and relationships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00780v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diogen Babuc</dc:creator>
    </item>
    <item>
      <title>Deep generative computed perfusion-deficit mapping of ischaemic stroke</title>
      <link>https://arxiv.org/abs/2502.01334</link>
      <description>arXiv:2502.01334v1 Announce Type: cross 
Abstract: Focal deficits in ischaemic stroke result from impaired perfusion downstream of a critical vascular occlusion. While parenchymal lesions are traditionally used to predict clinical deficits, the underlying pattern of disrupted perfusion provides information upstream of the lesion, potentially yielding earlier predictive and localizing signals. Such perfusion maps can be derived from routine CT angiography (CTA) widely deployed in clinical practice. Analysing computed perfusion maps from 1,393 CTA-imaged-patients with acute ischaemic stroke, we use deep generative inference to localise neural substrates of NIHSS sub-scores. We show that our approach replicates known lesion-deficit relations without knowledge of the lesion itself and reveals novel neural dependents. The high achieved anatomical fidelity suggests acute CTA-derived computed perfusion maps may be of substantial clinical-and-scientific value in rich phenotyping of acute stroke. Using only hyperacute imaging, deep generative inference could power highly expressive models of functional anatomical relations in ischaemic stroke within the pre-interventional window.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01334v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chayanin Tangwiriyasakul, Pedro Borges, Guilherme Pombo, Stefano Moriconi, Michael S. Elmalem, Paul Wright, Yee-Haur Mah, Jane Rondina, Robert Gray, Sebastien Ourselin, Parashkev Nachev, M. Jorge Cardoso</dc:creator>
    </item>
    <item>
      <title>A Relative Homology Theory of Representation in Neural Networks</title>
      <link>https://arxiv.org/abs/2502.01360</link>
      <description>arXiv:2502.01360v1 Announce Type: cross 
Abstract: Previous research has proven that the set of maps implemented by neural networks with a ReLU activation function is identical to the set of piecewise linear continuous maps. Furthermore, such networks induce a hyperplane arrangement splitting the input domain into convex polyhedra $G_J$ over which the network $\Phi$ operates in an affine manner.
  In this work, we leverage these properties to define the equivalence class of inputs $\sim_\Phi$, which can be split into two sets related to the local rank of $\Phi_J$ and the intersections $\cap \text{Im}\Phi_{J_i}$. We refer to the latter as the overlap decomposition $O_\Phi$ and prove that if the intersections between each polyhedron and the input manifold are convex, the homology groups of neural representations are isomorphic to relative homology groups $H_k(\Phi(M)) \simeq H_k(M,O_\Phi)$. This lets us compute Betti numbers without the choice of an external metric. We develop methods to numerically compute the overlap decomposition through linear programming and a union-find algorithm.
  Using this framework, we perform several experiments on toy datasets showing that, compared to standard persistent homology, our relative homology-based computation of Betti numbers tracks purely topological rather than geometric features. Finally, we study the evolution of the overlap decomposition during training on various classification problems while varying network width and depth and discuss some shortcomings of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01360v1</guid>
      <category>cs.LG</category>
      <category>math.AT</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kosio Beshkov</dc:creator>
    </item>
    <item>
      <title>Structural features of the fly olfactory circuit mitigate the stability-plasticity dilemma in continual learning</title>
      <link>https://arxiv.org/abs/2502.01427</link>
      <description>arXiv:2502.01427v1 Announce Type: cross 
Abstract: Artificial neural networks face the stability-plasticity dilemma in continual learning, while the brain can maintain memories and remain adaptable. However, the biological strategies for continual learning and their potential to inspire learning algorithms in neural networks are poorly understood. This study presents a minimal model of the fly olfactory circuit to investigate the biological strategies that support continual odor learning. We introduce the fly olfactory circuit as a plug-and-play component, termed the Fly Model, which can integrate with modern machine learning methods to address this dilemma. Our findings demonstrate that the Fly Model enhances both memory stability and learning plasticity, overcoming the limitations of current continual learning strategies. We validated its effectiveness across various challenging continual learning scenarios using commonly used datasets. The fly olfactory system serves as an elegant biological circuit for lifelong learning, offering a module that enhances continual learning with minimal additional computational cost for machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01427v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Heming Zou, Yunliang Zang, Xiangyang Ji</dc:creator>
    </item>
    <item>
      <title>An Operating Principle of the Cerebral Cortex, and a Cellular Mechanism for Attentional Trial-and-Error Pattern Learning and Useful Classification Extraction</title>
      <link>https://arxiv.org/abs/2309.10821</link>
      <description>arXiv:2309.10821v4 Announce Type: replace 
Abstract: A feature of the brains of intelligent animals is the ability to learn to respond to an ensemble of active neuronal inputs with a behaviorally appropriate ensemble of active neuronal outputs. Previously, a hypothesis was proposed on how this mechanism is implemented at the cellular level within the neocortical pyramidal neuron: the apical tuft or perisomatic inputs initiate "guess" neuron firings, while the basal dendrites identify input patterns based on excited synaptic clusters, with the cluster excitation strength adjusted based on reward feedback. This simple mechanism allows neurons to learn to classify their inputs in a surprisingly intelligent manner. Here, we revise and extend this hypothesis. We modify synaptic plasticity rules to align with behavioral time scale synaptic plasticity (BTSP) observed in hippocampal area CA1, making the framework more biophysically and behaviorally plausible. The neurons for the guess firings are selected in a voluntary manner via feedback connections to apical tufts in the neocortical layer 1, leading to dendritic Ca2+ spikes with burst firing, which are postulated to be neural correlates of attentional, aware processing. Once learned, the neuronal input classification is executed without voluntary or conscious control, enabling hierarchical incremental learning of classifications that is effective in our inherently classifiable world. In addition to voluntary, we propose that pyramidal neuron burst firing can be involuntary, also initiated via apical tuft inputs, drawing attention towards important cues such as novelty and noxious stimuli. We classify the excitations of neocortical pyramidal neurons into four categories based on their excitation pathway: attentional versus automatic and voluntary/acquired versus involuntary. Additionally, we hypothesize that dendrites within pyramidal neuron minicolumn bundles are coupled via depolarization...</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10821v4</guid>
      <category>q-bio.NC</category>
      <category>physics.bio-ph</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.3389/fncir.2024.1280604</arxiv:DOI>
      <arxiv:journal_reference>Front. Neural Circuits 18:1280604 (2024)</arxiv:journal_reference>
      <dc:creator>Marat M. Rvachev</dc:creator>
    </item>
    <item>
      <title>Two types of pyramidal cells and their role in temporal processing</title>
      <link>https://arxiv.org/abs/2312.07422</link>
      <description>arXiv:2312.07422v2 Announce Type: replace 
Abstract: Recent work has provided new insights into the temporal specialization of Intratelencephalic (IT) and Pyramidal tract neurons (PT). However, functional and anatomical differences of IT and PT have not been connected yet. This perspective article contributes by highlighting empirical studies about the connectivity of IT and PT as well as their specialization in sensory and motor processing in diverse brain regions. We further review conceptual models that would align with the connectivity motif. We conclude that further research is needed to understand the role of the unidirectional connectivity from IT to PT in temporal processing and suggest concrete experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07422v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anh Duong Vo, Elisabeth Abs, Pau Vilimelis Aceituno, Benjamin Friedrich Grewe, Katharina Anna Wilmes</dc:creator>
    </item>
    <item>
      <title>Brain-Inspired AI with Hyperbolic Geometry</title>
      <link>https://arxiv.org/abs/2409.12990</link>
      <description>arXiv:2409.12990v3 Announce Type: replace 
Abstract: Artificial neural networks (ANNs) were inspired by the architecture and functions of the human brain and have revolutionised the field of artificial intelligence (AI). Inspired by studies on the latent geometry of the brain, in this perspective paper we posit that an increase in the research and application of hyperbolic geometry in ANNs and machine learning will lead to increased accuracy, improved feature space representations and more efficient models across a range of tasks. We examine the structure and functions of the human brain, emphasising the correspondence between its scale-free hierarchical organization and hyperbolic geometry, and reflecting on the central role hyperbolic geometry plays in facilitating human intelligence. Empirical evidence indicates that hyperbolic neural networks outperform Euclidean models for tasks including natural language processing, computer vision and complex network analysis, requiring fewer parameters and exhibiting better generalisation. Despite its nascent adoption, hyperbolic geometry holds promise for improving machine learning models through brain-inspired geometric representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12990v3</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Joseph, Nathan Francis, Meijke Balay</dc:creator>
    </item>
    <item>
      <title>Single-neuron deep generative model uncovers underlying physics of neuronal activity in Ca imaging data</title>
      <link>https://arxiv.org/abs/2501.14615</link>
      <description>arXiv:2501.14615v2 Announce Type: replace 
Abstract: Calcium imaging has become a powerful alternative to electrophysiology for studying neuronal activity, offering spatial resolution and the ability to measure large populations of neurons in a minimally invasive manner. This technique has broad applications in neuroscience, neuroengineering, and medicine, enabling researchers to explore the relationship between neuron location and activity. Recent advancements in deep generative models (DGMs) have facilitated the modeling of neuronal population dynamics, uncovering latent representations that provide insights into behavior prediction and neuronal variance. However, these models often rely on spike inference algorithms and primarily focus on population-level dynamics, limiting their applicability for single-neuron analyses. To address this gap, we propose a novel framework for single-neuron representation learning using autoregressive variational autoencoders (AVAEs). Our approach embeds individual neurons' spatiotemporal signals into a reduced-dimensional space without the need for spike inference algorithms. The AVAE excels over traditional linear methods by generating more informative and discriminative latent representations, improving tasks such as visualization, clustering, and the understanding of neuronal activity. Additionally, the reconstruction performance of the AVAE outperforms the state of the art, demonstrating its ability to accurately recover the original fluorescence signal from the learned representation. Using realistic simulations, we show that our model captures underlying physical properties and connectivity patterns, enabling it to distinguish between different firing and connectivity types. These findings position the AVAE as a versatile and powerful tool for advancing single-neuron analysis and lays the groundwork for future integration of multimodal single-cell datasets in neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14615v2</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordi Abante, Angelo Piga, Berta Ros, Clara F L\'opez-Le\'on, Josep M Canals, Jordi Soriano</dc:creator>
    </item>
    <item>
      <title>Discovering robust biomarkers of psychiatric disorders from resting-state functional MRI via graph neural networks: A systematic review</title>
      <link>https://arxiv.org/abs/2405.00577</link>
      <description>arXiv:2405.00577v2 Announce Type: replace-cross 
Abstract: Graph neural networks (GNN) have emerged as a popular tool for modelling functional magnetic resonance imaging (fMRI) datasets. Many recent studies have reported significant improvements in disorder classification performance via more sophisticated GNN designs and highlighted salient features that could be potential biomarkers of the disorder. However, existing methods of evaluating their robustness are often limited to cross-referencing with existing literature, which is a subjective and inconsistent process. In this review, we provide an overview of how GNN and model explainability techniques (specifically, feature attributors) have been applied to fMRI datasets for disorder prediction tasks, with an emphasis on evaluating the robustness of potential biomarkers produced for psychiatric disorders. Then, 65 studies using GNNs that reported potential fMRI biomarkers for psychiatric disorders (attention-deficit hyperactivity disorder, autism spectrum disorder, major depressive disorder, schizophrenia) published before 9 October 2024 were identified from 2 online databases (Scopus, PubMed). We found that while most studies have performant models, salient features highlighted in these studies (as determined by feature attribution scores) vary greatly across studies on the same disorder. Reproducibility of biomarkers is only limited to a small subset at the level of regions and few transdiagnostic biomarkers were identified. To address these issues, we suggest establishing new standards that are based on objective evaluation metrics to determine the robustness of these potential biomarkers. We further highlight gaps in the existing literature and put together a prediction-attribution-evaluation framework that could set the foundations for future research on discovering robust biomarkers of psychiatric disorders via GNNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00577v2</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yi Hao Chan, Deepank Girish, Sukrit Gupta, Jing Xia, Chockalingam Kasi, Yinan He, Conghao Wang, Jagath C. Rajapakse</dc:creator>
    </item>
    <item>
      <title>Prescribed exponential stabilization of scalar neutral differential equations: Application to neural control</title>
      <link>https://arxiv.org/abs/2406.13730</link>
      <description>arXiv:2406.13730v4 Announce Type: replace-cross 
Abstract: This paper presents a control-oriented delay-based modeling approach for the exponential stabilization of a scalar neutral functional differential equation, which is then applied to the local exponential stabilization of a one-layer neural network of Hopfield type with delayed feedback. The proposed approach utilizes a recently developed partial pole placement method for linear functional differential equations, leveraging the coexistence of real spectral values to explicitly prescribe the exponential decay of the closed-loop solution. While a delayed proportional (P) feedback control may achieve stabilization, it requires higher gains and only allows for a shorter maximum delay compared to the proportional-derivative (PD) feedback control presented in this work. The framework provides a practical illustration of the stabilization strategy, improving upon previous literature results that characterize the solution's exponential decay for simple real spectral values. This approach enhances neural stability in cases where the inherent dynamics are stable and offers a method to achieve local exponential stabilization with a prescribed decay rate when the inherent dynamics are unstable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13730v4</guid>
      <category>math.SP</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cyprien Tamekue, Islam Boussaada, Karim Trabelsi</dc:creator>
    </item>
    <item>
      <title>Metareasoning in uncertain environments: a meta-BAMDP framework</title>
      <link>https://arxiv.org/abs/2408.01253</link>
      <description>arXiv:2408.01253v2 Announce Type: replace-cross 
Abstract: \textit{Reasoning} may be viewed as an algorithm $P$ that makes a choice of an action $a^* \in \mathcal{A}$, aiming to optimize some outcome. However, executing $P$ itself bears costs (time, energy, limited capacity, etc.) and needs to be considered alongside explicit utility obtained by making the choice in the underlying decision problem. Finding the right $P$ can itself be framed as an optimization problem over the space of reasoning processes $P$, generally referred to as \textit{metareasoning}. Conventionally, human metareasoning models assume that the agent knows the transition and reward distributions of the underlying MDP. This paper generalizes such models by proposing a meta Bayes-Adaptive MDP (meta-BAMDP) framework to handle metareasoning in environments with unknown reward/transition distributions, which encompasses a far larger and more realistic set of planning problems that humans and AI systems face. As a first step, we apply the framework to Bernoulli bandit tasks. Owing to the meta problem's complexity, our solutions are necessarily approximate. However, we introduce two novel theorems that significantly enhance the tractability of the problem, enabling stronger approximations that are robust within a range of assumptions grounded in realistic human decision-making scenarios. These results offer a resource-rational perspective and a normative framework for understanding human exploration under cognitive constraints, as well as providing experimentally testable predictions about human behavior in Bernoulli Bandit tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01253v2</guid>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prakhar Godara, Tilman Diego Al\'eman, Angela J. Yu</dc:creator>
    </item>
    <item>
      <title>Artificial Human Intelligence: The role of Humans in the Development of Next Generation AI</title>
      <link>https://arxiv.org/abs/2409.16001</link>
      <description>arXiv:2409.16001v2 Announce Type: replace-cross 
Abstract: Human intelligence, the most evident and accessible form of source of reasoning, hosted by biological hardware, has evolved and been refined over thousands of years, positioning itself today to create new artificial forms and preparing to self--design their evolutionary path forward. Beginning with the advent of foundation models, the rate at which human and artificial intelligence interact with each other has exceeded any anticipated quantitative figures. The close engagement led both bits of intelligence to be impacted in various ways, which naturally resulted in complex confluences that warrant close scrutiny. In the sequel, using a novel taxonomy, we shall explore the interplay between human and machine intelligence, focusing on the crucial role humans play in developing ethical, responsible, and robust intelligent systems. We briefly delve into various aspects of implementation inspired by the mechanisms underlying neuroscience and human cognition. In addition, we propose future perspectives, capitalizing on the advantages of symbiotic designs to suggest a human-centered direction for next-generation developments, focusing on the augmentation role of AI. We finalize this evolving document with some thoughts and open questions yet to be addressed by the broader community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16001v2</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suayb S. Arslan</dc:creator>
    </item>
    <item>
      <title>Covariance Regression for High Dimensional Neural Data via Graph</title>
      <link>https://arxiv.org/abs/2409.19717</link>
      <description>arXiv:2409.19717v2 Announce Type: replace-cross 
Abstract: Modern recording techniques enable neuroscientists to simultaneously study neural activity across large populations of neurons, with capturing predictor-dependent correlations being a fundamental challenge in neuroscience. Moreover, the fact that input covariates often lie in restricted subdomains, according to experimental settings, makes inference even more challenging. To address these challenges, we propose a set of nonparametric mean-covariance regression models for high-dimensional neural activity with restricted inputs. These models reduce the dimensionality of neural responses by employing a lower-dimensional latent factor model, where both factor loadings and latent factors are predictor-dependent, to jointly model mean and covariance across covariates. The smoothness of neural activity across experimental conditions is modeled nonparametrically using two Gaussian processes (GPs), applied to both loading basis and latent factors. Additionally, to account for the covariates lying in restricted subspace, we incorporate graph information into the covariance structure. To flexibly infer the model, we use an MCMC algorithm to sample from posterior distributions. After validating and studying the properties of proposed methods by simulations, we apply them to two neural datasets (local field potential and neural spiking data) to demonstrate the usage of models for continuous and counting observations. Overall, the proposed methods provide a framework to jointly model covariate-dependent mean and covariance in high dimensional neural data, especially when the covariates lie in restricted domains. The framework is general and can be easily adapted to various applications beyond neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19717v2</guid>
      <category>stat.AP</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 04 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ganchao Wei</dc:creator>
    </item>
  </channel>
</rss>
