<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Oct 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Accelerating Bayesian Inference via Multi-Fidelity Transport Map Coupling</title>
      <link>https://arxiv.org/abs/2510.17946</link>
      <description>arXiv:2510.17946v1 Announce Type: new 
Abstract: Mathematical models in computational physics contain uncertain parameters that impact prediction accuracy. In turbulence modeling, this challenge is especially significant: Reynolds averaged Navier-Stokes (RANS) models, such as the Spalart-Allmaras (SA) model, are widely used for their speed and robustness but often suffer from inaccuracies and associated uncertainties due to imperfect model parameters. Reliable quantification of these uncertainties is becoming increasingly important in aircraft certification by analysis, where predictive credibility is critical. Bayesian inference provides a framework to estimate these parameters and quantify output uncertainty, but traditional methods are prohibitively expensive, especially when relying on high-fidelity simulations. We address the challenge of expensive Bayesian parameter estimation by developing a multi-fidelity framework that combines Markov chain Monte Carlo (MCMC) methods with multilevel Monte Carlo (MLMC) estimators to efficiently solve inverse problems. The MLMC approach requires correlated samples across different fidelity levels, achieved through a novel transport map-based coupling algorithm. We demonstrate a 50% reduction in inference cost compared to traditional single-fidelity methods on the challenging NACA0012 airfoil at high angles of attack near stall, while delivering realistic uncertainty bounds for model predictions in complex separated flow regimes. These results demonstrate that multi-fidelity approaches significantly improve turbulence parameter calibration, paving the way for more accurate and efficient aircraft certification by analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17946v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanjan C. Muchandimath, Joaquim R. R. A. Martins, Alex A. Gorodetsky</dc:creator>
    </item>
    <item>
      <title>Assessing Monotone Dependence: Area Under the Curve Meets Rank Correlation</title>
      <link>https://arxiv.org/abs/2510.17994</link>
      <description>arXiv:2510.17994v1 Announce Type: new 
Abstract: The assessment of monotone dependence between random variables $X$ and $Y$ is a classical problem in statistics and a gamut of application domains. Consequently, researchers have sought measures of association that are invariant under strictly increasing transformations of the margins, with the extant literature being splintered. Rank correlation coefficients, such as Spearman's Rho and Kendall's Tau, have been studied at great length in the statistical literature, mostly under the assumption that $X$ and $Y$ are continuous. In the case of a dichotomous outcome $Y$, receiver operating characteristic analysis and the asymmetric area under the curve (AUC) measure are used to assess monotone dependence of $Y$ on a covariate $X$. Here we unify and extend thus far disconnected strands of literature, by developing common population level theory, estimators, and tests that bridge continuous and dichotomous settings and apply to all linearly ordered outcomes. In particular, we introduce asymmetric grade correlation, AGC$(X,Y)$, as the covariance of the mid distribution function transforms, or grades, of $X$ and $Y$, divided by the variance of the grade of $Y$. The coefficient of monotone association then is CMA$(X,Y) = \frac{1}{2} ($AGC$(X,Y) + 1)$. When $X$ and $Y$ are continuous, AGC is symmetric and equals Spearman's Rho. When $Y$ is dichotomous, CMA equals AUC. We establish central limit theorems for the sample versions of AGC and CMA and develop a test of DeLong type for the equality of AGC or CMA values with a shared outcome $Y$. In case studies, we apply the new measures to assess progress in data-driven weather prediction, and to evaluate methods of uncertainty quantification for large language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17994v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eva-Maria Walz, Andreas Eberl, Tilmann Gneiting</dc:creator>
    </item>
    <item>
      <title>Cartesian Statistics on Spheres</title>
      <link>https://arxiv.org/abs/2510.18068</link>
      <description>arXiv:2510.18068v1 Announce Type: new 
Abstract: Directional data consists of unit vectors in q-dimensions that can be described in polar or Cartesian coordinates. Axial data can be viewed as a pair of directions pointed in opposite directions or as a projection matrix of rank 1. Historically, their statistical analysis has largely been based on a few low-order exponential family models of distributions for random directions or axes. A lack of tractable algebraic forms for the normalizing constants has hindered the use of higher-order exponential families for less constrained modeling. Of interest are functionals of the unknown distribution of the directional/axial data, such as the directional/axial mean, dispersion, or distribution itself. This paper outlines nonparametric estimators and bootstrap confidence sets for such functionals. The procedures are based on the empirical distribution of the directional/axial sample expressed in Cartesian coordinates. Sketched as well are nonparametric comparisons among multiple mean directions or axes, estimation of trend in mean directions, and analysis of q-dimensional observations restricted to lie in a specified compact subset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18068v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rudolf Beran</dc:creator>
    </item>
    <item>
      <title>Adaptive Grid-Based Thompson Sampling for Efficient Trajectory Discovery</title>
      <link>https://arxiv.org/abs/2510.18099</link>
      <description>arXiv:2510.18099v1 Announce Type: new 
Abstract: Bayesian optimization (BO) is a powerful framework for estimating parameters of computationally expensive simulation models, particularly in settings where the likelihood is intractable and evaluations are costly. In stochastic models every simulation is run with a specific parameter set and an implicit or explicit random seed, where each parameter set and random seed combination generates an individual realization, or trajectory, sampled from an underlying random process. Existing BO approaches typically rely on summary statistics over the realizations, such as means, medians, or quantiles, potentially limiting their effectiveness when trajectory-level information is desired. We propose a trajectory-oriented Bayesian optimization method that incorporates a Gaussian process (GP) surrogate using both input parameters and random seeds as inputs, enabling direct inference at the trajectory level. Using a common random number (CRN) approach, we define a surrogate-based likelihood over trajectories and introduce an adaptive Thompson Sampling algorithm that refines a fixed-size input grid through likelihood-based filtering and Metropolis-Hastings-based densification. This approach concentrates computation on statistically promising regions of the input space while balancing exploration and exploitation. We apply the method to stochastic epidemic models, a simple compartmental and a more computationally demanding agent-based model, demonstrating improved sampling efficiency and faster identification of data-consistent trajectories relative to parameter-only inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18099v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arindam Fadikar, Abby Stevens, Mickael Binois, Nicholson Collier, Jonathan Ozik</dc:creator>
    </item>
    <item>
      <title>Copula Structural Equation Models for Mediation Pathway Analysis</title>
      <link>https://arxiv.org/abs/2510.18115</link>
      <description>arXiv:2510.18115v1 Announce Type: new 
Abstract: Structural equation models (SEMs) are fundamental to causal mediation pathway discovery. However, traditional SEM approaches often rely on \emph{ad hoc} model specifications when handling complex data structures such as mixed data types or non-normal data in which Gaussian assumptions for errors are rather restrictive. The invocation of copula dependence modeling methods to extend the classical linear SEMs mitigates several of key technical limitations, offering greater modeling flexibility to analyze non-Gaussian data. This paper presents a selective review of major developments in this area, highlighting recent advancements and their methodological implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18115v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Canyi Chen, Ritoban Kundu, Wei Hao, Peter X. -K. Song</dc:creator>
    </item>
    <item>
      <title>Conformal Inference For Missing Data under Multiple Robust Learning</title>
      <link>https://arxiv.org/abs/2510.18149</link>
      <description>arXiv:2510.18149v1 Announce Type: new 
Abstract: We develop a novel approach to tackle the common but challenging problem of conformal inference for missing data in machine learning, focusing on Missing at Random (MAR) data. We propose a new procedure Conformal prediction for Missing data under Multiple Robust Learning (CM--MRL) that combines split conformal calibration with a multiple robust empirical-likelihood (EL) reweighting scheme. The method proceeds via a double calibration by reweighting the complete-case scores by EL so that their distribution matches the full calibration distribution implied by MAR, even when some working models are misspecified. We demonstrate the asymptotic behavior of our estimators through empirical process theory and provide reliable coverage for our prediction intervals, both marginally and conditionally and we further show an interval-length dominance result. We show the effectiveness of the proposed method by several numerical experiments in the presence of missing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18149v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenlu Tang, Hongni Wang, Xingcai Zhou, Bei Jiang, Linglong Kong</dc:creator>
    </item>
    <item>
      <title>Non-Parametric Estimation Techniques of Factor Copula Model using Proxies</title>
      <link>https://arxiv.org/abs/2510.18241</link>
      <description>arXiv:2510.18241v1 Announce Type: new 
Abstract: Parametric factor copula models typically work well in modeling multivariate dependencies due to their flexibility and ability to capture complex dependency structures. However, accurately estimating the linking copulas within these mod- els remains challenging, especially when working with high-dimensional data. This paper proposes a novel approach for estimating linking copulas based on a non-parametric kernel estimator. Unlike conventional parametric methods, our approach utilizes the flexibility of kernel density estimation to capture the un- derlying dependencies more accurately, particularly in scenarios where the un- derlying copula structure is complex or unknown. We show that the proposed estimator is consistent under mild conditions and demonstrate its effectiveness through extensive simulation studies. Our findings suggest that the proposed approach offers a promising avenue for modeling multivariate dependencies, par- ticularly in applications requiring robust and efficient estimation of copula-based models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18241v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bahareh Ghanbari, Pavel Krupskiy, Laleh Tafakori, Yan Wang</dc:creator>
    </item>
    <item>
      <title>Quantifying Periodicity in Non-Euclidean Random Objects</title>
      <link>https://arxiv.org/abs/2510.18247</link>
      <description>arXiv:2510.18247v1 Announce Type: new 
Abstract: Time-varying non-Euclidean random objects are playing a growing role in modern data analysis, and periodicity is a fundamental characteristic of time-varying data. However, quantifying periodicity in general non-Euclidean random objects remains largely unexplored. In this work, we introduce a novel nonparametric framework for quantifying periodicity in random objects within a general metric space that lacks Euclidean structures. Our approach formulates periodicity estimation as a model selection problem and provides methodologies for period estimation, data-driven tuning parameter selection, and periodic component extraction. Our theoretical contributions include establishing the consistency of period estimation without relying on linearity properties used in the literature for Euclidean data, providing theoretical support for data-driven tuning parameter selection, and deriving uniform convergence results for periodic component estimation. Through extensive simulation studies covering three distinct types of time-varying random objects such as compositional data, networks, and functional data, we showcase the superior accuracy achieved by our approach in periodicity quantification. Finally, we apply our method to various real datasets, including U.S. electricity generation compositions, New York City transportation networks, and Germany's water consumption curves, highlighting its practical relevance in identifying and quantifying meaningful periodic patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18247v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiazhen Xu, Andrew T. A. Wood, Tao Zou</dc:creator>
    </item>
    <item>
      <title>Measuring deviations from spherical symmetry</title>
      <link>https://arxiv.org/abs/2510.18598</link>
      <description>arXiv:2510.18598v1 Announce Type: new 
Abstract: Most of the work on checking spherical symmetry assumptions on the distribution of the $p$-dimensional random vector $Y$ has its focus on statistical tests for the null hypothesis of exact spherical symmetry. In this paper, we take a different point of view and propose a measure for the deviation from spherical symmetry, which is based on the minimum distance between the distribution of the vector $\big (\|Y\|, Y/ \|Y\| )^\top $ and its best approximation by a distribution of a vector $\big (\|Y_s\|, Y_s/ \|Y_s \| )^\top $ corresponding to a random vector $Y_s$ with a spherical distribution. We develop estimators for the minimum distance with corresponding statistical guarantees (provided by asymptotic theory) and demonstrate the applicability of our approach by means of a simulation study and a real data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18598v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lujia Bai, Holger Dette</dc:creator>
    </item>
    <item>
      <title>A new implementation of Network GARCH Model</title>
      <link>https://arxiv.org/abs/2510.18599</link>
      <description>arXiv:2510.18599v1 Announce Type: new 
Abstract: Volatility clustering and spillovers are key features of real-world financial time series when there are a lot of cross-sectional financial assets. While network analysis helps connect stocks that are 'similar' or 'correlated', which is effective to link volatility spillovers between stocks, contemporary multivariate ARCH-GARCH formulations struggle to represent structured network dependence and remain parsimonious. We introduce the Generalised Network GARCH (GNGARCH) model as a network volatility model that embeds the GARCH dynamics within the Generalised Network Autoregressive (GNAR) framework, to capture the dynamic volatility of financial asset return by both the asset itself and its 'neighbouring' assets from the constructed virtual network. The proposed volatility model GNGARCH also addresses the limitations for current studies of network GARCH by adapting neighbouring volatility persistence, dynamic conditional covariance updates, and allowing higher-order neighbouring effects rather than only immediate neighbours. This paper provides the model derivation, vectorisation and conversion, stationarity conditions, and also an extension by incorporating threshold coefficients to capture leverage effects. We show that the GNGARCH is a valid volatility model satisfying the stylised facts of financial return series through simulation. Parameter estimation is then performed by using squared returns as variance proxy and minimising a loss function that is either mean squared error (MSE) or quasi-likelihood (QLIKE). We apply our model on 75 of the most active US stocks under a virtual network, and highlight the model's ability in volatility estimation and forecast.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18599v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peiyi Zhou</dc:creator>
    </item>
    <item>
      <title>Differentially Private E-Values</title>
      <link>https://arxiv.org/abs/2510.18654</link>
      <description>arXiv:2510.18654v1 Announce Type: new 
Abstract: E-values have gained prominence as flexible tools for statistical inference and risk control, enabling anytime- and post-hoc-valid procedures under minimal assumptions. However, many real-world applications fundamentally rely on sensitive data, which can be leaked through e-values. To ensure their safe release, we propose a general framework to transform non-private e-values into differentially private ones. Towards this end, we develop a novel biased multiplicative noise mechanism that ensures our e-values remain statistically valid. We show that our differentially private e-values attain strong statistical power, and are asymptotically as powerful as their non-private counterparts. Experiments across online risk monitoring, private healthcare, and conformal e-prediction demonstrate our approach's effectiveness and illustrate its broad applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18654v1</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Csillag, Diego Mesquita</dc:creator>
    </item>
    <item>
      <title>Testing Risk Difference of Two Proportions for Combined Unilateral and Bilateral Data</title>
      <link>https://arxiv.org/abs/2510.18834</link>
      <description>arXiv:2510.18834v1 Announce Type: new 
Abstract: In clinical studies with paired organs, binary outcomes often exhibit intra-subject correlation and may include a mixture of unilateral and bilateral observations. Under Donner's constant correlation model, we develop three likelihood-based test statistics (the likelihood ratio, Wald-type, and score tests) for assessing the risk difference between two proportions. Simulation studies demonstrate good control of type I error and comparable power among the three tests, with the score test showing slightly better stability. Applications to otolaryngologic and ophthalmologic data illustrate the methods. An online calculator is also provided for power analysis and risk difference testing. The score test is recommended for practical use and future studies with combined unilateral and bilateral binary data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18834v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jia Zhou, Chang-Xing Ma</dc:creator>
    </item>
    <item>
      <title>Inference on Local Variable Importance Measures for Heterogeneous Treatment Effects</title>
      <link>https://arxiv.org/abs/2510.18843</link>
      <description>arXiv:2510.18843v1 Announce Type: new 
Abstract: We provide an inferential framework to assess variable importance for heterogeneous treatment effects. This assessment is especially useful in high-risk domains such as medicine, where decision makers hesitate to rely on black-box treatment recommendation algorithms. The variable importance measures we consider are local in that they may differ across individuals, while the inference is global in that it tests whether a given variable is important for any individual. Our approach builds on recent developments in semiparametric theory for function-valued parameters, and is valid even when statistical machine learning algorithms are employed to quantify treatment effect heterogeneity. We demonstrate the applicability of our method to infectious disease prevention strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18843v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pawel Morzywolek, Peter B. Gilbert, Alex Luedtke</dc:creator>
    </item>
    <item>
      <title>Measure-Theoretic Anti-Causal Representation Learning</title>
      <link>https://arxiv.org/abs/2510.18052</link>
      <description>arXiv:2510.18052v1 Announce Type: cross 
Abstract: Causal representation learning in the anti-causal setting (labels cause features rather than the reverse) presents unique challenges requiring specialized approaches. We propose Anti-Causal Invariant Abstractions (ACIA), a novel measure-theoretic framework for anti-causal representation learning. ACIA employs a two-level design, low-level representations capture how labels generate observations, while high-level representations learn stable causal patterns across environment-specific variations. ACIA addresses key limitations of existing approaches by accommodating prefect and imperfect interventions through interventional kernels, eliminating dependency on explicit causal structures, handling high-dimensional data effectively, and providing theoretical guarantees for out-of-distribution generalization. Experiments on synthetic and real-world medical datasets demonstrate that ACIA consistently outperforms state-of-the-art methods in both accuracy and invariance metrics. Furthermore, our theoretical results establish tight bounds on performance gaps between training and unseen environments, confirming the efficacy of our approach for robust anti-causal learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18052v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arman Behnam, Binghui Wang</dc:creator>
    </item>
    <item>
      <title>Principled Argo Modeling using Vecchia-based Gaussian Processes</title>
      <link>https://arxiv.org/abs/2510.18067</link>
      <description>arXiv:2510.18067v1 Announce Type: cross 
Abstract: Argo is an international program that collects temperature and salinity observations in the upper two kilometers of the global ocean. Most existing approaches for modeling Argo temperature rely on spatial partitioning, where data are locally modeled by first estimating a prescribed mean structure and then fitting Gaussian processes (GPs) to the mean-subtracted anomalies. Such strategies introduce challenges in designing suitable mean structures and defining domain partitions, often resulting in ad hoc modeling choices. In this work, we propose a one-stop Gaussian process regression framework with a generic spatio-temporal covariance function to jointly model Argo temperature data across broad spatial domains. Our fully data-driven approach achieves superior predictive performance compared with methods that require domain partitioning or parametric regression. To ensure scalability over large spatial regions, we employ the Vecchia approximation, which reduces the computational complexity from cubic to quasi-linear in the number of observations while preserving predictive accuracy. Using Argo data from January to March over the years 2007-2016, the same dataset used in prior benchmark studies, we demonstrate that our approach provides a principled, scalable, and interpretable tool for large-scale oceanographic analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18067v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nian Liu, Jian Cao</dc:creator>
    </item>
    <item>
      <title>Arbitrated Indirect Treatment Comparisons</title>
      <link>https://arxiv.org/abs/2510.18071</link>
      <description>arXiv:2510.18071v1 Announce Type: cross 
Abstract: Matching-adjusted indirect comparison (MAIC) has been increasingly employed in health technology assessments (HTA). By reweighting subjects from a trial with individual participant data (IPD) to match the covariate summary statistics of another trial with only aggregate data (AgD), MAIC facilitates the estimation of a treatment effect defined with respect to the AgD trial population. This manuscript introduces a new class of methods, termed arbitrated indirect treatment comparisons, designed to address the ``MAIC paradox'' -- a phenomenon highlighted by Jiang et al.~(2025). The MAIC paradox arises when different sponsors, analyzing the same data, reach conflicting conclusions regarding which treatment is more effective. The underlying issue is that each sponsor implicitly targets a different population. To resolve this inconsistency, the proposed methods focus on estimating treatment effects in a common target population, specifically chosen to be the overlap population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18071v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixin Fang, Weili He</dc:creator>
    </item>
    <item>
      <title>On the Kolmogorov Distance of Max-Stable Distributions</title>
      <link>https://arxiv.org/abs/2510.18094</link>
      <description>arXiv:2510.18094v1 Announce Type: cross 
Abstract: In this contribution, we derive explicit bounds on the Kolmogorov distance for multivariate max-stable distributions with Fr\'echet margins. We formulate those bounds in terms of (i) Wasserstein distances between de Haan representers, (ii) total variation distances between spectral/angular measures - removing the dimension factor from earlier results in the canonical sphere case - and (iii) discrepancies of the Psi-functions in the inf-argmax decomposition. Extensions to different margins and Archimax/clustered Archimax copulas are further discussed. Examples include logistic, comonotonic, independent and Brown-Resnick models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18094v1</guid>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enkelejd Hashorva</dc:creator>
    </item>
    <item>
      <title>The Picard-Lagrange Framework for Higher-Order Langevin Monte Carlo</title>
      <link>https://arxiv.org/abs/2510.18242</link>
      <description>arXiv:2510.18242v1 Announce Type: cross 
Abstract: Sampling from log-concave distributions is a central problem in statistics and machine learning. Prior work establishes theoretical guarantees for Langevin Monte Carlo algorithm based on overdamped and underdamped Langevin dynamics and, more recently, some third-order variants. In this paper, we introduce a new sampling algorithm built on a general $K$th-order Langevin dynamics, extending beyond second- and third-order methods. To discretize the $K$th-order dynamics, we approximate the drift induced by the potential via Lagrange interpolation and refine the node values at the interpolation points using Picard-iteration corrections, yielding a flexible scheme that fully utilizes the acceleration of higher-order Langevin dynamics. For targets with smooth, strongly log-concave densities, we prove dimension-dependent convergence in Wasserstein distance: the sampler achieves $\varepsilon$-accuracy within $\widetilde O(d^{\frac{K-1}{2K-3}}\varepsilon^{-\frac{2}{2K-3}})$ gradient evaluations for $K \ge 3$. To our best knowledge, this is the first sampling algorithm achieving such query complexity. The rate improves with the order $K$ increases, yielding better rates than existing first to third-order approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18242v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaideep Mahajan, Kaihong Zhang, Feng Liang, Jingbo Liu</dc:creator>
    </item>
    <item>
      <title>Consistency of Nonparametric Density Estimators in CAT(0) Orthant Space</title>
      <link>https://arxiv.org/abs/2510.18290</link>
      <description>arXiv:2510.18290v1 Announce Type: cross 
Abstract: The inference of evolutionary histories is a central problem in evolutionary biology. The analysis of a sample of phylogenetic trees can be conducted in Billera-Holmes-Vogtmann tree space, which is a CAT(0) metric space of phylogenetic trees. The globally non-positively curved (CAT(0)) property of this space enables the extension of various statistical techniques. In the problem of nonparametric density estimation, two primary methods, kernel density estimation and log-concave maximum likelihood estimation, have been proposed, yet their theoretical properties remain largely unexplored.
  In this paper, we address this gap by proving the consistency of these estimators in a more general setting$\unicode{x2014}$CAT(0) orthant spaces, which include BHV tree space. We extend log-concave approximation techniques to this setting and establish consistency via the continuity of the log-concave projection map. We also modify the kernel density estimator to correct boundary bias and establish uniform consistency using empirical process theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18290v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuki Takazawa, Tomonari Sei</dc:creator>
    </item>
    <item>
      <title>Towards Identifiability of Hierarchical Temporal Causal Representation Learning</title>
      <link>https://arxiv.org/abs/2510.18310</link>
      <description>arXiv:2510.18310v1 Announce Type: cross 
Abstract: Modeling hierarchical latent dynamics behind time series data is critical for capturing temporal dependencies across multiple levels of abstraction in real-world tasks. However, existing temporal causal representation learning methods fail to capture such dynamics, as they fail to recover the joint distribution of hierarchical latent variables from \textit{single-timestep observed variables}. Interestingly, we find that the joint distribution of hierarchical latent variables can be uniquely determined using three conditionally independent observations. Building on this insight, we propose a Causally Hierarchical Latent Dynamic (CHiLD) identification framework. Our approach first employs temporal contextual observed variables to identify the joint distribution of multi-layer latent variables. Sequentially, we exploit the natural sparsity of the hierarchical structure among latent variables to identify latent variables within each layer. Guided by the theoretical results, we develop a time series generative model grounded in variational inference. This model incorporates a contextual encoder to reconstruct multi-layer latent variables and normalize flow-based hierarchical prior networks to impose the independent noise condition of hierarchical latent dynamics. Empirical evaluations on both synthetic and real-world datasets validate our theoretical claims and demonstrate the effectiveness of CHiLD in modeling hierarchical latent dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18310v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zijian Li, Minghao Fu, Junxian Huang, Yifan Shen, Ruichu Cai, Yuewen Sun, Guangyi Chen, Kun Zhang</dc:creator>
    </item>
    <item>
      <title>Partial VOROS: A Cost-aware Performance Metric for Binary Classifiers with Precision and Capacity Constraints</title>
      <link>https://arxiv.org/abs/2510.18520</link>
      <description>arXiv:2510.18520v1 Announce Type: cross 
Abstract: The ROC curve is widely used to assess binary classification performance. Yet for some applications such as alert systems for hospitalized patient monitoring, conventional ROC analysis cannot capture crucial factors that impact deployment, such as enforcing a minimum precision constraint to avoid false alarm fatigue or imposing an upper bound on the number of predicted positives to represent the capacity of hospital staff. The usual area under the curve metric also does not reflect asymmetric costs for false positives and false negatives. In this paper we address all three of these issues. First, we show how the subset of classifiers that meet given precision and capacity constraints can be represented as a feasible region in ROC space. We establish the geometry of this feasible region. We then define the partial area of lesser classifiers, a performance metric that is monotonic with cost and only accounts for the feasible portion of ROC space. Averaging this area over a desired range of cost parameters results in the partial volume over the ROC surface, or partial VOROS. In experiments predicting mortality risk using vital sign history on the MIMIC-IV dataset, we show this cost-aware metric is better than alternatives for ranking classifiers in hospital alert applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18520v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher Ratigan, Kyle Heuton, Carissa Wang, Lenore Cowen, Michael C. Hughes</dc:creator>
    </item>
    <item>
      <title>Distributional regression for seasonal data: an application to river flows</title>
      <link>https://arxiv.org/abs/2510.18639</link>
      <description>arXiv:2510.18639v1 Announce Type: cross 
Abstract: Risk assessment in casualty insurance, such as flood risk, traditionally relies on extreme-value methods that emphasizes rare events. These approaches are well-suited for characterizing tail risk, but do not capture the broader dynamics of environmental variables such as moderate or frequent loss events. To complement these methods, we propose a modelling framework for estimating the full (daily) distribution of environmental variables as a function of time, that is a distributional version of typical climatological summary statistics, thereby incorporating both seasonal variation and gradual long-term changes. Aside from the time trend, to capture seasonal variation our approach simultaneously estimates the distribution for each instant of the seasonal cycle, without explicitly modelling the temporal dependence present in the data. To do so, we adopt a framework inspired by GAMLSS (Generalized Additive Models for Location, Scale, and Shape), where the parameters of the distribution vary over the seasonal cycle as a function of explanatory variables depending only on the time of year, and not on the past values of the process under study. Ignoring the temporal dependence in the seasonal variation greatly simplifies the modelling but poses inference challenges that we clarify and overcome.
  We apply our framework to daily river flow data from three hydrometric stations along the Fraser River in British Columbia, Canada, and analyse the flood of the Fraser River in early winter of 2021.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18639v1</guid>
      <category>stat.AP</category>
      <category>q-fin.RM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samuel Perreault, Silvana M. Pesenti, Daniyal Shahzad</dc:creator>
    </item>
    <item>
      <title>A Frequentist Statistical Introduction to Variational Inference, Autoencoders, and Diffusion Models</title>
      <link>https://arxiv.org/abs/2510.18777</link>
      <description>arXiv:2510.18777v1 Announce Type: cross 
Abstract: While Variational Inference (VI) is central to modern generative models like Variational Autoencoders (VAEs) and Denoising Diffusion Models (DDMs), its pedagogical treatment is split across disciplines. In statistics, VI is typically framed as a Bayesian method for posterior approximation. In machine learning, however, VAEs and DDMs are developed from a Frequentist viewpoint, where VI is used to approximate a maximum likelihood estimator. This creates a barrier for statisticians, as the principles behind VAEs and DDMs are hard to contextualize without a corresponding Frequentist introduction to VI. This paper provides that introduction: we explain the theory for VI, VAEs, and DDMs from a purely Frequentist perspective, starting with the classical Expectation-Maximization (EM) algorithm. We show how VI arises as a scalable solution for intractable E-steps and how VAEs and DDMs are natural, deep-learning-based extensions of this framework, thereby bridging the gap between classical statistical inference and modern generative AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18777v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yen-Chi Chen</dc:creator>
    </item>
    <item>
      <title>Comparison of Simulation-Guided Design to Closed-Form Power Calculations in Planning a Cluster Randomized Trial with Covariate-Constrained Randomization: A Case Study in Rural Chad</title>
      <link>https://arxiv.org/abs/2510.18818</link>
      <description>arXiv:2510.18818v1 Announce Type: cross 
Abstract: Current practices for designing cluster-randomized trials (cRCTs) typically rely on closed-form formulas for power calculations. For cRCTs using covariate-constrained randomization, the utility of conventional calculations might be limited, particularly when data is nested. We compared simulation-based planning of a nested cRCT using covariate-constrained randomization to conventional power calculations using OptiMAx-Chad as a case study. OptiMAx-Chad will examine the impact of embedding mass distribution of small-quantity lipid-based nutrient supplements within an expanded programme on immunization on first-dose measles-containing vaccine (MCV1) coverage among children aged 12-24 months in rural villages in Ngouri. Within the 12 health areas to be randomized, a random subset of villages will be selected for outcome collection. 1,000,000 assignments of health areas with different possible village selections were generated using covariate-constrained randomization to balance baseline village characteristics. The empirically estimated intracluster correlation coefficient (ICC) and the World Health Organization (WHO) recommended values of 1/3 and 1/6 were considered. The desired operating characteristics were 80% power at 0.05 one-sided type I error rate. Using conventional calculations target power for a realistic treatment effect could not be achieved with the WHO recommended values. Conventional calculations also showed a plateau in power after a certain cluster size. Our simulations matched the design of OptiMAx-Chad with covariate adjustment and random selection, and showed that power did not plateau. Instead, power increased with increasing cluster size. Planning complex cRCTs with covariate constrained randomization and a multi-nested data structure with conventional closed-form formulas can be misleading. Simulations can improve the planning of cRCTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18818v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jay JH Park, Rebecca K. Metcalfe, Nathaniel Dyrkton, Yichen Yan, Shomoita Alam, Kevin Phelan, Ibrahim Sana, Susan Shepherd</dc:creator>
    </item>
    <item>
      <title>Scalable Bayesian inference for time series via divide-and-conquer</title>
      <link>https://arxiv.org/abs/2106.11043</link>
      <description>arXiv:2106.11043v4 Announce Type: replace 
Abstract: Bayesian computational algorithms tend to scale poorly as data size increases. This has motivated divide-and-conquer-based approaches for scalable inference. These divide the data into subsets, perform inference for each subset in parallel, and then combine these inferences. While appealing theoretical properties and practical performance have been demonstrated for independent observations, scalable inference for dependent data remains challenging. In this work, we study the problem of Bayesian inference from very long time series. The literature in this area focuses mainly on approximate approaches that usually lack rigorous theoretical guarantees and may provide arbitrarily poor accuracy in practice. We propose a simple and scalable divide-and-conquer method, and provide accuracy guarantees. Numerical simulations and real data applications demonstrate the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.11043v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rihui Ou, Lachlan Astfalck, Deborshee Sen, David Dunson</dc:creator>
    </item>
    <item>
      <title>A novel decomposition to explain heterogeneity in observational and randomized studies of causality</title>
      <link>https://arxiv.org/abs/2208.05543</link>
      <description>arXiv:2208.05543v4 Announce Type: replace 
Abstract: This paper introduces a novel decomposition framework to explain heterogeneity in causal effects observed across different studies, considering both observational and randomized settings. We present a formal decomposition of between-study heterogeneity, identifying sources of variability in treatment effects across studies. The proposed methodology allows for robust estimation of causal parameters under various assumptions, addressing differences in pre-treatment covariate distributions, mediating variables, and the outcome mechanism. Our approach is validated through a simulation study and applied to data from the Moving to Opportunity (MTO) study, demonstrating its practical relevance. This work contributes to the broader understanding of causal inference in multi-study environments, with potential applications in evidence synthesis and policy-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.05543v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Gilbert, Ivan D{\i}az, Kara E. Rudolph, Nicholas Williams, Tat-Thang Vo</dc:creator>
    </item>
    <item>
      <title>Assessing survival models by interval testing</title>
      <link>https://arxiv.org/abs/2406.00730</link>
      <description>arXiv:2406.00730v2 Announce Type: replace 
Abstract: When considering many survival models, decisions become more challenging in health economic evaluation. In this paper, we present a set of methods to assist with selecting the most appropriate parametric survival models. The methods highlight areas of particularly poor fit. Furthermore, plots and overall p-values provide guidance on whether a parametric survival model should be rejected or not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00730v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Lee</dc:creator>
    </item>
    <item>
      <title>DiPMInd: Distance Profile based Mutual Independence testing for random objects</title>
      <link>https://arxiv.org/abs/2412.06766</link>
      <description>arXiv:2412.06766v3 Announce Type: replace 
Abstract: This paper develops a novel unified framework for testing mutual independence among random objects residing in possibly different metric spaces. The framework generalizes existing methodologies and introduces new measures of mutual independence, and proposes associated tests that achieve minimax rate optimality and exhibit strong empirical power. The foundation of the proposed tests is the new concept of joint distance profiles, which uniquely characterize the joint law of random objects under a mild condition on either the joint law or the metric spaces. Our test statistics quantify the difference of the joint distance profiles of each data point with respect to the joint law and the product of marginal laws of the vector of random objects. To enhance power, we consider integrating this difference with respect to different measures and incorporate flexible data-adaptive weight profiles in the test statistics. We derive the limiting distribution of the test statistics under the null hypothesis of mutual independence and show that the proposed tests with certain weight profiles are asymptotically distribution-free if the marginal distance profiles are continuous. Furthermore, we establish the consistency of the tests under sequences of alternative hypotheses converging to the null. For practical implementations, we employ a permutation scheme to approximate the $p$-values and provide theoretical guarantees that the permutation-based tests maintain type I error control under the null and achieve consistency under alternatives. We demonstrate the power of the proposed tests across various types of data objects through simulations and real data applications, where the new tests exhibit better performance compared with popular existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06766v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaqing Chen, Paromita Dubey</dc:creator>
    </item>
    <item>
      <title>Pseudo-spectra of multivariate inhomogeneous spatial point processes</title>
      <link>https://arxiv.org/abs/2502.09948</link>
      <description>arXiv:2502.09948v2 Announce Type: replace 
Abstract: In this article, we propose a spectral method for a class of multivariate inhomogeneous spatial point processes, namely the second-order intensity reweighted stationary processes. A key ingredient of our approach is utilizing the asymptotic behavior of the periodogram. For second-order stationary point processes, the periodogram is an asymptotically unbiased estimator of the spectrum. By calculating the moment, we show that under inhomogeneity, the expectation of the periodogram converges to a matrix-valued function, which we refer to as the pseudo-spectrum. The pseudo-spectrum shares similar properties with the spectrum of stationary processes and admits interpretation in terms of local parameters. We derive a consistent nonparametric estimator of the pseudo-spectrum via kernel smoothing and propose two bandwidth selection methods. The performance and utility of the proposed methods are demonstrated through simulation studies and an application to rainforest point pattern data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09948v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi-Wen Ding, Junho Yang, Joonho Shin</dc:creator>
    </item>
    <item>
      <title>Choosing an analytic approach: Key study design considerations in state policy evaluation</title>
      <link>https://arxiv.org/abs/2504.03609</link>
      <description>arXiv:2504.03609v2 Announce Type: replace 
Abstract: This paper reviews and details methods for state policy evaluation to guide selection of a research approach based on evaluation setting and available data. We highlight key design considerations for an analysis, including treatment and control group selection, timing of policy adoption, expected effect heterogeneity, and data considerations. We then provide an overview of analytic approaches and differentiate between methods based on evaluation context, such as settings with no control units, a single treated unit, multiple treated units, or with multiple treatment cohorts. Methods discussed include interrupted time series models, difference-in-differences estimators, autoregressive models, and synthetic control methods, along with method extensions which address issues like staggered policy adoption and heterogenous treatment effects. We end with an illustrative example, applying the developed framework to evaluate the impacts of state-level naloxone standing order policies on overdose rates. Overall, we provide researchers with an approach for deciding on methods for state policy evaluations, which can be used to select study designs and inform methodological choices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03609v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Elizabeth M. Stone, Megan S. Schuler, Elizabeth A. Stuart, Max Rubinstein, Max Griswold, Bradley D. Stein, Beth Ann Griffin</dc:creator>
    </item>
    <item>
      <title>Penalised spline estimation of covariate-specific time-dependent ROC curves</title>
      <link>https://arxiv.org/abs/2506.13604</link>
      <description>arXiv:2506.13604v2 Announce Type: replace 
Abstract: The identification of biomarkers with high predictive accuracy is a crucial task in medical research, as it can aid clinicians in making early decisions, thereby reducing morbidity and mortality in high-risk populations. Time-dependent receiver operating characteristic (ROC) curves are the main tool used to assess the accuracy of prognostic biomarkers for outcomes that evolve over time. Recognising the need to account for patient heterogeneity when evaluating the accuracy of a prognostic biomarker, we introduce a novel penalised-based estimator of the time-dependent ROC curve that accommodates a possible modifying effect of covariates. We consider flexible models for both the hazard function of the event time given the covariates and biomarker and for the location-scale regression model of the biomarker given covariates, enabling the accommodation of non-proportional hazards and nonlinear effects through penalised splines, thus overcoming limitations of earlier methods. The simulation study demonstrates that our approach successfully recovers the true functional form of the covariate-specific time-dependent ROC curve and the corresponding area under the curve across a variety of scenarios. Comparisons with existing methods further show that our approach performs favourably in multiple settings. Our approach is applied to evaluate the ability of the Global Registry of Acute Coronary Events risk score to predict mortality over different time periods after discharge in patients who have suffered an acute coronary syndrome and to investigate how this ability may vary with the left ventricular ejection fraction. An R package, CondTimeROC, implementing the proposed method is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13604v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mar\'ia Xos\'e Rodr\'iguez-\'Alvarez, Vanda In\'acio</dc:creator>
    </item>
    <item>
      <title>A Nonparametric Bayesian Solution of the Empirical Stochastic Inverse Problem</title>
      <link>https://arxiv.org/abs/2509.22597</link>
      <description>arXiv:2509.22597v2 Announce Type: replace 
Abstract: The stochastic inverse problem is a key ingredient in making inferences, predictions, and decisions for complex science and engineering systems. We formulate and analyze a nonparametric Bayesian solution for the stochastic inverse problem. Key properties of the solution are proved and the convergence and error of a computational solution obtained by random sampling is analyzed. Several applications illustrate the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22597v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haiyi Shi, Lei Yang, Jiarui Chi, Troy Butler, Haonan Wang, Derek Bingham, Don Estep</dc:creator>
    </item>
    <item>
      <title>Elicitability and identifiability of tail risk measures</title>
      <link>https://arxiv.org/abs/2404.14136</link>
      <description>arXiv:2404.14136v3 Announce Type: replace-cross 
Abstract: Tail risk measures are fully determined by the distribution of the underlying loss beyond its quantile at a certain level, with Value-at-Risk, Expected Shortfall and Range Value-at-Risk being prime examples. They are induced by law-based risk measures, called their generators, evaluated on the tail distribution. This paper establishes joint identifiability and elicitability results of tail risk measures together with the corresponding quantile, provided that their generators are identifiable and elicitable, respectively. As an example, we establish the joint identifiability and elicitability of the tail expectile together with the quantile. The corresponding consistent scores constitute a novel class of weighted scores, nesting the known class of scores of Fissler and Ziegel for the Expected Shortfall together with the quantile. For statistical purposes, our results pave the way to easier model fitting for tail risk measures via regression and the generalized method of moments, but also model comparison and model validation in terms of established backtesting procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14136v3</guid>
      <category>q-fin.ST</category>
      <category>math.ST</category>
      <category>q-fin.RM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobias Fissler, Fangda Liu, Ruodu Wang, Linxiao Wei</dc:creator>
    </item>
    <item>
      <title>Can We Validate Counterfactual Estimations in the Presence of General Network Interference?</title>
      <link>https://arxiv.org/abs/2502.01106</link>
      <description>arXiv:2502.01106v2 Announce Type: replace-cross 
Abstract: Randomized experiments have become a cornerstone of evidence-based decision-making in contexts ranging from online platforms to public health. However, in experimental settings with network interference, a unit's treatment can influence outcomes of other units, challenging both causal effect estimation and its validation. Classic validation approaches fail as outcomes are only observable under a single treatment scenario and exhibit complex correlation patterns due to interference. To address these challenges, we introduce a framework that facilitates the use of machine learning tools for both estimation and validation in causal inference. Central to our approach is the new distribution-preserving network bootstrap, a theoretically-grounded technique that generates multiple statistically-valid subpopulations from a single experiment's data. This amplification of experimental samples enables our second contribution: a counterfactual cross-validation procedure. This procedure adapts the principles of model validation to the unique constraints of causal settings, providing a rigorous, data-driven method for selecting and evaluating estimators. We extend recent causal message-passing developments by incorporating heterogeneous unit-level characteristics and varying local interactions, ensuring reliable finite-sample performance through non-asymptotic analysis. Additionally, we develop and publicly release a comprehensive benchmark toolbox featuring diverse experimental environments, from networks of interacting AI agents to ride-sharing applications. These environments provide known ground truth values while maintaining realistic complexities, enabling systematic evaluation of causal inference methods. Extensive testing across these environments demonstrates our method's robustness to diverse forms of network interference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01106v2</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sadegh Shirani, Yuwei Luo, William Overman, Ruoxuan Xiong, Mohsen Bayati</dc:creator>
    </item>
    <item>
      <title>Kernel-Based Nonparametric Tests For Shape Constraints</title>
      <link>https://arxiv.org/abs/2510.16745</link>
      <description>arXiv:2510.16745v2 Announce Type: replace-cross 
Abstract: We develop a reproducing kernel Hilbert space (RKHS) framework for nonparametric mean-variance optimization and inference on shape constraints of the optimal rule. We derive statistical properties of the sample estimator and provide rigorous theoretical guarantees, such as asymptotic consistency, a functional central limit theorem, and a finite-sample deviation bound that matches the Monte Carlo rate up to regularization. Building on these findings, we introduce a joint Wald-type statistic to test for shape constraints over finite grids. The approach comes with an efficient computational procedure based on a pivoted Cholesky factorization, facilitating scalability to large datasets. Empirical tests suggest favorably of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16745v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohan Sen</dc:creator>
    </item>
  </channel>
</rss>
