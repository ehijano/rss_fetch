<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 May 2024 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Online Mental Stress Detection Using Frontal-channel EEG Recordings in a Classroom Scenario</title>
      <link>https://arxiv.org/abs/2405.11394</link>
      <description>arXiv:2405.11394v1 Announce Type: new 
Abstract: Objective: To investigate the effects of different approaches to EEG preprocessing, channel montage selection, and model architecture on the performance of an online-capable stress detection algorithm in a classroom scenario. Methods: This analysis used EEG data from a longitudinal stress and fatigue study conducted among university students. Their self-reported stress ratings during each class session were the basis for classifying EEG recordings into either normal or elevated stress states. We used a data-processing pipeline that combined Artifact Subspace Reconstruction (ASR)and an Independent Component Analysis (ICA)-based method to achieve online artifact removal. We compared the performance of a Linear Discriminant Analysis (LDA) and a 4-layer neural network as classifiers. We opted for accuracy, balanced accuracy, and F1 score as the metrics for assessing performance. We examined the impact of varying numbers of input channels using different channel montages. Additionally, we explored different window lengths and step sizes during online evaluation. Results: Our online artifact removal method achieved performance comparable to the offline ICA method in both offline and online evaluations. A balanced accuracy of 77% and 78% in an imbalanced binary classification were observed when using the 11-frontal-channel LDA model with the proposed artifact removal method. Moreover, the model performance remained intact when changing the channel montage from 30 full-scalp channels to just 11 frontal channels. During the online evaluation, we achieved the highest balanced accuracy (78%) with a window length of 20 seconds and a step size of 1 second. Significance: This study comprehensively investigates the deployment of stress detection in real-world scenarios. The findings of this study provide insight into the development of daily mental stress monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11394v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chi-Yuan Chang, Chieh Hsu, Ying Choon Wu, Siwen Wang, Darin Tsui, Tzyy-Ping Jung</dc:creator>
    </item>
    <item>
      <title>Lattice physics approaches for neural networks</title>
      <link>https://arxiv.org/abs/2405.12022</link>
      <description>arXiv:2405.12022v1 Announce Type: new 
Abstract: Modern neuroscience has evolved into a frontier field that draws on numerous disciplines, resulting in the flourishing of novel conceptual frames primarily inspired by physics and complex systems science. Contributing in this direction, we recently introduced a mathematical framework to describe the spatiotemporal interactions of systems of neurons using lattice field theory, the reference paradigm for theoretical particle physics. In this note, we provide a concise summary of the basics of the theory, aiming to be intuitive to the interdisciplinary neuroscience community. We contextualize our methods, illustrating how to readily connect the parameters of our formulation to experimental variables using well-known renormalization procedures. This synopsis yields the key concepts needed to describe neural networks using lattice physics. Such classes of methods are attention-worthy in an era of blistering improvements in numerical computations, as they can facilitate relating the observation of neural activity to generative models underpinned by physical principles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12022v1</guid>
      <category>q-bio.NC</category>
      <category>physics.app-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giampiero Bardella, Simone Franchini, Pierpaolo Pani, Stefano Ferraina</dc:creator>
    </item>
    <item>
      <title>The Projective Wave Theory of Consciousness</title>
      <link>https://arxiv.org/abs/2405.12071</link>
      <description>arXiv:2405.12071v1 Announce Type: new 
Abstract: Neural theories of consciousness face three difficulties: (1) The selection problem: how are those neurons which cause consciousness selected, from all the other neurons which do not? (2) the precision problem: how do neurons hold a detailed internal model of 3D space, as the origin of our spatial conscious experience? and (3) the decoding problem: how are the many distorted neural representations of space in the brain decoded, to give our largely undistorted conscious experience of space? These problems can all be addressed if the brains internal model of local 3D space is held not in neurons, but in a wave excitation (holding a projective transform of Euclidean space), and if the wave is the source of spatial consciousness. Such a wave has not yet been detected in the brain, but there are good reasons why it has not been detected; and there is indirect evidence for a wave, in the mammalian thalamus, and in the central body of the insect brain. The resulting projective wave theory of consciousness gives good agreement with the spatial form of our consciousness. It has a positive Bayesian balance between the complexity of its assumptions and the data it accounts for; this gives a basis to believe it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12071v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Worden</dc:creator>
    </item>
    <item>
      <title>Alterations of electrocortical activity during hand movements induced by motor cortex glioma</title>
      <link>https://arxiv.org/abs/2405.12144</link>
      <description>arXiv:2405.12144v1 Announce Type: new 
Abstract: Glioma cells can reshape functional neuronal networks by hijacking neuronal synapses, leading to partial or complete neurological dysfunction. These mechanisms have been previously explored for language functions. However, the impact of glioma on sensorimotor functions is still unknown. Therefore, we recruited a control group of patients with unaffected motor cortex and a group of patients with glioma-infiltrated motor cortex, and recorded high-density electrocortical signals during finger movement tasks. The results showed that glioma suppresses task-related synchronization in the high-gamma band and reduces the power across all frequency bands. The resulting atypical motor information transmission model with discrete signaling pathways and delayed responses disrupts the stability of neuronal encoding patterns for finger movement kinematics across various temporal-spatial scales. These findings demonstrate that gliomas functionally invade neural circuits within the motor cortex. This result advances our understanding of motor function processing in chronic disease states, which is important to advance the surgical strategies and neurorehabilitation approaches for patients with malignant gliomas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12144v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihan Wu, Tao Chang, Siliang Chen, Xiaodong Niu, Yu Li, Yuan Fang, Lei Yang, Yixuan Zong, Yaoxin Yang, Yuehua Li, Mengsong Wang, Wen Yang, Yixuan Wu, Chen Fu, Xia Fang, Yuxin Quan, Xilin Peng, Qiang Sun, Marc M. Van Hulle, Yanhui Liu, Ning Jiang, Dario Farina, Yuan Yang, Jiayuan He, Qing Mao</dc:creator>
    </item>
    <item>
      <title>Du-IN: Discrete units-guided mask modeling for decoding speech from Intracranial Neural signals</title>
      <link>https://arxiv.org/abs/2405.11459</link>
      <description>arXiv:2405.11459v1 Announce Type: cross 
Abstract: Invasive brain-computer interfaces have garnered significant attention due to their high performance. The current intracranial stereoElectroEncephaloGraphy (sEEG) foundation models typically build univariate representations based on a single channel. Some of them further use Transformer to model the relationship among channels. However, due to the locality and specificity of brain computation, their performance on more difficult tasks, e.g., speech decoding, which demands intricate processing in specific brain regions, is yet to be fully investigated. We hypothesize that building multi-variate representations within certain brain regions can better capture the specific neural processing. To explore this hypothesis, we collect a well-annotated Chinese word-reading sEEG dataset, targeting language-related brain networks, over 12 subjects. Leveraging this benchmark dataset, we developed the Du-IN model that can extract contextual embeddings from specific brain regions through discrete codebook-guided mask modeling. Our model achieves SOTA performance on the downstream 61-word classification task, surpassing all baseline models. Model comparison and ablation analysis reveal that our design choices, including (i) multi-variate representation by fusing channels in vSMC and STG regions and (ii) self-supervision by discrete codebook-guided mask modeling, significantly contribute to these performances. Collectively, our approach, inspired by neuroscience findings, capitalizing on multi-variate neural representation from specific brain regions, is suitable for invasive brain modeling. It marks a promising neuro-inspired AI approach in BCI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11459v1</guid>
      <category>eess.SP</category>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hui Zheng, Hai-Teng Wang, Wei-Bang Jiang, Zhong-Tao Chen, Li He, Pei-Yang Lin, Peng-Hu Wei, Guo-Guang Zhao, Yun-Zhe Liu</dc:creator>
    </item>
    <item>
      <title>Alternators For Sequence Modeling</title>
      <link>https://arxiv.org/abs/2405.11848</link>
      <description>arXiv:2405.11848v1 Announce Type: cross 
Abstract: This paper introduces alternators, a novel family of non-Markovian dynamical models for sequences. An alternator features two neural networks: the observation trajectory network (OTN) and the feature trajectory network (FTN). The OTN and the FTN work in conjunction, alternating between outputting samples in the observation space and some feature space, respectively, over a cycle. The parameters of the OTN and the FTN are not time-dependent and are learned via a minimum cross-entropy criterion over the trajectories. Alternators are versatile. They can be used as dynamical latent-variable generative models or as sequence-to-sequence predictors. When alternators are used as generative models, the FTN produces interpretable low-dimensional latent variables that capture the dynamics governing the observations. When alternators are used as sequence-to-sequence predictors, the FTN learns to predict the observed features. In both cases, the OTN learns to produce sequences that match the data. Alternators can uncover the latent dynamics underlying complex sequential data, accurately forecast and impute missing data, and sample new trajectories. We showcase the capabilities of alternators in three applications. We first used alternators to model the Lorenz equations, often used to describe chaotic behavior. We then applied alternators to Neuroscience, to map brain activity to physical activity. Finally, we applied alternators to Climate Science, focusing on sea-surface temperature forecasting. In all our experiments, we found alternators are stable to train, fast to sample from, yield high-quality generated samples and latent variables, and outperform strong baselines such as neural ODEs and diffusion models in the domains we studied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11848v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>physics.ao-ph</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Reza Rezaei, Adji Bousso Dieng</dc:creator>
    </item>
    <item>
      <title>Dynamics of specialization in neural modules under resource constraints</title>
      <link>https://arxiv.org/abs/2106.02626</link>
      <description>arXiv:2106.02626v5 Announce Type: replace 
Abstract: It has long been believed that the brain is highly modular both in terms of structure and function, although recent evidence has led some to question the extent of both types of modularity. We used artificial neural networks to test the hypothesis that structural modularity is sufficient to guarantee functional specialization, and find that in general, this doesn't necessarily hold. We then systematically tested which features of the environment and network do lead to the emergence of specialization. We used a simple toy environment, task and network, allowing us precise control, and show that in this setup, several distinct measures of specialization give qualitatively similar results. We further find that in this setup (1) specialization can only emerge in environments where features of that environment are meaningfully separable, (2) specialization preferentially emerges when the network is strongly resource-constrained, and (3) these findings are qualitatively similar across the different variations of network architectures that we tested, but that the quantitative relationships depend on the precise architecture. Finally, we show that functional specialization varies dynamically across time, and demonstrate that these dynamics depend on both the timing and bandwidth of information flow in the network. We conclude that a static notion of specialization, based on structural modularity, is likely too simple a framework for understanding intelligence in situations of real-world complexity, from biology to brain-inspired neuromorphic systems. We propose that thoroughly stress testing candidate definitions of functional modularity in simplified scenarios before extending to more complex data, network models and electrophysiological recordings is likely to be a fruitful approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.02626v5</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel B\'ena, Dan F. M. Goodman</dc:creator>
    </item>
    <item>
      <title>A Lipid Rafts Theory of Alzheimer's Disease</title>
      <link>https://arxiv.org/abs/2310.20232</link>
      <description>arXiv:2310.20232v2 Announce Type: replace 
Abstract: I present a theory of Alzheimer's Disease (AD) that explains its symptoms, pathology, and risk factors. To do this, I introduce a new theory of brain plasticity that elucidates the physiological roles of AD-related agents. New events generate synaptic and branching candidates competing for long-term enhancement. Competition resolution crucially depends on the formation of membrane lipid rafts, which requires astrocyte-produced cholesterol. Sporadic AD is caused by impaired formation of plasma membrane lipid rafts, which prevents the conversion of short- to long-term memory, and yields excessive tau phosphorylation, intracellular cholesterol accumulation, synaptic dysfunction, and neurodegeneration. Amyloid beta (Abeta) production is promoted by cholesterol during the switch to competition resolution, and cholesterol accumulation stimulates chronic Abeta production, secretion, and aggregation. The theory addresses all of the major established facts known about the disease, and is supported by strong evidence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.20232v2</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ari Rappoport</dc:creator>
    </item>
    <item>
      <title>SI-SD: Sleep Interpreter through awake-guided cross-subject Semantic Decoding</title>
      <link>https://arxiv.org/abs/2309.16457</link>
      <description>arXiv:2309.16457v3 Announce Type: replace-cross 
Abstract: Understanding semantic content from brain activity during sleep represents a major goal in neuroscience. While studies in rodents have shown spontaneous neural reactivation of memories during sleep, capturing the semantic content of human sleep poses a significant challenge due to the absence of well-annotated sleep datasets and the substantial differences in neural patterns between wakefulness and sleep. To address these challenges, we designed a novel cognitive neuroscience experiment and collected a comprehensive, well-annotated electroencephalography (EEG) dataset from 134 subjects during both wakefulness and sleep. Leveraging this benchmark dataset, we developed SI-SD that enhances sleep semantic decoding through the position-wise alignment of neural latent sequence between wakefulness and sleep. In the 15-way classification task, our model achieves 24.12% and 21.39% top-1 accuracy on unseen subjects for NREM 2/3 and REM sleep, respectively, surpassing all other baselines. With additional fine-tuning, decoding performance improves to 30.32% and 31.65%, respectively. Besides, inspired by previous neuroscientific findings, we systematically analyze how the "Slow Oscillation" event impacts decoding performance in NREM 2/3 sleep -- decoding performance on unseen subjects further improves to 40.02%. Together, our findings and methodologies contribute to a promising neuro-AI framework for decoding brain activity during sleep.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.16457v3</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hui Zheng, Zhong-Tao Chen, Hai-Teng Wang, Jian-Yang Zhou, Lin Zheng, Pei-Yang Lin, Yun-Zhe Liu</dc:creator>
    </item>
    <item>
      <title>Optimal input reverberation and homeostatic self-organization towards the edge of synchronization</title>
      <link>https://arxiv.org/abs/2402.05032</link>
      <description>arXiv:2402.05032v2 Announce Type: replace-cross 
Abstract: Transient or partial synchronization can be used to do computations, although a fully synchronized network is frequently related to epileptic seizures. Here, we propose a homeostatic mechanism that is capable of maintaining a neuronal network at the edge of a synchronization transition, thereby avoiding the harmful consequences of a fully synchronized network. We model neurons by maps since they are dynamically richer than integrate-and-fire models and more computationally efficient than conductance-based approaches. We first describe the synchronization phase transition of a dense network of neurons with different tonic spiking frequencies coupled by gap junctions. We show that at the transition critical point, inputs optimally reverberate through the network activity through transient synchronization. Then, we introduce a local homeostatic dynamic in the synaptic coupling and show that it produces a robust self-organization toward the edge of this phase transition. We discuss the potential biological consequences of this self-organization process, such as its relation to the Brain Criticality hypothesis, its input processing capacity, and how its malfunction could lead to pathological synchronization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05032v2</guid>
      <category>nlin.AO</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1063/5.0202743</arxiv:DOI>
      <arxiv:journal_reference>Chaos 34, 053127 (2024)</arxiv:journal_reference>
      <dc:creator>Sue L. Rh\^amidda, Mauricio Girardi-Schappo, Osame Kinouchi</dc:creator>
    </item>
  </channel>
</rss>
