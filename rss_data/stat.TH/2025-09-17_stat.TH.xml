<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Sep 2025 04:01:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Haussdorff consistency of MLE in folded normal and Gaussian mixtures</title>
      <link>https://arxiv.org/abs/2509.12206</link>
      <description>arXiv:2509.12206v1 Announce Type: new 
Abstract: We develop a constant-tracking likelihood theory for two nonregular models: the folded normal and finite Gaussian mixtures. For the folded normal, we prove boundary coercivity for the profiled likelihood, show that the profile path of the location parameter exists and is strictly decreasing by an implicit-function argument, and establish a unique profile maximizer in the scale parameter. Deterministic envelopes for the log-likelihood, the score, and the Hessian yield elementary uniform laws of large numbers with finite-sample bounds, avoiding covering numbers. Identification and Kullback-Leibler separation deliver consistency. A sixth-order expansion of the log hyperbolic cosine creates a quadratic-minus-quartic contrast around zero, leading to a nonstandard one-fourth-power rate for the location estimator at the kink and a standard square-root rate for the scale estimator, with a uniform remainder bound. For finite Gaussian mixtures with distinct components and positive weights, we give a short identifiability proof up to label permutations via Fourier and Vandermonde ideas, derive two-sided Gaussian envelopes and responsibility-based gradient bounds on compact sieves, and obtain almost-sure and high-probability uniform laws with explicit constants. Using a minimum-matching distance on permutation orbits, we prove Hausdorff consistency on fixed and growing sieves. We quantify variance-collapse spikes via an explicit spike-bonus bound and show that a quadratic penalty in location and log-scale dominates this bonus, making penalized likelihood coercive; when penalties shrink but sample size times penalty diverges, penalized estimators remain consistent. All proofs are constructive, track constants, verify measurability of maximizers, and provide practical guidance for tuning sieves, penalties, and EM-style optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12206v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Koustav Mallik</dc:creator>
    </item>
    <item>
      <title>Jackknife Variance Estimation for H\'ajek-Dominated Generalized U-Statistics</title>
      <link>https://arxiv.org/abs/2509.12356</link>
      <description>arXiv:2509.12356v1 Announce Type: new 
Abstract: We prove ratio-consistency of the jackknife variance estimator, and certain variants, for a broad class of generalized U-statistics whose variance is asymptotically dominated by their H\'ajek projection, with the classical fixed-order case recovered as a special instance. This H\'ajek projection dominance condition unifies and generalizes several criteria in the existing literature, placing the simple nonparametric jackknife on the same footing as the infinitesimal jackknife in the generalized setting. As an illustration, we apply our result to the two-scale distributional nearest-neighbor regression estimator, obtaining consistent variance estimates under substantially weaker conditions than previously required.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12356v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakob R. Juergens</dc:creator>
    </item>
    <item>
      <title>Sharp mean-field analysis of permutation mixtures and permutation-invariant decisions</title>
      <link>https://arxiv.org/abs/2509.12584</link>
      <description>arXiv:2509.12584v1 Announce Type: new 
Abstract: We develop sharp bounds on the statistical distance between high-dimensional permutation mixtures and their i.i.d. counterparts. Our approach establishes a new geometric link between the spectrum of a complex channel overlap matrix and the information geometry of the channel, yielding tight dimension-independent bounds that close gaps left by previous work. Within this geometric framework, we also derive dimension-dependent bounds that uncover phase transitions in dimensionality for Gaussian and Poisson families. Applied to compound decision problems, this refined control of permutation mixtures enables sharper mean-field analyses of permutation-invariant decision rules, yielding strong non-asymptotic equivalence results between two notions of compound regret in Gaussian and Poisson models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12584v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiguo Liang, Yanjun Han</dc:creator>
    </item>
    <item>
      <title>A Statistical Test for Comparing the Linkage and Admixture Model Based on Central Limit Theorems</title>
      <link>https://arxiv.org/abs/2509.12734</link>
      <description>arXiv:2509.12734v1 Announce Type: new 
Abstract: In the Admixture Model, the probability that an individual carries a certain allele at a specific marker depends on the allele frequencies in $K$ ancestral populations and the proportion of the individual's genome originating from these populations. The markers are assumed to be independent. The Linkage Model is a Hidden Markov Model (HMM) that extends the Admixture Model by incorporating linkage between neighboring loci.
  This study investigates the consistency and central limit behavior of maximum likelihood estimators (MLEs) for individual ancestry in the Linkage Model, complementing earlier results by \citep{pfaff2004information, pfaffelhuber2022central, heinzel2025consistency} for the Admixture Model. These theoretical results are used to prove theoretical properties of a statistical test that allows for model selection between the Admixture Model and the Linkage Model. Finally, we demonstrate the practical relevance of our results by applying the test to real-world data from \cite{10002015global}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12734v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carola Sophia Heinzel</dc:creator>
    </item>
    <item>
      <title>Gaussian Mixture Model with unknown diagonal covariances via continuous sparse regularization</title>
      <link>https://arxiv.org/abs/2509.12889</link>
      <description>arXiv:2509.12889v1 Announce Type: new 
Abstract: This paper addresses the statistical estimation of Gaussian Mixture Models (GMMs) with unknown diagonal covariances from independent and identically distributed samples. We employ the Beurling-LASSO (BLASSO), a convex optimization framework that promotes sparsity in the space of measures, to simultaneously estimate the number of components and their parameters. Our main contribution extends the BLASSO methodology to multivariate GMMs with component-specific unknown diagonal covariance matrices-a significantly more flexible setting than previous approaches requiring known and identical covariances. We establish non-asymptotic recovery guarantees with nearly parametric convergence rates for component means, diagonal covariances, and weights, as well as for density prediction. A key theoretical contribution is the identification of an explicit separation condition on mixture components that enables the construction of non-degenerate dual certificates-essential tools for establishing statistical guarantees for the BLASSO. Our analysis leverages the Fisher-Rao geometry of the statistical model and introduces a novel semi-distance adapted to our framework, providing new insights into the interplay between component separation, parameter space geometry, and achievable statistical recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12889v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Romane Giard (ECL, ICJ, PSPM), Yohann de Castro (ICJ, ECL, PSPM, IUF), Cl\'ement Marteau (PSPM, ICJ, UCBL)</dc:creator>
    </item>
    <item>
      <title>Non-parametric estimation of non-linear diffusion coefficient in parabolic SPDEs</title>
      <link>https://arxiv.org/abs/2509.12921</link>
      <description>arXiv:2509.12921v1 Announce Type: new 
Abstract: In this article, we introduce a novel non-parametric predictor, based on conditional expectation, for the unknown diffusion coefficient function $\sigma$ in the stochastic partial differential equation $Lu = \sigma(u)\dot{W}$, where $L$ is a parabolic second order differential operator and $\dot{W}$ is a suitable Gaussian noise. We prove consistency and derive an upper bound for the error in the $L^p$ norm, in terms of discretization and smoothening parameters $h$ and $\varepsilon$. We illustrate the applicability of the approach and the role of the parameters with several interesting numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12921v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Andersson, Benny Avelin, Valentin Garino, Pauliina Ilmonen, Lauri Viitasaari</dc:creator>
    </item>
    <item>
      <title>Optimal Conformal Prediction, E-values, Fuzzy Prediction Sets and Subsequent Decisions</title>
      <link>https://arxiv.org/abs/2509.13130</link>
      <description>arXiv:2509.13130v1 Announce Type: new 
Abstract: We make three contributions to conformal prediction. First, we propose fuzzy conformal confidence sets that offer a degree of exclusion, generalizing beyond the binary inclusion/exclusion offered by classical confidence sets. We connect fuzzy confidence sets to e-values to show this degree of exclusion is equivalent to an exclusion at different confidence levels, capturing precisely what e-values bring to conformal prediction. We show that a fuzzy confidence set is a predictive distribution with a more appropriate error guarantee. Second, we derive optimal conformal confidence sets by interpreting the minimization of the expected measure of the confidence set as an optimal testing problem against a particular alternative. We use this to characterize exactly in what sense traditional conformal prediction is optimal. Third, we generalize the inheritance of guarantees by subsequent minimax decisions from confidence sets to fuzzy confidence sets. All our results generalize beyond the exchangeable conformal setting to prediction sets for arbitrary models. In particular, we find that any valid test (e-value) for a hypothesis automatically defines a (fuzzy) prediction confidence set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13130v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick W. Koning, Sam van Meer</dc:creator>
    </item>
    <item>
      <title>De Finetti + Sanov = Bayes</title>
      <link>https://arxiv.org/abs/2509.13283</link>
      <description>arXiv:2509.13283v1 Announce Type: new 
Abstract: We develop a framework for the operationalization of models and parameters by combining de Finetti's representation theorem with a conditional form of Sanov's theorem. This synthesis, the tilted de Finetti theorem, shows that conditioning exchangeable sequences on empirical moment constraints yields predictive laws in exponential families via the I-projection of a baseline measure. Parameters emerge as limits of empirical functionals, providing a probabilistic foundation for maximum entropy (MaxEnt) principles. This explains why exponential tilting governs likelihood methods and Bayesian updating, connecting naturally to finite-sample concentration rates that anticipate PAC-Bayes bounds. Examples include Gaussian scale mixtures, where symmetry uniquely selects location-scale families, and Jaynes' Brandeis dice problem, where partial information tilts the uniform law. Broadly, the theorem unifies exchangeability, large deviations, and entropy concentration, clarifying the ubiquity of exponential families and MaxEnt's role as the inevitable predictive limit under partial information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13283v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas G. Polson, Daniel Zantedeschi</dc:creator>
    </item>
    <item>
      <title>New generalized unit distributions based on order statistics</title>
      <link>https://arxiv.org/abs/2509.12276</link>
      <description>arXiv:2509.12276v1 Announce Type: cross 
Abstract: In the present paper, the author discusses the derivation of unit distributions and the derivation of the generalized form using the order statistics. The author discusses the Kumaraswamy as the smallest order statistic of the unit power distribution derived from the inverse Weibull distribution. The author discusses the unit Rayleigh distribution and how it can be generalized using the smallest, largest, and kth order statistics. Using the order statistics to generalize a distribution differs from other techniques like the power transformation and T-X family (transformed-transformer) method. For the discussed distribution, the author demonstrates the basic functions and properties with real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12276v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohamed Attia</dc:creator>
    </item>
    <item>
      <title>System Reliability Estimation via Shrinkage</title>
      <link>https://arxiv.org/abs/2509.12420</link>
      <description>arXiv:2509.12420v1 Announce Type: cross 
Abstract: In a coherent reliability system composed of multiple components configured according to a specific structure function, the distribution of system time to failure, or system lifetime, is often of primary interest. Accurate estimation of system reliability is critical in a wide range of engineering and industrial applications, forming decisions in system design, maintenance planning, and risk assessment. The system lifetime distribution can be estimated directly using the observed system failure times. However, when component-level lifetime data is available, it can yield improved estimates of system reliability. In this work, we demonstrate that under nonparametric assumptions about the component time-to-failure distributions, traditional estimators such as the Product-Limit Estimator (PLE) can be further improved under specific loss functions. We propose a novel methodology that enhances the nonparametric system reliability estimation through a shrinkage transformation applied to component-level estimators. This shrinkage approach leads to improved efficiency in estimating system reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12420v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beidi Qiang, Edsel Pena</dc:creator>
    </item>
    <item>
      <title>A Note on Subadditivity of Value at Risks (VaRs): A New Connection to Comonotonicity</title>
      <link>https://arxiv.org/abs/2509.12558</link>
      <description>arXiv:2509.12558v1 Announce Type: cross 
Abstract: In this paper, we provide a new property of value at risk (VaR), which is a standard risk measure that is widely used in quantitative financial risk management. We show that the subadditivity of VaR for given loss random variables holds for any confidence level if and only if those are comonotonic. This result also gives a new equivalent condition for the comonotonicity of random vectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12558v1</guid>
      <category>q-fin.RM</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuri Imamura, Takashi Kato</dc:creator>
    </item>
    <item>
      <title>Power-Dominance in Estimation Theory: A Third Pathological Axis</title>
      <link>https://arxiv.org/abs/2509.12691</link>
      <description>arXiv:2509.12691v1 Announce Type: cross 
Abstract: This paper introduces a novel framework for estimation theory by introducing a second-order diagnostic for estimator design. While classical analysis focuses on the bias-variance trade-off, we present a more foundational constraint. This result is model-agnostic, domain-agnostic, and is valid for both parametric and non-parametric problems, Bayesian and frequentist frameworks. We propose to classify the estimators into three primary power regimes. We theoretically establish that any estimator operating in the `power-dominant regime' incurs an unavoidable mean-squared error penalty, making it structurally prone to sub-optimal performance. We propose a `safe-zone law' and make this diagnostic intuitive through two safe-zone maps. One map is a geometric visualization analogous to a receiver operating characteristic curve for estimators, and the other map shows that the safe-zone corresponds to a bounded optimization problem, while the forbidden `power-dominant zone' represents an unbounded optimization landscape. This framework reframes estimator design as a path optimization problem, providing new theoretical underpinnings for regularization and inspiring novel design philosophies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12691v1</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sri Satish Krishna Chaitanya Bulusu, Mikko Sillanp\"a\"a</dc:creator>
    </item>
    <item>
      <title>Least squares estimation of the transition density in bifurcating Markov models</title>
      <link>https://arxiv.org/abs/2509.12906</link>
      <description>arXiv:2509.12906v1 Announce Type: cross 
Abstract: In this article, we propose a least squares method for the estimation of the transition density in bifurcating Markov models. Unlike the kernel estimation, this method do not use the quotient which can be a source of errors. In order to study the rate of convergence for least squares estimators, we develop exponential inequalities for empirical process of bifurcating Markov chain under bracketing assumption. Unlike the classical processes, we observe that for bifurcating Markov chains, the complexity parameter depends on the ergodicity rate and as consequence, we have that the convergence rate of our estimator is a function of the ergodicity rate. We conclude with a numerical study to validate our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12906v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>S. Val\`ere Bitseki Penda</dc:creator>
    </item>
    <item>
      <title>Semiparametric Causal Inference for Right-Censored Outcomes with Many Weak Invalid Instruments</title>
      <link>https://arxiv.org/abs/2509.13176</link>
      <description>arXiv:2509.13176v1 Announce Type: cross 
Abstract: We propose a semiparametric framework for causal inference with right-censored survival outcomes and many weak invalid instruments, motivated by Mendelian randomization in biobank studies where classical methods may fail. We adopt an accelerated failure time model and construct a moment condition based on augmented inverse probability of censoring weighting, incorporating both uncensored and censored observations. Under a heteroscedasticity-based condition on the treatment model, we establish point identification of the causal effect despite censoring and invalid instruments. We propose GEL-NOW (Generalized Empirical Likelihood with Non-Orthogonal and Weak moments) for valid inference under these conditions. A divergent number of Neyman orthogonal nuisance functions is estimated using deep neural networks. A key challenge is that the conditional censoring distribution is a non-Neyman orthogonal nuisance, contributing to the first-order asymptotics of the estimator for the target causal effect parameter. We derive the asymptotic distribution and explicitly incorporate this additional uncertainty into the asymptotic variance formula. We also introduce a censoring-adjusted over-identification test that accounts for this variance component. Simulation studies and UK Biobank applications demonstrate the method's robustness and practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13176v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiushi Bu, Wen Su, Xingqiu Zhao, Zhonghua Liu</dc:creator>
    </item>
    <item>
      <title>Merging Rate of Opinions via Optimal Transport on Random Measures</title>
      <link>https://arxiv.org/abs/2305.06116</link>
      <description>arXiv:2305.06116v3 Announce Type: replace 
Abstract: Random measures provide flexible parameters for Bayesian nonparametric models. Given two different priors for a random measure, we develop a natural framework to investigate the rate at which the corresponding posteriors merge, as the sample size increases. We define a new distance between the laws of random measures that is built as a Wasserstein distance on the ground space of unbalanced measures, endowed with the bounded Lipschitz metric. We develop tight analytical bounds for its specification to completely random measures, including the special case of Poisson and gamma random measures. The bounds are interpreted in terms of an adapted extended Wasserstein distance between the L\'evy measures and are used to investigate the merging between the posteriors of normalized gamma and generalized gamma priors. After a careful study on the identifiability of the law of the random measure, interesting asymptotic and finite-sample insights are derived without putting any assumption on the true data generating process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.06116v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marta Catalano, Hugo Lavenant</dc:creator>
    </item>
    <item>
      <title>Conditional uncorrelation equals independence</title>
      <link>https://arxiv.org/abs/2406.01849</link>
      <description>arXiv:2406.01849v4 Announce Type: replace 
Abstract: We show that the stochastic independence of real-valued random variables is equivalent to the conditional uncorrelation, where the conditioning takes place over the Cartesian products of intervals. Next, we express the mutual independence in terms of the conditional correlation matrix. Our results extend the results of Jaworski et al. (Electron. J. Stat., 18(1), 653-673, 2024), which are based on the copula functions and assume the existence of the joint density of the variables. We relax this assumption and show that the independence characterization via conditional uncorrelation is valid in full generality - that is, for all kinds of random variables and any dependencies between them. Additionally, we analyse the assumptions under which the independence is determined by the local uncorrelation. The measure-theoretic methodology we present uses the Radon-Nikodym derivative to reduce the multidimensional characterization problem to the simple one-dimensional conditioning. To demonstrate the potential usefulness of the presented results, various numerical examples are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01849v4</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dawid Tar{\l}owski</dc:creator>
    </item>
    <item>
      <title>Approximate independence of permutation mixtures</title>
      <link>https://arxiv.org/abs/2408.09341</link>
      <description>arXiv:2408.09341v3 Announce Type: replace 
Abstract: We prove bounds on statistical distances between high-dimensional exchangeable mixture distributions (which we call \emph{permutation mixtures}) and their i.i.d. counterparts. Our results are based on a novel method for controlling $\chi^2$ divergences between exchangeable mixtures, which is tighter than the existing methods of moments or cumulants. At a technical level, a key innovation in our proofs is a new Maclaurin-type inequality for elementary symmetric polynomials of variables that sum to zero and an upper bound on permanents of doubly-stochastic positive semidefinite matrices. We obtain as a corollary a new de Finetti-style theorem (in the language of Diaconis and Freedman, 1987), as well as several new statistical results, including a differential privacy guarantee for the ``shuffled privacy model'' with Gaussian noise and improved generic consistency guarantees for empirical Bayes procedures in compound decision problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09341v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanjun Han, Jonathan Niles-Weed</dc:creator>
    </item>
    <item>
      <title>Stabilizing the Splits through Minimax Decision Trees</title>
      <link>https://arxiv.org/abs/2502.16758</link>
      <description>arXiv:2502.16758v2 Announce Type: replace 
Abstract: By revisiting the end-cut preference (ECP) phenomenon associated with a single CART (Breiman et al. (1984)), we introduce MinimaxSplit decision trees, a robust alternative to CART that selects splits by minimizing the worst-case child risk rather than the average risk. For regression, we minimize the maximum within-child squared error; for classification, we minimize the maximum child entropy, yielding a C4.5-compatible criterion. We also study a cyclic variant that deterministically cycles coordinates, leading to our main method of cyclic MinimaxSplit decision trees. We prove oracle inequalities that cover both regression and classification, under mild marginal non-atomicity conditions. The bounds control the tree's global excess risk by local worst-case impurities and yield fast convergence rates compared to CART. We extend the analysis to ensembles that subsample coordinates per node. Empirically, (cyclic) MinimaxSplit trees and their forests improve on structured heterogeneity data such as EEG amplitude regression over fixed time horizons, seasonal air quality forecasting, and image denoising framed as non-parametric regression on spatial coordinates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16758v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenyuan Zhang, Hengrui Luo</dc:creator>
    </item>
    <item>
      <title>Asymptotic Theory of the Best-Choice Rerandomization using the Mahalanobis Distance</title>
      <link>https://arxiv.org/abs/2312.02513</link>
      <description>arXiv:2312.02513v2 Announce Type: replace-cross 
Abstract: Rerandomization, a design that utilizes pretreatment covariates and improves their balance between different treatment groups, has received attention recently in both theory and practice. From a survey by Bruhn and McKenzie (2009), there are at least two types of rerandomization that are used in practice: the first rerandomizes the treatment assignment until covariate imbalance is below a prespecified threshold; the second randomizes the treatment assignment multiple times and chooses the one with the best covariate balance. In this paper we will consider the second type of rerandomization, namely the best-choice rerandomization, whose theory and inference are still lacking in the literature. In particular, we will focus on the best-choice rerandomization that uses the Mahalanobis distance to measure covariate imbalance, which is one of the most commonly used imbalance measure for multivariate covariates and is invariant to affine transformations of covariates. We will study the large-sample repeatedly sampling properties of the best-choice rerandomization, allowing both the number of covariates and the number of tried complete randomizations to increase with the sample size. We show that the asymptotic distribution of the difference-in-means estimator is more concentrated around the true average treatment effect under rerandomization than under the complete randomization, and propose large-sample accurate confidence intervals for rerandomization that are shorter than that for the completely randomized experiment. We further demonstrate that, with moderate number of covariates and with the number of tried randomizations increasing polynomially with the sample size, the best-choice rerandomization can achieve the ideally optimal precision that one can expect even with perfectly balanced covariates. The developed theory and methods are also illustrated using real field experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02513v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhao Wang, Xinran Li</dc:creator>
    </item>
    <item>
      <title>A Statistical Analysis of Deep Federated Learning for Intrinsically Low-dimensional Data</title>
      <link>https://arxiv.org/abs/2410.20659</link>
      <description>arXiv:2410.20659v2 Announce Type: replace-cross 
Abstract: Despite significant research on the optimization aspects of federated learning, the exploration of generalization error, especially in the realm of heterogeneous federated learning, remains an area that has been insufficiently investigated, primarily limited to developments in the parametric regime. This paper delves into the generalization properties of deep federated regression within a two-stage sampling model. Our findings reveal that the intrinsic dimension, characterized by the entropic dimension, plays a pivotal role in determining the convergence rates for deep learners when appropriately chosen network sizes are employed. Specifically, when the true relationship between the response and explanatory variables is described by a $\beta$-H\"older function and one has access to $n$ independent and identically distributed (i.i.d.) samples from $m$ participating clients, for participating clients, the error rate scales at most as $\Tilde{O}((mn)^{-2\beta/(2\beta + \bar{d}_{2\beta}(\lambda))})$, whereas for non-participating clients, it scales as $\Tilde{O}(\Delta \cdot m^{-2\beta/(2\beta + \bar{d}_{2\beta}(\lambda))} + (mn)^{-2\beta/(2\beta + \bar{d}_{2\beta}(\lambda))})$. Here $\bar{d}_{2\beta}(\lambda)$ denotes the corresponding $2\beta$-entropic dimension of $\lambda$, the marginal distribution of the explanatory variables. The dependence between the two stages of the sampling scheme is characterized by $\Delta$. Consequently, our findings not only explicitly incorporate the ``heterogeneity" of the clients, but also highlight that the convergence rates of errors of deep federated learners are not contingent on the nominal high dimensionality of the data but rather on its intrinsic dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20659v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saptarshi Chakraborty, Peter L. Bartlett</dc:creator>
    </item>
    <item>
      <title>A Stable Measure for Conditional Periodicity of Time Series using Persistent Homology</title>
      <link>https://arxiv.org/abs/2501.02817</link>
      <description>arXiv:2501.02817v3 Announce Type: replace-cross 
Abstract: Given a pair of time series, we study how the periodicity of one influences the periodicity of the other. There are several known methods to measure the similarity between a pair of time series, but we have yet to find any measures with theoretical stability results. Persistence homology has been utilized to construct a scoring function with theoretical guarantees of stability that quantifies the periodicity of a single univariate time series f1, denoted score(f1). Building on this concept, we propose a conditional periodicity score that quantifies the periodicity similarity of two univariate time series, denoted score(f1|f2), and derive theoretical stability results for the same. We prove stability of score(f1|f2) under orthogonal projection of the time series embeddings onto their first K principal components. We show that the change in our score is bounded by a function of the eigenvalues corresponding to the remaining (unused) N-K principal components and hence is small when the first K principal components capture most of the variation in the time series embeddings. We derive a lower bound on the embedding dimension to use in our pipeline which guarantees that any two such embeddings produce scores that are linearly within epsilon of each other. We present a procedure for computing conditional periodicity scores and implement it on several types of synthetic signals. We experimentally compare our similarity measure to the most-similar statistical measure of percent determinism (%DET) and show greater stability of score(f1|f2). We also compare both measures on several pairs of real time series describing monthly proportions of incoming calls to a police agency and highlight the decreased stability of %DET on the same.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02817v3</guid>
      <category>math.AT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bala Krishnamoorthy, Elizabeth P. Thompson</dc:creator>
    </item>
    <item>
      <title>Understanding Generalization in Physics Informed Models through Affine Variety Dimensions</title>
      <link>https://arxiv.org/abs/2501.18879</link>
      <description>arXiv:2501.18879v2 Announce Type: replace-cross 
Abstract: Physics-informed machine learning is gaining significant traction for enhancing statistical performance and sample efficiency through the integration of physical knowledge. However, current theoretical analyses often presume complete prior knowledge in non-hybrid settings, overlooking the crucial integration of observational data, and are frequently limited to linear systems, unlike the prevalent nonlinear nature of many real-world applications. To address these limitations, we introduce a unified residual form that unifies collocation and variational methods, enabling the incorporation of incomplete and complex physical constraints in hybrid learning settings. Within this formulation, we establish that the generalization performance of physics-informed regression in such hybrid settings is governed by the dimension of the affine variety associated with the physical constraint, rather than by the number of parameters. This enables a unified analysis that is applicable to both linear and nonlinear equations. We also present a method to approximate this dimension and provide experimental validation of our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18879v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takeshi Koshizuka, Issei Sato</dc:creator>
    </item>
    <item>
      <title>A Closed-Form Transition Density Expansion for Elliptic and Hypo-Elliptic SDEs</title>
      <link>https://arxiv.org/abs/2502.07047</link>
      <description>arXiv:2502.07047v2 Announce Type: replace-cross 
Abstract: We introduce a closed-form expansion for the transition density of elliptic and hypo-elliptic multivariate Stochastic Differential Equations (SDEs), over a period $\Delta\in (0,1)$, in terms of powers of $\Delta^{j/2}$, $j\ge 0$. Our methodology provides approximations of the transition density, easily evaluated via any software that performs symbolic calculations. A major part of the paper is devoted to an analytical control of the remainder in our expansion for fixed $\Delta\in(0,1)$. The obtained error bounds validate theoretically the methodology, by characterising the size of the distance from the true value. It is the first time that such a closed-form expansion becomes available for the important class of hypo-elliptic SDEs, to the best of our knowledge. For elliptic SDEs, closed-form expansions are available, with some works identifying the size of the error for fixed $\Delta$, as per our contribution. Our methodology allows for a uniform treatment of elliptic and hypo-elliptic SDEs, when earlier works are intrinsically restricted to an elliptic setting. We show numerical applications highlighting the effectiveness of our method, by carrying out parameter inference for hypo-elliptic SDEs that do not satisfy stated conditions. The latter are sufficient for controlling the remainder terms, but the closed-form expansion itself is applicable in general settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07047v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuga Iguchi, Alexandros Beskos</dc:creator>
    </item>
    <item>
      <title>Minimax optimal transfer learning for high-dimensional additive regression</title>
      <link>https://arxiv.org/abs/2509.06308</link>
      <description>arXiv:2509.06308v2 Announce Type: replace-cross 
Abstract: This paper studies high-dimensional additive regression under the transfer learning framework, where one observes samples from a target population together with auxiliary samples from different but potentially related regression models. We first introduce a target-only estimation procedure based on the smooth backfitting estimator with local linear smoothing. In contrast to previous work, we establish general error bounds under sub-Weibull($\alpha$) noise, thereby accommodating heavy-tailed error distributions. In the sub-exponential case ($\alpha=1$), we show that the estimator attains the minimax lower bound under regularity conditions, which requires a substantial departure from existing proof strategies. We then develop a novel two-stage estimation method within a transfer learning framework, and provide theoretical guarantees at both the population and empirical levels. Error bounds are derived for each stage under general tail conditions, and we further demonstrate that the minimax optimal rate is achieved when the auxiliary and target distributions are sufficiently close. All theoretical results are supported by simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06308v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Seung Hyun Moon</dc:creator>
    </item>
  </channel>
</rss>
