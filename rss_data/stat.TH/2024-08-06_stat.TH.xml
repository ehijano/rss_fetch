<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 Aug 2024 04:00:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 06 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Infinite random forests for imbalanced classification tasks</title>
      <link>https://arxiv.org/abs/2408.01777</link>
      <description>arXiv:2408.01777v1 Announce Type: new 
Abstract: This paper investigates predictive probability inference for classification tasks using random forests in the context of imbalanced data. In this setting, we analyze the asymptotic properties of simplified versions of the original Breiman's algorithm, namely subsampling and under-sampling Infinite Random Forests (IRFs), and establish the asymptotic normality of these two models. The under-sampling IRFs, that tackle the challenge of the predicting the minority class by a downsampling strategy to under-represent the majority class show asymptotic bias. To address this problem, we introduce a new estimator based on an Importance Sampling debiasing procedure built upon on odds ratios. We apply our results considering 1-Nearest Neighbors (1-NN) as individual trees of the IRFs. The resulting bagged 1-NN estimator achieves the same asymptotic rate in the three configurations but with a different asymptotic variance. Finally, we conduct simulations to validate the empirical relevance of our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01777v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moria Mayala, Olivier Wintenberger, Charles Tillier, Cl\'ement Dombry</dc:creator>
    </item>
    <item>
      <title>Probabilities in multimatrix variate distributions: an application in SARS-CoV-2</title>
      <link>https://arxiv.org/abs/2408.02059</link>
      <description>arXiv:2408.02059v1 Announce Type: new 
Abstract: Recently the termed \emph{multimatrix variate distributions} were proposed in \citet{dgcl:24a} as an alternative for univariate and vector variate copulas. The distributions are based on sample probabilistic dependent elliptically countered models and most of them are also invariant under this family of laws. Despite a large of results on matrix variate distributions since the last 70 years, the spherical multimatrix distributions and the associated probabilities on hyper cones can be computable. The multiple probabilities are set in terms of recurrent integrations allowing several matrix computation a feasible task. An application of the emerging probabilities is placed into a dynamic molecular docking in the SARS-CoV-2 main protease. Finally, integration over multimatrix Wishart distribution provides a simplification of a complex kernel integral in elliptical models under real normed division algebras and the solution was applied in elliptical affine shape theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02059v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francisco J. Caro-Lopera, Jos\'e A. D\'iaz-Garc\'ia</dc:creator>
    </item>
    <item>
      <title>Winners with Confidence: Discrete Argmin Inference with an Application to Model Selection</title>
      <link>https://arxiv.org/abs/2408.02060</link>
      <description>arXiv:2408.02060v1 Announce Type: new 
Abstract: We study the problem of finding the index of the minimum value of a vector from noisy observations. This problem is relevant in population/policy comparison, discrete maximum likelihood, and model selection. We develop a test statistic that is asymptotically normal, even in high-dimensional settings and with potentially many ties in the population mean vector, by integrating concepts and tools from cross-validation and differential privacy. The key technical ingredient is a central limit theorem for globally dependent data. We also propose practical ways to select the tuning parameter that adapts to the signal landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02060v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tianyu Zhang, Hao Lee, Jing Lei</dc:creator>
    </item>
    <item>
      <title>Optimal Estimation of Structured Covariance Operators</title>
      <link>https://arxiv.org/abs/2408.02109</link>
      <description>arXiv:2408.02109v1 Announce Type: new 
Abstract: This paper establishes optimal convergence rates for estimation of structured covariance operators of Gaussian processes. We study banded operators with kernels that decay rapidly off-the-diagonal and $L^q$-sparse operators with an unordered sparsity pattern. For these classes of operators, we find the minimax optimal rate of estimation in operator norm, identifying the fundamental dimension-free quantities that determine the sample complexity. In addition, we prove that tapering and thresholding estimators attain the optimal rate. The proof of the upper bound for tapering estimators requires novel techniques to circumvent the issue that discretization of a banded operator does not result, in general, in a banded covariance matrix. To derive lower bounds for banded and $L^q$-sparse classes, we introduce a general framework to lift theory from high-dimensional matrix estimation to the operator setting. Our work contributes to the growing literature on operator estimation and learning, building on ideas from high-dimensional statistics while also addressing new challenges that emerge in infinite dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02109v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omar Al-Ghattas, Jiaheng Chen, Daniel Sanz-Alonso, Nathan Waniorek</dc:creator>
    </item>
    <item>
      <title>Small dispersion asymptotics for an SPDE in two space dimensions using triple increments</title>
      <link>https://arxiv.org/abs/2408.02224</link>
      <description>arXiv:2408.02224v1 Announce Type: new 
Abstract: We consider parametric estimation for a second order linear parabolic stochastic partial differential equation (SPDE) in two space dimensions driven by a $Q$-Wiener process with a small noise based on high frequency spatio-temporal data. We first provide estimators of the diffusive and advective parameters in the SPDE using temporal and spatial increments. We then construct an estimator of the reaction parameter in the SPDE based on an approximate coordinate process. We also give simulation results of the proposed estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02224v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yozo Tonaki, Yusuke Kaino, Masayuki Uchida</dc:creator>
    </item>
    <item>
      <title>Kullback-Leibler-based characterizations of score-driven updates</title>
      <link>https://arxiv.org/abs/2408.02391</link>
      <description>arXiv:2408.02391v1 Announce Type: new 
Abstract: Score-driven models have been applied in some 400 published articles over the last decade. Much of this literature cites the optimality result in Blasques et al. (2015), which, roughly, states that sufficiently small score-driven updates are unique in locally reducing the Kullback-Leibler (KL) divergence relative to the true density for every observation. This is at odds with other well-known optimality results; the Kalman filter, for example, is optimal in a mean squared error sense, but may move in the wrong direction for atypical observations. We show that score-driven filters are, similarly, not guaranteed to improve the localized KL divergence at every observation. The seemingly stronger result in Blasques et al. (2015) is due to their use of an improper (localized) scoring rule. Even as a guaranteed improvement for every observation is unattainable, we prove that sufficiently small score-driven updates are unique in reducing the KL divergence relative to the true density in expectation. This positive$-$albeit weaker$-$result justifies the continued use of score-driven models and places their information-theoretic properties on solid footing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02391v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ramon de Punder, Timo Dimitriadis, Rutger-Jan Lange</dc:creator>
    </item>
    <item>
      <title>Meta-Posterior Consistency for the Bayesian Inference of Metastable System</title>
      <link>https://arxiv.org/abs/2408.01868</link>
      <description>arXiv:2408.01868v1 Announce Type: cross 
Abstract: The vast majority of the literature on learning dynamical systems or stochastic processes from time series has focused on stable or ergodic systems, for both Bayesian and frequentist inference procedures. However, most real-world systems are only metastable, that is, the dynamics appear to be stable on some time scale, but are in fact unstable over longer time scales. Consistency of inference for metastable systems may not be possible, but one can ask about metaconsistency: Do inference procedures converge when observations are taken over a large but finite time interval, but diverge on longer time scales? In this paper we introduce, discuss, and quantify metaconsistency in a Bayesian framework. We discuss how metaconsistency can be exploited to efficiently infer a model for a sub-system of a larger system, where inference on the global behavior may require much more data. We also discuss the relation between meta-consistency and the spectral properties of the model dynamical system in the case of uniformly ergodic diffusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01868v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zachary P Adams, Sayan Mukherjee</dc:creator>
    </item>
    <item>
      <title>A Sharp Convergence Theory for The Probability Flow ODEs of Diffusion Models</title>
      <link>https://arxiv.org/abs/2408.02320</link>
      <description>arXiv:2408.02320v1 Announce Type: cross 
Abstract: Diffusion models, which convert noise into new data instances by learning to reverse a diffusion process, have become a cornerstone in contemporary generative modeling. In this work, we develop non-asymptotic convergence theory for a popular diffusion-based sampler (i.e., the probability flow ODE sampler) in discrete time, assuming access to $\ell_2$-accurate estimates of the (Stein) score functions. For distributions in $\mathbb{R}^d$, we prove that $d/\varepsilon$ iterations -- modulo some logarithmic and lower-order terms -- are sufficient to approximate the target distribution to within $\varepsilon$ total-variation distance. This is the first result establishing nearly linear dimension-dependency (in $d$) for the probability flow ODE sampler. Imposing only minimal assumptions on the target data distribution (e.g., no smoothness assumption is imposed), our results also characterize how $\ell_2$ score estimation errors affect the quality of the data generation processes. In contrast to prior works, our theory is developed based on an elementary yet versatile non-asymptotic approach without the need of resorting to SDE and ODE toolboxes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02320v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>eess.SP</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gen Li, Yuting Wei, Yuejie Chi, Yuxin Chen</dc:creator>
    </item>
    <item>
      <title>Explaining and Connecting Kriging with Gaussian Process Regression</title>
      <link>https://arxiv.org/abs/2408.02331</link>
      <description>arXiv:2408.02331v1 Announce Type: cross 
Abstract: Kriging and Gaussian Process Regression are statistical methods that allow predicting the outcome of a random process or a random field by using a sample of correlated observations. In other words, the random process or random field is partially observed, and by using a sample a prediction is made, pointwise or as a whole, where the latter can be thought as a reconstruction. In addition, the techniques permit to give a measure of uncertainty of the prediction. The methods have different origins. Kriging comes from geostatistics, a field which started to develop around 1950 oriented to mining valuation problems, whereas Gaussian Process Regression has gained popularity in the area of machine learning in the last decade of the previous century. In the literature, the methods are usually presented as being the same technique. However, beyond this affirmation, the techniques have yet not been compared on a thorough mathematical basis and neither explained why and under which conditions this affirmation holds. Furthermore, Kriging has many variants and this affirmation should be precised. In this paper, this gap is filled. It is shown, step by step how both methods are deduced from the first principles -- with a major focus on Kriging, the mathematical connection between them, and which Kriging variant corresponds to which Gaussian Process Regression set up. The three most widely used versions of Kriging are considered: Simple Kriging, Ordinary Kriging and Universal Kriging. It is found, that despite their closeness, the techniques are different in their approach and assumptions, in a similar way the Least Square method, the Best Linear Unbiased Estimator method, and the Likelihood method in regression do. I hope this work can serve for a deeper understanding of the relationship between Kriging and Gaussian Process Regression, as well as a cohesive introductory resource for researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02331v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marius Marinescu</dc:creator>
    </item>
    <item>
      <title>Confidence intervals for median absolute deviations</title>
      <link>https://arxiv.org/abs/1910.00229</link>
      <description>arXiv:1910.00229v5 Announce Type: replace 
Abstract: The median absolute deviation (MAD) is a robust measure of scale that is simple to implement and easy to interpret. Motivated by this, we introduce interval estimators of the MAD to make reliable inferences for dispersion for a single population and ratios and differences of MADs for comparing two populations. Our simulation results show that the coverage probabilities of the intervals are very close to the nominal coverage for a variety of distributions. We have used partial influence functions to investigate the robustness properties of the difference and ratios of independent MADs.</description>
      <guid isPermaLink="false">oai:arXiv.org:1910.00229v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chandima N. P. G. Arachchige, Luke A. Prendergast</dc:creator>
    </item>
    <item>
      <title>On the role of parametrization in models with a misspecified nuisance component</title>
      <link>https://arxiv.org/abs/2402.05708</link>
      <description>arXiv:2402.05708v2 Announce Type: replace 
Abstract: The paper is concerned with inference for a parameter of interest in models that share a common interpretation for that parameter but that may differ appreciably in other respects. We study the general structure of models under which the maximum likelihood estimator of the parameter of interest is consistent under arbitrary misspecification of the nuisance part of the model. A specialization of the general results to matched-comparison and two-groups problems gives a more explicit and easily checkable condition in terms of a new notion of symmetric parametrization, leading to a broadening and unification of existing results in those problems. The role of a generalized definition of parameter orthogonality is highlighted, as well as connections to Neyman orthogonality. The issues involved in obtaining inferential guarantees beyond consistency are briefly discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05708v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heather Battey, Nancy Reid</dc:creator>
    </item>
    <item>
      <title>Estimating Lagged (Cross-)Covariance Operators of $L^p$-$m$-approximable Processes in Cartesian Product Hilbert Spaces</title>
      <link>https://arxiv.org/abs/2402.08110</link>
      <description>arXiv:2402.08110v3 Announce Type: replace 
Abstract: Estimating parameters of functional ARMA, GARCH and invertible processes requires estimating lagged covariance and cross-covariance operators of Cartesian product Hilbert space-valued processes. Asymptotic results have been derived in recent years, either less generally or under a strict condition. This article derives upper bounds of the estimation errors for such operators based on the mild condition Lp-m-approximability for each lag, Cartesian power(s) and sample size, where the two processes can take values in different spaces in the context of lagged cross-covariance operators. Implications of our results on eigenelements, parameters in functional AR(MA) models and other general situations are also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08110v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastian K\"uhnert</dc:creator>
    </item>
    <item>
      <title>Information divergences and likelihood ratios of Poisson processes and point patterns</title>
      <link>https://arxiv.org/abs/2404.00294</link>
      <description>arXiv:2404.00294v2 Announce Type: replace 
Abstract: This article develops an analytical framework for studying information divergences and likelihood ratios associated with Poisson processes and point patterns on general measurable spaces. The main results include explicit analytical formulas for Kullback-Leibler divergences, R\'enyi divergences, Hellinger distances, and likelihood ratios of the laws of Poisson point patterns in terms of their intensity measures. The general results yield similar formulas for inhomogeneous Poisson processes, compound Poisson processes, as well as spatial and marked Poisson point patterns. Additional results include simple characterisations of absolute continuity, mutual singularity, and the existence of common dominating measures. The analytical toolbox is based on Tsallis divergences of sigma-finite measures on abstract measurable spaces. The treatment is purely information-theoretic and free of topological assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00294v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lasse Leskel\"a</dc:creator>
    </item>
    <item>
      <title>Maximum mean discrepancies of Farey sequences</title>
      <link>https://arxiv.org/abs/2407.10214</link>
      <description>arXiv:2407.10214v2 Announce Type: replace 
Abstract: We identify a large class of positive-semidefinite kernels for which a certain polynomial rate of convergence of maximum mean discrepancies of Farey sequences is equivalent to the Riemann hypothesis. This class includes all Mat\'ern kernels of order at least one-half.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10214v2</guid>
      <category>math.ST</category>
      <category>math.NT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toni Karvonen, Anatoly Zhigljavsky</dc:creator>
    </item>
    <item>
      <title>Mean and Covariance Estimation for Discretely Observed High-Dimensional Functional Data: Rates of Convergence and Division of Observational Regimes</title>
      <link>https://arxiv.org/abs/2408.01326</link>
      <description>arXiv:2408.01326v2 Announce Type: replace 
Abstract: Estimation of the mean and covariance parameters for functional data is a critical task, with local linear smoothing being a popular choice. In recent years, many scientific domains are producing multivariate functional data for which $p$, the number of curves per subject, is often much larger than the sample size $n$. In this setting of high-dimensional functional data, much of developed methodology relies on preliminary estimates of the unknown mean functions and the auto- and cross-covariance functions. This paper investigates the convergence rates of local linear estimators in terms of the maximal error across components and pairs of components for mean and covariance functions, respectively, in both $L^2$ and uniform metrics. The local linear estimators utilize a generic weighting scheme that can adjust for differing numbers of discrete observations $N_{ij}$ across curves $j$ and subjects $i$, where the $N_{ij}$ vary with $n$. Particular attention is given to the equal weight per observation (OBS) and equal weight per subject (SUBJ) weighting schemes. The theoretical results utilize novel applications of concentration inequalities for functional data and demonstrate that, similar to univariate functional data, the order of the $N_{ij}$ relative to $p$ and $n$ divides high-dimensional functional data into three regimes (sparse, dense, and ultra-dense), with the high-dimensional parametric convergence rate of $\left\{\log(p)/n\right\}^{1/2}$ being attainable in the latter two.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01326v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexander Petersen</dc:creator>
    </item>
    <item>
      <title>Design-Based Inference for Spatial Experiments under Unknown Interference</title>
      <link>https://arxiv.org/abs/2010.13599</link>
      <description>arXiv:2010.13599v5 Announce Type: replace-cross 
Abstract: We consider design-based causal inference for spatial experiments in which treatments may have effects that bleed out and feed back in complex ways. Such spatial spillover effects violate the standard ``no interference'' assumption for standard causal inference methods. The complexity of spatial spillover effects also raises the risk of misspecification and bias in model-based analyses. We offer an approach for robust inference in such settings without having to specify a parametric outcome model. We define a spatial ``average marginalized effect'' (AME) that characterizes how, in expectation, units of observation that are a specified distance from an intervention location are affected by treatment at that location, averaging over effects emanating from other intervention nodes. We show that randomization is sufficient for non-parametric identification of the AME even if the nature of interference is unknown. Under mild restrictions on the extent of interference, we establish asymptotic distributions of estimators and provide methods for both sample-theoretic and randomization-based inference. We show conditions under which the AME recovers a structural effect. We illustrate our approach with a simulation study. Then we re-analyze a randomized field experiment and a quasi-experiment on forest conservation, showing how our approach offers robust inference on policy-relevant spillover effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.13599v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Wang, Cyrus Samii, Haoge Chang, P. M. Aronow</dc:creator>
    </item>
    <item>
      <title>Robust Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models</title>
      <link>https://arxiv.org/abs/2209.15224</link>
      <description>arXiv:2209.15224v4 Announce Type: replace-cross 
Abstract: Unsupervised learning has been widely used in many real-world applications. One of the simplest and most important unsupervised learning models is the Gaussian mixture model (GMM). In this work, we study the multi-task learning problem on GMMs, which aims to leverage potentially similar GMM parameter structures among tasks to obtain improved learning performance compared to single-task learning. We propose a multi-task GMM learning procedure based on the EM algorithm that effectively utilizes unknown similarities between related tasks and is robust against a fraction of outlier tasks from arbitrary distributions. The proposed procedure is shown to achieve the minimax optimal rate of convergence for both parameter estimation error and the excess mis-clustering error, in a wide range of regimes. Moreover, we generalize our approach to tackle the problem of transfer learning for GMMs, where similar theoretical results are derived. Additionally, iterative unsupervised multi-task and transfer learning methods may suffer from an initialization alignment problem, and two alignment algorithms are proposed to resolve the issue. Finally, we demonstrate the effectiveness of our methods through simulations and real data examples. To the best of our knowledge, this is the first work studying multi-task and transfer learning on GMMs with theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.15224v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Tian, Haolei Weng, Lucy Xia, Yang Feng</dc:creator>
    </item>
    <item>
      <title>Mode-wise Principal Subspace Pursuit and Matrix Spiked Covariance Model</title>
      <link>https://arxiv.org/abs/2307.00575</link>
      <description>arXiv:2307.00575v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel framework called Mode-wise Principal Subspace Pursuit (MOP-UP) to extract hidden variations in both the row and column dimensions for matrix data. To enhance the understanding of the framework, we introduce a class of matrix-variate spiked covariance models that serve as inspiration for the development of the MOP-UP algorithm. The MOP-UP algorithm consists of two steps: Average Subspace Capture (ASC) and Alternating Projection (AP). These steps are specifically designed to capture the row-wise and column-wise dimension-reduced subspaces which contain the most informative features of the data. ASC utilizes a novel average projection operator as initialization and achieves exact recovery in the noiseless setting. We analyze the convergence and non-asymptotic error bounds of MOP-UP, introducing a blockwise matrix eigenvalue perturbation bound that proves the desired bound, where classic perturbation bounds fail. The effectiveness and practical merits of the proposed framework are demonstrated through experiments on both simulated and real datasets. Lastly, we discuss generalizations of our approach to higher-order data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.00575v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Runshi Tang, Ming Yuan, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Solving PDEs on Spheres with Physics-Informed Convolutional Neural Networks</title>
      <link>https://arxiv.org/abs/2308.09605</link>
      <description>arXiv:2308.09605v2 Announce Type: replace-cross 
Abstract: Physics-informed neural networks (PINNs) have been demonstrated to be efficient in solving partial differential equations (PDEs) from a variety of experimental perspectives. Some recent studies have also proposed PINN algorithms for PDEs on surfaces, including spheres. However, theoretical understanding of the numerical performance of PINNs, especially PINNs on surfaces or manifolds, is still lacking. In this paper, we establish rigorous analysis of the physics-informed convolutional neural network (PICNN) for solving PDEs on the sphere. By using and improving the latest approximation results of deep convolutional neural networks and spherical harmonic analysis, we prove an upper bound for the approximation error with respect to the Sobolev norm. Subsequently, we integrate this with innovative localization complexity analysis to establish fast convergence rates for PICNN. Our theoretical results are also confirmed and supplemented by our experiments. In light of these findings, we explore potential strategies for circumventing the curse of dimensionality that arises when solving high-dimensional PDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.09605v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guanhang Lei, Zhen Lei, Lei Shi, Chenyu Zeng, Ding-Xuan Zhou</dc:creator>
    </item>
    <item>
      <title>Conjugacy properties of multivariate unified skew-elliptical distributions</title>
      <link>https://arxiv.org/abs/2402.09837</link>
      <description>arXiv:2402.09837v3 Announce Type: replace-cross 
Abstract: The broad class of multivariate unified skew-normal (SUN) distributions has been recently shown to possess important conjugacy properties. When used as priors for the coefficients vector in probit, tobit, and multinomial probit models, these distributions yield posteriors that still belong to the SUN family. Although this result has led to important advancements in Bayesian inference and computation, its applicability beyond likelihoods associated with fully-observed, discretized, or censored realizations from multivariate Gaussian models remains yet unexplored. This article covers such a gap by proving that the wider family of multivariate unified skew-elliptical (SUE) distributions, which extends SUNs to more general perturbations of elliptical densities, guarantees conjugacy for broader classes of models, beyond those relying on fully-observed, discretized or censored Gaussians. Such a result leverages the closure under linear combinations, conditioning and marginalization of SUE to prove that this family is conjugate to the likelihood induced by multivariate regression models for fully-observed, censored or dichotomized realizations from skew-elliptical distributions. This advancement enlarges the set of models that enable conjugate Bayesian inference to general formulations arising from elliptical and skew-elliptical families, including the multivariate Student's t and skew-t, among others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09837v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maicon J. Karling, Daniele Durante, Marc G. Genton</dc:creator>
    </item>
    <item>
      <title>High-arity PAC learning via exchangeability</title>
      <link>https://arxiv.org/abs/2402.14294</link>
      <description>arXiv:2402.14294v2 Announce Type: replace-cross 
Abstract: We develop a theory of high-arity PAC learning, which is statistical learning in the presence of "structured correlation". In this theory, hypotheses are either graphs, hypergraphs or, more generally, structures in finite relational languages, and i.i.d. sampling is replaced by sampling an induced substructure, producing an exchangeable distribution. Our main theorems establish a high-arity (agnostic) version of the fundamental theorem of statistical learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14294v2</guid>
      <category>cs.LG</category>
      <category>math.LO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonardo N. Coregliano, Maryanthe Malliaris</dc:creator>
    </item>
    <item>
      <title>Asymptotic Expansions of the Limit Laws of Gaussian and Laguerre (Wishart) Ensembles at the Soft Edge</title>
      <link>https://arxiv.org/abs/2403.07628</link>
      <description>arXiv:2403.07628v2 Announce Type: replace-cross 
Abstract: The large-matrix limit laws of the rescaled largest eigenvalue of the orthogonal, unitary and symplectic $n$-dimensional Gaussian ensembles -- and of the corresponding Laguerre ensembles (Wishart distributions) for various regimes of the parameter $\alpha$ (degrees of freedom $p$) -- are known to be the Tracy-Widom distributions $F_\beta$ ($\beta=1,2,4$). We will establish (paying particular attention to large, or small, ratios $p/n$) that, with careful choices of the rescaling constants and the expansion parameter $h$, the limit laws embed into asymptotic expansions in powers of $h$, where $h \asymp n^{-2/3}$ resp. $h \asymp (n\,\wedge\,p)^{-2/3}$. We find explicit analytic expressions of the first few expansions terms as linear combinations, with rational polynomial coefficients, of higher order derivatives of the limit law $F_\beta$. With a proper parametrization, the expansions in the Gaussian cases can be understood, for given $n$, as the limit $p\to\infty$ of the Laguerre cases. Whereas the results for $\beta=2$ are presented with proof, the discussion of the cases $\beta=1,4$ is based on some hypotheses, focussing on the algebraic aspects of actually computing the polynomial coefficients. For the purposes of illustration and validation, the various results are checked against simulation data with large sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07628v2</guid>
      <category>math.PR</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Folkmar Bornemann</dc:creator>
    </item>
    <item>
      <title>Conditioning of Banach Space Valued Gaussian Random Variables: An Approximation Approach Based on Martingales</title>
      <link>https://arxiv.org/abs/2404.03453</link>
      <description>arXiv:2404.03453v2 Announce Type: replace-cross 
Abstract: In this paper we investigate the conditional distributions of two Banach space valued, jointly Gaussian random variables. We show that these conditional distributions are again Gaussian and that their means and covariances are determined by a general finite dimensional approximation scheme based upon a martingale approach. In particular, it turns out that the covariance operators occurring in this scheme converge with respect to the nuclear norm and that the conditional probabilities converge weakly. Moreover, we discuss in detail, how our approximation scheme can be implemented in several classes of important Banach spaces such as RKHSs and $C(T)$. As an example, we then apply our general results to the case of Gaussian processes with continuous paths conditioned to partial but infinite observations of their paths. Here we show that conditioning on sufficiently rich, increasing sets of finitely many observations leads to consistent approximations, in the sense that both the mean and covariance functions converge uniformly. Moreover, we discuss how these results improve our understanding of the popular Gaussian processes for machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03453v2</guid>
      <category>math.PR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ingo Steinwart</dc:creator>
    </item>
    <item>
      <title>Evaluating Quantumness, Efficiency and Cost of Quantum Random Number Generators via Photon Statistics</title>
      <link>https://arxiv.org/abs/2405.14085</link>
      <description>arXiv:2405.14085v3 Announce Type: replace-cross 
Abstract: Despite the availability of commercial QRNG devices, distinguishing between PRNG and QRNG outputs computationally remains challenging. This paper presents two significant contributions from the perspectives of QRNG manufacturers and users. For manufacturers, the conventional method of assessing the quantumness of single-photon-based QRNGs through mean and variance comparisons of photon counts is statistically unreliable due to finite sample sizes. Given the sub-Poissonian statistics of single photons, confirming the underlying distribution is crucial for validating a QRNG's quantumness. We propose a more efficient two-fold statistical approach to ensure the quantumness of optical sources with the desired confidence level. Additionally, we demonstrate that the output of QRNGs from exponential and uniform distributions exhibit similarity under device noise, deriving corresponding photon statistics and conditions for $\epsilon$-randomness.
  From the user's perspective, the fundamental parameters of a QRNG are quantumness (security), efficiency (randomness and random number generation rate), and cost. Our analysis reveals that these parameters depend on three factors, expected photon count per unit time, external reference cycle duration, and detection efficiency. A lower expected photon count enhances security but increases cost and decreases the generation rate. A shorter external reference cycle boosts security but must exceed a minimum threshold to minimize timing errors, with minor impacts on cost and rate. Lower detection efficiency enhances security and lowers cost but reduces the generation rate. Finally, to validate our results, we perform statistical tests like NIST, Dieharder, AIS-31, ENT etc. over the data simulated with different values of the above parameters. Our findings can empower manufacturers to customize QRNGs to meet user needs effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14085v3</guid>
      <category>quant-ph</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Goutam Paul, Nirupam Basak, Soumya Das</dc:creator>
    </item>
  </channel>
</rss>
