<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Feb 2026 02:45:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>MathlibLemma: Folklore Lemma Generation and Benchmark for Formal Mathematics</title>
      <link>https://arxiv.org/abs/2602.02561</link>
      <description>arXiv:2602.02561v1 Announce Type: new 
Abstract: While the ecosystem of Lean and Mathlib has enjoyed celebrated success in formal mathematical reasoning with the help of large language models (LLMs), the absence of many folklore lemmas in Mathlib remains a persistent barrier that limits Lean's usability as an everyday tool for mathematicians like LaTeX or Maple. To address this, we introduce MathlibLemma, the first LLM-based multi-agent system to automate the discovery and formalization of mathematical folklore lemmas. This framework constitutes our primary contribution, proactively mining the missing connective tissue of mathematics. Its efficacy is demonstrated by the production of a verified library of folklore lemmas, a subset of which has already been formally merged into the latest build of Mathlib, thereby validating the system's real-world utility and alignment with expert standards. Leveraging this pipeline, we further construct the MathlibLemma benchmark, a suite of 4,028 type-checked Lean statements spanning a broad range of mathematical domains. By transforming the role of LLMs from passive consumers to active contributors, this work establishes a constructive methodology for the self-evolution of formal mathematical libraries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02561v1</guid>
      <category>cs.LO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyu Liu, Zixuan Xie, Amir Moeini, Claire Chen, Shuze Daniel Liu, Yu Meng, Aidong Zhang, Shangtong Zhang</dc:creator>
    </item>
    <item>
      <title>A Classical Linear $\lambda$-Calculus based on Contraposition</title>
      <link>https://arxiv.org/abs/2602.02822</link>
      <description>arXiv:2602.02822v1 Announce Type: new 
Abstract: We present a novel linear $\lambda$-calculus for Classical Multiplicative Exponential Linear Logic (\MELL) along the lines of the propositions-as-types paradigm. Starting from the standard term assignment for Intuitionistic Multiplicative Linear Logic (IMLL), we observe that if we incorporate linear negation, its involutive nature implies that both $A\multimap B$ and $B^\perp\multimap A^\perp$ should have the same proofs. The introduction of a linear modus tollens rule, stating that from $B^\perp\multimap A^\perp$ and $A$ we may conclude $B$, allows one to recover classical MLL. Furthermore, a term assignment for this elimination rule, {the study of proof normalization in a $\lambda$-calculus with this elimination rule} prompts us to define the novel notion of contra-substitution $t \{ a \backslash\!\backslash s \}$. Introduced alongside linear substitution, contra-substitution denotes the term that results from "grabbing" the unique occurrence of $a$ in $t$ and "pulling" from it, in order to turn the term $t$ inside out (much like a sock) and then replacing $a$ with $s$. We call the one-sided natural deduction presentation of classical MLL, the $\lambda_{\rm MLL}$-calculus. Guided by the behavior of contra-substitution in the presence of the exponentials, we extend it to a similar presentation for MELL. We prove that this calculus is sound and complete with respect to MELL and that it satisfies the standard properties of a typed programming language: subject reduction, confluence and strong normalization. Moreover, we show that several well-known term assignments for classical logic can be encoded in $\lambda_{\rm MLL}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02822v1</guid>
      <category>cs.LO</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo Barenbaum, Eduardo Bonelli, Leopoldo Lerena</dc:creator>
    </item>
    <item>
      <title>Towards Weak Stratification for Logics of Definitions</title>
      <link>https://arxiv.org/abs/2602.03072</link>
      <description>arXiv:2602.03072v1 Announce Type: new 
Abstract: The logic of definitions is a family of logics for encoding and reasoning about judgments, which are atomic predicates specified by inference rules. A definition associates an atomic predicate with a logical formula, which may itself depend on the predicate being defined. This leads to an apparent circularity which can be resolved by interpreting definitions as monotone fixed-point operators on terms, and which is enforced by imposing a stratification condition on definitions. In many instances, it is useful to consider definitions in which the predicate being defined appears negatively in the body of its definition. In the logic $\mathcal G$, underlying the Abella proof assistant, this is not allowed due to the stratification condition. One such application violating this condition is that of defining logical relations, which is a technique commonly used to prove properties about programming languages. Tiu has shown how to relax this stratification condition to allow for a broader body of definitions including that needed for logical relations. However, he only showed how to extend a core fragment of $\mathcal G$ with the weakened stratification condition, resulting in a logic he called $\mathrm{LD}$. In this work we show that the weakened stratification condition is also compatible with generic (nabla) quantification and general induction. The eventual aim of this work is to justify an extension of the Abella proof assistant allowing for such definitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03072v1</guid>
      <category>cs.LO</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Guermond</dc:creator>
    </item>
    <item>
      <title>On Complete Categorical Semantics for Effect Handlers</title>
      <link>https://arxiv.org/abs/2602.03275</link>
      <description>arXiv:2602.03275v1 Announce Type: new 
Abstract: Soundness and completeness with respect to equational theories for programming languages are fundamental properties in the study of categorical semantics. However, completeness results have not been established for programming languages with algebraic effects and handlers, which raises a question of whether the commonly used models in the literature, i.e., free model monads generated from algebraic theories, are the only valid semantic models for effect handlers. In this paper, we show that this is not the case. We identify the precise characterizations of categorical models of effect handlers that allow us to establish soundness and completeness results with respect to a certain equational theory for effect handling constructs. Notably, this allows us to capture not only free monad models but also the CPS semantics for effect handlers as models of the calculus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03275v1</guid>
      <category>cs.LO</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satoshi Kura</dc:creator>
    </item>
    <item>
      <title>Symbolic Model Checking using Intervals of Vectors</title>
      <link>https://arxiv.org/abs/2602.03565</link>
      <description>arXiv:2602.03565v1 Announce Type: new 
Abstract: Model checking is a powerful technique for software verification. However, the approach notably suffers from the infamous state space explosion problem. To tackle this, in this paper, we introduce a novel symbolic method for encoding Petri net markings. It is based on the use of generalised intervals on vectors, as opposed to existing methods based on vectors of intervals such as Interval Decision Diagrams. We develop a formalisation of these intervals, show that they possess homomorphic operations for model checking CTL on Petri nets, and define a canonical form that provides good performance characteristics. Our structure facilitates the symbolic evaluation of CTL formulas in the realm of global model checking, which aims to identify every state that satisfies a formula. Tests on examples of the model checking contest (MCC 2022) show that our approach yields promising results. To achieve this, we implement efficient computations based on saturation and clustering principles derived from other symbolic model checking techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03565v1</guid>
      <category>cs.LO</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Damien Morard, Lucas Donati, Didier Buchs</dc:creator>
    </item>
    <item>
      <title>A Formal Analysis of Capacity Scaling Algorithms for Minimum-Cost Flows</title>
      <link>https://arxiv.org/abs/2602.03701</link>
      <description>arXiv:2602.03701v1 Announce Type: new 
Abstract: We present formalisations of the correctness of executable algorithms to solve minimum-cost flow problems in Isabelle/HOL. Two of the algorithms are based on the technique of scaling, most notably Orlin's algorithm, which has the fastest known running time for solving the problem of minimum-cost flow. We also include a formalisation of the worst-case running time argument for Orlin's algorithm. Our verified implementation of this algorithm, which is derived by the technique of stepwise refinement, is fully executable and was integrated into a reusable formal library on graph algorithms. Because the problems for which Orlin's algorithm works are restricted, we also verified an executable reduction from the general minimum-cost flow problem. We believe we are the first to formally consider the problem of minimum-cost flows and, more generally, any scaling algorithms. Our work has also led to a number of mathematical insights and improvements to proofs as well as theorem statements, compared to all existing expositions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03701v1</guid>
      <category>cs.LO</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Abdulaziz, Thomas Ammer</dc:creator>
    </item>
    <item>
      <title>A vector logic for intensional formal semantics</title>
      <link>https://arxiv.org/abs/2602.02940</link>
      <description>arXiv:2602.02940v1 Announce Type: cross 
Abstract: Formal semantics and distributional semantics are distinct approaches to linguistic meaning: the former models meaning as reference via model-theoretic structures; the latter represents meaning as vectors in high-dimensional spaces shaped by usage. This paper proves that these frameworks are structurally compatible for intensional semantics. We establish that Kripke-style intensional models embed injectively into vector spaces, with semantic functions lifting to (multi)linear maps that preserve composition. The construction accommodates multiple index sorts (worlds, times, locations) via a compound index space, representing intensions as linear operators. Modal operators are derived algebraically: accessibility relations become linear operators, and modal conditions reduce to threshold checks on accumulated values. For uncountable index domains, we develop a measure-theoretic generalization in which necessity becomes truth almost everywhere and possibility becomes truth on a set of positive measure, a non-classical logic natural for continuous parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02940v1</guid>
      <category>math.LO</category>
      <category>cs.CL</category>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Quigley</dc:creator>
    </item>
    <item>
      <title>Formal Evidence Generation for Assurance Cases for Robotic Software Models</title>
      <link>https://arxiv.org/abs/2602.03550</link>
      <description>arXiv:2602.03550v1 Announce Type: cross 
Abstract: Robotics and Autonomous Systems are increasingly deployed in safety-critical domains, so that demonstrating their safety is essential. Assurance Cases (ACs) provide structured arguments supported by evidence, but generating and maintaining this evidence is labour-intensive, error-prone, and difficult to keep consistent as systems evolve. We present a model-based approach to systematically generating AC evidence by embedding formal verification into the assurance workflow. The approach addresses three challenges: systematically deriving formal assertions from natural language requirements using templates, orchestrating multiple formal verification tools to handle diverse property types, and integrating formal evidence production into the workflow. Leveraging RoboChart, a domain-specific modelling language with formal semantics, we combine model checking and theorem proving in our approach. Structured requirements are automatically transformed into formal assertions using predefined templates, and verification results are automatically integrated as evidence. Case studies demonstrate the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03550v1</guid>
      <category>cs.SE</category>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <category>cs.RO</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fang Yan, Simon Foster, Ana Cavalcanti, Ibrahim Habli, James Baxter</dc:creator>
    </item>
    <item>
      <title>Cargo Sherlock: An SMT-Based Checker for Software Trust Costs</title>
      <link>https://arxiv.org/abs/2512.12553</link>
      <description>arXiv:2512.12553v2 Announce Type: replace 
Abstract: Supply chain attacks threaten open-source software ecosystems. This paper proposes a formal framework for quantifying trust in third-party software dependencies that is both formally checkable - formalized in satisfiability modulo theories (SMT) - while at the same time incorporating human factors, like the number of downloads, authors, and other metadata that are commonly used to identify trustworthy software in practice. We use data from both software analysis tools and metadata to build a first-order relational model of software dependencies; to obtain an overall "trust cost" combining these factors, we propose a formalization based on the minimum trust problem which asks for the minimum cost of a set of assumptions which can be used to prove that the code is safe. We implement these ideas in Cargo Sherlock, targeted for Rust libraries (crates), incorporating a list of candidate assumptions motivated by quantifiable trust metrics identified in prior work. Our evaluation shows that Cargo Sherlock can be used to identify synthetically generated supply chain attacks and known incidents involving typosquatted and poorly AI-maintained crates, and that its performance scales to Rust crates with many dependencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12553v2</guid>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Hassnain, Anirudh Basu, Ethan Ng, Caleb Stanford</dc:creator>
    </item>
    <item>
      <title>Partial Rewriting and Value Interpretation of Logically Constrained Terms (Full Version)</title>
      <link>https://arxiv.org/abs/2601.22191</link>
      <description>arXiv:2601.22191v2 Announce Type: replace 
Abstract: Logically constrained term rewrite systems (LCTRSs) are a rewriting formalism that naturally supports built-in data structures, including integers and bit-vectors. The recent framework of existentially constrained terms and most general constrained rewriting on them (Takahata et al., 2025) has many advantages over the original approach of rewriting constrained terms. In this paper, we introduce partial constrained rewriting, a variant of rewriting existentially constrained terms whose underlying idea has already appeared implicitly in previous analyses and applications of LCTRSs. We examine the differences between these two notions of constrained rewriting. First, we establish a direct correspondence between them, leveraging subsumption and equivalence of constrained terms where appropriate. Then we give characterizations of each of them, using the interpretation of existentially constrained terms by instantiation. We further introduce the novel notion of value interpretation, that highlights subtle differences between partial and most general rewriting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22191v2</guid>
      <category>cs.LO</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takahito Aoto, Naoki Nishida, Jonas Sch\"opf</dc:creator>
    </item>
    <item>
      <title>Computational and Categorical Frameworks of Finite Ternary $\Gamma$-Semirings: Foundations, Algorithms, and Industrial Modeling Applications</title>
      <link>https://arxiv.org/abs/2511.12323</link>
      <description>arXiv:2511.12323v2 Announce Type: replace-cross 
Abstract: Purpose: This study extends the structural theory of finite commutative ternary $\Gamma$-semirings into a computational and categorical framework for explicit classification and constructive reasoning. Methods: Constraint-driven enumeration algorithms are developed to generate all non-isomorphic finite ternary $\Gamma$-semirings satisfying closure, distributivity, and symmetry. Automorphism analysis, canonical labeling, and pruning strategies ensure uniqueness and tractability, while categorical constructs formalize algebraic relationships. \\ \textit{Results:} The implementation classifies all systems of order $|T|\!\le\!4$ and verifies symmetry-based subvarieties. Complexity analysis confirms polynomial-time performance, and categorical interpretation connects ternary $\Gamma$-semirings with functorial models in universal algebra. \\ Conclusion: The work establishes a verified computational theory and categorical synthesis for finite ternary $\Gamma$-semirings, integrating algebraic structure, algorithmic enumeration, and symbolic computation to support future industrial and decision-model applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12323v2</guid>
      <category>math.RA</category>
      <category>cs.LO</category>
      <pubDate>Wed, 04 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chandrasekhar Gokavarapu (Lecturer in Mathematics, Government College), Dr D Madhusudhana Rao (Lecturer in Mathematics, Government College For Women)</dc:creator>
    </item>
  </channel>
</rss>
