<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Sep 2024 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A machine learning based material homogenization technique for in-plane loaded masonry walls</title>
      <link>https://arxiv.org/abs/2408.17018</link>
      <description>arXiv:2408.17018v1 Announce Type: new 
Abstract: In recent years, significant advancements have been made in computational methods for analyzing masonry structures. Within the Finite Element Method, two primary approaches have gained traction: Micro and Macro Scale modeling, and their subsequent integration via Multi-scale methods based on homogenization theory and the representative volume element concept. While Micro and Multi-scale approaches offer high fidelity, they often come with a substantial computational burden. On the other hand, calibrating homogenized material parameters in Macro-scale approaches presents challenges for practical engineering problems. Machine learning techniques have emerged as powerful tools for training models using vast datasets from various domains. In this context, we propose leveraging Machine Learning methods to develop a novel homogenization strategy for the in-plane analysis of masonry structures. This strategy involves automatically calibrating a continuum nonlinear damage constitutive law and an appropriate yield criteria using relevant data derived from Micro-scale analysis. The optimization process not only enhances material parameters but also refines yield criteria and damage evolution laws to better align with existing data. To achieve this, a virtual laboratory is created to conduct micro-model simulations that account for the individual behaviors of constituent materials. Subsequently, a data isotropization process is employed to reconcile the results with typical isotropic constitutive models. Next, an optimization algorithm that minimizes the difference of internal dissipated work between the micro and macro scales is executed. We apply this technique to the in-plane homogenization of a Flemish bond masonry wall. Evaluation examples, including simulations of shear and compression tests, demonstrate the method's accuracy compared to micro modeling of the entire structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17018v1</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Cornejo, Philip Kalkbrenner, Riccardo Rossi, Luca Pel\`a</dc:creator>
    </item>
    <item>
      <title>A survey on multi-fidelity surrogates for simulators with functional outputs: unified framework and benchmark</title>
      <link>https://arxiv.org/abs/2408.17075</link>
      <description>arXiv:2408.17075v1 Announce Type: new 
Abstract: Multi-fidelity surrogate models combining dimensionality reduction and an intermediate surrogate in the reduced space allow a cost-effective emulation of simulators with functional outputs. The surrogate is an input-output mapping learned from a limited number of simulator evaluations. This computational efficiency makes surrogates commonly used for many-query tasks. Diverse methods for building them have been proposed in the literature, but they have only been partially compared.
  This paper introduces a unified framework encompassing the different surrogate families, followed by a methodological comparison and the exposition of practical considerations. More than a dozen of existing multi-fidelity surrogates have been implemented under the unified framework and evaluated on a set of benchmark problems. Based on the results, guidelines and recommendations are proposed regarding multi-fidelity surrogates with functional outputs.
  Our study shows that most multi-fidelity surrogates outperform their tested single-fidelity counterparts under the considered settings. But no particular surrogate is performing better on every test case. Therefore, the selection of a surrogate should consider the specific properties of the emulated functions, in particular the correlation between the low- and high-fidelity simulators, the size of the training set, the local nonlinear variations in the residual fields, and the size of the training datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17075v1</guid>
      <category>cs.CE</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Brunel, Mathieu Balesdent, Lo\"ic Brevault, Rodolphe Le Riche, Bruno Sudret</dc:creator>
    </item>
    <item>
      <title>Modelling Growth, Remodelling and Damage of a Thick-walled Fibre-reinforced Artery with Active Response: Application to Cerebral Vasospasm and Treatment</title>
      <link>https://arxiv.org/abs/2408.17206</link>
      <description>arXiv:2408.17206v1 Announce Type: new 
Abstract: Cerebral vasospasm, a prolonged constriction of cerebral arteries, is the first cause of morbidity and mortality for patients who survive hospitalisation after aneurysmal subarachnoid haemorrhage. The recent finding that stent-retrievers can successfully treat the disease has challenged the viewpoint that damage to the extracellular matrix is necessary. We apply a 3D finite element rate-based constrained mixture model (rb-CMM) to simulate vasospasm, remodelling and treatment with stents. The artery is modelled as a thick-walled fibre-reinforced constrained mixture subject to physiological pressure and axial stretch. The model accounts for distributions of collagen fibre homeostatic stretches, VSMC active response, remodelling and damage. After simulating vasospasm and subsequent remodelling of the artery to a new homeostatic state, we simulate treatment with commonly available stent-retrievers. We perform a parameter study to examine how arterial diameter and thickness affect the success of stent treatment. The model predictions on the pressure required to mechanically resolve the constriction are consistent with stent-retrievers. In agreement with clinical observations, our model predicts that stent-retrievers tend to be effective in arteries of up to 3mm diameter, but fail in larger ones. Variations in arterial wall thickness significantly affect stent pressure requirements. We have developed a novel rb-CMM that accounts for VSMC active response, remodelling and damage. Consistently with clinical observations, simulations predict that stent-retrievers can mechanically resolve vasospasm. Moreover, accounting for a patient's arterial properties is important for predicting likelihood of stent success. This in silico tool has the potential to support clinical decision-making and guide the development and evaluation of dedicated stents for personalised treatment of vasospasm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17206v1</guid>
      <category>cs.CE</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giulia Pederzani, Andrii Grytsan, Alfons G. Hoekstra, Anne M. Robertson, Paul N. Watton</dc:creator>
    </item>
    <item>
      <title>Leveraging Blockchain and ANFIS for Optimal Supply Chain Management</title>
      <link>https://arxiv.org/abs/2408.17161</link>
      <description>arXiv:2408.17161v1 Announce Type: cross 
Abstract: The supply chain is a critical segment of the product manufacturing cycle, continuously influenced by risky, uncertain, and undesirable events. Optimizing flexibility in the supply chain presents a complex, multi-objective, and nonlinear programming challenge. In the poultry supply chain, the development of mass customization capabilities has led manufacturing companies to increasingly focus on offering tailored and customized services for individual products. To safeguard against data tampering and ensure the integrity of setup costs and overall profitability, a multi-signature decentralized finance (DeFi) protocol, integrated with the IoT on a blockchain platform, is proposed. Managing the poultry supply chain involves uncertainties that may not account for parameters such as delivery time to retailers, reorder time, and the number of requested products. To address these challenges, this study employs an adaptive neuro-fuzzy inference system (ANFIS), combining neural networks with fuzzy logic to compensate for the lack of data training in parameter identification. Through MATLAB simulations, the study investigates the average shop delivery duration, the reorder time, and the number of products per order. By implementing the proposed technique, the average delivery time decreases from 40 to 37 minutes, the reorder time decreases from five to four days, and the quantity of items requested per order grows from six to eleven. Additionally, the ANFIS model enhances overall supply chain performance by reducing transaction times by 15\% compared to conventional systems, thereby improving real-time responsiveness and boosting transparency in supply chain operations, effectively resolving operational issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17161v1</guid>
      <category>eess.SY</category>
      <category>cs.CE</category>
      <category>cs.SY</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amirfarhad Farhadi, Homayoun Safarpour Motealegh Mahalegi, Abolfazl Pourrezaeian Firouzabad, Azadeh Zamanifar, Majid Sorouri</dc:creator>
    </item>
    <item>
      <title>A Framework for Digital Asset Risks with Insurance Applications</title>
      <link>https://arxiv.org/abs/2408.17227</link>
      <description>arXiv:2408.17227v1 Announce Type: cross 
Abstract: The remarkable growth of digital assets, starting from the inception of Bitcoin in 2009 into a 1 trillion market in 2024, underscores the momentum behind disruptive technologies and the global appetite for digital assets. This paper develops a framework to enhance actuaries' understanding of the cyber risks associated with the developing digital asset ecosystem, as well as their measurement methods in the context of digital asset insurance. By integrating actuarial perspectives, we aim to enhance understanding and modeling of cyber risks at both the micro and systemic levels. The qualitative examination sheds light on blockchain technology and its associated risks, while our quantitative framework offers a rigorous approach to modeling cyber risks in digital asset insurance portfolios. This multifaceted approach serves three primary objectives: i) offer a clear and accessible education on the evolving digital asset ecosystem and the diverse spectrum of cyber risks it entails; ii) develop a scientifically rigorous framework for quantifying cyber risks in the digital asset ecosystem; iii) provide practical applications, including pricing strategies and tail risk management. Particularly, we develop frequency-severity models based on real loss data for pricing cyber risks in digit assets and utilize Monte Carlo simulation to estimate the tail risks, offering practical insights for risk management strategies. As digital assets continue to reshape finance, our work serves as a foundational step towards safeguarding the integrity and stability of this rapidly evolving landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17227v1</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengming Li, Jianxi Su, Maochao Xu, Jimmy Yuen</dc:creator>
    </item>
    <item>
      <title>Role of Data-driven Regional Growth Model in Shaping Brain Folding Patterns</title>
      <link>https://arxiv.org/abs/2408.17334</link>
      <description>arXiv:2408.17334v1 Announce Type: cross 
Abstract: The surface morphology of the developing mammalian brain is crucial for understanding brain function and dysfunction. Computational modeling offers valuable insights into the underlying mechanisms for early brain folding. While previous studies generally assume uniform growth, recent findings indicate significant regional variations in brain tissue growth. However, the role of these variations in cortical development remains unclear. In this study, we explored how regional cortical growth affects brain folding patterns. We first developed growth models for typical cortical regions using ML-assisted symbolic regression, based on longitudinal data from over 1,000 infant MRI scans that captured cortical surface area and thickness during perinatal and postnatal brains development. These models were subsequently integrated into computational software to simulate cortical development with anatomically realistic geometric models. We quantified the resulting folding patterns using metrics such as mean curvature, sulcal depth, and gyrification index. Our results demonstrate that regional growth models generate complex brain folding patterns that more closely match actual brains structures, both quantitatively and qualitatively, compared to uniform growth models. Growth magnitude plays a dominant role in shaping folding patterns, while growth trajectory has a minor influence. Moreover, multi-region models better capture the intricacies of brain folding than single-region models. Our results underscore the necessity and importance of incorporating regional growth heterogeneity into brain folding simulations, which could enhance early diagnosis and treatment of cortical malformations and neurodevelopmental disorders such as epilepsy and autism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17334v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CE</category>
      <category>cs.SC</category>
      <category>q-bio.TO</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jixin Hou, Zhengwang Wu, Xianyan Chen, Dajiang Zhu, Tianming Liu, Gang Li, Xianqiao Wang</dc:creator>
    </item>
    <item>
      <title>ECC Analyzer: Extract Trading Signal from Earnings Conference Calls using Large Language Model for Stock Performance Prediction</title>
      <link>https://arxiv.org/abs/2404.18470</link>
      <description>arXiv:2404.18470v2 Announce Type: replace 
Abstract: In the realm of financial analytics, leveraging unstructured data, such as earnings conference calls (ECCs), to forecast stock volatility is a critical challenge that has attracted both academics and investors. While previous studies have used multimodal deep learning-based models to obtain a general view of ECCs for volatility predicting, they often fail to capture detailed, complex information. Our research introduces a novel framework: \textbf{ECC Analyzer}, which utilizes large language models (LLMs) to extract richer, more predictive content from ECCs to aid the model's prediction performance. We use the pre-trained large models to extract textual and audio features from ECCs and implement a hierarchical information extraction strategy to extract more fine-grained information. This strategy first extracts paragraph-level general information by summarizing the text and then extracts fine-grained focus sentences using Retrieval-Augmented Generation (RAG). These features are then fused through multimodal feature fusion to perform volatility prediction. Experimental results demonstrate that our model outperforms traditional analytical benchmarks, confirming the effectiveness of advanced LLM techniques in financial analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18470v2</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>q-fin.RM</category>
      <category>q-fin.TR</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yupeng Cao, Zhi Chen, Qingyun Pei, Nathan Jinseok Lee, K. P. Subbalakshmi, Papa Momar Ndiaye</dc:creator>
    </item>
    <item>
      <title>The continuous accumulation of civilization core in the cycle of elements-creature, benefits and weapons</title>
      <link>https://arxiv.org/abs/2408.11317</link>
      <description>arXiv:2408.11317v2 Announce Type: replace 
Abstract: The comprehensive strength of a country varies from strong to weak, divided into three condition: descending, periodicity destruction or rapidly rising, Exploring the differences can solve the development crisis. the most important things for a country are interests, weapons and creature, corresponding to money, technology and people. The ship industry has two attribute of financial benefits and technological weapons. Commercial ships can transport massive commodity and warships carry updating of massive technological weapons; But a new core: equity incentives have emerged, and it has helped the rapid development of the computer industry. This article uses comparative analysis and comparative historical analysis to observe the changes in the United States and China after the mutual circulation of two elements and the double circulation of three elements in history, such as the growth rates of GDP and patent applications. Then, it summarizes the changes brought by the core of civilization to the country.Through this article, it can be concluded that the core of civilization consists of ships, equity incentives record-wisdom method; Through the circulation of new elements, a country can transform into civilizations with three cycles, achieving mutual circulation among the three and enhancing endogenous power; The core of civilization can enhance the stability of economic development, prevent economic crises, and achieve a more balanced civilization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11317v2</guid>
      <category>cs.CE</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongfa Zi, Zhen Liu</dc:creator>
    </item>
  </channel>
</rss>
