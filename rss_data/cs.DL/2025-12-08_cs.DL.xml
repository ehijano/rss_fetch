<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Dec 2025 05:01:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Analysis of Inter-Testamental References Reveal Five Groups of Books in the Christian Bible</title>
      <link>https://arxiv.org/abs/2512.05135</link>
      <description>arXiv:2512.05135v1 Announce Type: new 
Abstract: The Bible is packed with references from start to finish. This study aims to analyze a specific branch of these references: citations. While there are several types of references, both explicit and implicit, this study focuses on the types of references that can be detected with a simple algorithmic string comparison, or an n-gram string comparison. Words were compared by their Strong's Concordance numbers so they could be compared without conjugation or declension. We searched through the Greek Old Testament (Septuagint) and Greek New Testament manuscripts for direct quotations from the former in the latter. Our analysis of these references leads us to believe Old Testament books cluster into three groups of common use, and that New Testament books cluster into two books of common use. We analyze these clusters to show explicitly how they differ, and discover that New Testament books reference vastly different portions of the Old Testament.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05135v1</guid>
      <category>cs.DL</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaac Anderson, Wesley Stevick, Katrina Koehler</dc:creator>
    </item>
    <item>
      <title>Can ChatGPT evaluate research environments? Evidence from REF2021</title>
      <link>https://arxiv.org/abs/2512.05202</link>
      <description>arXiv:2512.05202v1 Announce Type: new 
Abstract: UK academic departments are evaluated partly on the statements that they write about the value of their research environments for the Research Excellence Framework (REF) periodic assessments. These statements mix qualitative narratives and quantitative data, typically requiring time-consuming and difficult expert judgements to assess. This article investigates whether Large Language Models (LLMs) can support the process or validate the results, using the UK REF2021 unit-level environment statements as a test case. Based on prompts mimicking the REF guidelines, ChatGPT 4o-mini scores correlated positively with expert scores in almost all 34 (field-based) Units of Assessment (UoAs). ChatGPT's scores had moderate to strong positive Spearman correlations with REF expert scores in 32 out of 34 UoAs: 14 UoAs above 0.7 and a further 13 between 0.6 and 0.7. Only two UoAs had weak or no significant associations (Classics and Clinical Medicine). From further tests for UoA34, multiple LLMs had significant positive correlations with REF2021 environment scores (all p &lt; .001), with ChatGPT 5 performing best (r=0.81; $\rho$=0.82), followed by ChatGPT-4o-mini (r=0.68; $\rho$=0.67) and Gemini Flash 2.5 (r=0.67; $\rho$=0.69). If LLM-generated scores for environment statements are used in future to help reduce workload, support more consistent interpretation, and complement human review then caution must be exercised because of the potential for biases, inaccuracy in some cases, and unwanted systemic effects. Even the strong correlations found here seem unlikely to be judged close enough to expert scores to fully delegate the assessment task to LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05202v1</guid>
      <category>cs.DL</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kayvan Kousha, Mike Thelwall, Elizabeth Gadd</dc:creator>
    </item>
    <item>
      <title>Big Tech-Funded AI Papers Have Higher Citation Impact, Greater Insularity, and Larger Recency Bias</title>
      <link>https://arxiv.org/abs/2512.05714</link>
      <description>arXiv:2512.05714v1 Announce Type: new 
Abstract: Over the past four decades, artificial intelligence (AI) research has flourished at the nexus of academia and industry. However, Big Tech companies have increasingly acquired the edge in computational resources, big data, and talent. So far, it has been largely unclear how many papers the industry funds, how their citation impact compares to non-funded papers, and what drives industry interest. This study fills that gap by quantifying the number of industry-funded papers at 10 top AI conferences (e.g., ICLR, CVPR, AAAI, ACL) and their citation influence. We analyze about 49.8K papers, about 1.8M citations from AI papers to other papers, and about 2.3M citations from other papers to AI papers from 1998-2022 in Scopus. Through seven research questions, we examine the volume and evolution of industry funding in AI research, the citation impact of funded papers, the diversity and temporal range of their citations, and the subfields in which industry predominantly acts. Our findings reveal that industry presence has grown markedly since 2015, from less than 2 percent to more than 11 percent in 2020. Between 2018 and 2022, 12 percent of industry-funded papers achieved high citation rates as measured by the h5-index, compared to 4 percent of non-industry-funded papers and 2 percent of non-funded papers. Top AI conferences engage more with industry-funded research than non-funded research, as measured by our newly proposed metric, the Citation Preference Ratio (CPR). We show that industry-funded research is increasingly insular, citing predominantly other industry-funded papers while referencing fewer non-funded papers. These findings reveal new trends in AI research funding, including a shift towards more industry-funded papers and their growing citation impact, greater insularity of industry-funded work than non-funded work, and a preference of industry-funded research to cite recent work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05714v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 08 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Max Martin Gnewuch, Jan Philip Wahle, Terry Ruas, Bela Gipp</dc:creator>
    </item>
  </channel>
</rss>
