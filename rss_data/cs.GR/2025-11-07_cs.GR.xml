<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GR</link>
    <description>cs.GR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Nov 2025 05:00:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Shellular Metamaterial Design via Compact Electric Potential Parametrization</title>
      <link>https://arxiv.org/abs/2511.04025</link>
      <description>arXiv:2511.04025v1 Announce Type: new 
Abstract: We introduce a compact yet highly expressive design space for shellular metamaterials. By employing only a few dozen degrees of freedom, this design space represents geometries ranging from simple planar configurations to complex triply periodic minimal surfaces. Coupled with this representation, we develop an efficient GPU-based homogenization pipeline that evaluates the structure in under 20 ms and computes the corresponding effective elastic tensor in near-real-time (0.5 s). The high speed of this evaluation facilitates an exhaustive exploration of the design space and supports an inverse-design scheme that tailors the shellular structure to specific macroscopic target property. Structures derived through this approach exhibit not only geometric diversity but also a wide spectrum of mechanical responses, covering a broad range of material properties. Moreover, they achieve up to 91.86% of theoretical upper bounds, a level of performance comparable to state-of-the-art shellular structures with low solid volume. Finally, our prototypes, fabricated via additive manufacturing, confirm the practical manufacturability of these designs, underscoring their potential for real-world engineering applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04025v1</guid>
      <category>cs.GR</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chang Liu, Bohan Wang</dc:creator>
    </item>
    <item>
      <title>Near-Lossless 3D Voxel Representation Free from Iso-surface</title>
      <link>https://arxiv.org/abs/2511.04029</link>
      <description>arXiv:2511.04029v1 Announce Type: cross 
Abstract: Accurate and efficient voxelized representations of 3D meshes are the foundation of 3D reconstruction and generation. However, existing representations based on iso-surface heavily rely on water-tightening or rendering optimization, which inevitably compromise geometric fidelity. We propose Faithful Contouring, a sparse voxelized representation that supports 2048+ resolutions for arbitrary meshes, requiring neither converting meshes to field functions nor extracting the isosurface during remeshing. It achieves near-lossless fidelity by preserving sharpness and internal structures, even for challenging cases with complex geometry and topology. The proposed method also shows flexibility for texturing, manipulation, and editing. Beyond representation, we design a dual-mode autoencoder for Faithful Contouring, enabling scalable and detail-preserving shape reconstruction. Extensive experiments show that Faithful Contouring surpasses existing methods in accuracy and efficiency for both representation and reconstruction. For direct representation, it achieves distance errors at the $10^{-5}$ level; for mesh reconstruction, it yields a 93\% reduction in Chamfer Distance and a 35\% improvement in F-score over strong baselines, confirming superior fidelity as a representation for 3D learning tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04029v1</guid>
      <category>cs.CV</category>
      <category>cs.GR</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yihao Luo, Xianglong He, Chuanyu Pan, Yiwen Chen, Jiaqi Wu, Yangguang Li, Wanli Ouyang, Yuanming Hu, Guang Yang, ChoonHwai Yap</dc:creator>
    </item>
    <item>
      <title>A LoD of Gaussians: Unified Training and Rendering for Ultra-Large Scale Reconstruction with External Memory</title>
      <link>https://arxiv.org/abs/2507.01110</link>
      <description>arXiv:2507.01110v3 Announce Type: replace 
Abstract: Gaussian Splatting has emerged as a high-performance technique for novel view synthesis, enabling real-time rendering and high-quality reconstruction of small scenes. However, scaling to larger environments has so far relied on partitioning the scene into chunks -- a strategy that introduces artifacts at chunk boundaries, complicates training across varying scales, and is poorly suited to unstructured scenarios such as city-scale flyovers combined with street-level views. Moreover, rendering remains fundamentally limited by GPU memory, as all visible chunks must reside in VRAM simultaneously. We introduce A LoD of Gaussians, a framework for training and rendering ultra-large-scale Gaussian scenes on a single consumer-grade GPU -- without partitioning. Our method stores the full scene out-of-core (e.g., in CPU memory) and trains a Level-of-Detail (LoD) representation directly, dynamically streaming only the relevant Gaussians. A hybrid data structure combining Gaussian hierarchies with Sequential Point Trees enables efficient, view-dependent LoD selection, while a lightweight caching and view scheduling system exploits temporal coherence to support real-time streaming and rendering. Together, these innovations enable seamless multi-scale reconstruction and interactive visualization of complex scenes -- from broad aerial views to fine-grained ground-level details.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.01110v3</guid>
      <category>cs.GR</category>
      <category>cs.LG</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Felix Windisch, Thomas K\"ohler, Lukas Radl, Michael Steiner, Dieter Schmalstieg, Markus Steinberger</dc:creator>
    </item>
  </channel>
</rss>
