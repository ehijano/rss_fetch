<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Dec 2024 05:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Efficient Circuit Cutting and Scheduling in a Multi-Node Quantum System with Dynamic EPR Pairs</title>
      <link>https://arxiv.org/abs/2412.18709</link>
      <description>arXiv:2412.18709v1 Announce Type: new 
Abstract: Despite advancements, current quantum hardware faces significant challenges, including limited qubit counts and high susceptibility to noise, which hinder the execution of large, complex algorithms. To address these limitations, multi-node quantum systems and quantum circuit cutting techniques partition large circuits into smaller subcircuits that can be executed on individual quantum machines and then reconstructed using classical resources. However, these methods introduce new challenges, such as the large overhead from subcircuit reconstruction and additional noise from entangled EPR pairs, especially in multi-node quantum systems. In this paper, we propose the Efficient Circuit Cutting and Scheduling (EC2S) system, which integrates EPR pairs with circuit cutting to address these issues. EC2S improves system performance by transitioning from logical to physical EPR pairs and further reduces computational overhead by minimizing the number of subcircuits during the reconstruction phase. \sol~ is implemented using Qiskit and evaluated on both real quantum hardware and various emulators. Compared to the state-of-the-art Qiskit-Addon-Cut, EC2S achieves significant improvements in fidelity, up to 16.7\%, and reduces system-wide expenditure by up to 99.5\%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18709v1</guid>
      <category>cs.DC</category>
      <category>quant-ph</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zefan Du, Wenrui Zhang, Wenqi Wei, Juntao Chen, Tao Han, Zhiding Liang, Ying Mao</dc:creator>
    </item>
    <item>
      <title>Adrenaline: Adaptive Rendering Optimization System for Scalable Cloud Gaming</title>
      <link>https://arxiv.org/abs/2412.19446</link>
      <description>arXiv:2412.19446v1 Announce Type: new 
Abstract: Cloud gaming requires a low-latency network connection, making it a prime candidate for being hosted at the network edge. However, an edge server is provisioned with a fixed compute capacity, causing an issue for multi-user service and resulting in users having to wait before they can play when the server is occupied. In this work, we present a new insight that when a user's network condition results in use of lossy compression, the end-to-end visual quality more degrades for frames of high rendering quality, wasting the server's computing resources. We leverage this observation to build Adrenaline, a new system which adaptively optimizes the game rendering qualities by considering the user-side visual quality and server-side rendering cost. The rendering quality optimization of Adrenaline is done via a scoring mechanism quantifying the effectiveness of server resource usage on the user-side gaming quality. Our open-sourced implementation of Adrenaline demonstrates easy integration with modern game engines. In our evaluations, Adrenaline achieves up to 24% higher service quality and 2x more users served with the same resource footprint compared to other baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19446v1</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.GR</category>
      <category>cs.MM</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Heo, Ketan Bhardwaj, Ada Gavrilovska</dc:creator>
    </item>
    <item>
      <title>Distributed Download from an External Data Source in Faulty Majority Settings</title>
      <link>https://arxiv.org/abs/2412.19649</link>
      <description>arXiv:2412.19649v1 Announce Type: new 
Abstract: We extend the study of retrieval problems in distributed networks, focusing on improving the efficiency and resilience of protocols in the \emph{Data Retrieval (DR) Model}. The DR Model consists of a complete network (i.e., a clique) with $k$ peers, up to $\beta k$ of which may be Byzantine (for $\beta \in [0, 1)$), and a trusted \emph{External Data Source} comprising an array $X$ of $n$ bits ($n \gg k$) that the peers can query. Additionally, the peers can also send messages to each other. In this work, we focus on the Download problem that requires all peers to learn $X$. Our primary goal is to minimize the maximum number of queries made by any honest peer and additionally optimize time.
  We begin with a randomized algorithm for the Download problem that achieves optimal query complexity up to a logarithmic factor. For the stronger dynamic adversary that can change the set of Byzantine peers from one round to the next, we achieve the optimal time complexity in peer-to-peer communication but with larger messages. In broadcast communication where all peers (including Byzantine peers) are required to send the same message to all peers, with larger messages, we achieve almost optimal time and query complexities for a dynamic adversary. Finally, in a more relaxed crash fault model, where peers stop responding after crashing, we address the Download problem in both synchronous and asynchronous settings. Using a deterministic protocol, we obtain nearly optimal results for both query complexity and message sizes in these scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19649v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>John Augustine, Soumyottam Chatterjee, Valerie King, Manish Kumar, Shachar Meir, David Peleg</dc:creator>
    </item>
    <item>
      <title>Geometric Freeze-Tag Problem</title>
      <link>https://arxiv.org/abs/2412.19706</link>
      <description>arXiv:2412.19706v1 Announce Type: new 
Abstract: We study the Freeze-Tag Problem (FTP), introduced by Arkin et al. (SODA'02), where the objective is to activate a group of n robots, starting from a single initially active robot. Robots are positioned in $\mathbb{R}^d$, and once activated, they move at a constant speed to wake up others. The goal is to minimize the time required to activate the last robot, known as the makespan. We establish new upper bounds for the makespan under the $l_1$ and $l_2$ norms in $\mathbb{R}^2$ and $\mathbb{R}^3$. Specifically, we improve the previous upper bound for $(\mathbb{R}^2, l_2)$ from $7.07r$ (Bonichon et al., DISC'24) to $5.064r$. For $(\mathbb{R}^3, l_1)$, we derive a makespan bound of $13r$, which translates to $22.52r$ for $(\mathbb{R}^3, l_2)$. Here, $r$ denotes the maximum distance of any robot from the initially active robot under the given norm. To our knowledge, these are the first makespan bounds for FTP in $\mathbb{R}^3$. Additionally, we show that the maximum makespan for $n$ robots is not necessarily achieved when robots are equally distributed along the boundary in $(\mathbb{R}^2, l_2)$. We further investigate FTP in $(\mathbb{R}^3, l_2)$ for specific configurations where robots lie on a boundary, providing insights into practical scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19706v1</guid>
      <category>cs.DC</category>
      <category>cs.RO</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sharareh Alipour, Kajal Baghestani, Mahdis Mirzaei, Soroush Sahraei</dc:creator>
    </item>
    <item>
      <title>TimelyLLM: Segmented LLM Serving System for Time-sensitive Robotic Applications</title>
      <link>https://arxiv.org/abs/2412.18695</link>
      <description>arXiv:2412.18695v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) such as GPT-4 and Llama3 can already comprehend complex commands and process diverse tasks. This advancement facilitates their application in controlling drones and robots for various tasks. However, existing LLM serving systems typically employ a first-come, first-served (FCFS) batching mechanism, which fails to address the time-sensitive requirements of robotic applications. To address it, this paper proposes a new system named TimelyLLM serving multiple robotic agents with time-sensitive requests. TimelyLLM introduces novel mechanisms of segmented generation and scheduling that optimally leverage redundancy between robot plan generation and execution phases. We report an implementation of TimelyLLM on a widely-used LLM serving framework and evaluate it on a range of robotic applications. Our evaluation shows that TimelyLLM improves the time utility up to 1.97x, and reduces the overall waiting time by 84%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18695v1</guid>
      <category>cs.RO</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neiwen Ling, Guojun Chen, Lin Zhong</dc:creator>
    </item>
    <item>
      <title>Circuit Folding: Modular and Qubit-Level Workload Management in Quantum-Classical Systems</title>
      <link>https://arxiv.org/abs/2412.18705</link>
      <description>arXiv:2412.18705v1 Announce Type: cross 
Abstract: Quantum computing is an emerging technology that offers exponential speedups for certain problems. At the core of quantum-centric supercomputing is advanced middleware that manages the interaction between quantum hardware and classical computing infrastructure. Circuit knitting is a technique that leverages classical computation to offload some of the computational burden from quantum circuits, enabling them to exceed the capacity of current Noisy Intermediate-Scale Quantum (NISQ) devices. This is done by partitioning large circuits into smaller subcircuits, though at the cost of classical reconstruction and increased sampling overhead. Despite significant advancements in reducing the theoretical costs of circuit knitting, efficiently deploying these techniques across a broad range of quantum algorithms remains a challenge. In this work, we propose CiFold, a novel graph-based system that, at the individual qubit's level, identifies and leverages repeated structures within quantum circuits. By folding these repeated modules in parallel, CiFold constructs a meta-graph that guides the partitioning process, optimizing the cutting strategy through the integration of advanced circuit knitting techniques. Our system has been extensively evaluated across various quantum algorithms, achieving up to 799.2\% reduction in quantum resource usage, demonstrating its scalability and substantial performance improvements over state-of-the-art approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18705v1</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuwen Kan, Yanni Li, Hao Wang, Sara Mouradian, Ying Mao</dc:creator>
    </item>
    <item>
      <title>Virtual Nodes Can Help: Tackling Distribution Shifts in Federated Graph Learning</title>
      <link>https://arxiv.org/abs/2412.19229</link>
      <description>arXiv:2412.19229v1 Announce Type: cross 
Abstract: Federated Graph Learning (FGL) enables multiple clients to jointly train powerful graph learning models, e.g., Graph Neural Networks (GNNs), without sharing their local graph data for graph-related downstream tasks, such as graph property prediction. In the real world, however, the graph data can suffer from significant distribution shifts across clients as the clients may collect their graph data for different purposes. In particular, graph properties are usually associated with invariant label-relevant substructures (i.e., subgraphs) across clients, while label-irrelevant substructures can appear in a client-specific manner. The issue of distribution shifts of graph data hinders the efficiency of GNN training and leads to serious performance degradation in FGL. To tackle the aforementioned issue, we propose a novel FGL framework entitled FedVN that eliminates distribution shifts through client-specific graph augmentation strategies with multiple learnable Virtual Nodes (VNs). Specifically, FedVN lets the clients jointly learn a set of shared VNs while training a global GNN model. To eliminate distribution shifts, each client trains a personalized edge generator that determines how the VNs connect local graphs in a client-specific manner. Furthermore, we provide theoretical analyses indicating that FedVN can eliminate distribution shifts of graph data across clients. Comprehensive experiments on four datasets under five settings demonstrate the superiority of our proposed FedVN over nine baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19229v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingbo Fu, Zihan Chen, Yinhan He, Song Wang, Binchi Zhang, Chen Chen, Jundong Li</dc:creator>
    </item>
    <item>
      <title>Memory-Centric Computing: Recent Advances in Processing-in-DRAM</title>
      <link>https://arxiv.org/abs/2412.19275</link>
      <description>arXiv:2412.19275v1 Announce Type: cross 
Abstract: Memory-centric computing aims to enable computation capability in and near all places where data is generated and stored. As such, it can greatly reduce the large negative performance and energy impact of data access and data movement, by 1) fundamentally avoiding data movement, 2) reducing data access latency &amp; energy, and 3) exploiting large parallelism of memory arrays. Many recent studies show that memory-centric computing can largely improve system performance &amp; energy efficiency. Major industrial vendors and startup companies have recently introduced memory chips with sophisticated computation capabilities. Going forward, both hardware and software stack should be revisited and designed carefully to take advantage of memory-centric computing.
  This work describes several major recent advances in memory-centric computing, specifically in Processing-in-DRAM, a paradigm where the operational characteristics of a DRAM chip are exploited and enhanced to perform computation on data stored in DRAM. Specifically, we describe 1) new techniques that slightly modify DRAM chips to enable both enhanced computation capability and easier programmability, 2) new experimental studies that demonstrate the functionally-complete bulk-bitwise computational capability of real commercial off-the-shelf DRAM chips, without any modifications to the DRAM chip or the interface, and 3) new DRAM designs that improve access granularity &amp; efficiency, unleashing the true potential of Processing-in-DRAM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19275v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Onur Mutlu, Ataberk Olgun, Geraldo F. Oliveira, Ismail Emir Yuksel</dc:creator>
    </item>
    <item>
      <title>A semi-algebraic model for automatic loop parallelization</title>
      <link>https://arxiv.org/abs/2412.19287</link>
      <description>arXiv:2412.19287v1 Announce Type: cross 
Abstract: In this work, we introduce a semi-algebraic model for automatic parallelization of perfectly nested polynomial loops, which generalizes the classical polyhedral model. This model supports the basic tasks for automatic loop parallelization, such as the representation of the nested loop, the dependence analysis, the computation of valid schedules, as well as the transformation of the loop program with a valid schedule.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19287v1</guid>
      <category>cs.SC</category>
      <category>cs.DC</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Changbo Chen</dc:creator>
    </item>
    <item>
      <title>A Survey on Large Language Model Acceleration based on KV Cache Management</title>
      <link>https://arxiv.org/abs/2412.19442</link>
      <description>arXiv:2412.19442v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have revolutionized a wide range of domains such as natural language processing, computer vision, and multi-modal tasks due to their ability to comprehend context and perform logical reasoning. However, the computational and memory demands of LLMs, particularly during inference, pose significant challenges when scaling them to real-world, long-context, and real-time applications. Key-Value (KV) cache management has emerged as a critical optimization technique for accelerating LLM inference by reducing redundant computations and improving memory utilization. This survey provides a comprehensive overview of KV cache management strategies for LLM acceleration, categorizing them into token-level, model-level, and system-level optimizations. Token-level strategies include KV cache selection, budget allocation, merging, quantization, and low-rank decomposition, while model-level optimizations focus on architectural innovations and attention mechanisms to enhance KV reuse. System-level approaches address memory management, scheduling, and hardware-aware designs to improve efficiency across diverse computing environments. Additionally, the survey provides an overview of both text and multimodal datasets and benchmarks used to evaluate these strategies. By presenting detailed taxonomies and comparative analyses, this work aims to offer useful insights for researchers and practitioners to support the development of efficient and scalable KV cache management techniques, contributing to the practical deployment of LLMs in real-world applications. The curated paper list for KV cache management is in: \href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19442v1</guid>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyang Li, Yiming Li, Anxin Tian, Tianhao Tang, Zhanchao Xu, Xuejia Chen, Nicole Hu, Wei Dong, Qing Li, Lei Chen</dc:creator>
    </item>
    <item>
      <title>Asymmetrical Reciprocity-based Federated Learning for Resolving Disparities in Medical Diagnosis</title>
      <link>https://arxiv.org/abs/2412.19654</link>
      <description>arXiv:2412.19654v1 Announce Type: cross 
Abstract: Geographic health disparities pose a pressing global challenge, particularly in underserved regions of low- and middle-income nations. Addressing this issue requires a collaborative approach to enhance healthcare quality, leveraging support from medically more developed areas. Federated learning emerges as a promising tool for this purpose. However, the scarcity of medical data and limited computation resources in underserved regions make collaborative training of powerful machine learning models challenging. Furthermore, there exists an asymmetrical reciprocity between underserved and developed regions. To overcome these challenges, we propose a novel cross-silo federated learning framework, named FedHelp, aimed at alleviating geographic health disparities and fortifying the diagnostic capabilities of underserved regions. Specifically, FedHelp leverages foundational model knowledge via one-time API access to guide the learning process of underserved small clients, addressing the challenge of insufficient data. Additionally, we introduce a novel asymmetric dual knowledge distillation module to manage the issue of asymmetric reciprocity, facilitating the exchange of necessary knowledge between developed large clients and underserved small clients. We validate the effectiveness and utility of FedHelp through extensive experiments on both medical image classification and segmentation tasks. The experimental results demonstrate significant performance improvement compared to state-of-the-art baselines, particularly benefiting clients in underserved regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19654v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaqi Wang, Ziyi Yin, Quanzeng You, Lingjuan Lyu, Fenglong Ma</dc:creator>
    </item>
    <item>
      <title>Mobile Traffic Prediction at the Edge Through Distributed and Deep Transfer Learning</title>
      <link>https://arxiv.org/abs/2310.14456</link>
      <description>arXiv:2310.14456v2 Announce Type: replace 
Abstract: Traffic prediction represents one of the crucial tasks for smartly optimizing the mobile network. Recently, Artificial Intelligence (AI) has attracted attention to solve this problem thanks to its ability in cognizing the state of the mobile network and make intelligent decisions. Research on this topic has concentrated on making predictions in a centralized fashion, i.e., by collecting data from the different network elements and process them in a cloud center. This translates into inefficiencies due to the large amount of data transmissions and computations required, leading to high energy consumption. In this work, we investigate a fully decentralized AI solution for mobile traffic prediction that allows data to be kept locally, reducing energy consumption through collaboration among the base station sites. To do so, we propose a novel prediction framework based on edge computing and Deep Transfer Learning (DTL) techniques, using datasets obtained at the edge through a large measurement campaign. Two main Deep Learning architectures are designed based on Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) and tested under different training conditions. Simulation results show that the CNN architectures outperform the RNNs in accuracy and consume less energy. In both scenarios, DTL contributes to an accuracy enhancement in 85% of the examined cases compared to their stand-alone counterparts. Additionally, DTL significantly reduces computational complexity and energy consumption during training, resulting in a reduction of the energy footprint by 60% for CNNs and 90% for RNNs. Finally, two cutting-edge eXplainable Artificial Intelligence techniques are employed to interpret the derived learning models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14456v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2024.3518483</arxiv:DOI>
      <arxiv:journal_reference>IEEE Access, volume 12 (2024) 191288-191303</arxiv:journal_reference>
      <dc:creator>Alfredo Petrella, Marco Miozzo, Paolo Dini</dc:creator>
    </item>
    <item>
      <title>A Starting Point for Dynamic Community Detection with Leiden Algorithm</title>
      <link>https://arxiv.org/abs/2405.11658</link>
      <description>arXiv:2405.11658v4 Announce Type: replace 
Abstract: Real-world graphs often evolve over time, making community or cluster detection a crucial task. In this technical report, we extend three dynamic approaches - Naive-dynamic (ND), Delta-screening (DS), and Dynamic Frontier (DF) - to our multicore implementation of the Leiden algorithm, known for its high-quality community detection. Our experiments, conducted on a server with a 64-core AMD EPYC-7742 processor, show that ND, DS, and DF Leiden achieve average speedups of 1.37x, 1.47x, and 1.98x on large graphs with random batch updates, compared to the Static Leiden algorithm - while scaling at a rate of 1.6x for every doubling of threads. To our knowledge, this is the first attempt to apply dynamic approaches to the Leiden algorithm. We hope these early results pave the way for further development of dynamic approaches for evolving graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11658v4</guid>
      <category>cs.DC</category>
      <category>cs.SI</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Subhajit Sahu</dc:creator>
    </item>
    <item>
      <title>LLOR: Automated Repair of OpenMP Programs</title>
      <link>https://arxiv.org/abs/2411.14590</link>
      <description>arXiv:2411.14590v2 Announce Type: replace 
Abstract: In this paper, we present a technique for repairing data race errors in parallel programs written in C/C++ and Fortran using the OpenMP API. Our technique can also remove barriers that are deemed unnecessary for correctness. We implement these ideas in our tool called LLOR, which takes a language-independent approach to provide appropriate placements of synchronization constructs to avoid data races. To the best of our knowledge, LLOR is the only tool that can repair parallel programs that use the OpenMP API. We showcase the capabilities of LLOR by performing extensive experiments on 415 parallel programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14590v2</guid>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Utpal Bora, Saurabh Joshi, Gautam Muduganti, Ramakrishna Upadrasta</dc:creator>
    </item>
    <item>
      <title>Taming the Memory Beast: Strategies for Reliable ML Training on Kubernetes</title>
      <link>https://arxiv.org/abs/2412.14701</link>
      <description>arXiv:2412.14701v2 Announce Type: replace 
Abstract: Kubernetes offers a powerful orchestration platform for machine learning training, but memory management can be challenging due to specialized needs and resource constraints. This paper outlines how Kubernetes handles memory requests, limits, Quality of Service classes, and eviction policies for ML workloads, with special focus on GPU memory and ephemeral storage. Common pitfalls such as overcommitment, memory leaks, and ephemeral volume exhaustion are examined. We then provide best practices for stable, scalable memory utilization to help ML practitioners prevent out-of-memory events and ensure high-performance ML training pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14701v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaideep Ray</dc:creator>
    </item>
    <item>
      <title>KunServe: Elastic and Efficient Large Language Model Serving with Parameter-centric Memory Management</title>
      <link>https://arxiv.org/abs/2412.18169</link>
      <description>arXiv:2412.18169v2 Announce Type: replace 
Abstract: The stateful nature of large language model (LLM) servingcan easily throttle precious GPU memory under load burstor long-generation requests like chain-of-thought reasoning,causing latency spikes due to queuing incoming requests. However, state-of-the-art KVCache centric approaches handleload spikes by dropping, migrating, or swapping KVCache,which faces an essential tradeoff between the performance ofongoing vs. incoming requests and thus still severely violatesSLO.This paper makes a key observation such that model param-eters are independent of the requests and are replicated acrossGPUs, and thus proposes a parameter-centric approach byselectively dropping replicated parameters to leave preciousmemory for requests. However, LLM requires KVCache tobe saved in bound with model parameters and thus droppingparameters can cause either huge computation waste or longnetwork delay, affecting all ongoing requests. Based on the ob-servation that attention operators can be decoupled from otheroperators, this paper further proposes a novel remote attentionmechanism through pipeline parallelism so as to serve up-coming requests with the additional memory borrowed fromparameters on remote GPUs. This paper further addresses sev-eral other challenges including lively exchanging KVCachewith incomplete parameters, generating an appropriate planthat balances memory requirements with cooperative exe-cution overhead, and seamlessly restoring parameters whenthe throttling has gone. Evaluations show thatKUNSERVEreduces the tail TTFT of requests under throttling by up to 27.3x compared to the state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18169v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rongxin Cheng, Yifan Peng, Yuxin Lai, Xingda Wei, Rong Chen, Haibo Chen</dc:creator>
    </item>
    <item>
      <title>FedMoE-DA: Federated Mixture of Experts via Domain Aware Fine-grained Aggregation</title>
      <link>https://arxiv.org/abs/2411.02115</link>
      <description>arXiv:2411.02115v2 Announce Type: replace-cross 
Abstract: Federated learning (FL) is a collaborative machine learning approach that enables multiple clients to train models without sharing their private data. With the rise of deep learning, large-scale models have garnered significant attention due to their exceptional performance. However, a key challenge in FL is the limitation imposed by clients with constrained computational and communication resources, which hampers the deployment of these large models. The Mixture of Experts (MoE) architecture addresses this challenge with its sparse activation property, which reduces computational workload and communication demands during inference and updates. Additionally, MoE facilitates better personalization by allowing each expert to specialize in different subsets of the data distribution. To alleviate the communication burdens between the server and clients, we propose FedMoE-DA, a new FL model training framework that leverages the MoE architecture and incorporates a novel domain-aware, fine-grained aggregation strategy to enhance the robustness, personalizability, and communication efficiency simultaneously. Specifically, the correlation between both intra-client expert models and inter-client data heterogeneity is exploited. Moreover, we utilize peer-to-peer (P2P) communication between clients for selective expert model synchronization, thus significantly reducing the server-client transmissions. Experiments demonstrate that our FedMoE-DA achieves excellent performance while reducing the communication pressure on the server.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02115v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziwei Zhan, Wenkuan Zhao, Yuanqing Li, Weijie Liu, Xiaoxi Zhang, Chee Wei Tan, Chuan Wu, Deke Guo, Xu Chen</dc:creator>
    </item>
    <item>
      <title>Enhanced Quantum Circuit Cutting Framework for Sampling Overhead Reduction</title>
      <link>https://arxiv.org/abs/2412.17704</link>
      <description>arXiv:2412.17704v2 Announce Type: replace-cross 
Abstract: The recent quantum circuit cutting technique enables simulating large quantum circuits on distributed smaller devices, significantly extending the capabilities of current noisy intermediate-scale quantum (NISQ) hardware. However, this method incurs substantial classical postprocessing and additional quantum resource demands, as both postprocessing complexity and sampling overhead scale exponentially with the number of cuts introduced. In this work, we propose an enhanced circuit cutting framework ShotQC with effective sampling overhead reduction. It effectively reduces sampling overhead through two key optimizations: shot distribution and cut parameterization. The former employs an adaptive Monte Carlo method to dynamically allocate more quantum resources to subcircuit configurations that contribute more to variance in the final outcome. The latter leverages additional degrees of freedom in postprocessing to further suppress variance. By integrating these optimization methods, ShotQC achieves significant reductions in sampling overhead without increasing classical postprocessing complexity, as demonstrated on a range of benchmark circuits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17704v2</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Po-Hung Chen, Dah-Wei Chiou, Jie-Hong Roland Jiang</dc:creator>
    </item>
    <item>
      <title>Pilot-Quantum: A Quantum-HPC Middleware for Resource, Workload and Task Management</title>
      <link>https://arxiv.org/abs/2412.18519</link>
      <description>arXiv:2412.18519v2 Announce Type: replace-cross 
Abstract: As quantum hardware continues to scale, managing the heterogeneity of resources and applications -- spanning diverse quantum and classical hardware and software frameworks -- becomes increasingly critical. Pilot-Quantum addresses these challenges as a middleware designed to provide unified application-level management of resources and workloads across hybrid quantum-classical environments. It is built on a rigorous analysis of existing quantum middleware systems and application execution patterns. It implements the Pilot Abstraction conceptual model, originally developed for HPC, to manage resources, workloads, and tasks. It is designed for quantum applications that rely on task parallelism, including: (i) Hybrid algorithms, such as variational approaches, and (ii) Circuit cutting systems, used to partition and execute large quantum circuits. Pilot-Quantum facilitates seamless integration of quantum processing units (QPUs), classical CPUs, and GPUs, while supporting high-level programming frameworks like Qiskit and Pennylane. This enables users to design and execute hybrid workflows across diverse computing resources efficiently. The capabilities of Pilot-Quantum are demonstrated through mini-applications -- simplified yet representative kernels focusing on critical performance bottlenecks. We present several mini-apps, including circuit execution across hardware and simulator platforms (e.g., IBM's Eagle QPU), distributed state vector simulation, circuit cutting, and quantum machine learning workflows, demonstrating significant scale (e.g., a 41-qubit simulation on 256 GPUs) and speedups (e.g., 15x for QML, 3.5x for circuit cutting).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18519v2</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pradeep Mantha, Florian J. Kiwit, Nishant Saurabh, Shantenu Jha, Andre Luckow</dc:creator>
    </item>
  </channel>
</rss>
