<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Jul 2025 04:03:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>PanoGAN A Deep Generative Model for Panoramic Dental Radiographs</title>
      <link>https://arxiv.org/abs/2507.21200</link>
      <description>arXiv:2507.21200v1 Announce Type: cross 
Abstract: This paper presents the development of a generative adversarial network (GAN) for synthesizing dental panoramic radiographs. Although exploratory in nature, the study aims to address the scarcity of data in dental research and education. We trained a deep convolutional GAN (DCGAN) using a Wasserstein loss with gradient penalty (WGANGP) on a dataset of 2322 radiographs of varying quality. The focus was on the dentoalveolar regions, other anatomical structures were cropped out. Extensive preprocessing and data cleaning were performed to standardize the inputs while preserving anatomical variability. We explored four candidate models by varying critic iterations, feature depth, and the use of denoising prior to training. A clinical expert evaluated the generated radiographs based on anatomical visibility and realism, using a 5-point scale (1 very poor 5 excellent). Most images showed moderate anatomical depiction, although some were degraded by artifacts. A trade-off was observed the model trained on non-denoised data yielded finer details especially in structures like the mandibular canal and trabecular bone, while a model trained on denoised data offered superior overall image clarity and sharpness. These findings provide a foundation for future work on GAN-based methods in dental imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21200v1</guid>
      <category>cs.CV</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3390/jimaging11020041</arxiv:DOI>
      <dc:creator>Soren Pedersen, Sanyam Jain, Mikkel Chavez, Viktor Ladehoff, Bruna Neves de Freitas, Ruben Pauwels</dc:creator>
    </item>
    <item>
      <title>Impact of eHMI on Pedestrians' Interactions with Level-5 Automated Driving Systems</title>
      <link>https://arxiv.org/abs/2507.21303</link>
      <description>arXiv:2507.21303v1 Announce Type: cross 
Abstract: Each year, over half of global traffic fatalities involve vulnerable road users (e.g. pedestrians), often due to human error. Level-5 automated driving systems (ADSs) could reduce driver errors contributing to pedestrian accidents, though effectiveness depends on clarity and understandability for other road users. External human-machine interfaces (eHMIs) have been proposed to facilitate pedestrian-ADS communication, though consensus on optimal eHMI features remains unclear. In an online survey, 153 participants responded to road-crossing scenarios involving level-5 ADSs, with and without eHMIs. With eHMIs, pedestrians crossed earlier and more confidently, and reported significantly increased perceptions of safety, trust, and understanding when interacting with level-5 ADSs. Visual eHMI features (including a text display and external speedometer) were ranked more necessary than auditory ones, though auditory cues received positive feedback. This study demonstrates that eHMIs can significantly improve pedestrians' understanding of level-5 ADS intent and enhance perceived safety and trust, facilitating more intuitive pedestrian-ADS interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21303v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Viktoria Marcus, Griffin Pitts, Sanaz Motamedi</dc:creator>
    </item>
    <item>
      <title>LLM4VV: Evaluating Cutting-Edge LLMs for Generation and Evaluation of Directive-Based Parallel Programming Model Compiler Tests</title>
      <link>https://arxiv.org/abs/2507.21447</link>
      <description>arXiv:2507.21447v1 Announce Type: cross 
Abstract: The usage of Large Language Models (LLMs) for software and test development has continued to increase since LLMs were first introduced, but only recently have the expectations of LLMs become more realistic. Verifying the correctness of code generated by LLMs is key to improving their usefulness, but there have been no comprehensive and fully autonomous solutions developed yet. Hallucinations are a major concern when LLMs are applied blindly to problems without taking the time and effort to verify their outputs, and an inability to explain the logical reasoning of LLMs leads to issues with trusting their results. To address these challenges while also aiming to effectively apply LLMs, this paper proposes a dual-LLM system (i.e. a generative LLM and a discriminative LLM) and experiments with the usage of LLMs for the generation of a large volume of compiler tests. We experimented with a number of LLMs possessing varying parameter counts and presented results using ten carefully-chosen metrics that we describe in detail in our narrative. Through our findings, it is evident that LLMs possess the promising potential to generate quality compiler tests and verify them automatically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21447v1</guid>
      <category>cs.SE</category>
      <category>cs.ET</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zachariah Sollenberger, Rahul Patel, Saieda Ali Zada, Sunita Chandrasekaran</dc:creator>
    </item>
    <item>
      <title>Real-Time Audio-Visual Speech Enhancement Using Pre-trained Visual Representations</title>
      <link>https://arxiv.org/abs/2507.21448</link>
      <description>arXiv:2507.21448v1 Announce Type: cross 
Abstract: Speech enhancement in audio-only settings remains challenging, particularly in the presence of interfering speakers. This paper presents a simple yet effective real-time audio-visual speech enhancement (AVSE) system, RAVEN, which isolates and enhances the on-screen target speaker while suppressing interfering speakers and background noise. We investigate how visual embeddings learned from audio-visual speech recognition (AVSR) and active speaker detection (ASD) contribute to AVSE across different SNR conditions and numbers of interfering speakers. Our results show concatenating embeddings from AVSR and ASD models provides the greatest improvement in low-SNR, multi-speaker environments, while AVSR embeddings alone perform best in noise-only scenarios. In addition, we develop a real-time streaming system that operates on a computer CPU and we provide a video demonstration and code repository. To our knowledge, this is the first open-source implementation of a real-time AVSE system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21448v1</guid>
      <category>eess.AS</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> Teng (Aleksandra),  Ma, Sile Yin, Li-Chia Yang, Shuo Zhang</dc:creator>
    </item>
    <item>
      <title>A Grover-Based Quantum Algorithm for Solving Perfect Mazes via Fitness-Guided Search</title>
      <link>https://arxiv.org/abs/2507.21937</link>
      <description>arXiv:2507.21937v1 Announce Type: cross 
Abstract: We present a quantum algorithm for solving perfect mazes by casting the pathfinding task as a structured search problem. Building on Grover's amplitude amplification, the algorithm encodes all candidate paths in superposition and evaluates their proximity to the goal using a reversible fitness operator based on quantum arithmetic. A Grover-compatible oracle marks high-fitness states, and an adaptive cutoff strategy refines the search iteratively. We provide formal definitions, unitary constructions, and convergence guarantees, along with a resource analysis showing efficient scaling with maze size and path length. The framework serves as a foundation for quantum-hybrid pathfinding and planning. The full algorithmic pipeline is specified from encoding to amplification, including oracle design and fitness evaluation. The approach is readily extensible to other search domains, including navigation over tree-like or acyclic graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21937v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <category>math.QA</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michelle L. Wu</dc:creator>
    </item>
    <item>
      <title>A Novel Framework for Near-Field Covert Communications with RIS and RSMA</title>
      <link>https://arxiv.org/abs/2507.21956</link>
      <description>arXiv:2507.21956v1 Announce Type: cross 
Abstract: This paper explores the near field (NF) covert communication with the aid of rate-splitting multiple access (RSMA) and reconfigurable intelligent surfaces (RIS). In particular, the RIS operates in the NF of both the legitimate user and the passive adversary, enhancing the legitimate users received signal while suppressing the adversarys detection capability. Whereas, the base station (BS) applies RSMA to increase the covert communication rate composed of a private and a shared rate component. To characterize system covertness, we derive closed form expressions for the detection error probability (DEP), outage probability (OP), and optimal detection threshold for the adversary. We formulate a non-convex joint beamforming optimization problem at the BS and RIS under unit-modulus constraints to maximize the covert rate. To tackle this, we propose an alternating optimization (AO) algorithm, where the BS beamformer is designed using a two-stage iterative method based on successive convex approximation (SCA). Additionally, two low-complexity techniques are introduced to further reduce the adversarys received power. Simulation results demonstrate that the proposed algorithm effectively improves the covert communication rate, highlighting the potential of near field RSMA-RIS integration in covert communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21956v1</guid>
      <category>eess.SP</category>
      <category>cs.ET</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Atiquzzaman Mondal, Amira Bendaimi, Huseyin Arslan</dc:creator>
    </item>
    <item>
      <title>Higher-Order Kuramoto Oscillator Network for Dense Associative Memory</title>
      <link>https://arxiv.org/abs/2507.21984</link>
      <description>arXiv:2507.21984v1 Announce Type: cross 
Abstract: Networks of phase oscillators can serve as dense associative memories if they incorporate higher-order coupling beyond the classical Kuramoto model's pairwise interactions. Here we introduce a generalized Kuramoto model with combined second-harmonic (pairwise) and fourth-harmonic (quartic) coupling, inspired by dense Hopfield memory theory. Using mean-field theory and its dynamical approximation, we obtain a phase diagram for dense associative memory model that exhibits a tricritical point at which the continuous onset of memory retrieval is supplanted by a discontinuous, hysteretic transition. In the quartic-dominated regime, the system supports bistable phase-locked states corresponding to stored memory patterns, with a sizable energy barrier between memory and incoherent states. We analytically determine this bistable region and show that the escape time from a memory state (due to noise) grows exponentially with network size, indicating robust storage. Extending the theory to finite memory load, we show that higher-order couplings achieve superlinear scaling of memory capacity with system size, far exceeding the limit of pairwise-only oscillators. Large-scale simulations of the oscillator network confirm our theoretical predictions, demonstrating rapid pattern retrieval and robust storage of many phase patterns. These results bridge the Kuramoto synchronization with modern Hopfield memories, pointing toward experimental realization of high-capacity, analog associative memory in oscillator systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21984v1</guid>
      <category>nlin.AO</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jona Nagerl, Natalia G. Berloff</dc:creator>
    </item>
    <item>
      <title>Implementing Large Quantum Boltzmann Machines as Generative AI Models for Dataset Balancing</title>
      <link>https://arxiv.org/abs/2502.03086</link>
      <description>arXiv:2502.03086v2 Announce Type: replace 
Abstract: This study explores the implementation of large Quantum Restricted Boltzmann Machines (QRBMs), a key advancement in Quantum Machine Learning (QML), as generative models on D-Wave's Pegasus quantum hardware to address dataset imbalance in Intrusion Detection Systems (IDS). By leveraging Pegasus's enhanced connectivity and computational capabilities, a QRBM with 120 visible and 120 hidden units was successfully embedded, surpassing the limitations of default embedding tools. The QRBM synthesized over 1.6 million attack samples, achieving a balanced dataset of over 4.2 million records. Comparative evaluations with traditional balancing methods, such as SMOTE and RandomOversampler, revealed that QRBMs produced higher-quality synthetic samples, significantly improving detection rates, precision, recall, and F1 score across diverse classifiers. The study underscores the scalability and efficiency of QRBMs, completing balancing tasks in milliseconds. These findings highlight the transformative potential of QML and QRBMs as next-generation tools in data preprocessing, offering robust solutions for complex computational challenges in modern information systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03086v2</guid>
      <category>cs.ET</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>quant-ph</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/NGISE64126.2025.11085158</arxiv:DOI>
      <dc:creator>Salvatore Sinno, Markus Bertl, Arati Sahoo, Bhavika Bhalgamiya, Thomas Gro{\ss}, Nicholas Chancellor</dc:creator>
    </item>
    <item>
      <title>Receding Hamiltonian-Informed Optimal Neural Control and State Estimation for Closed-Loop Dynamical Systems</title>
      <link>https://arxiv.org/abs/2411.01297</link>
      <description>arXiv:2411.01297v3 Announce Type: replace-cross 
Abstract: This paper formalizes Hamiltonian-Informed Optimal Neural (Hion) controllers, a novel class of neural network-based controllers for dynamical systems and explicit non-linear model-predictive control. Hion controllers estimate future states and develop an optimal control strategy using Pontryagin's Maximum Principle. The proposed framework, along with our Taylored Multi-Faceted Approach for Neural ODE and Optimal Control (T-mano) architecture, allows for custom transient behavior, predictive control, and closed-loop feedback, addressing limitations of existing methods. Comparative analyses with established model-predictive controllers revealed Hion controllers' superior optimality and tracking capabilities. Optimal control strategies are also demonstrated for both linear and non-linear dynamical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01297v3</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Josue N. Rivera, Dengfeng Sun</dc:creator>
    </item>
    <item>
      <title>Quantum Boltzmann Machines using Parallel Annealing for Medical Image Classification</title>
      <link>https://arxiv.org/abs/2507.14116</link>
      <description>arXiv:2507.14116v2 Announce Type: replace-cross 
Abstract: Exploiting the fact that samples drawn from a quantum annealer inherently follow a Boltzmann-like distribution, annealing-based Quantum Boltzmann Machines (QBMs) have gained increasing popularity in the quantum research community. While they harbor great promises for quantum speed-up, their usage currently stays a costly endeavor, as large amounts of QPU time are required to train them. This limits their applicability in the NISQ era. Following the idea of No\`e et al. (2024), who tried to alleviate this cost by incorporating parallel quantum annealing into their unsupervised training of QBMs, this paper presents an improved version of parallel quantum annealing that we employ to train QBMs in a supervised setting. Saving qubits to encode the inputs, the latter setting allows us to test our approach on medical images from the MedMNIST data set (Yang et al., 2023), thereby moving closer to real-world applicability of the technology. Our experiments show that QBMs using our approach already achieve reasonable results, comparable to those of similarly-sized Convolutional Neural Networks (CNNs), with markedly smaller numbers of epochs than these classical models. Our parallel annealing technique leads to a speed-up of almost 70 % compared to regular annealing-based BM executions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14116v2</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dani\"elle Schuman, Mark V. Seebode, Tobias Rohe, Maximilian Balthasar Mansky, Michael Schroedl-Baumann, Jonas Stein, Claudia Linnhoff-Popien, Florian Krellner</dc:creator>
    </item>
  </channel>
</rss>
