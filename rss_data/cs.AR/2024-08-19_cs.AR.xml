<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 20 Aug 2024 02:33:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 19 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Accelerating Mini-batch HGNN Training by Reducing CUDA Kernels</title>
      <link>https://arxiv.org/abs/2408.08490</link>
      <description>arXiv:2408.08490v1 Announce Type: new 
Abstract: Heterogeneous graph neural networks (HGNNs) are essential for capturing the structure and semantic information in heterogeneous graphs. However, existing GPU-based solutions, such as PyTorch Geometric, suffer from low GPU utilization due to numerous short-execution-time and memory-bound CUDA kernels during HGNN training.
  To address this issue, we introduce HiFuse, an enhancement for PyTorch Geometric designed to accelerate mini-batch HGNN training on CPU-GPU systems. From the data perspective, we reorganize and merge multiple smaller vertex feature matrices into larger ones, enabling a single kernel to process larger data chunks. This efficiently exploits data locality, reduces the kernel launch overhead, and improves overall GPU utilization. From the workflow perspective, we sophisticatedly offload the construction of semantic graphs from GPU to CPU to reduce the number of CUDA kernels. To meet the parallelism requirements on CPU and ensure seamless execution between CPU and GPU, we employ parallelization techniques including multi-threading and asynchronous pipeline. This allows different stages of the process to overlap, enhancing GPU utilization and reducing end-to-end execution latency, leading to a more efficient and balanced use of computational resources. Through extensive experiments, HiFuse demonstrates an average 2.38 times speedup compared to a state-of-the-art solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08490v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meng Wu, Jingkai Qiu, Mingyu Yan, Wenming Li, Yang Zhang, Zhimin Zhang, Xiaochun Ye, Dongrui Fan</dc:creator>
    </item>
    <item>
      <title>R-HLS: An IR for Dynamic High-Level Synthesis and Memory Disambiguation based on Regions and State Edges</title>
      <link>https://arxiv.org/abs/2408.08712</link>
      <description>arXiv:2408.08712v1 Announce Type: new 
Abstract: Dynamically scheduled hardware enables high-level synthesis (HLS) for applications with irregular control flow and latencies, which perform poorly with conventional statically scheduled approaches. Since dynamically scheduled hardware is inherently data flow based, it is beneficial to have an intermediate representation (IR) that captures the global data flow to enable easier transformations. State-of-the-art dynamic HLS utilize control flow based IRs, which model data flow only at the basic block level, requiring the rediscovery of inter-block parallelism. The Regionalized Value State Dependence Graph (RVSDG) is an IR that models (1) control flow as part of the global data flow utilizing regions and (2) memory dependencies using state edges.
  We propose R-HLS, a new RVSDG dialect targeted for dynamic high-level synthesis. R-HLS explicitly models control flow decisions, routing, and memory, which are only abstractly represented in the RVSDG. Expressing the control flow as part of the data flow reduces the need for complex optimizations to extract performance and enables easy conversion to parallel circuits. Furthermore, we present a distributed memory disambiguation optimization that leverages memory state edges to decouple address generation from data accesses, resulting in resource efficient out-of-program-order execution of memory operations. Our results show that R-HLS effectively exposes parallelism, resulting in fewer executed cycles and a 10% speedup on average, compared to the state-of-the-art in dynamic HLS with optimized memory disambiguation. These results are achieved with a significant reduction in resource utilization, such as a 79% reduction in lookup-tables and 22% reduction in flip-flops, on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08712v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3676536.3676671</arxiv:DOI>
      <dc:creator>David Metz, Nico Reissmann, Magnus Sj\"alander</dc:creator>
    </item>
    <item>
      <title>Xpikeformer: Hybrid Analog-Digital Hardware Acceleration for Spiking Transformers</title>
      <link>https://arxiv.org/abs/2408.08794</link>
      <description>arXiv:2408.08794v1 Announce Type: new 
Abstract: This paper introduces Xpikeformer, a hybrid analog-digital hardware architecture designed to accelerate spiking neural network (SNN)-based transformer models. By combining the energy efficiency and temporal dynamics of SNNs with the powerful sequence modeling capabilities of transformers, Xpikeformer leverages mixed analog-digital computing techniques to enhance performance and energy efficiency. The architecture integrates analog in-memory computing (AIMC) for feedforward and fully connected layers, and a stochastic spiking attention (SSA) engine for efficient attention mechanisms. We detail the design, implementation, and evaluation of Xpikeformer, demonstrating significant improvements in energy consumption and computational efficiency. Through an image classification task and a wireless communication symbol detection task, we show that Xpikeformer can achieve software-comparable inference accuracy. Energy evaluations reveal that Xpikeformer achieves up to a $17.8$--$19.2\times$ reduction in energy consumption compared to state-of-the-art digital ANN transformers and up to a $5.9$--$6.8\times$ reduction compared to fully digital SNN transformers. Xpikeformer also achieves a $12.0\times$ speedup compared to the GPU implementation of spiking transformers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08794v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zihang Song, Prabodh Katti, Osvaldo Simeone, Bipin Rajendran</dc:creator>
    </item>
    <item>
      <title>RollingCache: Using Runtime Behavior to Defend Against Cache Side Channel Attacks</title>
      <link>https://arxiv.org/abs/2408.08795</link>
      <description>arXiv:2408.08795v1 Announce Type: cross 
Abstract: Shared caches are vulnerable to side channel attacks through contention in cache sets. Besides being a simple source of information leak, these side channels form useful gadgets for more sophisticated attacks that compromise the security of shared systems.
  The fundamental design aspect that contention attacks exploit is the deterministic nature of the set of addresses contending for a cache set. In this paper, we present RollingCache, a cache design that defends against contention attacks by dynamically changing the set of addresses contending for cache sets. Unlike prior defenses, RollingCache does not rely on address encryption/decryption, data relocation, or cache partitioning. We use one level of indirection to implement dynamic mapping controlled by the whole-cache runtime behavior. Our solution does not depend on having defined security domains, and can defend against an attacker running on the same or another core.
  We evaluate RollingCache on ChampSim using the SPEC-2017 benchmark suite. Our security evaluation shows that our dynamic mapping removes the deterministic ability to identify the source of contention. The performance evaluation shows an impact of 1.67\% over a mix of workloads, with a corresponding</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08795v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Divya Ojha, Sandhya Dwarkadas</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Survey on GNN Characterization</title>
      <link>https://arxiv.org/abs/2408.01902</link>
      <description>arXiv:2408.01902v2 Announce Type: replace 
Abstract: Characterizing graph neural networks (GNNs) is essential for identifying performance bottlenecks and facilitating their deployment. Despite substantial work in this area, a comprehensive survey on GNN characterization is lacking. This work presents a comprehensive survey, proposing a triple-level classification method to categorize, summarize, and compare existing efforts. In addition, we identify promising future directions for GNN characterization. Our survey aims to help scholars systematically understand GNN performance bottlenecks and patterns from a computer architecture perspective, contributing to more efficient GNN execution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01902v2</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meng Wu, Mingyu Yan, Wenming Li, Xiaochun Ye, Dongrui Fan, Yuan Xie</dc:creator>
    </item>
    <item>
      <title>Characterizing and Understanding HGNN Training on GPUs</title>
      <link>https://arxiv.org/abs/2407.11790</link>
      <description>arXiv:2407.11790v3 Announce Type: replace-cross 
Abstract: Owing to their remarkable representation capabilities for heterogeneous graph data, Heterogeneous Graph Neural Networks (HGNNs) have been widely adopted in many critical real-world domains such as recommendation systems and medical analysis. Prior to their practical application, identifying the optimal HGNN model parameters tailored to specific tasks through extensive training is a time-consuming and costly process. To enhance the efficiency of HGNN training, it is essential to characterize and analyze the execution semantics and patterns within the training process to identify performance bottlenecks. In this study, we conduct an in-depth quantification and analysis of two mainstream HGNN training scenarios, including single-GPU and multi-GPU distributed training. Based on the characterization results, we disclose the performance bottlenecks and their underlying causes in different HGNN training scenarios and provide optimization guidelines from both software and hardware perspectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11790v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dengke Han, Mingyu Yan, Xiaochun Ye, Dongrui Fan</dc:creator>
    </item>
    <item>
      <title>Control-Flow Attestation: Concepts, Solutions, and Open Challenges</title>
      <link>https://arxiv.org/abs/2408.06304</link>
      <description>arXiv:2408.06304v3 Announce Type: replace-cross 
Abstract: Control-flow attestation unifies the worlds of control-flow integrity and platform attestation by measuring and reporting a target's run-time behaviour to a verifier. Trust assurances in the target are provided by testing whether its execution follows an authorised control-flow path. The problem has been explored in various settings, such as assessing the trustworthiness of cyber-physical systems, Internet of Things devices, cloud platforms, and many others. Despite a significant number of proposals being made in recent years, the area remains fragmented, addressing different adversarial behaviours, verification paradigms, and deployment challenges. In this paper, we present the first survey of control-flow attestation, examining the core ideas and solutions in state-of-the-art schemes. In total, we survey over 30 papers published between 2016-2024, consolidate and compare their key features, and pose several challenges and recommendations for future research in the area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06304v3</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhanyu Sha, Carlton Shepherd, Amir Rafi, Konstantinos Markantonakis</dc:creator>
    </item>
  </channel>
</rss>
