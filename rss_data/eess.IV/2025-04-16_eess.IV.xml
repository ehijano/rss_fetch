<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Apr 2025 04:00:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Integrating electrocardiogram and fundus images for early detection of cardiovascular diseases</title>
      <link>https://arxiv.org/abs/2504.10493</link>
      <description>arXiv:2504.10493v1 Announce Type: new 
Abstract: Cardiovascular diseases (CVD) are a predominant health concern globally, emphasizing the need for advanced diagnostic techniques. In our research, we present an avant-garde methodology that synergistically integrates ECG readings and retinal fundus images to facilitate the early disease tagging as well as triaging of the CVDs in the order of disease priority. Recognizing the intricate vascular network of the retina as a reflection of the cardiovascular system, alongwith the dynamic cardiac insights from ECG, we sought to provide a holistic diagnostic perspective. Initially, a Fast Fourier Transform (FFT) was applied to both the ECG and fundus images, transforming the data into the frequency domain. Subsequently, the Earth Mover's Distance (EMD) was computed for the frequency-domain features of both modalities. These EMD values were then concatenated, forming a comprehensive feature set that was fed into a Neural Network classifier. This approach, leveraging the FFT's spectral insights and EMD's capability to capture nuanced data differences, offers a robust representation for CVD classification. Preliminary tests yielded a commendable accuracy of 84 percent, underscoring the potential of this combined diagnostic strategy. As we continue our research, we anticipate refining and validating the model further to enhance its clinical applicability in resource limited healthcare ecosystems prevalent across the Indian sub-continent and also the world at large.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10493v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s41598-025-87634-z</arxiv:DOI>
      <arxiv:journal_reference>Sci Rep 15, 4390 (2025)</arxiv:journal_reference>
      <dc:creator>K. A. Muthukumar, Dhruva Nandi, Priya Ranjan, Krithika Ramachandran, Shiny PJ, Anirban Ghosh, Ashwini M, Aiswaryah Radhakrishnan, V. E. Dhandapani, Rajiv Janardhanan</dc:creator>
    </item>
    <item>
      <title>Remote Sensing Based Crop Health Classification Using NDVI and Fully Connected Neural Networks</title>
      <link>https://arxiv.org/abs/2504.10522</link>
      <description>arXiv:2504.10522v1 Announce Type: new 
Abstract: Accurate crop health monitoring is not only essential for improving agricultural efficiency but also for ensuring sustainable food production in the face of environmental challenges. Traditional approaches often rely on visual inspection or simple NDVI measurements, which, though useful, fall short in detecting nuanced variations in crop stress and disease conditions. In this research, we propose a more sophisticated method that leverages NDVI data combined with a Fully Connected Neural Network (FCNN) to classify crop health with greater precision. The FCNN, trained using satellite imagery from various agricultural regions, is capable of identifying subtle distinctions between healthy crops, rust-affected plants, and other stressed conditions. Our approach not only achieved a remarkable classification accuracy of 97.80% but it also significantly outperformed conventional models in terms of precision, recall, and F1-scores. The ability to map the relationship between NDVI values and crop health using deep learning presents new opportunities for real-time, large-scale monitoring of agricultural fields, reducing manual efforts, and offering a scalable solution to address global food security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10522v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>J. Judith, R. Tamilselvi, M. Parisa Beham, S. Sathiya Pandiya Lakshmi, Alavikunhu Panthakkan, Saeed Al Mansoori, Hussain Al Ahmad</dc:creator>
    </item>
    <item>
      <title>PathSeqSAM: Sequential Modeling for Pathology Image Segmentation with SAM2</title>
      <link>https://arxiv.org/abs/2504.10526</link>
      <description>arXiv:2504.10526v1 Announce Type: new 
Abstract: Current methods for pathology image segmentation typically treat 2D slices independently, ignoring valuable cross-slice information. We present PathSeqSAM, a novel approach that treats 2D pathology slices as sequential video frames using SAM2's memory mechanisms. Our method introduces a distance-aware attention mechanism that accounts for variable physical distances between slices and employs LoRA for domain adaptation. Evaluated on the KPI Challenge 2024 dataset for glomeruli segmentation, PathSeqSAM demonstrates improved segmentation quality, particularly in challenging cases that benefit from cross-slice context. We have publicly released our code at https://github.com/JackyyyWang/PathSeqSAM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10526v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingyang Zhu, Yinting Liu, Mingyu Li, Jiacheng Wang</dc:creator>
    </item>
    <item>
      <title>Imaging Transformer for MRI Denoising: a Scalable Model Architecture that enables SNR &lt;&lt; 1 Imaging</title>
      <link>https://arxiv.org/abs/2504.10534</link>
      <description>arXiv:2504.10534v1 Announce Type: new 
Abstract: Purpose: To propose a flexible and scalable imaging transformer (IT) architecture with three attention modules for multi-dimensional imaging data and apply it to MRI denoising with very low input SNR.
  Methods: Three independent attention modules were developed: spatial local, spatial global, and frame attentions. They capture long-range signal correlation and bring back the locality of information in images. An attention-cell-block design processes 5D tensors ([B, C, F, H, W]) for 2D, 2D+T, and 3D image data. A High Resolution (HRNet) backbone was built to hold IT blocks. Training dataset consists of 206,677 cine series and test datasets had 7,267 series. Ten input SNR levels from 0.05 to 8.0 were tested. IT models were compared to seven convolutional and transformer baselines. To test scalability, four IT models 27m to 218m parameters were trained. Two senior cardiologists reviewed IT model outputs from which the EF was measured and compared against the ground-truth.
  Results: IT models significantly outperformed other models over the tested SNR levels. The performance gap was most prominent at low SNR levels. The IT-218m model had the highest SSIM and PSNR, restoring good image quality and anatomical details even at SNR 0.2. Two experts agreed at this SNR or above, the IT model output gave the same clinical interpretation as the ground-truth. The model produced images that had accurate EF measurements compared to ground-truth values.
  Conclusions: Imaging transformer model offers strong performance, scalability, and versatility for MR denoising. It recovers image quality suitable for confident clinical reading and accurate EF measurement, even at very low input SNR of 0.2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10534v1</guid>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <category>physics.med-ph</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hui Xue, Sarah M. Hooper, Rhodri H. Davies, Thomas A. Treibel, Iain Pierce, John Stairs, Joseph Naegele, Charlotte Manisty, James C. Moon, Adrienne E. Campbell-Washburn, Peter Kellman, Michael S. Hansen</dc:creator>
    </item>
    <item>
      <title>Efficient and Robust Remote Sensing Image Denoising Using Randomized Approximation of Geodesics' Gramian on the Manifold Underlying the Patch Space</title>
      <link>https://arxiv.org/abs/2504.10820</link>
      <description>arXiv:2504.10820v1 Announce Type: new 
Abstract: Remote sensing images are widely utilized in many disciplines such as feature recognition and scene semantic segmentation. However, due to environmental factors and the issues of the imaging system, the image quality is often degraded which may impair subsequent visual tasks. Even though denoising remote sensing images plays an essential role before applications, the current denoising algorithms fail to attain optimum performance since these images possess complex features in the texture. Denoising frameworks based on artificial neural networks have shown better performance; however, they require exhaustive training with heterogeneous samples that extensively consume resources like power, memory, computation, and latency. Thus, here we present a computationally efficient and robust remote sensing image denoising method that doesn't require additional training samples. This method partitions patches of a remote-sensing image in which a low-rank manifold, representing the noise-free version of the image, underlies the patch space. An efficient and robust approach to revealing this manifold is a randomized approximation of the singular value spectrum of the geodesics' Gramian matrix of the patch space. The method asserts a unique emphasis on each color channel during denoising so the three denoised channels are merged to produce the final image.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10820v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kelum Gajamannage, Dilhani I. Jayathilake, Maria Vasilyeva</dc:creator>
    </item>
    <item>
      <title>Intraoperative perfusion assessment by continuous, low-latency hyperspectral light-field imaging: development, methodology, and clinical application</title>
      <link>https://arxiv.org/abs/2504.10953</link>
      <description>arXiv:2504.10953v1 Announce Type: new 
Abstract: Accurate assessment of tissue perfusion is crucial in visceral surgery, especially during anastomosis. Currently, subjective visual judgment is commonly employed in clinical settings. Hyperspectral imaging (HSI) offers a non-invasive, quantitative alternative. However, HSI imaging lacks continuous integration into the clinical workflow. This study presents a hyperspectral light field system for intraoperative tissue oxygen saturation (SO2) analysis and visualization. We present a correlation method for determining SO2 saturation with low computational demands. We demonstrate clinical application, with our results aligning with the perfusion boundaries determined by the surgeon. We perform and compare continuous perfusion analysis using two hyperspectral cameras (Cubert S5, Cubert X20), achieving processing times of &lt; 170 ms and &lt; 400 ms, respectively. We discuss camera characteristics, system parameters, and the suitability for clinical use and real-time applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10953v1</guid>
      <category>eess.IV</category>
      <category>physics.med-ph</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1117/12.3042238</arxiv:DOI>
      <arxiv:journal_reference>Proc. SPIE 13306, Advanced Biomedical and Clinical Diagnostic and Surgical Guidance Systems XXIII, 1330606 (20 March 2025)</arxiv:journal_reference>
      <dc:creator>Stefan Kray (Institute of Smart Systems and Services, Pforzheim University), Andreas Schmid (Institute of Smart Systems and Services, Pforzheim University), Eric L. Wisotzky (Computer Vision &amp; Graphics, Vision &amp; Imaging Technologies, Fraunhofer Heinrich-Hertz-Institute HHI, Berlin, Visual Computing, Humboldt-University, Berlin), Moritz Gerlich (Institute of Smart Systems and Services, Pforzheim University), Sebastian Apweiler (Department of General-, Visceral- and Thoracic Surgery, Klinikum Darmstadt), Anna Hilsmann (Computer Vision &amp; Graphics, Vision &amp; Imaging Technologies, Fraunhofer Heinrich-Hertz-Institute HHI, Berlin), Thomas Greiner (Institute of Smart Systems and Services, Pforzheim University), Peter Eisert (Computer Vision &amp; Graphics, Vision &amp; Imaging Technologies, Fraunhofer Heinrich-Hertz-Institute HHI, Berlin, Visual Computing, Humboldt-University, Berlin), Werner Kneist (Department of General-, Visceral- and Thoracic Surgery, Klinikum Darmstadt)</dc:creator>
    </item>
    <item>
      <title>AgentPolyp: Accurate Polyp Segmentation via Image Enhancement Agent</title>
      <link>https://arxiv.org/abs/2504.10978</link>
      <description>arXiv:2504.10978v1 Announce Type: new 
Abstract: Since human and environmental factors interfere, captured polyp images usually suffer from issues such as dim lighting, blur, and overexposure, which pose challenges for downstream polyp segmentation tasks. To address the challenges of noise-induced degradation in polyp images, we present AgentPolyp, a novel framework integrating CLIP-based semantic guidance and dynamic image enhancement with a lightweight neural network for segmentation. The agent first evaluates image quality using CLIP-driven semantic analysis (e.g., identifying ``low-contrast polyps with vascular textures") and adapts reinforcement learning strategies to dynamically apply multi-modal enhancement operations (e.g., denoising, contrast adjustment). A quality assessment feedback loop optimizes pixel-level enhancement and segmentation focus in a collaborative manner, ensuring robust preprocessing before neural network segmentation. This modular architecture supports plug-and-play extensions for various enhancement algorithms and segmentation networks, meeting deployment requirements for endoscopic devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10978v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pu Wang, Zhihua Zhang, Dianjie Lu, Guijuan Zhang, Youshan Zhang, Zhuoran Zheng</dc:creator>
    </item>
    <item>
      <title>Efficient Medical Image Restoration via Reliability Guided Learning in Frequency Domain</title>
      <link>https://arxiv.org/abs/2504.11286</link>
      <description>arXiv:2504.11286v1 Announce Type: new 
Abstract: Medical image restoration tasks aim to recover high-quality images from degraded observations, exhibiting emergent desires in many clinical scenarios, such as low-dose CT image denoising, MRI super-resolution, and MRI artifact removal. Despite the success achieved by existing deep learning-based restoration methods with sophisticated modules, they struggle with rendering computationally-efficient reconstruction results. Moreover, they usually ignore the reliability of the restoration results, which is much more urgent in medical systems. To alleviate these issues, we present LRformer, a Lightweight Transformer-based method via Reliability-guided learning in the frequency domain. Specifically, inspired by the uncertainty quantification in Bayesian neural networks (BNNs), we develop a Reliable Lesion-Semantic Prior Producer (RLPP). RLPP leverages Monte Carlo (MC) estimators with stochastic sampling operations to generate sufficiently-reliable priors by performing multiple inferences on the foundational medical image segmentation model, MedSAM. Additionally, instead of directly incorporating the priors in the spatial domain, we decompose the cross-attention (CA) mechanism into real symmetric and imaginary anti-symmetric parts via fast Fourier transform (FFT), resulting in the design of the Guided Frequency Cross-Attention (GFCA) solver. By leveraging the conjugated symmetric property of FFT, GFCA reduces the computational complexity of naive CA by nearly half. Extensive experimental results in various tasks demonstrate the superiority of the proposed LRformer in both effectiveness and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11286v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengcheng Zheng, Kecheng Chen, Jiaxin Huang, Bohao Chen, Ju Liu, Yazhou Ren, Xiaorong Pu</dc:creator>
    </item>
    <item>
      <title>Ring Artifacts Correction Based on Global-Local Features Interaction Guidance in the Projection Domain</title>
      <link>https://arxiv.org/abs/2504.11375</link>
      <description>arXiv:2504.11375v1 Announce Type: new 
Abstract: Ring artifacts are common artifacts in CT imaging, typically caused by inconsistent responses of detector units to X-rays, resulting in stripe artifacts in the projection data. Under circular scanning mode, such artifacts manifest as concentric rings radiating from the center of rotation, severely degrading image quality. In the Radon transform domain, even if the object's density function is piecewise discontinuous in certain regions, the projection images remain nearly continuous in the angular direction, making the ideal projections exhibit a smooth global low-frequency characteristic. In practical scanning, the local disturbances of the same detector unit at different scanning angles lead to a prominent high-frequency locality of stripe artifacts. Existing studies generally model ring artifacts disturbances as fixed additive errors, which overlooks the dynamic variation of detector responses during practical scanning. However, the degree of detector response inconsistency is a function of the projection values, as revealed in our experiments, thereby requiring consideration of the interaction between global and local features in the process of stripe artifacts extraction and correction. Therefore, we propose a CT ring artifacts correction method based on global and local features in the projection domain. We employ the VSS block and Dense block to respectively correct the low-frequency sub-band, which capture the global correlations of the projection, and the high-frequency sub-band, which contain local stripe artifacts after wavelet decomposition. Specifically, the accuracy of artifacts correction is enhanced by the interaction guidance between global and local features. Extensive experiments demonstrate that our method achieves superior performance in both quantitative metrics and visual quality, verifying its robustness and practical applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11375v1</guid>
      <category>eess.IV</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunze Liu, Congyi Su, Xing Zhao</dc:creator>
    </item>
    <item>
      <title>H3AE: High Compression, High Speed, and High Quality AutoEncoder for Video Diffusion Models</title>
      <link>https://arxiv.org/abs/2504.10567</link>
      <description>arXiv:2504.10567v1 Announce Type: cross 
Abstract: Autoencoder (AE) is the key to the success of latent diffusion models for image and video generation, reducing the denoising resolution and improving efficiency. However, the power of AE has long been underexplored in terms of network design, compression ratio, and training strategy. In this work, we systematically examine the architecture design choices and optimize the computation distribution to obtain a series of efficient and high-compression video AEs that can decode in real time on mobile devices. We also unify the design of plain Autoencoder and image-conditioned I2V VAE, achieving multifunctionality in a single network. In addition, we find that the widely adopted discriminative losses, i.e., GAN, LPIPS, and DWT losses, provide no significant improvements when training AEs at scale. We propose a novel latent consistency loss that does not require complicated discriminator design or hyperparameter tuning, but provides stable improvements in reconstruction quality. Our AE achieves an ultra-high compression ratio and real-time decoding speed on mobile while outperforming prior art in terms of reconstruction metrics by a large margin. We finally validate our AE by training a DiT on its latent space and demonstrate fast, high-quality text-to-video generation capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10567v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yushu Wu, Yanyu Li, Ivan Skorokhodov, Anil Kag, Willi Menapace, Sharath Girish, Aliaksandr Siarohin, Yanzhi Wang, Sergey Tulyakov</dc:creator>
    </item>
    <item>
      <title>The Tenth NTIRE 2025 Efficient Super-Resolution Challenge Report</title>
      <link>https://arxiv.org/abs/2504.10686</link>
      <description>arXiv:2504.10686v1 Announce Type: cross 
Abstract: This paper presents a comprehensive review of the NTIRE 2025 Challenge on Single-Image Efficient Super-Resolution (ESR). The challenge aimed to advance the development of deep models that optimize key computational metrics, i.e., runtime, parameters, and FLOPs, while achieving a PSNR of at least 26.90 dB on the $\operatorname{DIV2K\_LSDIR\_valid}$ dataset and 26.99 dB on the $\operatorname{DIV2K\_LSDIR\_test}$ dataset. A robust participation saw \textbf{244} registered entrants, with \textbf{43} teams submitting valid entries. This report meticulously analyzes these methods and results, emphasizing groundbreaking advancements in state-of-the-art single-image ESR techniques. The analysis highlights innovative approaches and establishes benchmarks for future research in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10686v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bin Ren, Hang Guo, Lei Sun, Zongwei Wu, Radu Timofte, Yawei Li, Yao Zhang, Xinning Chai, Zhengxue Cheng, Yingsheng Qin, Yucai Yang, Li Song, Hongyuan Yu, Pufan Xu, Cheng Wan, Zhijuan Huang, Peng Guo, Shuyuan Cui, Chenjun Li, Xuehai Hu, Pan Pan, Xin Zhang, Heng Zhang, Qing Luo, Linyan Jiang, Haibo Lei, Qifang Gao, Yaqing Li, Weihua Luo, Tsing Li, Qing Wang, Yi Liu, Yang Wang, Hongyu An, Liou Zhang, Shijie Zhao, Lianhong Song, Long Sun, Jinshan Pan, Jiangxin Dong, Jinhui Tang, Jing Wei, Mengyang Wang, Ruilong Guo, Qian Wang, Qingliang Liu, Yang Cheng,  Davinci, Enxuan Gu, Pinxin Liu, Yongsheng Yu, Hang Hua, Yunlong Tang, Shihao Wang, Yukun Yang, Zhiyu Zhang, Yukun Yang, Jiyu Wu, Jiancheng Huang, Yifan Liu, Yi Huang, Shifeng Chen, Rui Chen, Yi Feng, Mingxi Li, Cailu Wan, Xiangji Wu, Zibin Liu, Jinyang Zhong, Kihwan Yoon, Ganzorig Gankhuyag, Shengyun Zhong, Mingyang Wu, Renjie Li, Yushen Zuo, Zhengzhong Tu, Zongang Gao, Guannan Chen, Yuan Tian, Wenhui Chen, Weijun Yuan, Zhan Li, Yihang Chen, Yifan Deng, Ruting Deng, Yilin Zhang, Huan Zheng, Yanyan Wei, Wenxuan Zhao, Suiyi Zhao, Fei Wang, Kun Li, Yinggan Tang, Mengjie Su, Jae-hyeon Lee, Dong-Hyeop Son, Ui-Jin Choi, Tiancheng Shao, Yuqing Zhang, Mengcheng Ma, Donggeun Ko, Youngsang Kwak, Jiun Lee, Jaehwa Kwak, Yuxuan Jiang, Qiang Zhu, Siyue Teng, Fan Zhang, Shuyuan Zhu, Bing Zeng, David Bull, Jing Hu, Hui Deng, Xuan Zhang, Lin Zhu, Qinrui Fan, Weijian Deng, Junnan Wu, Wenqin Deng, Yuquan Liu, Zhaohong Xu, Jameer Babu Pinjari, Kuldeep Purohit, Zeyu Xiao, Zhuoyuan Li, Surya Vashisth, Akshay Dudhane, Praful Hambarde, Sachin Chaudhary, Satya Naryan Tazi, Prashant Patil, Santosh Kumar Vipparthi, Subrahmanyam Murala, Wei-Chen Shen, I-Hsiang Chen, Yunzhe Xu, Chen Zhao, Zhizhou Chen, Akram Khatami-Rizi, Ahmad Mahmoudi-Aznaveh, Alejandro Merino, Bruno Longarela, Javier Abad, Marcos V. Conde, Simone Bianco, Luca Cogo, Gianmarco Corti</dc:creator>
    </item>
    <item>
      <title>HippoMM: Hippocampal-inspired Multimodal Memory for Long Audiovisual Event Understanding</title>
      <link>https://arxiv.org/abs/2504.10739</link>
      <description>arXiv:2504.10739v1 Announce Type: cross 
Abstract: Comprehending extended audiovisual experiences remains a fundamental challenge for computational systems. Current approaches struggle with temporal integration and cross-modal associations that humans accomplish effortlessly through hippocampal-cortical networks. We introduce HippoMM, a biologically-inspired architecture that transforms hippocampal mechanisms into computational advantages for multimodal understanding. HippoMM implements three key innovations: (i) hippocampus-inspired pattern separation and completion specifically designed for continuous audiovisual streams, (ii) short-to-long term memory consolidation that transforms perceptual details into semantic abstractions, and (iii) cross-modal associative retrieval pathways enabling modality-crossing queries. Unlike existing retrieval systems with static indexing schemes, HippoMM dynamically forms integrated episodic representations through adaptive temporal segmentation and dual-process memory encoding. Evaluations on our challenging HippoVlog benchmark demonstrate that HippoMM significantly outperforms state-of-the-art approaches (78.2% vs. 64.2% accuracy) while providing substantially faster response times (20.4s vs. 112.5s). Our results demonstrate that translating neuroscientific memory principles into computational architectures provides a promising foundation for next-generation multimodal understanding systems. The code and benchmark dataset are publicly available at https://github.com/linyueqian/HippoMM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10739v1</guid>
      <category>cs.MM</category>
      <category>eess.IV</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yueqian Lin, Qinsi Wang, Hancheng Ye, Yuzhe Fu, Hai "Helen" Li, Yiran Chen</dc:creator>
    </item>
    <item>
      <title>Three-dimensional neural network driving self-interference digital holography enables high-fidelity, non-scanning volumetric fluorescence microscopy</title>
      <link>https://arxiv.org/abs/2504.10769</link>
      <description>arXiv:2504.10769v1 Announce Type: cross 
Abstract: We present a deep learning driven computational approach to overcome the limitations of self-interference digital holography that imposed by inferior axial imaging performances. We demonstrate a 3D deep neural network model can simultaneously suppresses the defocus noise and improves the spatial resolution and signal-to-noise ratio of conventional numerical back-propagation-obtained holographic reconstruction. Compared with existing 2D deep neural networks used for hologram reconstruction, our 3D model exhibits superior performance in enhancing the resolutions along all the three spatial dimensions. As the result, 3D non-scanning volumetric fluorescence microscopy can be achieved, using 2D self-interference hologram as input, without any mechanical and opto-electronic scanning and complicated system calibration. Our method offers a high spatiotemporal resolution 3D imaging approach which can potentially benefit, for example, the visualization of dynamics of cellular structure and measurement of 3D behavior of high-speed flow field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10769v1</guid>
      <category>physics.optics</category>
      <category>eess.IV</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianlong Man, Yuwen Zhang, Yuchen Wu, Zhiqing Zhang, Hongqiang Zhou, Liyun Zhong, Yuhong Wan</dc:creator>
    </item>
    <item>
      <title>A comprehensive review of remote sensing in wetland classification and mapping</title>
      <link>https://arxiv.org/abs/2504.10842</link>
      <description>arXiv:2504.10842v1 Announce Type: cross 
Abstract: Wetlands constitute critical ecosystems that support both biodiversity and human well-being; however, they have experienced a significant decline since the 20th century. Back in the 1970s, researchers began to employ remote sensing technologies for wetland classification and mapping to elucidate the extent and variations of wetlands. Although some review articles summarized the development of this field, there is a lack of a thorough and in-depth understanding of wetland classification and mapping: (1) the scientific importance of wetlands, (2) major data, methods used in wetland classification and mapping, (3) driving factors of wetland changes, (4) current research paradigm and limitations, (5) challenges and opportunities in wetland classification and mapping under the context of technological innovation and global environmental change. In this review, we aim to provide a comprehensive perspective and new insights into wetland classification and mapping for readers to answer these questions. First, we conduct a meta-analysis of over 1,200 papers, encompassing wetland types, methods, sensor types, and study sites, examining prevailing trends in wetland classification and mapping. Next, we review and synthesize the wetland features and existing data and methods in wetland classification and mapping. We also summarize typical wetland mapping products and explore the intrinsic driving factors of wetland changes across multiple spatial and temporal scales. Finally, we discuss current limitations and propose future directions in response to global environmental change and technological innovation. This review consolidates our understanding of wetland remote sensing and offers scientific recommendations that foster transformative progress in wetland science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10842v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuai Yuan, Xiangan Liang, Tianwu Lin, Shuang Chen, Rui Liu, Jie Wang, Hongsheng Zhang, Peng Gong</dc:creator>
    </item>
    <item>
      <title>Self-Supervised Enhancement of Forward-Looking Sonar Images: Bridging Cross-Modal Degradation Gaps through Feature Space Transformation and Multi-Frame Fusion</title>
      <link>https://arxiv.org/abs/2504.10974</link>
      <description>arXiv:2504.10974v1 Announce Type: cross 
Abstract: Enhancing forward-looking sonar images is critical for accurate underwater target detection. Current deep learning methods mainly rely on supervised training with simulated data, but the difficulty in obtaining high-quality real-world paired data limits their practical use and generalization. Although self-supervised approaches from remote sensing partially alleviate data shortages, they neglect the cross-modal degradation gap between sonar and remote sensing images. Directly transferring pretrained weights often leads to overly smooth sonar images, detail loss, and insufficient brightness. To address this, we propose a feature-space transformation that maps sonar images from the pixel domain to a robust feature domain, effectively bridging the degradation gap. Additionally, our self-supervised multi-frame fusion strategy leverages complementary inter-frame information to naturally remove speckle noise and enhance target-region brightness. Experiments on three self-collected real-world forward-looking sonar datasets show that our method significantly outperforms existing approaches, effectively suppressing noise, preserving detailed edges, and substantially improving brightness, demonstrating strong potential for underwater target detection applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10974v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhisheng Zhang, Peng Zhang, Fengxiang Wang, Liangli Ma, Fuchun Sun</dc:creator>
    </item>
    <item>
      <title>K-means Enhanced Density Gradient Analysis for Urban and Transport Metrics Using Multi-Modal Satellite Imagery</title>
      <link>https://arxiv.org/abs/2504.11128</link>
      <description>arXiv:2504.11128v1 Announce Type: cross 
Abstract: This paper presents a novel computational approach for evaluating urban metrics through density gradient analysis using multi-modal satellite imagery, with applications including public transport and other urban systems. By combining optical and Synthetic Aperture Radar (SAR) data, we develop a method to segment urban areas, identify urban centers, and quantify density gradients. Our approach calculates two key metrics: the density gradient coefficient ($\alpha$) and the minimum effective distance (LD) at which density reaches a target threshold. We further employ machine learning techniques, specifically K-means clustering, to objectively identify uniform and high-variability regions within density gradient plots. We demonstrate that these metrics provide an effective screening tool for public transport analyses by revealing the underlying urban structure. Through comparative analysis of two representative cities with contrasting urban morphologies (monocentric vs polycentric), we establish relationships between density gradient characteristics and public transport network topologies. Cities with clear density peaks in their gradient plots indicate distinct urban centers requiring different transport strategies than those with more uniform density distributions. This methodology offers urban planners a cost-effective, globally applicable approach to preliminary public transport assessment using freely available satellite data. The complete implementation, with additional examples and documentation, is available in an open-source repository under the MIT license at https://github.com/nexri/Satellite-Imagery-Urban-Analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11128v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P. Tomkiewicz, J. Jaworski, P. Zielonka, A. Wilinski</dc:creator>
    </item>
    <item>
      <title>SAR-to-RGB Translation with Latent Diffusion for Earth Observation</title>
      <link>https://arxiv.org/abs/2504.11154</link>
      <description>arXiv:2504.11154v1 Announce Type: cross 
Abstract: Earth observation satellites like Sentinel-1 (S1) and Sentinel-2 (S2) provide complementary remote sensing (RS) data, but S2 images are often unavailable due to cloud cover or data gaps. To address this, we propose a diffusion model (DM)-based approach for SAR-to-RGB translation, generating synthetic optical images from SAR inputs. We explore three different setups: two using Standard Diffusion, which reconstruct S2 images by adding and removing noise (one without and one with class conditioning), and one using Cold Diffusion, which blends S2 with S1 before removing the SAR signal. We evaluate the generated images in downstream tasks, including land cover classification and cloud removal. While generated images may not perfectly replicate real S2 data, they still provide valuable information. Our results show that class conditioning improves classification accuracy, while cloud removal performance remains competitive despite our approach not being optimized for it. Interestingly, despite exhibiting lower perceptual quality, the Cold Diffusion setup performs well in land cover classification, suggesting that traditional quantitative evaluation metrics may not fully reflect the practical utility of generated images. Our findings highlight the potential of DMs for SAR-to-RGB translation in RS applications where RGB images are missing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11154v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaan Aydin, Joelle Hanna, Damian Borth</dc:creator>
    </item>
    <item>
      <title>Focal Split: Untethered Snapshot Depth from Differential Defocus</title>
      <link>https://arxiv.org/abs/2504.11202</link>
      <description>arXiv:2504.11202v1 Announce Type: cross 
Abstract: We introduce Focal Split, a handheld, snapshot depth camera with fully onboard power and computing based on depth-from-differential-defocus (DfDD). Focal Split is passive, avoiding power consumption of light sources. Its achromatic optical system simultaneously forms two differentially defocused images of the scene, which can be independently captured using two photosensors in a snapshot. The data processing is based on the DfDD theory, which efficiently computes a depth and a confidence value for each pixel with only 500 floating point operations (FLOPs) per pixel from the camera measurements. We demonstrate a Focal Split prototype, which comprises a handheld custom camera system connected to a Raspberry Pi 5 for real-time data processing. The system consumes 4.9 W and is powered on a 5 V, 10,000 mAh battery. The prototype can measure objects with distances from 0.4 m to 1.2 m, outputting 480$\times$360 sparse depth maps at 2.1 frames per second (FPS) using unoptimized Python scripts. Focal Split is DIY friendly. A comprehensive guide to building your own Focal Split depth camera, code, and additional data can be found at https://focal-split.qiguo.org.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11202v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Junjie Luo, John Mamish, Alan Fu, Thomas Concannon, Josiah Hester, Emma Alexander, Qi Guo</dc:creator>
    </item>
    <item>
      <title>Deep Learning-based Bathymetry Retrieval without In-situ Depths using Remote Sensing Imagery and SfM-MVS DSMs with Data Gaps</title>
      <link>https://arxiv.org/abs/2504.11416</link>
      <description>arXiv:2504.11416v1 Announce Type: cross 
Abstract: Accurate, detailed, and high-frequent bathymetry is crucial for shallow seabed areas facing intense climatological and anthropogenic pressures. Current methods utilizing airborne or satellite optical imagery to derive bathymetry primarily rely on either SfM-MVS with refraction correction or Spectrally Derived Bathymetry (SDB). However, SDB methods often require extensive manual fieldwork or costly reference data, while SfM-MVS approaches face challenges even after refraction correction. These include depth data gaps and noise in environments with homogeneous visual textures, which hinder the creation of accurate and complete Digital Surface Models (DSMs) of the seabed. To address these challenges, this work introduces a methodology that combines the high-fidelity 3D reconstruction capabilities of the SfM-MVS methods with state-of-the-art refraction correction techniques, along with the spectral analysis capabilities of a new deep learning-based method for bathymetry prediction. This integration enables a synergistic approach where SfM-MVS derived DSMs with data gaps are used as training data to generate complete bathymetric maps. In this context, we propose Swin-BathyUNet that combines U-Net with Swin Transformer self-attention layers and a cross-attention mechanism, specifically tailored for SDB. Swin-BathyUNet is designed to improve bathymetric accuracy by capturing long-range spatial relationships and can also function as a standalone solution for standard SDB with various training depth data, independent of the SfM-MVS output. Experimental results in two completely different test sites in the Mediterranean and Baltic Seas demonstrate the effectiveness of the proposed approach through extensive experiments that demonstrate improvements in bathymetric accuracy, detail, coverage, and noise reduction in the predicted DSM. The code is available at https://github.com/pagraf/Swin-BathyUNet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11416v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Panagiotis Agrafiotis, Beg\"um Demir</dc:creator>
    </item>
    <item>
      <title>Ring deconvolution microscopy: exploiting symmetry for efficient spatially varying aberration correction</title>
      <link>https://arxiv.org/abs/2206.08928</link>
      <description>arXiv:2206.08928v4 Announce Type: replace 
Abstract: The most ubiquitous form of computational aberration correction for microscopy is deconvolution. However, deconvolution relies on the assumption that the point spread function is the same across the entire field-of-view. This assumption is often inadequate, but space-variant deblurring techniques generally require impractical amounts of calibration and computation. We present a new imaging pipeline that leverages symmetry to provide simple and fast spatially-varying aberration correction. Our ring deconvolution microscopy (RDM) method leverages the rotational symmetry of most microscopes and cameras, and naturally extends to sheet deconvolution in the case of lateral symmetry. We formally derive theory and algorithms for image recovery and additionally propose a neural network based on Seidel coefficients as a fast alternative, as well as extension of RDM to blind deconvolution. We demonstrate significant improvements in speed and image quality as compared to standard deconvolution and existing spatially-varying deconvolution across a diverse range of microscope modalities, including miniature microscopy, multicolor fluorescence microscopy, point-scanning multimode fiber micro-endoscopy, and light-sheet fluorescence microscopy. Our approach enables near-isotropic, subcellular resolution in each of these applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.08928v4</guid>
      <category>eess.IV</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amit Kohli, Anastasios N. Angelopoulos, David McAllister, Esther Whang, Sixian You, Kyrollos Yanny, Federico M. Gasparoli, Bo-Jui Chang, Reto Fiolka, Laura Waller</dc:creator>
    </item>
    <item>
      <title>SlicerNNInteractive: A 3D Slicer extension for nnInteractive</title>
      <link>https://arxiv.org/abs/2504.07991</link>
      <description>arXiv:2504.07991v2 Announce Type: replace 
Abstract: SlicerNNInteractive integrates nnInteractive, a state-of-the-art promptable deep learning-based framework for 3D image segmentation, into the widely used 3D Slicer platform. Our extension implements a client-server architecture that decouples computationally intensive model inference from the client-side interface. Therefore, SlicerNNInteractive eliminates heavy hardware constraints on the client-side and enables better operating system compatibility than existing plugins for nnInteractive. Running both the client and server-side on a single machine is also possible, offering flexibility across different deployment scenarios. The extension provides an intuitive user interface with all interaction types available in the original framework (point, bounding box, scribble, and lasso prompts), while including a comprehensive set of keyboard shortcuts for efficient workflow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07991v2</guid>
      <category>eess.IV</category>
      <pubDate>Wed, 16 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Coen de Vente, Kiran Vaidhya Venkadesh, Bram van Ginneken, Clara I. S\'anchez</dc:creator>
    </item>
  </channel>
</rss>
