<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Nov 2025 05:00:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>MECHBench: A Set of Black-Box Optimization Benchmarks originated from Structural Mechanics</title>
      <link>https://arxiv.org/abs/2511.10821</link>
      <description>arXiv:2511.10821v1 Announce Type: new 
Abstract: Benchmarking is essential for developing and evaluating black-box optimization algorithms, providing a structured means to analyze their search behavior. Its effectiveness relies on carefully selected problem sets used for evaluation. To date, most established benchmark suites for black-box optimization consist of abstract or synthetic problems that only partially capture the complexities of real-world engineering applications, thereby severely limiting the insights that can be gained for application-oriented optimization scenarios and reducing their practical impact. To close this gap, we propose a new benchmarking suite that addresses it by presenting a curated set of optimization benchmarks rooted in structural mechanics. The current implemented benchmarks are derived from vehicle crashworthiness scenarios, which inherently require the use of gradient-free algorithms due to the non-smooth, highly non-linear nature of the underlying models. Within this paper, the reader will find descriptions of the physical context of each case, the corresponding optimization problem formulations, and clear guidelines on how to employ the suite.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10821v1</guid>
      <category>cs.NE</category>
      <category>cs.CE</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iv\'an Olarte Rodr\'iguez, Maria Laura Santoni, Fabian Duddeck, Carola Doerr, Thomas B\"ack, Elena Raponi</dc:creator>
    </item>
    <item>
      <title>Spectral Neuro-Symbolic Reasoning II: Semantic Node Merging, Entailment Filtering, and Knowledge Graph Alignment</title>
      <link>https://arxiv.org/abs/2511.10655</link>
      <description>arXiv:2511.10655v1 Announce Type: cross 
Abstract: This report extends the Spectral Neuro-Symbolic Reasoning (Spectral NSR) framework by introducing three semantically grounded enhancements: (1) transformer-based node merging using contextual embeddings (e.g., Sentence-BERT, SimCSE) to reduce redundancy, (2) sentence-level entailment validation with pretrained NLI classifiers (e.g., RoBERTa, DeBERTa) to improve edge quality, and (3) alignment with external knowledge graphs (e.g., ConceptNet, Wikidata) to augment missing context. These modifications enhance graph fidelity while preserving the core spectral reasoning pipeline. Experimental results on ProofWriter, EntailmentBank, and CLUTRR benchmarks show consistent accuracy gains (up to +3.8\%), improved generalization to adversarial cases, and reduced inference noise. The novelty lies in performing semantic and symbolic refinement entirely upstream of the spectral inference stage, enabling efficient, interpretable, and scalable reasoning without relying on quadratic attention mechanisms. In summary, this work extends the Spectral NSR framework with modular, semantically grounded preprocessing steps that improve graph quality without altering the core spectral reasoning engine. The result is a more robust, interpretable, and scalable reasoning system suitable for deployment in open-domain and real-world settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10655v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Kiruluta, Priscilla Burity</dc:creator>
    </item>
    <item>
      <title>Toward Generalized Detection of Synthetic Media: Limitations, Challenges, and the Path to Multimodal Solutions</title>
      <link>https://arxiv.org/abs/2511.11116</link>
      <description>arXiv:2511.11116v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) in media has advanced rapidly over the last decade. The introduction of Generative Adversarial Networks (GANs) improved the quality of photorealistic image generation. Diffusion models later brought a new era of generative media. These advances made it difficult to separate real and synthetic content. The rise of deepfakes demonstrated how these tools could be misused to spread misinformation, political conspiracies, privacy violations, and fraud. For this reason, many detection models have been developed. They often use deep learning methods such as Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs). These models search for visual, spatial, or temporal anomalies. However, such approaches often fail to generalize across unseen data and struggle with content from different models. In addition, existing approaches are ineffective in multimodal data and highly modified content. This study reviews twenty-four recent works on AI-generated media detection. Each study was examined individually to identify its contributions and weaknesses, respectively. The review then summarizes the common limitations and key challenges faced by current approaches. Based on this analysis, a research direction is suggested with a focus on multimodal deep learning models. Such models have the potential to provide more robust and generalized detection. It offers future researchers a clear starting point for building stronger defenses against harmful synthetic media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11116v1</guid>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Redwan Hussain, Mizanur Rahman, Prithwiraj Bhattacharjee</dc:creator>
    </item>
    <item>
      <title>CHNNet: An Artificial Neural Network With Connected Hidden Neurons</title>
      <link>https://arxiv.org/abs/2305.10468</link>
      <description>arXiv:2305.10468v3 Announce Type: replace 
Abstract: In contrast to biological neural circuits, conventional artificial neural networks are commonly organized as strictly hierarchical architectures that exclude direct connections among neurons within the same layer. Consequently, information flow is primarily confined to feedforward and feedback pathways across layers, which limits lateral interactions and constrains the potential for intra-layer information integration. We introduce an artificial neural network featuring intra-layer connections among hidden neurons to overcome this limitation. Owing to the proposed method for facilitating intra-layer connections, the model is theoretically anticipated to achieve faster convergence compared to conventional feedforward neural networks. The experimental findings provide further validation of the theoretical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.10468v3</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Rafiad Sadat Shahir, Zayed Humayun, Mashrufa Akter Tamim, Shouri Saha, Md. Golam Rabiul Alam, Abu Mohammad Khan</dc:creator>
    </item>
    <item>
      <title>Mapping Literature Landscapes with Data-Driven Discovery: A Case Study on MOEA/D</title>
      <link>https://arxiv.org/abs/2404.14228</link>
      <description>arXiv:2404.14228v2 Announce Type: replace 
Abstract: We are living in an era of "big literature", where scientific literature is expanding exponentially. While this growth presents new opportunities, it complicates mapping global scientific research landscapes, as manual review methods become infeasible. Recent advancements in machine learning, complex networks, and natural language processing have enabled numerous data-driven discovery methods. Building upon these tools, we introduce an end-to-end workflow for analyzing large-scale literature landscapes, LitLA. This workflow first integrates diverse publication metadata into a bibliographic knowledge graph (KG) representing the research landscape. It then offers tools for exploratory analysis of various landscape aspects. We demonstrate the effectiveness of LitLA via a case study on follow-up works of multi-objective evolutionary algorithm based on decomposition (MOEA/D). In doing so, we constructed the MOEA/D research landscape as a KG comprising over 5,400 papers, 10,000 authors, 1,600 institutions, and 78,000 keywords. With this landscape, we start with descriptive statistics and investigate prominent topics pertaining to MOEA/D and interrogate their spatial-temporal and bilateral relationships. We then map the collaboration and citation networks to reveal the community's growth over time. We further experiment whether learning on latent patterns of this landscape can hint on future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14228v2</guid>
      <category>cs.NE</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyu Huang, Shasha Zhou, Ke Li</dc:creator>
    </item>
    <item>
      <title>NervePool: A Simplicial Pooling Layer</title>
      <link>https://arxiv.org/abs/2305.06315</link>
      <description>arXiv:2305.06315v2 Announce Type: replace-cross 
Abstract: For deep learning problems on graph-structured data, pooling layers are important for down sampling, reducing computational cost, and to minimize overfitting. We define a pooling layer, nervePool, for data structured as simplicial complexes, which are generalizations of graphs that include higher-dimensional simplices beyond vertices and edges; this structure allows for greater flexibility in modeling higher-order relationships. The proposed simplicial coarsening scheme is built upon partitions of vertices, which allow us to generate hierarchical representations of simplicial complexes, collapsing information in a learned fashion. NervePool builds on the learned vertex cluster assignments and extends to coarsening of higher dimensional simplices in a deterministic fashion. While in practice the pooling operations are computed via a series of matrix operations, the topological motivation is a set-theoretic construction based on unions of stars of simplices and the nerve complex.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.06315v2</guid>
      <category>cs.CG</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarah McGuire Scullen, Ernst R\"oell, Elizabeth Munch, Bastian Rieck, Matthew Hirn</dc:creator>
    </item>
    <item>
      <title>Evolutionary Retrofitting</title>
      <link>https://arxiv.org/abs/2410.11330</link>
      <description>arXiv:2410.11330v2 Announce Type: replace-cross 
Abstract: AfterLearnER (After Learning Evolutionary Retrofitting) consists in applying evolutionary optimization to refine fully trained machine learning models by optimizing a set of carefully chosen parameters or hyperparameters of the model, with respect to some actual, exact, and hence possibly non-differentiable error signal, performed on a subset of the standard validation set. The efficiency of AfterLearnER is demonstrated by tackling non-differentiable signals such as threshold-based criteria in depth sensing, the word error rate in speech re-synthesis, the number of kills per life at Doom, computational accuracy or BLEU in code translation, image quality in 3D generative adversarial networks (GANs), and user feedback in image generation via Latent Diffusion Models (LDM). This retrofitting can be done after training, or dynamically at inference time by taking into account the user feedback. The advantages of AfterLearnER are its versatility, the possibility to use non-differentiable feedback, including human evaluations (i.e., no gradient is needed), the limited overfitting supported by a theoretical study, and its anytime behavior. Last but not least, AfterLearnER requires only a small amount of feedback, i.e., a few dozen to a few hundred scalars, compared to the tens of thousands needed in most related published works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11330v2</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>math.OC</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mathurin Videau (TAU), Mariia Zameshina (LIGM), Alessandro Leite (TAU), Laurent Najman (LIGM, KUSTAR), Marc Schoenauer (TAU), Olivier Teytaud (TAU)</dc:creator>
    </item>
    <item>
      <title>CoEvo: Continual Evolution of Symbolic Solutions Using Large Language Models</title>
      <link>https://arxiv.org/abs/2412.18890</link>
      <description>arXiv:2412.18890v2 Announce Type: replace-cross 
Abstract: The discovery of symbolic solutions -- mathematical expressions, logical rules, and algorithmic structures -- is fundamental to advancing scientific and engineering progress.
  However, traditional methods often struggle with search efficiency and fail to integrate knowledge effectively.
  While recent large language model-based (LLM-based) approaches have demonstrated improvements in search efficiency, they lack the ability to continually refine and expand upon discovered solutions and their underlying knowledge, limiting their potential for open-ended innovation.
  To address these limitations, we introduce CoEvo, a novel framework that leverages large language models within an evolutionary search methodology to continually generate and refine symbolic solutions. CoEvo integrates a dynamic knowledge library, enabling open-ended innovation of solutions through effective knowledge management. Additionally, CoEvo leverages multiple representations of solutions -- including natural language, mathematical expressions, and code -- to further enhance search efficiency.
  By combining the reasoning capabilities of LLMs with the exploratory power of evolutionary algorithms, CoEvo significantly improves the efficiency and scope of symbolic discovery.
  Our experimental results demonstrate that this method not only enhances the efficiency of searching for symbolic solutions but also supports the ongoing discovery process, akin to human scientific endeavors. This study represents a first effort in conceptualizing the search for symbolic solutions as a lifelong, iterative
  process, marking a significant step towards harnessing LLMs in the perpetual pursuit of scientific and engineering breakthroughs.
  Our code is available at https://github.com/pgg3/CoEvo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18890v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ping Guo, Qingfu Zhang, Xi Lin</dc:creator>
    </item>
    <item>
      <title>When Federated Learning Meets Quantum Computing: Survey and Research Opportunities</title>
      <link>https://arxiv.org/abs/2504.08814</link>
      <description>arXiv:2504.08814v4 Announce Type: replace-cross 
Abstract: Quantum Federated Learning (QFL) is an emerging field that harnesses advances in Quantum Computing (QC) to improve the scalability and efficiency of decentralized Federated Learning (FL) models. This paper provides a systematic and comprehensive survey of the emerging problems and solutions when FL meets QC, from research protocol to a novel taxonomy, particularly focusing on both quantum and federated limitations, such as their architectures, Noisy Intermediate Scale Quantum (NISQ) devices, and privacy preservation, so on. With the introduction of two novel metrics, qubit utilization efficiency and quantum model training strategy, we present a thorough analysis of the current status of the QFL research. This work explores key developments and integration strategies, along with the impact of QC on FL, keeping a sharp focus on hybrid quantum-classical approaches. The paper offers an in-depth understanding of how the strengths of QC, such as gradient hiding, state entanglement, quantum key distribution, quantum security, and quantum-enhanced differential privacy, have been integrated into FL to ensure the privacy of participants in an enhanced, fast, and secure framework. Finally, this study proposes potential future directions to address the identified research gaps and challenges, aiming to inspire faster and more secure QFL models for practical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08814v4</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Mon, 17 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aakar Mathur, Ashish Gupta, Sajal K. Das</dc:creator>
    </item>
  </channel>
</rss>
