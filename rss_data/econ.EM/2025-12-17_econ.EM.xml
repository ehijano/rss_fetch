<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 18 Dec 2025 02:36:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Fast Test Inversion for Resampling Methods</title>
      <link>https://arxiv.org/abs/2512.14024</link>
      <description>arXiv:2512.14024v1 Announce Type: new 
Abstract: Randomization-based inference commonly relies on grid search methods to construct confidence intervals by inverting hypothesis tests over a range of parameter values. While straightforward, this approach is computationally intensive and can yield conservative intervals due to discretization. We propose a novel method that exploits the algebraic structure of a broad class of test statistics--including those with variance estimators dependent on the null hypothesis--to produce exact confidence intervals efficiently. By expressing randomization statistics as rational functions of the parameter of interest, we analytically identify critical values where the test statistic's rank changes relative to the randomization distribution. This characterization allows us to derive the exact p-value curve and construct precise confidence intervals without exhaustive computation. For cases where the parameter of interest is a vector and a confidence region is needed, our method extends by calculating and storing the coefficients of the polynomial functions involved. This approach enables us to compute approximate p-value functions and confidence regions more efficiently than traditional grid search methods, as we avoid recalculating test statistics from scratch for each parameter value. We illustrate our method using tests from Pouliot (2024) and extend it to other randomization tests, such as those developed by DiCiccio and Romano (2017) and D'Haultf{\oe}uille and Tuvaandorj (2024). Our approach significantly reduces computational burden and overcomes the limitations of traditional grid search methods, providing a practical and efficient solution for confidence interval and region construction in randomization-based inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14024v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ian Xu</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Effects of Endogenous Treatments with Interference and Spillovers in a Large Network</title>
      <link>https://arxiv.org/abs/2512.14515</link>
      <description>arXiv:2512.14515v1 Announce Type: new 
Abstract: This paper studies the identification and estimation of heterogeneous effects of an endogenous treatment under interference and spillovers in a large single-network setting. We model endogenous treatment selection as an equilibrium outcome that explicitly accounts for spillovers and derive conditions guaranteeing the existence and uniqueness of this equilibrium. We then identify heterogeneous marginal exposure effects (MEEs), which may vary with both the treatment status of neighboring nodes and unobserved heterogeneity. We develop estimation strategies and establish their large-sample properties. Equipped with these tools, we analyze the heterogeneous effects of import competition on U.S. local labor markets in the presence of interference and spillovers. We find negative MEEs, consistent with the existing literature. However, these effects are amplified by spillovers in the presence of treated neighbors and among localities that tend to select into lower levels of import competition. These additional empirical findings are novel and would not be credibly obtainable without the econometric framework proposed in this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14515v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Lin Chen, Yuya Sasaki</dc:creator>
    </item>
    <item>
      <title>Estimating Program Participation with Partial Validation</title>
      <link>https://arxiv.org/abs/2512.14616</link>
      <description>arXiv:2512.14616v1 Announce Type: new 
Abstract: This paper considers the estimation of binary choice models when survey responses are possibly misclassified but one of the response category can be validated. Partial validation may occur when survey questions about participation include follow-up questions on that particular response category. In this case, we show that the initial two-sided misclassification problem can be transformed into a one-sided one, based on the partially validated responses. Using the updated responses naively for estimation does not solve or mitigate the misclassification bias, and we derive the ensuing asymptotic bias under general conditions. We then show how the partially validated responses can be used to construct a model for participation and propose consistent and asymptotically normal estimators that overcome misclassification error. Monte Carlo simulations are provided to demonstrate the finite sample performance of the proposed and selected existing methods. We provide an empirical illustration on the determinants of health insurance coverage in Ghana. We discuss implications for the design of survey questionnaires that allow researchers to overcome misclassification biases without recourse to relatively costly and often imperfect validation data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14616v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Augustine Denteh, Pierre E. Nguimkeu</dc:creator>
    </item>
    <item>
      <title>Inflation Attitudes of Large Language Models</title>
      <link>https://arxiv.org/abs/2512.14306</link>
      <description>arXiv:2512.14306v1 Announce Type: cross 
Abstract: This paper investigates the ability of Large Language Models (LLMs), specifically GPT-3.5-turbo (GPT), to form inflation perceptions and expectations based on macroeconomic price signals. We compare the LLM's output to household survey data and official statistics, mimicking the information set and demographic characteristics of the Bank of England's Inflation Attitudes Survey (IAS). Our quasi-experimental design exploits the timing of GPT's training cut-off in September 2021 which means it has no knowledge of the subsequent UK inflation surge. We find that GPT tracks aggregate survey projections and official statistics at short horizons. At a disaggregated level, GPT replicates key empirical regularities of households' inflation perceptions, particularly for income, housing tenure, and social class. A novel Shapley value decomposition of LLM outputs suited for the synthetic survey setting provides well-defined insights into the drivers of model outputs linked to prompt content. We find that GPT demonstrates a heightened sensitivity to food inflation information similar to that of human respondents. However, we also find that it lacks a consistent model of consumer price inflation. More generally, our approach could be used to evaluate the behaviour of LLMs for use in the social sciences, to compare different models, or to assist in survey design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14306v1</guid>
      <category>cs.CL</category>
      <category>econ.EM</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikoleta Anesti, Edward Hill, Andreas Joseph</dc:creator>
    </item>
    <item>
      <title>Nowcasting using regression on signatures</title>
      <link>https://arxiv.org/abs/2305.10256</link>
      <description>arXiv:2305.10256v2 Announce Type: replace 
Abstract: We introduce a new method of nowcasting using regression on path signatures. Path signatures capture the geometric properties of sequential data. Because signatures embed observations in continuous time, they naturally handle mixed frequencies and missing data. We prove theoretically, and with simulations, that regression on signatures subsumes the linear Kalman filter and retains desirable consistency properties. Nowcasting with signatures is more robust to disruptions in data series than previous methods, making it useful in stressed times (for example, during COVID-19). This approach is performant in nowcasting US GDP growth, and in nowcasting UK unemployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.10256v2</guid>
      <category>econ.EM</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel N. Cohen, Giulia Mantoan, Lars Nesheim, \'Aureo de Paula, Arthur Turrell, Lingyi Yang</dc:creator>
    </item>
    <item>
      <title>Semiparametric inference for impulse response functions using double/debiased machine learning</title>
      <link>https://arxiv.org/abs/2411.10009</link>
      <description>arXiv:2411.10009v2 Announce Type: replace 
Abstract: We introduce a double/debiased machine learning estimator for the impulse response function in settings where a time series of interest is subjected to multiple discrete treatments, assigned over time, which can have a causal effect on future outcomes. The proposed estimator can rely on fully nonparametric relations between treatment and outcome variables, opening up the possibility to use flexible machine learning approaches to estimate impulse response functions. To this end, we extend the theory of double machine learning from an i.i.d. to a time series setting and show that the proposed estimator is consistent and asymptotically normally distributed at the parametric rate, allowing for semiparametric inference for dynamic effects in a time series setting. The properties of the estimator are validated numerically in finite samples by applying it to learn the impulse response function in the presence of serial dependence in both the confounder and observation innovation processes. We also illustrate the methodology empirically by applying it to the estimation of the effects of macroeconomic shocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10009v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniele Ballinari, Alexander Wehrli</dc:creator>
    </item>
    <item>
      <title>The Exact Variance of the Average Treatment Effect Estimator in Cluster Randomized Controlled Trials</title>
      <link>https://arxiv.org/abs/2511.05801</link>
      <description>arXiv:2511.05801v2 Announce Type: replace 
Abstract: In cluster randomized controlled trials (CRCT) with a finite populations, the exact design-based variance of the Horvitz-Thompson (HT) estimator for the average treatment effect (ATE) depends on the joint distribution of unobserved cluster-aggregated potential outcomes and is therefore not point-identifiable. We study a common two-stage sampling design-random sampling of clusters followed by sampling units within sampled clusters-with treatment assigned at the cluster level. First, we derive the exact (infeasible) design-based variance of the HT ATE estimator that accounts jointly for cluster- and unit-level sampling as well as random assignment. Second, extending Aronow et al (2014), we provide a sharp, attanable upper bound on that variance and propose a consistent estimator of the bound using only observed outcomes and known sampling/assignment probabilities. In simulations and an empirical application, confidence intervals based on our bound are valid and typically narrower than those based on cluster standard errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05801v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yue Fang, Geert Ridder</dc:creator>
    </item>
    <item>
      <title>Semiparametric Estimation of Fractional Integration: An Evaluation of Local Whittle Methods</title>
      <link>https://arxiv.org/abs/2511.15689</link>
      <description>arXiv:2511.15689v2 Announce Type: replace 
Abstract: Fractionally integrated time series, exhibiting long memory with slowly decaying autocorrelations, are frequently encountered in economics, finance, and related fields. Since the seminal work of Robinson (1995), a variety of semiparametric local Whittle estimators have been proposed for estimating the memory parameter $d$. However, applied researchers must determine which estimator to use, and under what conditions. This paper compares several local Whittle estimators, first replicating key findings from the literature and then extending these with new Monte Carlo experiments and in-depth empirical studies. We compare how each estimator performs in the presence of short-run dynamics, unknown means, time trends, and structural breaks, and discuss how to interpret potentially conflicting results with real datasets. Based on the findings, we offer guidance to practitioners on estimator choice, bandwidth selection, and the potential diagnostic value of disagreements between estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15689v2</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jason R. Blevins</dc:creator>
    </item>
    <item>
      <title>Spatial-Network Treatment Effects: A Continuous Functional Approach</title>
      <link>https://arxiv.org/abs/2512.12653</link>
      <description>arXiv:2512.12653v3 Announce Type: replace 
Abstract: This paper develops a continuous functional framework for treatment effects that propagate through geographic space and economic networks. We derive a master equation governing propagation from three economic foundations -- heterogeneous agent aggregation, market equilibrium, and cost minimization -- establishing that the framework rests on fundamental principles rather than ad hoc specifications. A key result shows that the spatial-network interaction coefficient equals the mutual information between geographic and market coordinates. The Feynman-Kac representation decomposes effects into inherited and accumulated components along stochastic paths representing economic linkages. The framework nests the no-spillover case as a testable restriction. Monte Carlo simulations demonstrate that conventional estimators -- two-way fixed effects, difference-in-differences, and generalized propensity score -- exhibit 25-38% bias and severe undercoverage when spillovers exist, while our estimator maintains correct inference regardless of whether spillovers are present. Applying the framework to U.S. minimum wage policy, we reject the no-spillover null and find total effects at state borders four times larger than direct effects -- conventional methods capture only one-quarter of policy impact. Structural estimates reveal spatial diffusion consistent with commuting-distance labor mobility, network diffusion consistent with quarterly supply chain adjustment, and significant spatial-network interaction reflecting geographic clustering of industries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12653v3</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>stat.ME</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Finite-Sample NonParametric Bounds with an Application to the Causal Effect of Workforce Gender Diversity on Firm Performance</title>
      <link>https://arxiv.org/abs/2509.01622</link>
      <description>arXiv:2509.01622v2 Announce Type: replace-cross 
Abstract: Classical Manski bounds identify average treatment effects under minimal assumptions but, in finite samples, they rely on latent outcome expectations being bounded by the sample's own extrema or known population bounds, an assumption often violated in firm-level data with heavy-tailed outcomes. We develop a finite-sample, concentration-driven confidence band (concATE) that replaces this requirement with a Dvoretzky-Kiefer-Wolfowitz tail bound, combines it with delta-method variance, and allocates size via a Bonferroni correction. The band extends to a group-sequential design that controls the family-wise error rate when the first "significant" diversity threshold is data-chosen. Applied to data on 901 listed firms (2015 Q2-2022 Q1), concATE shows that senior-level gender diversity has a significant positive effect on firm value (Tobin's Q) only after crossing substantial representation thresholds: in Growth &amp; Innovation sectors the effect becomes statistically significant at the 5% level once women hold roughly 55% of senior leadership roles, whereas in Defensive sectors a significant impact appears only once female leadership reaches about 60%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01622v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 17 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grace Lordan, Kaveh Salehzadeh Nobari</dc:creator>
    </item>
  </channel>
</rss>
