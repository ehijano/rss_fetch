<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Jun 2025 02:20:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Leave No One Undermined: Policy Targeting with Regret Aversion</title>
      <link>https://arxiv.org/abs/2506.16430</link>
      <description>arXiv:2506.16430v1 Announce Type: new 
Abstract: While the importance of personalized policymaking is widely recognized, fully personalized implementation remains rare in practice. We study the problem of policy targeting for a regret-averse planner when training data gives a rich set of observable characteristics while the assignment rules can only depend on its subset. Grounded in decision theory, our regret-averse criterion reflects a planner's concern about regret inequality across the population, which generally leads to a fractional optimal rule due to treatment effect heterogeneity beyond the average treatment effects conditional on the subset characteristics. We propose a debiased empirical risk minimization approach to learn the optimal rule from data. Viewing our debiased criterion as a weighted least squares problem, we establish new upper and lower bounds for the excess risk, indicating a convergence rate of 1/n and asymptotic efficiency in certain cases. We apply our approach to the National JTPA Study and the International Stroke Trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16430v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Toru Kitagawa, Sokbae Lee, Chen Qiu</dc:creator>
    </item>
    <item>
      <title>Assessing Omitted Variable Bias when the Controls are Endogenous</title>
      <link>https://arxiv.org/abs/2206.02303</link>
      <description>arXiv:2206.02303v5 Announce Type: replace 
Abstract: Omitted variables are one of the most important threats to the identification of causal effects. Several widely used methods assess the impact of omitted variables on empirical conclusions by comparing measures of selection on observables with measures of selection on unobservables. The recent literature has discussed various limitations of these existing methods, however. This includes a companion paper of ours which explains issues that arise when the omitted variables are endogenous, meaning that they are correlated with the included controls. In the present paper, we develop a new approach to sensitivity analysis that avoids those limitations, while still allowing researchers to calibrate sensitivity parameters by comparing the magnitude of selection on observables with the magnitude of selection on unobservables as in previous methods. We illustrate our results in an empirical study of the effect of historical American frontier life on modern cultural beliefs. Finally, we implement these methods in the companion Stata module regsensitivity for easy use in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.02303v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Diegert, Matthew A. Masten, Alexandre Poirier</dc:creator>
    </item>
    <item>
      <title>Measurement Error and Counterfactuals in Quantitative Trade and Spatial Models</title>
      <link>https://arxiv.org/abs/2311.14032</link>
      <description>arXiv:2311.14032v4 Announce Type: replace 
Abstract: Counterfactuals in quantitative trade and spatial models are functions of the current state of the world and the model parameters. Common practice treats the current state of the world as perfectly observed, but there is good reason to believe that it is measured with error. This paper provides tools for quantifying uncertainty about counterfactuals when the current state of the world is measured with error. I recommend an empirical Bayes approach to uncertainty quantification, and show that it is both practical and theoretically justified. I apply the proposed method to the settings in Adao, Costinot, and Donaldson (2017) and Allen and Arkolakis (2022) and find non-trivial uncertainty about counterfactuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14032v4</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bas Sanders</dc:creator>
    </item>
    <item>
      <title>When Can We Use Two-Way Fixed-Effects (TWFE): A Comparison of TWFE and Novel Dynamic Difference-in-Differences Estimators</title>
      <link>https://arxiv.org/abs/2402.09928</link>
      <description>arXiv:2402.09928v3 Announce Type: replace 
Abstract: The conventional Two-Way Fixed-Effects (TWFE) estimator has come under scrutiny lately. Recent literature has revealed potential shortcomings of TWFE when the treatment effects are heterogeneous. Scholars have developed new advanced dynamic Difference-in-Differences (DiD) estimators to tackle these potential shortcomings. However, confusion remains in applied research as to when the conventional TWFE is biased and what issues the novel estimators can and cannot address. In this study, we first provide an intuitive explanation of the problems of TWFE and elucidate the key features of the novel alternative DiD estimators. We then systematically demonstrate the conditions under which the conventional TWFE is inconsistent. We employ Monte Carlo simulations to assess the performance of dynamic DiD estimators under violations of key assumptions, which likely happens in applied cases. While the new dynamic DiD estimators offer notable advantages in capturing heterogeneous treatment effects, we show that the conventional TWFE performs generally well if the model specifies an event-time function. All estimators are equally sensitive to violations of the parallel trends assumption, anticipation effects or violations of time-varying exogeneity. Despite their advantages, the new dynamic DiD estimators tackle a very specific problem and they do not serve as a universal remedy for violations of the most critical assumptions. We finally derive, based on our simulations, recommendations for how and when to use TWFE and the new DiD estimators in applied research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09928v3</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tobias R\"uttenauer, Ozan Aksoy</dc:creator>
    </item>
    <item>
      <title>Statistical Inference of Optimal Allocations I: Regularities and their Implications</title>
      <link>https://arxiv.org/abs/2403.18248</link>
      <description>arXiv:2403.18248v3 Announce Type: replace 
Abstract: In this paper, we develop a functional differentiability approach for solving statistical optimal allocation problems. We derive Hadamard differentiability of the value functions through analyzing the properties of the sorting operator using tools from geometric measure theory. Building on our Hadamard differentiability results, we apply the functional delta method to obtain the asymptotic properties of the value function process for the binary constrained optimal allocation problem and the plug-in ROC curve estimator. Moreover, the convexity of the optimal allocation value functions facilitates demonstrating the degeneracy of first order derivatives with respect to the policy. We then present a double / debiased estimator for the value functions. Importantly, the conditions that validate Hadamard differentiability justify the margin assumption from the statistical classification literature for the fast convergence rate of plug-in methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18248v3</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai Feng, Han Hong, Denis Nekipelov</dc:creator>
    </item>
    <item>
      <title>Sequential Synthetic Difference in Differences</title>
      <link>https://arxiv.org/abs/2404.00164</link>
      <description>arXiv:2404.00164v2 Announce Type: replace 
Abstract: We propose the Sequential Synthetic Difference-in-Differences (Sequential SDiD) estimator for event studies with staggered treatment adoption, particularly when the parallel trends assumption fails. The method uses an iterative imputation procedure on aggregated data, where estimates for early-adopting cohorts are used to construct counterfactuals for later ones. We prove the estimator is asymptotically equivalent to an infeasible oracle OLS estimator within a linear model with interactive fixed effects. This key theoretical result provides a foundation for standard inference by establishing asymptotic normality and clarifying the estimator's efficiency. By offering a robust and transparent method with formal statistical guarantees, Sequential SDiD is a powerful alternative to conventional difference-in-differences strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00164v2</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dmitry Arkhangelsky, Aleksei Samkov</dc:creator>
    </item>
    <item>
      <title>Forecasting with panel data: Estimation uncertainty versus parameter heterogeneity</title>
      <link>https://arxiv.org/abs/2404.11198</link>
      <description>arXiv:2404.11198v2 Announce Type: replace 
Abstract: We provide a comprehensive examination of the predictive performance of panel forecasting methods based on individual, pooling, fixed effects, and empirical Bayes estimation, and propose optimal weights for forecast combination schemes. We consider linear panel data models, allowing for weakly exogenous regressors and correlated heterogeneity. We quantify the gains from exploiting panel data and demonstrate how forecasting performance depends on the degree of parameter heterogeneity, whether such heterogeneity is correlated with the regressors, the goodness of fit of the model, and the dimensions of the data. Monte Carlo simulations and empirical applications to house prices and CPI inflation show that empirical Bayes and forecast combination methods perform best overall and rarely produce the least accurate forecasts for individual series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11198v2</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. Hashem Pesaran, Andreas Pick, Allan Timmermann</dc:creator>
    </item>
    <item>
      <title>An Axiomatic Approach to Comparing Sensitivity Parameters</title>
      <link>https://arxiv.org/abs/2504.21106</link>
      <description>arXiv:2504.21106v2 Announce Type: replace 
Abstract: Many methods are available for assessing the importance of omitted variables. These methods typically make different, non-falsifiable assumptions. Hence the data alone cannot tell us which method is most appropriate. Since it is unreasonable to expect results to be robust against all possible robustness checks, researchers often use methods deemed "interpretable", a subjective criterion with no formal definition. In contrast, we develop the first formal, axiomatic framework for comparing and selecting among these methods. Our framework is analogous to the standard approach for comparing estimators based on their sampling distributions. We propose that sensitivity parameters be selected based on their covariate sampling distributions, a design distribution of parameter values induced by an assumption on how covariates are assigned to be observed or unobserved. Using this idea, we define a new concept of parameter consistency, and argue that a reasonable sensitivity parameter should be consistent. We prove that the literature's most popular approach is inconsistent, while several alternatives are consistent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21106v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Diegert, Matthew A. Masten, Alexandre Poirier</dc:creator>
    </item>
    <item>
      <title>Adaptive Experimental Design for Policy Learning</title>
      <link>https://arxiv.org/abs/2401.03756</link>
      <description>arXiv:2401.03756v4 Announce Type: replace-cross 
Abstract: This study investigates the contextual best arm identification (BAI) problem, aiming to design an adaptive experiment to identify the best treatment arm conditioned on contextual information (covariates). We consider a decision-maker who assigns treatment arms to experimental units during an experiment and recommends the estimated best treatment arm based on the contexts at the end of the experiment. The decision-maker uses a policy for recommendations, which is a function that provides the estimated best treatment arm given the contexts. In our evaluation, we focus on the worst-case expected regret, a relative measure between the expected outcomes of an optimal policy and our proposed policy. We derive a lower bound for the expected simple regret and then propose a strategy called Adaptive Sampling-Policy Learning (PLAS). We prove that this strategy is minimax rate-optimal in the sense that its leading factor in the regret upper bound matches the lower bound as the number of experimental units increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03756v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato, Kyohei Okumura, Takuya Ishihara, Toru Kitagawa</dc:creator>
    </item>
    <item>
      <title>Assumption-robust Causal Inference</title>
      <link>https://arxiv.org/abs/2505.08729</link>
      <description>arXiv:2505.08729v2 Announce Type: replace-cross 
Abstract: In observational causal inference, it is common to encounter multiple adjustment sets that appear equally plausible. It is often untestable which of these adjustment sets are valid to adjust for (i.e., satisfies ignorability). This discrepancy can pose practical challenges as it is typically unclear how to reconcile multiple, possibly conflicting estimates of the average treatment effect (ATE). A naive approach is to report the whole range (convex hull of the union) of the resulting confidence intervals. However, the width of this interval might not shrink to zero in large samples and can be unnecessarily wide in real applications. To address this issue, we propose a summary procedure that generates a single estimate, one confidence interval, and identifies a set of units for which the causal effect estimate remains valid, provided at least one adjustment set is valid. The width of our proposed confidence interval shrinks to zero with sample size at $n^{-1/2}$ rate, unlike the original range which is of constant order. Thus, our assumption-robust approach enables reliable causal inference on the ATE even in scenarios where most of the adjustment sets are invalid. Admittedly, this robustness comes at a cost:~our inferential guarantees apply to a target population close to, but different from, the one originally intended. We use synthetic and real-data examples to demonstrate that our proposed procedure provides substantially tighter confidence intervals for the ATE as compared to the whole range.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08729v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Ghosh, Dominik Rothenh\"ausler</dc:creator>
    </item>
  </channel>
</rss>
