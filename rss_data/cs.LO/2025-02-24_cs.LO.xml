<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.LO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.LO</link>
    <description>cs.LO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.LO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Feb 2025 04:12:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Breaking Symmetries in Quantified Graph Search: A Comparative Study</title>
      <link>https://arxiv.org/abs/2502.15078</link>
      <description>arXiv:2502.15078v1 Announce Type: new 
Abstract: Graph generation and enumeration problems often require handling equivalent graphs -- those that differ only in vertex labeling. We study how to extend SAT Modulo Symmetries (SMS), a framework for eliminating such redundant graphs, to handle more complex constraints. While SMS was originally designed for constraints in propositional logic (in NP), we now extend it to handle quantified Boolean formulas (QBF), allowing for more expressive specifications like non-3-colorability (a coNP-complete property). We develop two approaches: a static QBF encoding and a dynamic method integrating SMS into QBF solvers. Our analysis reveals that while specialized approaches can be faster, QBF-based methods offer easier implementation and formal verification capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15078v1</guid>
      <category>cs.LO</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mikol\'a\v{s} Janota, Markus Kirchweger, Tom\'a\v{s} Peitl, Stefan Szeider</dc:creator>
    </item>
    <item>
      <title>Automata for Enriched Trees and Applications</title>
      <link>https://arxiv.org/abs/2502.15316</link>
      <description>arXiv:2502.15316v1 Announce Type: new 
Abstract: We study trees where each successor set is equipped with some additional structure. We introduce a family of automaton models for such trees and prove their equivalence to certain fixed-point logics. As a consequence we obtain characterisations of various variants of monadic second-order logic in terms of automata and fixed-point logics. Finally, we use our machinery to give a simplified proof of the Theorem of Muchnik and we derive several variants of this theorem for other logics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15316v1</guid>
      <category>cs.LO</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Achim Blumensath</dc:creator>
    </item>
    <item>
      <title>A modular risk concept for complex systems</title>
      <link>https://arxiv.org/abs/2502.15482</link>
      <description>arXiv:2502.15482v1 Announce Type: new 
Abstract: Our ways of managing risk have in the past been adapted to changes in technology and society. Amidst the ongoing digital transformation, the ur-gency of adapting risk management to changing needs seems higher than ever. This paper starts with a brief historic overview of the development of risk management in the past. The paper motivates the views that for com-plex systems, risk should be controlled by enforcing constrains in a modular way at different system levels, that the constraints can be expressed as assur-ance contracts and that acceptable risk mitigation can be demonstrated in as-surance case modules. Based on extensive industry experience of the authors, a major contribution is to explain how already existing methodologies have been combined to cre-ate a concept for modular risk assessment. Examples from assurance of au-tonomous sea navigation and autonomous driving are used to illustrate the concept. Beyond the existing methodologies this paper generalizes risk con-straints to assurance contracts as an enabler of modular risk assessment spanning all relevant system levels and stakeholder perspectives while main-taining the dependencies between the system parts and accounting for emer-gent system behavior. Furthermore, the use of safety integrity levels (SIL) and similar concepts for assigning assurance rigor have been avoided in favor of direct assessment of assurance case argument rigor, because technology and applications change too fast to justify using past experience as evidence of validity of such prescriptive schemes. This paper aims to help practitioners making efficient and timely risk-informed decisions about complex integrated systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15482v1</guid>
      <category>cs.LO</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dag McGeorge, Jon Arne Glomsrud</dc:creator>
    </item>
    <item>
      <title>Categorical algebra of conditional probability</title>
      <link>https://arxiv.org/abs/2502.14941</link>
      <description>arXiv:2502.14941v1 Announce Type: cross 
Abstract: In the field of categorical probability, one uses concepts and techniques from category theory, such as monads and monoidal categories, to study the structures of probability and statistics. In this paper, we connect some ideas from categorical algebra, namely weakly cartesian functors and natural transformations, to the idea of conditioning in probability theory, using Markov categories and probability monads. First of all, we show that under some conditions, the monad associated to a Markov category with conditionals has a weakly cartesian functor and weakly cartesian multiplication (a condition known as Beck-Chevalley, or BC). In particular, we show that this is the case for the Giry monad on standard Borel spaces. We then connect this theory to existing results on statistical experiments. We show that for deterministic statistical experiments, the so-called standard measure construction (which can be seen as a generalization of the "hyper-normalizations" introduced by Jacobs) satisfies a universal property, allowing an equivalent definition which does not rely on the existence of conditionals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14941v1</guid>
      <category>math.CT</category>
      <category>cs.LO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mika Bohinen, Paolo Perrone</dc:creator>
    </item>
    <item>
      <title>What does it take to certify conversion?</title>
      <link>https://arxiv.org/abs/2502.15500</link>
      <description>arXiv:2502.15500v1 Announce Type: cross 
Abstract: We report on a detailed exploration of the properties of conversion (definitional equality) in dependent type theory, with the goal of certifying decision procedures for it. While in that context the property of normalisation has attracted the most light, we instead emphasize the importance of injectivity properties, showing that they alone are both crucial and sufficient to certify most desirable properties of conversion checkers.
  We also explore the certification of a fully untyped conversion checker, with respect to a typed specification, and show that the story is mostly unchanged, although the exact injectivity properties needed are subtly different.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15500v1</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meven Lennon-Bertrand</dc:creator>
    </item>
    <item>
      <title>Loop unrolling: formal definition and application to testing</title>
      <link>https://arxiv.org/abs/2502.15535</link>
      <description>arXiv:2502.15535v1 Announce Type: cross 
Abstract: Testing processes usually aim at high coverage, but loops severely limit coverage ambitions since the number of iterations is generally not predictable. Most testing teams address this issue by adopting the extreme solution of limiting themselves to branch coverage, which only considers loop executions that iterate the body either once or not at all. This approach misses any bug that only arises after two or more iterations.
  To achieve more meaningful coverage, testing strategies may unroll loops, in the sense of using executions that iterate loops up to n times for some n greater than one, chosen pragmatically in consideration of the available computational power.
  While loop unrolling is a standard part of compiler optimization techniques, its use in testing is far less common. Part of the reason is that the concept, while seemingly intuitive, lacks a generally accepted and precise specification. The present article provides a formal definition and a set of formal properties of unrolling. All the properties have mechanically been proved correct (through the Isabelle proof assistant).
  Using this definition as the conceptual basis, we have applied an unrolling strategy to an existing automated testing framework and report the results: how many more bugs get detected once we unroll loops more than once?
  These results provide a first assessment of whether unrolling should become a standard part of test generation and test coverage measurement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15535v1</guid>
      <category>cs.SE</category>
      <category>cs.ET</category>
      <category>cs.LO</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Huang, Bertrand Meyer, Reto Weber</dc:creator>
    </item>
    <item>
      <title>Infinitary Refinement Types for Temporal Properties in Scott Domains</title>
      <link>https://arxiv.org/abs/2502.11917</link>
      <description>arXiv:2502.11917v2 Announce Type: replace 
Abstract: We discuss an infinitary refinement type system for input-output temporal specifications of functions that handle infinite objects like streams or infinite trees. Our system is based on a reformulation of Bonsangue and Kok's infinitary extension of Abramsky's Domain Theory in Logical Form to saturated properties. We show that in an interesting range of cases, our system is complete without the need of an infinitary rule introduced by Bonsangue and Kok to reflect the well-filteredness of Scott domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11917v2</guid>
      <category>cs.LO</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Colin Riba, Alexandre Kejikian</dc:creator>
    </item>
    <item>
      <title>Large Language Models for Interpretable Mental Health Diagnosis</title>
      <link>https://arxiv.org/abs/2501.07653</link>
      <description>arXiv:2501.07653v2 Announce Type: replace-cross 
Abstract: We propose a clinical decision support system (CDSS) for mental health diagnosis that combines the strengths of large language models (LLMs) and constraint logic programming (CLP). Having a CDSS is important because of the high complexity of diagnostic manuals used by mental health professionals and the danger of diagnostic errors. Our CDSS is a software tool that uses an LLM to translate diagnostic manuals to a logic program and solves the program using an off-the-shelf CLP engine to query a patient's diagnosis based on the encoded rules and provided data. By giving domain experts the opportunity to inspect the LLM-generated logic program, and making modifications when needed, our CDSS ensures that the diagnosis is not only accurate but also interpretable. We experimentally compare it with two baseline approaches of using LLMs: diagnosing patients using the LLM-only approach, and using the LLM-generated logic program but without expert inspection. The results show that, while LLMs are extremely useful in generating candidate logic programs, these programs still require expert inspection and modification to guarantee faithfulness to the official diagnostic manuals. Additionally, ethical concerns arise from the direct use of patient data in LLMs, underscoring the need for a safer hybrid approach like our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07653v2</guid>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <pubDate>Mon, 24 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Hyeongseok Kim, Chao Wang</dc:creator>
    </item>
  </channel>
</rss>
