<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 Aug 2024 04:00:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 02 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A new unit-bimodal distribution based on correlated Birnbaum-Saunders random variables</title>
      <link>https://arxiv.org/abs/2408.00100</link>
      <description>arXiv:2408.00100v1 Announce Type: new 
Abstract: In this paper, we propose a new distribution over the unit interval which can be characterized as a ratio of the type Z=Y/(X+Y) where X and Y are two correlated Birnbaum-Saunders random variables. The stress-strength probability between X and Y is calculated explicitly when the respective scale parameters are equal. Two applications of the ratio distribution are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00100v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo, Felipe Quintino, Peter Z\"ornig</dc:creator>
    </item>
    <item>
      <title>Fast variational Bayesian inference for correlated survival data: an application to invasive mechanical ventilation duration analysis</title>
      <link>https://arxiv.org/abs/2408.00177</link>
      <description>arXiv:2408.00177v1 Announce Type: new 
Abstract: Correlated survival data are prevalent in various clinical settings and have been extensively discussed in literature. One of the most common types of correlated survival data is clustered survival data, where the survival times from individuals in a cluster are associated. Our study is motivated by invasive mechanical ventilation data from different intensive care units (ICUs) in Ontario, Canada, forming multiple clusters. The survival times from patients within the same ICU cluster are correlated. To address this association, we introduce a shared frailty log-logistic accelerated failure time model that accounts for intra-cluster correlation through a cluster-specific random intercept. We present a novel, fast variational Bayes (VB) algorithm for parameter inference and evaluate its performance using simulation studies varying the number of clusters and their sizes. We further compare the performance of our proposed VB algorithm with the h-likelihood method and a Markov Chain Monte Carlo (MCMC) algorithm. The proposed algorithm delivers satisfactory results and demonstrates computational efficiency over the MCMC algorithm. We apply our method to the ICU ventilation data from Ontario to investigate the ICU site random effect on ventilation duration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00177v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengqian Xian, Camila P. E. de Souza, Wenqing He, Felipe F. Rodrigues, Renfang Tian</dc:creator>
    </item>
    <item>
      <title>Operator on Operator Regression in Quantum Probability</title>
      <link>https://arxiv.org/abs/2408.00289</link>
      <description>arXiv:2408.00289v1 Announce Type: new 
Abstract: This article introduces operator on operator regression in quantum probability. Here in the regression model, the response and the independent variables are certain operator valued observables, and they are linearly associated with unknown scalar coefficient (denoted by $\beta$), and the error is a random operator. In the course of this study, we propose a quantum version of a class of estimators (denoted by $M$ estimator) of $\beta$, and the large sample behaviour of those quantum version of the estimators are derived, given the fact that the true model is also linear and the samples are observed eigenvalue pairs of the operator valued observables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00289v1</guid>
      <category>stat.ME</category>
      <category>quant-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suprio Bhar, Subhra Sankar Dhar, Soumalya Joardar</dc:creator>
    </item>
    <item>
      <title>Facilitating heterogeneous effect estimation via statistically efficient categorical modifiers</title>
      <link>https://arxiv.org/abs/2408.00618</link>
      <description>arXiv:2408.00618v1 Announce Type: new 
Abstract: Categorical covariates such as race, sex, or group are ubiquitous in regression analysis. While main-only (or ANCOVA) linear models are predominant, cat-modified linear models that include categorical-continuous or categorical-categorical interactions are increasingly important and allow heterogeneous, group-specific effects. However, with standard approaches, the addition of cat-modifiers fundamentally alters the estimates and interpretations of the main effects, often inflates their standard errors, and introduces significant concerns about group (e.g., racial) biases. We advocate an alternative parametrization and estimation scheme using abundance-based constraints (ABCs). ABCs induce a model parametrization that is both interpretable and equitable. Crucially, we show that with ABCs, the addition of cat-modifiers 1) leaves main effect estimates unchanged and 2) enhances their statistical power, under reasonable conditions. Thus, analysts can, and arguably should include cat-modifiers in linear regression models to discover potential heterogeneous effects--without compromising estimation, inference, and interpretability for the main effects. Using simulated data, we verify these invariance properties for estimation and inference and showcase the capabilities of ABCs to increase statistical power. We apply these tools to study demographic heterogeneities among the effects of social and environmental factors on STEM educational outcomes for children in North Carolina. An R package lmabc is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00618v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel R. Kowal</dc:creator>
    </item>
    <item>
      <title>A Dirichlet stochastic block model for composition-weighted networks</title>
      <link>https://arxiv.org/abs/2408.00651</link>
      <description>arXiv:2408.00651v1 Announce Type: new 
Abstract: Network data are observed in various applications where the individual entities of the system interact with or are connected to each other, and often these interactions are defined by their associated strength or importance. Clustering is a common task in network analysis that involves finding groups of nodes displaying similarities in the way they interact with the rest of the network. However, most clustering methods use the strengths of connections between entities in their original form, ignoring the possible differences in the capacities of individual nodes to send or receive edges. This often leads to clustering solutions that are heavily influenced by the nodes' capacities. One way to overcome this is to analyse the strengths of connections in relative rather than absolute terms, expressing each edge weight as a proportion of the sending (or receiving) capacity of the respective node. This, however, induces additional modelling constraints that most existing clustering methods are not designed to handle. In this work we propose a stochastic block model for composition-weighted networks based on direct modelling of compositional weight vectors using a Dirichlet mixture, with the parameters determined by the cluster labels of the sender and the receiver nodes. Inference is implemented via an extension of the classification expectation-maximisation algorithm that uses a working independence assumption, expressing the complete data likelihood of each node of the network as a function of fixed cluster labels of the remaining nodes. A model selection criterion is derived to aid the choice of the number of clusters. The model is validated using simulation studies, and showcased on network data from the Erasmus exchange program and a bike sharing network for the city of London.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00651v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iuliia Promskaia, Adrian O'Hagan, Michael Fop</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes Linked Matrix Decomposition</title>
      <link>https://arxiv.org/abs/2408.00237</link>
      <description>arXiv:2408.00237v1 Announce Type: cross 
Abstract: Data for several applications in diverse fields can be represented as multiple matrices that are linked across rows or columns. This is particularly common in molecular biomedical research, in which multiple molecular "omics" technologies may capture different feature sets (e.g., corresponding to rows in a matrix) and/or different sample populations (corresponding to columns). This has motivated a large body of work on integrative matrix factorization approaches that identify and decompose low-dimensional signal that is shared across multiple matrices or specific to a given matrix. We propose an empirical variational Bayesian approach to this problem that has several advantages over existing techniques, including the flexibility to accommodate shared signal over any number of row or column sets (i.e., bidimensional integration), an intuitive model-based objective function that yields appropriate shrinkage for the inferred signals, and a relatively efficient estimation algorithm with no tuning parameters. A general result establishes conditions for the uniqueness of the underlying decomposition for a broad family of methods that includes the proposed approach. For scenarios with missing data, we describe an associated iterative imputation approach that is novel for the single-matrix context and a powerful approach for "blockwise" imputation (in which an entire row or column is missing) in various linked matrix contexts. Extensive simulations show that the method performs very well under different scenarios with respect to recovering underlying low-rank signal, accurately decomposing shared and specific signals, and accurately imputing missing data. The approach is applied to gene expression and miRNA data from breast cancer tissue and normal breast tissue, for which it gives an informative decomposition of variation and outperforms alternative strategies for missing data imputation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00237v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10994-024-06599-8</arxiv:DOI>
      <arxiv:journal_reference>Machine Learning, 2024</arxiv:journal_reference>
      <dc:creator>Eric F. Lock</dc:creator>
    </item>
    <item>
      <title>Strong Oracle Guarantees for Partial Penalized Tests of High Dimensional Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2408.00270</link>
      <description>arXiv:2408.00270v1 Announce Type: cross 
Abstract: Partial penalized tests provide flexible approaches to testing linear hypotheses in high dimensional generalized linear models. However, because the estimators used in these tests are local minimizers of potentially non-convex folded-concave penalized objectives, the solutions one computes in practice may not coincide with the unknown local minima for which we have nice theoretical guarantees. To close this gap between theory and computation, we introduce local linear approximation (LLA) algorithms to compute the full and reduced model estimators for these tests and develop theory specifically for the LLA solutions. We prove that our LLA algorithms converge to oracle estimators for the full and reduced models in two steps with overwhelming probability. We then leverage this strong oracle result and the asymptotic properties of the oracle estimators to show that the partial penalized test statistics evaluated at the two-step LLA solutions are approximately chi-square in large samples, giving us guarantees for the tests using specific computed solutions and thereby closing the theoretical gap. We conduct simulation studies to assess the finite-sample performance of our testing procedures, finding that partial penalized tests using the LLA solutions agree with tests using the oracle estimators, and demonstrate our testing procedures in a real data application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00270v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tate Jacobson</dc:creator>
    </item>
    <item>
      <title>Unsupervised Pairwise Causal Discovery on Heterogeneous Data using Mutual Information Measures</title>
      <link>https://arxiv.org/abs/2408.00399</link>
      <description>arXiv:2408.00399v1 Announce Type: cross 
Abstract: A fundamental task in science is to determine the underlying causal relations because it is the knowledge of this functional structure what leads to the correct interpretation of an effect given the apparent associations in the observed data. In this sense, Causal Discovery is a technique that tackles this challenge by analyzing the statistical properties of the constituent variables. In this work, we target the generalizability of the discovery method by following a reductionist approach that only involves two variables, i.e., the pairwise or bi-variate setting. We question the current (possibly misleading) baseline results on the basis that they were obtained through supervised learning, which is arguably contrary to this genuinely exploratory endeavor. In consequence, we approach this problem in an unsupervised way, using robust Mutual Information measures, and observing the impact of the different variable types, which is oftentimes ignored in the design of solutions. Thus, we provide a novel set of standard unbiased results that can serve as a reference to guide future discovery tasks in completely unknown environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00399v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Trilla, Nenad Mijatovic</dc:creator>
    </item>
    <item>
      <title>Confounder importance learning for treatment effect inference</title>
      <link>https://arxiv.org/abs/2110.00314</link>
      <description>arXiv:2110.00314v5 Announce Type: replace 
Abstract: We address modelling and computational issues for multiple treatment effect inference under many potential confounders.
  Our main contribution is providing a trade-off between preventing the omission of relevant confounders, while not running into an over-selection of instruments that significantly inflates variance. We propose a novel empirical Bayes framework for Bayesian model averaging that learns from data the extent to which the inclusion of key covariates should be encouraged.
  Our framework sets a prior that asymptotically matches the true amount of confounding in the data, as measured by a novel confounding coefficient. A key challenge is computational. We develop fast algorithms, using an exact gradient of the marginal likelihood that has linear cost in the number of covariates, and a variational counterpart. Our framework uses widely-used ingredients and largely existing software, and it is implemented within the R package mombf. We illustrate our work with two applications. The first is the association between salary variation and discriminatory factors. The second, that has been debated in previous works, is the association between abortion policies and crime. Our approach provides insights that differ from previous analyses especially in situations with weaker treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.00314v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Miquel Torrens-i-Dinar\`es, Omiros Papaspiliopoulos, David Rossell</dc:creator>
    </item>
    <item>
      <title>Statistical Inferences and Predictions for Areal Data and Spatial Data Fusion with Hausdorff--Gaussian Processes</title>
      <link>https://arxiv.org/abs/2208.07900</link>
      <description>arXiv:2208.07900v2 Announce Type: replace 
Abstract: Accurate modeling of spatial dependence is pivotal in analyzing spatial data, influencing parameter estimation and out-of-sample predictions. The spatial structure and geometry of the data significantly impact valid statistical inference. Existing models for areal data often rely on adjacency matrices, struggling to differentiate between polygons of varying sizes and shapes. Conversely, data fusion models, while effective, rely on computationally intensive numerical integrals, presenting challenges for moderately large datasets. In response to these issues, we propose the Hausdorff-Gaussian process (HGP), a versatile model class utilizing the Hausdorff distance to capture spatial dependence in both point and areal data. We introduce a valid correlation function within the HGP framework, accommodating diverse modeling techniques, including geostatistical and areal models. Integration into generalized linear mixed-effects models enhances its applicability, particularly in addressing change of support and data fusion challenges. We validate our approach through a comprehensive simulation study and application to two real-world scenarios: one involving areal data and another demonstrating its effectiveness in data fusion. The results suggest that the HGP is competitive with specialized models regarding goodness-of-fit and prediction performances. In summary, the HGP offers a flexible and robust solution for modeling spatial data of various types and shapes, with potential applications spanning fields such as public health and climate science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.07900v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas da Cunha Godoy, Marcos Oliveira Prates, Jun Yan</dc:creator>
    </item>
    <item>
      <title>Low-Rank Covariance Completion for Graph Quilting with Applications to Functional Connectivity</title>
      <link>https://arxiv.org/abs/2209.08273</link>
      <description>arXiv:2209.08273v2 Announce Type: replace 
Abstract: As a tool for estimating networks in high dimensions, graphical models are commonly applied to calcium imaging data to estimate functional neuronal connectivity, i.e. relationships between the activities of neurons. However, in many calcium imaging data sets, the full population of neurons is not recorded simultaneously, but instead in partially overlapping blocks. This leads to the Graph Quilting problem, as first introduced by (Vinci et.al. 2019), in which the goal is to infer the structure of the full graph when only subsets of features are jointly observed. In this paper, we study a novel two-step approach to Graph Quilting, which first imputes the complete covariance matrix using low-rank covariance completion techniques before estimating the graph structure. We introduce three approaches to solve this problem: block singular value decomposition, nuclear norm penalization, and non-convex low-rank factorization. While prior works have studied low-rank matrix completion, we address the challenges brought by the block-wise missingness and are the first to investigate the problem in the context of graph learning. We discuss theoretical properties of the two-step procedure, showing graph selection consistency of one proposed approach by proving novel L infinity-norm error bounds for matrix completion with block-missingness. We then investigate the empirical performance of the proposed methods on simulations and on real-world data examples, through which we show the efficacy of these methods for estimating functional connectivity from calcium imaging data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.08273v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andersen Chang, Lili Zheng, Genevera I. Allen</dc:creator>
    </item>
    <item>
      <title>Identified vaccine efficacy for binary post-infection outcomes under misclassification without monotonicity</title>
      <link>https://arxiv.org/abs/2211.16502</link>
      <description>arXiv:2211.16502v4 Announce Type: replace 
Abstract: In order to meet regulatory approval, pharmaceutical companies often must demonstrate that new vaccines reduce the total risk of a post-infection outcome like transmission, symptomatic disease, severe illness, or death in randomized, placebo-controlled trials. Given that infection is a necessary precondition for a post-infection outcome, one can use principal stratification to partition the total causal effect of vaccination into two causal effects: vaccine efficacy against infection, and the principal effect of vaccine efficacy against a post-infection outcome in the patients that would be infected under both placebo and vaccination. Despite the importance of such principal effects to policymakers, these estimands are generally unidentifiable, even under strong assumptions that are rarely satisfied in real-world trials. We develop a novel method to nonparametrically point identify these principal effects while eliminating the monotonicity assumption and allowing for measurement error. Furthermore, our results allow for multiple treatments, and are general enough to be applicable outside of vaccine efficacy. Our method relies on the fact that many vaccine trials are run at geographically disparate health centers, and measure biologically-relevant categorical pretreatment covariates. We show that our method can be applied to a variety of clinical trial settings where vaccine efficacy against infection and a post-infection outcome can be jointly inferred. This can yield new insights from existing vaccine efficacy trial data and will aid researchers in designing new multi-arm clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.16502v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rob Trangucci, Yang Chen, Jon Zelner</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Continuous Multiple Time Point Interventions</title>
      <link>https://arxiv.org/abs/2305.06645</link>
      <description>arXiv:2305.06645v3 Announce Type: replace 
Abstract: There are limited options to estimate the treatment effects of variables which are continuous and measured at multiple time points, particularly if the true dose-response curve should be estimated as closely as possible. However, these situations may be of relevance: in pharmacology, one may be interested in how outcomes of people living with -- and treated for -- HIV, such as viral failure, would vary for time-varying interventions such as different drug concentration trajectories. A challenge for doing causal inference with continuous interventions is that the positivity assumption is typically violated. To address positivity violations, we develop projection functions, which reweigh and redefine the estimand of interest based on functions of the conditional support for the respective interventions. With these functions, we obtain the desired dose-response curve in areas of enough support, and otherwise a meaningful estimand that does not require the positivity assumption. We develop $g$-computation type plug-in estimators for this case. Those are contrasted with g-computation estimators which are applied to continuous interventions without specifically addressing positivity violations, which we propose to be presented with diagnostics. The ideas are illustrated with longitudinal data from HIV positive children treated with an efavirenz-based regimen as part of the CHAPAS-3 trial, which enrolled children $&lt;13$ years in Zambia/Uganda. Simulations show in which situations a standard g-computation approach is appropriate, and in which it leads to bias and how the proposed weighted estimation approach then recovers the alternative estimand of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.06645v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Michael Schomaker, Helen McIlleron, Paolo Denti, Iv\'an D\'iaz</dc:creator>
    </item>
    <item>
      <title>Conformal prediction for frequency-severity modeling</title>
      <link>https://arxiv.org/abs/2307.13124</link>
      <description>arXiv:2307.13124v3 Announce Type: replace 
Abstract: We present a model-agnostic framework for the construction of prediction intervals of insurance claims, with finite sample statistical guarantees, extending the technique of split conformal prediction to the domain of two-stage frequency-severity modeling. The framework effectiveness is showcased with simulated and real datasets using classical parametric models and contemporary machine learning methods. When the underlying severity model is a random forest, we extend the two-stage split conformal prediction algorithm, showing how the out-of-bag mechanism can be leveraged to eliminate the need for a calibration set in the conformal procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.13124v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Helton Graziadei, Paulo C. Marques F., Eduardo F. L. de Melo, Rodrigo S. Targino</dc:creator>
    </item>
    <item>
      <title>Likelihood-ratio inference on differences in quantiles</title>
      <link>https://arxiv.org/abs/2401.10233</link>
      <description>arXiv:2401.10233v2 Announce Type: replace 
Abstract: Quantiles can represent key operational and business metrics, but the computational challenges associated with inference has hampered their adoption in online experimentation. One-sample confidence intervals are trivial to construct; however, two-sample inference has traditionally required bootstrapping or a density estimator. This paper presents a new two-sample difference-in-quantile hypothesis test and confidence interval based on a likelihood-ratio test statistic. A conservative version of the test does not involve a density estimator; a second version of the test, which uses a density estimator, yields confidence intervals very close to the nominal coverage level. It can be computed using only four order statistics from each sample.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10233v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evan Miller</dc:creator>
    </item>
    <item>
      <title>Semi-parametric local variable selection under misspecification</title>
      <link>https://arxiv.org/abs/2401.10235</link>
      <description>arXiv:2401.10235v3 Announce Type: replace 
Abstract: Local variable selection aims to test for the effect of covariates on an outcome within specific regions. We outline a challenge that arises in the presence of non-linear effects and model misspecification. Specifically, for common semi-parametric methods even slight model misspecification can result in a high false positive rate, in a manner that is highly sensitive to the chosen basis functions. We propose a methodology based on orthogonal cut splines that avoids false positive inflation for any choice of knots, and achieves consistent local variable selection. Our approach offers simplicity, handles both continuous and categorical covariates, and provides theory for high-dimensional covariates and model misspecification. We discuss settings with either independent or dependent data. Our proposal allows including adjustment covariates that do not undergo selection, enhancing the model's flexibility. Our examples describe salary gaps associated with various discrimination factors at different ages, and the effects of covariates on functional data measuring brain activation at different times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10235v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Rossell, Arnold Kisuk Kseung, Ignacio Saez, Michele Guindani</dc:creator>
    </item>
    <item>
      <title>Leveraging Quadratic Polynomials in Python for Advanced Data Analysis</title>
      <link>https://arxiv.org/abs/2402.06133</link>
      <description>arXiv:2402.06133v3 Announce Type: replace 
Abstract: This research explores the application of quadratic polynomials in Python for advanced data analysis. The study demonstrates how quadratic models can effectively capture nonlinear relationships in complex datasets by leveraging Python libraries such as NumPy, Matplotlib, scikit-learn, and Pandas. The methodology involves fitting quadratic polynomials to the data using least-squares regression and evaluating the model fit using the coefficient of determination (R-squared). The results highlight the strong performance of the quadratic polynomial fit, as evidenced by high R-squared values, indicating the model's ability to explain a substantial proportion of the data variability. Comparisons with linear and cubic models further underscore the quadratic model's balance between simplicity and precision for many practical applications. The study also acknowledges the limitations of quadratic polynomials and proposes future research directions to enhance their accuracy and efficiency for diverse data analysis tasks. This research bridges the gap between theoretical concepts and practical implementation, providing an accessible Python-based tool for leveraging quadratic polynomials in data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06133v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rostyslav Sipakov, Olena Voloshkina, Anastasiia Kovalova</dc:creator>
    </item>
    <item>
      <title>A Correlation-induced Finite Difference Estimator</title>
      <link>https://arxiv.org/abs/2405.05638</link>
      <description>arXiv:2405.05638v3 Announce Type: replace 
Abstract: Finite difference (FD) approximation is a classic approach to stochastic gradient estimation when only noisy function realizations are available. In this paper, we first provide a sample-driven method via the bootstrap technique to estimate the optimal perturbation, and then propose an efficient FD estimator based on correlated samples at the estimated optimal perturbation. Furthermore, theoretical analyses of both the perturbation estimator and the FD estimator reveal that, {\it surprisingly}, the correlation enables the proposed FD estimator to achieve a reduction in variance and, in some cases, a decrease in bias compared to the traditional optimal FD estimator. Numerical results confirm the efficiency of our estimators and align well with the theory presented, especially in scenarios with small sample sizes. Finally, we apply the estimator to solve derivative-free optimization (DFO) problems, and numerical studies show that DFO problems with 100 dimensions can be effectively solved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05638v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guo Liang, Guangwu Liu, Kun Zhang</dc:creator>
    </item>
    <item>
      <title>How should parallel cluster randomized trials with a baseline period be analyzed? A survey of estimands and common estimators</title>
      <link>https://arxiv.org/abs/2406.02028</link>
      <description>arXiv:2406.02028v2 Announce Type: replace 
Abstract: The parallel cluster randomized trial with baseline (PB-CRT) is a common variant of the standard parallel cluster randomized trial (P-CRT). We define two natural estimands in the context of PB-CRTs with informative cluster sizes, the participant-average treatment effect (pATE) and cluster-average treatment effect (cATE), to address participant and cluster-level hypotheses. In this work, we theoretically derive the convergence of the unweighted and inverse cluster-period size weighted (i.) independence estimating equation, (ii.) fixed-effects model, (iii.) exchangeable mixed-effects model, and (iv.) nested-exchangeable mixed-effects model treatment effect estimators in a PB-CRT with continuous outcomes. Overall, we theoretically show that the unweighted and weighted independence estimating equation and fixed-effects model yield consistent estimators for the pATE and cATE estimands. Although mixed-effects models yield inconsistent estimators to these two natural estimands under informative cluster sizes, we empirically demonstrate that the exchangeable mixed-effects model is surprisingly robust to bias. This is in sharp contrast to the corresponding analyses in P-CRTs and the nested-exchangeable mixed-effects model in PB-CRTs, and may carry implications for practice. We report a simulation study and conclude with a re-analysis of a PB-CRT examining the effects of community youth teams on improving mental health among adolescent girls in rural eastern India.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02028v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenneth Menglin Lee, Fan Li</dc:creator>
    </item>
    <item>
      <title>Flexible max-stable processes for fast and efficient inference</title>
      <link>https://arxiv.org/abs/2407.13958</link>
      <description>arXiv:2407.13958v3 Announce Type: replace 
Abstract: Max-stable processes serve as the fundamental distributional family in extreme value theory. However, likelihood-based inference methods for max-stable processes still heavily rely on composite likelihoods, rendering them intractable in high dimensions due to their intractable densities. In this paper, we introduce a fast and efficient inference method for max-stable processes based on their angular densities for a class of max-stable processes whose angular densities do not put mass on the boundary space of the simplex. This class can also be used to construct r-Pareto processes. We demonstrate the efficiency of the proposed method through two new max-stable processes: the truncated extremal-t process and the skewed Brown-Resnick process. The skewed Brown-Resnick process contains the popular Brown-Resnick model as a special case and possesses nonstationary extremal dependence structures. The proposed method is shown to be computationally efficient and can be applied to large datasets. We showcase the new max-stable processes on simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13958v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peng Zhong, Scott A. Sisson, Boris Beranger</dc:creator>
    </item>
    <item>
      <title>Estimating heterogeneous treatment effects by W-MCM based on Robust reduced rank regression</title>
      <link>https://arxiv.org/abs/2407.19659</link>
      <description>arXiv:2407.19659v2 Announce Type: replace 
Abstract: Recently, from the personalized medicine perspective, there has been an increased demand to identify subgroups of subjects for whom treatment is effective. Consequently, the estimation of heterogeneous treatment effects (HTE) has been attracting attention. While various estimation methods have been developed for a single outcome, there are still limited approaches for estimating HTE for multiple outcomes. Accurately estimating HTE remains a challenge especially for datasets where there is a high correlation between outcomes or the presence of outliers. Therefore, this study proposes a method that uses a robust reduced-rank regression framework to estimate treatment effects and identify effective subgroups. This approach allows the consideration of correlations between treatment effects and the estimation of treatment effects with an accurate low-rank structure. It also provides robust estimates for outliers. This study demonstrates that, when treatment effects are estimated using the reduced rank regression framework with an appropriate rank, the expected value of the estimator equals the treatment effect. Finally, we illustrate the effectiveness and interpretability of the proposed method through simulations and real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19659v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryoma Hieda, Shintaro Yuki, Kensuke Tanioka, Hiroshi Yadohisa</dc:creator>
    </item>
    <item>
      <title>Neyman-Pearson Multi-class Classification via Cost-sensitive Learning</title>
      <link>https://arxiv.org/abs/2111.04597</link>
      <description>arXiv:2111.04597v4 Announce Type: replace-cross 
Abstract: Most existing classification methods aim to minimize the overall misclassification error rate. However, in applications such as loan default prediction, different types of errors can have varying consequences. To address this asymmetry issue, two popular paradigms have been developed: the Neyman-Pearson (NP) paradigm and the cost-sensitive (CS) paradigm. Previous studies on the NP paradigm have primarily focused on the binary case, while the multi-class NP problem poses a greater challenge due to its unknown feasibility. In this work, we tackle the multi-class NP problem by establishing a connection with the CS problem via strong duality and propose two algorithms. We extend the concept of NP oracle inequalities, crucial in binary classifications, to NP oracle properties in the multi-class context. Our algorithms satisfy these NP oracle properties under certain conditions. Furthermore, we develop practical algorithms to assess the feasibility and strong duality in multi-class NP problems, which can offer practitioners the landscape of a multi-class NP problem with various target error levels. Simulations and real data studies validate the effectiveness of our algorithms. To our knowledge, this is the first study to address the multi-class NP problem with theoretical guarantees. The proposed algorithms have been implemented in the R package \texttt{npcs}, which is available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.04597v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Tian, Yang Feng</dc:creator>
    </item>
    <item>
      <title>Space-time extremes of severe US thunderstorm environments</title>
      <link>https://arxiv.org/abs/2201.05102</link>
      <description>arXiv:2201.05102v3 Announce Type: replace-cross 
Abstract: Severe thunderstorms cause substantial economic and human losses in the United States. Simultaneous high values of convective available potential energy (CAPE) and storm relative helicity (SRH) are favorable to severe weather, and both they and the composite variable $\mathrm{PROD}=\sqrt{\mathrm{CAPE}} \times \mathrm{SRH}$ can be used as indicators of severe thunderstorm activity. Their extremal spatial dependence exhibits temporal non-stationarity due to seasonality and large-scale atmospheric signals such as El Ni\~no-Southern Oscillation (ENSO). In order to investigate this, we introduce a space-time model based on a max-stable, Brown--Resnick, field whose range depends on ENSO and on time through a tensor product spline. We also propose a max-stability test based on empirical likelihood and the bootstrap. The marginal and dependence parameters must be estimated separately owing to the complexity of the model, and we develop a bootstrap-based model selection criterion that accounts for the marginal uncertainty when choosing the dependence model. In the case study, the out-sample performance of our model is good. We find that extremes of PROD, CAPE and SRH are generally more localized in summer and, in some regions, less localized during El Ni\~no and La Ni\~na events, and give meteorological interpretations of these phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.05102v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Koh, Erwan Koch, Anthony C. Davison</dc:creator>
    </item>
    <item>
      <title>Robust Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models</title>
      <link>https://arxiv.org/abs/2209.15224</link>
      <description>arXiv:2209.15224v3 Announce Type: replace-cross 
Abstract: Unsupervised learning has been widely used in many real-world applications. One of the simplest and most important unsupervised learning models is the Gaussian mixture model (GMM). In this work, we study the multi-task learning problem on GMMs, which aims to leverage potentially similar GMM parameter structures among tasks to obtain improved learning performance compared to single-task learning. We propose a multi-task GMM learning procedure based on the EM algorithm that effectively utilizes unknown similarities between related tasks and is robust against a fraction of outlier tasks from arbitrary distributions. The proposed procedure is shown to achieve the minimax optimal rate of convergence for both parameter estimation error and the excess mis-clustering error, in a wide range of regimes. Moreover, we generalize our approach to tackle the problem of transfer learning for GMMs, where similar theoretical results are derived. Additionally, iterative unsupervised multi-task and transfer learning methods may suffer from an initialization alignment problem, and two alignment algorithms are proposed to resolve the issue. Finally, we demonstrate the effectiveness of our methods through simulations and real data examples. To the best of our knowledge, this is the first work studying multi-task and transfer learning on GMMs with theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.15224v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Tian, Haolei Weng, Lucy Xia, Yang Feng</dc:creator>
    </item>
    <item>
      <title>Enhanced Local Explainability and Trust Scores with Random Forest Proximities</title>
      <link>https://arxiv.org/abs/2310.12428</link>
      <description>arXiv:2310.12428v2 Announce Type: replace-cross 
Abstract: We initiate a novel approach to explain the predictions and out of sample performance of random forest (RF) regression and classification models by exploiting the fact that any RF can be mathematically formulated as an adaptive weighted K nearest-neighbors model. Specifically, we employ a recent result that, for both regression and classification tasks, any RF prediction can be rewritten exactly as a weighted sum of the training targets, where the weights are RF proximities between the corresponding pairs of data points. We show that this linearity facilitates a local notion of explainability of RF predictions that generates attributions for any model prediction across observations in the training set, and thereby complements established feature-based methods like SHAP, which generate attributions for a model prediction across input features. We show how this proximity-based approach to explainability can be used in conjunction with SHAP to explain not just the model predictions, but also out-of-sample performance, in the sense that proximities furnish a novel means of assessing when a given model prediction is more or less likely to be correct. We demonstrate this approach in the modeling of US corporate bond prices and returns in both regression and classification cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12428v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua Rosaler, Dhruv Desai, Bhaskarjit Sarmah, Dimitrios Vamvourellis, Deran Onay, Dhagash Mehta, Stefano Pasquali</dc:creator>
    </item>
    <item>
      <title>Modeling Latent Selection with Structural Causal Models</title>
      <link>https://arxiv.org/abs/2401.06925</link>
      <description>arXiv:2401.06925v2 Announce Type: replace-cross 
Abstract: Selection bias is ubiquitous in real-world data, and can lead to misleading results if not dealt with properly. We introduce a conditioning operation on Structural Causal Models (SCMs) to model latent selection from a causal perspective. We show that the conditioning operation transforms an SCM with the presence of an explicit latent selection mechanism into an SCM without such selection mechanism, which partially encodes the causal semantics of the selected subpopulation according to the original SCM. Furthermore, we show that this conditioning operation preserves the simplicity, acyclicity, and linearity of SCMs, and commutes with marginalization. Thanks to these properties, combined with marginalization and intervention, the conditioning operation offers a valuable tool for conducting causal reasoning tasks within causal models where latent details have been abstracted away. We demonstrate by example how classical results of causal inference can be generalized to include selection bias and how the conditioning operation helps with modeling of real-world problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06925v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leihao Chen, Onno Zoeter, Joris M. Mooij</dc:creator>
    </item>
    <item>
      <title>Debiased Distribution Compression</title>
      <link>https://arxiv.org/abs/2404.12290</link>
      <description>arXiv:2404.12290v3 Announce Type: replace-cross 
Abstract: Modern compression methods can summarize a target distribution $\mathbb{P}$ more succinctly than i.i.d. sampling but require access to a low-bias input sequence like a Markov chain converging quickly to $\mathbb{P}$. We introduce a new suite of compression methods suitable for compression with biased input sequences. Given $n$ points targeting the wrong distribution and quadratic time, Stein kernel thinning (SKT) returns $\sqrt{n}$ equal-weighted points with $\widetilde{O}(n^{-1/2})$ maximum mean discrepancy (MMD) to $\mathbb{P}$. For larger-scale compression tasks, low-rank SKT achieves the same feat in sub-quadratic time using an adaptive low-rank debiasing procedure that may be of independent interest. For downstream tasks that support simplex or constant-preserving weights, Stein recombination and Stein Cholesky achieve even greater parsimony, matching the guarantees of SKT with as few as $\text{poly-log}(n)$ weighted points. Underlying these advances are new guarantees for the quality of simplex-weighted coresets, the spectral decay of kernel matrices, and the covering numbers of Stein kernel Hilbert spaces. In our experiments, our techniques provide succinct and accurate posterior summaries while overcoming biases due to burn-in, approximate Markov chain Monte Carlo, and tempering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12290v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingxiao Li, Raaz Dwivedi, Lester Mackey</dc:creator>
    </item>
    <item>
      <title>Greedy Capon Beamformer</title>
      <link>https://arxiv.org/abs/2404.15329</link>
      <description>arXiv:2404.15329v2 Announce Type: replace-cross 
Abstract: We propose greedy Capon beamformer (GCB) for direction finding of narrow-band sources present in the array's viewing field. After defining the grid covering the location search space, the algorithm greedily builds the interference-plus-noise covariance matrix by identifying a high-power source on the grid using Capon's principle of maximizing the signal to interference plus noise ratio while enforcing unit gain towards the signal of interest. An estimate of the power of the detected source is derived by exploiting the unit power constraint, which subsequently allows to update the noise covariance matrix by simple rank-1 matrix addition composed of outerproduct of the selected steering matrix with itself scaled by the signal power estimate. Our numerical examples demonstrate effectiveness of the proposed GCB in direction finding where it performs favourably compared to the state-of-the-art algorithms under a broad variety of settings. Furthermore, GCB estimates of direction-of-arrivals (DOAs) are very fast to compute.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15329v2</guid>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Esa Ollila</dc:creator>
    </item>
    <item>
      <title>COKE: Causal Discovery with Chronological Order and Expert Knowledge in High Proportion of Missing Manufacturing Data</title>
      <link>https://arxiv.org/abs/2407.12254</link>
      <description>arXiv:2407.12254v2 Announce Type: replace-cross 
Abstract: Understanding causal relationships between machines is crucial for fault diagnosis and optimization in manufacturing processes. Real-world datasets frequently exhibit up to 90% missing data and high dimensionality from hundreds of sensors. These datasets also include domain-specific expert knowledge and chronological order information, reflecting the recording order across different machines, which is pivotal for discerning causal relationships within the manufacturing data. However, previous methods for handling missing data in scenarios akin to real-world conditions have not been able to effectively utilize expert knowledge. Conversely, prior methods that can incorporate expert knowledge struggle with datasets that exhibit missing values. Therefore, we propose COKE to construct causal graphs in manufacturing datasets by leveraging expert knowledge and chronological order among sensors without imputing missing data. Utilizing the characteristics of the recipe, we maximize the use of samples with missing values, derive embeddings from intersections with an initial graph that incorporates expert knowledge and chronological order, and create a sensor ordering graph. The graph-generating process has been optimized by an actor-critic architecture to obtain a final graph that has a maximum reward. Experimental evaluations in diverse settings of sensor quantities and missing proportions demonstrate that our approach compared with the benchmark methods shows an average improvement of 39.9% in the F1-score. Moreover, the F1-score improvement can reach 62.6% when considering the configuration similar to real-world datasets, and 85.0% in real-world semiconductor datasets. The source code is available at https://github.com/OuTingYun/COKE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12254v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ting-Yun Ou, Ching Chang, Wen-Chih Peng</dc:creator>
    </item>
  </channel>
</rss>
