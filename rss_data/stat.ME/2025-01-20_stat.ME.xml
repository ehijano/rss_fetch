<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Jan 2025 05:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Design-based causal inference in bipartite experiments</title>
      <link>https://arxiv.org/abs/2501.09844</link>
      <description>arXiv:2501.09844v1 Announce Type: new 
Abstract: Bipartite experiments are widely used across various fields, yet existing methods often rely on strong assumptions about modeling the potential outcomes and exposure mapping. In this paper, we explore design-based causal inference in bipartite experiments, where treatments are randomized over one set of units, while outcomes are measured over a separate set of units. We first formulate the causal inference problem under a design-based framework that generalizes the classic assumption to account for bipartite interference. We then propose point and variance estimators for the total treatment effect, establish a central limit theorem for the estimator, and propose a conservative variance estimator. Additionally, we discuss a covariate adjustment strategy to enhance estimation efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09844v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sizhu Lu, Lei Shi, Yue Fang, Wenxin Zhang, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Model Averaging Under Flexible Loss Functions</title>
      <link>https://arxiv.org/abs/2501.09924</link>
      <description>arXiv:2501.09924v1 Announce Type: new 
Abstract: To address model uncertainty under flexible loss functions in prediction p blems, we propose a model averaging method that accommodates various loss functions, including asymmetric linear and quadratic loss functions as well as many other asymmetric/ symmetric loss functions as special cases. The flexible loss function allows the proposed method to average a large range of models such as the quantile and expectile regression models. To determine the weights of the candidate models, we establish a J-fold cross-validation criterion. Asymptotic optimality and weight convergence are proved for the proposed method. Simulations and an empirical application show the superior performance of the proposed method compared with other methods of model selection and averaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09924v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1287/ijoc.2023.0291</arxiv:DOI>
      <dc:creator>Dieqi Gu, Qingfeng Liu, Xinyu Zhang</dc:creator>
    </item>
    <item>
      <title>Regularized Sparse Optimal Discriminant Clustering</title>
      <link>https://arxiv.org/abs/2501.10147</link>
      <description>arXiv:2501.10147v1 Announce Type: new 
Abstract: We propose a new method based on sparse optimal discriminant clustering (SODC), by a penalty term to scoring matrix based on convex clustering. With the addition of this penalty term, it is expected to improve the accuracy of cluster identification by attaching points from the same cluster closer together and points from different clusters further apart. Moreover, we develop a novel algorithm to derive the updated formula of this scoring matrix using majorizing function. It solves the difficulty to satisfy both constraint and containing the clustering structure to the scoring matrix. We have demonstrated the numerical simulations and its an application to real data to assess the performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10147v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mayu Hiraishi, Kensuke Tanioka, Hiroshi Yadohisa</dc:creator>
    </item>
    <item>
      <title>Prior distributions for structured semi-orthogonal matrices</title>
      <link>https://arxiv.org/abs/2501.10263</link>
      <description>arXiv:2501.10263v1 Announce Type: new 
Abstract: Statistical models for multivariate data often include a semi-orthogonal matrix parameter. In many applications, there is reason to expect that the semi-orthogonal matrix parameter satisfies a structural assumption such as sparsity or smoothness. From a Bayesian perspective, these structural assumptions should be incorporated into an analysis through the prior distribution. In this work, we introduce a general approach to constructing prior distributions for structured semi-orthogonal matrices that leads to tractable posterior inference via parameter-expanded Markov chain Monte Carlo. We draw upon recent results from random matrix theory to establish a theoretical basis for the proposed approach. We then introduce specific prior distributions for incorporating sparsity or smoothness and illustrate their use through applications to biological and oceanographic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10263v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Jauch, Marie-Christine D\"uker, Peter Hoff</dc:creator>
    </item>
    <item>
      <title>Cheap Subsampling bootstrap confidence intervals for fast and robust inference in biostatistics</title>
      <link>https://arxiv.org/abs/2501.10289</link>
      <description>arXiv:2501.10289v1 Announce Type: new 
Abstract: Bootstrapping is often applied to get confidence limits for semiparametric inference of a target parameter in the presence of nuisance parameters. Bootstrapping with replacement can be computationally expensive and problematic when cross-validation is used in the estimation algorithm due to duplicate observations in the bootstrap samples. We provide a valid, fast, easy-to-implement subsampling bootstrap method for constructing confidence intervals for asymptotically linear estimators and discuss its application to semiparametric causal inference. Our method, inspired by the Cheap Bootstrap (Lam, 2022), leverages the quantiles of a t-distribution and has the desired coverage with few bootstrap replications. We show that the method is asymptotically valid if the subsample size is chosen appropriately as a function of the sample size. We illustrate our method with data from the LEADER trial (Marso et al., 2016), obtaining confidence intervals for a longitudinal targeted minimum loss-based estimator (van der Laan and Gruber, 2012). Through a series of empirical experiments, we also explore the impact of subsample size, sample size, and the number of bootstrap repetitions on the performance of the confidence interval.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10289v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johan Sebastian Ohlendorff, Anders Munch, Kathrine Kold S{\o}rensen, Thomas Alexander Gerds</dc:creator>
    </item>
    <item>
      <title>SBAMDT: Bayesian Additive Decision Trees with Adaptive Soft Semi-multivariate Split Rules</title>
      <link>https://arxiv.org/abs/2501.09900</link>
      <description>arXiv:2501.09900v1 Announce Type: cross 
Abstract: Bayesian Additive Regression Trees [BART, Chipman et al., 2010] have gained significant popularity due to their remarkable predictive performance and ability to quantify uncertainty. However, standard decision tree models rely on recursive data splits at each decision node, using deterministic decision rules based on a single univariate feature. This approach limits their ability to effectively capture complex decision boundaries, particularly in scenarios involving multiple features, such as spatial domains, or when transitions are either sharp or smoothly varying. In this paper, we introduce a novel probabilistic additive decision tree model that employs a soft split rule. This method enables highly flexible splits that leverage both univariate and multivariate features, while also respecting the geometric properties of the feature domain. Notably, the probabilistic split rule adapts dynamically across decision nodes, allowing the model to account for varying levels of smoothness in the regression function. We demonstrate the utility of the proposed model through comparisons with existing tree-based models on synthetic datasets and a New York City education dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09900v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stamatina Lamprinakou, Huiyan Sang, Bledar A. Konomi, Ligang Lu</dc:creator>
    </item>
    <item>
      <title>Prediction Sets and Conformal Inference with Censored Outcomes</title>
      <link>https://arxiv.org/abs/2501.10117</link>
      <description>arXiv:2501.10117v1 Announce Type: cross 
Abstract: Given data on a scalar random variable $Y$, a prediction set for $Y$ with miscoverage level $\alpha$ is a set of values for $Y$ that contains a randomly drawn $Y$ with probability $1 - \alpha$, where $\alpha \in (0,1)$. Among all prediction sets that satisfy this coverage property, the oracle prediction set is the one with the smallest volume. This paper provides estimation methods of such prediction sets given observed conditioning covariates when $Y$ is censored or measured in intervals. We first characterise the oracle prediction set under interval censoring and develop a consistent estimator for the shortest prediction interval that satisfies this coverage property. We then extend these consistency results to accommodate cases where the prediction set consists of multiple disjoint intervals. Second, we use conformal inference to construct a prediction set that achieves a particular notion of finite-sample validity under censoring and maintains consistency as sample size increases. This notion exploits exchangeability to obtain finite sample guarantees on coverage using a specially constructed conformity score function. The procedure accomodates the prediction uncertainty that is irreducible (due to the stochastic nature of outcomes), the modelling uncertainty due to partial identification and also sampling uncertainty that gets reduced as samples get larger. We conduct a set of Monte Carlo simulations and an application to data from the Current Population Survey. The results highlight the robustness and efficiency of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10117v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weiguang Liu, \'Aureo de Paula, Elie Tamer</dc:creator>
    </item>
    <item>
      <title>Conformal Prediction Sets with Improved Conditional Coverage using Trust Scores</title>
      <link>https://arxiv.org/abs/2501.10139</link>
      <description>arXiv:2501.10139v1 Announce Type: cross 
Abstract: Standard conformal prediction offers a marginal guarantee on coverage, but for prediction sets to be truly useful, they should ideally ensure coverage conditional on each test point. Unfortunately, it is impossible to achieve exact, distribution-free conditional coverage in finite samples. In this work, we propose an alternative conformal prediction algorithm that targets coverage where it matters most--in instances where a classifier is overconfident in its incorrect predictions. We start by dissecting miscoverage events in marginally-valid conformal prediction, and show that miscoverage rates vary based on the classifier's confidence and its deviation from the Bayes optimal classifier. Motivated by this insight, we develop a variant of conformal prediction that targets coverage conditional on a reduced set of two variables: the classifier's confidence in a prediction and a nonparametric trust score that measures its deviation from the Bayes classifier. Empirical evaluation on multiple image datasets shows that our method generally improves conditional coverage properties compared to standard conformal prediction, including class-conditional coverage, coverage over arbitrary subgroups, and coverage over demographic groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10139v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jivat Neet Kaur, Michael I. Jordan, Ahmed Alaa</dc:creator>
    </item>
    <item>
      <title>Fixed Confidence and Fixed Tolerance Bi-level Optimization for Selecting the Best Optimized System</title>
      <link>https://arxiv.org/abs/2501.10268</link>
      <description>arXiv:2501.10268v1 Announce Type: cross 
Abstract: In this paper, we study a fixed-confidence, fixed-tolerance formulation of a class of stochastic bi-level optimization problems, where the upper-level problem selects from a finite set of systems based on a performance metric, and the lower-level problem optimizes continuous decision variables for each system. Notably, the objective functions for the upper and lower levels can differ. This class of problems has a wide range of applications, including model selection, ranking and selection under input uncertainty, and optimal design. To address this, we propose a multi-stage Pruning-Optimization framework that alternates between comparing the performance of different systems (Pruning) and optimizing systems (Optimization). % In the Pruning stage, we design a sequential algorithm that identifies and eliminates inferior systems through systematic performance evaluations. In the Optimization stage, the goal is to solve for a near-optimal solution that meets specified confidence and tolerance requirements. This multi-stage framework is designed to enhance computational efficiency by pruning inferior systems with high tolerance early on, thereby avoiding unnecessary computational efforts. We demonstrate the effectiveness of the proposed algorithm through both theoretical analysis of statistical validity and sample complexity and numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10268v1</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhao Wang, Seong-Hee Kim, Enlu Zhou</dc:creator>
    </item>
    <item>
      <title>Treatment Effect Estimation with Observational Network Data using Machine Learning</title>
      <link>https://arxiv.org/abs/2206.14591</link>
      <description>arXiv:2206.14591v4 Announce Type: replace 
Abstract: Causal inference methods for treatment effect estimation usually assume independent units. However, this assumption is often questionable because units may interact, resulting in spillover effects between them. We develop augmented inverse probability weighting (AIPW) for estimation and inference of the expected average treatment effect (EATE) with observational data from a single (social) network with spillover effects. In contrast to overall effects such as the global average treatment effect (GATE), the EATE measures, in expectation and on average over all units, how the outcome of a unit is causally affected by its own treatment, marginalizing over the spillover effects from other units. We develop cross-fitting theory with plugin machine learning to obtain a semiparametric treatment effect estimator that converges at the parametric rate and asymptotically follows a Gaussian distribution. The asymptotics are developed using the dependency graph rather than the network graph, which makes explicit that we allow for spillover effects beyond immediate neighbors in the network. We apply our AIPW method to the Swiss StudentLife Study data to investigate the effect of hours spent studying on exam performance accounting for the students' social network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.14591v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Corinne Emmenegger, Meta-Lina Spohn, Timon Elmer, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>Coordinated Trading Strategies for Battery Storage in Reserve and Spot Markets</title>
      <link>https://arxiv.org/abs/2406.08390</link>
      <description>arXiv:2406.08390v3 Announce Type: replace 
Abstract: Quantity and price risks are key uncertainties market participants face in electricity markets with increased volatility, for instance, due to high shares of renewables. From day ahead until real-time, there is a large variation in the best available information, leading to price changes that flexible assets, such as battery storage, can exploit economically. This study contributes to understanding how coordinated bidding strategies can enhance multi-market trading and large-scale energy storage integration. Our findings shed light on the complexities arising from interdependencies and the high-dimensional nature of the problem. We show how stochastic dual dynamic programming is a suitable solution technique for such an environment. We include the three markets of the frequency containment reserve, day-ahead, and intraday in stochastic modelling and develop a multi-stage stochastic program. Prices are represented in a multidimensional Markov Chain, following the scheduling of the markets and allowing for time-dependent randomness. Using the example of a battery storage in the German energy sector, we provide valuable insights into the technical aspects of our method and the economic feasibility of battery storage operation. We find that capacity reservation in the frequency containment reserve dominates over the battery's cycling in spot markets at the given resolution on prices in 2022. In an adjusted price environment, we find that coordination can yield an additional value of up to 12.5%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08390v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paul E. Seifert, Emil Kraft, Steffen Bakker, Stein-Erik Fleten</dc:creator>
    </item>
    <item>
      <title>Enhancing reliability in prediction intervals using point forecasters: Heteroscedastic Quantile Regression and Width-Adaptive Conformal Inference</title>
      <link>https://arxiv.org/abs/2406.14904</link>
      <description>arXiv:2406.14904v2 Announce Type: replace 
Abstract: Constructing prediction intervals for time series forecasting is challenging, particularly when practitioners rely solely on point forecasts. While previous research has focused on creating increasingly efficient intervals, we argue that standard measures alone are inadequate. Beyond efficiency, prediction intervals must adapt their width based on the difficulty of the prediction while preserving coverage regardless of complexity. To address these issues, we propose combining Heteroscedastic Quantile Regression (HQR) with Width-Adaptive Conformal Inference (WACI). This integrated procedure guarantees theoretical coverage and enables interval widths to vary with predictive uncertainty. We assess its performance using both a synthetic example and a real world Electricity Price Forecasting scenario. Our results show that this combined approach meets or surpasses typical benchmarks for validity and efficiency, while also fulfilling important yet often overlooked practical requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14904v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Carlos Sebasti\'an, Carlos E. Gonz\'alez-Guill\'en, Jes\'us Juan</dc:creator>
    </item>
    <item>
      <title>Geodesic Causal Inference</title>
      <link>https://arxiv.org/abs/2406.19604</link>
      <description>arXiv:2406.19604v2 Announce Type: replace 
Abstract: Adjusting for confounding and imbalance when establishing statistical relationships is an increasingly important task, and causal inference methods have emerged as the most popular tool to achieve this. Causal inference has been developed mainly for scalar outcomes and recently for distributional outcomes. We introduce here a general framework for causal inference when outcomes reside in general geodesic metric spaces, where we draw on a novel geodesic calculus that facilitates scalar multiplication for geodesics and the characterization of treatment effects through the concept of the geodesic average treatment effect. Using ideas from Fr\'echet regression, we develop estimation methods of the geodesic average treatment effect and derive consistency and rates of convergence for the proposed estimators. We also study uncertainty quantification and inference for the treatment effect. Our methodology is illustrated by a simulation study and real data examples for compositional outcomes of U.S. statewise energy source data to study the effect of coal mining, network data of New York taxi trips, where the effect of the COVID-19 pandemic is of interest, and brain functional connectivity network data to study the effect of Alzheimer's disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19604v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daisuke Kurisu, Yidong Zhou, Taisuke Otsu, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>Tree-structured Markov random fields with Poisson marginal distributions</title>
      <link>https://arxiv.org/abs/2408.13649</link>
      <description>arXiv:2408.13649v2 Announce Type: replace 
Abstract: A new family of tree-structured Markov random fields for a vector of discrete counting random variables is introduced. According to the characteristics of the family, the marginal distributions of the Markov random fields are all Poisson with the same mean, and are untied from the strength or structure of their built-in dependence. This key feature is uncommon for Markov random fields and most convenient for applications purposes. The specific properties of this new family confer a straightforward sampling procedure and analytic expressions for the joint probability mass function and the joint probability generating function of the vector of counting random variables, thus granting computational methods that scale well to vectors of high dimension. We study the distribution of the sum of random variables constituting a Markov random field from the proposed family, analyze a random variable's individual contribution to that sum through expected allocations, and establish stochastic orderings to assess a wide understanding of their behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13649v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin C\^ot\'e, H\'el\`ene Cossette, Etienne Marceau</dc:creator>
    </item>
    <item>
      <title>Identification of distributions for risks based on the first moment and c-statistic</title>
      <link>https://arxiv.org/abs/2409.09178</link>
      <description>arXiv:2409.09178v2 Announce Type: replace 
Abstract: We show that for any family of distributions with support on [0,1] with strictly monotonic cumulative distribution function that has no jumps and is quantile-identifiable (i.e., any two distinct quantiles identify the distribution), knowing the first moment and c-statistic is enough to identify the distribution. The derivations motivate numerical algorithms for mapping a given pair of expected value and c-statistic to the parameters of specified two-parameter distributions for probabilities. We implemented these algorithms in R and in a simulation study evaluated their numerical accuracy for common families of distributions for risks (beta, logit-normal, and probit-normal). An area of application for these developments is in risk prediction modeling (e.g., sample size calculations and Value of Information analysis), where one might need to estimate the parameters of the distribution of predicted risks from the reported summary statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09178v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Sadatsafavi, Tae Yoon Lee, John Petkau</dc:creator>
    </item>
    <item>
      <title>A semi-supervised framework for diverse multiple hypothesis testing scenarios</title>
      <link>https://arxiv.org/abs/2411.15771</link>
      <description>arXiv:2411.15771v2 Announce Type: replace 
Abstract: Standard multiple testing procedures are designed to report a list of discoveries, or suspected false null hypotheses, given the hypotheses' p-values or test scores. Recently there has been a growing interest in enhancing such procedures by combining additional information with the primary p-value or score. Specifically, such so-called ``side information'' can be leveraged to improve the separation between true and false nulls along additional ``dimensions'' thereby increasing the overall sensitivity. In line with this idea, we develop RESET (REScoring via Estimating and Training) which uses a unique data-splitting protocol that subsequently allows any semi-supervised learning approach to factor in the available side-information while maintaining finite-sample error rate control. Our practical implementation, RESET Ensemble, selects from an ensemble of classification algorithms so that it is compatible to a range of multiple testing scenarios without the need for the user to select the appropriate one. We apply RESET to both p-value and competition based multiple testing problems and show that RESET is (1) power-wise competitive, (2) fast compared to most tools and (3) is able to uniquely achieve finite sample FDR or FDP control, depending on the user's preference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15771v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jack Freestone, William Stafford Noble, Uri Keich</dc:creator>
    </item>
    <item>
      <title>Logistic lasso regression with nearest neighbors for gradient-based dimension reduction</title>
      <link>https://arxiv.org/abs/2407.08485</link>
      <description>arXiv:2407.08485v2 Announce Type: replace-cross 
Abstract: This paper investigates a new approach to estimate the gradient of the conditional probability given the covariates in the binary classification framework. The proposed approach consists in fitting a localized nearest-neighbor logistic model with $\ell_1$-penalty in order to cope with possibly high-dimensional covariates. Our theoretical analysis shows that the pointwise convergence rate of the gradient estimator is optimal under very mild conditions. Moreover, using an outer product of such gradient estimates at several points in the covariate space, we establish the rate of convergence for estimating the so-called central subspace, a well-known object allowing to carry out dimension reduction within the covariate space. Our implementation uses cross-validation on the misclassification rate to estimate the dimension of this subspace. We find that the proposed approach outperforms existing competitors in synthetic and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08485v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Touqeer Ahmad, Fran\c{c}ois Portier, Gilles Stupfler</dc:creator>
    </item>
  </channel>
</rss>
