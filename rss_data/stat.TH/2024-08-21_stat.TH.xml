<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 21 Aug 2024 06:23:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 21 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Constructive and consistent estimation of quadratic minimax</title>
      <link>https://arxiv.org/abs/2408.10218</link>
      <description>arXiv:2408.10218v1 Announce Type: new 
Abstract: We consider $k$ square integrable random variables $Y_1,...,Y_k$ and $k$ random (row) vectors of length $p$, $X_1,...,X_k$ such that $X_i(l)$ is square integrable for $1\le i\le k$ and $1\le l\le p$. No assumptions whatsoever are made of any relationship between the $X_i$:s and $Y_i$:s. We shall refer to each pairing of $X_i$ and $Y_i$ as an environment. We form the square risk functions $R_i(\beta)=\mathbb{E}\left[(Y_i-\beta X_i)^2\right]$ for every environment and consider $m$ affine combinations of these $k$ risk functions. Next, we define a parameter space $\Theta$ where we associate each point with a subset of the unique elements of the covariance matrix of $(X_i,Y_i)$ for an environment. Then we study estimation of the $\arg\min$-solution set of the maximum of a the $m$ affine combinations the of quadratic risk functions. We provide a constructive method for estimating the entire $\arg\min$-solution set which is consistent almost surely outside a zero set in $\Theta^k$. This method is computationally expensive, since it involves solving polynomials of general degree. To overcome this, we define another approximate estimator that also provides a consistent estimation of the solution set based on the bisection method, which is computationally much more efficient. We apply the method to worst risk minimization in the setting of structural equation models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10218v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philip Kennerberg, Ernst C. Wit</dc:creator>
    </item>
    <item>
      <title>Extension of the one-sample Kolmogorov-Smirnov test</title>
      <link>https://arxiv.org/abs/2408.10612</link>
      <description>arXiv:2408.10612v1 Announce Type: new 
Abstract: We propose here a new goodness-of-fit test, named the one-sample OVL-q test (q = 1, 2, . . .), which can be considered an extension of the one-sample Kolmogorov-Smirnov test (equivalent to the one-sample OVL-1 test). We have analyzed the asymptotic properties of the one-sample OVL-2 test statistic and enabled the calculation of asymptotic p-values for the test statistic. We further conducted numerical experiments and demonstrated that the one-sample OVL-2 test can sometimes exceed the detection power of conventional goodness-of-fit tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10612v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Atsushi Komaba, Hisashi Johno, Kazunori Nakamoto</dc:creator>
    </item>
    <item>
      <title>Conditional Delta-Method for Resampling Empirical Processes in Multiple Sample Problems</title>
      <link>https://arxiv.org/abs/2408.10856</link>
      <description>arXiv:2408.10856v1 Announce Type: new 
Abstract: The functional delta-method has a wide range of applications in statistics. Applications on functionals of empirical processes yield various limit results for classical statistics. To improve the finite sample properties of statistical inference procedures that are based on the limit results, resampling procedures such as random permutation and bootstrap methods are a popular solution. In order to analyze the behaviour of the functionals of the resampling empirical processes, corresponding conditional functional delta-methods are desirable. While conditional functional delta-methods for some special cases already exist, there is a lack of more general conditional functional delta-methods for resampling procedures for empirical processes, such as the permutation and pooled bootstrap method. This gap is addressed in the present paper. Thereby, a general multiple sample problem is considered. The flexible application of the developed conditional delta-method is shown in various relevant examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10856v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Merle Munko, Dennis Dobler</dc:creator>
    </item>
    <item>
      <title>Highly Multivariate High-dimensionality Spatial Stochastic Processes-A Mixed Conditional Approach</title>
      <link>https://arxiv.org/abs/2408.10396</link>
      <description>arXiv:2408.10396v1 Announce Type: cross 
Abstract: We propose a hybrid mixed spatial graphical model framework and novel concepts, e.g., cross-Markov Random Field (cross-MRF), to comprehensively address all feature aspects of highly multivariate high-dimensionality (HMHD) spatial data class when constructing the desired joint variance and precision matrix (where both p and n are large). Specifically, the framework accommodates any customized conditional independence (CI) among any number of p variate fields at the first stage, alleviating dynamic memory burden. Meanwhile, it facilitates parallel generation of covariance and precision matrix, with the latter's generation order scaling only linearly in p. In the second stage, we demonstrate the multivariate Hammersley-Clifford theorem from a column-wise conditional perspective and unearth the existence of cross-MRF. The link of the mixed spatial graphical framework and the cross-MRF allows for a mixed conditional approach, resulting in the sparsest possible representation of the precision matrix via accommodating the doubly CI among both p and n, with the highest possible exact-zero-value percentage. We also explore the possibility of the co-existence of geostatistical and MRF modelling approaches in one unified framework, imparting a potential solution to an open problem. The derived theories are illustrated with 1D simulation and 2D real-world spatial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10396v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoqing Chen, Gavin Shaddick</dc:creator>
    </item>
    <item>
      <title>Asymptotic Classification Error for Heavy-Tailed Renewal Processes</title>
      <link>https://arxiv.org/abs/2408.10502</link>
      <description>arXiv:2408.10502v1 Announce Type: cross 
Abstract: Despite the widespread occurrence of classification problems and the increasing collection of point process data across many disciplines, study of error probability for point process classification only emerged very recently. Here, we consider classification of renewal processes. We obtain asymptotic expressions for the Bhattacharyya bound on misclassification error probabilities for heavy-tailed renewal processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10502v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xinhui Rong, Victor Solo</dc:creator>
    </item>
    <item>
      <title>Conditional nonparametric variable screening by neural factor regression</title>
      <link>https://arxiv.org/abs/2408.10825</link>
      <description>arXiv:2408.10825v1 Announce Type: cross 
Abstract: High-dimensional covariates often admit linear factor structure. To effectively screen correlated covariates in high-dimension, we propose a conditional variable screening test based on non-parametric regression using neural networks due to their representation power. We ask the question whether individual covariates have additional contributions given the latent factors or more generally a set of variables. Our test statistics are based on the estimated partial derivative of the regression function of the candidate variable for screening and a observable proxy for the latent factors. Hence, our test reveals how much predictors contribute additionally to the non-parametric regression after accounting for the latent factors. Our derivative estimator is the convolution of a deep neural network regression estimator and a smoothing kernel. We demonstrate that when the neural network size diverges with the sample size, unlike estimating the regression function itself, it is necessary to smooth the partial derivative of the neural network estimator to recover the desired convergence rate for the derivative. Moreover, our screening test achieves asymptotic normality under the null after finely centering our test statistics that makes the biases negligible, as well as consistency for local alternatives under mild conditions. We demonstrate the performance of our test in a simulation study and two real world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10825v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianqing Fan (Princeton University), Weining Wang (University of Groningen), Yue Zhao (University of York)</dc:creator>
    </item>
    <item>
      <title>Algebraic algorithm for direct sampling from toric models and hypergeometric functions</title>
      <link>https://arxiv.org/abs/2110.14992</link>
      <description>arXiv:2110.14992v2 Announce Type: replace 
Abstract: We show that Pfaffians or contiguity relations of hypergeometric functions of several variables give a direct sampling algorithm from toric models in statistics, which is a Markov chain on a lattice generated by a matrix $A$. A correspondence among graphical models and $A$-hypergeometric system is discussed and we give a sum formula of special values of $A$-hypergeometric polynomials. Some hypergeometric series which are interesting in view of statistics are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.14992v2</guid>
      <category>math.ST</category>
      <category>math.CA</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuhei Mano, Nobuki Takayama</dc:creator>
    </item>
    <item>
      <title>Specification tests for normal/gamma and stable/gamma stochastic frontier models based on empirical transforms</title>
      <link>https://arxiv.org/abs/2212.08867</link>
      <description>arXiv:2212.08867v2 Announce Type: replace 
Abstract: Goodness--of--fit tests for the distribution of the composed error term in a Stochastic Frontier Model (SFM) are suggested. The focus is on the case of a normal/gamma SFM and the heavy--tailed stable/gamma SFM. In the first case the moment generating function is used as tool while in the latter case the characteristic function of the error term is employed. In both cases our test statistics are formulated as weighted integrals of properly standardized data. The new normal/gamma test is consistent, and is shown to have an intrinsic relation to moment--based tests. The finite--sample behavior of resampling versions of both tests is investigated by Monte Carlo simulation, while several real--data applications are also included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.08867v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christos K. Papadimitriou, Simos G. Meintanis, Bernardo B. Andrade, Mike G. Tsionas</dc:creator>
    </item>
    <item>
      <title>Dimension-free uniform concentration bound for logistic regression</title>
      <link>https://arxiv.org/abs/2405.18055</link>
      <description>arXiv:2405.18055v4 Announce Type: replace 
Abstract: We provide a novel dimension-free uniform concentration bound for the empirical risk function of constrained logistic regression. Our bound yields a milder sufficient condition for a uniform law of large numbers than conditions derived by the Rademacher complexity argument and McDiarmid's inequality. The derivation is based on the PAC-Bayes approach with second-order expansion and Rademacher-complexity-based bounds for the residual term of the expansion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18055v4</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shogo Nakakita</dc:creator>
    </item>
    <item>
      <title>Sequential Bayesian Neural Subnetwork Ensembles</title>
      <link>https://arxiv.org/abs/2206.00794</link>
      <description>arXiv:2206.00794v2 Announce Type: replace-cross 
Abstract: Deep ensembles have emerged as a powerful technique for improving predictive performance and enhancing model robustness across various applications by leveraging model diversity. However, traditional deep ensemble methods are often computationally expensive and rely on deterministic models, which may limit their flexibility. Additionally, while sparse subnetworks of dense models have shown promise in matching the performance of their dense counterparts and even enhancing robustness, existing methods for inducing sparsity typically incur training costs comparable to those of training a single dense model, as they either gradually prune the network during training or apply thresholding post-training. In light of these challenges, we propose an approach for sequential ensembling of dynamic Bayesian neural subnetworks that consistently maintains reduced model complexity throughout the training process while generating diverse ensembles in a single forward pass. Our approach involves an initial exploration phase to identify high-performing regions within the parameter space, followed by multiple exploitation phases that take advantage of the compactness of the sparse model. These exploitation phases quickly converge to different minima in the energy landscape, corresponding to high-performing subnetworks that together form a diverse and robust ensemble. We empirically demonstrate that our proposed approach outperforms traditional dense and sparse deterministic and Bayesian ensemble models in terms of prediction accuracy, uncertainty estimation, out-of-distribution detection, and adversarial robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.00794v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanket Jantre, Shrijita Bhattacharya, Nathan M. Urban, Byung-Jun Yoon, Tapabrata Maiti, Prasanna Balaprakash, Sandeep Madireddy</dc:creator>
    </item>
    <item>
      <title>Projectivity revisited</title>
      <link>https://arxiv.org/abs/2207.00625</link>
      <description>arXiv:2207.00625v4 Announce Type: replace-cross 
Abstract: The behaviour of statistical relational representations across differently sized domains has become a focal area of research from both a modelling and a complexity viewpoint.Recently, projectivity of a family of distributions emerged as a key property, ensuring that marginal probabilities are independent of the domain size. However, the formalisation used currently assumes that the domain is characterised only by its size. This contribution extends the notion of projectivity from families of distributions indexed by domain size to functors taking extensional data from a database. This makes projectivity available for the large range of applications taking structured input. We transfer key known results on projective families of distributions to the new setting. This includes a characterisation of projective fragments in different statistical relational formalisms as well as a general representation theorem for projective families of distributions. Furthermore, we prove a correspondence between projectivity and distributions on countably infinite domains, which we use to unify and generalise earlier work on statistical relational representations in infinite domains. Finally, we use the extended notion of projectivity to define a further strengthening, which we call $\sigma$-projectivity, and which allows the use of the same representation in different modes while retaining projectivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.00625v4</guid>
      <category>cs.AI</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ijar.2023.109031</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Approximate Reasoning 162:109031 (2023)</arxiv:journal_reference>
      <dc:creator>Felix Weitk\"amper</dc:creator>
    </item>
    <item>
      <title>A New measure of income inequality</title>
      <link>https://arxiv.org/abs/2310.02273</link>
      <description>arXiv:2310.02273v2 Announce Type: replace-cross 
Abstract: A new measure of income inequality that captures the heavy tail behavior of the income distribution is proposed. We discuss two different approaches to find the estimators of the proposed measure. We show that these estimators are consistent and have an asymptotically normal distribution. We also obtain a jackknife empirical likelihood (JEL) confidence interval of the income inequality measure. A Monte Carlo simulation study is conducted to evaluate the finite sample properties of the estimators and JEL-based confidence inerval. Finally, we use our measure to study the income inequality of three states in India.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02273v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Sudheesh K Kattumannil, Saparya Suresh</dc:creator>
    </item>
    <item>
      <title>Inferring Dynamic Networks from Marginals with Iterative Proportional Fitting</title>
      <link>https://arxiv.org/abs/2402.18697</link>
      <description>arXiv:2402.18697v2 Announce Type: replace-cross 
Abstract: A common network inference problem, arising from real-world data constraints, is how to infer a dynamic network from its time-aggregated adjacency matrix and time-varying marginals (i.e., row and column sums). Prior approaches to this problem have repurposed the classic iterative proportional fitting (IPF) procedure, also known as Sinkhorn's algorithm, with promising empirical results. However, the statistical foundation for using IPF has not been well understood: under what settings does IPF provide principled estimation of a dynamic network from its marginals, and how well does it estimate the network? In this work, we establish such a setting, by identifying a generative network model whose maximum likelihood estimates are recovered by IPF. Our model both reveals implicit assumptions on the use of IPF in such settings and enables new analyses, such as structure-dependent error bounds on IPF's parameter estimates. When IPF fails to converge on sparse network data, we introduce a principled algorithm that guarantees IPF converges under minimal changes to the network structure. Finally, we conduct experiments with synthetic and real-world data, which demonstrate the practical value of our theoretical and algorithmic contributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18697v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 41st International Conference on Machine Learning, PMLR 235:6202-6252, 2024</arxiv:journal_reference>
      <dc:creator>Serina Chang, Frederic Koehler, Zhaonan Qu, Jure Leskovec, Johan Ugander</dc:creator>
    </item>
    <item>
      <title>Simple and Nearly-Optimal Sampling for Rank-1 Tensor Completion via Gauss-Jordan</title>
      <link>https://arxiv.org/abs/2408.05431</link>
      <description>arXiv:2408.05431v2 Announce Type: replace-cross 
Abstract: We revisit the sample and computational complexity of completing a rank-1 tensor in $\otimes_{i=1}^{N} \mathbb{R}^{d}$, given a uniformly sampled subset of its entries. We present a characterization of the problem (i.e. nonzero entries) which admits an algorithm amounting to Gauss-Jordan on a pair of random linear systems. For example, when $N = \Theta(1)$, we prove it uses no more than $m = O(d^2 \log d)$ samples and runs in $O(md^2)$ time. Moreover, we show any algorithm requires $\Omega(d\log d)$ samples.
  By contrast, existing upper bounds on the sample complexity are at least as large as $d^{1.5} \mu^{\Omega(1)} \log^{\Omega(1)} d$, where $\mu$ can be $\Theta(d)$ in the worst case. Prior work obtained these looser guarantees in higher rank versions of our problem, and tend to involve more complicated algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05431v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Gomez-Leos, Oscar L\'opez</dc:creator>
    </item>
  </channel>
</rss>
