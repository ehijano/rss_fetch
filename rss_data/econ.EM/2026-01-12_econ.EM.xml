<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 12 Jan 2026 05:00:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>From Unstructured Data to Demand Counterfactuals: Theory and Practice</title>
      <link>https://arxiv.org/abs/2601.05374</link>
      <description>arXiv:2601.05374v1 Announce Type: new 
Abstract: Empirical models of demand for differentiated products rely on low-dimensional product representations to capture substitution patterns. These representations are increasingly proxied by applying ML methods to high-dimensional, unstructured data, including product descriptions and images. When proxies fail to capture the true dimensions of differentiation that drive substitution, standard workflows will deliver biased counterfactuals and invalid inference. We develop a practical toolkit that corrects this bias and ensures valid inference for a broad class of counterfactuals. Our approach applies to market-level and/or individual data, requires minimal additional computation, is efficient, delivers simple formulas for standard errors, and accommodates data-dependent proxies, including embeddings from fine-tuned ML models. It can also be used with standard quantitative attributes when mismeasurement is a concern. In addition, we propose diagnostics to assess the adequacy of the proxy construction and dimension. The approach yields meaningful improvements in predicting counterfactual substitution in both simulations and an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05374v1</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timothy Christensen, Giovanni Compiani</dc:creator>
    </item>
    <item>
      <title>Event Studies with Feedback</title>
      <link>https://arxiv.org/abs/2601.05493</link>
      <description>arXiv:2601.05493v1 Announce Type: new 
Abstract: Event studies often conflate direct treatment effects with indirect effects operating through endogenous covariate adjustment. We develop a dynamic panel event study framework that separates these effects. The framework allows for persistent outcomes and treatment effects and for covariates that respond to past outcomes and treatment exposure. Under sequential exogeneity and homogeneous feedback, we establish point identification of common parameters governing outcome and treatment effect dynamics, the distribution of heterogeneous treatment effects, and the covariate feedback process. We propose an algorithm for dynamic decomposition that enables researchers to assess the relative importance of each effect in driving treatment effect dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05493v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Irene Botosaru, Laura Liu</dc:creator>
    </item>
    <item>
      <title>Dynamic Mortality Forecasting via Mixed-Frequency State-Space Models</title>
      <link>https://arxiv.org/abs/2601.05702</link>
      <description>arXiv:2601.05702v1 Announce Type: new 
Abstract: High-frequency death counts are now widely available and contain timely information about intra-year mortality dynamics, but most stochastic mortality models are still estimated on annual data and therefore update only when annual totals are released. We propose a mixed-frequency state-space (MF--SS) extension of the Lee--Carter framework that jointly uses annual mortality rates and monthly death counts. The two series are linked through a shared latent monthly mortality factor, with the annual period factor defined as the intra-year average of the monthly factors. The latent monthly factor follows a seasonal ARIMA process, and parameters are estimated by maximum likelihood using an EM algorithm with Kalman filtering and smoothing. This setup enables real-time intra-year updates of the latent state and forecasts as new monthly observations arrive without re-estimating model parameters.
  Using U.S. data for ages 20--90 over 1999--2019, we evaluate intra-year annual nowcasts and one- to five-year-ahead forecasts. The MF--SS model produces both a direct annual forecast and an annual forecast implied by aggregating monthly projections. In our application, the aggregated monthly forecast is typically more accurate. Incorporating monthly information substantially improves intra-year annual nowcasts, especially after the first few months of the year. As a benchmark, we also fit separate annual and monthly Lee--Carter models and combine their forecasts using temporal reconciliation. Reconciliation improves these independent forecasts but adds little to MF--SS forecasts, consistent with MF--SS pooling information across frequencies during estimation. The MF--SS aggregated monthly forecasts generally outperform both unreconciled and temporally reconciled Lee--Carter forecasts and produce more cautious predictive intervals than the reconciled Lee--Carter approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05702v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Runze Li, Rui Zhou, David Pitt</dc:creator>
    </item>
    <item>
      <title>Learning and Testing Exposure Mappings of Interference using Graph Convolutional Autoencoder</title>
      <link>https://arxiv.org/abs/2601.05728</link>
      <description>arXiv:2601.05728v1 Announce Type: new 
Abstract: Interference or spillover effects arise when an individual's outcome (e.g., health) is influenced not only by their own treatment (e.g., vaccination) but also by the treatment of others, creating challenges for evaluating treatment effects. Exposure mappings provide a framework to study such interference by explicitly modeling how the treatment statuses of contacts within an individual's network affect their outcome. Most existing research relies on a priori exposure mappings of limited complexity, which may fail to capture the full range of interference effects. In contrast, this study applies a graph convolutional autoencoder to learn exposure mappings in a data-driven way, which exploit dependencies and relations within a network to more accurately capture interference effects. As our main contribution, we introduce a machine learning-based test for the validity of exposure mappings and thus test the identification of the direct effect. In this testing approach, the learned exposure mapping is used as an instrument to test the validity of a simple, user-defined exposure mapping. The test leverages the fact that, if the user-defined exposure mapping is valid (so that all interference operates through it), then the learned exposure mapping is statistically independent of any individual's outcome, conditional on the user-defined exposure mapping. We assess the finite-sample performance of this proposed validity test through a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05728v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Huber, Jannis Kueck, Mara Mattes</dc:creator>
    </item>
    <item>
      <title>How Carbon Border Adjustment Mechanism is Energizing the EU Carbon Market and Industrial Transformation</title>
      <link>https://arxiv.org/abs/2601.05490</link>
      <description>arXiv:2601.05490v1 Announce Type: cross 
Abstract: The global carbon market is fragmented and characterized by limited pricing transparency and empirical evidence, creating challenges for investors and policymakers in identifying carbon management opportunities. The European Union is among several regions that have implemented emissions pricing through an Emissions Trading System (EU ETS). While the EU ETS has contributed to emissions reductions, it has also raised concerns related to international competitiveness and carbon leakage, particularly given the strong integration of EU industries into global value chains. To address these challenges, the European Commission proposed the Carbon Border Adjustment Mechanism (CBAM) in 2021. CBAM is designed to operate alongside the EU ETS by applying a carbon price to selected imported goods, thereby aligning carbon costs between domestic and foreign producers. It will gradually replace existing carbon leakage mitigation measures, including the allocation of free allowances under the EU ETS. The initial scope of CBAM covers electricity, cement, fertilizer, aluminium, iron, and steel. As climate policies intensify under the Paris Agreement, CBAM-like mechanisms are expected to play an increasingly important role in managing carbon-related trade risks and supporting the transition to net zero emissions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05490v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>econ.EM</category>
      <category>stat.OT</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Joseph Nyangon, Brecht Seifi</dc:creator>
    </item>
    <item>
      <title>Probabilistic Targeted Factor Analysis</title>
      <link>https://arxiv.org/abs/2412.06688</link>
      <description>arXiv:2412.06688v5 Announce Type: replace 
Abstract: We develop Probabilistic Targeted Factor Analysis (PTFA), a likelihood-based framework for constructing latent factors that are explicitly targeted to variables of economic interest. PTFA provides a probabilistic foundation for Partial Least Squares, allowing supervised factor extraction under uncertainty. The model is estimated via a fast expectation maximization algorithm and naturally accommodates missing data, mixed-frequency observations, stochastic volatility, and factor dynamics. Simulation evidence shows that PTFA improves recovery of economically relevant latent factors relative to standard PLS, particularly in noisy environments. Applications to financial conditions indices, macroeconomic forecasting, and equity premium prediction illustrate the measurement and forecasting gains delivered by targeted probabilistic factor extraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06688v5</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Miguel C. Herculano, Santiago Montoya-Bland\'on</dc:creator>
    </item>
    <item>
      <title>Epsilon-Minimax Solutions of Statistical Decision Problems</title>
      <link>https://arxiv.org/abs/2509.08107</link>
      <description>arXiv:2509.08107v2 Announce Type: replace 
Abstract: A decision rule is epsilon-minimax if it is minimax up to an additive factor epsilon. We present an algorithm for provably obtaining epsilon-minimax solutions for a class of statistical decision problems. In particular, we are interested in problems where the statistician chooses randomly among I decision rules. The minimax solution of these problems admits a convex programming representation over the (I-1)-simplex. Our suggested algorithm is a well-known mirror subgradient descent routine, designed to approximately solve the convex optimization problem that defines the minimax decision rule. This iterative routine is known in the computer science literature as the hedge algorithm and is used in algorithmic game theory as a practical tool to find approximate solutions of two-person zero-sum games. We apply the suggested algorithm to different minimax problems in the econometrics literature. An empirical application to the problem of optimally selecting sites to maximize the external validity of an experimental policy evaluation illustrates the usefulness of the suggested procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08107v2</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andr\'es Aradillas Fern\'andez, Jos\'e Blanchet, Jos\'e Luis Montiel Olea, Chen Qiu, J\"org Stoye, Lezhi Tan</dc:creator>
    </item>
    <item>
      <title>Technology Adoption and Network Externalities in Financial Systems: A Spatial-Network Approach</title>
      <link>https://arxiv.org/abs/2601.04246</link>
      <description>arXiv:2601.04246v2 Announce Type: replace 
Abstract: This paper develops a unified framework for analyzing technology adoption in financial networks that incorporates spatial spillovers, network externalities, and their interaction. The framework characterizes adoption dynamics through a master equation whose solution admits a Feynman-Kac representation as expected cumulative adoption pressure along stochastic paths through spatial-network space. From this representation, I derive the Adoption Amplification Factor -- a structural measure of technology leadership that captures the ratio of total system-wide adoption to initial adoption following a localized shock. A Levy jump-diffusion extension with state-dependent jump intensity captures critical mass dynamics: below threshold, adoption evolves through gradual diffusion; above threshold, cascade dynamics accelerate adoption through discrete jumps. Applying the framework to SWIFT gpi adoption among 17 Global Systemically Important Banks, I find strong support for the two-regime characterization. Network-central banks adopt significantly earlier ($\rho = -0.69$, $p = 0.002$), and pre-threshold adopters have significantly higher amplification factors than post-threshold adopters (11.81 versus 7.83, $p = 0.010$). Founding members, representing 29 percent of banks, account for 39 percent of total system amplification -- sufficient to trigger cascade dynamics. Controlling for firm size and network position, CEO age delays adoption by 11-15 days per year.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04246v2</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>q-fin.GN</category>
      <category>q-fin.TR</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
  </channel>
</rss>
