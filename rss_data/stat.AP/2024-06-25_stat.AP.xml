<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Jun 2024 04:01:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Comparison of methods for mediation analysis with multiple correlated mediators</title>
      <link>https://arxiv.org/abs/2406.16174</link>
      <description>arXiv:2406.16174v1 Announce Type: new 
Abstract: Various methods have emerged for conducting mediation analyses with multiple correlated mediators, each with distinct strengths and limitations. However, a comparative evaluation of these methods is lacking, providing the motivation for this paper. This study examines six mediation analysis methods for multiple correlated mediators that provide insights to the contributors for health disparities. We assessed the performance of each method in identifying joint or path-specific mediation effects in the context of binary outcome variables varying mediator types and levels of residual correlation between mediators. Through comprehensive simulations, the performance of six methods in estimating joint and/or path-specific mediation effects was assessed rigorously using a variety of metrics including bias, mean squared error, coverage and width of the 95$\%$ confidence intervals. Subsequently, these methods were applied to the REasons for Geographic And Racial Differences in Stroke (REGARDS) study, where differing conclusions were obtained depending on the mediation method employed. This evaluation provides valuable guidance for researchers grappling with complex multi-mediator scenarios, enabling them to select an optimal mediation method for their research question and dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16174v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mary Appah, D. Leann Long, George Howard, Melissa J. Smith</dc:creator>
    </item>
    <item>
      <title>Spatially Structured Regression for Non-conformable Spaces: Integrating Pathology Imaging and Genomics Data in Cancer</title>
      <link>https://arxiv.org/abs/2406.16721</link>
      <description>arXiv:2406.16721v1 Announce Type: new 
Abstract: The spatial composition and cellular heterogeneity of the tumor microenvironment plays a critical role in cancer development and progression. High-definition pathology imaging of tumor biopsies provide a high-resolution view of the spatial organization of different types of cells. This allows for systematic assessment of intra- and inter-patient spatial cellular interactions and heterogeneity by integrating accompanying patient-level genomics data. However, joint modeling across tumor biopsies presents unique challenges due to non-conformability (lack of a common spatial domain across biopsies) as well as high-dimensionality. To address this problem, we propose the Dual random effect and main effect selection model for Spatially structured regression model (DreameSpase). DreameSpase employs a Bayesian variable selection framework that facilitates the assessment of spatial heterogeneity with respect to covariates both within (through fixed effects) and between spaces (through spatial random effects) for non-conformable spatial domains. We demonstrate the efficacy of DreameSpase via simulations and integrative analyses of pathology imaging and gene expression data obtained from $335$ melanoma biopsies. Our findings confirm several existing relationships, e.g. neutrophil genes being associated with both inter- and intra-patient spatial heterogeneity, as well as discovering novel associations. We also provide freely available and computationally efficient software for implementing DreameSpase.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16721v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Osher, Jian Kang, Arvind Rao, Veerabhadran Baladandayuthapani</dc:creator>
    </item>
    <item>
      <title>Unveiling Activity-Travel Patterns through Topological Data Analysis</title>
      <link>https://arxiv.org/abs/2406.16742</link>
      <description>arXiv:2406.16742v1 Announce Type: new 
Abstract: In the context of rapid urbanization, understanding the patterns of urban residents' activities and mobility is crucial for optimizing transportation systems and enhancing urban management efficiency. This study addresses the limitations of traditional travel analysis methods in handling high-dimensional and large-scale spatiotemporal data by incorporating Topological Data Analysis (TDA) techniques, specifically using persistent homology. This method allows for the extraction of information from the topological structure of data, enabling the effective identification and analysis of complex spatiotemporal behavior patterns without reducing the data's dimensionality. We utilized mobile signaling data from a community in Shenzhen, which includes detailed geographic and temporal information, providing an ideal sample for analyzing urban residents' behavior patterns. Using our pattern mining framework, we successfully identified five main patterns of residents' activities and travel, revealing daily behavioral habits and reflecting the activity heterogeneity among residents with different socio-economic attributes. These findings not only assist urban planners in better session design but also provide new characteristics for predictive mobility models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16742v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nuoxian Huang, Yulin Wu</dc:creator>
    </item>
    <item>
      <title>Valuation methods for professional sports clubs: A historical review, a model development, and the application to Japanese football clubs</title>
      <link>https://arxiv.org/abs/2406.16773</link>
      <description>arXiv:2406.16773v1 Announce Type: new 
Abstract: In the trend towards the globalization of football and the increasing commercialization of professional football clubs, a methodology for calculating the firm value of clubs in non-western countries has yet to be established. This study reviews the valuation methods for the club firm values in Europe and North America and how values are calculated at the time of changing ownership of Japanese clubs and develops regression models with higher explanatory power than before to estimate the more accurate firm value of Japanese football clubs. A review of the existing literature on methods for calculating the firm value of professional sports clubs in Europe and North America, as well as financial statements and registers relating to changes of ownership of Japanese clubs, was conducted. After that, multiple regression analyses were conducted using the firm value of European clubs as the explained variable. From the literature review and the Japanese case studies, it has become clear that European clubs' standard valuation methods are based on revenue and other factors, while in Japan, valuation is based solely on the par value of stocks or net assets. Multiple regression analysis revealed that the firm value of European clubs over the past three years is best explained by revenue or player market value and the number of SNS followers. Two models with high explanatory power were developed. The estimated firm value using the revenue-based formula was higher than the one based on player market value. However, in the J.League, the former was more than three times higher than the latter, while the former was only 1.2 times higher for European clubs. The discrepancy relates to differences in European and J.League clubs' revenues and asset structures. In either formula, the firm value of J.League clubs exceeded the actual transaction price when the change of ownership occurred in the past.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16773v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masaaki Kimura, Zen Walsh, Takuo Inoue, Toshiya Takahashi, Hideki Koizumi</dc:creator>
    </item>
    <item>
      <title>Practical privacy metrics for synthetic data</title>
      <link>https://arxiv.org/abs/2406.16826</link>
      <description>arXiv:2406.16826v1 Announce Type: new 
Abstract: This paper explains how the synthpop package for R has been extended to include functions to calculate measures of identity and attribute disclosure risk for synthetic data that measure risks for the records used to create the synthetic data. The basic function, disclosure, calculates identity disclosure for a set of quasi-identifiers (keys) and attribute disclosure for one variable specified as a target from the same set of keys. The second function, disclosure.summary, is a wrapper for the first and presents summary results for a set of targets. This short paper explains the measures of disclosure risk and documents how they are calculated. We recommend two measures: $RepU$ (replicated uniques) for identity disclosure and $DiSCO$ (Disclosive in Synthetic Correct Original) for attribute disclosure. Both are expressed a \% of the original records and each can be compared to similar measures calculated from the original data. Experience with using the functions on real data found that some apparent disclosures could be identified as coming from relationships in the data that would be expected to be known to anyone familiar with its features. We flag cases when this seems to have occurred and provide means of excluding them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16826v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gillian M Raab, Beata Nowok, Chris Dibben</dc:creator>
    </item>
    <item>
      <title>What is Best for Students, Numerical Scores or Letter Grades?</title>
      <link>https://arxiv.org/abs/2406.15405</link>
      <description>arXiv:2406.15405v1 Announce Type: cross 
Abstract: We study letter grading schemes, which are routinely employed for evaluating student performance. Typically, a numerical score obtained via one or more evaluations is converted into a letter grade (e.g., A+, B-, etc.) by associating a disjoint interval of numerical scores to each letter grade.
  We propose the first model for studying the (de)motivational effects of such grading on the students and, consequently, on their performance in future evaluations. We use the model to compare uniform letter grading schemes, in which the range of scores is divided into equal-length parts that are mapped to the letter grades, to numerical scoring, in which the score is not converted to any letter grade (equivalently, every score is its own letter grade).
  Theoretically, we identify realistic conditions under which numerical scoring is better than any uniform letter grading scheme. Our experiments confirm that this holds under even weaker conditions, but also find cases where the converse occurs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15405v1</guid>
      <category>cs.GT</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Evi Micha, Shreyas Sekar, Nisarg Shah</dc:creator>
    </item>
    <item>
      <title>Heterogeneous peer effects of college roommates on academic performance</title>
      <link>https://arxiv.org/abs/2406.15439</link>
      <description>arXiv:2406.15439v1 Announce Type: cross 
Abstract: Understanding how student peers influence learning outcomes is crucial for effective education management in complex social systems. The complexities of peer selection and evolving peer relationships, however, pose challenges for identifying peer effects using static observational data. Here we use both null-model and regression approaches to examine peer effects using longitudinal data from 5,272 undergraduates, where roommate assignments are plausibly random upon enrollment and roommate relationships persist until graduation. Specifically, we construct a roommate null model by randomly shuffling students among dorm rooms and introduce an assimilation metric to quantify similarities in roommate academic performance. We find significantly larger assimilation in actual data than in the roommate null model, suggesting roommate peer effects, whereby roommates have more similar performance than expected by chance alone. Moreover, assimilation exhibits an overall increasing trend over time, suggesting that peer effects become stronger the longer roommates live together. Our regression analysis further reveals the moderating role of peer heterogeneity. In particular, when roommates perform similarly, the positive relationship between a student's future performance and their roommates' average prior performance is more pronounced, and their ordinal rank in the dorm room has an independent effect. Our findings contribute to understanding the role of college roommates in influencing student academic performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15439v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Cao, Tao Zhou, Jian Gao</dc:creator>
    </item>
    <item>
      <title>Statistical Inference and A/B Testing in Fisher Markets and Paced Auctions</title>
      <link>https://arxiv.org/abs/2406.15522</link>
      <description>arXiv:2406.15522v1 Announce Type: cross 
Abstract: We initiate the study of statistical inference and A/B testing for two market equilibrium models: linear Fisher market (LFM) equilibrium and first-price pacing equilibrium (FPPE). LFM arises from fair resource allocation systems such as allocation of food to food banks and notification opportunities to different types of notifications. For LFM, we assume that the data observed is captured by the classical finite-dimensional Fisher market equilibrium, and its steady-state behavior is modeled by a continuous limit Fisher market. The second type of equilibrium we study, FPPE, arises from internet advertising where advertisers are constrained by budgets and advertising opportunities are sold via first-price auctions. For platforms that use pacing-based methods to smooth out the spending of advertisers, FPPE provides a hindsight-optimal configuration of the pacing method. We propose a statistical framework for the FPPE model, in which a continuous limit FPPE models the steady-state behavior of the auction platform, and a finite FPPE provides the data to estimate primitives of the limit FPPE. Both LFM and FPPE have an Eisenberg-Gale convex program characterization, the pillar upon which we derive our statistical theory. We start by deriving basic convergence results for the finite market to the limit market. We then derive asymptotic distributions, and construct confidence intervals. Furthermore, we establish the asymptotic local minimax optimality of estimation based on finite markets. We then show that the theory can be used for conducting statistically valid A/B testing on auction platforms. Synthetic and semi-synthetic experiments verify the validity and practicality of our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15522v1</guid>
      <category>cs.GT</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luofeng Liao, Christian Kroer</dc:creator>
    </item>
    <item>
      <title>Graphical copula GARCH modeling with dynamic conditional dependence</title>
      <link>https://arxiv.org/abs/2406.15582</link>
      <description>arXiv:2406.15582v1 Announce Type: cross 
Abstract: Modeling returns on large portfolios is a challenging problem as the number of parameters in the covariance matrix grows as the square of the size of the portfolio. Traditional correlation models, for example, the dynamic conditional correlation (DCC)-GARCH model, often ignore the nonlinear dependencies in the tail of the return distribution. In this paper, we aim to develop a framework to model the nonlinear dependencies dynamically, namely the graphical copula GARCH (GC-GARCH) model. Motivated from the capital asset pricing model, to allow modeling of large portfolios, the number of parameters can be greatly reduced by introducing conditional independence among stocks given some risk factors. The joint distribution of the risk factors is factorized using a directed acyclic graph (DAG) with pair-copula construction (PCC) to enhance the modeling of the tails of the return distribution while offering the flexibility of having complex dependent structures. The DAG induces topological orders to the risk factors, which can be regarded as a list of directions of the flow of information. The conditional distributions among stock returns are also modeled using PCC. Dynamic conditional dependence structures are incorporated to allow the parameters in the copulas to be time-varying. Three-stage estimation is used to estimate parameters in the marginal distributions, the risk factor copulas, and the stock copulas. The simulation study shows that the proposed estimation procedure can estimate the parameters and the underlying DAG structure accurately. In the investment experiment of the empirical study, we demonstrate that the GC-GARCH model produces more precise conditional value-at-risk prediction and considerably higher cumulative portfolio returns than the DCC-GARCH model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15582v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lupe Shun Hin Chan, Amanda Man Ying Chu, Mike Ka Pui So</dc:creator>
    </item>
    <item>
      <title>Bayesian modeling of multi-species labeling errors in ecological studies</title>
      <link>https://arxiv.org/abs/2406.15844</link>
      <description>arXiv:2406.15844v1 Announce Type: cross 
Abstract: Ecological and conservation studies monitoring bird communities typically rely on species classification based on bird vocalizations. Historically, this has been based on expert volunteers going into the field and making lists of the bird species that they observe. Recently, machine learning algorithms have emerged that can accurately classify bird species based on audio recordings of their vocalizations. Such algorithms crucially rely on training data that are labeled by experts. Automated classification is challenging when multiple species are vocalizing simultaneously, there is background noise, and/or the bird is far from the microphone. In continuously monitoring different locations, the size of the audio data become immense and it is only possible for human experts to label a tiny proportion of the available data. In addition, experts can vary in their accuracy and breadth of knowledge about different species. This article focuses on the important problem of combining sparse expert annotations to improve bird species classification while providing uncertainty quantification. We additionally are interested in providing expert performance scores to increase their engagement and encourage improvements. We propose a Bayesian hierarchical modeling approach and evaluate this approach on a new community science platform developed in Finland.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15844v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoxuan Wang, Patrik Lauha, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Multistep Criticality Search and Power Shaping in Microreactors with Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2406.15931</link>
      <description>arXiv:2406.15931v1 Announce Type: cross 
Abstract: Reducing operation and maintenance costs is a key objective for advanced reactors in general and microreactors in particular. To achieve this reduction, developing robust autonomous control algorithms is essential to ensure safe and autonomous reactor operation. Recently, artificial intelligence and machine learning algorithms, specifically reinforcement learning (RL) algorithms, have seen rapid increased application to control problems, such as plasma control in fusion tokamaks and building energy management. In this work, we introduce the use of RL for intelligent control in nuclear microreactors. The RL agent is trained using proximal policy optimization (PPO) and advantage actor-critic (A2C), cutting-edge deep RL techniques, based on a high-fidelity simulation of a microreactor design inspired by the Westinghouse eVinci\textsuperscript{TM} design. We utilized a Serpent model to generate data on drum positions, core criticality, and core power distribution for training a feedforward neural network surrogate model. This surrogate model was then used to guide a PPO and A2C control policies in determining the optimal drum position across various reactor burnup states, ensuring critical core conditions and symmetrical power distribution across all six core portions. The results demonstrate the excellent performance of PPO in identifying optimal drum positions, achieving a hextant power tilt ratio of approximately 1.002 (within the limit of $&lt;$ 1.02) and maintaining criticality within a 10 pcm range. A2C did not provide as competitive of a performance as PPO in terms of performance metrics for all burnup steps considered in the cycle. Additionally, the results highlight the capability of well-trained RL control policies to quickly identify control actions, suggesting a promising approach for enabling real-time autonomous control through digital twins.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15931v1</guid>
      <category>eess.SY</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Majdi I. Radaideh, Leo Tunkle, Dean Price, Kamal Abdulraheem, Linyu Lin, Moutaz Elias</dc:creator>
    </item>
    <item>
      <title>Exploring the difficulty of estimating win probability: a simulation study</title>
      <link>https://arxiv.org/abs/2406.16171</link>
      <description>arXiv:2406.16171v1 Announce Type: cross 
Abstract: Estimating win probability is one of the classic modeling tasks of sports analytics. Many widely used win probability estimators are statistical win probability models, which fit the relationship between a binary win/loss outcome variable and certain game-state variables using data-driven regression or machine learning approaches. To illustrate just how difficult it is to accurately fit a statistical win probability model from noisy and highly correlated observational data, in this paper we conduct a simulation study. We create a simplified random walk version of football in which true win probability at each game-state is known, and we see how well a model recovers it. We find that the dependence structure of observational play-by-play data substantially inflates the bias and variance of estimators and lowers the effective sample size. This makes it essential to quantify uncertainty in win probability estimates, but typical bootstrapped confidence intervals are too narrow and don't achieve nominal coverage. Hence, we introduce a novel method, the fractional bootstrap, to calibrate these intervals to achieve adequate coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16171v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan S. Brill, Ronald Yurko, Abraham J. Wyner</dc:creator>
    </item>
    <item>
      <title>Reinterpreting Economic Complexity: A co-clustering approach</title>
      <link>https://arxiv.org/abs/2406.16199</link>
      <description>arXiv:2406.16199v1 Announce Type: cross 
Abstract: Economic growth results from countries' accumulation of organizational and technological capabilities. The Economic and Product Complexity Indices, introduced as an attempt to measure these capabilities from a country's basket of exported products, have become popular to study economic development, the geography of innovation, and industrial policies. Despite this reception, the interpretation of these indicators proved difficult. Although the original Method of Reflections suggested a direct interconnection between country and product metrics, it has been proved that the Economic and Product Complexity Indices result from a spectral clustering algorithm that separately groups similar countries or similar products, respectively. This recent approach to economic and product complexity conflicts with the original one and treats separately countries and products. However, building on previous interpretations of the indices and the recent evolution in spectral clustering, we show that these indices simultaneously identify two co-clusters of similar countries and products. This viewpoint reconciles the spectral clustering interpretation of the indices with the original Method of Reflections interpretation. By proving the often neglected intimate relationship between country and product complexity, this approach emphasizes the role of a selected set of products in determining economic development while extending the range of applications of these indicators in economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16199v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlo Bottai, Jacopo Di Iorio, Martina Iori</dc:creator>
    </item>
    <item>
      <title>Distance-based Chatterjee correlation: a new generalized robust measure of directed association for multivariate real and complex-valued data</title>
      <link>https://arxiv.org/abs/2406.16458</link>
      <description>arXiv:2406.16458v1 Announce Type: cross 
Abstract: Building upon the Chatterjee correlation (2021: J. Am. Stat. Assoc. 116, p2009) for two real-valued variables, this study introduces a generalized measure of directed association between two vector variables, real or complex-valued, and of possibly different dimensions. The new measure is denoted as the "distance-based Chatterjee correlation", owing to the use here of the "distance transformed data" defined in Szekely et al (2007: Ann. Statist. 35, p2769) for the distance correlation. A main property of the new measure, inherited from the original Chatterjee correlation, is its predictive and asymmetric nature: it measures how well one variable can be predicted by the other, asymmetrically. This allows for inferring the causal direction of the association, by using the method of Blobaum et al (2019: PeerJ Comput. Sci. 1, e169). Since the original Chatterjee correlation is based on ranks, it is not available for complex variables, nor for general multivariate data. The novelty of our work is the extension to multivariate real and complex-valued pairs of vectors, offering a robust measure of directed association in a completely non-parametric setting. Informally, the intuitive assumption used here is that distance correlation is mathematically equivalent to Pearson's correlation when applied to "distance transformed" data. The next logical step is to compute Chatterjee's correlation on the same "distance transformed" data, thereby extending the analysis to multivariate vectors of real and complex valued data. As a bonus, the new measure here is robust to outliers, which is not true for the distance correlation of Szekely et al. Additionally, this approach allows for inference regarding the causal direction of the association between the variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16458v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Roberto D. Pascual-Marqui, Kieko Kochi, Toshihiko Kinoshita</dc:creator>
    </item>
    <item>
      <title>Influence analyses of "designs" for evaluating inconsistency in network meta-analysis</title>
      <link>https://arxiv.org/abs/2406.16485</link>
      <description>arXiv:2406.16485v1 Announce Type: cross 
Abstract: Network meta-analysis is an evidence synthesis method for comparative effectiveness analyses of multiple available treatments. To justify evidence synthesis, consistency is a relevant assumption; however, existing methods founded on statistical testing possibly have substantial limitations of statistical powers or several drawbacks in treating multi-arm studies. Besides, inconsistency is theoretically explained as design-by-treatment interactions, and the primary purpose of these analyses is prioritizing "designs" for further investigations to explore sources of biases and irregular issues that might influence the overall results. In this article, we propose an alternative framework for inconsistency evaluations using influence diagnostic methods that enable quantitative evaluations of the influences of individual designs to the overall results. We provide four new methods to quantify the influences of individual designs through a "leave-one-design-out" analysis framework. We also propose a simple summary measure, the O-value, for prioritizing designs and interpreting these influential analyses straightforwardly. Furthermore, we propose another testing approach based on the leave-one-design-out analysis framework. By applying the new methods to a network meta-analysis of antihypertensive drugs, we demonstrate the new methods located potential sources of inconsistency accurately. The proposed methods provide new insights into alternatives to existing test-based methods, especially quantifications of influences of individual designs on the overall network meta-analysis results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16485v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kotaro Sasaki, Hisashi Noma</dc:creator>
    </item>
    <item>
      <title>Meta-experiments: Improving experimentation through experimentation</title>
      <link>https://arxiv.org/abs/2406.16629</link>
      <description>arXiv:2406.16629v1 Announce Type: cross 
Abstract: A/B testing is widexly used in the industry to optimize customer facing websites. Many companies employ experimentation specialists to facilitate and improve the process of A/B testing. Here, we present the application of A/B testing to this improvement effort itself, by running experiments on the experimentation process, which we call 'meta-experiments'. We discuss the challenges of this approach using the example of one of our meta-experiments, which helped experimenters to run more sufficiently powered A/B tests. We also point out the benefits of 'dog fooding' for the experimentation specialists when running their own experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16629v1</guid>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Melanie J. I. M\"uller</dc:creator>
    </item>
    <item>
      <title>Conformal time series decomposition with component-wise exchangeability</title>
      <link>https://arxiv.org/abs/2406.16766</link>
      <description>arXiv:2406.16766v1 Announce Type: cross 
Abstract: Conformal prediction offers a practical framework for distribution-free uncertainty quantification, providing finite-sample coverage guarantees under relatively mild assumptions on data exchangeability. However, these assumptions cease to hold for time series due to their temporally correlated nature. In this work, we present a novel use of conformal prediction for time series forecasting that incorporates time series decomposition. This approach allows us to model different temporal components individually. By applying specific conformal algorithms to each component and then merging the obtained prediction intervals, we customize our methods to account for the different exchangeability regimes underlying each component. Our decomposition-based approach is thoroughly discussed and empirically evaluated on synthetic and real-world data. We find that the method provides promising results on well-structured time series, but can be limited by factors such as the decomposition step for more complex data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16766v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Derck W. E. Prinzhorn, Thijmen Nijdam, Putri A. van der Linden, Alexander Timans</dc:creator>
    </item>
    <item>
      <title>Adjusting for Selection Bias Due to Missing Eligibility Criteria in Emulated Target Trials</title>
      <link>https://arxiv.org/abs/2406.16830</link>
      <description>arXiv:2406.16830v1 Announce Type: cross 
Abstract: Target trial emulation (TTE) is a popular framework for observational studies based on electronic health records (EHR). A key component of this framework is determining the patient population eligible for inclusion in both a target trial of interest and its observational emulation. Missingness in variables that define eligibility criteria, however, presents a major challenge towards determining the eligible population when emulating a target trial with an observational study. In practice, patients with incomplete data are almost always excluded from analysis despite the possibility of selection bias, which can arise when subjects with observed eligibility data are fundamentally different than excluded subjects. Despite this, to the best of our knowledge, very little work has been done to mitigate this concern. In this paper, we propose a novel conceptual framework to address selection bias in TTE studies, tailored towards time-to-event endpoints, and describe estimation and inferential procedures via inverse probability weighting (IPW). Under an EHR-based simulation infrastructure, developed to reflect the complexity of EHR data, we characterize common settings under which missing eligibility data poses the threat of selection bias and investigate the ability of the proposed methods to address it. Finally, using EHR databases from Kaiser Permanente, we demonstrate the use of our method to evaluate the effect of bariatric surgery on microvascular outcomes among a cohort of severely obese patients with Type II diabetes mellitus (T2DM).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16830v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Benz, Rajarshi Mukherjee, Issa Dahabreh, Rui Wang, David Arterburn, Catherine Lee, Heidi Fischer, Susan Shortreed, Sebastien Haneuse</dc:creator>
    </item>
    <item>
      <title>Retention in STEM: Factors Influencing Student Persistence and Employment</title>
      <link>https://arxiv.org/abs/2311.14142</link>
      <description>arXiv:2311.14142v2 Announce Type: replace 
Abstract: This study utilizes data from the Baccalaureate and Beyond Longitudinal Study to explore factors associated with the likelihood of students' employment in STEM fields one year after graduation. We examined various factors related to students' individual characteristics (e.g., gender, race, and financial situation), institutional experiences (e.g., major, academic standing, research involvement, internships, extracurricular activities, and undergraduate practicum), and institutional and national trends. The results indicate lower STEM employment likelihood for minority groups and students with academic probation. The findings also highlight the positive impact of undergraduate practicum and job relevance to major on STEM employment likelihood. On the contrary, career services were negatively associated with the likelihood of students' STEM occupation choice, suggesting potential shortcomings in STEM job preparation within these services. The study provides valuable insights and actionable recommendations for policymakers and educators seeking to increase diversity and inclusion in STEM fields, suggesting the need for more efficient and tailored educational interventions and curriculum development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14142v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>The Proceedings of the 19th Annual National Symposium on Student Retention, 2023</arxiv:journal_reference>
      <dc:creator>Linli Zhou, Damji Heo Stratton, Xin Li</dc:creator>
    </item>
    <item>
      <title>Communication-Efficient Distributed Estimation and Inference for Cox's Model</title>
      <link>https://arxiv.org/abs/2302.12111</link>
      <description>arXiv:2302.12111v3 Announce Type: replace-cross 
Abstract: Motivated by multi-center biomedical studies that cannot share individual data due to privacy and ownership concerns, we develop communication-efficient iterative distributed algorithms for estimation and inference in the high-dimensional sparse Cox proportional hazards model. We demonstrate that our estimator, even with a relatively small number of iterations, achieves the same convergence rate as the ideal full-sample estimator under very mild conditions. To construct confidence intervals for linear combinations of high-dimensional hazard regression coefficients, we introduce a novel debiased method, establish central limit theorems, and provide consistent variance estimators that yield asymptotically valid distributed confidence intervals. In addition, we provide valid and powerful distributed hypothesis tests for any coordinate element based on a decorrelated score test. We allow time-dependent covariates as well as censored survival times. Extensive numerical experiments on both simulated and real data lend further support to our theory and demonstrate that our communication-efficient distributed estimators, confidence intervals, and hypothesis tests improve upon alternative methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.12111v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre Bayle, Jianqing Fan, Zhipeng Lou</dc:creator>
    </item>
    <item>
      <title>Similarity-based Random Partition Distribution for Clustering Functional Data</title>
      <link>https://arxiv.org/abs/2308.01704</link>
      <description>arXiv:2308.01704v3 Announce Type: replace-cross 
Abstract: Random partition distribution is a crucial tool for model-based clustering. This study advances the field of random partition in the context of functional spatial data, focusing on the challenges posed by hourly population data across various regions and dates. We propose an extended generalized Dirichlet process, named the similarity-based generalized Dirichlet process (SGDP), to address the limitations of simple random partition distributions (e.g., those induced by the Dirichlet process), such as an overabundance of clusters. This model prevents excess cluster production as well as incorporates pairwise similarity information to ensure accurate and meaningful grouping. The theoretical properties of the SGDP are studied. Then, SGDP-based random partition is applied to a real-world dataset of hourly population flow in $500\text{m}^2$ meshes in the central part of Tokyo. In this empirical context, our method excels at detecting meaningful patterns in the data while accounting for spatial nuances. The results underscore the adaptability and utility of the method, showcasing its prowess in revealing intricate spatiotemporal dynamics. The proposed SGDP will significantly contribute to urban planning, transportation, and policy-making and will be a helpful tool for understanding population dynamics and their implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01704v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoya Wakayama, Shonosuke Sugasawa, Genya Kobayashi</dc:creator>
    </item>
    <item>
      <title>Followers do not dictate the virality of news outlets on social media</title>
      <link>https://arxiv.org/abs/2401.17890</link>
      <description>arXiv:2401.17890v2 Announce Type: replace-cross 
Abstract: Initially conceived for entertainment, social media platforms have profoundly transformed the dissemination of information and consequently reshaped the dynamics of agenda-setting. In this scenario, understanding the factors that capture audience attention and drive viral content is crucial. Employing Gibrat's Law, which posits that an entity's growth rate is unrelated to its size, we examine the engagement growth dynamics of news outlets on social media. Our analysis encloses the Facebook historical data of over a thousand news outlets, encompassing approximately 57 million posts in four European languages from 2008 to the end of 2022. We discover universal growth dynamics according to which news virality is independent of the traditional size or engagement with the outlet. Moreover, our analysis reveals a significant long-term impact of news source reliability on engagement growth, with engagement induced by unreliable sources decreasing over time. We conclude the paper by presenting a statistical model replicating the observed growth dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17890v2</guid>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emanuele Sangiorgio, Matteo Cinelli, Roy Cerqueti, Walter Quattrociocchi</dc:creator>
    </item>
    <item>
      <title>Adaptive Online Experimental Design for Causal Discovery</title>
      <link>https://arxiv.org/abs/2405.11548</link>
      <description>arXiv:2405.11548v3 Announce Type: replace-cross 
Abstract: Causal discovery aims to uncover cause-and-effect relationships encoded in causal graphs by leveraging observational, interventional data, or their combination. The majority of existing causal discovery methods are developed assuming infinite interventional data. We focus on data interventional efficiency and formalize causal discovery from the perspective of online learning, inspired by pure exploration in bandit problems. A graph separating system, consisting of interventions that cut every edge of the graph at least once, is sufficient for learning causal graphs when infinite interventional data is available, even in the worst case. We propose a track-and-stop causal discovery algorithm that adaptively selects interventions from the graph separating system via allocation matching and learns the causal graph based on sampling history. Given any desired confidence value, the algorithm determines a termination condition and runs until it is met. We analyze the algorithm to establish a problem-dependent upper bound on the expected number of required interventional samples. Our proposed algorithm outperforms existing methods in simulations across various randomly generated causal graphs. It achieves higher accuracy, measured by the structural hamming distance (SHD) between the learned causal graph and the ground truth, with significantly fewer samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11548v3</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Qasim Elahi, Lai Wei, Murat Kocaoglu, Mahsa Ghasemi</dc:creator>
    </item>
  </channel>
</rss>
