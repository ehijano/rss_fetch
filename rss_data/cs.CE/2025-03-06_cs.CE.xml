<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Mar 2025 05:02:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Predict-Then-Optimize Customer Allocation Framework for Online Fund Recommendation</title>
      <link>https://arxiv.org/abs/2503.03165</link>
      <description>arXiv:2503.03165v1 Announce Type: new 
Abstract: With the rapid growth of online investment platforms, funds can be distributed to individual customers online. The central issue is to match funds with potential customers under constraints. Most mainstream platforms adopt the recommendation formulation to tackle the problem. However, the traditional recommendation regime has its inherent drawbacks when applying the fund-matching problem with multiple constraints. In this paper, we model the fund matching under the allocation formulation. We design PTOFA, a Predict-Then-Optimize Fund Allocation framework. This data-driven framework consists of two stages, i.e., prediction and optimization, which aim to predict expected revenue based on customer behavior and optimize the impression allocation to achieve the maximum revenue under the necessary constraints, respectively. Extensive experiments on real-world datasets from an industrial online investment platform validate the effectiveness and efficiency of our solution. Additionally, the online A/B tests demonstrate PTOFA's effectiveness in the real-world fund recommendation scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03165v1</guid>
      <category>cs.CE</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xing Tang, Yunpeng Weng, Fuyuan Lyu, Dugang Liu, Xiuqiang He</dc:creator>
    </item>
    <item>
      <title>A Novel Multi-Criteria Local Latin Hypercube Refinement System for Commutation Angle Improvement in IPMSMs</title>
      <link>https://arxiv.org/abs/2503.03372</link>
      <description>arXiv:2503.03372v1 Announce Type: new 
Abstract: The commutation angle is defined as the angle between the fundamental of the motor phase current and the fundamental of the back-EMF. It can be utilised to provide a compensating effect in IPMSMs. This is due to the reluctance torque component being dependent on the commutation angle of the phase current even before entering the extended speed range. A real-time maximum torque per current and voltage strategy is demonstrated to find the trajectory and optimum commutation angles, gamma, where the level of accuracy depends on the application and available computational speed. A magnet volume reduction using a novel multi-criteria local Latin hypercube refinement (MLHR) sampling system is also presented to improve the optimisation process. The proposed new technique minimises the magnet mass to motor torque density whilst maintaining a similar phase current level. A mapping of gamma allows the determination of the optimum angles, as shown in this paper. The 3rd generation Toyota Prius IPMSM is considered as the reference motor, where the rotor configuration is altered to allow for an individual assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03372v1</guid>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TIA.2022.3225786</arxiv:DOI>
      <dc:creator>Pedram Asef, Mouloud Denai, Johannes J. H. Paulides, Bruno Ricardo Marques, Andrew Lapthorn</dc:creator>
    </item>
    <item>
      <title>An Automated Computational Pipeline for Generating Large-Scale Cohorts of Patient-Specific Ventricular Models in Electromechanical In Silico Trials</title>
      <link>https://arxiv.org/abs/2503.03706</link>
      <description>arXiv:2503.03706v1 Announce Type: new 
Abstract: In recent years, human in silico trials have gained significant traction as a powerful approach to evaluate the effects of drugs, clinical interventions, and medical devices. In silico trials not only minimise patient risks but also reduce reliance on animal testing. However, the implementation of in silico trials presents several time-consuming challenges. It requires the creation of large cohorts of virtual patients. Each virtual patient is described by their anatomy with a volumetric mesh and electrophysiological and mechanical dynamics through mathematical equations and parameters. Furthermore, simulated conditions need definition including stimulation protocols and therapy evaluation. For large virtual cohorts, this requires automatic and efficient pipelines for generation of corresponding files. In this work, we present a computational pipeline to automatically create large virtual patient cohort files to conduct large-scale in silico trials through cardiac electromechanical simulations. The pipeline generates the files describing meshes, labels, and data required for the simulations directly from unprocessed surface meshes. We applied the pipeline to generate over 100 virtual patients from various datasets and performed simulations to demonstrate capacity to conduct in silico trials for virtual patients using verified and validated electrophysiology and electromechanics models for the context of use. The proposed pipeline is adaptable to accommodate different types of ventricular geometries and mesh processing tools, ensuring its versatility in handling diverse clinical datasets. By establishing an automated framework for large scale simulation studies as required for in silico trials and providing open-source code, our work aims to support scalable, personalised cardiac simulations in research and clinical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03706v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruben Doste, Julia Camps, Zhinuo Jenny Wang, Lucas Arantes Berg, Maxx Holmes, Hannah Smith, Marcel Beetz, Lei Li, Abhirup Banerjee, Vicente Grau, Blanca Rodriguez</dc:creator>
    </item>
    <item>
      <title>Function-Coherent Gambles with Non-Additive Sequential Dynamics</title>
      <link>https://arxiv.org/abs/2503.02889</link>
      <description>arXiv:2503.02889v1 Announce Type: cross 
Abstract: The desirable gambles framework provides a rigorous foundation for imprecise probability theory but relies heavily on linear utility via its coherence axioms. In our related work, we introduced function-coherent gambles to accommodate non-linear utility. However, when repeated gambles are played over time -- especially in intertemporal choice where rewards compound multiplicatively -- the standard additive combination axiom fails to capture the appropriate long-run evaluation. In this paper we extend the framework by relaxing the additive combination axiom and introducing a nonlinear combination operator that effectively aggregates repeated gambles in the log-domain. This operator preserves the time-average (geometric) growth rate and addresses the ergodicity problem. We prove the key algebraic properties of the operator, discuss its impact on coherence, risk assessment, and representation, and provide a series of illustrative examples. Our approach bridges the gap between expectation values and time averages and unifies normative theory with empirically observed non-stationary reward dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02889v1</guid>
      <category>econ.TH</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>math.PR</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregory Wheeler</dc:creator>
    </item>
    <item>
      <title>Navigating Intelligence: A Survey of Google OR-Tools and Machine Learning for Global Path Planning in Autonomous Vehicles</title>
      <link>https://arxiv.org/abs/2503.03338</link>
      <description>arXiv:2503.03338v1 Announce Type: cross 
Abstract: We offer a new in-depth investigation of global path planning (GPP) for unmanned ground vehicles, an autonomous mining sampling robot named ROMIE. GPP is essential for ROMIE's optimal performance, which is translated into solving the traveling salesman problem, a complex graph theory challenge that is crucial for determining the most effective route to cover all sampling locations in a mining field. This problem is central to enhancing ROMIE's operational efficiency and competitiveness against human labor by optimizing cost and time. The primary aim of this research is to advance GPP by developing, evaluating, and improving a cost-efficient software and web application. We delve into an extensive comparison and analysis of Google operations research (OR)-Tools optimization algorithms. Our study is driven by the goal of applying and testing the limits of OR-Tools capabilities by integrating Reinforcement Learning techniques for the first time. This enables us to compare these methods with OR-Tools, assessing their computational effectiveness and real-world application efficiency. Our analysis seeks to provide insights into the effectiveness and practical application of each technique. Our findings indicate that Q-Learning stands out as the optimal strategy, demonstrating superior efficiency by deviating only 1.2% on average from the optimal solutions across our datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03338v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>eess.SP</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/aisy.202300840</arxiv:DOI>
      <dc:creator>Alexandre Benoit, Pedram Asef</dc:creator>
    </item>
    <item>
      <title>Solving Inverse Problem for Multi-armed Bandits via Convex Optimization</title>
      <link>https://arxiv.org/abs/2501.18945</link>
      <description>arXiv:2501.18945v2 Announce Type: replace 
Abstract: We consider the inverse problem of multi-armed bandits (IMAB) that are widely used in neuroscience and psychology research for behavior modelling. We first show that the IMAB problem is not convex in general, but can be relaxed to a convex problem via variable transformation. Based on this result, we propose a two-step sequential heuristic for (approximately) solving the IMAB problem. We discuss a condition where our method provides global solution to the IMAB problem with certificate, as well as approximations to further save computing time. Numerical experiments indicate that our heuristic method is more robust than directly solving the IMAB problem via repeated local optimization, and can achieve the performance of Monte Carlo methods within a significantly decreased running time. We provide the implementation of our method based on CVXPY, which allows straightforward application by users not well versed in convex optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18945v2</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Zhu, Joschka Boedecker</dc:creator>
    </item>
    <item>
      <title>Aligning Large Language Models and Geometric Deep Models for Protein Representation</title>
      <link>https://arxiv.org/abs/2411.05316</link>
      <description>arXiv:2411.05316v2 Announce Type: replace-cross 
Abstract: Latent representation alignment has become a foundational technique for constructing multimodal large language models (MLLM) by mapping embeddings from different modalities into a shared space, often aligned with the embedding space of large language models (LLMs) to enable effective cross-modal understanding. While preliminary protein-focused MLLMs have emerged, they have predominantly relied on heuristic approaches, lacking a fundamental understanding of optimal alignment practices across representations. In this study, we explore the alignment of multimodal representations between LLMs and Geometric Deep Models (GDMs) in the protein domain. We comprehensively evaluate three state-of-the-art LLMs (Gemma2-2B, LLaMa3.1-8B, and LLaMa3.1-70B) with four protein-specialized GDMs (GearNet, GVP, ScanNet, GAT). Our work examines alignment factors from both model and protein perspectives, identifying challenges in current alignment methodologies and proposing strategies to improve the alignment process. Our key findings reveal that GDMs incorporating both graph and 3D structural information align better with LLMs, larger LLMs demonstrate improved alignment capabilities, and protein rarity significantly impacts alignment performance. We also find that increasing GDM embedding dimensions, using two-layer projection heads, and fine-tuning LLMs on protein-specific data substantially enhance alignment quality. These strategies offer potential enhancements to the performance of protein-related multimodal models. Our code and data are available at https://github.com/Tizzzzy/LLM-GDM-alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05316v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>q-bio.BM</category>
      <pubDate>Thu, 06 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dong Shu, Bingbing Duan, Kai Guo, Kaixiong Zhou, Jiliang Tang, Mengnan Du</dc:creator>
    </item>
  </channel>
</rss>
