<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Sep 2025 04:01:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>SANVI: A Fast Spectral-Assisted Network Variational Inference Method with an Extended Surrogate Likelihood Function</title>
      <link>https://arxiv.org/abs/2509.00562</link>
      <description>arXiv:2509.00562v1 Announce Type: new 
Abstract: Bayesian inference has been broadly applied to statistical network analysis, but suffers from the expensive computational costs due to the nature of Markov chain Monte Carlo sampling algorithms. This paper proposes a novel and computationally efficient Spectral-Assisted Network Variational Inference (SANVI) method within the framework of the generalized random dot product graph. The key idea is a cleverly designed extended surrogate likelihood function that enjoys two convenient features. Firstly, it decouples the generalized inner product of latent positions in the random graph model. Secondly, it relaxes the complicated domain of the original likelihood function to the entire Euclidean space. Leveraging these features, we design a computationally efficient Gaussian variational inference algorithm via stochastic gradient descent. Furthermore, we show the asymptotic efficiency of the maximum extended surrogate likelihood estimator and the Bernstein-von Mises limit of the variational posterior distribution. Through extensive numerical studies, we demonstrate the usefulness of the proposed SANVI algorithm compared to the classical Markov chain Monte Carlo algorithm, including comparable estimation accuracy for the latent positions and less computational costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00562v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dingbo Wu, Fangzheng Xie</dc:creator>
    </item>
    <item>
      <title>Removal of Redundant Candidate Points for the Exact D-Optimal Design Problem</title>
      <link>https://arxiv.org/abs/2509.00719</link>
      <description>arXiv:2509.00719v1 Announce Type: new 
Abstract: One of the most common problems in statistical experimentation is computing D-optimal designs on large finite candidate sets. While optimal approximate (i.e., infinite-sample) designs can be efficiently computed using convex methods, constructing optimal exact (i.e., finite-sample) designs is a substantially more difficult integer-optimization problem. In this paper, we propose necessary conditions, based on approximate designs, that must be satisfied by any support point of a D-optimal exact design. These conditions enable rapid elimination of redundant candidate points without loss of optimality, thereby reducing memory requirements and runtime of subsequent exact design algorithms. In addition, we prove that for sufficiently large sample sizes, the supports of D-optimal exact designs are contained in a typically small maximum-variance set. We demonstrate the approach on randomly generated benchmark models with candidate sets up to 100 million points, and on commonly used constrained mixture models with up to one million points. The proposed approach reduces the initial candidate sets by several orders of magnitude, thereby making it possible to compute exact D-optimal designs for these problems via mixed-integer second-order cone programming, which provides optimality guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00719v1</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Radoslav Harman, Samuel Rosa</dc:creator>
    </item>
    <item>
      <title>Regime-Switching Langevin Monte Carlo Algorithms</title>
      <link>https://arxiv.org/abs/2509.00941</link>
      <description>arXiv:2509.00941v1 Announce Type: new 
Abstract: Langevin Monte Carlo (LMC) algorithms are popular Markov Chain Monte Carlo (MCMC) methods to sample a target probability distribution, which arises in many applications in machine learning. Inspired by regime-switching stochastic differential equations in the probability literature, we propose and study regime-switching Langevin dynamics (RS-LD) and regime-switching kinetic Langevin dynamics (RS-KLD). Based on their discretizations, we introduce regime-switching Langevin Monte Carlo (RS-LMC) and regime-switching kinetic Langevin Monte Carlo (RS-KLMC) algorithms, which can also be viewed as LMC and KLMC algorithms with random stepsizes. We also propose frictional-regime-switching kinetic Langevin dynamics (FRS-KLD) and its associated algorithm frictional-regime-switching kinetic Langevin Monte Carlo (FRS-KLMC), which can also be viewed as the KLMC algorithm with random frictional coefficients. We provide their 2-Wasserstein non-asymptotic convergence guarantees to the target distribution, and analyze the iteration complexities. Numerical experiments using both synthetic and real data are provided to illustrate the efficiency of our proposed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00941v1</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyu Wang, Yingli Wang, Lingjiong Zhu</dc:creator>
    </item>
    <item>
      <title>Ensemble Control Variates</title>
      <link>https://arxiv.org/abs/2509.01091</link>
      <description>arXiv:2509.01091v1 Announce Type: new 
Abstract: Control variates have become an increasingly popular variance-reduction technique in Bayesian inference. Many broadly applicable control variates are based on the Langevin-Stein operator, which leverages gradient information from any gradient-based sampler to produce variance-reduced estimators of expectations. These control variates typically require optimising over a function $u(\theta)$ within a user-defined functional class $G$, such as the space of $Q$th-order polynomials or a reproducing kernel Hilbert space. We propose using averaging-based ensemble learning to construct Stein-based control variates. While the proposed framework is broadly applicable, we focus on ensembles constructed from zero-variance control variates (ZVCV), a popular parametric approach based on solving a linear approximation problem that can easily be over-parameterised in medium-to-high dimensional settings. A common remedy is to use regularised ZVCV via penalised regression, but these methods can be prohibitively slow. We introduce ensemble ZVCV methods based on ensembles of OLS estimators and evaluate the proposed methods against established methods in the literature in a simulation study. Our results show that ensemble ZVCV methods are competitive with regularised ZVCV methods in terms of statistical efficiency, but are substantially faster. This work opens a new direction for constructing broadly applicable control variate techniques via ensemble learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01091v1</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Long M. Nguyen, Christopher Drovandi, Leah F. South</dc:creator>
    </item>
    <item>
      <title>Probit Monotone BART</title>
      <link>https://arxiv.org/abs/2509.00263</link>
      <description>arXiv:2509.00263v1 Announce Type: cross 
Abstract: Bayesian Additive Regression Trees (BART) of Chipman et al. (2010) has proven to be a powerful tool for nonparametric modeling and prediction. Monotone BART (Chipman et al., 2022) is a recent development that allows BART to be more precise in estimating monotonic functions. We further these developments by proposing probit monotone BART, which allows the monotone BART framework to estimate conditional mean functions when the outcome variable is binary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00263v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jared D. Fisher</dc:creator>
    </item>
    <item>
      <title>Identifying Causal Direction via Dense Functional Classes</title>
      <link>https://arxiv.org/abs/2509.00538</link>
      <description>arXiv:2509.00538v1 Announce Type: cross 
Abstract: We address the problem of determining the causal direction between two univariate, continuous-valued variables, X and Y, under the assumption of no hidden confounders. In general, it is not possible to make definitive statements about causality without some assumptions on the underlying model. To distinguish between cause and effect, we propose a bivariate causal score based on the Minimum Description Length (MDL) principle, using functions that possess the density property on a compact real interval. We prove the identifiability of these causal scores under specific conditions. These conditions can be easily tested. Gaussianity of the noise in the causal model equations is not assumed, only that the noise is low. The well-studied class of cubic splines possesses the density property on a compact real interval. We propose LCUBE as an instantiation of the MDL-based causal score utilizing cubic regression splines. LCUBE is an identifiable method that is also interpretable, simple, and very fast. It has only one hyperparameter. Empirical evaluations compared to state-of-the-art methods demonstrate that LCUBE achieves superior precision in terms of AUDRC on the real-world Tuebingen cause-effect pairs dataset. It also shows superior average precision across common 10 benchmark datasets and achieves above average precision on 13 datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00538v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katerina Hlavackova-Schindler, Suzana Marsela</dc:creator>
    </item>
    <item>
      <title>FBMS: An R Package for Flexible Bayesian Model Selection and Model Averaging</title>
      <link>https://arxiv.org/abs/2509.00753</link>
      <description>arXiv:2509.00753v1 Announce Type: cross 
Abstract: The FBMS R package facilitates Bayesian model selection and model averaging in complex regression settings by employing a variety of Monte Carlo model exploration methods. At its core, the package implements an efficient Mode Jumping Markov Chain Monte Carlo (MJMCMC) algorithm, designed to improve mixing in multi-modal posterior landscapes within Bayesian generalized linear models. In addition, it provides a genetically modified MJMCMC (GMJMCMC) algorithm that introduces nonlinear feature generation, thereby enabling the estimation of Bayesian generalized nonlinear models (BGNLMs). Within this framework, the algorithm maintains and updates populations of transformed features, computes their posterior probabilities, and evaluates the posteriors of models constructed from them. We demonstrate the effective use of FBMS for both inferential and predictive modeling in Gaussian regression, focusing on different instances of the BGNLM class of models. Furthermore, through a broad set of applications, we illustrate how the methodology can be extended to increasingly complex modeling scenarios, extending to other response distributions and mixed effect models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00753v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Frommlet, Jon Lachmann, Geir Storvik, Aliaksandr Hubin</dc:creator>
    </item>
    <item>
      <title>Sampling as Bandits: Evaluation-Efficient Design for Black-Box Densities</title>
      <link>https://arxiv.org/abs/2509.01437</link>
      <description>arXiv:2509.01437v1 Announce Type: cross 
Abstract: We introduce bandit importance sampling (BIS), a new class of importance sampling methods designed for settings where the target density is expensive to evaluate. In contrast to adaptive importance sampling, which optimises a proposal distribution, BIS directly designs the samples through a sequential strategy that combines space-filling designs with multi-armed bandits. Our method leverages Gaussian process surrogates to guide sample selection, enabling efficient exploration of the parameter space with minimal target evaluations. We establish theoretical guarantees on convergence and demonstrate the effectiveness of the method across a broad range of sampling tasks. BIS delivers accurate approximations with fewer target evaluations, outperforming competing approaches across multimodal, heavy-tailed distributions, and real-world applications to Bayesian inference of computationally expensive models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01437v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takuo Matsubara, Andrew Duncan, Simon Cotter, Konstantinos Zygalakis</dc:creator>
    </item>
    <item>
      <title>Preconditioned Regularized Wasserstein Proximal Sampling</title>
      <link>https://arxiv.org/abs/2509.01685</link>
      <description>arXiv:2509.01685v1 Announce Type: cross 
Abstract: We consider sampling from a Gibbs distribution by evolving finitely many particles. We propose a preconditioned version of a recently proposed noise-free sampling method, governed by approximating the score function with the numerically tractable score of a regularized Wasserstein proximal operator. This is derived by a Cole--Hopf transformation on coupled anisotropic heat equations, yielding a kernel formulation for the preconditioned regularized Wasserstein proximal. The diffusion component of the proposed method is also interpreted as a modified self-attention block, as in transformer architectures. For quadratic potentials, we provide a discrete-time non-asymptotic convergence analysis and explicitly characterize the bias, which is dependent on regularization and independent of step-size. Experiments demonstrate acceleration and particle-level stability on various log-concave and non-log-concave toy examples to Bayesian total-variation regularized image deconvolution, and competitive/better performance on non-convex Bayesian neural network training when utilizing variable preconditioning matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01685v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hong Ye Tan, Stanley Osher, Wuchen Li</dc:creator>
    </item>
    <item>
      <title>hdMTD: An R Package for High-Dimensional Mixture Transition Distribution Models</title>
      <link>https://arxiv.org/abs/2509.01808</link>
      <description>arXiv:2509.01808v1 Announce Type: cross 
Abstract: Several natural phenomena exhibit long-range conditional dependencies. High-order mixture transition distribution (MTD) are parsimonious non-parametric models to study these phenomena. An MTD is a Markov chain in which the transition probabilities are expressed as a convex combination of lower-order conditional distributions. Despite their generality, inference for MTD models has traditionally been limited by the need to estimate high-dimensional joint distributions. In particular, for a sample of size n, the feasible order d of the MTD is typically restricted to d approximately O(log n). To overcome this limitation, Ost and Takahashi (2023) recently introduced a computationally efficient non-parametric inference method that identifies the relevant lags in high-order MTD models, even when d is approximately O(n), provided that the set of relevant lags is sparse. In this article, we introduce hdMTD, an R package allowing us to estimate parameters of such high-dimensional Markovian models. Given a sample from an MTD chain, hdMTD can retrieve the relevant past set using the BIC algorithm or the forward stepwise and cut algorithm described in Ost and Takahashi (2023). The package also computes the maximum likelihood estimate for transition probabilities and estimates high-order MTD parameters through the expectation-maximization algorithm. Additionally, hdMTD also allows for simulating an MTD chain from its stationary invariant distribution using the perfect (exact) sampling algorithm, enabling Monte Carlo simulation of the model. We illustrate the package's capabilities through simulated data and a real-world application involving temperature records from Brazil.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01808v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maiara Gripp, Giulio Iacobelli, Guilherme Ost, Daniel Y. Takahashi</dc:creator>
    </item>
    <item>
      <title>generalRSS: Sampling and Inference for Balanced and Unbalanced Ranked Set Sampling in R</title>
      <link>https://arxiv.org/abs/2509.02039</link>
      <description>arXiv:2509.02039v1 Announce Type: cross 
Abstract: Ranked set sampling (RSS) is a stratified sampling method that improves efficiency over simple random sampling (SRS) by utilizing auxiliary information for ranking and stratification. While balanced RSS (BRSS) assumes equal allocation across strata, unbalanced RSS (URSS) allows unequal allocation, making it particularly effective for skewed distributions. The generalRSS package provides extensive tools for both BRSS and URSS, addressing limitations in existing RSS software that primarily focus on balanced designs. It supports RSS data generation, efficient sample allocation strategies for URSS, and statistical inference for both balanced and unbalanced designs. This paper presents the RSS methodology and demonstrates the utility of generalRSS through two medical data applications: a one-sample mean inference and a two-sample area under the curve (AUC) comparison using NHANES datasets. These applications illustrate the practical implementation of URSS and show how generalRSS facilitates ranked set sampling and inference in real-world data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02039v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chul Moon, Soohyun Ahn</dc:creator>
    </item>
    <item>
      <title>Bayesian analysis of diffusion-driven multi-type epidemic models with application to COVID-19</title>
      <link>https://arxiv.org/abs/2211.15229</link>
      <description>arXiv:2211.15229v4 Announce Type: replace 
Abstract: We consider a flexible Bayesian evidence synthesis approach to model the age-specific transmission dynamics of COVID-19 based on daily mortality counts. The temporal evolution of transmission rates in populations containing multiple types of individuals is reconstructed via an appropriate dimension-reduction formulation driven by independent diffusion processes. A suitably tailored compartmental model is used to learn the latent counts of infection, accounting for fluctuations in transmission influenced by public health interventions and changes in human behaviour. The model is fitted to freely available COVID-19 data sources from the UK, Greece, and Austria and validated using a large-scale prevalence survey in England. In particular, we demonstrate how model expansion can facilitate evidence reconciliation at a latent level. The code implementing this work is made freely available via the Bernadette R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.15229v4</guid>
      <category>stat.CO</category>
      <category>physics.soc-ph</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssa/qnaf130</arxiv:DOI>
      <arxiv:journal_reference>Journal of the Royal Statistical Society Series A: Statistics in Society, 2025, qnaf130</arxiv:journal_reference>
      <dc:creator>Lampros Bouranis, Nikolaos Demiris, Konstantinos Kalogeropoulos, Ioannis Ntzoufras</dc:creator>
    </item>
    <item>
      <title>CDsampling: An R Package for Constrained D-Optimal Sampling in Paid Research Studies</title>
      <link>https://arxiv.org/abs/2410.20606</link>
      <description>arXiv:2410.20606v4 Announce Type: replace 
Abstract: In the context of paid research studies and clinical trials, budget considerations often require patient sampling from available populations which comes with inherent constraints. We introduce the R package CDsampling, which is the first to our knowledge to integrate optimal design theories within the framework of constrained sampling. This package offers the possibility to find both D-optimal approximate and exact allocations for samplings with or without constraints. Additionally, it provides functions to find constrained uniform sampling as a robust sampling strategy when the model information is limited. To demonstrate its efficacy, we provide simulated examples and a real-data example with datasets embedded in the package and compare them with classical sampling methods. Furthermore, the package revisits the theoretical results of the Fisher information matrix for generalized linear models (including regular linear regression model) and multinomial logistic models, offering functions for its computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20606v4</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifei Huang, Liping Tong, Jie Yang</dc:creator>
    </item>
    <item>
      <title>Fast and Cheap Krylov-Based Covariance Smoothing</title>
      <link>https://arxiv.org/abs/2501.08265</link>
      <description>arXiv:2501.08265v2 Announce Type: replace 
Abstract: We introduce the Tensorized-and-Restricted Krylov (TReK) method, a simple and efficient algorithm for estimating covariance tensors with large observational sizes. TReK extends the conjugate gradient method to incorporate range restrictions, enabling its use in a variety of covariance smoothing applications. By leveraging matrix-level operations, it achieves significant improvements in both computational speed and memory cost, improving over existing methods by an order of magnitude. TReK ensures finite-step convergence in the absence of rounding errors and converges fast in practice, making it well-suited for large-scale problems. The algorithm is also highly flexible, supporting a wide range of forward and projection tensors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08265v2</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ho Yun, Victor M. Panaretos</dc:creator>
    </item>
    <item>
      <title>Random sampling of contingency tables and partitions: Two practical examples of the Burnside process</title>
      <link>https://arxiv.org/abs/2503.02818</link>
      <description>arXiv:2503.02818v2 Announce Type: replace 
Abstract: This paper gives new, efficient algorithms for approximate uniform sampling of contingency tables and integer partitions. The algorithms use the Burnside process, a general algorithm for sampling a uniform orbit of a finite group acting on a finite set. We show that a technique called `lumping' can be used to derive efficient implementations of the Burnside process. For both contingency tables and partitions, the lumped processes have far lower per step complexity than the original Markov chains. We also define a second Markov chain for partitions called the reflected Burnside process. The reflected Burnside process maintains the computational advantages of the lumped process but empirically converges to the uniform distribution much more rapidly. By using the reflected Burnside process we can easily sample uniform partitions of size $10^{10}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02818v2</guid>
      <category>stat.CO</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11222-025-10708-5</arxiv:DOI>
      <arxiv:journal_reference>Stat Comput 35, 181 (2025)</arxiv:journal_reference>
      <dc:creator>Persi Diaconis, Michael Howes</dc:creator>
    </item>
    <item>
      <title>On the Wasserstein median of probability measures</title>
      <link>https://arxiv.org/abs/2209.03318</link>
      <description>arXiv:2209.03318v5 Announce Type: replace-cross 
Abstract: The primary choice to summarize a finite collection of random objects is by using measures of central tendency, such as mean and median. In the field of optimal transport, the Wasserstein barycenter corresponds to the Fr\'{e}chet or geometric mean of a set of probability measures, which is defined as a minimizer of the sum of squared distances to each element in a given set with respect to the Wasserstein distance of order 2. We introduce the Wasserstein median as a robust alternative to the Wasserstein barycenter. The Wasserstein median corresponds to the Fr\'{e}chet median under the 2-Wasserstein metric. The existence and consistency of the Wasserstein median are first established, along with its robustness property. In addition, we present a general computational pipeline that employs any recognized algorithms for the Wasserstein barycenter in an iterative fashion and demonstrate its convergence. The utility of the Wasserstein median as a robust measure of central tendency is demonstrated using real and simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.03318v5</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1080/10618600.2024.2374580</arxiv:DOI>
      <dc:creator>Kisung You, Dennis Shung, Mauro Giuffr\`e</dc:creator>
    </item>
    <item>
      <title>Adaptive greedy forward variable selection for linear regression models with incomplete data using multiple imputation</title>
      <link>https://arxiv.org/abs/2210.10967</link>
      <description>arXiv:2210.10967v2 Announce Type: replace-cross 
Abstract: Variable selection is crucial for sparse modeling in this age of big data. Missing values are common in data, and make variable selection more complicated. The approach of multiple imputation (MI) results in multiply imputed datasets for missing values, and has been widely applied in various variable selection procedures. However, directly performing variable selection on the whole MI data or bootstrapped MI data may not be worthy in terms of computation cost. To fast identify the active variables in the linear regression model, we propose the adaptive grafting procedure with three pooling rules on MI data. The proposed methods proceed iteratively, which starts from finding the active variables based on the complete case subset and then expand the working data matrix with both the number of active variables and available observations. A comprehensive simulation study shows the selection accuracy in different aspects and computational efficiency of the proposed methods. Two real-life examples illustrate the strength of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.10967v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yong-Shiuan Lee</dc:creator>
    </item>
    <item>
      <title>A microsimulation model of behaviour change calibrated to reversal learning data</title>
      <link>https://arxiv.org/abs/2406.14062</link>
      <description>arXiv:2406.14062v2 Announce Type: replace-cross 
Abstract: Behaviour change lies at the heart of many observable collective phenomena such as the transmission and control of infectious diseases, adoption of public health policies, and migration of animals to new habitats. Representing the process of individual behaviour change in computer simulations of these phenomena remains an open challenge. Often, computational models use phenomenological implementations with limited support from behavioural data. Without a strong connection to observable quantities, such models have limited utility for simulating observed and counterfactual scenarios of emergent phenomena because they cannot be validated or calibrated. Here, we present a simple stochastic microsimulation model of reversal learning that captures fundamental properties of individual behaviour change, namely, the capacity to learn based on accumulated reward signals, and the transient persistence of learned behaviour after rewards are removed or altered. The model has only two parameters, and we use approximate Bayesian computation to demonstrate that they are fully identifiable from empirical reversal learning time series data. Finally, we demonstrate how the model can be extended to account for the increased complexity of behavioural dynamics over longer time scales involving fluctuating stimuli. This work is a step towards the development and evaluation of fully identifiable individual-level behaviour change models that can function as validated submodels for complex simulations of collective behaviour change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14062v2</guid>
      <category>q-bio.QM</category>
      <category>physics.bio-ph</category>
      <category>stat.CO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.18564/jasss.5637</arxiv:DOI>
      <arxiv:journal_reference>Journal of Artificial Societies &amp; Social Simulation 28 (2025)</arxiv:journal_reference>
      <dc:creator>Roben Delos Reyes, Hugo Lyons Keenan, Cameron Zachreson</dc:creator>
    </item>
    <item>
      <title>WeSpeR: Computing non-linear shrinkage formulas for the weighted sample covariance</title>
      <link>https://arxiv.org/abs/2410.14413</link>
      <description>arXiv:2410.14413v2 Announce Type: replace-cross 
Abstract: We address the issue of computing the non-linear shrinkage formulas for the weighted sample covariance in high dimension. We use theoretical properties of the asymptotic sample spectrum in order to derive the \textit{WeSpeR} algorithm and significantly speed up non-linear shrinkage in dimension higher than $1000$. Empirical tests confirm the good properties of the \textit{WeSpeR} algorithm. We provide the implementation in PyTorch for it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14413v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Benoit Oriol</dc:creator>
    </item>
    <item>
      <title>Metric Oja Depth, New Statistical Tool for Estimating the Most Central Objects</title>
      <link>https://arxiv.org/abs/2411.11580</link>
      <description>arXiv:2411.11580v2 Announce Type: replace-cross 
Abstract: The Oja depth (simplicial volume depth) is one of the classical statistical techniques for measuring the central tendency of data in multivariate space. Despite the widespread emergence of object data like images, texts, matrices or graphs, a well-developed and suitable version of Oja depth for object data is lacking. To address this shortcoming, a novel measure of statistical depth, the metric Oja depth applicable to any object data, is proposed. Two competing strategies are used for optimizing metric depth functions, i.e., finding the deepest objects with respect to them. The performance of the metric Oja depth is compared with three other depth functions (half-space, lens, and spatial) in diverse data scenarios.
  Keywords: Object Data, Metric Oja depth, Statistical depth, Optimization, Metric statistics</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11580v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vida Zamanifarizhandi, Joni Virta</dc:creator>
    </item>
    <item>
      <title>GPU-Accelerated Parallel Selected Inversion for Structured Matrices Using sTiles</title>
      <link>https://arxiv.org/abs/2504.19171</link>
      <description>arXiv:2504.19171v2 Announce Type: replace-cross 
Abstract: Selected inversion is essential for applications such as Bayesian inference, electronic structure calculations, and inverse covariance estimation, where computing only specific elements of large sparse matrix inverses significantly reduces computational and memory overhead. We present an efficient implementation of a two-phase parallel algorithm for computing selected elements of the inverse of a sparse symmetric matrix A, which can be expressed as A = LL^T through sparse Cholesky factorization. Our approach leverages a tile-based structure, focusing on selected dense tiles to optimize computational efficiency and parallelism. While the focus is on arrowhead matrices, the method can be extended to handle general structured matrices. Performance evaluations on a dual-socket 26-core Intel Xeon CPU server demonstrate that sTiles outperforms state-of-the-art direct solvers such as Panua-PARDISO, achieving up to 13X speedup on large-scale structured matrices. Additionally, our GPU implementation using an NVIDIA A100 GPU demonstrates substantial acceleration over its CPU counterpart, achieving up to 5X speedup for large, high-bandwidth matrices with high computational intensity. These results underscore the robustness and versatility of sTiles, validating its effectiveness across various densities and problem configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19171v2</guid>
      <category>cs.PF</category>
      <category>stat.CO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Esmail Abdul Fattah, Hatem Ltaief, Havard Rue, David Keyes</dc:creator>
    </item>
  </channel>
</rss>
