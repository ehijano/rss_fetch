<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Apr 2025 04:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>BOLDSimNet: Examining Brain Network Similarity between Task and Resting-State fMRI</title>
      <link>https://arxiv.org/abs/2504.01274</link>
      <description>arXiv:2504.01274v1 Announce Type: new 
Abstract: Traditional causal connectivity methods in task-based and resting-state functional magnetic resonance imaging (fMRI) face challenges in accurately capturing directed information flow due to their sensitivity to noise and inability to model multivariate dependencies. These limitations hinder the effective comparison of brain networks between cognitive states, making it difficult to analyze network reconfiguration during task and resting states. To address these issues, we propose BOLDSimNet, a novel framework utilizing Multivariate Transfer Entropy (MTE) to measure causal connectivity and network similarity across different cognitive states. Our method groups functionally similar regions of interest (ROIs) rather than spatially adjacent nodes, improving accuracy in network alignment. We applied BOLDSimNet to fMRI data from 40 healthy controls and found that children exhibited higher similarity scores between task and resting states compared to adolescents, indicating reduced variability in attention shifts. In contrast, adolescents showed more differences between task and resting states in the Dorsal Attention Network (DAN) and the Default Mode Network (DMN), reflecting enhanced network adaptability. These findings emphasize developmental variations in the reconfiguration of the causal brain network, showcasing BOLDSimNet's ability to quantify network similarity and identify attentional fluctuations between different cognitive states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01274v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boseong Kim, Debashis Das Chakladar, Haejun Chung, Ikbeom Jang</dc:creator>
    </item>
    <item>
      <title>Emotional Responses to Auditory Hierarchical Structures is Shaped by Bodily Sensations and Listeners' Sensory Traits</title>
      <link>https://arxiv.org/abs/2504.01287</link>
      <description>arXiv:2504.01287v1 Announce Type: new 
Abstract: Emotional responses to auditory stimuli are a common part of everyday life. However, for some individuals, these responses can be distressing enough to interfere with daily functioning. Despite their prevalence, the mechanisms underlying auditory-induced emotion remain only partially understood. Prior research has identified contributing factors such as auditory features, listener traits, and bodily sensations. However, most studies have focused on acoustic features, leaving the role of syntactic structure largely unexplored. This study specifically investigates how hierarchical syntactic structures influence emotional experience, in conjunction with listener traits and bodily sensations. An online experiment was conducted with 715 participants, who listened to 26 sound sequences varying systematically in hierarchical syntactic complexity. Sequences were generated by combining three types of local pitch movement with three types of global pitch movement in ascending and descending pitch directions, resulting in nine complexity levels. Participants rated the valence and arousal of each sequence and indicated any bodily sensations on a body map. Measures of sensory processing patterns were also collected. Results showed that emotional valence was associated with the complex interplay of moderate syntactic complexity ("not too simple, not too complex"), sensory sensitivity, and upper torso sensations. These findings expand existing research by identifying syntactic features that shape auditory-induced emotional experience and highlight the link between bodily sensation and emotional response. They also suggest potential applications for incorporating syntactic design into therapeutic approaches to emotion regulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01287v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Maiko Minatoya, Tatsuya Daikoku, Yasuo Kuniyoshi</dc:creator>
    </item>
    <item>
      <title>Tinnitus, lucid dreaming and awakening. An online survey and theoretical implications</title>
      <link>https://arxiv.org/abs/2504.01453</link>
      <description>arXiv:2504.01453v1 Announce Type: new 
Abstract: (1) Background: Tinnitus is the perception of phantom sound in the absence of a corresponding external source. Previous studies reported that the presence of tinnitus is notably absent during dreams. This study aimed at replicating previous findings regarding tinnitus-free dreams, while also gaining a deeper understanding of tinnitus manifestations during dreams and after awakening. (2) Methods: For this observational study, 195 tinnitus patients answered an online survey on the mutual-help community Siopi. (3) Results: 160 patients could recall their dreams. Among them, 92.5% state they do not hear their tinnitus while dreaming. The rest (7.5%) report higher tinnitus burden, higher stress and more often exhibit objective tinnitus and/or tinnitus related to peripheral auditory pathology and/or drug intake. 13% of the participants frequently experience lucid dreams. Among them, 36% could perceive their tinnitus during lucid dreams, and this was strongly associated with the concomitant perception of external sounds during lucid dreaming. While the majority of patients report perceiving their tinnitus instantly upon awakening, during nocturnal awakenings, 18% declared they could be awakened by their tinnitus and 9.8% mentioned that their tinnitus can temporarily cease. (4) Conclusions: Our findings confirm the previous findings: tinnitus is rarely perceived during dreams. Remarkably, our study is the first to document the case of tinnitus during lucid dreaming. 64% of these patients gain higher-order consciousness attributes while still experiencing a tinnitus-free state. Our observations suggest that the presence or absence of gating of external auditory information during dreams acts as a tinnitus on-off switch, refining the previously proposed integrative model of auditory phantom perception.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01453v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.heares.2025.109204</arxiv:DOI>
      <arxiv:journal_reference>Hearing Research, 2025, 458, pp.109204</arxiv:journal_reference>
      <dc:creator>Robin Guillard (GIPSA-VIBS), Nicolas Dauman (RPPsy), Aur\'elien Cadix (GIPSA-VIBS), Charlotte Glabasnia Linck (GIPSA-VIBS), Marco Congedo (GIPSA-VIBS), Dirk de Ridder, Alain Londero</dc:creator>
    </item>
    <item>
      <title>Dual mechanism of Anti-Seizure Medications in controlling seizure activity</title>
      <link>https://arxiv.org/abs/2504.01887</link>
      <description>arXiv:2504.01887v1 Announce Type: new 
Abstract: Background: Anti-seizure medications (ASMs) can reduce seizure duration, but their precise modes of action are unclear. Specifically, it is unknown whether ASMs shorten seizures by simply compressing existing seizure activity into a shorter time frame or by selectively suppressing certain seizure activity patterns.
  Methods: We analysed intracranial EEG (iEEG) recordings of 457 seizures from 28 people with epilepsy undergoing ASM tapering. Beyond measuring seizure occurrence and duration, we categorized distinct seizure activity patterns (states) based on spatial and frequency power characteristics and related these to different ASM levels.
  Results: We found that reducing ASM levels led to increased seizure frequency (r = 0.87, p &lt; 0.001) and longer seizure duration ($\beta$ = -0.033, p &lt; 0.001), consistent with prior research. Further analysis revealed two distinct mechanisms in which seizures became prolonged:
  Emergence of new seizure patterns - In approx. 40% of patients, ASM tapering unmasked additional seizure activity states, and seizures containing these 'taper-emergent states' were substantially longer (r = 0.49, p &lt; 0.001).
  Prolongation of existing seizure patterns - Even in seizures without taper-emergent states, lower ASM levels still resulted in approx. 12-224% longer durations depending on the ASM dosage and tapering ($\beta$ = -0.049, p &lt; 0.001).
  Conclusion: ASMs influence seizures through two mechanisms: they (i) suppress specific seizure activity patterns (states) in an all-or-nothing fashion and (ii) curtail the duration of other seizure patterns. These findings highlight the complex role of ASMs in seizure modulation and could inform personalized dosing strategies for epilepsy management. These findings may also have implications in understanding the effects of ASMs on cognition and mood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01887v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillermo M. Besne, Emmanuel Molefi, Sarah J. Gascoigne, Nathan Evans, Billy Smith, Chris Thornton, Fahmida A. Chowdhury, Beate Diehl, John S. Duncan, Andrew W. McEvoy, Anna Miserocchi, Jane de Tisi, Matthew Walker, Peter N. Taylor, Yujiang Wang</dc:creator>
    </item>
    <item>
      <title>SpikeSift: A Computationally Efficient and Drift-Resilient Spike Sorting Algorithm</title>
      <link>https://arxiv.org/abs/2504.01604</link>
      <description>arXiv:2504.01604v1 Announce Type: cross 
Abstract: Spike sorting is a fundamental step in analyzing extracellular recordings, enabling the isolation of individual neuronal activity, yet it remains a challenging problem due to overlapping signals and recording instabilities, including electrode drift. While numerous algorithms have been developed to address these challenges, many struggle to balance accuracy and computational efficiency, limiting their applicability to largescale datasets. In response, we introduce SpikeSift, a novel spike sorting algorithm designed to mitigate drift by partitioning recordings into short, relatively stationary segments, with spikes subsequently sorted within each. To preserve neuronal identity across segment boundaries, a computationally efficient alignment process merges clusters without relying on continuous trajectory estimation. In contrast to conventional methods that separate spike detection from clustering, SpikeSift integrates these processes within an iterative detect-andsubtract framework, enhancing clustering accuracy while maintaining computational efficiency. Evaluations on intracellularly validated datasets and biophysically realistic MEArec simulations confirm that SpikeSift maintains high sorting accuracy even in the presence of electrode drift, providing a scalable and computationally efficient solution for large-scale extracellular recordings</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01604v1</guid>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vasileios Georgiadis, Panagiotis C. Petrantonakis</dc:creator>
    </item>
    <item>
      <title>Cortical network reconfiguration aligns with shifts of basal ganglia and cerebellar influence</title>
      <link>https://arxiv.org/abs/2408.07977</link>
      <description>arXiv:2408.07977v2 Announce Type: replace 
Abstract: Mammalian functional architecture flexibly adapts, transitioning from integration where information is distributed across the cortex, to segregation where information is focal in densely connected communities of brain regions. This flexibility in cortical brain networks is hypothesized to be driven by control signals originating from subcortical pathways, with the basal ganglia shifting the cortex towards integrated processing states and the cerebellum towards segregated states. In a sample of healthy human participants (N=242), we used fMRI to measure temporal variation in global brain networks while participants performed two tasks with similar cognitive demands (Stroop and Multi-Source Inference Task (MSIT)). Using the modularity index, we determined cortical networks shifted from integration (low modularity) at rest to high modularity during easier i.e. congruent (segregation). Increased task difficulty (incongruent) resulted in lower modularity in comparison to the easier counterpart indicating more integration of the cortical network. Influence of basal ganglia and cerebellum was measured using eigenvector centrality. Results correlated with decreases and increases in cortical modularity respectively, with only the basal ganglia influence preceding cortical integration. Our results support the theory the basal ganglia shifts cortical networks to integrated states due to environmental demand. Cerebellar influence correlates with shifts to segregated cortical states, though may not play a causal role.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07977v2</guid>
      <category>q-bio.NC</category>
      <category>cs.SI</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kimberly Nestor</dc:creator>
    </item>
    <item>
      <title>Confidence-weighted integration of human and machine judgments for superior decision-making</title>
      <link>https://arxiv.org/abs/2408.08083</link>
      <description>arXiv:2408.08083v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have emerged as powerful tools in various domains. Recent studies have shown that LLMs can surpass humans in certain tasks, such as predicting the outcomes of neuroscience studies. What role does this leave for humans in the overall decision process? One possibility is that humans, despite performing worse than LLMs, can still add value when teamed with them. A human and machine team can surpass each individual teammate when team members' confidence is well-calibrated and team members diverge in which tasks they find difficult (i.e., calibration and diversity are needed). We simplified and extended a Bayesian approach to combining judgments using a logistic regression framework that integrates confidence-weighted judgments for any number of team members. Using this straightforward method, we demonstrated in a neuroscience forecasting task that, even when humans were inferior to LLMs, their combination with one or more LLMs consistently improved team performance. Our hope is that this simple and effective strategy for integrating the judgments of humans and machines will lead to productive collaborations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08083v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felipe Y\'a\~nez, Xiaoliang Luo, Omar Valerio Minero, Bradley C. Love</dc:creator>
    </item>
  </channel>
</rss>
