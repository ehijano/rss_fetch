<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Nov 2024 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Comparative Study on the Impact of Test-Driven Development (TDD) and Behavior-Driven Development (BDD) on Enterprise Software Delivery Effectiveness</title>
      <link>https://arxiv.org/abs/2411.04141</link>
      <description>arXiv:2411.04141v1 Announce Type: new 
Abstract: This paper compares the impact of Test-Driven Development (TDD) and Behavior-Driven Development (BDD) on software delivery effectiveness within enterprise environments. Using a qualitative research design, data were collected through in-depth interviews with developers and project managers from enterprises adopting TDD or BDD. Moreover, the findings reveal distinct effects of each model on delivery speed, software quality, and team collaboration. Specifically, TDD emphasizes early testing and iterative development, leading to enhanced code quality and fewer defects, while BDD improves cross-functional communication by focusing on behavior specifications that involve stakeholders directly. However, TDD may create a higher initial time investment, and BDD might encounter challenges in requirement clarity. These differences highlight gaps in understanding how each model aligns with varying project types and stakeholder needs, which can guide enterprises in selecting the most suitable model for their unique requirements. The study contributes to the literature by providing insights into the practical application and challenges of TDD and BDD, suggesting future research on their long-term impacts in diverse settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04141v1</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun Cui</dc:creator>
    </item>
    <item>
      <title>Software Design Pattern Model and Data Structure Algorithm Abilities on Microservices Architecture Design in High-tech Enterprises</title>
      <link>https://arxiv.org/abs/2411.04143</link>
      <description>arXiv:2411.04143v1 Announce Type: new 
Abstract: This study investigates the impact of software design model capabilities and data structure algorithm abilities on microservices architecture design within enterprises. Utilizing a qualitative methodology, the research involved in-depth interviews with software architects and developers who possess extensive experience in microservices implementation. The findings reveal that organizations emphasizing robust design models and efficient algorithms achieve superior scalability, performance, and flexibility in their microservices architecture. Notably, participants highlighted that a strong foundation in these areas facilitates better service decomposition, optimizes data processing, and enhances system responsiveness. Despite these insights, gaps remain regarding the integration of emerging technologies and the evolving nature of software design practices. This paper contributes to the existing literature by underscoring the critical role of these competencies in fostering effective microservices architectures and suggests avenues for future research to address identified gaps</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04143v1</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun Cui</dc:creator>
    </item>
    <item>
      <title>Crystal: Illuminating LLM Abilities on Language and Code</title>
      <link>https://arxiv.org/abs/2411.04156</link>
      <description>arXiv:2411.04156v1 Announce Type: new 
Abstract: Large Language Models (LLMs) specializing in code generation (which are also often referred to as code LLMs), e.g., StarCoder and Code Llama, play increasingly critical roles in various software development scenarios. It is also crucial for code LLMs to possess both code generation and natural language abilities for many specific applications, such as code snippet retrieval using natural language or code explanations. The intricate interaction between acquiring language and coding skills complicates the development of strong code LLMs. Furthermore, there is a lack of thorough prior studies on the LLM pretraining strategy that mixes code and natural language. In this work, we propose a pretraining strategy to enhance the integration of natural language and coding capabilities within a single LLM. Specifically, it includes two phases of training with appropriately adjusted code/language ratios. The resulting model, Crystal, demonstrates remarkable capabilities in both domains. Specifically, it has natural language and coding performance comparable to that of Llama 2 and Code Llama, respectively. Crystal exhibits better data efficiency, using 1.4 trillion tokens compared to the more than 2 trillion tokens used by Llama 2 and Code Llama. We verify our pretraining strategy by analyzing the training process and observe consistent improvements in most benchmarks. We also adopted a typical application adaptation phase with a code-centric data mixture, only to find that it did not lead to enhanced performance or training efficiency, underlining the importance of a carefully designed data recipe. To foster research within the community, we commit to open-sourcing every detail of the pretraining, including our training datasets, code, loggings and 136 checkpoints throughout the training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04156v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianhua Tao, Junbo Li, Bowen Tan, Hongyi Wang, William Marshall, Bhargav M Kanakiya, Joel Hestness, Natalia Vassilieva, Zhiqiang Shen, Eric P. Xing, Zhengzhong Liu</dc:creator>
    </item>
    <item>
      <title>An Empirical Study on Automatically Detecting AI-Generated Source Code: How Far Are We?</title>
      <link>https://arxiv.org/abs/2411.04299</link>
      <description>arXiv:2411.04299v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) techniques, especially Large Language Models (LLMs), have started gaining popularity among researchers and software developers for generating source code. However, LLMs have been shown to generate code with quality issues and also incurred copyright/licensing infringements. Therefore, detecting whether a piece of source code is written by humans or AI has become necessary. This study first presents an empirical analysis to investigate the effectiveness of the existing AI detection tools in detecting AI-generated code. The results show that they all perform poorly and lack sufficient generalizability to be practically deployed. Then, to improve the performance of AI-generated code detection, we propose a range of approaches, including fine-tuning the LLMs and machine learning-based classification with static code metrics or code embedding generated from Abstract Syntax Tree (AST). Our best model outperforms state-of-the-art AI-generated code detector (GPTSniffer) and achieves an F1 score of 82.55. We also conduct an ablation study on our best-performing model to investigate the impact of different source code features on its performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04299v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyunjae Suh, Mahan Tafreshipour, Jiawei Li, Adithya Bhattiprolu, Iftekhar Ahmed</dc:creator>
    </item>
    <item>
      <title>Automated Update of Android Deprecated API Usages with Large Language Models</title>
      <link>https://arxiv.org/abs/2411.04387</link>
      <description>arXiv:2411.04387v1 Announce Type: new 
Abstract: Android apps rely on application programming interfaces (APIs) to access various functionalities of Android devices. These APIs however are regularly updated to incorporate new features while the old APIs get deprecated. Even though the importance of updating deprecated API usages with the recommended replacement APIs has been widely recognized, it is non-trivial to update the deprecated API usages. Therefore, the usages of deprecated APIs linger in Android apps and cause compatibility issues in practice. This paper introduces GUPPY, an automated approach that utilizes large language models (LLMs) to update Android deprecated API usages. By employing carefully crafted prompts, GUPPY leverages GPT-4, one of the most powerful LLMs, to update deprecated-API usages, ensuring compatibility in both the old and new API levels. Additionally, GUPPY uses GPT-4 to generate tests, identify incorrect updates, and refine the API usage through an iterative process until the tests pass or a specified limit is reached. Our evaluation, conducted on 360 benchmark API usages from 20 deprecated APIs and an additional 156 deprecated API usages from the latest API levels 33 and 34, demonstrates GUPPY's advantages over the state-of-the-art techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04387v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tarek Mahmud, Bin Duan, Meiru Che, Awatif Yasmin, Anne H. H. Ngu, Guowei Yang</dc:creator>
    </item>
    <item>
      <title>An Empirical Study on the Potential of LLMs in Automated Software Refactoring</title>
      <link>https://arxiv.org/abs/2411.04444</link>
      <description>arXiv:2411.04444v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs), make it potentially feasible to automatically refactor source code with LLMs. However, it remains unclear how well LLMs perform compared to human experts in conducting refactorings automatically and accurately. To fill this gap, in this paper, we conduct an empirical study to investigate the potential of LLMs in automated software refactoring, focusing on the identification of refactoring opportunities and the recommendation of refactoring solutions. We first construct a high-quality refactoring dataset comprising 180 real-world refactorings from 20 projects, and conduct the empirical study on the dataset. With the to-be-refactored Java documents as input, ChatGPT and Gemini identified only 28 and 7 respectively out of the 180 refactoring opportunities. However, explaining the expected refactoring subcategories and narrowing the search space in the prompts substantially increased the success rate of ChatGPT from 15.6% to 86.7%. Concerning the recommendation of refactoring solutions, ChatGPT recommended 176 refactoring solutions for the 180 refactorings, and 63.6% of the recommended solutions were comparable to (even better than) those constructed by human experts. However, 13 out of the 176 solutions suggested by ChatGPT and 9 out of the 137 solutions suggested by Gemini were unsafe in that they either changed the functionality of the source code or introduced syntax errors, which indicate the risk of LLM-based refactoring. To this end, we propose a detect-and-reapply tactic, called RefactoringMirror, to avoid such unsafe refactorings. By reapplying the identified refactorings to the original code using thoroughly tested refactoring engines, we can effectively mitigate the risks associated with LLM-based automated refactoring while still leveraging LLM's intelligence to obtain valuable refactoring recommendations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04444v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bo Liu, Yanjie Jiang, Yuxia Zhang, Nan Niu, Guangjie Li, Hui Liu</dc:creator>
    </item>
    <item>
      <title>Mint: Cost-Efficient Tracing with All Requests Collection via Commonality and Variability Analysis</title>
      <link>https://arxiv.org/abs/2411.04605</link>
      <description>arXiv:2411.04605v1 Announce Type: new 
Abstract: Distributed traces contain valuable information but are often massive in volume, posing a core challenge in tracing framework design: balancing the tradeoff between preserving essential trace information and reducing trace volume. To address this tradeoff, previous approaches typically used a '1 or 0' sampling strategy: retaining sampled traces while completely discarding unsampled ones. However, based on an empirical study on real-world production traces, we discover that the '1 or 0' strategy actually fails to effectively balance this tradeoff.
  To achieve a more balanced outcome, we shift the strategy from the '1 or 0' paradigm to the 'commonality + variability' paradigm. The core of 'commonality + variability' paradigm is to first parse traces into common patterns and variable parameters, then aggregate the patterns and filter the parameters. We propose a cost-efficient tracing framework, Mint, which implements the 'commonality + variability' paradigm on the agent side to enable all requests capturing. Our experiments show that Mint can capture all traces and retain more trace information while optimizing trace storage (reduced to an average of 2.7%) and network overhead (reduced to an average of 4.2%). Moreover, experiments also demonstrate that Mint is lightweight enough for production use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04605v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haiyu Huang, Cheng Chen, Kunyi Chen, Pengfei Chen, Guangba Yu, Zilong He, Yilun Wang, Huxing Zhang, Qi Zhou</dc:creator>
    </item>
    <item>
      <title>Distinguishing LLM-generated from Human-written Code by Contrastive Learning</title>
      <link>https://arxiv.org/abs/2411.04704</link>
      <description>arXiv:2411.04704v1 Announce Type: new 
Abstract: Large language models (LLMs), such as ChatGPT released by OpenAI, have attracted significant attention from both industry and academia due to their demonstrated ability to generate high-quality content for various tasks. Despite the impressive capabilities of LLMs, there are growing concerns regarding their potential risks in various fields, such as news, education, and software engineering. Recently, several commercial and open-source LLM-generated content detectors have been proposed, which, however, are primarily designed for detecting natural language content without considering the specific characteristics of program code. This paper aims to fill this gap by proposing a novel ChatGPT-generated code detector, CodeGPTSensor, based on a contrastive learning framework and a semantic encoder built with UniXcoder. To assess the effectiveness of CodeGPTSensor on differentiating ChatGPT-generated code from human-written code, we first curate a large-scale Human and Machine comparison Corpus (HMCorp), which includes 550K pairs of human-written and ChatGPT-generated code (i.e., 288K Python code pairs and 222K Java code pairs). Based on the HMCorp dataset, our qualitative and quantitative analysis of the characteristics of ChatGPT-generated code reveals the challenge and opportunity of distinguishing ChatGPT-generated code from human-written code with their representative features. Our experimental results indicate that CodeGPTSensor can effectively identify ChatGPT-generated code, outperforming all selected baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04704v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaodan Xu, Chao Ni, Xinrong Guo, Shaoxuan Liu, Xiaoya Wang, Kui Liu, Xiaohu Yang</dc:creator>
    </item>
    <item>
      <title>The New Dynamics of Open Source: Relicensing, Forks, &amp; Community Impact</title>
      <link>https://arxiv.org/abs/2411.04739</link>
      <description>arXiv:2411.04739v1 Announce Type: new 
Abstract: Many popular open source projects are owned and driven by vendors, and in today's difficult economic climate, those vendors are under increasing pressure from investors to deliver a strong return on their investments. One response to this pressure has been the relicensing of popular open source projects to more restrictive licenses in the hopes of generating more revenue, disrupting the idea of open source as a digital commons. In some cases, relicensing has resulted in a hard fork of the original project. These relicensing events and resulting forks can be disruptive to the organizations and individuals using these open source projects. This research compares and contrasts organizational affiliation data from three case studies based on license changes that resulted in forks: Elasticsearch / OpenSearch, Redis / Valkey, and Terraform / OpenTofu. The research indicates that the forks resulting from these relicensing events have more organizational diversity than the original projects, especially when the forks are created under a neutral foundation, like the Linux Foundation, rather than by a single company.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04739v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Dawn Foster</dc:creator>
    </item>
    <item>
      <title>Quantum Neural Network Classifier for Cancer Registry System Testing: A Feasibility Study</title>
      <link>https://arxiv.org/abs/2411.04740</link>
      <description>arXiv:2411.04740v1 Announce Type: new 
Abstract: The Cancer Registry of Norway (CRN) is a part of the Norwegian Institute of Public Health (NIPH) and is tasked with producing statistics on cancer among the Norwegian population. For this task, CRN develops, tests, and evolves a software system called Cancer Registration Support System (CaReSS). It is a complex socio-technical software system that interacts with many entities (e.g., hospitals, medical laboratories, and other patient registries) to achieve its task. For cost-effective testing of CaReSS, CRN has employed EvoMaster, an AI-based REST API testing tool combined with an integrated classical machine learning model. Within this context, we propose Qlinical to investigate the feasibility of using, inside EvoMaster, a Quantum Neural Network (QNN) classifier, i.e., a quantum machine learning model, instead of the existing classical machine learning model. Results indicate that Qlinical can achieve performance comparable to that of EvoClass. We further explore the effects of various QNN configurations on performance and offer recommendations for optimal QNN settings for future QNN developers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04740v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyi Wang, Shaukat Ali, Paolo Arcaini, Narasimha Raghavan Veeraragavan, Jan F. Nyg{\aa}rd</dc:creator>
    </item>
    <item>
      <title>Using Rely/Guarantee to Pinpoint Assumptions underlying Security Protocols</title>
      <link>https://arxiv.org/abs/2311.15189</link>
      <description>arXiv:2311.15189v2 Announce Type: cross 
Abstract: The verification of security protocols is essential, in order to ensure the absence of potential attacks. However, verification results are only valid with respect to the assumptions under which the verification was performed. These assumptions are often hidden and are difficult to identify, making it unclear whether a given protocol is safe to deploy into a particular environment. Rely/guarantee provides a mechanism for abstractly reasoning about the interference from the environment. Using this approach, the assumptions are made clear and precise. This paper investigates this approach on the Needham-Schroeder Public Key protocol, showing that the technique can effectively uncover the assumptions under which the protocol can withstand attacks from intruders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15189v2</guid>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nisansala P. Yatapanage, Cliff B. Jones</dc:creator>
    </item>
    <item>
      <title>Benchmarking Large Language Models with Integer Sequence Generation Tasks</title>
      <link>https://arxiv.org/abs/2411.04372</link>
      <description>arXiv:2411.04372v1 Announce Type: cross 
Abstract: This paper presents a novel benchmark where the large language model (LLM) must write code that computes integer sequences from the Online Encyclopedia of Integer Sequences (OEIS), a widely-used resource for mathematical sequences. The benchmark is designed to evaluate both the correctness of the generated code and its computational efficiency. Our benchmark reveals that the o1 series of models outperform other frontier models from OpenAI, Anthropic, Meta, and Google in accuracy and cheating rates across both easy and hard integer sequences. In order to ensure models do not exploit memorized sequence values, we introduce an automated cheating detection mechanism that flags the use of lookup tables and validated this automation against human cheating evaluations. This benchmark provides a meaningful challenge for current LLMs, offering insights into their mathematical reasoning and code writing capabilities, which can guide future research directions and model development in mathematical reasoning and code synthesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04372v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel O'Malley, Manish Bhattarai, Javier Santos</dc:creator>
    </item>
    <item>
      <title>Using AI-Based Coding Assistants in Practice: State of Affairs, Perceptions, and Ways Forward</title>
      <link>https://arxiv.org/abs/2406.07765</link>
      <description>arXiv:2406.07765v2 Announce Type: replace 
Abstract: Context. The last several years saw the emergence of AI assistants for code - multi-purpose AI-based helpers in software engineering. As they become omnipresent in all aspects of software development, it becomes critical to understand their usage patterns.
  Objective. We aim to better understand how specifically developers are using AI assistants, why they are not using them in certain parts of their development workflow, and what needs to be improved in the future.
  Methods. In this work, we carried out a large-scale survey aimed at how AI assistants are used, focusing on specific software development activities and stages. We collected opinions of 481 programmers on five broad activities: (a) implementing new features, (b) writing tests, (c) bug triaging, (d) refactoring, and (e) writing natural-language artifacts, as well as their individual stages.
  Results. Our results provide a novel comparison of different stages where AI assistants are used that is both comprehensive and detailed. It highlights specific activities that developers find less enjoyable and want to delegate to an AI assistant, e.g., writing tests and natural-language artifacts. We also determine more granular stages where AI assistants are used, such as generating tests and generating docstrings, as well as less studied parts of the workflow, such as generating test data. Among the reasons for not using assistants, there are general aspects like trust and company policies, as well as more concrete issues like the lack of project-size context, which can be the focus of the future research.
  Conclusion. The provided analysis highlights stages of software development that developers want to delegate and that are already popular for using AI assistants, which can be a good focus for features aimed to help developers right now. The main reasons for not using AI assistants can serve as a guideline for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07765v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.infsof.2024.107610</arxiv:DOI>
      <dc:creator>Agnia Sergeyuk, Yaroslav Golubev, Timofey Bryksin, Iftekhar Ahmed</dc:creator>
    </item>
    <item>
      <title>A Taxonomy of Multi-Layered Runtime Guardrails for Designing Foundation Model-Based Agents: Swiss Cheese Model for AI Safety by Design</title>
      <link>https://arxiv.org/abs/2408.02205</link>
      <description>arXiv:2408.02205v2 Announce Type: replace 
Abstract: Foundation Model (FM) based agents are revolutionizing application development across various domains. However, their rapidly growing capabilities and autonomy have raised significant concerns about AI safety. Designing effective guardrails for these agents is challenging due to their autonomous and non-deterministic behavior, and the involvement of multiple artifacts -- such as goals, prompts, plans, tools, knowledge bases, and intermediate and final results. Addressing these unique challenges runtime requires multi-layered guardrails that operate effectively at various levels of the agent architecture, similar to the Swiss Cheese Model. In this paper, we present a taxonomy of multi-layered runtime guardrails to classify and compare their characteristics and design options, grounded on a systematic literature review and guided by the Swiss Cheese Model. This taxonomy is organized into external and internal quality attributes and design options categories. We also highlight the relationships between guardrails, the associated risks they mitigate, and the quality attributes they impact in agent architectures. Thus, the proposed taxonomy provides structured and concrete guidance for making architectural design decisions when implementing multi-layered guardrails while emphasizing the trade-offs inherent in these decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02205v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Shamsujjoha, Qinghua Lu, Dehai Zhao, Liming Zhu</dc:creator>
    </item>
    <item>
      <title>Identifying Performance Issues in Cloud Service Systems Based on Relational-Temporal Features</title>
      <link>https://arxiv.org/abs/2307.10869</link>
      <description>arXiv:2307.10869v3 Announce Type: replace-cross 
Abstract: Cloud systems are susceptible to performance issues, which may cause service-level agreement violations and financial losses. In current practice, crucial metrics are monitored periodically to provide insight into the operational status of components. Identifying performance issues is often formulated as an anomaly detection problem, which is tackled by analyzing each metric independently. However, this approach overlooks the complex dependencies existing among cloud components. Some graph neural network-based methods take both temporal and relational information into account, however, the correlation violations in the metrics that serve as indicators of underlying performance issues are difficult for them to identify. Furthermore, a large volume of components in a cloud system results in a vast array of noisy metrics. This complexity renders it impractical for engineers to fully comprehend the correlations, making it challenging to identify performance issues accurately. To address these limitations, we propose Identifying Performance Issues based on Relational-Temporal Features (ISOLATE ), a learning-based approach that leverages both the relational and temporal features of metrics to identify performance issues. In particular, it adopts a graph neural network with attention to characterizing the relations among metrics and extracts long-term and multi-scale temporal patterns using a GRU and a convolution network, respectively. The learned graph attention weights can be further used to localize the correlation-violated metrics. Moreover, to relieve the impact of noisy data, ISOLATE utilizes a positive unlabeled learning strategy that tags pseudo-labels based on a small portion of confirmed negative examples. Extensive evaluation on both public and industrial datasets shows that ISOLATE outperforms all baseline models with 0.945 F1-score and 0.920 Hit rate@3.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10869v3</guid>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wenwei Gu, Jinyang Liu, Zhuangbin Chen, Jianping Zhang, Yuxin Su, Jiazhen Gu, Cong Feng, Zengyin Yang, Yongqiang Yang, Michael Lyu</dc:creator>
    </item>
    <item>
      <title>A Security Risk Assessment Method for Distributed Ledger Technology (DLT) based Applications: Three Industry Case Studies</title>
      <link>https://arxiv.org/abs/2401.12358</link>
      <description>arXiv:2401.12358v2 Announce Type: replace-cross 
Abstract: Distributed ledger technologies have gained significant attention and adoption in recent years. Despite various security features distributed ledger technology provides, they are vulnerable to different and new malicious attacks, such as selfish mining and Sybil attacks. While such vulnerabilities have been investigated, detecting and discovering appropriate countermeasures still need to be reported. Cybersecurity knowledge is limited and fragmented in this domain, while distributed ledger technology usage grows daily. Thus, research focusing on overcoming potential attacks on distributed ledgers is required. This study aims to raise awareness of the cybersecurity of distributed ledger technology by designing a security risk assessment method for distributed ledger technology applications. We have developed a database with possible security threats and known attacks on distributed ledger technologies to accompany the method, including sets of countermeasures. We employed a semi-systematic literature review combined with method engineering to develop a method that organizations can use to assess their cybersecurity risk for distributed ledger applications. The method has subsequently been evaluated in three case studies, which show that the method helps to effectively conduct security risk assessments for distributed ledger applications in these organizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12358v2</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Fri, 08 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Elena Baninemeh, Marre Slikker, Katsiaryna Labunets, Slinger Jansen</dc:creator>
    </item>
  </channel>
</rss>
