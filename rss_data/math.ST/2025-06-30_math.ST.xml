<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Jun 2025 04:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Monte Carlo and quasi-Monte Carlo integration for likelihood functions</title>
      <link>https://arxiv.org/abs/2506.21733</link>
      <description>arXiv:2506.21733v1 Announce Type: new 
Abstract: We compare the integration error of Monte Carlo (MC) and quasi-Monte Carlo (QMC) methods for approximating the normalizing constant of posterior distributions and certain marginal likelihoods. In doing so, we characterize the dependency of the relative and absolute integration errors on the number of data points ($n$), the number of grid points ($m$) and the dimension of the integral ($p$). We find that if the dimension of the integral remains fixed as $n$ and $m$ tend to infinity, the scaling rate of the relative error of MC integration includes an additional $n^{1/2}\log(n)^{p/2}$ data-dependent factor, while for QMC this factor is $\log(n)^{p/2}$. In this scenario, QMC will outperform MC if $\log(m)^{p - 1/2}/\sqrt{mn\log(n)} &lt; 1$, which differs from the usual result that QMC will outperform MC if $\log(m)^p/m^{1/2} &lt; 1$.The accuracies of MC and QMC methods are also examined in the high-dimensional setting as $p \rightarrow \infty$, where MC gives more optimistic results as the scaling in dimension is slower than that of QMC when the Halton sequence is used to construct the low discrepancy grid; however both methods display poor dimensional scaling as expected. An additional contribution of this work is a bound on the high-dimensional scaling of the star discrepancy for the Halton sequence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.21733v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanbo Tang</dc:creator>
    </item>
    <item>
      <title>Schoenberg characterization of continuous non-stationary isotropic positive definite kernels</title>
      <link>https://arxiv.org/abs/2506.22048</link>
      <description>arXiv:2506.22048v1 Announce Type: new 
Abstract: We provide a characterization for the continuous positive definite kernels on $\mathbb R^d$ that are invariant to linear isometries, i.e. invariant under the orthogonal group $O(d)$. Furthermore, we provide necessary and sufficient conditions for these kernels to be strictly positive definite. This class of isotropic kernels is fairly general: First, it unifies stationary isotropic and dot product kernels, and second, it includes neural network kernels that arise from infinite-width limits of neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22048v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Felix Benning, Max David Sch\"olpple</dc:creator>
    </item>
    <item>
      <title>Mixing Time Bounds for the Gibbs Sampler under Isoperimetry</title>
      <link>https://arxiv.org/abs/2506.22258</link>
      <description>arXiv:2506.22258v1 Announce Type: new 
Abstract: We establish bounds on the conductance for the systematic-scan and random-scan Gibbs samplers when the target distribution satisfies a Poincare or log-Sobolev inequality and possesses sufficiently regular conditional distributions. These bounds lead to mixing time guarantees that extend beyond the log-concave setting, offering new insights into the convergence behavior of Gibbs sampling in broader regimes. Moreover, we demonstrate that our results remain valid for log-Lipschitz and log-smooth target distributions. Our approach relies on novel three-set isoperimetric inequalities and a sequential coupling argument for the Gibbs sampler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22258v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Goyal, George Deligiannidis, Nikolas Kantas</dc:creator>
    </item>
    <item>
      <title>PC Adjusted Testing for Low Dimensional Parameters</title>
      <link>https://arxiv.org/abs/2209.10774</link>
      <description>arXiv:2209.10774v2 Announce Type: replace 
Abstract: In this paper, we investigate the impact of high-dimensional Principal Component (PC) adjustments on inferring the effects of variables on outcomes, with a focus on applications in genetic association studies where PC adjustment is commonly used to account for population stratification. We consider high-dimensional linear regression in the regime where the number of covariates grows proportionally to the number of samples. In this setting, we provide an asymptotically precise understanding of when PC adjustments yield valid tests with controlled Type I error rates. Our results demonstrate that, under both fixed and diverging signal strengths, PC regression often fails to control the Type I error at the desired nominal level. Furthermore, we establish necessary and sufficient conditions for Type I error inflation based on covariate distributions. These theoretical findings are further supported by a series of numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.10774v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sohom Bhattacharya, Rounak Dey, Rajarshi Mukherjee</dc:creator>
    </item>
    <item>
      <title>Is it easier to count communities than find them?</title>
      <link>https://arxiv.org/abs/2212.10872</link>
      <description>arXiv:2212.10872v2 Announce Type: replace 
Abstract: Random graph models with community structure have been studied extensively in the literature. For both the problems of detecting and recovering community structure, an interesting landscape of statistical and computational phase transitions has emerged. A natural unanswered question is: might it be possible to infer properties of the community structure (for instance, the number and sizes of communities) even in situations where actually finding those communities is believed to be computationally hard? We show the answer is no. In particular, we consider certain hypothesis testing problems between models with different community structures, and we show (in the low-degree polynomial framework) that testing between two options is as hard as finding the communities.
  Our methods give the first computational lower bounds for testing between two different ``planted'' distributions, whereas previous results have considered testing between a planted distribution and an i.i.d. ``null'' distribution. We also show a formal relationship between the low--degree frameworks for recovery in a planted model and for testing two planted models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.10872v2</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cynthia Rush, Fiona Skerman, Alexander S. Wein, Dana Yang</dc:creator>
    </item>
    <item>
      <title>Characterizing Dependence of Samples along the Langevin Dynamics and Algorithms via Contraction of $\Phi$-Mutual Information</title>
      <link>https://arxiv.org/abs/2402.17067</link>
      <description>arXiv:2402.17067v3 Announce Type: replace 
Abstract: The mixing time of a Markov chain determines how fast the iterates of the Markov chain converge to the stationary distribution; however, it does not control the dependencies between samples along the Markov chain. In this paper, we study the question of how fast the samples become approximately independent along popular Markov chains for continuous-space sampling: the Langevin dynamics in continuous time, and the Unadjusted Langevin Algorithm and the Proximal Sampler in discrete time. We measure the dependence between samples via $\Phi$-mutual information, which is a broad generalization of the standard mutual information, and which is equal to $0$ if and only if the the samples are independent. We show that along these Markov chains, the $\Phi$-mutual information between the first and the $k$-th iterate decreases to $0$ exponentially fast in $k$ when the target distribution is strongly log-concave. Our proof technique is based on showing the Strong Data Processing Inequalities (SDPIs) hold along the Markov chains. To prove fast mixing of the Markov chains, we only need to show the SDPIs hold for the stationary distribution. In contrast, to prove the contraction of $\Phi$-mutual information, we need to show the SDPIs hold along the entire trajectories of the Markov chains; we prove this when the iterates along the Markov chains satisfy the corresponding $\Phi$-Sobolev inequality, which is implied by the strong log-concavity of the target distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17067v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaming Liang, Siddharth Mitra, Andre Wibisono</dc:creator>
    </item>
    <item>
      <title>Adaptive inference with random ellipsoids through Conformal Conditional Linear Expectation</title>
      <link>https://arxiv.org/abs/2409.18508</link>
      <description>arXiv:2409.18508v2 Announce Type: replace 
Abstract: We propose two new conformity scores for conformal prediction, in a general multivariate regression framework. The underlying score functions are based on a covariance analysis of the residuals and the input points. We give theoretical guarantees on the prediction sets, which consist in explicit ellipsoids. We study the asymptotic properties of the ellipsoids, and show that their volume is reduced compared to that of classic balls, under ellipticity assumptions. Finally, we illustrate the effectiveness of all our results on an in-depth numerical study, including heavy-tailed as well as non-elliptical distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18508v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iain Henderson (IMT), Adrien Mazoyer (IMT), Fabrice Gamboa (IMT)</dc:creator>
    </item>
    <item>
      <title>To Study Properties of a Known Procedure in Adaptive Sequential Sampling Design</title>
      <link>https://arxiv.org/abs/2412.17791</link>
      <description>arXiv:2412.17791v3 Announce Type: replace 
Abstract: We consider the procedure proposed by Bhandari et al. (2009) in the context of two-treatment clinical trials, with the objective of minimizing the applications of the less effective drug to the least number of patients. Our focus is on an adaptive sequential procedure that is both simple and intuitive. Through a refined theoretical analysis, we establish that the number of applications of the less effective drug is a finite random variable whose all moments are also finite. In contrast, Bhandari et al. (2009) observed that this number increases logarithmically with the total sample size. We attribute this discrepancy to differences in their choice of the initial sample size and the method of analysis employed. We further extend the allocation rule to multi-treatment setup and derive analogous finiteness results, reinforcing the generalizability of our findings. Extensive simulation studies and real-data analyses support theoretical developments, showing stabilization in allocation and reduced patient exposure to inferior treatments as the total sample size grows. These results enhance the long-term ethical strength of the proposed adaptive allocation strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17791v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sampurna Kundu, Jayant Jha, Subir Kumar Bhandari</dc:creator>
    </item>
    <item>
      <title>Estimation of rates in population-age-dependent processes by means of test functions</title>
      <link>https://arxiv.org/abs/2504.06516</link>
      <description>arXiv:2504.06516v2 Announce Type: replace 
Abstract: This paper aims to develop practical applications of the model for the highly technical measure-valued populations developed by the authors in \cite{FanEtal20}. We consider the problem of estimation of parameters in the general age and population-dependent model, in which the individual birth and death rates depend not only on the age of the individual but also on the whole population composition. We derive new estimators of the rates based on the use of test functions in the functional Law of Large Numbers and Central Limit Theorem for populations with a large carrying capacity. We consider the rates to be simple functions, that take finitely many values both in age $x$ and measure $A$, which leads to systems of linear equations. The proposed method of using test functions for estimation is a radically new approach which can be applied to a wide range of models of dynamical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06516v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>q-bio.PE</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Yen Fan, Kais Hamza, Fima C. Klebaner, Ziwen Zhong</dc:creator>
    </item>
    <item>
      <title>Robust Detection of Watermarks for Large Language Models Under Human Edits</title>
      <link>https://arxiv.org/abs/2411.13868</link>
      <description>arXiv:2411.13868v2 Announce Type: replace-cross 
Abstract: Watermarking has offered an effective approach to distinguishing text generated by large language models (LLMs) from human-written text. However, the pervasive presence of human edits on LLM-generated text dilutes watermark signals, thereby significantly degrading detection performance of existing methods. In this paper, by modeling human edits through mixture model detection, we introduce a new method in the form of a truncated goodness-of-fit test for detecting watermarked text under human edits, which we refer to as Tr-GoF. We prove that the Tr-GoF test achieves optimality in robust detection of the Gumbel-max watermark in a certain asymptotic regime of substantial text modifications and vanishing watermark signals. Importantly, Tr-GoF achieves this optimality \textit{adaptively} as it does not require precise knowledge of human edit levels or probabilistic specifications of the LLMs, in contrast to the optimal but impractical (Neyman--Pearson) likelihood ratio test. Moreover, we establish that the Tr-GoF test attains the highest detection efficiency rate in a certain regime of moderate text modifications. In stark contrast, we show that sum-based detection rules, as employed by existing methods, fail to achieve optimal robustness in both regimes because the additive nature of their statistics is less resilient to edit-induced noise. Finally, we demonstrate the competitive and sometimes superior empirical performance of the Tr-GoF test on both synthetic data and open-source LLMs in the OPT and LLaMA families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13868v2</guid>
      <category>stat.ME</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Li, Feng Ruan, Huiyuan Wang, Qi Long, Weijie J. Su</dc:creator>
    </item>
  </channel>
</rss>
