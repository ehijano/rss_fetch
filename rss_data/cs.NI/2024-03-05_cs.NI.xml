<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Mar 2024 14:39:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 05 Mar 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Design and Evaluation of SEANet: a Software-defined Networking Platform for the Internet of Underwater Things</title>
      <link>https://arxiv.org/abs/2403.01009</link>
      <description>arXiv:2403.01009v1 Announce Type: new 
Abstract: Investigating and safeguarding our oceans is vital for a host of applications and tasks, including combating climate change, ensuring the integrity of subsea infrastructures, and for coastal protection. Achieving these essential functions depends on the deployment of cost-effective, versatile underwater sensor networks that can efficiently collect and transmit data to land. However, the success of such networks is currently hindered by the significant limitations of existing underwater modems, which limits their operational use to a narrow range of applications. This paper presents and evaluates the performance of the SEANet software-defined networking platform, for the Internet of Underwater Things (IoUT), addressing the limitations of existing underwater communication technologies. It presents the development and comprehensive testing of an adaptable, high-data-rate, and integration-friendly underwater platform that reconfigures in real-time to meet the demands of various marine applications. With an acoustic front end, the platform significantly outperforms conventional modems, achieving more than double the data rate at 150 kbit/s. Experiments conducted in oceanic conditions demonstrate its capabilities in channel characterization, OFDM link establishment, and compatibility with the JANUS communication standard. Our platform advances the IoUT by providing a versatile, scalable solution that can incorporate multiple physical layers and support an array of tasks, making it pivotal for real-time ocean data analysis and the expansion of ocean-related digital applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01009v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deniz Unal, Sara Falleni, Kerem Enhos, Emrecan Demirors, Stefano Basagni, Tommaso Melodia</dc:creator>
    </item>
    <item>
      <title>Misconfiguration in O-RAN: Analysis of the impact of AI/ML</title>
      <link>https://arxiv.org/abs/2403.01180</link>
      <description>arXiv:2403.01180v1 Announce Type: new 
Abstract: User demand on network communication infrastructure has never been greater with applications such as extended reality, holographic telepresence, and wireless brain-computer interfaces challenging current networking capabilities. Open RAN (O-RAN) is critical to supporting new and anticipated uses of 6G and beyond. It promotes openness and standardisation, increased flexibility through the disaggregation of Radio Access Network (RAN) components, supports programmability, flexibility, and scalability with technologies such as Software-Defined Networking (SDN), Network Function Virtualization (NFV), and cloud, and brings automation through the RAN Intelligent Controller (RIC). Furthermore, the use of xApps, rApps, and Artificial Intelligence/Machine Learning (AI/ML) within the RIC enables efficient management of complex RAN operations. However, due to the open nature of O-RAN and its support for heterogeneous systems, the possibility of misconfiguration problems becomes critical. In this paper, we present a thorough analysis of the potential misconfiguration issues in O-RAN with respect to integration and operation, the use of SDN and NFV, and, specifically, the use of AI/ML. The opportunity for AI/ML to be used to identify these misconfigurations is investigated. A case study is presented to illustrate the direct impact on the end user of conflicting policies amongst xApps along with a potential AI/ML-based solution to this problem. This research presents a first analysis of the impact of AI/ML on misconfiguration challenges in O-RAN</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01180v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noe Yungaicela-Naula, Vishal Sharma, Sandra Scott-Hayward</dc:creator>
    </item>
    <item>
      <title>Experimental Evaluation of the ETSI DCC Adaptive Approach and Related Algorithms</title>
      <link>https://arxiv.org/abs/2403.01297</link>
      <description>arXiv:2403.01297v1 Announce Type: new 
Abstract: Decentralized Congestion Control (DCC) mechanisms have been a core part of protocol stacks for vehicular networks since their inception and standardization. The ETSI ITS-G5 protocol stack for vehicular communications considers the usage of DCC not only in the network or access layers, but also as a part of the cross-layer architecture that influences how often messages are generated and transmitted. ETSI DCC mechanisms have evolved from a reactive approach based on a finite state machine, to an adaptive approach that relies on a linear control algorithm. This linear control algorithm, called LIMERIC, is the basis of the mechanism used in the ETSI DCC Adaptive Approach. The behavior of this algorithm depends on a set of parameters. Different values for these parameters have been proposed in the literature, including those defined in the ETSI specification. A recent proposal is Dual-$\alpha$, which chooses parameters to improve convergence and fairness when the algorithm has to react to fast changes in the use of the shared medium (transitory situations). This article evaluates, by means of simulations, the performance of the ETSI DCC Adaptive Approach and related algorithms, considering both steady state and transitory situations. Results show that a bad selection of parameters can make a DCC algorithm ineffective, that the ETSI DCC Adaptive algorithm performs well in steady state conditions, and that Dual-$\alpha$ performs as well in steady state conditions and outperforms the ETSI DCC Adaptive Approach in transitory scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01297v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2020.2980377</arxiv:DOI>
      <arxiv:journal_reference>Oscar Amador, Ignacio Soto, Maria Calderon and Manuel Urue\~na, "Experimental Evaluation of the ETSI DCC Adaptive Approach and Related Algorithms," in IEEE Access, vol. 8, pp. 49798-49811, 2020</arxiv:journal_reference>
      <dc:creator>Oscar Amador, Ignacio Soto, Maria Calderon, Manuel Urue\~na</dc:creator>
    </item>
    <item>
      <title>Superflows: A New Tool for Forensic Network Flow Analysis</title>
      <link>https://arxiv.org/abs/2403.01314</link>
      <description>arXiv:2403.01314v1 Announce Type: new 
Abstract: Network security analysts gather data from diverse sources, from high-level summaries of network flow and traffic volumes to low-level details such as service logs from servers and the contents of individual packets. They validate and check this data against traffic patterns and historical indicators of compromise. Based on the results of this analysis, a decision is made to either automatically manage the traffic or report it to an analyst for further investigation. Unfortunately, due rapidly increasing traffic volumes, there are far more events to check than operational teams can handle for effective forensic analysis. However, just as packets are grouped into flows that share a commonality, we argue that a high-level construct for grouping network flows into a set a flows that share a hypothesis is needed to significantly improve the quality of operational network response by increasing Events Per Analysts Hour (EPAH).
  In this paper, we propose a formalism for describing a superflow construct, which we characterize as an aggregation of one or more flows based on an analyst-specific hypothesis about traffic behavior. We demonstrate simple superflow constructions and representations, and perform a case study to explain how the formalism can be used to reduce the volume of data for forensic analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01314v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Collins, Jyotirmoy V. Deshmukh, Dristi Dinesh, Mukund Raghothaman, Srivatsan Ravi, Yuan Xia</dc:creator>
    </item>
    <item>
      <title>Towards Memory-Efficient Traffic Policing in Time-Sensitive Networking</title>
      <link>https://arxiv.org/abs/2403.01652</link>
      <description>arXiv:2403.01652v1 Announce Type: new 
Abstract: Time-Sensitive Networking (TSN) is an emerging real-time Ethernet technology that provides deterministic communication for time-critical traffic. At its core, TSN relies on Time-Aware Shaper (TAS) for pre-allocating frames in specific time intervals and Per-Stream Filtering and Policing (PSFP) for mitigating the fatal disturbance of unavoidable frame drift. However, as first identified in this work, PSFP incurs heavy memory consumption during policing, hindering normal switching functionalities.
  This work proposes a lightweight policing design called FooDog, which could facilitate sub-microsecond jitter with ultra-low memory consumption. FooDog employs a period-wise and stream-wise structure to realize the memory-efficient PSFP without loss of determinism. Results using commercial FPGAs in typical aerospace scenarios show that FooDog could keep end-to-end time-sensitive traffic jitter &lt;150 nanoseconds in the presence of abnormal traffic, comparable to typical TSN performance without anomalies. Meanwhile, it consumes merely hundreds of kilobits of memory, reducing &gt;90% of on-chip memory overheads than unoptimized PSFP design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01652v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuyan Jiang, Xiangrui Yang, Tongqing Zhou, Wenwen Fu, Wei Quan, Yihao Jiao, Yinhan Sun, Zhigang Sun</dc:creator>
    </item>
    <item>
      <title>Graph neural network for in-network placement of real-time metaverse tasks in next-generation network</title>
      <link>https://arxiv.org/abs/2403.01780</link>
      <description>arXiv:2403.01780v1 Announce Type: new 
Abstract: This study addresses the challenge of real-time metaverse applications by proposing an in-network placement and task-offloading solution for delay-constrained computing tasks in next-generation networks. The metaverse, envisioned as a parallel virtual world, requires seamless real-time experiences across diverse applications. The study introduces a software-defined networking (SDN)-based architecture and employs graph neural network (GNN) techniques for intelligent and adaptive task allocation in in-network computing (INC). Considering time constraints and computing capabilities, the proposed model optimally decides whether to offload rendering tasks to INC nodes or edge server. Extensive experiments demonstrate the superior performance of the proposed GNN model, achieving 97% accuracy compared to 72% for multilayer perceptron (MLP) and 70% for decision trees (DTs). The study fills the research gap in in-network placement for real-time metaverse applications, offering insights into efficient rendering task handling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01780v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sulaiman Muhammad Rashid, Ibrahim Aliyu, Il-Kwon Jeong, Tai-Won Um, Jinsul Kim</dc:creator>
    </item>
    <item>
      <title>Towards Fair and Efficient Learning-based Congestion Control</title>
      <link>https://arxiv.org/abs/2403.01798</link>
      <description>arXiv:2403.01798v1 Announce Type: new 
Abstract: Recent years have witnessed a plethora of learning-based solutions for congestion control (CC) that demonstrate better performance over traditional TCP schemes. However, they fail to provide consistently good convergence properties, including {\em fairness}, {\em fast convergence} and {\em stability}, due to the mismatch between their objective functions and these properties. Despite being intuitive, integrating these properties into existing learning-based CC is challenging, because: 1) their training environments are designed for the performance optimization of single flow but incapable of cooperative multi-flow optimization, and 2) there is no directly measurable metric to represent these properties into the training objective function.
  We present Astraea, a new learning-based congestion control that ensures fast convergence to fairness with stability. At the heart of Astraea is a multi-agent deep reinforcement learning framework that explicitly optimizes these convergence properties during the training process by enabling the learning of interactive policy between multiple competing flows, while maintaining high performance. We further build a faithful multi-flow environment that emulates the competing behaviors of concurrent flows, explicitly expressing convergence properties to enable their optimization during training. We have fully implemented Astraea and our comprehensive experiments show that Astraea can quickly converge to fairness point and exhibit better stability than its counterparts. For example, \sys achieves near-optimal bandwidth sharing (i.e., fairness) when multiple flows compete for the same bottleneck, delivers up to 8.4$\times$ faster convergence speed and 2.8$\times$ smaller throughput deviation, while achieving comparable or even better performance over prior solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01798v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xudong Liao, Han Tian, Chaoliang Zeng, Xinchen Wan, Kai Chen</dc:creator>
    </item>
    <item>
      <title>Towards Intent-Based Network Management: Large Language Models for Intent Extraction in 5G Core Networks</title>
      <link>https://arxiv.org/abs/2403.02238</link>
      <description>arXiv:2403.02238v1 Announce Type: new 
Abstract: The integration of Machine Learning and Artificial Intelligence (ML/AI) into fifth-generation (5G) networks has made evident the limitations of network intelligence with ever-increasing, strenuous requirements for current and next-generation devices. This transition to ubiquitous intelligence demands high connectivity, synchronicity, and end-to-end communication between users and network operators, and will pave the way towards full network automation without human intervention. Intent-based networking is a key factor in the reduction of human actions, roles, and responsibilities while shifting towards novel extraction and interpretation of automated network management. This paper presents the development of a custom Large Language Model (LLM) for 5G and next-generation intent-based networking and provides insights into future LLM developments and integrations to realize end-to-end intent-based networking for fully automated network intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02238v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dimitrios Michael Manias, Ali Chouman, Abdallah Shami</dc:creator>
    </item>
    <item>
      <title>Probabilistic Fault-Tolerant Robust Traffic Grooming in OTN-over-DWDM Networks</title>
      <link>https://arxiv.org/abs/2403.02254</link>
      <description>arXiv:2403.02254v1 Announce Type: new 
Abstract: The development of next-generation networks is revolutionizing network operators' management and orchestration practices worldwide. The critical services supported by these networks require increasingly stringent performance requirements, especially when considering the aspect of network reliability. This increase in reliability, coupled with the mass generation and consumption of information stemming from the increasing complexity of the network and the integration of artificial intelligence agents, affects transport networks, which will be required to allow the feasibility of such services to materialize. To this end, traditional recovery schemes are inadequate to ensure the resilience requirements of next-generation critical services given the increasingly dynamic nature of the network. The work presented in this paper proposes a probabilistic and fault-tolerant robust traffic grooming model for OTN-over-DWDM networks. The model's parameterization gives network operators the ability to control the level of protection and reliability required to meet their quality of service and service level agreement guarantees. The results demonstrate that the robust solution can ensure fault tolerance even in the face of demand uncertainty without service disruptions and the need for reactive network maintenance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02254v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dimitrios Michael Manias, Joe Naoum-Sawaya, Abbas Javadtalab, Abdallah Shami</dc:creator>
    </item>
    <item>
      <title>FedRDMA: Communication-Efficient Cross-Silo Federated LLM via Chunked RDMA Transmission</title>
      <link>https://arxiv.org/abs/2403.00881</link>
      <description>arXiv:2403.00881v1 Announce Type: cross 
Abstract: Communication overhead is a significant bottleneck in federated learning (FL), which has been exaggerated with the increasing size of AI models. In this paper, we propose FedRDMA, a communication-efficient cross-silo FL system that integrates RDMA into the FL communication protocol. To overcome the limitations of RDMA in wide-area networks (WANs), FedRDMA divides the updated model into chunks and designs a series of optimization techniques to improve the efficiency and robustness of RDMA-based communication. We implement FedRDMA atop the industrial federated learning framework and evaluate it on a real-world cross-silo FL scenario. The experimental results show that \sys can achieve up to 3.8$\times$ speedup in communication efficiency compared to traditional TCP/IP-based FL systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00881v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeling Zhang, Dongqi Cai, Yiran Zhang, Mengwei Xu, Shangguang Wang, Ao Zhou</dc:creator>
    </item>
    <item>
      <title>IoT Device Labeling Using Large Language Models</title>
      <link>https://arxiv.org/abs/2403.01586</link>
      <description>arXiv:2403.01586v1 Announce Type: cross 
Abstract: The IoT market is diverse and characterized by a multitude of vendors that support different device functions (e.g., speaker, camera, vacuum cleaner, etc.). Within this market, IoT security and observability systems use real-time identification techniques to manage these devices effectively. Most existing IoT identification solutions employ machine learning techniques that assume the IoT device, labeled by both its vendor and function, was observed during their training phase. We tackle a key challenge in IoT labeling: how can an AI solution label an IoT device that has never been seen before and whose label is unknown?
  Our solution extracts textual features such as domain names and hostnames from network traffic, and then enriches these features using Google search data alongside catalog of vendors and device functions. The solution also integrates an auto-update mechanism that uses Large Language Models (LLMs) to update these catalogs with emerging device types. Based on the information gathered, the device's vendor is identified through string matching with the enriched features. The function is then deduced by LLMs and zero-shot classification from a predefined catalog of IoT functions.
  In an evaluation of our solution on 97 unique IoT devices, our function labeling approach achieved HIT1 and HIT2 scores of 0.7 and 0.77, respectively. As far as we know, this is the first research to tackle AI-automated IoT labeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01586v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Bar Meyuhas, Anat Bremler-Barr, Tal Shapira</dc:creator>
    </item>
    <item>
      <title>MTS: Bringing Multi-Tenancy to Virtual Networking</title>
      <link>https://arxiv.org/abs/2403.01862</link>
      <description>arXiv:2403.01862v1 Announce Type: cross 
Abstract: Multi-tenant cloud computing provides great benefits in terms of resource sharing, elastic pricing, and scalability, however, it also changes the security landscape and intro- duces the need for strong isolation between the tenants, also inside the network. This paper is motivated by the observation that while multi-tenancy is widely used in cloud computing, the virtual switch designs currently used for network virtualization lack sufficient support for tenant isolation. Hence, we present, implement, and evaluate a virtual switch architecture, MTS, which brings secure design best-practice to the context of multi-tenant virtual networking: compartmentalization of virtual switches, least-privilege execution, complete mediation of all network communication, and reducing the trusted computing base shared between tenants. We build MTS from commodity components, providing an incrementally deployable and inexpensive upgrade path to cloud operators. Our extensive experiments, extending to both micro-benchmarks and cloud applications, show that, depending on the way it is deployed, MTS may produce 1.5- 2x the throughput compared to state-of-the-art, with similar or better latency and modest resource overhead (1 extra CPU). MTS is available as open source software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01862v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>USENIX ATC 2019</arxiv:journal_reference>
      <dc:creator>Kashyap Thimmaraju, Saad Hermak, G\'abor R\'etv\'ari, Stefan Schmid</dc:creator>
    </item>
    <item>
      <title>I DPID It My Way! A Covert Timing Channel in Software-Defined Networks</title>
      <link>https://arxiv.org/abs/2403.01878</link>
      <description>arXiv:2403.01878v1 Announce Type: cross 
Abstract: Software-defined networking is considered a promising new paradigm, enabling more reliable and formally verifiable communication networks. However, this paper shows that the separation of the control plane from the data plane, which lies at the heart of Software-Defined Networks (SDNs), can be exploited for covert channels based on SDN Teleportation, even when the data planes are physically disconnected.
  This paper describes the theoretical model and design of our covert timing channel based on SDN Teleportation. We implement our covert channel using a popular SDN switch, Open vSwitch, and a popular SDN controller, ONOS. Our evaluation of the prototype shows that even under load at the controller, throughput rates of 20 bits per second are possible, with a communication accuracy of approximately 90\%. We also discuss techniques to increase the throughput further.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01878v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>IFIP Networking 2018</arxiv:journal_reference>
      <dc:creator>Robert Kr\"osche, Kashyap Thimmaraju, Liron Schiff, Stefan Schmid</dc:creator>
    </item>
    <item>
      <title>HyperFedNet: Communication-Efficient Personalized Federated Learning Via Hypernetwork</title>
      <link>https://arxiv.org/abs/2402.18445</link>
      <description>arXiv:2402.18445v2 Announce Type: replace 
Abstract: In response to the challenges posed by non-independent and identically distributed (non-IID) data and the escalating threat of privacy attacks in Federated Learning (FL), we introduce HyperFedNet (HFN), a novel architecture that incorporates hypernetworks to revolutionize parameter aggregation and transmission in FL. Traditional FL approaches, characterized by the transmission of extensive parameters, not only incur significant communication overhead but also present vulnerabilities to privacy breaches through gradient analysis. HFN addresses these issues by transmitting a concise set of hypernetwork parameters, thereby reducing communication costs and enhancing privacy protection. Upon deployment, the HFN algorithm enables the dynamic generation of parameters for the basic layer of the FL main network, utilizing local database features quantified by embedding vectors as input. Through extensive experimentation, HFN demonstrates superior performance in reducing communication overhead and improving model accuracy compared to conventional FL methods. By integrating the HFN algorithm into the FL framework, HFN offers a solution to the challenges of non-IID data and privacy threats.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18445v2</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingyun Chen, Yan Huang, Zhenzhen Xie, Junjie Pang</dc:creator>
    </item>
    <item>
      <title>Goal-oriented Estimation of Multiple Markov Sources in Resource-constrained Systems</title>
      <link>https://arxiv.org/abs/2311.07346</link>
      <description>arXiv:2311.07346v2 Announce Type: replace-cross 
Abstract: This paper investigates goal-oriented communication for remote estimation of multiple Markov sources in resource-constrained networks. An agent decides the updating times of the sources and transmits the packet to a remote destination over an unreliable channel with delay. The destination is tasked with source reconstruction for actuation. We utilize the metric \textit{cost of actuation error} (CAE) to capture the state-dependent actuation costs. We aim for a sampling policy that minimizes the long-term average CAE subject to an average resource constraint. We formulate this problem as an average-cost constrained Markov Decision Process (CMDP) and relax it into an unconstrained problem by utilizing \textit{Lyapunov drift} techniques. Then, we propose a low-complexity \textit{drift-plus-penalty} (DPP) policy for systems with known source/channel statistics and a Lyapunov optimization-based deep reinforcement learning (LO-DRL) policy for unknown environments. Our policies significantly reduce the number of uninformative transmissions by exploiting the timing of the important information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.07346v2</guid>
      <category>eess.SY</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiping Luo, Nikolaos Pappas</dc:creator>
    </item>
    <item>
      <title>T-PRIME: Transformer-based Protocol Identification for Machine-learning at the Edge</title>
      <link>https://arxiv.org/abs/2401.04837</link>
      <description>arXiv:2401.04837v2 Announce Type: replace-cross 
Abstract: Spectrum sharing allows different protocols of the same standard (e.g., 802.11 family) or different standards (e.g., LTE and DVB) to coexist in overlapping frequency bands. As this paradigm continues to spread, wireless systems must also evolve to identify active transmitters and unauthorized waveforms in real time under intentional distortion of preambles, extremely low signal-to-noise ratios and challenging channel conditions. We overcome limitations of correlation-based preamble matching methods in such conditions through the design of T-PRIME: a Transformer-based machine learning approach. T-PRIME learns the structural design of transmitted frames through its attention mechanism, looking at sequence patterns that go beyond the preamble alone. The paper makes three contributions: First, it compares Transformer models and demonstrates their superiority over traditional methods and state-of-the-art neural networks. Second, it rigorously analyzes T-PRIME's real-time feasibility on DeepWave's AIR-T platform. Third, it utilizes an extensive 66 GB dataset of over-the-air (OTA) WiFi transmissions for training, which is released along with the code for community use. Results reveal nearly perfect (i.e. $&gt;98\%$) classification accuracy under simulated scenarios, showing $100\%$ detection improvement over legacy methods in low SNR ranges, $97\%$ classification accuracy for OTA single-protocol transmissions and up to $75\%$ double-protocol classification accuracy in interference scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.04837v2</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mauro Belgiovine, Joshua Groen, Miquel Sirera, Chinenye Tassie, Ayberk Yark{\i}n Y{\i}ld{\i}z, Sage Trudeau, Stratis Ioannidis, Kaushik Chowdhury</dc:creator>
    </item>
    <item>
      <title>Knowledge Distillation from Language-Oriented to Emergent Communication for Multi-Agent Remote Control</title>
      <link>https://arxiv.org/abs/2401.12624</link>
      <description>arXiv:2401.12624v2 Announce Type: replace-cross 
Abstract: In this work, we compare emergent communication (EC) built upon multi-agent deep reinforcement learning (MADRL) and language-oriented semantic communication (LSC) empowered by a pre-trained large language model (LLM) using human language. In a multi-agent remote navigation task, with multimodal input data comprising location and channel maps, it is shown that EC incurs high training cost and struggles when using multimodal data, whereas LSC yields high inference computing cost due to the LLM's large size. To address their respective bottlenecks, we propose a novel framework of language-guided EC (LEC) by guiding the EC training using LSC via knowledge distillation (KD). Simulations corroborate that LEC achieves faster travel time while avoiding areas with poor channel conditions, as well as speeding up the MADRL training convergence by up to 61.8% compared to EC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12624v2</guid>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongjun Kim, Sejin Seo, Jihong Park, Mehdi Bennis, Seong-Lyun Kim, Junil Choi</dc:creator>
    </item>
    <item>
      <title>Toward Autonomous Cooperation in Heterogeneous Nanosatellite Constellations Using Dynamic Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2403.00692</link>
      <description>arXiv:2403.00692v2 Announce Type: replace-cross 
Abstract: The upcoming landscape of Earth Observation missions will defined by networked heterogeneous nanosatellite constellations required to meet strict mission requirements, such as revisit times and spatial resolution. However, scheduling satellite communications in these satellite networks through efficiently creating a global satellite Contact Plan (CP) is a complex task, with current solutions requiring ground-based coordination or being limited by onboard computational resources. The paper proposes a novel approach to overcome these challenges by modeling the constellations and CP as dynamic networks and employing graph-based techniques. The proposed method utilizes a state-of-the-art dynamic graph neural network to evaluate the performance of a given CP and update it using a heuristic algorithm based on simulated annealing. The trained neural network can predict the network delay with a mean absolute error of 3.6 minutes. Simulation results show that the proposed method can successfully design a contact plan for large satellite networks, improving the delay by 29.1%, similar to a traditional approach, while performing the objective evaluations 20x faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00692v2</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guillem Casadesus-Vila, Joan-Adria Ruiz-de-Azua, Eduard Alarcon</dc:creator>
    </item>
  </channel>
</rss>
