<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 08 Jan 2025 05:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Extending Internet Access Over LoRa for Internet of Things and Critical Applications</title>
      <link>https://arxiv.org/abs/2501.03465</link>
      <description>arXiv:2501.03465v1 Announce Type: new 
Abstract: LoRa bridges the gap between remote locations and mainstream networks, enabling large-scale Internet of Things (IoT) deployments. Despite the recent advancements around LoRa, Internet access over this technology is still largely unexplored. Most existing solutions only handle packets within the local LoRa network and do not interact with web applications. This limits the scalability and the ability to deliver essential web services in disconnected regions. This work proposes and implements ILoRa to extend the public Internet to disconnected areas for essential service delivery. ILoRa enables accessing Application Programming Interfaces (APIs) and web pages on the Internet over a LoRa backbone network. It comprises a ILoRa coordinator code (ICN) and access point nodes (APNs). The ICN interfaces the LoRa network with the public Internet and interprets content. The APN tethers a WiFi hotspot to which devices connect and access the web content. This work further proposes data handling methods for ICNs and APNs. An actual hardware-based implementation validates the proposed system. The implementation achieves a throughput of 1.06 kbps tested for an Internet-based API returning JSON data of 930 B. Furthermore, the APN consumed approximately $0.162$A current, and the resource utilization on the ICN was minimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03465v1</guid>
      <category>cs.NI</category>
      <category>cs.CY</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Atonu Ghosh, Devadeep Misra, Hirdesh Mewada</dc:creator>
    </item>
    <item>
      <title>Dynamic Authentication and Granularized Authorization with a Cross-Domain Zero Trust Architecture for Federated Learning in Large-Scale IoT Networks</title>
      <link>https://arxiv.org/abs/2501.03601</link>
      <description>arXiv:2501.03601v1 Announce Type: new 
Abstract: With the increasing number of connected devices and complex networks involved, current domain-specific security techniques become inadequate for diverse large-scale Internet of Things (IoT) systems applications. While cross-domain authentication and authorization brings lots of security improvement, it creates new challenges of efficiency and security. Zero trust architecture (ZTA), an emerging network security architecture, offers a more granular and robust security environment for IoT systems. However, extensive cross-domain data exchange in ZTA can cause reduced authentication and authorization efficiency and data privacy concerns. Therefore, in this paper, we propose a dynamic authentication and granularized authorization scheme based on ZTA integrated with decentralized federated learning (DFL) for cross-domain IoT networks. Specifically, device requests in the cross-domain process are continuously monitored and evaluated, and only necessary access permissions are granted. To protect user data privacy and reduce latency, we integrate DFL with ZTA to securely and efficiently share device data across different domains. Particularly, the DFL model is compressed to reduce the network transmission load. Meanwhile, a dynamic adaptive weight adjustment mechanism is proposed to enable the DFL model to adapt to data characteristics from different domains. We analyze the performance of the proposed scheme in terms of security proof, including confidentiality, integrity and availability. Simulation results demonstrate the superior performance of the proposed scheme in terms of lower latency and higher throughput compared to other existing representative schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03601v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyu Ma, Fang Fang, Xianbin Wang</dc:creator>
    </item>
    <item>
      <title>IEEE 802.11bn Multi-AP Coordinated Spatial Reuse with Hierarchical Multi-Armed Bandits</title>
      <link>https://arxiv.org/abs/2501.03680</link>
      <description>arXiv:2501.03680v1 Announce Type: new 
Abstract: Coordination among multiple access points (APs) is integral to IEEE 802.11bn (Wi-Fi 8) for managing contention in dense networks. This letter explores the benefits of Coordinated Spatial Reuse (C-SR) and proposes the use of reinforcement learning to optimize C-SR group selection. We develop a hierarchical multi-armed bandit (MAB) framework that efficiently selects APs for simultaneous transmissions across various network topologies, demonstrating reinforcement learning's promise in Wi-Fi settings. Among several MAB algorithms studied, we identify the upper confidence bound (UCB) as particularly effective, offering rapid convergence, adaptability to changes, and sustained performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03680v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/LCOMM.2024.3521079</arxiv:DOI>
      <arxiv:journal_reference>IEEE Communications Letters (2024)</arxiv:journal_reference>
      <dc:creator>Maksymilian Wojnar, Wojciech Ciezobka, Katarzyna Kosek-Szott, Krzysztof Rusek, Szymon Szott, David Nunez, Boris Bellalta</dc:creator>
    </item>
    <item>
      <title>Bayesian EM Digital Twins Channel Estimation</title>
      <link>https://arxiv.org/abs/2501.03731</link>
      <description>arXiv:2501.03731v1 Announce Type: new 
Abstract: This letter proposes a Bayesian channel estimation method that leverages on the a priori information provided by the Electromagnetic Digital Twin's (EM-DT) representation of the environment. The proposed approach is compared with several conventional techniques in terms of Normalized Mean Square Error (NMSE), spectral efficiency, and number of pilots. Simulations prove more than $10\,$dB gain in NMSE and a spectral efficiency comparable to that of the ideal channel state information, for different signal-to-noise ratio (SNR) values. Additionally, the Bayesian EM-DT-empowered channel estimation enables a remarkable pilot reduction compared to maximum likelihood methods at low SNR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03731v1</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Del Moro, Francesco Linsalata, Marouan Mizmizi, Maurizio Magarini, Damiano Badini, Umberto Spagnolini</dc:creator>
    </item>
    <item>
      <title>mFabric: An Efficient and Scalable Fabric for Mixture-of-Experts Training</title>
      <link>https://arxiv.org/abs/2501.03905</link>
      <description>arXiv:2501.03905v1 Announce Type: new 
Abstract: Mixture-of-Expert (MoE) models outperform conventional models by selectively activating different subnets, named \emph{experts}, on a per-token basis. This gated computation generates dynamic communications that cannot be determined beforehand, challenging the existing GPU interconnects that remain \emph{static} during the distributed training process. In this paper, we advocate for a first-of-its-kind system, called mFabric, that unlocks topology reconfiguration \emph{during} distributed MoE training. Towards this vision, we first perform a production measurement study and show that the MoE dynamic communication pattern has \emph{strong locality}, alleviating the requirement of global reconfiguration. Based on this, we design and implement a \emph{regionally reconfigurable high-bandwidth domain} on top of existing electrical interconnects using optical circuit switching (OCS), achieving scalability while maintaining rapid adaptability. We have built a fully functional mFabric prototype with commodity hardware and a customized collective communication runtime that trains state-of-the-art MoE models with \emph{in-training} topology reconfiguration across 32 A100 GPUs. Large-scale packet-level simulations show that mFabric delivers comparable performance as the non-blocking fat-tree fabric while boosting the training cost efficiency (e.g., performance per dollar) of four representative MoE models by 1.2$\times$--1.5$\times$ and 1.9$\times$--2.3$\times$ at 100 Gbps and 400 Gbps link bandwidths, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03905v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xudong Liao, Yijun Sun, Han Tian, Xinchen Wan, Yilun Jin, Zilong Wang, Zhenghang Ren, Xinyang Huang, Wenxue Li, Kin Fai Tse, Zhizhen Zhong, Guyue Liu, Ying Zhang, Xiaofeng Ye, Yiming Zhang, Kai Chen</dc:creator>
    </item>
    <item>
      <title>SPECTRE: A Hybrid System for an Adaptative and Optimised Cyber Threats Detection, Response and Investigation in Volatile Memory</title>
      <link>https://arxiv.org/abs/2501.03898</link>
      <description>arXiv:2501.03898v1 Announce Type: cross 
Abstract: The increasing sophistication of modern cyber threats, particularly file-less malware relying on living-off-the-land techniques, poses significant challenges to traditional detection mechanisms. Memory forensics has emerged as a crucial method for uncovering such threats by analysing dynamic changes in memory. This research introduces SPECTRE (Snapshot Processing, Emulation, Comparison, and Threat Reporting Engine), a modular Cyber Incident Response System designed to enhance threat detection, investigation, and visualization. By adopting Volatility JSON format as an intermediate output, SPECTRE ensures compatibility with widely used DFIR tools, minimizing manual data transformations and enabling seamless integration into established workflows. Its emulation capabilities safely replicate realistic attack scenarios, such as credential dumping and malicious process injections, for controlled experimentation and validation. The anomaly detection module addresses critical attack vectors, including RunDLL32 abuse and malicious IP detection, while the IP forensics module enhances threat intelligence by integrating tools like Virus Total and geolocation APIs. SPECTRE advanced visualization techniques transform raw memory data into actionable insights, aiding Red, Blue and Purple teams in refining strategies and responding effectively to threats. Bridging gaps between memory and network forensics, SPECTRE offers a scalable, robust platform for advancing threat detection, team training, and forensic research in combating sophisticated cyber threats.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03898v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <category>cs.OS</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arslan Tariq Syed, Mohamed Chahine Ghanem, Elhadj Benkhelifa, Fauzia Idrees Abro</dc:creator>
    </item>
    <item>
      <title>Accelerating Handover in Mobile Satellite Network</title>
      <link>https://arxiv.org/abs/2403.11502</link>
      <description>arXiv:2403.11502v2 Announce Type: replace 
Abstract: The construction of Low Earth Orbit (LEO) satellite constellations has recently spurred tremendous attention from academia and industry. 5G and 6G standards have specified LEO satellite network as a key component of 5G and 6G networks. However, ground terminals experience frequent, high-latency handover incurred by satellites' fast travelling speed, which deteriorates the performance of latency-sensitive applications. To address this challenge, we propose a novel handover flowchart for mobile satellite networks, which can considerably reduce the handover latency. The innovation behind this scheme is to mitigate the interaction between the access and core networks that occupy the majority of time overhead by leveraging the predictable travelling trajectory and spatial distribution inherent in mobile satellite networks. Specifically, we design a fine-grained synchronized algorithm to address the synchronization problem due to the lack of control signalling delivery between the access and core networks. Moreover, we minimize the computational complexity of the core network using information such as the satellite access strategy and unique spatial distribution, which is caused by frequent prediction operations. We have built a prototype for a mobile satellite network using modified Open5GS and UERANSIM, which is driven by actual LEO satellite constellations such as Starlink and Kuiper. We have conducted extensive experiments, and the results demonstrate that our proposed handover scheme can considerably reduce the handover latency compared to the 3GPP Non-terrestrial Networks (NTN) and two other existing handover schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11502v2</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiasheng Wu, Shaojie Su, Xiong Wang, Jingjing Zhang, Yue Gao</dc:creator>
    </item>
    <item>
      <title>Deep Diffusion Deterministic Policy Gradient based Performance Optimization for Wi-Fi Networks</title>
      <link>https://arxiv.org/abs/2404.15684</link>
      <description>arXiv:2404.15684v3 Announce Type: replace 
Abstract: Generative Diffusion Models (GDMs), have made significant strides in modeling complex data distributions across diverse domains. Meanwhile, Deep Reinforcement Learning (DRL) has demonstrated substantial improvements in optimizing Wi-Fi network performance. Wi-Fi optimization problems are highly challenging to model mathematically, and DRL methods can bypass complex mathematical modeling, while GDMs excel in handling complex data modeling. Therefore, combining DRL with GDMs can mutually enhance their capabilities. The current MAC layer access mechanism in Wi-Fi networks is the Distributed Coordination Function (DCF), which dramatically decreases in performance with a high number of terminals. In this study, we propose the Deep Diffusion Deterministic Policy Gradient (D3PG) algorithm, which integrates diffusion models with the Deep Deterministic Policy Gradient (DDPG) framework to optimize Wi-Fi network performance. To the best of our knowledge, this is the first work to apply such an integration in Wi-Fi performance optimization. We propose an access mechanism that jointly adjusts the contention window and the aggregation frame length based on the D3PG algorithm. Through simulations, we have demonstrated that this mechanism significantly outperforms existing Wi-Fi standards in dense Wi-Fi scenarios, maintaining performance even as the number of users increases sharply.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15684v3</guid>
      <category>cs.NI</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tie Liu, Xuming Fang, Rong He</dc:creator>
    </item>
    <item>
      <title>Driving Innovation in 6G Wireless Technologies: The OpenAirInterface Approach</title>
      <link>https://arxiv.org/abs/2412.13295</link>
      <description>arXiv:2412.13295v2 Announce Type: replace 
Abstract: The development of 6G wireless technologies is rapidly advancing, with the 3rd Generation Partnership Project (3GPP) entering the pre-standardization phase and aiming to deliver the first specifications by 2028. This paper explores the OpenAirInterface (OAI) project, an open-source initiative that plays a crucial role in the evolution of 5G and the future 6G networks. OAI provides a comprehensive implementation of 3GPP and O-RAN compliant networks, including Radio Access Network (RAN), Core Network (CN), and software-defined User Equipment (UE) components. The paper details the history and evolution of OAI, its licensing model, and the various projects under its umbrella, such as RAN, the CN, as well as the Operations, Administration and Maintenance (OAM) projects. It also highlights the development methodology, Continuous Integration/Continuous Delivery (CI/CD) processes, and end-to-end systems powered by OAI. Furthermore, the paper discusses the potential of OAI for 6G research, focusing on spectrum, reflective intelligent surfaces, and Artificial Intelligence (AI)/Machine Learning (ML) integration. The open-source approach of OAI is emphasized as essential for tackling the challenges of 6G, fostering community collaboration, and driving innovation in next-generation wireless technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13295v2</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Kaltenberger, Tommaso Melodia, Irfan Ghauri, Michele Polese, Raymond Knopp, Tien Thinh Nguyen, Sakthivel Velumani, Davide Villa, Leonardo Bonati, Robert Schmidt, Sagar Arora, Mikel Irazabal, Navid Nikaein</dc:creator>
    </item>
    <item>
      <title>Toward Digital Network Twins: Integrating Sionna RT in ns-3 for 6G Multi-RAT Networks Simulations</title>
      <link>https://arxiv.org/abs/2501.00372</link>
      <description>arXiv:2501.00372v2 Announce Type: replace 
Abstract: The increasing complexity of 6G systems demands innovative tools for network management, simulation, and optimization. This work introduces the integration of ns-3 with Sionna RT, establishing the foundation for the first open source full-stack Digital Network Twin (DNT) capable of supporting multi-RAT. By incorporating a deterministic ray tracer for precise and site-specific channel modeling, this framework addresses limitations of traditional stochastic models and enables realistic, dynamic, and multilayered wireless network simulations. Tested in a challenging vehicular urban scenario, the proposed solution demonstrates significant improvements in accurately modeling wireless channels and their cascading effects on higher network layers. With up to 65% observed differences in application-layer performance compared to stochastic models, this work highlights the transformative potential of ray-traced simulations for 6G research, training, and network management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00372v2</guid>
      <category>cs.NI</category>
      <category>cs.SE</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Pegurri, Francesco Linsalata, Eugenio Moro, Jakob Hoydis, Umberto Spagnolini</dc:creator>
    </item>
    <item>
      <title>Edge Graph Intelligence: Reciprocally Empowering Edge Networks with Graph Intelligence</title>
      <link>https://arxiv.org/abs/2407.15320</link>
      <description>arXiv:2407.15320v2 Announce Type: replace-cross 
Abstract: Recent years have witnessed a thriving growth of computing facilities connected at the network edge, cultivating edge networks as a fundamental infrastructure for supporting miscellaneous intelligent services.Meanwhile, Artificial Intelligence (AI) frontiers have extrapolated to the graph domain and promoted Graph Intelligence (GI). Given the inherent relation between graphs and networks, the interdiscipline of graph learning and edge networks, i.e., Edge GI or EGI, has revealed a novel interplay between them -- GI aids in optimizing edge networks, while edge networks facilitate GI model deployment. Driven by this delicate closed-loop, EGI is recognized as a promising solution to fully unleash the potential of edge computing power and is garnering growing attention. Nevertheless, research on EGI remains nascent, and there is a soaring demand within both the communications and AI communities for a dedicated venue to share recent advancements. To this end, this paper promotes the concept of EGI, explores its scope and core principles, and conducts a comprehensive survey concerning recent research efforts on this emerging field. Specifically, this paper introduces and discusses: 1) fundamentals of edge computing and graph learning,2) emerging techniques centering on the closed loop between graph intelligence and edge networks, and 3) open challenges and research opportunities of future EGI. By bridging the gap across communication, networking, and graph learning areas, we believe that this survey can garner increased attention, foster meaningful discussions, and inspire further research ideas in EGI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15320v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liekang Zeng (Sherman), Shengyuan Ye (Sherman), Xu Chen (Sherman), Xiaoxi Zhang (Sherman), Ju Ren (Sherman), Jian Tang (Sherman), Yang Yang (Sherman),  Xuemin (Sherman),  Shen</dc:creator>
    </item>
    <item>
      <title>CONTINUUM: Detecting APT Attacks through Spatial-Temporal Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2501.02981</link>
      <description>arXiv:2501.02981v2 Announce Type: replace-cross 
Abstract: Advanced Persistent Threats (APTs) represent a significant challenge in cybersecurity due to their sophisticated and stealthy nature. Traditional Intrusion Detection Systems (IDS) often fall short in detecting these multi-stage attacks. Recently, Graph Neural Networks (GNNs) have been employed to enhance IDS capabilities by analyzing the complex relationships within networked data. However, existing GNN-based solutions are hampered by high false positive rates and substantial resource consumption. In this paper, we present a novel IDS designed to detect APTs using a Spatio-Temporal Graph Neural Network Autoencoder. Our approach leverages spatial information to understand the interactions between entities within a graph and temporal information to capture the evolution of the graph over time. This dual perspective is crucial for identifying the sequential stages of APTs. Furthermore, to address privacy and scalability concerns, we deploy our architecture in a federated learning environment. This setup ensures that local data remains on-premise while encrypted model-weights are shared and aggregated using homomorphic encryption, maintaining data privacy and security. Our evaluation shows that this system effectively detects APTs with lower false positive rates and optimized resource usage compared to existing methods, highlighting the potential of spatio-temporal analysis and federated learning in enhancing cybersecurity defenses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02981v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Wed, 08 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Atmane Ayoub Mansour Bahar, Kamel Soaid Ferrahi, Mohamed-Lamine Messai, Hamida Seba, Karima Amrouche</dc:creator>
    </item>
  </channel>
</rss>
