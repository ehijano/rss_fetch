<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 20 Aug 2025 04:00:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Unified Modelling of Infrastructure Asset Performance Deterioration -- a bounded gamma process approach</title>
      <link>https://arxiv.org/abs/2508.13359</link>
      <description>arXiv:2508.13359v1 Announce Type: new 
Abstract: Infrastructure asset management systems require a flexible deterioration model that can handle various degradation patterns in a unified way. Owing to its appealing monotonic sample paths, independent increments and mathematical tractability, gamma process has been widely employed as an infrastructure performance deterioration model. This model was recently enhanced by introducing an upper bound to satisfy a practical modelling need that many infrastructure performance deterioration processes are constrained by physical or managerial limits. Several bounded transformed gamma process (BTGP) alternatives had been proposed; however, they lacked due flexibility to characterize different deterioration patterns. This paper proposed a new BTGP model that is deeply grounded upon the traditional regression modelling tradition in infrastructure asset management systems. Qualitative and quantitative comparisons were carried out between the proposed BTGP and a bounded nonstationary gamma process (BNGP) model from both deterioration modelling and asset management decision-making perspectives. An empirical study using the real-world historical bridge condition data was performed to examine the flexibility of the BTGP against the BNGP and six other BTGP alternatives. The results confirmed the flexibility and significance of the proposed BTGP model for infrastructure systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13359v1</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <category>math.PR</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wang Chen, Arnold X. -X. Yuan</dc:creator>
    </item>
    <item>
      <title>Monotonic Path-Specific Effects: Application to Estimating Educational Returns</title>
      <link>https://arxiv.org/abs/2508.13366</link>
      <description>arXiv:2508.13366v1 Announce Type: new 
Abstract: Conventional research on educational effects typically either employs a "years of schooling" measure of education, or dichotomizes attainment as a point-in-time treatment. Yet, such a conceptualization of education is misaligned with the sequential process by which individuals make educational transitions. In this paper, I propose a causal mediation framework for the study of educational effects on outcomes such as earnings. The framework considers the effect of a given educational transition as operating indirectly, via progression through subsequent transitions, as well as directly, net of these transitions. I demonstrate that the average treatment effect (ATE) of education can be additively decomposed into mutually exclusive components that capture these direct and indirect effects. The decomposition has several special properties which distinguish it from conventional mediation decompositions of the ATE, properties which facilitate less restrictive identification assumptions as well as identification of all causal paths in the decomposition. An analysis of the returns to high school completion in the NLSY97 cohort suggests that the payoff to a high school degree stems overwhelmingly from its direct labor market returns. Mediation via college attendance, completion and graduate school attendance is small because of individuals' low counterfactual progression rates through these subsequent transitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13366v1</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aleksei Opacic</dc:creator>
    </item>
    <item>
      <title>Time-varying confounding in epidemic intervention evaluations</title>
      <link>https://arxiv.org/abs/2508.13427</link>
      <description>arXiv:2508.13427v1 Announce Type: new 
Abstract: Estimating the causal effect of a time-varying public health intervention on the course of an infectious disease epidemic is an important methodological challenge. During the COVID-19 pandemic, researchers attempted to estimate the effects of social distancing policies, stay-at-home orders, school closures, mask mandates, vaccination programs, and many other interventions on population-level infection outcomes. However, measuring the effect of these interventions is complicated by time-varying confounding: public health interventions are causal consequences of prior outcomes and interventions, as well as causes of future outcomes and interventions. Researchers have shown repeatedly that neglecting time-varying confounding for individual-level longitudinal interventions can result in profoundly biased estimates of causal effects. However, the issue with time-varying confounding bias has often been overlooked in population-level epidemic intervention evaluations. In this paper, we explain why associational modeling to estimate the effects of interventions on epidemic outcomes based on observations can be prone to time-varying confounding bias. Using causal reasoning and model-based simulation, we show how directional bias due to time-varying confounding arises in associational modeling and the misleading conclusions it induces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13427v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yichi Zhang, Forrest W. Crawford</dc:creator>
    </item>
    <item>
      <title>Simulation of extreme functionals in meteoceanic data: Application to surge evolution over tidal cycles</title>
      <link>https://arxiv.org/abs/2508.13687</link>
      <description>arXiv:2508.13687v1 Announce Type: new 
Abstract: We investigate the influence of time-varying meteoceanic conditions on coastal flooding under the prism of rare events. Focusing on conditions observed over half tidal cycles, we observe that such data fall within the framework of functional extreme value theory, but violate standard assumptions due to temporal dependence and short-tailed behavior.a To address this, we propose a two-stage methodology. First, we introduce an autoregressive model to eliminate temporal dependence between cycles. Second, considering the model residuals, we adapt existing techniques based on Pareto processes. This allows us to build a simulator of extreme scenarios, by applying inverse transformations. These simulations depend on an initial time series, which can be randomly selected to tune the desired level of extremes. We validate the simulator performance by comparing simulated times series with observations, through several criteria, based on principal component analysis, extreme value analysis, and classification algorithms. The approach is applied to the surge data, on the G{\^a}vres site, located in southern Brittany, France.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13687v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Gorse (INSA Toulouse, IMT), Olivier Roustant (INSA Toulouse, IMT), J\'er\'emy Rohmer (BRGM), D\'eborah Idier (BRGM)</dc:creator>
    </item>
    <item>
      <title>Add-On Regimes and Their Relevance for Quantifying the Effects of Opioid-Sparing Treatments</title>
      <link>https://arxiv.org/abs/2508.13848</link>
      <description>arXiv:2508.13848v1 Announce Type: new 
Abstract: Medical researchers and practitioners want to know if supplementing opioid treatments with other analgesics, such as nonsteroidal anti-inflammatory drugs (NSAIDs), can reduce opioid consumption. However, quantifying opioid-sparing effects is challenging; even coming up with a policy-relevant estimand requires care. We propose defining these effects in terms of add-on regimes. An add-on regime assigns NSAIDs over time based on the opioid and NSAID treatments a patient would naturally take without any intervention. The regime uses the physician's decision to administer opioids as a clinically meaningful, and practically feasible, indication for NSAID administration. In contrast, static regimes assign NSAIDs at predefined time points, regardless of clinical context. When opioids are not administered, the add-on regime requires no intervention, thereby preserving the natural level of NSAIDs. This differs from conventional dynamic regimes, which define treatment decisions at every time point during the treatment period. We identify the effect of add-on regimes under assumptions that are easier to assess than those used in existing methods. Finally, we apply the methods to estimate opioid-sparing effects of NSAIDs in a cohort of Norwegian trauma patients using national registry data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13848v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Catharina Stoltenberg, Matias Janvin, Mats Julius Stensrud, Leiv Arne Rosseland, Jon Michael Gran</dc:creator>
    </item>
    <item>
      <title>The Course Difficulty Analysis Cookbook</title>
      <link>https://arxiv.org/abs/2508.13218</link>
      <description>arXiv:2508.13218v1 Announce Type: cross 
Abstract: Curriculum analytics (CA) studies curriculum structure and student data to ensure the quality of educational programs. An essential aspect is studying course properties, which involves assigning each course a representative difficulty value. This is critical for several aspects of CA, such as quality control (e.g., monitoring variations over time), course comparisons (e.g., articulation), and course recommendation (e.g., advising). Measuring course difficulty requires careful consideration of multiple factors: First, when difficulty measures are sensitive to the performance level of enrolled students, it can bias interpretations by overlooking student diversity. By assessing difficulty independently of enrolled students' performances, we can reduce the risk of bias and enable fair, representative assessments of difficulty. Second, from a measurement theoretic perspective, the measurement must be reliable and valid to provide a robust basis for subsequent analyses. Third, difficulty measures should account for covariates, such as the characteristics of individual students within a diverse populations (e.g., transfer status). In recent years, various notions of difficulty have been proposed. This paper provides the first comprehensive review and comparison of existing approaches for assessing course difficulty based on grade point averages and latent trait modeling. It further offers a hands-on tutorial on model selection, assumption checking, and practical CA applications. These applications include monitoring course difficulty over time and detecting courses with disparate outcomes between distinct groups of students (e.g., dropouts vs. graduates), ultimately aiming to promote high-quality, fair, and equitable learning experiences. To support further research and application, we provide an open-source software package and artificial datasets, facilitating reproducibility and adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13218v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frederik Baucks, Robin Schmucker, Laurenz Wiskott</dc:creator>
    </item>
    <item>
      <title>Time Profile of U.S. Neighborhoods: Datasets of Time Use at Social Infrastructure Places</title>
      <link>https://arxiv.org/abs/2508.13295</link>
      <description>arXiv:2508.13295v1 Announce Type: cross 
Abstract: Social infrastructure plays a critical role in shaping neighborhood well-being by fostering social and cultural interaction, enabling service provision, and encouraging exposure to diverse environments. Despite the growing knowledge of its spatial accessibility, time use at social infrastructure places is underexplored due to the lack of a spatially resolved national dataset. We address this gap by developing scalable Social-Infrastructure Time Use measures (STU) that capture length and depth of engagement, activity diversity, and spatial inequality, supported by first-of-their-kind datasets spanning multiple geographic scales from census tracts to metropolitan areas. Our datasets leverage anonymized and aggregated foot traffic data collected between 2019 and 2024 across 49 continental U.S. states. The data description reveals variances in STU across time, space, and differing neighborhood sociodemographic characteristics. Validation demonstrates generally robust population representation, consistent with established national survey findings while revealing more nuanced patterns. Future analyses could link STU with public health outcomes and environmental factors to inform targeted interventions aimed at enhancing population well-being and guiding social infrastructure planning and usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13295v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s41597-025-05504-9</arxiv:DOI>
      <arxiv:journal_reference>Sci. Data (2025)</arxiv:journal_reference>
      <dc:creator>Yan Wang, Ziyi Guo</dc:creator>
    </item>
    <item>
      <title>What makes a study design quasi-experimental? The case of difference-in-differences</title>
      <link>https://arxiv.org/abs/2508.13945</link>
      <description>arXiv:2508.13945v1 Announce Type: cross 
Abstract: Study designs classified as quasi- or natural experiments are typically accorded more face validity than observational study designs more broadly. However, there is ambiguity in the literature about what qualifies as a quasi-experiment. Here, we attempt to resolve this ambiguity by distinguishing two different ways of defining this term. One definition is based on identifying assumptions being uncontroversial, and the other is based on the ability to account for unobserved sources of confounding (under assumptions). We argue that only the former deserves an additional measure of credibility for reasons of design. We use the difference-in-differences approach to illustrate our discussion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13945v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Audrey Renson, Daniel Westreich</dc:creator>
    </item>
    <item>
      <title>On the reconstruction limits of complex networks</title>
      <link>https://arxiv.org/abs/2501.01437</link>
      <description>arXiv:2501.01437v3 Announce Type: replace 
Abstract: Network reconstruction consists in retrieving the hidden interaction structure of a system from observations. Many reconstruction algorithms have been proposed, although less research has been devoted to describe their theoretical limitations. In this work, we take a first-principles approach and build on our earlier definition of reconstructability-the fraction of structural information recoverable from data. We relate this quantity to the true data-generating (TDG) process and delineate an information-theoretic reconstruction limit, i.e., the upper bound of the mutual information between the true underlying graph and any graph reconstructed from observations. These concepts lead us to a principled numerical method to assess the validity of empirically reconstructed networks, based on model selection and a quantity we introduce: the reconstruction index. This index approximates the reconstructability from data, quantifies the variability of the reconstructed network ensemble, and is shown to predict reconstruction error without requiring knowledge of the true underlying network. We characterize this method and test it on empirical time series and networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01437v3</guid>
      <category>stat.AP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charles Murphy, Simon Lizotte, Fran\c{c}ois Thibault, Vincent Thibeault, Patrick Desrosiers, Antoine Allard</dc:creator>
    </item>
    <item>
      <title>Stochastic highway capacity: Unsuitable Kaplan-Meier estimator, revised maximum likelihood estimator, and impact of speed harmonisation</title>
      <link>https://arxiv.org/abs/2507.00893</link>
      <description>arXiv:2507.00893v2 Announce Type: replace 
Abstract: The Kaplan-Meier estimate, also known as the product-limit method (PLM), is a widely used non-parametric maximum likelihood estimator (MLE) in survival analysis. In the context of highway engineering, it has been repeatedly applied to estimate stochastic traffic flow capacity. However, this paper demonstrates that PLM is fundamentally unsuitable for this purpose. The method implicitly assumes continuous exposure to failure risk over time - a premise invalid for traffic flow, where intensity does not increase linearly, and capacity is not even directly observable. Although parametric MLE approach offers a viable alternative, its earlier derivation for this use case suffers from flawed likelihood formulation, likely due to attempt to preserve consistency with PLM. This study derives a corrected likelihood formula for stochastic capacity MLE and validates it using two empirical datasets. The proposed method is then applied in a case study examining the effect of a variable speed limit (VSL) system used for traffic flow speed harmonisation at a 2-to-1 lane drop. Results show that the VSL improved capacity by approximately 10 % or reduced breakdown probability at the same flow intensity by up to 50 %. The findings underscore the methodological importance of correct model formulation and highlight the practical relevance of stochastic capacity estimation for evaluating traffic control strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00893v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Igor Mikol\'a\v{s}ek</dc:creator>
    </item>
    <item>
      <title>Climate Change in Austria: Precipitation and Dry Spells over the last 60 years</title>
      <link>https://arxiv.org/abs/2408.11497</link>
      <description>arXiv:2408.11497v3 Announce Type: replace-cross 
Abstract: This study unveils localised changes in Austria's precipitation patterns, often missed by broader assessments, by comparing the 1961-1990 and 1991-2020 climate normal periods on a high resolution 2x2 km grid. Our extended model explicitly accounts for diverse topographical influences, including slope, aspect, and a monthly-varying elevation effect, when analysing monthly normals of mean precipitation and maximum daily sums, as well as maximum dry spell lengths. We found that while mean precipitation generally declined early in the year, it notably increased in March, September, and October (up to +50%). In contrast, the maximum duration of dry spells extended significantly in January, February, and June, particularly in the southern regions (up to +30%). Maximum daily precipitation amounts surged in late summer and autumn (up to +30%). This research offers a transferable modelling approach for understanding critical shifts, vital for climate adaptation both within Austria and globally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11497v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Corinna Perchtold</dc:creator>
    </item>
    <item>
      <title>Bayesian Shrinkage in High-Dimensional VAR Models: A Comparative Study</title>
      <link>https://arxiv.org/abs/2504.05489</link>
      <description>arXiv:2504.05489v2 Announce Type: replace-cross 
Abstract: High-dimensional vector autoregressive (VAR) models offer a versatile framework for multivariate time series analysis, yet face critical challenges from over-parameterization and uncertain lag order. In this paper, we systematically compare three Bayesian shrinkage priors (horseshoe, lasso, and normal) and two frequentist regularization approaches (ridge and nonparametric shrinkage) under three carefully crafted simulation scenarios. These scenarios encompass (i) overfitting in a low-dimensional setting, (ii) sparse high-dimensional processes, and (iii) a combined scenario where both large dimension and overfitting complicate inference.
  We evaluate each method in quality of parameter estimation (root mean squared error, coverage, and interval length) and out-of-sample forecasting (one-step-ahead forecast RMSE). Our findings show that local-global Bayesian methods, particularly the horseshoe, dominate in maintaining accurate coverage and minimizing parameter error, even when the model is heavily over-parameterized. Frequentist ridge often yields competitive point forecasts but underestimates uncertainty, leading to sub-nominal coverage. A real-data application using macroeconomic variables from Canada illustrates how these methods perform in practice, reinforcing the advantages of local-global priors in stabilizing inference when dimension or lag order is inflated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05489v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison Katz, Robert E. Weiss</dc:creator>
    </item>
  </channel>
</rss>
