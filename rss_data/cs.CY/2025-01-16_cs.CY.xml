<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Jan 2025 05:02:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Towards Best Practices for Open Datasets for LLM Training</title>
      <link>https://arxiv.org/abs/2501.08365</link>
      <description>arXiv:2501.08365v1 Announce Type: new 
Abstract: Many AI companies are training their large language models (LLMs) on data without the permission of the copyright owners. The permissibility of doing so varies by jurisdiction: in countries like the EU and Japan, this is allowed under certain restrictions, while in the United States, the legal landscape is more ambiguous. Regardless of the legal status, concerns from creative producers have led to several high-profile copyright lawsuits, and the threat of litigation is commonly cited as a reason for the recent trend towards minimizing the information shared about training datasets by both corporate and public interest actors. This trend in limiting data information causes harm by hindering transparency, accountability, and innovation in the broader ecosystem by denying researchers, auditors, and impacted individuals access to the information needed to understand AI models.
  While this could be mitigated by training language models on open access and public domain data, at the time of writing, there are no such models (trained at a meaningful scale) due to the substantial technical and sociological challenges in assembling the necessary corpus. These challenges include incomplete and unreliable metadata, the cost and complexity of digitizing physical records, and the diverse set of legal and technical skills required to ensure relevance and responsibility in a quickly changing landscape. Building towards a future where AI systems can be trained on openly licensed data that is responsibly curated and governed requires collaboration across legal, technical, and policy domains, along with investments in metadata standards, digitization, and fostering a culture of openness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08365v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefan Baack, Stella Biderman, Kasia Odrozek, Aviya Skowron, Ayah Bdeir, Jillian Bommarito, Jennifer Ding, Maximilian Gahntz, Paul Keller, Pierre-Carl Langlais, Greg Lindahl, Sebastian Majstorovic, Nik Marda, Guilherme Penedo, Maarten Van Segbroeck, Jennifer Wang, Leandro von Werra, Mitchell Baker, Julie Beli\~ao, Kasia Chmielinski, Marzieh Fadaee, Lisa Gutermuth, Hynek Kydl\'i\v{c}ek, Greg Leppert, EM Lewis-Jong, Solana Larsen, Shayne Longpre, Angela Oduor Lungati, Cullen Miller, Victor Miller, Max Ryabinin, Kathleen Siminyu, Andrew Strait, Mark Surman, Anna Tumad\'ottir, Maurice Weber, Rebecca Weiss, Lee White, Thomas Wolf</dc:creator>
    </item>
    <item>
      <title>Modeling Discrimination with Causal Abstraction</title>
      <link>https://arxiv.org/abs/2501.08429</link>
      <description>arXiv:2501.08429v1 Announce Type: new 
Abstract: A person is directly racially discriminated against only if her race caused her worse treatment. This implies that race is an attribute sufficiently separable from other attributes to isolate its causal role. But race is embedded in a nexus of social factors that resist isolated treatment. If race is socially constructed, in what sense can it cause worse treatment? Some propose that the perception of race, rather than race itself, causes worse treatment. Others suggest that since causal models require modularity, i.e. the ability to isolate causal effects, attempts to causally model discrimination are misguided.
  This paper addresses the problem differently. We introduce a framework for reasoning about discrimination, in which race is a high-level abstraction of lower-level features. In this framework, race can be modeled as itself causing worse treatment. Modularity is ensured by allowing assumptions about social construction to be precisely and explicitly stated, via an alignment between race and its constituents. Such assumptions can then be subjected to normative and empirical challenges, which lead to different views of when discrimination occurs. By distinguishing constitutive and causal relations, the abstraction framework pinpoints disagreements in the current literature on modeling discrimination, while preserving a precise causal account of discrimination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08429v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Milan Moss\'e, Kara Schechtman, Frederick Eberhardt, Thomas Icard</dc:creator>
    </item>
    <item>
      <title>A Short-Term Predict-Then-Cluster Framework for Meal Delivery Services</title>
      <link>https://arxiv.org/abs/2501.08466</link>
      <description>arXiv:2501.08466v1 Announce Type: new 
Abstract: Micro-delivery services offer promising solutions for on-demand city logistics, but their success relies on efficient real-time delivery operations and fleet management. On-demand meal delivery platforms seek to optimize real-time operations based on anticipatory insights into citywide demand distributions. To address these needs, this study proposes a short-term predict-then-cluster framework for on-demand meal delivery services. The framework utilizes ensemble-learning methods for point and distributional forecasting with multivariate features, including lagged-dependent inputs to capture demand dynamics. We introduce Constrained K-Means Clustering (CKMC) and Contiguity Constrained Hierarchical Clustering with Iterative Constraint Enforcement (CCHC-ICE) to generate dynamic clusters based on predicted demand and geographical proximity, tailored to user-defined operational constraints. Evaluations of European and Taiwanese case studies demonstrate that the proposed methods outperform traditional time series approaches in both accuracy and computational efficiency. Clustering results demonstrate that the incorporation of distributional predictions effectively addresses demand uncertainties, improving the quality of operational insights. Additionally, a simulation study demonstrates the practical value of short-term demand predictions for proactive strategies, such as idle fleet rebalancing, significantly enhancing delivery efficiency. By addressing demand uncertainties and operational constraints, our predict-then-cluster framework provides actionable insights for optimizing real-time operations. The approach is adaptable to other on-demand platform-based city logistics and passenger mobility services, promoting sustainable and efficient urban operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08466v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jingyi Cheng, Shadi Sharif Azadeh</dc:creator>
    </item>
    <item>
      <title>The Impact of AI-Driven Tools on Student Writing Development: A Case Study From The CGScholar AI Helper Project</title>
      <link>https://arxiv.org/abs/2501.08473</link>
      <description>arXiv:2501.08473v1 Announce Type: new 
Abstract: The case study examines the impact of the CGScholar (Common Ground Scholar) AI Helper on a pilot research initiative involving the writing development of 11th-grade students in English Language Arts (ELA). CGScholar AI Helper is an evolving and innovative web-based application designed to support students in their writing tasks by providing specified AI-generated feedback. This study is one of six interventions. It involved one teacher and six students in a diverse school with low income students and explored to what extent customized AI-driven feedback can support students' writing development. The findings suggest that the implementation of AI Helper supported the development of students' writing in a number of ways. It also elicited suggestions from the teacher and students about ways of improving the still in development tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08473v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raigul Zheldibayeva, Ana Karina de Oliveira Nascimento, Vania Castro, Mary Kalantzis, Bill Cope</dc:creator>
    </item>
    <item>
      <title>Addressing Intersectionality, Explainability, and Ethics in AI-Driven Diagnostics: A Rebuttal and Call for Transdiciplinary Action</title>
      <link>https://arxiv.org/abs/2501.08497</link>
      <description>arXiv:2501.08497v1 Announce Type: new 
Abstract: The increasing integration of artificial intelligence (AI) into medical diagnostics necessitates a critical examination of its ethical and practical implications. While the prioritization of diagnostic accuracy, as advocated by Sabuncu et al. (2025), is essential, this approach risks oversimplifying complex socio-ethical issues, including fairness, privacy, and intersectionality. This rebuttal emphasizes the dangers of reducing multifaceted health disparities to quantifiable metrics and advocates for a more transdisciplinary approach. By incorporating insights from social sciences, ethics, and public health, AI systems can address the compounded effects of intersecting identities and safeguard sensitive data. Additionally, explainability and interpretability must be central to AI design, fostering trust and accountability. This paper calls for a framework that balances accuracy with fairness, privacy, and inclusivity to ensure AI-driven diagnostics serve diverse populations equitably and ethically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08497v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Myles Joshua Toledo Tan, Panayiotis V. Benos</dc:creator>
    </item>
    <item>
      <title>A Refreshment Stirred, Not Shaken (II): Invariant-Preserving Deployments of Differential Privacy for the US Decennial Census</title>
      <link>https://arxiv.org/abs/2501.08449</link>
      <description>arXiv:2501.08449v1 Announce Type: cross 
Abstract: Through the lens of the system of differential privacy specifications developed in Part I of a trio of articles, this second paper examines two statistical disclosure control (SDC) methods for the United States Decennial Census: the Permutation Swapping Algorithm (PSA), which is similar to the 2010 Census's disclosure avoidance system (DAS), and the TopDown Algorithm (TDA), which was used in the 2020 DAS. To varying degrees, both methods leave unaltered some statistics of the confidential data $\unicode{x2013}$ which are called the method's invariants $\unicode{x2013}$ and hence neither can be readily reconciled with differential privacy (DP), at least as it was originally conceived. Nevertheless, we establish that the PSA satisfies $\varepsilon$-DP subject to the invariants it necessarily induces, thereby showing that this traditional SDC method can in fact still be understood within our more-general system of DP specifications. By a similar modification to $\rho$-zero concentrated DP, we also provide a DP specification for the TDA. Finally, as a point of comparison, we consider the counterfactual scenario in which the PSA was adopted for the 2020 Census, resulting in a reduction in the nominal privacy loss, but at the cost of releasing many more invariants. Therefore, while our results explicate the mathematical guarantees of SDC provided by the PSA, the TDA and the 2020 DAS in general, care must be taken in their translation to actual privacy protection $\unicode{x2013}$ just as is the case for any DP deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08449v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.DS</category>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Bailie, Ruobin Gong, Xiao-Li Meng</dc:creator>
    </item>
    <item>
      <title>SAIF: A Comprehensive Framework for Evaluating the Risks of Generative AI in the Public Sector</title>
      <link>https://arxiv.org/abs/2501.08814</link>
      <description>arXiv:2501.08814v1 Announce Type: cross 
Abstract: The rapid adoption of generative AI in the public sector, encompassing diverse applications ranging from automated public assistance to welfare services and immigration processes, highlights its transformative potential while underscoring the pressing need for thorough risk assessments. Despite its growing presence, evaluations of risks associated with AI-driven systems in the public sector remain insufficiently explored. Building upon an established taxonomy of AI risks derived from diverse government policies and corporate guidelines, we investigate the critical risks posed by generative AI in the public sector while extending the scope to account for its multimodal capabilities. In addition, we propose a Systematic dAta generatIon Framework for evaluating the risks of generative AI (SAIF). SAIF involves four key stages: breaking down risks, designing scenarios, applying jailbreak methods, and exploring prompt types. It ensures the systematic and consistent generation of prompt data, facilitating a comprehensive evaluation while providing a solid foundation for mitigating the risks. Furthermore, SAIF is designed to accommodate emerging jailbreak methods and evolving prompt types, thereby enabling effective responses to unforeseen risk scenarios. We believe that this study can play a crucial role in fostering the safe and responsible integration of generative AI into the public sector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08814v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyeongryul Lee, Heehyeon Kim, Joyce Jiyoung Whang</dc:creator>
    </item>
    <item>
      <title>Analyzing the Ethical Logic of Six Large Language Models</title>
      <link>https://arxiv.org/abs/2501.08951</link>
      <description>arXiv:2501.08951v1 Announce Type: cross 
Abstract: This study examines the ethical reasoning of six prominent generative large language models: OpenAI GPT-4o, Meta LLaMA 3.1, Perplexity, Anthropic Claude 3.5 Sonnet, Google Gemini, and Mistral 7B. The research explores how these models articulate and apply ethical logic, particularly in response to moral dilemmas such as the Trolley Problem, and Heinz Dilemma. Departing from traditional alignment studies, the study adopts an explainability-transparency framework, prompting models to explain their ethical reasoning. This approach is analyzed through three established ethical typologies: the consequentialist-deontological analytic, Moral Foundations Theory, and the Kohlberg Stages of Moral Development Model. Findings reveal that LLMs exhibit largely convergent ethical logic, marked by a rationalist, consequentialist emphasis, with decisions often prioritizing harm minimization and fairness. Despite similarities in pre-training and model architecture, a mixture of nuanced and significant differences in ethical reasoning emerge across models, reflecting variations in fine-tuning and post-training processes. The models consistently display erudition, caution, and self-awareness, presenting ethical reasoning akin to a graduate-level discourse in moral philosophy. In striking uniformity these systems all describe their ethical reasoning as more sophisticated than what is characteristic of typical human moral logic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08951v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>W. Russell Neuman, Chad Coleman, Manan Shah</dc:creator>
    </item>
    <item>
      <title>AoI in Context-Aware Hybrid Radio-Optical IoT Networks</title>
      <link>https://arxiv.org/abs/2412.12914</link>
      <description>arXiv:2412.12914v2 Announce Type: replace 
Abstract: With the surge in IoT devices ranging from wearables to smart homes, prompt transmission is crucial. The Age of Information (AoI) emerges as a critical metric in this context, representing the freshness of the information transmitted across the network. This paper studies hybrid IoT networks that employ Optical Communication (OC) as a reinforcement medium to Radio Frequency (RF). We formulate a non-linear convex optimization that adopts a multi-objective optimization strategy to dynamically schedule the communication between devices and select their corresponding communication technology, aiming to balance the maximization of network throughput with the minimization of energy usage and the frequency of switching between technologies. To mitigate the impact of dominant sub-objectives and their scale disparity, the designed approach employs a regularization method that approximates adequate sub-objective scaling weights. Simulation results show that the OC supplementary integration alongside RF enhances the network's overall performances and significantly reduces the Mean AoI and Peak AoI, allowing the collection of the freshest possible data using the best available communication technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12914v2</guid>
      <category>cs.CY</category>
      <category>eess.SP</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aymen Hamrouni, Sofie Pollin, Hazem Sallouha</dc:creator>
    </item>
    <item>
      <title>Pseudo-Automation: How Labor-Offsetting Technologies Reconfigure Roles and Relationships in Frontline Retail Work</title>
      <link>https://arxiv.org/abs/2410.02888</link>
      <description>arXiv:2410.02888v2 Announce Type: replace-cross 
Abstract: Self-service machines are a form of pseudo-automation; rather than actually automate tasks, they offset them to unpaid customers. Typically implemented for customer convenience and to reduce labor costs, self-service is often criticized for worsening customer service and increasing loss and theft for retailers. Though millions of frontline service workers continue to interact with these technologies on a day-to-day basis, little is known about how these machines change the nature of frontline labor. Through interviews with current and former cashiers who work with self-checkout technologies, we investigate how technology that offsets labor from an employee to a customer can reconfigure frontline work. We find three changes to cashiering tasks as a result of self-checkout: (1) Working at self-checkout involved parallel demands from multiple customers, (2) self-checkout work was more problem-oriented (including monitoring and policing customers), and (3) traditional checkout began to become more demanding as easier transactions were filtered to self-checkout. As their interactions with customers became more focused on problem solving and rule enforcement, cashiers were often positioned as adversaries to customers at self-checkout. To cope with perceived adversarialism, cashiers engaged in a form of relational patchwork, using techniques like scapegoating the self-checkout machine and providing excessive customer service in order to maintain positive customer interactions in the face of potential conflict. Our findings highlight how even under pseudo-automation, workers must engage in relational work to manage and mend negative human-to-human interactions so that machines can be properly implemented in context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02888v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3711051</arxiv:DOI>
      <dc:creator>Pegah Moradi, Karen Levy, Cristobal Cheyre</dc:creator>
    </item>
    <item>
      <title>Taming the Long Tail in Human Mobility Prediction</title>
      <link>https://arxiv.org/abs/2410.14970</link>
      <description>arXiv:2410.14970v4 Announce Type: replace-cross 
Abstract: With the popularity of location-based services, human mobility prediction plays a key role in enhancing personalized navigation, optimizing recommendation systems, and facilitating urban mobility and planning. This involves predicting a user's next POI (point-of-interest) visit using their past visit history. However, the uneven distribution of visitations over time and space, namely the long-tail problem in spatial distribution, makes it difficult for AI models to predict those POIs that are less visited by humans. In light of this issue, we propose the Long-Tail Adjusted Next POI Prediction (LoTNext) framework for mobility prediction, combining a Long-Tailed Graph Adjustment module to reduce the impact of the long-tailed nodes in the user-POI interaction graph and a novel Long-Tailed Loss Adjustment module to adjust loss by logit score and sample weight adjustment strategy. Also, we employ the auxiliary prediction task to enhance generalization and accuracy. Our experiments with two real-world trajectory datasets demonstrate that LoTNext significantly surpasses existing state-of-the-art works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14970v4</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaohang Xu, Renhe Jiang, Chuang Yang, Zipei Fan, Kaoru Sezaki</dc:creator>
    </item>
  </channel>
</rss>
