<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 04 Mar 2024 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 04 Mar 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Spatio-temporal modeling for record-breaking temperature events in Spain</title>
      <link>https://arxiv.org/abs/2403.00080</link>
      <description>arXiv:2403.00080v1 Announce Type: new 
Abstract: Record-breaking temperature events are now very frequently in the news, viewed as evidence of climate change. With this as motivation, we undertake the first substantial spatial modeling investigation of temperature record-breaking across years for any given day within the year. We work with a dataset consisting of over sixty years (1960-2021) of daily maximum temperatures across peninsular Spain. Formal statistical analysis of record-breaking events is an area that has received attention primarily within the probability community, dominated by results for the stationary record-breaking setting with some additional work addressing trends. Such effort is inadequate for analyzing actual record-breaking data. Effective analysis requires rich modeling of the indicator events which define record-breaking sequences. Resulting from novel and detailed exploratory data analysis, we propose hierarchical conditional models for the indicator events. After suitable model selection, we discover explicit trend behavior, necessary autoregression, significance of distance to the coast, useful interactions, helpful spatial random effects, and very strong daily random effects. Illustratively, the model estimates that global warming trends have increased the number of records expected in the past decade almost two-fold, 1.93 (1.89,1.98), but also estimates highly differentiated climate warming rates in space and by season.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00080v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jorge Castillo-MateoUniversity of Zaragoza, Alan E. GelfandDuke University, Zeus Gracia-TabuencaUniversity of Zaragoza, Jes\'us As\'inUniversity of Zaragoza, Ana C. Cebri\'anUniversity of Zaragoza</dc:creator>
    </item>
    <item>
      <title>Estimating the linear relation between variables that are never jointly observed: an application in in vivo experiments</title>
      <link>https://arxiv.org/abs/2403.00140</link>
      <description>arXiv:2403.00140v1 Announce Type: new 
Abstract: This work is motivated by in vivo experiments in which measurement are destructive so that the variables of interest can never be observed simultaneously when the aim is to estimate the regression coefficients of a linear regression. Assuming that the global experiment can be decomposed into sub experiments (corresponding for example to different doses) with distinct first moments, we propose different estimators of the linear regression which take account of that additional information. We consider estimators based on moments as well as estimators based optimal transport theory. These estimators are proved to be consistent as well as asymptotically Gaussian under weak hypotheses. The asymptotic variance has no explicit expression, except in some particular cases, and specific bootstrap approaches are developed to build confidence intervals for the estimated parameter. A Monte Carlo study is conducted to assess and compare the finite sample performances of the different approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00140v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Polina Arsenteva, Mohamed Amine Benadjaoud, Herv\'e Cardot</dc:creator>
    </item>
    <item>
      <title>Tobit models for count time series</title>
      <link>https://arxiv.org/abs/2403.00224</link>
      <description>arXiv:2403.00224v1 Announce Type: new 
Abstract: Several models for count time series have been developed during the last decades, often inspired by traditional autoregressive moving average (ARMA) models for real-valued time series, including integer-valued ARMA (INARMA) and integer-valued generalized autoregressive conditional heteroscedasticity (INGARCH) models. Both INARMA and INGARCH models exhibit an ARMA-like autocorrelation function (ACF). To achieve negative ACF values within the class of INGARCH models, log and softplus link functions are suggested in the literature, where the softplus approach leads to conditional linearity in good approximation. However, the softplus approach is limited to the INGARCH family for unbounded counts, i.e. it can neither be used for bounded counts, nor for count processes from the INARMA family. In this paper, we present an alternative solution, named the Tobit approach, for achieving approximate linearity together with negative ACF values, which is more generally applicable than the softplus approach. A Skellam--Tobit INGARCH model for unbounded counts is studied in detail, including stationarity, approximate computation of moments, maximum likelihood and censored least absolute deviations estimation for unknown parameters and corresponding simulations. Extensions of the Tobit approach to other situations are also discussed, including underlying discrete distributions, INAR models, and bounded counts. Three real-data examples are considered to illustrate the usefulness of the new approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00224v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian H. Wei\ss, Fukang Zhu</dc:creator>
    </item>
    <item>
      <title>Stable Reduced-Rank VAR Identification</title>
      <link>https://arxiv.org/abs/2403.00237</link>
      <description>arXiv:2403.00237v1 Announce Type: new 
Abstract: The vector autoregression (VAR) has been widely used in system identification, econometrics, natural science, and many other areas. However, when the state dimension becomes large the parameter dimension explodes. So rank reduced modelling is attractive and is well developed. But a fundamental requirement in almost all applications is stability of the fitted model. And this has not been addressed in the rank reduced case. Here, we develop, for the first time, a closed-form formula for an estimator of a rank reduced transition matrix which is guaranteed to be stable. We show that our estimator is consistent and asymptotically statistically efficient and illustrate it in comparative simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00237v1</guid>
      <category>stat.ME</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xinhui Rong, Victor Solo</dc:creator>
    </item>
    <item>
      <title>Wavelet Based Periodic Autoregressive Moving Average Models</title>
      <link>https://arxiv.org/abs/2403.00281</link>
      <description>arXiv:2403.00281v1 Announce Type: new 
Abstract: This paper proposes a wavelet-based method for analysing periodic autoregressive moving average (PARMA) time series. Even though Fourier analysis provides an effective method for analysing periodic time series, it requires the estimation of a large number of Fourier parameters when the PARMA parameters do not vary smoothly. The wavelet-based analysis helps us to obtain a parsimonious model with a reduced number of parameters. We have illustrated this with simulated and actual data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00281v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rhea Davis, N. Balakrishna</dc:creator>
    </item>
    <item>
      <title>Coherent forecasting of NoGeAR(1) model</title>
      <link>https://arxiv.org/abs/2403.00304</link>
      <description>arXiv:2403.00304v1 Announce Type: new 
Abstract: This article focuses on the coherent forecasting of the recently introduced novel geometric AR(1) (NoGeAR(1)) model - an INAR model based on inflated - parameter binomial thinning approach. Various techniques are available to achieve h - step ahead coherent forecasts of count time series, like median and mode forecasting. However, there needs to be more body of literature addressing coherent forecasting in the context of overdispersed count time series. Here, we study the forecasting distribution corresponding to NoGeAR(1) process using the Monte Carlo (MC) approximation method. Accordingly, several forecasting measures are employed in the simulation study to facilitate a thorough comparison of the forecasting capability of NoGeAR(1) with other models. The methodology is also demonstrated using real-life data, specifically the data on CW{\ss} TeXpert downloads and Barbados COVID-19 data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00304v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Divya Kuttenchalil Andrews, N. Balakrishna</dc:creator>
    </item>
    <item>
      <title>The Mollified (Discrete) Uniform Distribution and its Applications</title>
      <link>https://arxiv.org/abs/2403.00383</link>
      <description>arXiv:2403.00383v1 Announce Type: new 
Abstract: The mollified uniform distribution is rediscovered, which constitutes a ``soft'' version of the continuous uniform distribution. Important stochastic properties are derived and used to demonstrate potential fields of applications. For example, it constitutes a model covering platykurtic, mesokurtic and leptokurtic shapes. Its cumulative distribution function may also serve as the soft-clipping response function for defining generalized linear models with approximately linear dependence. Furthermore, it might be considered for teaching, as an appealing example for the convolution of random variables. Finally, a discrete type of mollified uniform distribution is briefly discussed as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00383v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian H. Wei{\ss}</dc:creator>
    </item>
    <item>
      <title>Population Power Curves in ASCA with Permutation Testing</title>
      <link>https://arxiv.org/abs/2403.00429</link>
      <description>arXiv:2403.00429v1 Announce Type: new 
Abstract: In this paper, we revisit the Power Curves in ANOVA Simultaneous Component Analysis (ASCA) based on permutation testing, and introduce the Population Curves derived from population parameters describing the relative effect among factors and interactions. We distinguish Relative from Absolute Population Curves, where the former represent statistical power in terms of the normalized effect size between structure and noise, and the latter in terms of the sample size. Relative Population Curves are useful to find the optimal ASCA model (e.g., fixed/random factors, crossed/nested relationships, interactions, the test statistic, transformations, etc.) for the analysis of an experimental design at hand. Absolute Population Curves are useful to determine the sample size and the optimal number of levels for each factor during the planning phase on an experiment. We illustrate both types of curves through simulation. We expect Population Curves to become the go-to approach to plan the optimal analysis pipeline and the required sample size in an omics study analyzed with ASCA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00429v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose Camacho, Michael Sorochan Armstrong</dc:creator>
    </item>
    <item>
      <title>Changepoint problem with angular data using a measure of variation based on the intrinsic geometry of torus</title>
      <link>https://arxiv.org/abs/2403.00508</link>
      <description>arXiv:2403.00508v1 Announce Type: new 
Abstract: In many temporally ordered data sets, it is observed that the parameters of the underlying distribution change abruptly at unknown times. The detection of such changepoints is important for many applications. While this problem has been studied substantially in the linear data setup, not much work has been done for angular data. In this article, we utilize the intrinsic geometry of a torus to introduce the notion of the `square of an angle' and use it to propose a new measure of variation, called the `curved variance', of an angular random variable. Using the above ideas, we propose new tests for the existence of changepoint(s) in the concentration, mean direction, and/or both of these. The limiting distributions of the test statistics are derived and their powers are obtained using extensive simulation. It is seen that the tests have better power than the corresponding existing tests. The proposed methods have been implemented on three real-life data sets revealing interesting insights. In particular, our method when used to detect simultaneous changes in mean direction and concentration for hourly wind direction measurements of the cyclonic storm `Amphan' identified changepoints that could be associated with important meteorological events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00508v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Surojit Biswas, Buddhananda Banerjee, Arnab Kumar Laha</dc:creator>
    </item>
    <item>
      <title>Random Interval Distillation for Detecting Multiple Changes in General Dependent Data</title>
      <link>https://arxiv.org/abs/2403.00600</link>
      <description>arXiv:2403.00600v1 Announce Type: new 
Abstract: We propose a new and generic approach for detecting multiple change-points in general dependent data, termed random interval distillation (RID). By collecting random intervals with sufficient strength of signals and reassembling them into a sequence of informative short intervals, our new approach captures the shifts in signal characteristics across diverse dependent data forms including locally stationary high-dimensional time series and dynamic networks with Markov formation. We further propose a range of secondary refinements tailored to various data types to enhance the localization precision. Notably, for univariate time series and low-rank autoregressive networks, our methods achieve the minimax optimality as their independent counterparts. For practical applications, we introduce a clustering-based and data-driven procedure to determine the optimal threshold for signal strength, which is adaptable to a wide array of dependent data scenarios utilizing the connection between RID and clustering. Additionally, our method has been extended to identify kinks and changes in signals characterized by piecewise polynomial trends. We examine the effectiveness and usefulness of our methodology via extensive simulation studies and a real data example, implementing it in the R-package rid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00600v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyuan Fan, Weichi Wu</dc:creator>
    </item>
    <item>
      <title>Distortion in Correspondence Analysis and in Taxicab Correspondence Analysis: A Comparison</title>
      <link>https://arxiv.org/abs/2403.00617</link>
      <description>arXiv:2403.00617v1 Announce Type: new 
Abstract: Distortion is a fundamental well-studied topic in dimension reduction papers, and intimately related with the underlying intrinsic dimension of a mapping of a high dimensional data set onto a lower dimension. In this paper, we study embedding distortions produced by Correspondence Analysis and its robust l1 variant Taxicab Correspondence analysis, which are visualization methods for contingency tables. For high dimensional data, distortions in Correspondence Analysis are contractions; while distortions in Taxicab Correspondence Analysis could be contractions or stretchings. This shows that Euclidean geometry is quite rigid, because of the orthogonality property; while Taxicab geometry is quite flexible, because the orthogonality property is replaced by the conjugacy property.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00617v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vartan Choulakian</dc:creator>
    </item>
    <item>
      <title>Hierarchical Bayesian Models to Mitigate Systematic Disparities in Prediction with Proxy Outcomes</title>
      <link>https://arxiv.org/abs/2403.00639</link>
      <description>arXiv:2403.00639v1 Announce Type: new 
Abstract: Label bias occurs when the outcome of interest is not directly observable and instead modeling is performed with proxy labels. When the difference between the true outcome and the proxy label is correlated with predictors, this can yield systematic disparities in predictions for different groups of interest. We propose Bayesian hierarchical measurement models to address these issues. Through practical examples, we demonstrate how our approach improves accuracy and helps with algorithmic fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00639v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Mikhaeil, Andrew Gelman, Philip Greengard</dc:creator>
    </item>
    <item>
      <title>Structurally Aware Robust Model Selection for Mixtures</title>
      <link>https://arxiv.org/abs/2403.00687</link>
      <description>arXiv:2403.00687v1 Announce Type: new 
Abstract: Mixture models are often used to identify meaningful subpopulations (i.e., clusters) in observed data such that the subpopulations have a real-world interpretation (e.g., as cell types). However, when used for subpopulation discovery, mixture model inference is usually ill-defined a priori because the assumed observation model is only an approximation to the true data-generating process. Thus, as the number of observations increases, rather than obtaining better inferences, the opposite occurs: the data is explained by adding spurious subpopulations that compensate for the shortcomings of the observation model. However, there are two important sources of prior knowledge that we can exploit to obtain well-defined results no matter the dataset size: known causal structure (e.g., knowing that the latent subpopulations cause the observed signal but not vice-versa) and a rough sense of how wrong the observation model is (e.g., based on small amounts of expert-labeled data or some understanding of the data-generating process). We propose a new model selection criteria that, while model-based, uses this available knowledge to obtain mixture model inferences that are robust to misspecification of the observation model. We provide theoretical support for our approach by proving a first-of-its-kind consistency result under intuitive assumptions. Simulation studies and an application to flow cytometry data demonstrate our model selection criteria consistently finds the correct number of subpopulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00687v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Li, Jonathan H. Huggins</dc:creator>
    </item>
    <item>
      <title>Bayesian Model Averaging for Partial Ordering Continual Reassessment Methods</title>
      <link>https://arxiv.org/abs/2403.00701</link>
      <description>arXiv:2403.00701v1 Announce Type: new 
Abstract: Phase I clinical trials are essential to bringing novel therapies from chemical development to widespread use. Traditional approaches to dose-finding in Phase I trials, such as the '3+3' method and the Continual Reassessment Method (CRM), provide a principled approach for escalating across dose levels. However, these methods lack the ability to incorporate uncertainty regarding the dose-toxicity ordering as found in combination drug trials. Under this setting, dose-levels vary across multiple drugs simultaneously, leading to multiple possible dose-toxicity orderings. The Partial Ordering CRM (POCRM) extends to these settings by allowing for multiple dose-toxicity orderings. In this work, it is shown that the POCRM is vulnerable to 'estimation incoherency' whereby toxicity estimates shift in an illogical way, threatening patient safety and undermining clinician trust in dose-finding models. To this end, the Bayesian model averaged POCRM (BMA-POCRM) is proposed. BMA-POCRM uses Bayesian model averaging to take into account all possible orderings simultaneously, reducing the frequency of estimation incoherencies. The effectiveness of BMA-POCRM in drug combination settings is demonstrated through a specific instance of estimate incoherency of POCRM and simulation studies. The results highlight the improved safety, accuracy and reduced occurrence of estimate incoherency in trials applying the BMA-POCRM relative to the POCRM model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00701v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luka KovacevicMRC Biostatistics Unit, University of Cambridge, Thomas JakiMRC Biostatistics Unit, University of Cambridge, Department of Machine Learning and Data Science, University of Regensburg, Helen BarnettLancaster University, Pavel MozgunovMRC Biostatistics Unit, University of Cambridge</dc:creator>
    </item>
    <item>
      <title>Automated Efficient Estimation using Monte Carlo Efficient Influence Functions</title>
      <link>https://arxiv.org/abs/2403.00158</link>
      <description>arXiv:2403.00158v1 Announce Type: cross 
Abstract: Many practical problems involve estimating low dimensional statistical quantities with high-dimensional models and datasets. Several approaches address these estimation tasks based on the theory of influence functions, such as debiased/double ML or targeted minimum loss estimation. This paper introduces \textit{Monte Carlo Efficient Influence Functions} (MC-EIF), a fully automated technique for approximating efficient influence functions that integrates seamlessly with existing differentiable probabilistic programming systems. MC-EIF automates efficient statistical estimation for a broad class of models and target functionals that would previously require rigorous custom analysis. We prove that MC-EIF is consistent, and that estimators using MC-EIF achieve optimal $\sqrt{N}$ convergence rates. We show empirically that estimators using MC-EIF are at parity with estimators using analytic EIFs. Finally, we demonstrate a novel capstone example using MC-EIF for optimal portfolio selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00158v1</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raj Agrawal, Sam Witty, Andy Zane, Eli Bingham</dc:creator>
    </item>
    <item>
      <title>Defining Expertise: Applications to Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2403.00694</link>
      <description>arXiv:2403.00694v1 Announce Type: cross 
Abstract: Decision-makers are often experts of their domain and take actions based on their domain knowledge. Doctors, for instance, may prescribe treatments by predicting the likely outcome of each available treatment. Actions of an expert thus naturally encode part of their domain knowledge, and can help make inferences within the same domain: Knowing doctors try to prescribe the best treatment for their patients, we can tell treatments prescribed more frequently are likely to be more effective. Yet in machine learning, the fact that most decision-makers are experts is often overlooked, and "expertise" is seldom leveraged as an inductive bias. This is especially true for the literature on treatment effect estimation, where often the only assumption made about actions is that of overlap. In this paper, we argue that expertise - particularly the type of expertise the decision-makers of a domain are likely to have - can be informative in designing and selecting methods for treatment effect estimation. We formally define two types of expertise, predictive and prognostic, and demonstrate empirically that: (i) the prominent type of expertise in a domain significantly influences the performance of different methods in treatment effect estimation, and (ii) it is possible to predict the type of expertise present in a dataset, which can provide a quantitative basis for model selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00694v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alihan H\"uy\"uk, Qiyao Wei, Alicia Curth, Mihaela van der Schaar</dc:creator>
    </item>
    <item>
      <title>Shrinkage estimators in zero-inflated Bell regression model with application</title>
      <link>https://arxiv.org/abs/2403.00749</link>
      <description>arXiv:2403.00749v1 Announce Type: cross 
Abstract: We propose Stein-type estimators for zero-inflated Bell regression models by incorporating information on model parameters. These estimators combine the advantages of unrestricted and restricted estimators. We derive the asymptotic distributional properties, including bias and mean squared error, for the proposed shrinkage estimators. Monte Carlo simulations demonstrate the superior performance of our shrinkage estimators across various scenarios. Furthermore, we apply the proposed estimators to analyze a real dataset, showcasing their practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00749v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Solmaz Seifollahi, Hossein Bevrani, Zakariya Yahya Algamal</dc:creator>
    </item>
    <item>
      <title>Disentangling the structure of ecological bipartite networks from observation processes</title>
      <link>https://arxiv.org/abs/2211.16364</link>
      <description>arXiv:2211.16364v2 Announce Type: replace 
Abstract: The structure of a bipartite interaction network can be described by providing a clustering for each of the two types of nodes. Such clusterings are outputted by fitting a Latent Block Model (LBM) on an observed network that comes from a sampling of species interactions in the field. However, the sampling is limited and possibly uneven. This may jeopardize the fit of the LBM and then the description of the structure of the network by detecting structures which result from the sampling and not from actual underlying ecological phenomena. If the observed interaction network consists of a weighted bipartite network where the number of observed interactions between two species is available, the sampling efforts for all species can be estimated and used to correct the LBM fit. We propose to combine an observation model that accounts for sampling and an LBM for describing the structure of underlying possible ecological interactions. We develop an original inference procedure for this model, the efficiency of which is demonstrated on simulation studies. The pratical interest in ecology of our model is highlighted on a large dataset of plant-pollinator network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.16364v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emre Anakok, Pierre Barbillon, Colin Fontaine, Elisa Thebault</dc:creator>
    </item>
    <item>
      <title>Modifying Survival Models To Accommodate Thresholding Behavior</title>
      <link>https://arxiv.org/abs/2212.07602</link>
      <description>arXiv:2212.07602v2 Announce Type: replace 
Abstract: Survival models capture the relationship between an accumulating hazard and the occurrence of a singular event stimulated by that accumulation. When the model for the hazard is sufficiently flexible survival models can accommodate a wide range of behaviors. If the hazard model is less flexible, for example when it is constrained by an external physical process, then the resulting survival model can be much too rigid. In this paper I introduce a modified survival model that generalizes the relationship between accumulating hazard and event occurrence with particular emphasis on capturing thresholding behavior. Finally I demonstrate the utility of this approach on a physiological application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.07602v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Betancourt</dc:creator>
    </item>
    <item>
      <title>Multivariate Bayesian variable selection with application to multi-trait genetic fine mapping</title>
      <link>https://arxiv.org/abs/2212.13294</link>
      <description>arXiv:2212.13294v3 Announce Type: replace 
Abstract: Variable selection has played a critical role in modern statistical learning and scientific discoveries. Numerous regularization and Bayesian variable selection methods have been developed in the past two decades for variable selection, but most of these methods consider selecting variables for only one response. As more data is being collected nowadays, it is common to analyze multiple related responses from the same study. Existing multivariate variable selection methods select variables for all responses without considering the possible heterogeneity across different responses, i.e. some features may only predict a subset of responses but not the rest. Motivated by the multi-trait fine mapping problem in genetics to identify the causal variants for multiple related traits, we developed a novel multivariate Bayesian variable selection method to select critical predictors from a large number of grouped predictors that target at multiple correlated and possibly heterogeneous responses. Our new method is featured by its selection at multiple levels, its incorporation of prior biological knowledge to guide selection and identification of best subset of responses predictors target at. We showed the advantage of our method via extensive simulations and a real fine mapping example to identify causal variants associated with different subsets of addictive behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.13294v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Travis Canida, Hongjie Ke, Shuo Chen, Zhenayo Ye, Tianzhou Ma</dc:creator>
    </item>
    <item>
      <title>Discovery of Critical Thresholds in Mixed Exposures and Estimation of Policy Intervention Effects using Targeted Learning</title>
      <link>https://arxiv.org/abs/2302.07976</link>
      <description>arXiv:2302.07976v2 Announce Type: replace 
Abstract: Traditional regulations of chemical exposure tend to focus on single exposures, overlooking the potential amplified toxicity due to multiple concurrent exposures. We are interested in understanding the average outcome if exposures were limited to fall under a multivariate threshold. Because threshold levels are often unknown \textit{a priori}, we provide an algorithm that finds exposure threshold levels where the expected outcome is maximized or minimized. Because both identifying thresholds and estimating policy effects on the same data would lead to overfitting bias, we also provide a data-adaptive estimation framework, which allows for both threshold discovery and policy estimation. Simulation studies show asymptotic convergence to the optimal exposure region and to the true effect of an intervention. We demonstrate how our method identifies true interactions in a public synthetic mixture data set. Finally, we applied our method to NHANES data to discover metal exposures that have the most harmful effects on telomere length. We provide an implementation in the \texttt{CVtreeMLE} R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.07976v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David McCoy, Alan Hubbard, Alejandro Schuler, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>Resolving power: A general approach to compare the distinguishing ability of threshold-free evaluation metrics</title>
      <link>https://arxiv.org/abs/2304.00059</link>
      <description>arXiv:2304.00059v2 Announce Type: replace 
Abstract: Selecting an evaluation metric is fundamental to model development, but uncertainty remains about when certain metrics are preferable and why. This paper introduces the concept of resolving power to describe the ability of an evaluation metric to distinguish between binary classifiers of similar quality. This ability depends on two attributes: 1. The metric's response to improvements in classifier quality (its signal), and 2. The metric's sampling variability (its noise). The paper defines resolving power generically as a metric's sampling uncertainty scaled by its signal. The primary application of resolving power is to assess threshold-free evaluation metrics, such as the area under the receiver operating characteristic curve (AUROC) and the area under the precision-recall curve (AUPRC). A simulation study compares the AUROC and the AUPRC in a variety of contexts. It finds that the AUROC generally has greater resolving power, but that the AUPRC is better when searching among high-quality classifiers applied to low prevalence outcomes. The paper concludes by proposing an empirical method to estimate resolving power that can be applied to any dataset and any initial classification model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.00059v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Colin S. Beam</dc:creator>
    </item>
    <item>
      <title>Finite Population Survey Sampling: An Unapologetic Bayesian Perspective</title>
      <link>https://arxiv.org/abs/2306.10635</link>
      <description>arXiv:2306.10635v2 Announce Type: replace 
Abstract: This article attempts to offer some perspectives on Bayesian inference for finite population quantities when the units in the population are assumed to exhibit complex dependencies. Beginning with an overview of Bayesian hierarchical models, including some that yield design-based Horvitz-Thompson estimators, the article proceeds to introduce dependence in finite populations and sets out inferential frameworks for ignorable and nonignorable responses. Multivariate dependencies using graphical models and spatial processes are discussed and some salient features of two recent analyses for spatial finite populations are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10635v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>CR-Lasso: Robust cellwise regularized sparse regression</title>
      <link>https://arxiv.org/abs/2307.05234</link>
      <description>arXiv:2307.05234v2 Announce Type: replace 
Abstract: Cellwise contamination remains a challenging problem for data scientists, particularly in research fields that require the selection of sparse features. Traditional robust methods may not be feasible nor efficient in dealing with such contaminated datasets. We propose CR-Lasso, a robust Lasso-type cellwise regularization procedure that performs feature selection in the presence of cellwise outliers by minimising a regression loss and cell deviation measure simultaneously. To evaluate the approach, we conduct empirical studies comparing its selection and prediction performance with several sparse regression methods. We show that CR-Lasso is competitive under the settings considered. We illustrate the effectiveness of the proposed method on real data through an analysis of a bone mineral density dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.05234v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Su, Garth Tarr, Samuel Muller, Suojin Wang</dc:creator>
    </item>
    <item>
      <title>Spectral Ranking Inferences based on General Multiway Comparisons</title>
      <link>https://arxiv.org/abs/2308.02918</link>
      <description>arXiv:2308.02918v3 Announce Type: replace 
Abstract: This paper studies the performance of the spectral method in the estimation and uncertainty quantification of the unobserved preference scores of compared entities in a general and more realistic setup. Specifically, the comparison graph consists of hyper-edges of possible heterogeneous sizes, and the number of comparisons can be as low as one for a given hyper-edge. Such a setting is pervasive in real applications, circumventing the need to specify the graph randomness and the restrictive homogeneous sampling assumption imposed in the commonly used Bradley-Terry-Luce (BTL) or Plackett-Luce (PL) models. Furthermore, in scenarios where the BTL or PL models are appropriate, we unravel the relationship between the spectral estimator and the Maximum Likelihood Estimator (MLE). We discover that a two-step spectral method, where we apply the optimal weighting estimated from the equal weighting vanilla spectral method, can achieve the same asymptotic efficiency as the MLE. Given the asymptotic distributions of the estimated preference scores, we also introduce a comprehensive framework to carry out both one-sample and two-sample ranking inferences, applicable to both fixed and random graph settings. It is noteworthy that this is the first time effective two-sample rank testing methods have been proposed. Finally, we substantiate our findings via comprehensive numerical simulations and subsequently apply our developed methodologies to perform statistical inferences for statistical journals and movie rankings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.02918v3</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianqing Fan, Zhipeng Lou, Weichen Wang, Mengxin Yu</dc:creator>
    </item>
    <item>
      <title>Simulation-based stacking</title>
      <link>https://arxiv.org/abs/2310.17009</link>
      <description>arXiv:2310.17009v2 Announce Type: replace 
Abstract: Simulation-based inference has been popular for amortized Bayesian computation. It is typical to have more than one posterior approximation, from different inference algorithms, different architectures, or simply the randomness of initialization and stochastic gradients. With a consistency guarantee, we present a general posterior stacking framework to make use of all available approximations. Our stacking method is able to combine densities, simulation draws, confidence intervals, and moments, and address the overall precision, calibration, coverage, and bias of the posterior approximation at the same time. We illustrate our method on several benchmark simulations and a challenging cosmological inference task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17009v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuling Yao, Bruno R\'egaldo-Saint Blancard, Justin Domke</dc:creator>
    </item>
    <item>
      <title>The `Why' behind including `Y' in your imputation model</title>
      <link>https://arxiv.org/abs/2310.17434</link>
      <description>arXiv:2310.17434v2 Announce Type: replace 
Abstract: Missing data is a common challenge when analyzing epidemiological data, and imputation is often used to address this issue. Here, we investigate the scenario where a covariate used in an analysis has missingness and will be imputed. There are recommendations to include the outcome from the analysis model in the imputation model for missing covariates, but it is not necessarily clear if this recommendation always holds and why this is sometimes true. We examine deterministic imputation (i.e., single imputation with fixed values) and stochastic imputation (i.e., single or multiple imputation with random values) methods and their implications for estimating the relationship between the imputed covariate and the outcome. We mathematically demonstrate that including the outcome variable in imputation models is not just a recommendation but a requirement to achieve unbiased results when using stochastic imputation methods. Moreover, we dispel common misconceptions about deterministic imputation models and demonstrate why the outcome should not be included in these models. This paper aims to bridge the gap between imputation in theory and in practice, providing mathematical derivations to explain common statistical recommendations. We offer a better understanding of the considerations involved in imputing missing covariates and emphasize when it is necessary to include the outcome variable in the imputation model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17434v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucy D'Agostino McGowan, Sarah C. Lotspeich, Staci A. Hepler</dc:creator>
    </item>
    <item>
      <title>Measurement and comparison of distributional shift and its relation to rarity, poverty, and scarcity</title>
      <link>https://arxiv.org/abs/2401.11119</link>
      <description>arXiv:2401.11119v3 Announce Type: replace 
Abstract: The comparison of frequency distributions is a common statistical task with broad applications. However, existing measures do not explicitly quantify the magnitude and direction by which one distribution is shifted relative to another. In the present study, we define distributional shift (DS) as the concentration of frequencies towards the lowest discrete class, e.g., the left-most bin of a histogram. We measure DS via the sum of cumulative frequencies and define relative distributional shift (RDS) as the difference in DS between distributions. Using simulated random sampling, we show that RDS is highly related to measures that are widely used to compare frequency distributions. Focusing on specific applications, we show that DS and RDS provide insights into healthcare billing distributions, ecological species-abundance distributions, and economic distributions of wealth. RDS has the unique advantage of being a signed (i.e., directional) measure based on a simple difference in an intuitive property that, in turn, serves as a measure of rarity, poverty, and scarcity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11119v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenneth J. Locey, Brian D. Stein</dc:creator>
    </item>
    <item>
      <title>Generation and analysis of synthetic data via Bayesian networks: a robust approach for uncertainty quantification via Bayesian paradigm</title>
      <link>https://arxiv.org/abs/2402.17915</link>
      <description>arXiv:2402.17915v2 Announce Type: replace 
Abstract: Safe and reliable disclosure of information from confidential data is a challenging statistical problem. A common approach considers the generation of synthetic data, to be disclosed instead of the original data. Efficient approaches ought to deal with the trade-off between reliability and confidentiality of the released data. Ultimately, the aim is to be able to reproduce as accurately as possible statistical analysis of the original data using the synthetic one. Bayesian networks is a model-based approach that can be used to parsimoniously estimate the underlying distribution of the original data and generate synthetic datasets. These ought to not only approximate the results of analyses with the original data but also robustly quantify the uncertainty involved in the approximation. This paper proposes a fully Bayesian approach to generate and analyze synthetic data based on the posterior predictive distribution of statistics of the synthetic data, allowing for efficient uncertainty quantification. The methodology makes use of probability properties of the model to devise a computationally efficient algorithm to obtain the target predictive distributions via Monte Carlo. Model parsimony is handled by proposing a general class of penalizing priors for Bayesian network models. Finally, the efficiency and applicability of the proposed methodology is empirically investigated through simulated and real examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17915v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Larissa N. A. Martins, Fl\'avio B. Gon\c{c}alves, Thais P. Galletti</dc:creator>
    </item>
    <item>
      <title>Hamiltonian Monte Carlo for Regression with High-Dimensional Categorical Data</title>
      <link>https://arxiv.org/abs/2107.08112</link>
      <description>arXiv:2107.08112v2 Announce Type: replace-cross 
Abstract: Latent variable models are increasingly used in economics for high-dimensional categorical data like text and surveys. We demonstrate the effectiveness of Hamiltonian Monte Carlo (HMC) with parallelized automatic differentiation for analyzing such data in a computationally efficient and methodologically sound manner. Our new model, Supervised Topic Model with Covariates, shows that carefully modeling this type of data can have significant implications on conclusions compared to a simpler, frequently used, yet methodologically problematic, two-step approach. A simulation study and revisiting Bandiera et al. (2020)'s study of executive time use demonstrate these results. The approach accommodates thousands of parameters and doesn't require custom algorithms specific to each model, making it accessible for applied researchers</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.08112v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Szymon Sacher, Laura Battaglia, Stephen Hansen</dc:creator>
    </item>
    <item>
      <title>On Recoding Ordered Treatments as Binary Indicators</title>
      <link>https://arxiv.org/abs/2111.12258</link>
      <description>arXiv:2111.12258v4 Announce Type: replace-cross 
Abstract: Researchers using instrumental variables to investigate ordered treatments often recode treatment into an indicator for any exposure. We investigate this estimand under the assumption that the instruments shift compliers from no treatment to some but not from some treatment to more. We show that when there are extensive margin compliers only (EMCO) this estimand captures a weighted average of treatment effects that can be partially unbundled into each complier group's potential outcome means. We also establish an equivalence between EMCO and a two-factor selection model and apply our results to study treatment heterogeneity in the Oregon Health Insurance Experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.12258v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evan K. Rose, Yotam Shem-Tov</dc:creator>
    </item>
    <item>
      <title>lpcde: Estimation and Inference for Local Polynomial Conditional Density Estimators</title>
      <link>https://arxiv.org/abs/2204.10375</link>
      <description>arXiv:2204.10375v2 Announce Type: replace-cross 
Abstract: This paper discusses the R package lpcde, which stands for local polynomial conditional density estimation. It implements the kernel-based local polynomial smoothing methods introduced in Cattaneo, Chandak, Jansson, Ma (2024( for statistical estimation and inference of conditional distributions, densities, and derivatives thereof. The package offers mean square error optimal bandwidth selection and associated point estimators, as well as uncertainty quantification based on robust bias correction both pointwise (e.g., confidence intervals) and uniformly (e.g., confidence bands) over evaluation points. The methods implemented are boundary adaptive whenever the data is compactly supported. The package also implements regularized conditional density estimation methods, ensuring the resulting density estimate is non-negative and integrates to one. We contrast the functionalities of lpcde with existing R packages for conditional density estimation, and showcase its main features using simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10375v2</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rajita Chandak, Michael Jansson, Xinwei Ma</dc:creator>
    </item>
    <item>
      <title>An improved BISG for inferring race from surname and geolocation</title>
      <link>https://arxiv.org/abs/2304.09126</link>
      <description>arXiv:2304.09126v3 Announce Type: replace-cross 
Abstract: Bayesian Improved Surname Geocoding (BISG) is a ubiquitous tool for predicting race and ethnicity using an individual's geolocation and surname. Here we demonstrate that statistical dependence of surname and geolocation within racial/ethnic categories in the United States results in biases for minority subpopulations, and we introduce a raking-based improvement. Our method augments the data used by BISG--distributions of race by geolocation and race by surname--with the distribution of surname by geolocation obtained from state voter files. We validate our algorithm on state voter registration lists that contain self-identified race/ethnicity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.09126v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philip Greengard, Andrew Gelman</dc:creator>
    </item>
    <item>
      <title>Tuning-Free Maximum Likelihood Training of Latent Variable Models via Coin Betting</title>
      <link>https://arxiv.org/abs/2305.14916</link>
      <description>arXiv:2305.14916v2 Announce Type: replace-cross 
Abstract: We introduce two new particle-based algorithms for learning latent variable models via marginal maximum likelihood estimation, including one which is entirely tuning-free. Our methods are based on the perspective of marginal maximum likelihood estimation as an optimization problem: namely, as the minimization of a free energy functional. One way to solve this problem is via the discretization of a gradient flow associated with the free energy. We study one such approach, which resembles an extension of Stein variational gradient descent, establishing a descent lemma which guarantees that the free energy decreases at each iteration. This method, and any other obtained as the discretization of the gradient flow, necessarily depends on a learning rate which must be carefully tuned by the practitioner in order to ensure convergence at a suitable rate. With this in mind, we also propose another algorithm for optimizing the free energy which is entirely learning rate free, based on coin betting techniques from convex optimization. We validate the performance of our algorithms across several numerical experiments, including several high-dimensional settings. Our results are competitive with existing particle-based methods, without the need for any hyperparameter tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.14916v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis Sharrock, Daniel Dodd, Christopher Nemeth</dc:creator>
    </item>
    <item>
      <title>Is it possible to know cosmological fine-tuning?</title>
      <link>https://arxiv.org/abs/2401.04190</link>
      <description>arXiv:2401.04190v2 Announce Type: replace-cross 
Abstract: Fine-tuning studies whether some physical parameters, or relevant ratios between them, are located within so-called life-permitting intervals of small probability outside of which carbon-based life would not be possible. Recent developments have found estimates of these probabilities that circumvent previous concerns of measurability and selection bias. However, the question remains if fine-tuning can indeed be known. Using a mathematization of the epistemological concepts of learning and knowledge acquisition, we argue that most examples that have been touted as fine-tuned cannot be formally assessed as such. Nevertheless, fine-tuning can be known when the physical parameter is seen as a random variable and it is supported in the nonnegative real line, provided the size of the life-permitting interval is small in relation to the observed value of the parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.04190v2</guid>
      <category>astro-ph.CO</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel Andr\'es D\'iaz-Pach\'on, Ola H\"ossjer, Calvin Mathew</dc:creator>
    </item>
  </channel>
</rss>
