<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 May 2024 04:07:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Finite Sample Analysis and Bounds of Generalization Error of Gradient Descent in In-Context Linear Regression</title>
      <link>https://arxiv.org/abs/2405.02462</link>
      <description>arXiv:2405.02462v1 Announce Type: new 
Abstract: Recent studies show that transformer-based architectures emulate gradient descent during a forward pass, contributing to in-context learning capabilities - an ability where the model adapts to new tasks based on a sequence of prompt examples without being explicitly trained or fine tuned to do so. This work investigates the generalization properties of a single step of gradient descent in the context of linear regression with well-specified models. A random design setting is considered and analytical expressions are derived for the statistical properties of generalization error in a non-asymptotic (finite sample) setting. These expressions are notable for avoiding arbitrary constants, and thus offer robust quantitative information and scaling relationships. These results are contrasted with those from classical least squares regression (for which analogous finite sample bounds are also derived), shedding light on systematic and noise components, as well as optimal step sizes. Additionally, identities involving high-order products of Gaussian random matrices are presented as a byproduct of the analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02462v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karthik Duraisamy</dc:creator>
    </item>
    <item>
      <title>Multimatrix variate distributions</title>
      <link>https://arxiv.org/abs/2405.02498</link>
      <description>arXiv:2405.02498v1 Announce Type: new 
Abstract: A new family of distributions indexed by the class of matrix variate contoured elliptically distribution is proposed as an extension of some bimatrix variate distributions. The termed \emph{multimatrix variate distributions} open new perspectives for the classical distribution theory, usually based on probabilistic independent models and preferred untested fitting laws. Most of the multimatrix models here derived are invariant under the spherical family, a fact that solves the testing and prior knowledge of the underlying distributions and elucidates the statistical methodology in contrasts with some weakness of current studies as copulas. The paper also includes a number of diverse special cases, properties and generalisations. The new joint distributions allows several unthinkable combinations for copulas, such as scalars, vectors and matrices, all of them adjustable to the required models of the experts. The proposed joint distributions are also easily computable, then several applications are plausible. In particular, an exhaustive example in molecular docking on SARS-CoV-2 presents the results on matrix dependent samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02498v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jos\'e A. D\'iaz-Garc\'ia, Francisco J. Caro-Lopera</dc:creator>
    </item>
    <item>
      <title>Stability of a Generalized Debiased Lasso with Applications to Resampling-Based Variable Selection</title>
      <link>https://arxiv.org/abs/2405.03063</link>
      <description>arXiv:2405.03063v1 Announce Type: new 
Abstract: Suppose that we first apply the Lasso to a design matrix, and then update one of its columns. In general, the signs of the Lasso coefficients may change, and there is no closed-form expression for updating the Lasso solution exactly. In this work, we propose an approximate formula for updating a debiased Lasso coefficient. We provide general nonasymptotic error bounds in terms of the norms and correlations of a given design matrix's columns, and then prove asymptotic convergence results for the case of a random design matrix with i.i.d.\ sub-Gaussian row vectors and i.i.d.\ Gaussian noise. Notably, the approximate formula is asymptotically correct for most coordinates in the proportional growth regime, under the mild assumption that each row of the design matrix is sub-Gaussian with a covariance matrix having a bounded condition number. Our proof only requires certain concentration and anti-concentration properties to control various error terms and the number of sign changes. In contrast, rigorously establishing distributional limit properties (e.g.\ Gaussian limits for the debiased Lasso) under similarly general assumptions has been considered open problem in the universality theory. As applications, we show that the approximate formula allows us to reduce the computation complexity of variable selection algorithms that require solving multiple Lasso problems, such as the conditional randomization test and a variant of the knockoff filter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03063v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingbo Liu</dc:creator>
    </item>
    <item>
      <title>Precision-based designs for sequential randomized experiments</title>
      <link>https://arxiv.org/abs/2405.03487</link>
      <description>arXiv:2405.03487v1 Announce Type: new 
Abstract: In this paper, we consider an experimental setting where units enter the experiment sequentially. Our goal is to form stopping rules which lead to estimators of treatment effects with a given precision. We propose a fixed-width confidence interval design (FWCID) where the experiment terminates once a pre-specified confidence interval width is achieved. We show that under this design, the difference-in-means estimator is a consistent estimator of the average treatment effect and standard confidence intervals have asymptotic guarantees of coverage and efficiency for several versions of the design. In addition, we propose a version of the design that we call fixed power design (FPD) where a given power is asymptotically guaranteed for a given treatment effect, without the need to specify the variances of the outcomes under treatment or control. In addition, this design also gives a consistent difference-in-means estimator with correct coverage of the corresponding standard confidence interval. We complement our theoretical findings with Monte Carlo simulations where we compare our proposed designs with standard designs in the sequential experiments literature, showing that our designs outperform these designs in several important aspects. We believe our results to be relevant for many experimental settings where units enter sequentially, such as in clinical trials, as well as in online A/B tests used by the tech and e-commerce industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03487v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mattias Nordin, M{\aa}rten Schultzberg</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference for Estimating Heat Sources through Temperature Assimilation</title>
      <link>https://arxiv.org/abs/2405.02319</link>
      <description>arXiv:2405.02319v1 Announce Type: cross 
Abstract: This paper introduces a Bayesian inference framework for two-dimensional steady-state heat conduction, focusing on the estimation of unknown distributed heat sources in a thermally-conducting medium with uniform conductivity. The goal is to infer heater locations, strengths, and shapes using temperature assimilation in the Euclidean space, employing a Fourier series to represent each heater's shape. The Markov Chain Monte Carlo (MCMC) method, incorporating the random-walk Metropolis-Hasting algorithm and parallel tempering, is utilized for posterior distribution exploration in both unbounded and wall-bounded domains. Strong correlations between heat strength and heater area prompt caution against simultaneously estimating these two quantities. It is found that multiple solutions arise in cases where the number of temperature sensors is less than the number of unknown states. Moreover, smaller heaters introduce greater uncertainty in estimated strength. The diffusive nature of heat conduction smooths out any deformations in the temperature contours, especially in the presence of multiple heaters positioned near each other, impacting convergence. In wall-bounded domains with Neumann boundary conditions, the inference of heater parameters tends to be more accurate than in unbounded domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02319v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hanieh Mousavi, Jeff D. Eldredge</dc:creator>
    </item>
    <item>
      <title>Power-Enhanced Two-Sample Mean Tests for High-Dimensional Compositional Data with Application to Microbiome Data Analysis</title>
      <link>https://arxiv.org/abs/2405.02551</link>
      <description>arXiv:2405.02551v1 Announce Type: cross 
Abstract: Testing differences in mean vectors is a fundamental task in the analysis of high-dimensional compositional data. Existing methods may suffer from low power if the underlying signal pattern is in a situation that does not favor the deployed test. In this work, we develop two-sample power-enhanced mean tests for high-dimensional compositional data based on the combination of $p$-values, which integrates strengths from two popular types of tests: the maximum-type test and the quadratic-type test. We provide rigorous theoretical guarantees on the proposed tests, showing accurate Type-I error rate control and enhanced testing power. Our method boosts the testing power towards a broader alternative space, which yields robust performance across a wide range of signal pattern settings. Our theory also contributes to the literature on power enhancement and Gaussian approximation for high-dimensional hypothesis testing. We demonstrate the performance of our method on both simulated data and real-world microbiome data, showing that our proposed approach improves the testing power substantially compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02551v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danning Li, Lingzhou Xue, Haoyi Yang, Xiufan Yu</dc:creator>
    </item>
    <item>
      <title>Grouping predictors via network-wide metrics</title>
      <link>https://arxiv.org/abs/2405.02715</link>
      <description>arXiv:2405.02715v1 Announce Type: cross 
Abstract: When multitudes of features can plausibly be associated with a response, both privacy considerations and model parsimony suggest grouping them to increase the predictive power of a regression model. Specifically, the identification of groups of predictors significantly associated with the response variable eases further downstream analysis and decision-making. This paper proposes a new data analysis methodology that utilizes the high-dimensional predictor space to construct an implicit network with weighted edges %and weights on the edges to identify significant associations between the response and the predictors. Using a population model for groups of predictors defined via network-wide metrics, a new supervised grouping algorithm is proposed to determine the correct group, with probability tending to one as the sample size diverges to infinity. For this reason, we establish several theoretical properties of the estimates of network-wide metrics. A novel model-assisted bootstrap procedure that substantially decreases computational complexity is developed, facilitating the assessment of uncertainty in the estimates of network-wide metrics. The proposed methods account for several challenges that arise in the high-dimensional data setting, including (i) a large number of predictors, (ii) uncertainty regarding the true statistical model, and (iii) model selection variability. The performance of the proposed methods is demonstrated through numerical experiments, data from sports analytics, and breast cancer data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02715v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brandon Woosuk Park, Anand N. Vidyashankar, Tucker S. McElroy</dc:creator>
    </item>
    <item>
      <title>Unscented Trajectory Optimization</title>
      <link>https://arxiv.org/abs/2405.02753</link>
      <description>arXiv:2405.02753v1 Announce Type: cross 
Abstract: In a nutshell, unscented trajectory optimization is the generation of optimal trajectories through the use of an unscented transform. Although unscented trajectory optimization was introduced by the authors about a decade ago, it is reintroduced in this paper as a special instantiation of tychastic optimal control theory. Tychastic optimal control theory (from \textit{Tyche}, the Greek goddess of chance) avoids the use of a Brownian motion and the resulting It\^{o} calculus even though it uses random variables across the entire spectrum of a problem formulation. This approach circumvents the enormous technical and numerical challenges associated with stochastic trajectory optimization. Furthermore, it is shown how a tychastic optimal control problem that involves nonlinear transformations of the expectation operator can be quickly instantiated using an unscented transform. These nonlinear transformations are particularly useful in managing trajectory dispersions be it associated with path constraints or targeted values of final-time conditions. This paper also presents a systematic and rapid process for formulating and computing the most desirable tychastic trajectory using an unscented transform. Numerical examples are used to illustrate how unscented trajectory optimization may be used for risk reduction and mission recovery caused by uncertainties and failures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02753v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>I. M. Ross, R. J. Proulx, M. Karpenko</dc:creator>
    </item>
    <item>
      <title>Limiting Behavior of Maxima under Dependence</title>
      <link>https://arxiv.org/abs/2405.02833</link>
      <description>arXiv:2405.02833v1 Announce Type: cross 
Abstract: Weak convergence of maxima of dependent sequences of identically distributed continuous random variables is studied under normalizing sequences arising as subsequences of the normalizing sequences from an associated iid sequence. This general framework allows one to derive several generalizations of the well-known Fisher-Tippett-Gnedenko theorem under conditions on the univariate marginal distribution and the dependence structure of the sequence. The limiting distributions are shown to be compositions of a generalized extreme value distribution and a distortion function which reflects the limiting behavior of the diagonal of the underlying copula. Uniform convergence rates for the weak convergence to the limiting distribution are also derived. Examples covering well-known dependence structures are provided. Several existing results, e.g. for exchangeable sequences or stationary time series, are embedded in the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02833v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Klaus Herrmann, Marius Hofert, Johanna G. Neslehova</dc:creator>
    </item>
    <item>
      <title>Probabilistic cellular automata with local transition matrices: synchronization, ergodicity, and inference</title>
      <link>https://arxiv.org/abs/2405.02928</link>
      <description>arXiv:2405.02928v1 Announce Type: cross 
Abstract: We introduce a new class of probabilistic cellular automata that are capable of exhibiting rich dynamics such as synchronization and ergodicity and can be easily inferred from data. The system is a finite-state locally interacting Markov chain on a circular graph. Each site's subsequent state is random, with a distribution determined by its neighborhood's empirical distribution multiplied by a local transition matrix. We establish sufficient and necessary conditions on the local transition matrix for synchronization and ergodicity. Also, we introduce novel least squares estimators for inferring the local transition matrix from various types of data, which may consist of either multiple trajectories, a long trajectory, or ensemble sequences without trajectory information. Under suitable identifiability conditions, we show the asymptotic normality of these estimators and provide non-asymptotic bounds for their accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02928v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Erhan Bayrakta, Fei Lu, Mauro Maggioni, Ruoyu Wu, Sichen Yang</dc:creator>
    </item>
    <item>
      <title>Tuning parameter selection in econometrics</title>
      <link>https://arxiv.org/abs/2405.03021</link>
      <description>arXiv:2405.03021v1 Announce Type: cross 
Abstract: I review some of the main methods for selecting tuning parameters in nonparametric and $\ell_1$-penalized estimation. For the nonparametric estimation, I consider the methods of Mallows, Stein, Lepski, cross-validation, penalization, and aggregation in the context of series estimation. For the $\ell_1$-penalized estimation, I consider the methods based on the theory of self-normalized moderate deviations, bootstrap, Stein's unbiased risk estimation, and cross-validation in the context of Lasso estimation. I explain the intuition behind each of the methods and discuss their comparative advantages. I also give some extensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03021v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Denis Chetverikov</dc:creator>
    </item>
    <item>
      <title>Negative Probability</title>
      <link>https://arxiv.org/abs/2405.03043</link>
      <description>arXiv:2405.03043v1 Announce Type: cross 
Abstract: Negative probabilities arise primarily in quantum theory and computing. Bartlett provides a definition based on characteristic functions and extraordinary random variables. As Bartlett observes, negative probabilities must always be combined with positive probabilities to yield a valid probability distribution before any physical interpretation is admissible. Negative probabilities arise as mixing distributions of unobserved latent variables in Bayesian modeling. Our goal is to provide a link with dual densities and the class of scale mixtures of normal distributions. We provide an analysis of the classic half coin distribution and Feynman's negative probability examples. A number of examples of dual densities with negative mixing measures including the linnik distribution, Wigner distribution and the stable distribution are provided. Finally, we conclude with directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03043v1</guid>
      <category>quant-ph</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nick Polson, Vadim Sokolov</dc:creator>
    </item>
    <item>
      <title>Strang Splitting for Parametric Inference in Second-order Stochastic Differential Equations</title>
      <link>https://arxiv.org/abs/2405.03606</link>
      <description>arXiv:2405.03606v1 Announce Type: cross 
Abstract: We address parameter estimation in second-order stochastic differential equations (SDEs), prevalent in physics, biology, and ecology. Second-order SDE is converted to a first-order system by introducing an auxiliary velocity variable raising two main challenges. First, the system is hypoelliptic since the noise affects only the velocity, making the Euler-Maruyama estimator ill-conditioned. To overcome that, we propose an estimator based on the Strang splitting scheme. Second, since the velocity is rarely observed we adjust the estimator for partial observations. We present four estimators for complete and partial observations, using full likelihood or only velocity marginal likelihood. These estimators are intuitive, easy to implement, and computationally fast, and we prove their consistency and asymptotic normality. Our analysis demonstrates that using full likelihood with complete observations reduces the asymptotic variance of the diffusion estimator. With partial observations, the asymptotic variance increases due to information loss but remains unaffected by the likelihood choice. However, a numerical study on the Kramers oscillator reveals that using marginal likelihood for partial observations yields less biased estimators. We apply our approach to paleoclimate data from the Greenland ice core and fit it to the Kramers oscillator model, capturing transitions between metastable states reflecting observed climatic conditions during glacial eras.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03606v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Predrag Pilipovic, Adeline Samson, Susanne Ditlevsen</dc:creator>
    </item>
    <item>
      <title>MARS via LASSO</title>
      <link>https://arxiv.org/abs/2111.11694</link>
      <description>arXiv:2111.11694v4 Announce Type: replace 
Abstract: Multivariate adaptive regression splines (MARS) is a popular method for nonparametric regression introduced by Friedman in 1991. MARS fits simple nonlinear and non-additive functions to regression data. We propose and study a natural lasso variant of the MARS method. Our method is based on least squares estimation over a convex class of functions obtained by considering infinite-dimensional linear combinations of functions in the MARS basis and imposing a variation based complexity constraint. Our estimator can be computed via finite-dimensional convex optimization, although it is defined as a solution to an infinite-dimensional optimization problem. Under a few standard design assumptions, we prove that our estimator achieves a rate of convergence that depends only logarithmically on dimension and thus avoids the usual curse of dimensionality to some extent. We also show that our method is naturally connected to nonparametric estimation techniques based on smoothness constraints. We implement our method with a cross-validation scheme for the selection of the involved tuning parameter and compare it to the usual MARS method in various simulation and real data settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.11694v4</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dohyeong Ki, Billy Fang, Adityanand Guntuboyina</dc:creator>
    </item>
    <item>
      <title>Seeded graph matching for the correlated Gaussian Wigner model via the projected power method</title>
      <link>https://arxiv.org/abs/2204.04099</link>
      <description>arXiv:2204.04099v3 Announce Type: replace 
Abstract: In the \emph{graph matching} problem we observe two graphs $G,H$ and the goal is to find an assignment (or matching) between their vertices such that some measure of edge agreement is maximized. We assume in this work that the observed pair $G,H$ has been drawn from the Correlated Gaussian Wigner (CGW) model -- a popular model for correlated weighted graphs -- where the entries of the adjacency matrices of $G$ and $H$ are independent Gaussians and each edge of $G$ is correlated with one edge of $H$ (determined by the unknown matching) with the edge correlation described by a parameter $\sigma\in [0,1)$. In this paper, we analyse the performance of the \emph{projected power method} (PPM) as a \emph{seeded} graph matching algorithm where we are given an initial partially correct matching (called the seed) as side information. We prove that if the seed is close enough to the ground-truth matching, then with high probability, PPM iteratively improves the seed and recovers the ground-truth matching (either partially or exactly) in $\mathcal{O}(\log n)$ iterations. Our results prove that PPM works even in regimes of constant $\sigma$, thus extending the analysis in (Mao et al. 2023) for the sparse Correlated Erdos-Renyi(CER) model to the (dense) CGW model. As a byproduct of our analysis, we see that the PPM framework generalizes some of the state-of-art algorithms for seeded graph matching. We support and complement our theoretical findings with numerical experiments on synthetic data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.04099v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research 25(5), 1-43, 2024</arxiv:journal_reference>
      <dc:creator>Ernesto Araya, Guillaume Braun, Hemant Tyagi</dc:creator>
    </item>
    <item>
      <title>Reversibility of elliptical slice sampling revisited</title>
      <link>https://arxiv.org/abs/2301.02426</link>
      <description>arXiv:2301.02426v2 Announce Type: replace 
Abstract: We extend elliptical slice sampling, a Markov chain transition kernel suggested in Murray, Adams and MacKay 2010, to infinite-dimensional separable Hilbert spaces and discuss its well-definedness. We point to a regularity requirement, provide an alternative proof of the desirable reversibility property and show that it induces a positive semi-definite Markov operator. Crucial within the proof of the formerly mentioned results is the analysis of a shrinkage Markov chain that may be interesting on its own.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.02426v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mareike Hasenpflug, Viacheslav Telezhnikov, Daniel Rudolf</dc:creator>
    </item>
    <item>
      <title>Adaptive Ridge Approach to Heteroscedastic Regression</title>
      <link>https://arxiv.org/abs/2402.13642</link>
      <description>arXiv:2402.13642v2 Announce Type: replace 
Abstract: We propose an adaptive ridge (AR) based estimation scheme for a heteroscedastic linear model equipped with log-linear errors. We simultaneously estimate the mean and variance parameters and show new asymptotic distributional and tightness properties in a sparse setting. We also show that estimates for zero parameters shrink with more iterations under suitable assumptions for tuning parameters. We observe possible generalizations of this paper's results through simulations and will apply the estimation method in forecasting electricity consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13642v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ka Long Keith Ho, Hiroki Masuda</dc:creator>
    </item>
    <item>
      <title>Finding Super-spreaders in Network Cascades</title>
      <link>https://arxiv.org/abs/2403.03205</link>
      <description>arXiv:2403.03205v3 Announce Type: replace 
Abstract: Suppose that a cascade (e.g., an epidemic) spreads on an unknown graph, and only the infection times of vertices are observed. What can be learned about the graph from the infection times caused by multiple distinct cascades? Most of the literature on this topic focuses on the task of recovering the entire graph, which requires $\Omega ( \log n)$ cascades for an $n$-vertex bounded degree graph. Here we ask a different question: can the important parts of the graph be estimated from just a few (i.e., constant number) of cascades, even as $n$ grows large?
  In this work, we focus on identifying super-spreaders (i.e., high-degree vertices) from infection times caused by a Susceptible-Infected process on a graph. Our first main result shows that vertices of degree greater than $n^{3/4}$ can indeed be estimated from a constant number of cascades. Our algorithm for doing so leverages a novel connection between vertex degrees and the second derivative of the cumulative infection curve. Conversely, we show that estimating vertices of degree smaller than $n^{1/2}$ requires at least $\log(n) / \log \log (n)$ cascades. Surprisingly, this matches (up to $\log \log n$ factors) the number of cascades needed to learn the \emph{entire} graph if it is a tree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03205v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.SI</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elchanan Mossel, Anirudh Sridhar</dc:creator>
    </item>
    <item>
      <title>Causal Inference with High-dimensional Discrete Covariates</title>
      <link>https://arxiv.org/abs/2405.00118</link>
      <description>arXiv:2405.00118v2 Announce Type: replace 
Abstract: When estimating causal effects from observational studies, researchers often need to adjust for many covariates to deconfound the non-causal relationship between exposure and outcome, among which many covariates are discrete. The behavior of commonly used estimators in the presence of many discrete covariates is not well understood since their properties are often analyzed under structural assumptions including sparsity and smoothness, which do not apply in discrete settings. In this work, we study the estimation of causal effects in a model where the covariates required for confounding adjustment are discrete but high-dimensional, meaning the number of categories $d$ is comparable with or even larger than sample size $n$. Specifically, we show the mean squared error of commonly used regression, weighting and doubly robust estimators is bounded by $\frac{d^2}{n^2}+\frac{1}{n}$. We then prove the minimax lower bound for the average treatment effect is of order $\frac{d^2}{n^2 \log^2 n}+\frac{1}{n}$, which characterizes the fundamental difficulty of causal effect estimation in the high-dimensional discrete setting, and shows the estimators mentioned above are rate-optimal up to log-factors. We further consider additional structures that can be exploited, namely effect homogeneity and prior knowledge of the covariate distribution, and propose new estimators that enjoy faster convergence rates of order $\frac{d}{n^2} + \frac{1}{n}$, which achieve consistency in a broader regime. The results are illustrated empirically via simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00118v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenghao Zeng, Sivaraman Balakrishnan, Yanjun Han, Edward H. Kennedy</dc:creator>
    </item>
    <item>
      <title>On Linear Separation Capacity of Self-Supervised Representation Learning</title>
      <link>https://arxiv.org/abs/2310.19041</link>
      <description>arXiv:2310.19041v2 Announce Type: replace-cross 
Abstract: Recent advances in self-supervised learning have highlighted the efficacy of data augmentation in learning data representation from unlabeled data. Training a linear model atop these enhanced representations can yield an adept classifier. Despite the remarkable empirical performance, the underlying mechanisms that enable data augmentation to unravel nonlinear data structures into linearly separable representations remain elusive. This paper seeks to bridge this gap by investigating under what conditions learned representations can linearly separate manifolds when data is drawn from a multi-manifold model. Our investigation reveals that data augmentation offers additional information beyond observed data and can thus improve the information-theoretic optimal rate of linear separation capacity. In particular, we show that self-supervised learning can linearly separate manifolds with a smaller distance than unsupervised learning, underscoring the additional benefits of data augmentation. Our theoretical analysis further underscores that the performance of downstream linear classifiers primarily hinges on the linear separability of data representations rather than the size of the labeled data set, reaffirming the viability of constructing efficient classifiers with limited labeled data amid an expansive unlabeled data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19041v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shulei Wang</dc:creator>
    </item>
    <item>
      <title>Estimating Counterfactual Matrix Means with Short Panel Data</title>
      <link>https://arxiv.org/abs/2312.07520</link>
      <description>arXiv:2312.07520v2 Announce Type: replace-cross 
Abstract: We develop a new, spectral approach for identifying and estimating average counterfactual outcomes under a low-rank factor model with short panel data and general outcome missingness patterns. Applications include event studies and studies of outcomes of "matches" between agents of two types, e.g. workers and firms, typically conducted under less-flexible Two-Way-Fixed-Effects (TWFE) models of outcomes. Given an infinite population of units and a finite number of outcomes, we show our approach identifies all counterfactual outcome means, including those not estimable by existing methods, if a particular graph constructed based on overlaps in observed outcomes between subpopulations is connected. Our analogous, computationally efficient estimation procedure yields consistent, asymptotically normal estimates of counterfactual outcome means under fixed-$T$ (number of outcomes), large-$N$ (sample size) asymptotics. In a semi-synthetic simulation study based on matched employer-employee data, our estimator has lower bias and only slightly higher variance than a TWFE-model-based estimator when estimating average log-wages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07520v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lihua Lei, Brad Ross</dc:creator>
    </item>
  </channel>
</rss>
