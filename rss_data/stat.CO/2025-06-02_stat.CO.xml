<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Jun 2025 03:03:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An MCMC hypothesis test to check a claimed sampler: applied to a claimed sampler for the G-Wishart distribution</title>
      <link>https://arxiv.org/abs/2505.24400</link>
      <description>arXiv:2505.24400v1 Announce Type: new 
Abstract: Suppose we have a distribution of interest, with density $p(x),x\in {\cal X}$ say, and an algorithm
  claimed to generate samples from $p(x)$. Moreover, assume we have available a Metropolis--Hastings
  transition kernel fulfilling detail balance with respect to $p(x)$. In such a situation we
  formulate a hypothesis test where $H_0$ is that the claimed sampler really generates
  correct samples from $p(x)$. We use that if initialising the Metropolis--Hastings algorithm with a
  sample generated by the claimed sampler and run the chain for a fixed number of updates, the
  initial and final states are exchangeable if $H_0$ is true. Combining this idea with the
  permutation strategy we define a natural test statistic and a valid p-value.
  Our motivation for considering the hypothesis test situation is a proposed sampler in the
  literature, claimed to generate samples from G-Wishart distribution. As no proper
  proof for the validity of this sampler seems to be available, we are exactly in the hypothesis
  test situation discussed above. We therefore apply the defined hypothesis test to the
  claimed sampler. For comparison we also apply the hypothesis test to a known exact sampler
  for a subset of G-Wishart distributions. The obtained p-values clearly show that
  the sampler claimed to be able to generate samples from any G-Wishart distribution is
  in fact not sampling from the specified distribution. In contrast, and as one should expect,
  the p-values obtained when using the known exact algorithm does not indicate any problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24400v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H{\aa}kon Tjelmeland, Hanna Bu Kval{\o}y</dc:creator>
    </item>
    <item>
      <title>Alternate Groundwater Modelling Strategies: A Multi-Faceted Data-Driven Approach</title>
      <link>https://arxiv.org/abs/2505.24235</link>
      <description>arXiv:2505.24235v1 Announce Type: cross 
Abstract: The impact of statistical methodologies on studying groundwater has been significant in the last several decades, due to cheaper computational abilities and presence of technologies that enable us to extract and measure more and more data. This paper focuses on the validation of statistical methodologies that are in practice and continue to be at the earliest disposal of the researcher, demonstrating how traditional time-series models and modern neural networks may be a viable option to analyze and make viable forecasts from data commonly available in this domain, and suggesting a copula-based strategy to obtain directional dependencies of groundwater level, spatially. This paper also proposes a sphere of model validation, seldom addressed in this domain: the model longevity or the model shelf-life. Use of such validation techniques not only ensure lower computational cost while maintaining reasonably high accuracy, but also, in some cases, ensure robust predictions or forecasts, and assist in comparing multiple models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24235v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muralidharan K., Agniva Das, Shrey Pandya, Jong Min Kim</dc:creator>
    </item>
    <item>
      <title>Multi-task Learning for Heterogeneous Multi-source Block-Wise Missing Data</title>
      <link>https://arxiv.org/abs/2505.24413</link>
      <description>arXiv:2505.24413v1 Announce Type: cross 
Abstract: Multi-task learning (MTL) has emerged as an imperative machine learning tool to solve multiple learning tasks simultaneously and has been successfully applied to healthcare, marketing, and biomedical fields. However, in order to borrow information across different tasks effectively, it is essential to utilize both homogeneous and heterogeneous information. Among the extensive literature on MTL, various forms of heterogeneity are presented in MTL problems, such as block-wise, distribution, and posterior heterogeneity. Existing methods, however, struggle to tackle these forms of heterogeneity simultaneously in a unified framework. In this paper, we propose a two-step learning strategy for MTL which addresses the aforementioned heterogeneity. First, we impute the missing blocks using shared representations extracted from homogeneous source across different tasks. Next, we disentangle the mappings between input features and responses into a shared component and a task-specific component, respectively, thereby enabling information borrowing through the shared component. Our numerical experiments and real-data analysis from the ADNI database demonstrate the superior MTL performance of the proposed method compared to other competing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24413v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Sui, Qi Xu, Yang Bai, Annie Qu</dc:creator>
    </item>
    <item>
      <title>Mixing time of the conditional backward sampling particle filter</title>
      <link>https://arxiv.org/abs/2312.17572</link>
      <description>arXiv:2312.17572v3 Announce Type: replace 
Abstract: The conditional backward sampling particle filter (CBPF) is a powerful Markov chain Monte Carlo sampler for general state space hidden Markov model (HMM) smoothing. It was proposed as an improvement over the conditional particle filter (CPF), which is known to have an $O(T^2)$ computational time complexity under a general `strong' mixing assumption, where $T$ is the time horizon. While there is empirical evidence of the superiority of the CBPF over the CPF in practice, this has never been theoretically quantified. We show that the CBPF has $O(T \log T)$ time complexity under strong mixing. In particular, the CBPF's mixing time is upper bounded by $O(\log T)$, for any sufficiently large number of particles $N$ that depends only on the mixing assumptions and not $T$. We also show that an $O(\log T)$ mixing time is optimal. To prove our main result, we introduce a novel coupling of two CBPFs, which employs a maximal coupling of two particle systems at each time instant. As the coupling is implementable, it thus has practical applications. We use it to construct unbiased, finite variance, estimates of functionals which have arbitrary dependence on the latent state's path, with a total expected cost of $O(T \log T)$. As the specific application to real-data analysis, we construct unbiased estimates of the HMM's score function, leading to stochastic gradient maximum likelihood estimation of a financial time-series model. Finally, we also investigate other couplings and show that some of these alternatives can have improved empirical behaviour.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17572v3</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joona Karjalainen, Anthony Lee, Sumeetpal S. Singh, Matti Vihola</dc:creator>
    </item>
    <item>
      <title>collapse: Advanced and Fast Statistical Computing and Data Transformation in R</title>
      <link>https://arxiv.org/abs/2403.05038</link>
      <description>arXiv:2403.05038v5 Announce Type: replace 
Abstract: collapse is a large C/C++-based infrastructure package facilitating complex statistical computing, data transformation, and exploration tasks in R - at outstanding levels of performance and memory efficiency. It also implements a class-agnostic approach to R programming, supporting vector, matrix and data frame-like objects and their popular extensions ('units', 'integer64', 'xts', 'tibble', 'data.table', 'sf', 'pdata.frame'), enabling its seamless integration with the bulk of the R ecosystem. This article introduces the package's key components and design principles in a structured way, supported by a rich set of examples. A small benchmark demonstrates its computational performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05038v5</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Krantz</dc:creator>
    </item>
    <item>
      <title>Dimension reduction via score ratio matching</title>
      <link>https://arxiv.org/abs/2410.19990</link>
      <description>arXiv:2410.19990v2 Announce Type: replace 
Abstract: Gradient-based dimension reduction decreases the cost of Bayesian inference and probabilistic modeling by identifying maximally informative (and informed) low-dimensional projections of the data and parameters, allowing high-dimensional problems to be reformulated as cheaper low-dimensional problems. A broad family of such techniques identify these projections and provide error bounds on the resulting posterior approximations, via eigendecompositions of certain diagnostic matrices. Yet these matrices require gradients or even Hessians of the log-likelihood, excluding the purely data-driven setting and many problems of simulation-based inference. We propose a framework, derived from score-matching, to extend gradient-based dimension reduction to problems where gradients are unavailable. Specifically, we formulate an objective function to directly learn the score ratio function needed to compute the diagnostic matrices, propose a tailored parameterization for the score ratio network, and introduce regularization methods that capitalize on the hypothesized low-dimensional structure. We also introduce a novel algorithm to iteratively identify the low-dimensional reduced basis vectors more accurately with limited data based on eigenvalue deflation methods. We show that our approach outperforms standard score-matching for problems with low-dimensional structure, and demonstrate its effectiveness for PDE-constrained Bayesian inverse problems and conditional generative modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19990v2</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ricardo Baptista, Michael Brennan, Youssef Marzouk</dc:creator>
    </item>
    <item>
      <title>A Dirichlet stochastic block model for composition-weighted networks</title>
      <link>https://arxiv.org/abs/2408.00651</link>
      <description>arXiv:2408.00651v2 Announce Type: replace-cross 
Abstract: Network data are observed in various applications where the individual entities of the system interact with or are connected to each other, and often these interactions are defined by their associated strength or importance. Clustering is a common task in network analysis that involves finding groups of nodes displaying similarities in the way they interact with the rest of the network. However, most clustering methods use the strengths of connections between entities in their original form, ignoring the possible differences in the capacities of individual nodes to send or receive edges. This often leads to clustering solutions that are heavily influenced by the nodes' capacities. One way to overcome this is to analyse the strengths of connections in relative rather than absolute terms, expressing each edge weight as a proportion of the sending (or receiving) capacity of the respective node. This, however, induces additional modelling constraints that most existing clustering methods are not designed to handle. In this work we propose a stochastic block model for composition-weighted networks based on direct modelling of compositional weight vectors using a Dirichlet mixture, with the parameters determined by the cluster labels of the sender and the receiver nodes. Inference is implemented via an extension of the classification expectation-maximisation algorithm that uses a working independence assumption, expressing the complete data likelihood of each node of the network as a function of fixed cluster labels of the remaining nodes. A model selection criterion is derived to aid the choice of the number of clusters. The model is validated using simulation studies, and showcased on network data from the Erasmus exchange program and a bike sharing network for the city of London.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00651v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 02 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iuliia Promskaia, Adrian O'Hagan, Michael Fop</dc:creator>
    </item>
  </channel>
</rss>
