<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 09 Jan 2025 05:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A cautious use of auxiliary outcomes for decision-making in randomized clinical trials</title>
      <link>https://arxiv.org/abs/2501.04187</link>
      <description>arXiv:2501.04187v1 Announce Type: new 
Abstract: Clinical trials often collect data on multiple outcomes, such as overall survival (OS), progression-free survival (PFS), and response to treatment (RT). In most cases, however, study designs only use primary outcome data for interim and final decision-making. In several disease settings, clinically relevant outcomes, for example OS, become available years after patient enrollment. Moreover, the effects of experimental treatments on OS might be less pronounced compared to auxiliary outcomes such as RT. We develop a Bayesian decision-theoretic framework that uses both primary and auxiliary outcomes for interim and final decision-making. The framework allows investigators to control standard frequentist operating characteristics, such as the type I error rate, and can be used with auxiliary outcomes from emerging technologies, such as circulating tumor assays. False positive rates and other frequentist operating characteristics are rigorously controlled without any assumption about the concordance between primary and auxiliary outcomes. We discuss algorithms to implement this decision-theoretic approach and show that incorporating auxiliary information into interim and final decision-making can lead to relevant efficiency gains according to established and interpretable metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04187v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Massimiliano Russo, Steffen Ventz, Lorenzo Trippa</dc:creator>
    </item>
    <item>
      <title>Mediation analysis in longitudinal intervention studies with an ordinal treatment-dependent confounder</title>
      <link>https://arxiv.org/abs/2501.04581</link>
      <description>arXiv:2501.04581v1 Announce Type: new 
Abstract: In interventional health studies, causal mediation analysis can be employed to investigate mechanisms through which the intervention affects the targeted health outcome. Identifying direct and indirect (i.e. mediated) effects from empirical data, however, becomes complicated if the mediator-outcome association is confounded by a variable itself affected by the treatment. Here, we investigate identification of mediational effects under such post-treatment confounding in a setting with a longitudinal mediator, time-to-event outcome and a trichotomous ordinal treatment-dependent confounder. We show that if the intervention always affects the treatment-dependent confounder only in one direction (monotonicity), the mediational effects are identified up to a sensitivity parameter and derive their empirical non-parametric expressions. The monotonicity assumption can be assessed from empirical data, based on restrictions on the conditional distribution of the treatment-dependent confounder. We avoid pitfalls related to post-treatment conditioning by treating the mediator as a functional entity and defining the time-to-event outcome as a restricted disease-free time. In an empirical analysis, we use data from the Finnish Diabetes Prevention Study to assess the extent to which the effect of a lifestyle intervention on avoiding type 2 diabetes is mediated through weight reduction in a high-risk population, with other health-related changes acting as treatment-dependent confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04581v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikko Valtanen, Tommi H\"ark\"anen, Matti Uusitupa, Jaakko Tuomilehto, Jaana Lindstr\"om, Kari Auranen</dc:creator>
    </item>
    <item>
      <title>MERCURY: A fast and versatile multi-resolution based global emulator of compound climate hazards</title>
      <link>https://arxiv.org/abs/2501.04018</link>
      <description>arXiv:2501.04018v1 Announce Type: cross 
Abstract: High-impact climate damages are often driven by compounding climate conditions. For example, elevated heat stress conditions can arise from a combination of high humidity and temperature. To explore future changes in compounding hazards under a range of climate scenarios and with large ensembles, climate emulators can provide light-weight, data-driven complements to Earth System Models. Yet, only a few existing emulators can jointly emulate multiple climate variables. In this study, we present the Multi-resolution EmulatoR for CompoUnd climate Risk analYsis: MERCURY. MERCURY extends multi-resolution analysis to a spatio-temporal framework for versatile emulation of multiple variables. MERCURY leverages data-driven, image compression techniques to generate emulations in a memory-efficient manner. MERCURY consists of a regional component that represents the monthly, regional response of a given variable to yearly Global Mean Temperature (GMT) using a probabilistic regression based additive model, resolving regional cross-correlations. It then adapts a reverse lifting-scheme operator to jointly spatially disaggregate regional, monthly values to grid-cell level. We demonstrate MERCURY's capabilities on representing the humid-heat metric, Wet Bulb Globe Temperature, as derived from temperature and relative humidity emulations. The emulated WBGT spatial correlations correspond well to those of ESMs and the 95% and 97.5% quantiles of WBGT distributions are well captured, with an average of 5% deviation. MERCURY's setup allows for region-specific emulations from which one can efficiently "zoom" into the grid-cell level across multiple variables by means of the reverse lifting-scheme operator. This circumvents the traditional problem of having to emulate complete, global-fields of climate data and resulting storage requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04018v1</guid>
      <category>physics.ao-ph</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shruti Nath, Julie Carreau, Kai Kornhuber, Peter Pfleiderer, Carl-Friedrich Schleussner, Philippe Naveau</dc:creator>
    </item>
    <item>
      <title>Revisiting The Cosmological Time Dilation of Distant Quasars: Influence of Source Properties and Evolution</title>
      <link>https://arxiv.org/abs/2501.04171</link>
      <description>arXiv:2501.04171v1 Announce Type: cross 
Abstract: After decades of searching, cosmological time dilation was recently identified in the timescale of variability seen in distant quasars. Here, we expand on the previous analysis to disentangle this cosmological signal from the influence of the properties of the source population, specifically the quasar bolometric luminosity and the rest-frame emission wavelength at which the variability was observed. Furthermore, we consider the potential influence of the evolution of the quasar population over cosmic time. We find that a significant intrinsic scatter of 0.288 +- 0.021 dex in the variability timescales, which was not considered in the previous analysis, is favoured by the data. This slightly increases the uncertainty in the results. However, the expected cosmological dependence of the variability timescales is confirmed to be robust to changes in the underlying assumptions. We find that the variability timescales increase smoothly with both wavelength and bolometric luminosity, and that black hole mass has no effect on the variability timescale once rest wavelength and bolometric luminosity are accounted for. Moreover, if the standard cosmological model is correct, governed by relativistic expansion, we also find very little cosmological evolution in the intrinsic variability timescales of distant quasars.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04171v1</guid>
      <category>astro-ph.CO</category>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brendon J. Brewer (Cher), Geraint F. Lewis (Cher),  Yuan (Cher),  Li</dc:creator>
    </item>
    <item>
      <title>Statistical Uncertainty Quantification for Aggregate Performance Metrics in Machine Learning Benchmarks</title>
      <link>https://arxiv.org/abs/2501.04234</link>
      <description>arXiv:2501.04234v1 Announce Type: cross 
Abstract: Modern artificial intelligence is supported by machine learning models (e.g., foundation models) that are pretrained on a massive data corpus and then adapted to solve a variety of downstream tasks. To summarize performance across multiple tasks, evaluation metrics are often aggregated into a summary metric, e.g., average accuracy across 10 question-answering tasks. When aggregating evaluation metrics, it is useful to incorporate uncertainty in the aggregate metric in order to gain a more realistic understanding of model performance. Our objective in this work is to demonstrate how statistical methodology can be used for quantifying uncertainty in metrics that have been aggregated across multiple tasks. The methods we emphasize are bootstrapping, Bayesian hierarchical (i.e., multilevel) modeling, and the visualization of task weightings that consider standard errors. These techniques reveal insights such as the dominance of a specific model for certain types of tasks despite an overall poor performance. We use a popular ML benchmark, the Visual Task Adaptation Benchmark (VTAB), to demonstrate the usefulness of our approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04234v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rachel Longjohn, Giri Gopalan, Emily Casleton</dc:creator>
    </item>
    <item>
      <title>Spherical Double K-Means: a co-clustering approach for text data analysis</title>
      <link>https://arxiv.org/abs/2501.04562</link>
      <description>arXiv:2501.04562v1 Announce Type: cross 
Abstract: In this study, we introduce the Spherical Double K-Means (SDKM) clustering method for text data. A novel approach for simultaneous clustering of terms and documents. Using the strengths of k-means, double k-means, and spherical k-means, SDKM addresses the challenges of high dimensionality, noise, and sparsity inherent in text analysis. We address the choice of the number of clusters, both for the words and documents, using the cluster validity index pseudo-F, and verify the reliability of the method through simulation studies.
  We apply SDKM to the corpus of US presidential inaugural addresses, spanning from George Washington in 1789 to Joe Biden in 2021. Our analysis reveals distinct clusters of words and documents that correspond to significant historical themes and periods, showcasing the method's ability to facilitate a deeper understanding of the data. Our findings demonstrate the efficacy of SDKM in uncovering underlying patterns in textual data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04562v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emiliano Seri, Ilaria Bombelli, Domenica Fioredistella Iezzi, Maurizio Vichi</dc:creator>
    </item>
    <item>
      <title>A Direct Importance Sampling-based Framework for Rare Event Uncertainty Quantification in Non-Gaussian Spaces</title>
      <link>https://arxiv.org/abs/2405.14149</link>
      <description>arXiv:2405.14149v2 Announce Type: replace-cross 
Abstract: This work introduces a novel framework for precisely and efficiently estimating rare event probabilities in complex, high-dimensional non-Gaussian spaces, building on our foundational Approximate Sampling Target with Post-processing Adjustment (ASTPA) approach. An unnormalized sampling target is first constructed and sampled, relaxing the optimal importance sampling distribution and appropriately designed for non-Gaussian spaces. Post-sampling, its normalizing constant is estimated using a stable inverse importance sampling procedure, employing an importance sampling density based on the already available samples. The sought probability is then computed based on the estimates evaluated in these two stages. The proposed estimator is theoretically analyzed, proving its unbiasedness and deriving its analytical coefficient of variation. To sample the constructed target, we resort to our developed Quasi-Newton mass preconditioned Hamiltonian MCMC (QNp-HMCMC) and we prove that it converges to the correct stationary target distribution. To avoid the challenging task of tuning the trajectory length in complex spaces, QNp-HMCMC is effectively utilized in this work with a single-step integration. We thus show the equivalence of QNp-HMCMC with single-step implementation to a unique and efficient preconditioned Metropolis-adjusted Langevin algorithm (MALA). An optimization approach is also leveraged to initiate QNp-HMCMC effectively, and the implementation of the developed framework in bounded spaces is eventually discussed. A series of diverse problems involving high dimensionality (several hundred inputs), strong nonlinearity, and non-Gaussianity is presented, showcasing the capabilities and efficiency of the suggested framework and demonstrating its advantages compared to relevant state-of-the-art sampling methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14149v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elsayed Eshra, Konstantinos G. Papakonstantinou, Hamed Nikbakht</dc:creator>
    </item>
    <item>
      <title>Parrondo's effects with aperiodic protocols</title>
      <link>https://arxiv.org/abs/2410.02987</link>
      <description>arXiv:2410.02987v2 Announce Type: replace-cross 
Abstract: In this work, we study the effectiveness of employing archetypal aperiodic sequencing -- namely Fibonacci, Thue-Morse, and Rudin-Shapiro -- on the Parrondian effect. From a capital gain perspective, our results show that these series do yield a Parrondo's Paradox with the Thue-Morse based strategy outperforming not only the other two aperiodic strategies but benchmark Parrondian games with random and periodical ($AABBAABB\ldots$) switching as well. The least performing of the three aperiodic strategies is the Rudin-Shapiro. To elucidate the underlying causes of these results, we analyze the cross-correlation between the capital generated by the switching protocols and that of the isolated losing games. This analysis reveals that a strong anticorrelation with both isolated games is typically required to achieve a robust manifestation of Parrondo's effect. We also study the influence of the sequencing on the capital using the lacunarity and persistence measures. In general, we observe that the switching protocols tend to become less performing in terms of the capital as one increases the persistence and thus approaches the features of an isolated losing game. For the (log-)lacunarity, a property related to heterogeneity, we notice that for small persistence (less than 0.5) the performance increases with the lacunarity with a maximum around 0.4. In respect of this, our work shows that the optimization of a switching protocol is strongly dependent on a fine-tuning between persistence and heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02987v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.GT</category>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1063/5.0233604</arxiv:DOI>
      <arxiv:journal_reference>Chaos: An Interdisciplinary Journal of Nonlinear Science, 2024, 34 (12): 123126</arxiv:journal_reference>
      <dc:creator>Marcelo A. Pires, Erveton P. Pinto, Rone N. da Silva, S\'ilvio M. Duarte Queir\'os</dc:creator>
    </item>
  </channel>
</rss>
