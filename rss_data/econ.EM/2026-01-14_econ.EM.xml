<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 15 Jan 2026 02:49:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Utility-Weighted Forecasting and Calibration for Risk-Adjusted Decisions under Trading Frictions</title>
      <link>https://arxiv.org/abs/2601.07852</link>
      <description>arXiv:2601.07852v1 Announce Type: new 
Abstract: Forecasting accuracy is routinely optimised in financial prediction tasks even though investment and risk-management decisions are executed under transaction costs, market impact, capacity limits, and binding risk constraints. This paper treats forecasting as an econometric input to a constrained decision problem. A predictive distribution induces a decision rule through a utility objective combined with an explicit friction operator consisting of both a cost functional and a feasible-set constraint system. The econometric target becomes minimisation of expected decision loss net of costs rather than minimisation of prediction error. The paper develops a utility-weighted calibration criterion aligned to the decision loss and establishes sufficient conditions under which calibrated predictive distributions weakly dominate uncalibrated alternatives. An empirical study using a pre-committed nested walk-forward protocol on liquid equity index futures confirms the theory: the proposed utility-weighted calibration reduces realised decision loss by over 30\% relative to an uncalibrated baseline ($t$-stat -30.31) for loss differential and improves the Sharpe ratio from -3.62 to -2.29 during a drawdown regime. The mechanism is identified as a structural reduction in the frequency of binding constraints (from 16.0\% to 5.1\%), preventing the "corner solution" failures that characterize overconfident forecasts in high-friction environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07852v1</guid>
      <category>econ.EM</category>
      <category>q-fin.CP</category>
      <category>q-fin.PM</category>
      <category>q-fin.TR</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Craig S Wright</dc:creator>
    </item>
    <item>
      <title>Fake Date Tests: Can We Trust In-sample Accuracy of LLMs in Macroeconomic Forecasting?</title>
      <link>https://arxiv.org/abs/2601.07992</link>
      <description>arXiv:2601.07992v1 Announce Type: new 
Abstract: Large language models (LLMs) are a type of machine learning tool that economists have started to apply in their empirical research. One such application is macroeconomic forecasting with backtesting of LLMs, even though they are trained on the same data that is used to estimate their forecasting performance. Can these in-sample accuracy results be extrapolated to the model's out-of-sample performance? To answer this question, we developed a family of prompt sensitivity tests and two members of this family, which we call the fake date tests. These tests aim to detect two types of biases in LLMs' in-sample forecasts: lookahead bias and context bias. According to the empirical results, none of the modern LLMs tested in this study passed our first test, signaling the presence of lookahead bias in their in-sample forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07992v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Eliseev, Sergei Seleznev</dc:creator>
    </item>
    <item>
      <title>Estimating Treatment Effects in Panel Data Without Parallel Trends</title>
      <link>https://arxiv.org/abs/2601.08281</link>
      <description>arXiv:2601.08281v1 Announce Type: new 
Abstract: This paper proposes a novel approach for estimating treatment effects in panel data settings, addressing key limitations of the standard difference-in-differences (DID) approach. The standard approach relies on the parallel trends assumption, implicitly requiring that unobservable factors correlated with treatment assignment be unidimensional, time-invariant, and affect untreated potential outcomes in an additively separable manner. This paper introduces a more flexible framework that allows for multidimensional unobservables and non-additive separability, and provides sufficient conditions for identifying the average treatment effect on the treated. An empirical application to job displacement reveals substantially smaller long-run earnings losses compared to the standard DID approach, demonstrating the framework's ability to account for unobserved heterogeneity that manifests as differential outcome trajectories between treated and control groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08281v1</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shoya Ishimaru</dc:creator>
    </item>
    <item>
      <title>Systemic Risk Surveillance</title>
      <link>https://arxiv.org/abs/2601.08598</link>
      <description>arXiv:2601.08598v1 Announce Type: new 
Abstract: Following several episodes of financial market turmoil in recent decades, changes in systemic risk have drawn growing attention. Therefore, we propose surveillance schemes for systemic risk, which allow to detect misspecified systemic risk forecasts in an "online" fashion. This enables daily monitoring of the forecasts while controlling for the accumulation of false test rejections. Such online schemes are vital in taking timely countermeasures to avoid financial distress. Our monitoring procedures allow multiple series at once to be monitored, thus increasing the likelihood and the speed at which early signs of trouble may be picked up. The tests hold size by construction, such that the null of correct systemic risk assessments is only rejected during the monitoring period with (at most) a pre-specified probability. Monte Carlo simulations illustrate the good finite-sample properties of our procedures. An empirical application to US banks during multiple crises demonstrates the usefulness of our surveillance schemes for both regulators and financial institutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08598v1</guid>
      <category>econ.EM</category>
      <category>q-fin.RM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timo Dimitriadis, Yannick Hoga</dc:creator>
    </item>
    <item>
      <title>Automatic debiased machine learning and sensitivity analysis for sample selection models</title>
      <link>https://arxiv.org/abs/2601.08643</link>
      <description>arXiv:2601.08643v1 Announce Type: new 
Abstract: In this paper, we extend the Riesz representation framework to causal inference under sample selection, where both treatment assignment and outcome observability are non-random. Formulating the problem in terms of a Riesz representer enables stable estimation and a transparent decomposition of omitted variable bias into three interpretable components: a data-identified scale factor, outcome confounding strength, and selection confounding strength. For estimation, we employ the ForestRiesz estimator, which accounts for selective outcome observability while avoiding the instability associated with direct propensity score inversion. We assess finite-sample performance through a simulation study and show that conventional double machine learning approaches can be highly sensitive to tuning parameters due to their reliance on inverse probability weighting, whereas the ForestRiesz estimator delivers more stable performance by leveraging automatic debiased machine learning. In an empirical application to the gender wage gap in the U.S., we find that our ForestRiesz approach yields larger treatment effect estimates than a standard double machine learning approach, suggesting that ignoring sample selection leads to an underestimation of the gender wage gap. Sensitivity analysis indicates that implausibly strong unobserved confounding would be required to overturn our results. Overall, our approach provides a unified, robust, and computationally attractive framework for causal inference under sample selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08643v1</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakob Bjelac, Victor Chernozhukov, Phil-Adrian Klotz, Jannis Kueck, Theresa M. A. Schmitz</dc:creator>
    </item>
    <item>
      <title>Incorporating Cognitive Biases into Reinforcement Learning for Financial Decision-Making</title>
      <link>https://arxiv.org/abs/2601.08247</link>
      <description>arXiv:2601.08247v1 Announce Type: cross 
Abstract: Financial markets are influenced by human behavior that deviates from rationality due to cognitive biases. Traditional reinforcement learning (RL) models for financial decision-making assume rational agents, potentially overlooking the impact of psychological factors. This study integrates cognitive biases into RL frameworks for financial trading, hypothesizing that such models can exhibit human-like trading behavior and achieve better risk-adjusted returns than standard RL agents. We introduce biases, such as overconfidence and loss aversion, into reward structures and decision-making processes and evaluate their performance in simulated and real-world trading environments. Despite its inconclusive or negative results, this study provides insights into the challenges of incorporating human-like biases into RL, offering valuable lessons for developing robust financial AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08247v1</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liu He</dc:creator>
    </item>
    <item>
      <title>A Blessing in Disguise: How DeFi Hacks Trigger Unintended Liquidity Injections into US Money Markets</title>
      <link>https://arxiv.org/abs/2601.08263</link>
      <description>arXiv:2601.08263v1 Announce Type: cross 
Abstract: Do vulnerabilities in Decentralized Finance (DeFi) destabilize traditional short-term funding markets? While the prevailing "Contagion Hypothesis" posits that the liquidation of stablecoin reserves triggers fire-sale spirals that transmit distress to traditional markets , we document a robust "Flight-to-Quality" effect to the contrary. In the wake of major DeFi exploits, spreads on 3-month AA-rated commercial paper (CP) exhibit a paradoxical narrowing. We identify a "liquidity recycling" mechanism driving this outcome: capital fleeing DeFi protocols is re-intermediated into the traditional financial system via Prime Money Market Funds (MMFs) , where strict regulatory constraints (e.g., SEC Rule 2a-7) compel these funds to purchase high-quality paper. Our estimates indicate that this institutional demand shock quantitatively overwhelms the supply shock driven by stablecoin issuer redemptions. Rather than acting as vectors of financial contagion , these crypto native shocks serve as an inadvertent "safety valve" in segmented markets , providing transient liquidity support and effectively subsidizing borrowing costs for high-grade issuers in the real economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.08263v1</guid>
      <category>q-fin.GN</category>
      <category>econ.EM</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tingyi Lin</dc:creator>
    </item>
    <item>
      <title>Double Robustness of Local Projections and Some Unpleasant VARithmetic</title>
      <link>https://arxiv.org/abs/2405.09509</link>
      <description>arXiv:2405.09509v4 Announce Type: replace 
Abstract: We consider impulse response inference in a locally misspecified vector autoregression (VAR) model. The conventional local projection (LP) confidence interval has correct coverage even when the misspecification is so large that it can be detected with probability approaching 1. This result follows from a "double robustness" property analogous to that of popular partially linear regression estimators. By contrast, the conventional VAR confidence interval with short-to-moderate lag length can severely undercover for misspecification that is small, difficult to detect statistically, and cannot be ruled out based on economic theory. The VAR confidence interval has robust coverage if, and only if, the lag length is so large that the interval is as wide as the LP interval.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09509v4</guid>
      <category>econ.EM</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jos\'e Luis Montiel Olea, Mikkel Plagborg-M{\o}ller, Eric Qian, Christian K. Wolf</dc:creator>
    </item>
    <item>
      <title>A Hybrid Framework Combining Autoregression and Common Factors for Matrix Time Series</title>
      <link>https://arxiv.org/abs/2503.05340</link>
      <description>arXiv:2503.05340v3 Announce Type: replace-cross 
Abstract: Matrix-valued time series are ubiquitous in modern economics and finance, yet modeling them requires navigating a trade-off between flexibility and parsimony. We propose the Matrix Autoregressive model with Common Factors (MARCF), a unified framework for high-dimensional matrix time series that bridges the structural gap between the Matrix Autoregression (MAR) and Matrix Factor Model (MFM). While MAR typically assumes distinct predictor and response subspaces and MFM enforces identical ones, MARCF explicitly characterizes the intersection of these subspaces. By decomposing the coefficient matrices into common, predictor-specific, and response-specific components, the framework accommodates distinct input and output structures while exploiting their overlap for dimension reduction. We develop a regularized gradient descent estimator that is scalable for high-dimensional data and can efficiently handle the non-convex parameter space. Theoretical analysis establishes local linear convergence of the algorithm and statistical consistency of the estimator under high-dimensional scaling. The estimation efficiency and interpretability of the proposed methods are demonstrated through simulations and an application to global macroeconomic forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05340v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyun Fan, Xiaoyu Zhang, Di Wang</dc:creator>
    </item>
    <item>
      <title>Bayesian Smoothed Quantile Regression</title>
      <link>https://arxiv.org/abs/2508.01738</link>
      <description>arXiv:2508.01738v4 Announce Type: replace-cross 
Abstract: The standard asymmetric Laplace framework for Bayesian quantile regression (BQR) suffers from a fundamental decision-theoretic misalignment, yielding biased finite-sample estimates, and precludes gradient-based computation due to non-smoothness. We propose Bayesian smoothed quantile regression (BSQR), a principled framework built on a kernel-smoothed, fully differentiable likelihood. Methodologically, the symmetrizing property of our objective reduces inferential bias and aligns the posterior mean with the true conditional quantile. Theoretically, we establish posterior consistency and a Bernstein--von Mises theorem under misspecification, delivering asymptotic normality and valid frequentist coverage via a generalized Wilks phenomenon, while guaranteeing global posterior existence unlike empirical likelihood approaches. Computationally, BSQR enables Hamiltonian Monte Carlo for BQR, alleviating high-dimensional mixing bottlenecks. In simulations, BSQR reduces out-of-sample prediction error by up to 50% and improves sampling efficiency by up to 80% relative to asymmetric Laplace benchmarks, with uniform and triangular kernels performing particularly well. In a financial application to asymmetric systemic risk, BSQR uncovers distinct regime shifts around the COVID-19 period and yields sharper yet well-calibrated predictive quantiles, underscoring its practical relevance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01738v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingqi Liu, Kangqiang Li, Tianxiao Pang</dc:creator>
    </item>
    <item>
      <title>Modified Delayed Acceptance MCMC for Quasi-Bayesian Inference with Linear Moment Conditions</title>
      <link>https://arxiv.org/abs/2511.17117</link>
      <description>arXiv:2511.17117v3 Announce Type: replace-cross 
Abstract: We develop a computationally efficient framework for quasi-Bayesian inference based on linear moment conditions. The approach employs a delayed acceptance Markov chain Monte Carlo (DA-MCMC) algorithm that uses a surrogate target kernel and a proposal distribution derived from an approximate conditional posterior, thereby exploiting the structure of the quasi-likelihood. Two implementations are introduced. DA-MCMC-Exact fully incorporates prior information into the proposal distribution and maximizes per-iteration efficiency, whereas DA-MCMC-Approx omits the prior in the proposal to reduce matrix inversions, improving numerical stability and computational speed in higher dimensions. Simulation studies on heteroskedastic linear regressions show substantial gains over standard MCMC and conventional DA-MCMC baselines, measured by multivariate effective sample size per iteration and per second. The Approx variant yields the best overall throughput, while the Exact variant attains the highest per-iteration efficiency. Applications to two empirical instrumental variable regressions corroborate these findings: the Approx implementation scales to larger designs where other methods become impractical, while still delivering precise inference. Although developed for moment-based quasi-posteriors, the proposed approach also extends to risk-based quasi-Bayesian formulations when first-order conditions are linear and can be transformed analogously. Overall, the proposed algorithms provide a practical and robust tool for quasi-Bayesian analysis in statistical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17117v3</guid>
      <category>stat.CO</category>
      <category>econ.EM</category>
      <pubDate>Wed, 14 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masahiro Tanaka</dc:creator>
    </item>
  </channel>
</rss>
