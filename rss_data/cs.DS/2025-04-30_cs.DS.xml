<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 May 2025 01:48:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>List Decoding Expander-Based Codes up to Capacity in Near-Linear Time</title>
      <link>https://arxiv.org/abs/2504.20333</link>
      <description>arXiv:2504.20333v1 Announce Type: new 
Abstract: We give a new framework based on graph regularity lemmas, for list decoding and list recovery of codes based on spectral expanders. Using existing algorithms for computing regularity decompositions of sparse graphs in (randomized) near-linear time, and appropriate choices for the constant-sized inner/base codes, we prove the following:
  - Expander-based codes constructed using the distance amplification technique of Alon, Edmonds and Luby [FOCS 1995] with rate $\rho$, can be list decoded to a radius $1 - \rho - \epsilon$ in near-linear time. By known results, the output list has size $O(1/\epsilon)$.
  - The above codes of Alon, Edmonds and Luby, with rate $\rho$, can also be list recovered to radius $1 - \rho - \epsilon$ in near-linear time, with constant-sized output lists.
  - The Tanner code construction of Sipser and Spielman [IEEE Trans. Inf. Theory 1996] with distance $\delta$, can be list decoded to radius $\delta - \epsilon$ in near-linear time, with constant-sized output lists.
  Our results imply novel combinatorial as well as algorithmic bounds for each of the above explicit constructions. All of these bounds are obtained via combinatorial rigidity phenomena, proved using (weak) graph regularity. The regularity framework allows us to lift the list decoding and list recovery properties for the local base codes, to the global codes obtained via the above constructions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20333v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shashank Srivastava, Madhur Tulsiani</dc:creator>
    </item>
    <item>
      <title>Breaking the Barrier of Self-Concordant Barriers: Faster Interior Point Methods for M-Matrices</title>
      <link>https://arxiv.org/abs/2504.20619</link>
      <description>arXiv:2504.20619v1 Announce Type: new 
Abstract: We study two fundamental optimization problems: (1) scaling a symmetric positive definite matrix by a positive diagonal matrix so that the resulting matrix has row and column sums equal to 1; and (2) minimizing a quadratic function subject to hard non-negativity constraints. Both problems lend themselves to efficient algorithms based on interior point methods (IPMs). For general instances, standard self-concordance theory places a limit on the iteration complexity of these methods at $\widetilde{O}\left(n^{1/2}\right)$, where $n$ denotes the matrix dimension. We show via an amortized analysis that, when the input matrix is an M-matrix, an IPM with adaptive step sizes solves both problems in only $\widetilde{O}\left(n^{1/3}\right)$ iterations. As a corollary, using fast Laplacian solvers, we obtain an $\ell_{2}$ flow diffusion algorithm with depth $\widetilde{O}\left(n^{1/3}\right)$ and work $\widetilde{O}$$\left(n^{1/3}\cdot\text{nnz}\right)$. This result marks a significant instance in which a standard log-barrier IPM permits provably fewer than $\Theta\left(n^{1/2}\right)$ iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20619v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adrian Vladu</dc:creator>
    </item>
    <item>
      <title>Deterministic Dynamic Maximal Matching in Sublinear Update Time</title>
      <link>https://arxiv.org/abs/2504.20780</link>
      <description>arXiv:2504.20780v1 Announce Type: new 
Abstract: We give a fully dynamic deterministic algorithm for maintaining a maximal matching of an $n$-vertex graph in $\tilde{O}(n^{8/9})$ amortized update time. This breaks the long-standing $\Omega(n)$-update-time barrier on dense graphs, achievable by trivially scanning all incident vertices of the updated edge, and affirmatively answers a major open question repeatedly asked in the literature [BGS15, BCHN18, Sol22]. We also present a faster randomized algorithm against an adaptive adversary with $\tilde{O}(n^{3/4})$ amortized update time.
  Our approach employs the edge degree constrained subgraph (EDCS), a central object for optimizing approximation ratio, in a completely novel way; we instead use it for maintaining a matching that matches all high degree vertices in sublinear update time so that it remains to handle low degree vertices rather straightforwardly. To optimize this approach, we employ tools never used in the dynamic matching literature prior to our work, including sublinear-time algorithms for matching high degree vertices, random walks on directed expanders, and the monotone Even-Shiloach tree for dynamic shortest paths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20780v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Bernstein, Sayan Bhattacharya, Peter Kiss, Thatchaphol Saranurak</dc:creator>
    </item>
    <item>
      <title>Effective Index Construction Algorithm for Optimal $(k,\eta)$-cores Computation</title>
      <link>https://arxiv.org/abs/2504.20795</link>
      <description>arXiv:2504.20795v1 Announce Type: new 
Abstract: Computing $(k,\eta)$-cores from uncertain graphs is a fundamental problem in uncertain graph analysis. UCF-Index is the state-of-the-art resolution to support $(k,\eta)$-core queries, allowing the $(k,\eta)$-core for any combination of $k$ and $\eta$ to be computed in an optimal time. However, this index constructed by current algorithm is usually incorrect. During decomposition, the key is to obtain the $k$-probabilities of its neighbors when the vertex with minimum $k$-probability is deleted. Current method uses recursive floating-point division to update it, which can lead to serious errors. We propose a correct and efficient index construction algorithm to address this issue. Firstly, we propose tight bounds on the $k$-probabilities of the vertices that need to be updated, and the accurate $k$-probabilities are recalculated in an on-demand manner. Secondly, vertices partitioning and progressive refinement strategy is devised to search the vertex with the minimum $k$-probability, thereby reducing initialization overhead for each $k$ and avoiding unnecessary recalculations. Finally, extensive experiments demonstrate the efficiency and scalability of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20795v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengli Sun, Peng Xu, Guanming Jiang, Philip S. Yu, Yi Li</dc:creator>
    </item>
    <item>
      <title>Online General Knapsack with Reservation Costs</title>
      <link>https://arxiv.org/abs/2504.20855</link>
      <description>arXiv:2504.20855v1 Announce Type: new 
Abstract: In the online general knapsack problem, an algorithm is presented with an item $x=(s,v)$ of size $s$ and value $v$ and must irrevocably choose to pack such an item into the knapsack or reject it before the next item appears. The goal is to maximize the total value of the packed items without overflowing the knapsack's capacity.
  As this classical setting is way too harsh for many real-life applications, we will analyze the online general knapsack problem under the reservation model. Here, instead of accepting or rejecting an item immediately, an algorithm can delay the decision of whether to pack the item by paying a fraction $0\le \alpha$ of the size or the value of the item. This models many practical applications, where, for example, decisions can be delayed for some costs e.g. cancellation fees. We present results for both variants: First, for costs depending on the size of the items and then for costs depending on the value of the items.
  If the reservation costs depend on the size of the items, we find a matching upper and lower bound of $2$ for every $\alpha$. On the other hand, if the reservation costs depend on the value of the items, we find that no algorithm is competitive for reservation costs larger than $1/2$ of the item value, and we find upper and lower bounds for the rest of the reservation range $0\le\alpha&lt; 1/2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20855v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Elisabet Burjons, Matthias Gehnen</dc:creator>
    </item>
    <item>
      <title>Guessing Efficiently for Constrained Subspace Approximation</title>
      <link>https://arxiv.org/abs/2504.20883</link>
      <description>arXiv:2504.20883v1 Announce Type: new 
Abstract: In this paper we study constrained subspace approximation problem. Given a set of $n$ points $\{a_1,\ldots,a_n\}$ in $\mathbb{R}^d$, the goal of the {\em subspace approximation} problem is to find a $k$ dimensional subspace that best approximates the input points. More precisely, for a given $p\geq 1$, we aim to minimize the $p$th power of the $\ell_p$ norm of the error vector $(\|a_1-\bm{P}a_1\|,\ldots,\|a_n-\bm{P}a_n\|)$, where $\bm{P}$ denotes the projection matrix onto the subspace and the norms are Euclidean. In \emph{constrained} subspace approximation (CSA), we additionally have constraints on the projection matrix $\bm{P}$. In its most general form, we require $\bm{P}$ to belong to a given subset $\mathcal{S}$ that is described explicitly or implicitly.
  We introduce a general framework for constrained subspace approximation. Our approach, that we term coreset-guess-solve, yields either $(1+\varepsilon)$-multiplicative or $\varepsilon$-additive approximations for a variety of constraints. We show that it provides new algorithms for partition-constrained subspace approximation with applications to {\it fair} subspace approximation, $k$-means clustering, and projected non-negative matrix factorization, among others. Specifically, while we reconstruct the best known bounds for $k$-means clustering in Euclidean spaces, we improve the known results for the remainder of the problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20883v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Bhaskara, Sepideh Mahabadi, Madhusudhan Reddy Pittu, Ali Vakilian, David P. Woodruff</dc:creator>
    </item>
    <item>
      <title>Approximately Dominating Sets in Elections</title>
      <link>https://arxiv.org/abs/2504.20372</link>
      <description>arXiv:2504.20372v1 Announce Type: cross 
Abstract: Condorcet's paradox is a fundamental result in social choice theory which states that there exist elections in which, no matter which candidate wins, a majority of voters prefer a different candidate. In fact, even if we can select any $k$ winners, there still may exist another candidate that would beat each of the winners in a majority vote. That is, elections may require arbitrarily large dominating sets.
  We show that approximately dominating sets of constant size always exist. In particular, for every $\varepsilon &gt; 0$, every election (irrespective of the number of voters or candidates) can select $O(\frac{1}{\varepsilon ^2})$ winners such that no other candidate beats each of the winners by a margin of more than $\varepsilon$ fraction of voters.
  Our proof uses a simple probabilistic construction using samples from a maximal lottery, a well-studied distribution over candidates derived from the Nash equilibrium of a two-player game. In stark contrast to general approximate equilibria, which may require support logarithmic in the number of pure strategies, we show that maximal lotteries can be approximated with constant support size. These approximate maximal lotteries may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20372v1</guid>
      <category>cs.GT</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moses Charikar, Prasanna Ramakrishnan, Kangning Wang</dc:creator>
    </item>
    <item>
      <title>Warehouse storage and retrieval optimization via clustering, dynamic systems modeling, and GPU-accelerated routing</title>
      <link>https://arxiv.org/abs/2504.20655</link>
      <description>arXiv:2504.20655v1 Announce Type: cross 
Abstract: This paper introduces a warehouse optimization procedure aimed at enhancing the efficiency of product storage and retrieval. By representing product locations and order flows within a time-evolving graph structure, we employ unsupervised clustering to define and refine compact order regions, effectively reducing picking distances. We describe the procedure using a dynamic mathematical model formulated using tools from random dynamical systems theory, enabling a principled analysis of the system's behavior over time even under random operational variations. For routing within this framework, we implement a parallelized Bellman-Ford algorithm, utilizing GPU acceleration to evaluate path segments efficiently. To address scalability challenges inherent in large routing graphs, we introduce a segmentation strategy that preserves performance while maintaining tractable memory requirements. Our results demonstrate significant improvements in both operational efficiency and computational feasibility for large-scale warehouse environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20655v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>math.DS</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Magnus Bengtsson, Jens Wittsten, Jonas Waidringer</dc:creator>
    </item>
    <item>
      <title>Approximate Lifted Model Construction</title>
      <link>https://arxiv.org/abs/2504.20784</link>
      <description>arXiv:2504.20784v1 Announce Type: cross 
Abstract: Probabilistic relational models such as parametric factor graphs enable efficient (lifted) inference by exploiting the indistinguishability of objects. In lifted inference, a representative of indistinguishable objects is used for computations. To obtain a relational (i.e., lifted) representation, the Advanced Colour Passing (ACP) algorithm is the state of the art. The ACP algorithm, however, requires underlying distributions, encoded as potential-based factorisations, to exactly match to identify and exploit indistinguishabilities. Hence, ACP is unsuitable for practical applications where potentials learned from data inevitably deviate even if associated objects are indistinguishable. To mitigate this problem, we introduce the $\varepsilon$-Advanced Colour Passing ($\varepsilon$-ACP) algorithm, which allows for a deviation of potentials depending on a hyperparameter $\varepsilon$. $\varepsilon$-ACP efficiently uncovers and exploits indistinguishabilities that are not exact. We prove that the approximation error induced by $\varepsilon$-ACP is strictly bounded and our experiments show that the approximation error is close to zero in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20784v1</guid>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Malte Luttermann, Jan Speller, Marcel Gehrke, Tanya Braun, Ralf M\"oller, Mattis Hartwig</dc:creator>
    </item>
    <item>
      <title>Provably faster randomized and quantum algorithms for k-means clustering via uniform sampling</title>
      <link>https://arxiv.org/abs/2504.20982</link>
      <description>arXiv:2504.20982v1 Announce Type: cross 
Abstract: The $k$-means algorithm (Lloyd's algorithm) is a widely used method for clustering unlabeled data. A key bottleneck of the $k$-means algorithm is that each iteration requires time linear in the number of data points, which can be expensive in big data applications. This was improved in recent works proposing quantum and quantum-inspired classical algorithms to approximate the $k$-means algorithm locally, in time depending only logarithmically on the number of data points (along with data dependent parameters) [$q$-means: A quantum algorithm for unsupervised machine learning; Kerenidis, Landman, Luongo, and Prakash, NeurIPS 2019; Do you know what $q$-means?, Doriguello, Luongo, Tang]. In this work, we describe a simple randomized mini-batch $k$-means algorithm and a quantum algorithm inspired by the classical algorithm. We prove worse-case guarantees that significantly improve upon the bounds for previous algorithms. Our improvements are due to a careful use of uniform sampling, which preserves certain symmetries of the $k$-means problem that are not preserved in previous algorithms that use data norm-based sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20982v1</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tyler Chen, Archan Ray, Akshay Seshadri, Dylan Herman, Bao Bach, Pranav Deshpande, Abhishek Som, Niraj Kumar, Marco Pistoia</dc:creator>
    </item>
    <item>
      <title>Optimal Oblivious Subspace Embeddings with Near-optimal Sparsity</title>
      <link>https://arxiv.org/abs/2411.08773</link>
      <description>arXiv:2411.08773v2 Announce Type: replace 
Abstract: An oblivious subspace embedding is a random $m\times n$ matrix $\Pi$ such that, for any $d$-dimensional subspace, with high probability $\Pi$ preserves the norms of all vectors in that subspace within a $1\pm\epsilon$ factor. In this work, we give an oblivious subspace embedding with the optimal dimension $m=\Theta(d/\epsilon^2)$ that has a near-optimal sparsity of $\tilde O(1/\epsilon)$ non-zero entries per column of $\Pi$. This is the first result to nearly match the conjecture of Nelson and Nguyen [FOCS 2013] in terms of the best sparsity attainable by an optimal oblivious subspace embedding, improving on a prior bound of $\tilde O(1/\epsilon^6)$ non-zeros per column [Chenakkod et al., STOC 2024]. We further extend our approach to the non-oblivious setting, proposing a new family of Leverage Score Sparsified embeddings with Independent Columns, which yield faster runtimes for matrix approximation and regression tasks.
  In our analysis, we develop a new method which uses a decoupling argument together with the cumulant method for bounding the edge universality error of isotropic random matrices. To achieve near-optimal sparsity, we combine this general-purpose approach with new traces inequalities that leverage the specific structure of our subspace embedding construction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08773v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shabarish Chenakkod, Micha{\l} Derezi\'nski, Xiaoyu Dong</dc:creator>
    </item>
    <item>
      <title>The Overlap Gap Property limits limit swapping in QAOA</title>
      <link>https://arxiv.org/abs/2404.06087</link>
      <description>arXiv:2404.06087v5 Announce Type: replace-cross 
Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is a quantum algorithm designed for Combinatorial Optimization Problem (COP). We show that if a local algorithm is limited in performance at logarithmic depth for a spin glass type COP with an underlying Erd\"os--R\'enyi hypergraph, then a random regular hypergraph exhibits it as well. As such, we re-derived the fact that the average-case value obtained by QAOA for the Max-$q$-XORSAT for even $q\ge 4$ is bounded away from optimality even when the algorithm runs indefinitely if optimised using the so-called tree parameters due to the presence of the Overlap Gap Property (OGP). While this result was proven before, the proof is rather technical compared to ours. In addition, we show that the earlier result implicitly also implies limitation at logarithmic depth $p \le \epsilon \log n$ providing an improvement over limitation at constant depth. Lastly, the results suggests that even when sub-optimised, the performance of QAOA on spin glass is equal in performance to classical algorithms in solving the mean field spin glass problem providing further evidence that the conjecture of getting the exact solution under limit swapping for the Sherrington--Kirkpatrick model to be true.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06087v5</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.DS</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark Xin Hong Goh</dc:creator>
    </item>
    <item>
      <title>NPA Hierarchy for Quantum Isomorphism and Homomorphism Indistinguishability</title>
      <link>https://arxiv.org/abs/2407.10635</link>
      <description>arXiv:2407.10635v2 Announce Type: replace-cross 
Abstract: Man\v{c}inska and Roberson [FOCS'20] showed that two graphs are quantum isomorphic if and only if they are homomorphism indistinguishable over the class of planar graphs. Atserias et al. [JCTB'19] proved that quantum isomorphism is undecidable in general. The NPA hierarchy gives a sequence of semidefinite programming relaxations of quantum isomorphism. Recently, Roberson and Seppelt [ICALP'23] obtained a homomorphism indistinguishability characterization of the feasibility of each level of the Lasserre hierarchy of semidefinite programming relaxations of graph isomorphism. We prove a quantum analogue of this result by showing that each level of the NPA hierarchy of SDP relaxations for quantum isomorphism of graphs is equivalent to homomorphism indistinguishability over an appropriate class of planar graphs. By combining the convergence of the NPA hierarchy with the fact that the union of these graph classes is the set of all planar graphs, we are able to give a new proof of the result of Man\v{c}inska and Roberson [FOCS'20] that avoids the use of the theory of quantum groups. This homomorphism indistinguishability characterization also allows us to give a randomized polynomial-time algorithm deciding exact feasibility of each fixed level of the NPA hierarchy of SDP relaxations for quantum isomorphism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10635v2</guid>
      <category>quant-ph</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prem Nigam Kar, David E. Roberson, Tim Seppelt, Peter Zeman</dc:creator>
    </item>
    <item>
      <title>Characterization of Circular-arc Graphs: II. McConnell Flipping</title>
      <link>https://arxiv.org/abs/2408.10892</link>
      <description>arXiv:2408.10892v3 Announce Type: replace-cross 
Abstract: McConnell [FOCS 2001] presented a flipping transformation from circular-arc graphs to interval graphs with certain patterns of representations. Beyond its algorithmic implications, this transformation is instrumental in identifying all minimal graphs that are not circular-arc graphs. We conduct a structural study of this transformation, and for $C_{4}$-free graphs, we achieve a complete characterization of these patterns. This characterization allows us, among other things, to identify all minimal chordal graphs that are not circular-arc graphs in a companion paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10892v3</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yixin Cao, Tomasz Krawczyk</dc:creator>
    </item>
    <item>
      <title>An average case efficient algorithm for solving two variable linear diophantine equations</title>
      <link>https://arxiv.org/abs/2409.14052</link>
      <description>arXiv:2409.14052v2 Announce Type: replace-cross 
Abstract: Solving two variable linear Diophantine equations has application in many cryptographic protocols such as RSA and Elliptic curve cryptography. The Extended Euclid's algorithm is the most widely used algorithm to solve these equations. We revisit two algorithms to solve two variable linear Diophantine equations. For one of those, we do a fine-grained analysis of the number of recursive calls and arrive at a periodic function that represents the number of recursive calls. We find the period and use it to derive an accurate closed-form expression for the average number of recursive calls incurred by that algorithm. We find multiple loose upper bounds on the average number of recursive calls in different cases based on whether a solution exists or not. If we know that for a fixed value of $a,b$ and a varying $c$, an equation $ax+by=c$ (where $a&gt;b$) is solvable, then we can find the solution in $O\left(\frac{\log b}{gcd(a,b)}\right)$ average number of recursion or steps. We computationally evaluate this bound as well as one more upper bound and compare them with the average number of recursive calls in Extended Euclid's algorithm on a number of random $ n$-bit inputs. We observe that the average number of iterations in the analyzed algorithm decreases with an increase in $gcd(a,b)$. We propose an iterative version of the algorithm. We implement this algorithm and find that the average number of iterations by our algorithm is less than that of two existing algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14052v2</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>math.NT</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mayank Deora, Pinakpani Pal</dc:creator>
    </item>
    <item>
      <title>Regularized Robustly Reliable Learners and Instance Targeted Attacks</title>
      <link>https://arxiv.org/abs/2410.10572</link>
      <description>arXiv:2410.10572v3 Announce Type: replace-cross 
Abstract: Instance-targeted data poisoning attacks, where an adversary corrupts a training set to induce errors on specific test points, have raised significant concerns. Balcan et al (2022) proposed an approach to addressing this challenge by defining a notion of robustly-reliable learners that provide per-instance guarantees of correctness under well-defined assumptions, even in the presence of data poisoning attacks. They then give a generic optimal (but computationally inefficient) robustly reliable learner as well as a computationally efficient algorithm for the case of linear separators over log-concave distributions.
  In this work, we address two challenges left open by Balcan et al (2022). The first is that the definition of robustly-reliable learners in Balcan et al (2022) becomes vacuous for highly-flexible hypothesis classes: if there are two classifiers h_0, h_1 \in H both with zero error on the training set such that h_0(x) \neq h_1(x), then a robustly-reliable learner must abstain on x. We address this problem by defining a modified notion of regularized robustly-reliable learners that allows for nontrivial statements in this case. The second is that the generic algorithm of Balcan et al (2022) requires re-running an ERM oracle (essentially, retraining the classifier) on each test point x, which is generally impractical even if ERM can be implemented efficiently. To tackle this problem, we show that at least in certain interesting cases we can design algorithms that can produce their outputs in time sublinear in training time, by using techniques from dynamic algorithm design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10572v3</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Avrim Blum, Donya Saless</dc:creator>
    </item>
    <item>
      <title>PAC Learning is just Bipartite Matching (Sort of)</title>
      <link>https://arxiv.org/abs/2502.00607</link>
      <description>arXiv:2502.00607v2 Announce Type: replace-cross 
Abstract: The main goal of this article is to convince you, the reader, that supervised learning in the Probably Approximately Correct (PAC) model is closely related to -- of all things -- bipartite matching! En-route from PAC learning to bipartite matching, I will overview a particular transductive model of learning, and associated one-inclusion graphs, which can be viewed as a generalization of some of the hat puzzles that are popular in recreational mathematics. Whereas this transductive model is far from new, it has recently seen a resurgence of interest as a tool for tackling deep questions in learning theory. A secondary purpose of this article could be as a (biased) tutorial on the connections between the PAC and transductive models of learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00607v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Wed, 30 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaddin Dughmi</dc:creator>
    </item>
  </channel>
</rss>
