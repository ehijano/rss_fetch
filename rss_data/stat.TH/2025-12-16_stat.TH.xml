<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 16 Dec 2025 05:01:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Local Asymptotic Normality for Multi-Armed Bandits</title>
      <link>https://arxiv.org/abs/2512.12192</link>
      <description>arXiv:2512.12192v1 Announce Type: new 
Abstract: Van den Akker, Werker, and Zhou (2025) showed that the limit experiment, in the sense of H\a'{a}jek-Le Cam, for (contextual) bandits whose arms' expected payoffs differ by $O(T^{-1/2})$, is Locally Asymptotically Quadratic (LAQ) but highly non-standard, being characterized by a system of coupled stochastic differential equations. The present paper considers the complementary case where the arms' expected payoffs are fixed with a unique optimal (in the sense of highest expected payoff) arm. It is shown that, under sampling schemes satisfying mild regularity conditions (including UCB and Thompson sampling), the model satisfies the standard Locally Asymptotically Normal (LAN) property.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12192v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ramon van den Akker, Bas J. M. Werker, Bo Zhou</dc:creator>
    </item>
    <item>
      <title>A complete characterization of maximal copulas with a given track section</title>
      <link>https://arxiv.org/abs/2512.12257</link>
      <description>arXiv:2512.12257v1 Announce Type: new 
Abstract: Bivariate copulas with prescribed diagonal section were first studied by Bertino. Their maximality was studied so far only from the point of view of upper bounds which brings quasi-copulas into the picture and limits the resulting set substantially. We propose to study maximality of these families in the order theoretic sense. A copula C with given diagonal section {\delta} is called undominated if there is no copula C' {\neq} C with the same diagonal section {\delta} such that C {\leq} C'. The main contribution of this paper is a new method that provides copulas of the kind. This method generates a much wider class that contains the known upper bounds as a very small subclass. There was a recent call for the study of asymmetry which is addressed by our class better than by the known ones. Corresponding quasi-copulas can be obtained from our copulas via splicing techniques. Most results are given on the level of tracks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12257v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matja\v{z} Omladi\v{c}, Damjan \v{S}kulj</dc:creator>
    </item>
    <item>
      <title>Marshall-Olkin copulas revisited</title>
      <link>https://arxiv.org/abs/2512.12265</link>
      <description>arXiv:2512.12265v1 Announce Type: new 
Abstract: Almost seventy years old Marshall-Olkin copulas, then wider Marshall copulas, and finally even wider shock model (SM) copulas constitute a substantial part of nowadays copula theory due to numerous applications. Recently, Christian Genest with some coauthors introduced a new stochastic model for a special subclass of SM copulas which gives not only a new angle on these copulas but also widens the range of applications. In this paper we extend this type of stochastic model to all known subclasses of SM copulas. We also introduce a novel class of SM copulas and extend the new stochastic model to this class as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12265v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Toma\v{z} Ko\v{s}ir, Petra Lazi\'c, Matja\v{z} Omladi\v{c}</dc:creator>
    </item>
    <item>
      <title>On the epsilon-delta Structure Underlying Chatterjee's Rank Correlation</title>
      <link>https://arxiv.org/abs/2512.12363</link>
      <description>arXiv:2512.12363v1 Announce Type: new 
Abstract: We provide an epsilon-delta interpretation of Chatterjee's rank correlation by tracing its origin to a notion of local dependence between random variables. Starting from a primitive epsilon-delta construction, we show that rank-based dependence measures arise naturally as epsilon to zero limits of local averaging procedures. Within this framework, Chatterjee's rank correlation admits a transparent interpretation as an empirical realization of a local L1 residual.
  We emphasize that the probability integral transform plays no structural role in the underlying epsilon-delta mechanism, and is introduced only as a normalization step that renders the final expression distribution-free. We further consider a moment-based analogue obtained by replacing the absolute deviation with a squared residual. This L2 formulation is independent of rank transformations and, under a Gaussian assumption, recovers Pearson's coefficient of determination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12363v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeusu Sato</dc:creator>
    </item>
    <item>
      <title>Early Highlights in the History of the Bernstein-von Mises Theorem</title>
      <link>https://arxiv.org/abs/2512.12379</link>
      <description>arXiv:2512.12379v1 Announce Type: new 
Abstract: The designation ``Bernstein-von Mises theorem'' is apparently due to Lucien Le Cam. Roughly, the assertion of this theorem states that the posterior distribution of a parameter, conditioned on a large sample, is approximately normal, independent of a particular prior. The present paper discusses important steps in the development of this theorem and its applications, from Laplace in 1774 to Le Cam in 1953. Regarding Bernstein and his disciple Neyman, it thereby relies on sources which were widely unknown and hard to obtain until recently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12379v1</guid>
      <category>math.ST</category>
      <category>math.HO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hans Fischer</dc:creator>
    </item>
    <item>
      <title>Improved Concentration for Mean Estimators via Shrinkage</title>
      <link>https://arxiv.org/abs/2512.12750</link>
      <description>arXiv:2512.12750v1 Announce Type: new 
Abstract: We study a class of robust mean estimators $\widehat{\mu}$ obtained by adaptively shrinking the weights of sample points far from a base estimator $\widehat{\kappa}$. Given a data-dependent scaling factor $\widehat{\alpha}$ and a weighting function $w:[0, \infty) \to [0,1]$, we let $\widehat{\mu} = \widehat{\kappa} + \frac{1}{n}\sum_{i=1}^n(X_i - \widehat{\kappa})w(\widehat{\alpha}|X_i-\widehat{\kappa}|) $. We prove that, under mild assumptions over $w$, these estimators achieve stronger concentration bounds than the base estimate $\widehat{\kappa}$, including sub-Gaussian guarantees. This framework unifies and extends several existing approaches to robust mean estimation in $\mathbb{R}$. Through numerical experiments, we show that our shrinking approach translates to faster concentration, even for small sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12750v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ant\^onio Cat\~ao, Lucas Resende, Paulo Orenstein</dc:creator>
    </item>
    <item>
      <title>Spectral Equivariance and Geometric Transport in Reproducing Kernel Hilbert Spaces: A Unified Framework for Orthogonal Polynomial and Kernel Estimation</title>
      <link>https://arxiv.org/abs/2512.13073</link>
      <description>arXiv:2512.13073v1 Announce Type: new 
Abstract: We develop a unified geometric framework for nonparametric estimation based on the notion of Twin Kernel Spaces, defined as orbits of a reproducing kernel under a group action. This structure induces a family of transported RKHS geometries in which classical orthogonal polynomial estimators, kernel estimators, and spectral smoothing methods arise as projections onto transported eigenfunction systems. Our main contribution is a Spectral Equivariance Theorem showing that the eigenfunctions of any transported kernel are obtained by unitary transport of the base eigenfunctions. As a consequence, orthogonal polynomial estimators are equivariant under geometric deformation, kernel estimators correspond to soft spectral filtering in a twin space, and minimax rates and bias--variance tradeoffs are invariant under transport. We provide examples based on Hermite and Legendre polynomials, affine and Gaussian groups, and illustrate the effectiveness of twin transport for adaptive and multimodal estimation. The framework reveals a deep connection between group actions, RKHS geometry, and spectral nonparametrics, offering a unified perspective that encompasses kernel smoothing, orthogonal series, splines, and multiscale methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13073v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jocelyn Nemb\'e</dc:creator>
    </item>
    <item>
      <title>Convergence of covariance and spectral density estimates for high-dimensional functional time series</title>
      <link>https://arxiv.org/abs/2512.13310</link>
      <description>arXiv:2512.13310v1 Announce Type: new 
Abstract: Second-order characteristics including covariance and spectral density functions are fundamentally important for both statistical applications and theoretical analysis in functional time series. In the high-dimensional setting where the number of functional variables is large relative to the length of functional time series, non-asymptotic theory for covariance function estimation has been developed for Gaussian and sub-Gaussian functional linear processes. However, corresponding non-asymptotic results for high-dimensional non-Gaussian and nonlinear functional time series, as well as for spectral density function estimation, are largely unexplored. In this paper, we introduce novel functional dependence measures, based on which we establish systematic non-asymptotic concentration bounds for estimates of (auto)covariance and spectral density functions in high-dimensional and non-Gaussian settings. We then illustrate the usefulness of our convergence results through two applications to dynamic functional principal component analysis and sparse spectral density function estimation. To handle the practical scenario where curves are discretely observed with errors, we further develop convergence rates of the corresponding estimates obtained via a nonparametric smoothing method. Finally, extensive simulation studies are conducted to corroborate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13310v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bufan Li, Xinghao Qiao, Weichi Wu, Holger Dette</dc:creator>
    </item>
    <item>
      <title>A fine-grained look at causal effects in causal spaces</title>
      <link>https://arxiv.org/abs/2512.11919</link>
      <description>arXiv:2512.11919v1 Announce Type: cross 
Abstract: The notion of causal effect is fundamental across many scientific disciplines. Traditionally, quantitative researchers have studied causal effects at the level of variables; for example, how a certain drug dose (W) causally affects a patient's blood pressure (Y). However, in many modern data domains, the raw variables-such as pixels in an image or tokens in a language model-do not have the semantic structure needed to formulate meaningful causal questions. In this paper, we offer a more fine-grained perspective by studying causal effects at the level of events, drawing inspiration from probability theory, where core notions such as independence are first given for events and sigma-algebras, before random variables enter the picture. Within the measure-theoretic framework of causal spaces, a recently introduced axiomatisation of causality, we first introduce several binary definitions that determine whether a causal effect is present, as well as proving some properties of them linking causal effect to (in)dependence under an intervention measure. Further, we provide quantifying measures that capture the strength and nature of causal effects on events, and show that we can recover the common measures of treatment effect as special cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11919v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhyung Park, Yuqing Zhou</dc:creator>
    </item>
    <item>
      <title>Contextual Peano Scan and Fast Image Segmentation Using Hidden and Evidential Markov Chains</title>
      <link>https://arxiv.org/abs/2512.11939</link>
      <description>arXiv:2512.11939v1 Announce Type: cross 
Abstract: Transforming bi-dimensional sets of image pixels into mono-dimensional sequences with a Peano scan (PS) is an established technique enabling the use of hidden Markov chains (HMCs) for unsupervised image segmentation. Related Bayesian segmentation methods can compete with hidden Markov fields (HMFs)-based ones and are much faster. PS has recently been extended to the contextual PS, and some initial experiments have shown the value of the associated HMC model, denoted as HMC-CPS, in image segmentation. Moreover, HMCs have been extended to hidden evidential Markov chains (HEMCs), which are capable of improving HMC-based Bayesian segmentation. In this study, we introduce a new HEMC-CPS model by simultaneously considering contextual PS and evidential HMC. We show its effectiveness for Bayesian maximum posterior mode (MPM) segmentation using synthetic and real images. Segmentation is performed in an unsupervised manner, with parameters being estimated using the stochastic expectation--maximization (SEM) method. The new HEMC-CPS model presents potential for the modeling and segmentation of more complex images, such as three-dimensional or multi-sensor multi-resolution images. Finally, the HMC-CPS and HEMC-CPS models are not limited to image segmentation and could be used for any kind of spatially correlated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11939v1</guid>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Mathematics 2025, 13 (10), pp.1589</arxiv:journal_reference>
      <dc:creator>Cl\'ement Fernandes (SAMOVAR, SOP - SAMOVAR, TSP), Wojciech Pieczynski (SAMOVAR, SOP - SAMOVAR, TSP)</dc:creator>
    </item>
    <item>
      <title>Interval Fisher's Discriminant Analysis and Visualisation</title>
      <link>https://arxiv.org/abs/2512.11945</link>
      <description>arXiv:2512.11945v1 Announce Type: cross 
Abstract: In Data Science, entities are typically represented by single valued measurements. Symbolic Data Analysis extends this framework to more complex structures, such as intervals and histograms, that express internal variability. We propose an extension of multiclass Fisher's Discriminant Analysis to interval-valued data, using Moore's interval arithmetic and the Mallows' distance. Fisher's objective function is generalised to consider simultaneously the contributions of the centres and the ranges of intervals and is numerically maximised. The resulting discriminant directions are then used to classify interval-valued observations.To support visual assessment, we adapt the class map, originally introduced for conventional data, to classifiers that assign labels through minimum distance rules. We also extend the silhouette plot to this setting and use stacked mosaic plots to complement the visual display of class assignments. Together, these graphical tools provide insight into classifier performance and the strength of class membership. Applications to real datasets illustrate the proposed methodology and demonstrate its value in interpreting classification results for interval-valued data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11945v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Diogo Pinheiro, M. Ros\'ario Oliveira, Igor Kravchenko, Lina Oliveira</dc:creator>
    </item>
    <item>
      <title>Debiased Inference for High-Dimensional Regression Models Based on Profile M-Estimation</title>
      <link>https://arxiv.org/abs/2512.12003</link>
      <description>arXiv:2512.12003v1 Announce Type: cross 
Abstract: Debiased inference for high-dimensional regression models has received substantial recent attention to ensure regularized estimators have valid inference. All existing methods focus on achieving Neyman orthogonality through explicitly constructing projections onto the space of nuisance parameters, which is infeasible when an explicit form of the projection is unavailable. We introduce a general debiasing framework, Debiased Profile M-Estimation (DPME), which applies to a broad class of models and does not require model-specific Neyman orthogonalization or projection derivations as in existing methods. Our approach begins by obtaining an initial estimator of the parameters by optimizing a penalized objective function. To correct for the bias introduced by penalization, we construct a one-step estimator using the Newton-Raphson update, applied to the gradient of a profile function defined as the optimal objective function with the parameter of interest held fixed. We use numerical differentiation without requiring the explicit calculation of the gradients. The resulting DPME estimator is shown to be asymptotically linear and normally distributed. Through extensive simulations, we demonstrate that the proposed method achieves better coverage rates than existing alternatives, with largely reduced computational cost. Finally, we illustrate the utility of our method with an application to estimating a treatment rule for multiple myeloma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12003v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Wang, Yuhao Deng, Yu Gu, Yuanjia Wang, Donglin Zeng</dc:creator>
    </item>
    <item>
      <title>Estimation of a Dynamic Tobit Model with a Unit Root</title>
      <link>https://arxiv.org/abs/2512.12110</link>
      <description>arXiv:2512.12110v1 Announce Type: cross 
Abstract: This paper studies robust estimation in the dynamic Tobit model under local-to-unity (LUR) asymptotics. We show that both Gaussian maximum likelihood (ML) and censored least absolute deviations (CLAD) estimators are consistent, extending results from the stationary case where ordinary least squares (OLS) is inconsistent. The asymptotic distributions of MLE and CLAD are derived; for the short-run parameters they are shown to be Gaussian, yielding standard normal t-statistics. In contrast, although OLS remains consistent under LUR, its t-statistics are not standard normal. These results enable reliable model selection via sequential t-tests based on ML and CLAD, paralleling the linear autoregressive case. Applications to financial and epidemiological time series illustrate their practical relevance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12110v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Bykhovskaya, James A. Duffy</dc:creator>
    </item>
    <item>
      <title>Balancing Accuracy and Speed: A Multi-Fidelity Ensemble Kalman Filter with a Machine Learning Surrogate Model</title>
      <link>https://arxiv.org/abs/2512.12276</link>
      <description>arXiv:2512.12276v1 Announce Type: cross 
Abstract: Currently, more and more machine learning (ML) surrogates are being developed for computationally expensive physical models. In this work we investigate the use of a Multi-Fidelity Ensemble Kalman Filter (MF-EnKF) in which the low-fidelity model is such a machine learning surrogate model, instead of a traditional low-resolution or reduced-order model. The idea behind this is to use an ensemble of a few expensive full model runs, together with an ensemble of many cheap but less accurate ML model runs. In this way we hope to reach increased accuracy within the same computational budget. We investigate the performance by testing the approach on two common test problems, namely the Lorenz-2005 model and the Quasi-Geostrophic model. By keeping the original physical model in place, we obtain a higher accuracy than when we completely replace it by the ML model. Furthermore, the MF-EnKF reaches improved accuracy within the same computational budget. The ML surrogate has similar or improved accuracy compared to the low-resolution one, but it can provide a larger speed-up. Our method contributes to increasing the effective ensemble size in the EnKF, which improves the estimation of the initial condition and hence accuracy of the predictions in fields such as meteorology and oceanography.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12276v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>physics.ao-ph</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeffrey van der Voort, Martin Verlaan, Hanne Kekkonen</dc:creator>
    </item>
    <item>
      <title>Precise Deviations for the Ewens-Pitman Model</title>
      <link>https://arxiv.org/abs/2512.12323</link>
      <description>arXiv:2512.12323v1 Announce Type: cross 
Abstract: In this paper, we derive an integral representation for the distribution of the number of types $K_n$ in the Ewens-Pitman model. Based on this representation, we also establish precise large deviations and precise moderate deviations for $K_n$. After careful examination, we find that the rate function exhibits a second-order phase transition and the critical point is $\alpha=\frac{1}{2}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12323v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiqi Peng, Youzhou Zhou</dc:creator>
    </item>
    <item>
      <title>Eventually LIL Regret: Almost Sure $\ln\ln T$ Regret for a sub-Gaussian Mixture on Unbounded Data</title>
      <link>https://arxiv.org/abs/2512.12325</link>
      <description>arXiv:2512.12325v1 Announce Type: cross 
Abstract: We prove that a classic sub-Gaussian mixture proposed by Robbins in a stochastic setting actually satisfies a path-wise (deterministic) regret bound. For every path in a natural ``Ville event'' $E_\alpha$, this regret till time $T$ is bounded by $\ln^2(1/\alpha)/V_T + \ln (1/\alpha) + \ln \ln V_T$ up to universal constants, where $V_T$ is a nonnegative, nondecreasing, cumulative variance process. (The bound reduces to $\ln(1/\alpha) + \ln \ln V_T$ if $V_T \geq \ln(1/\alpha)$.) If the data were stochastic, then one can show that $E_\alpha$ has probability at least $1-\alpha$ under a wide class of distributions (eg: sub-Gaussian, symmetric, variance-bounded, etc.). In fact, we show that on the Ville event $E_0$ of probability one, the regret on every path in $E_0$ is eventually bounded by $\ln \ln V_T$ (up to constants). We explain how this work helps bridge the world of adversarial online learning (which usually deals with regret bounds for bounded data), with game-theoretic statistics (which can handle unbounded data, albeit using stochastic assumptions). In short, conditional regret bounds serve as a bridge between stochastic and adversarial betting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12325v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shubhada Agrawal, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Understanding Overparametrization in Survival Models through Double-Descent</title>
      <link>https://arxiv.org/abs/2512.12463</link>
      <description>arXiv:2512.12463v1 Announce Type: cross 
Abstract: Classical statistical learning theory predicts a U-shaped relationship between test loss and model capacity, driven by the bias-variance trade-off. Recent advances in modern machine learning have revealed a more complex pattern, double-descent, in which test loss, after peaking near the interpolation threshold, decreases again as model capacity continues to grow. While this behavior has been extensively analyzed in regression and classification, its manifestation in survival analysis remains unexplored. This study investigates double-descent in four representative survival models: DeepSurv, PC-Hazard, Nnet-Survival, and N-MTLR. We rigorously define interpolation and finite-norm interpolation, two key characteristics of loss-based models to understand double-descent. We then show the existence (or absence) of (finite-norm) interpolation of all four models. Our findings clarify how likelihood-based losses and model implementation jointly determine the feasibility of interpolation and show that overfitting should not be regarded as benign for survival models. All theoretical results are supported by numerical experiments that highlight the distinct generalization behaviors of survival models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12463v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yin Liu, Jianwen Cai, Didong Li</dc:creator>
    </item>
    <item>
      <title>Complexity of Markov Chain Monte Carlo for Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2512.12748</link>
      <description>arXiv:2512.12748v1 Announce Type: cross 
Abstract: Markov Chain Monte Carlo (MCMC), Laplace approximation (LA) and variational inference (VI) methods are popular approaches to Bayesian inference, each with trade-offs between computational cost and accuracy. However, a theoretical understanding of these differences is missing, particularly when both the sample size $n$ and the dimension $d$ are large. LA and Gaussian VI are justified by Bernstein-von Mises (BvM) theorems, and recent work has derived the characteristic condition $n\gg d^2$ for their validity, improving over the condition $n\gg d^3$. In this paper, we show for linear, logistic and Poisson regression that for $n\gtrsim d$, MCMC attains the same complexity scaling in $n$, $d$ as first-order optimization algorithms, up to sub-polynomial factors. Thus MCMC is competitive with LA and Gaussian VI in complexity, under a scaling between $n$ and $d$ more general than BvM regimes. Our complexities apply to appropriately scaled priors that are not necessarily Gaussian-tailed, including Student-$t$ and flat priors, with log-posteriors that are not necessarily globally concave or gradient-Lipschitz.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12748v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Chak, Giacomo Zanella</dc:creator>
    </item>
    <item>
      <title>Asymptotic Normality of Subgraph Counts in Sparse Inhomogeneous Random Graphs</title>
      <link>https://arxiv.org/abs/2512.12937</link>
      <description>arXiv:2512.12937v1 Announce Type: cross 
Abstract: In this paper, we derive the asymptotic distribution of the number of copies of a fixed graph $H$ in a random graph $G_n$ sampled from a sparse graphon model. Specifically, we provide a refined analysis that separates the contributions of edge randomness and vertex-label randomness, allowing us to identify distinct sparsity regimes in which each component dominates or both contribute jointly to the fluctuations. As a result, we establish asymptotic normality for the count of any fixed graph $H$ in $G_n$ across the entire range of sparsity (above the containment threshold for $H$ in $G_n$). These results provide a complete description of subgraph count fluctuations in sparse inhomogeneous networks, closing several gaps in the existing literature that were limited to specific motifs or suboptimal sparsity assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12937v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sayak Chatterjee, Anirban Chatterjee, Abhinav Chakraborty, Bhaswar B. Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Asymptotic Inference for Constrained Regression</title>
      <link>https://arxiv.org/abs/2512.12953</link>
      <description>arXiv:2512.12953v1 Announce Type: cross 
Abstract: We consider statistical inference in high-dimensional regression problems under affine constraints on the parameter space. The theoretical study of this is motivated by the study of genetic determinants of diseases, such as diabetes, using external information from mediating protein expression levels. Specifically, we develop rigorous methods for estimating genetic effects on diabetes-related continuous outcomes when these associations are constrained based on external information about genetic determinants of proteins, and genetic relationships between proteins and the outcome of interest. In this regard, we discuss multiple candidate estimators and study their theoretical properties, sharp large sample optimality, and numerical qualities under a high-dimensional proportional asymptotic framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12953v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Madhav Sankaranarayanan, Yana Hrytsenko, Jerome I. Rotter, Tamar Sofer, Rajarshi Mukherjee</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach to learning mixtures of nonparametric components</title>
      <link>https://arxiv.org/abs/2512.12988</link>
      <description>arXiv:2512.12988v1 Announce Type: cross 
Abstract: Mixture models are widely used in modeling heterogeneous data populations. A standard approach of mixture modeling is to assume that the mixture component takes a parametric kernel form, while the flexibility of the model can be obtained by using a large or possibly unbounded number of such parametric kernels. In many applications, making parametric assumptions on the latent subpopulation distributions may be unrealistic, which motivates the need for nonparametric modeling of the mixture components themselves. In this paper we study finite mixtures with nonparametric mixture components, using a Bayesian nonparametric modeling approach. In particular, it is assumed that the data population is generated according to a finite mixture of latent component distributions, where each component is endowed with a Bayesian nonparametric prior such as the Dirichlet process mixture. We present conditions under which the individual mixture component's distributions can be identified, and establish posterior contraction behavior for the data population's density, as well as densities of the latent mixture components. We develop an efficient MCMC algorithm for posterior inference and demonstrate via simulation studies and real-world data illustrations that it is possible to efficiently learn complex distributions for the latent subpopulations. In theory, the posterior contraction rate of the component densities is nearly polynomial, which is a significant improvement over the logarithm convergence rate of estimating mixing measures via deconvolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12988v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yilei Zhang, Yun Wei, Aritra Guha, XuanLong Nguyen</dc:creator>
    </item>
    <item>
      <title>Stopping Rules for Stochastic Gradient Descent via Anytime-Valid Confidence Sequences</title>
      <link>https://arxiv.org/abs/2512.13123</link>
      <description>arXiv:2512.13123v1 Announce Type: cross 
Abstract: We study stopping rules for stochastic gradient descent (SGD) for convex optimization from the perspective of anytime-valid confidence sequences. Classical analyses of SGD provide convergence guarantees in expectation or at a fixed horizon, but offer no statistically valid way to assess, at an arbitrary time, how close the current iterate is to the optimum. We develop an anytime-valid, data-dependent upper confidence sequence for the weighted average suboptimality of projected SGD, constructed via nonnegative supermartingales and requiring no smoothness or strong convexity. This confidence sequence yields a simple stopping rule that is provably $\varepsilon$-optimal with probability at least $1-\alpha$ and is almost surely finite under standard stochastic approximation stepsizes. To the best of our knowledge, these are the first rigorous, time-uniform performance guarantees and finite-time $\varepsilon$-optimality certificates for projected SGD with general convex objectives, based solely on observable trajectory quantities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13123v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liviu Aolaritei, Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>q-Analogue of Hamiltonian Monte Carlo method</title>
      <link>https://arxiv.org/abs/2512.13246</link>
      <description>arXiv:2512.13246v1 Announce Type: cross 
Abstract: Building upon Lagrangian mechanics on Wess's $q$-commutative spaces, we derive the $q$-deformed Hamiltonian dynamics as formulated by Lavagno et al. (2006). We then develop a computationally tractable scheme and propose a novel Hamiltonian Monte Carlo sampler ($q$-HMC). The proposed $q$-HMC method is shown to satisfy the detailed balance principle. Numerical experiments on distributions with explicit potential functions demonstrate its efficacy, particularly in exploring stiff energy landscapes. This method is also applied to draw samples from the Bayesian posterior distribution of inverse problems. The numerical test for the posterior distribution with stiff potential further shows the advantage of $q$-HMC. And it yields the identical computational implementation process to that of HMC when used to deal with functional reconstruction problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13246v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xiaomei Yang, Zhiliang Deng</dc:creator>
    </item>
    <item>
      <title>From Zipf's Law to Neural Scaling through Heaps' Law and Hilberg's Hypothesis</title>
      <link>https://arxiv.org/abs/2512.13491</link>
      <description>arXiv:2512.13491v1 Announce Type: cross 
Abstract: We inspect the deductive connection between the neural scaling law and Zipf's law -- two statements discussed in machine learning and quantitative linguistics. The neural scaling law describes how the cross entropy rate of a foundation model -- such as a large language model -- changes with respect to the amount of training tokens, parameters, and compute. By contrast, Zipf's law posits that the distribution of tokens exhibits a power law tail. Whereas similar claims have been made in more specific settings, we show that the neural scaling law is a consequence of Zipf's law under certain broad assumptions that we reveal systematically. The derivation steps are as follows: We derive Heaps' law on the vocabulary growth from Zipf's law, Hilberg's hypothesis on the entropy scaling from Heaps' law, and the neural scaling from Hilberg's hypothesis. We illustrate these inference steps by a toy example of the Santa Fe process that satisfies all the four statistical laws.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13491v1</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>{\L}ukasz D\k{e}bowski</dc:creator>
    </item>
    <item>
      <title>Universality of high-dimensional scaling limits of stochastic gradient descent</title>
      <link>https://arxiv.org/abs/2512.13634</link>
      <description>arXiv:2512.13634v1 Announce Type: cross 
Abstract: We consider statistical tasks in high dimensions whose loss depends on the data only through its projection into a fixed-dimensional subspace spanned by the parameter vectors and certain ground truth vectors. This includes classifying mixture distributions with cross-entropy loss with one and two-layer networks, and learning single and multi-index models with one and two-layer networks. When the data is drawn from an isotropic Gaussian mixture distribution, it is known that the evolution of a finite family of summary statistics under stochastic gradient descent converges to an autonomous ordinary differential equation (ODE), as the dimension and sample size go to $\infty$ and the step size goes to $0$ commensurately. Our main result is that these ODE limits are universal in that this convergence occurs even when the data is drawn from mixtures of product measures provided the first two moments match the corresponding Gaussian distribution and the initialization and ground truth vectors are sufficiently coordinate-delocalized. We complement this by proving two corresponding non-universality results. We provide a simple example where the ODE limits are non-universal if the initialization is coordinate aligned. We also show that the stochastic differential equation limits arising as fluctuations of the summary statistics around their ODE's fixed points are not universal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13634v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reza Gheissari, Aukosh Jagannath</dc:creator>
    </item>
    <item>
      <title>A Kernel-Based Approach for Modelling Gaussian Processes with Functional Information</title>
      <link>https://arxiv.org/abs/2201.11023</link>
      <description>arXiv:2201.11023v3 Announce Type: replace 
Abstract: Gaussian processes (GPs) are ubiquitous tools for modeling and predicting continuous processes in physical and engineering sciences. This is partly due to the fact that one may employ a Gaussian process as an interpolator while facilitating straightforward uncertainty quantification at other locations. In addition to training data, it is sometimes the case that available information is not in the form of a finite collection of points. For example, boundary value problems contain information on the boundary of a domain, or underlying physics lead to known behavior on an entire uncountable subset of the domain of interest. While an approximation to such known information may be obtained via pseudo-training points in the known subset, such a procedure is ad hoc with little guidance on the number of points to use, nor the behavior as the number of pseudo-observations grows large. We propose and construct Gaussian processes that unify, via reproducing kernel Hilbert space, the typical finite training data case with the case of having uncountable information by exploiting the equivalence of conditional expectation and orthogonal projections in Hilbert space. We show existence of the proposed process and establish that it is the limit of a conventional GP conditioned on an increasing number of training points. We illustrate the flexibility and advantages of our proposed approach via numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.11023v3</guid>
      <category>math.ST</category>
      <category>math.FA</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>D. Andrew Brown, Peter Kiessler, John Nicholson</dc:creator>
    </item>
    <item>
      <title>Statistical inference for pairwise comparison models</title>
      <link>https://arxiv.org/abs/2401.08463</link>
      <description>arXiv:2401.08463v3 Announce Type: replace 
Abstract: Pairwise comparison models have been widely used for utility evaluation and rank aggregation across various fields. The increasing scale of modern problems underscores the need to understand statistical inference in these models when the number of subjects diverges, a topic that is currently underexplored in the literature. To address this gap, this paper establishes a near-optimal asymptotic normality result for the maximum likelihood estimator in a broad class of pairwise comparison models. The key idea lies in identifying the Fisher information matrix as a weighted graph Laplacian, which can be studied via a meticulous spectral analysis. Our findings provide theoretical foundations for performing statistical inference in a wide range of pairwise comparison models beyond the Bradley--Terry model. Simulations utilizing synthetic data are conducted to validate the asymptotic normality result, followed by a hypothesis test using a tennis competition dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08463v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruijian Han, Wenlu Tang, Yiming Xu</dc:creator>
    </item>
    <item>
      <title>Polynomial Log-Marginals and Tweedie's Formula : When Is Bayes Possible?</title>
      <link>https://arxiv.org/abs/2509.05823</link>
      <description>arXiv:2509.05823v2 Announce Type: replace 
Abstract: Motivated by Tweedie's formula for the Compound Decision problem, we examine the theoretical foundations of empirical Bayes estimators that directly model the marginal density $m(y)$. Our main result shows that polynomial log-marginals of degree $k \ge 3 $ cannot arise from any valid prior distribution in exponential family models, while quadratic forms correspond exactly to Gaussian priors. This provides theoretical justification for why certain empirical Bayes decision rules, while practically useful, do not correspond to any formal Bayes procedures. We also strengthen the diagnostic by showing that a marginal is a Gaussian convolution only if it extends to a bounded solution of the heat equation in a neighborhood of the smoothing parameter, beyond the convexity of $c(y)=\tfrac12 y^2+\log m(y)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05823v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jyotishka Datta, Nicholas G. Polson</dc:creator>
    </item>
    <item>
      <title>Nonparametric Least squares estimators for interval censoring</title>
      <link>https://arxiv.org/abs/2511.01103</link>
      <description>arXiv:2511.01103v4 Announce Type: replace 
Abstract: The limit distribution of the nonparametric maximum likelihood estimator for interval censored data with more than one observation time per unobservable observation, is still unknown in general. For the so-called separated case, where one has observation times which are at a distance larger than a fixed $\epsilon&gt;0$, the limit distribution was derived in [4]. For the non-separated case there is a conjectured limit distribution, given in [9], Section 5.2 of Part 2. But the findings of the present paper suggest that this conjecture may not hold.
  We prove consistency of a closely related nonparametric isotonic least squares estimator and give a sketch of the proof for a result on its limit distribution. We also provide simulation results to show how the nonparametric MLE and least squares estimator behave in comparison. Moreover, we discuss a simpler least squares estimator that can be computed in one step, but is inferior to the other least squares estimator, since it does not use all information.
  For the simplest model of interval censoring, the current status model, the nonparametric maximum likelihood and least squares estimators are the same. This equivalence breaks down if there are more observation times per unobservable observation. The computations for the simulation of the more complicated interval censoring model were performed by using the iterative convex minorant algorithm. They are provided in the GitHub repository [6].</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01103v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Piet Groeneboom</dc:creator>
    </item>
    <item>
      <title>Asymptotics of constrained $M$-estimation under convexity</title>
      <link>https://arxiv.org/abs/2511.04612</link>
      <description>arXiv:2511.04612v2 Announce Type: replace 
Abstract: M-estimation, aka empirical risk minimization, is at the heart of statistics and machine learning: Classification, regression, location estimation, etc. Asymptotic theory is well understood when the loss satisfies some smoothness assumptions and its derivatives are dominated locally. However, these conditions are typically technical and can be too restrictive or heavy to check. Here, we consider the case of a convex loss function, which may not even be differentiable: We establish an asymptotic theory for M-estimation with convex loss (which needs not be differentiable) under convex constraints. We show that the asymptotic distributions of the corresponding M-estimators depend on an interplay between the loss function and the boundary structure of the set of constraints. We extend our results to U-estimators, building on the asymptotic theory of U-statistics. Applications of our work include, among other, robust location/scatter estimation, estimation of deepest points relative to depth functions such as Oja's depth, etc.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04612v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor-Emmanuel Brunel</dc:creator>
    </item>
    <item>
      <title>Imprecise Markov Semigroups and their Ergodicity</title>
      <link>https://arxiv.org/abs/2405.00081</link>
      <description>arXiv:2405.00081v4 Announce Type: replace-cross 
Abstract: We introduce the concept of an imprecise Markov semigroup $\mathbf{Q}$. It is a tool that allows us to represent ambiguity around both the initial and the transition probabilities of a continuous-time Markov process via a compact collection of Markov semigroups, each associated with a (possibly different) Markov process. We use techniques from topology, geometry, and probability to study the ergodic behavior of $\mathbf{Q}$. We show that, under some conditions that also involve the geometry of the state space, eventually the ambiguity fades. We call this property ergodicity of the imprecise Markov semigroup, and we relate it to the classical notion of ergodicity. We prove ergodicity both when the state space is Euclidean or a Riemannian manifold, and when it is an arbitrary measurable space. The importance of our findings for the fields of artificial intelligence and computer vision is also discussed, in particular in the study of how the probability of an output evolves over time as we perturb the input of a convolutional autoencoder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00081v4</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michele Caprio</dc:creator>
    </item>
    <item>
      <title>On the physics of nested Markov models: a generalized probabilistic theory perspective</title>
      <link>https://arxiv.org/abs/2411.11614</link>
      <description>arXiv:2411.11614v2 Announce Type: replace-cross 
Abstract: Determining potential probability distributions with a given causal graph is vital for causality studies. To bypass the difficulty in characterizing latent variables in a Bayesian network, the nested Markov model provides an elegant algebraic approach by listing exactly all the equality constraints on the observed variables. However, this algebraically motivated causal model comprises distributions outside Bayesian networks, and its physical interpretation remains vague. In this work, we inspect the nested Markov model through the lens of generalized probabilistic theory, an axiomatic framework to describe general physical theories. We prove that all the equality constraints defining the nested Markov model are valid theory-independently. At the same time, not every distribution within the nested Markov model is implementable, not even via exotic physical theories associated with generalized probability theories (GPTs). To interpret the origin of such a gap, we study three causal models standing between the nested Markov model and the set of all distributions admitting some GPT realization. Each of the successive three models gives a strictly tighter characterization of the physically implementable distribution set; that is, each successive model manifests new types of GPT-inviolable constraints. We further demonstrate each gap through a specially chosen illustrative causal structure. We anticipate our results will enlighten further explorations on the unification of algebraic and physical perspectives of causality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11614v2</guid>
      <category>quant-ph</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xingjian Zhang, Yuhao Wang, Elie Wolfe</dc:creator>
    </item>
    <item>
      <title>Extreme mass distributions for quasi-copulas</title>
      <link>https://arxiv.org/abs/2506.20672</link>
      <description>arXiv:2506.20672v2 Announce Type: replace-cross 
Abstract: A recent survey, nicknamed "Hitchhiker's Guide", J.J. Arias-Garc{\i}a, R. Mesiar, and B. De Baets, A hitchhiker's guide to quasi-copulas, Fuzzy Sets and Systems 393 (2020) 1-28, has raised the rating of quasi-copula problems in the dependence modeling community in spite of the lack of statistical interpretation of quasi-copulas. In our previous work (Fuzzy Sets and Systems 517 (2025) 109457), we addressed the question of extreme values of the mass distribution associated with multivariate quasi-copulas. Using a linear programming approach, we were able to solve Open Problem 5 of the "Guide" up to dimension d = 17 and disprove a recent conjecture on the solution to that problem. In this paper, we use an analytical approach to provide a complete answer to the original question.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20672v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.fss.2025.109698</arxiv:DOI>
      <arxiv:journal_reference>Fuzzy Sets and Systems Fuzzy Sets and Systems, Volume 527, 15 March 2026, 109698</arxiv:journal_reference>
      <dc:creator>Matja\v{z} Omladi\v{c}, Martin Vuk, Alja\v{z} Zalar</dc:creator>
    </item>
    <item>
      <title>On a $T_1$ Transport inequality for the adapted Wasserstein distance</title>
      <link>https://arxiv.org/abs/2507.19215</link>
      <description>arXiv:2507.19215v2 Announce Type: replace-cross 
Abstract: The $L^1$ transport-entropy inequality (or $T_1$ inequality), which bounds the $1$-Wasserstein distance in terms of the relative entropy, is known to characterize Gaussian concentration. To extend the $T_1$ inequality to laws of discrete-time processes while preserving their temporal structure, we investigate the adapted $T_1$ inequality which relates the $1$-adapted Wasserstein distance to the relative entropy. Building on the Bolley--Villani inequality, we establish the adapted $T_1$ inequality under the same moment assumption as the classical $T_1$ inequality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19215v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonghwa Park</dc:creator>
    </item>
    <item>
      <title>Optimal Convergence Analysis of DDPM for General Distributions</title>
      <link>https://arxiv.org/abs/2510.27562</link>
      <description>arXiv:2510.27562v2 Announce Type: replace-cross 
Abstract: Score-based diffusion models have achieved remarkable empirical success in generating high-quality samples from target data distributions. Among them, the Denoising Diffusion Probabilistic Model (DDPM) is one of the most widely used samplers, generating samples via estimated score functions. Despite its empirical success, a tight theoretical understanding of DDPM -- especially its convergence properties -- remains limited.
  In this paper, we provide a refined convergence analysis of the DDPM sampler and establish near-optimal convergence rates under general distributional assumptions. Specifically, we introduce a relaxed smoothness condition parameterized by a constant $L$, which is small for many practical distributions (e.g., Gaussian mixture models). We prove that the DDPM sampler with accurate score estimates achieves a convergence rate of $$\widetilde{O}\left(\frac{d\min\{d,L^2\}}{T^2}\right)~\text{in Kullback-Leibler divergence},$$ where $d$ is the data dimension, $T$ is the number of iterations, and $\widetilde{O}$ hides polylogarithmic factors in $T$. This result substantially improves upon the best-known $d^2/T^2$ rate when $L &lt; \sqrt{d}$. By establishing a matching lower bound, we show that our convergence analysis is tight for a wide array of target distributions. Moreover, it reveals that DDPM and DDIM share the same dependence on $d$, raising an interesting question of why DDIM often appears empirically faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27562v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchen Jiao, Yuchen Zhou, Gen Li</dc:creator>
    </item>
    <item>
      <title>Nonparametric Uniform Inference in Binary Classification and Policy Values</title>
      <link>https://arxiv.org/abs/2511.14700</link>
      <description>arXiv:2511.14700v2 Announce Type: replace-cross 
Abstract: We develop methods for nonparametric uniform inference in cost-sensitive binary classification, a framework that encompasses maximum score estimation, predicting utility maximizing actions, and policy learning. These problems are well known for slow convergence rates and non-standard limiting behavior, even under point identified parametric frameworks. In nonparametric settings, they may further suffer from failures of identification. To address these challenges, we introduce a strictly convex surrogate loss that point-identifies a representative nonparametric policy function. We then estimate this representative policy function to conduct inference on both the optimal classification policy and the optimal policy value. This approach enables Gaussian inference, substantially simplifying empirical implementation relative to working directly with the original classification problem. In particular, we establish root-$n$ asymptotic normality for the optimal policy value and derive a Gaussian approximation for the optimal classification policy at the standard nonparametric rate. Extensive simulation studies corroborate the theoretical findings. We apply our method to the National JTPA Study to conduct inference on the optimal treatment assignment policy and its associated welfare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14700v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Nan Liu, Yanbo Liu, Yuya Sasaki, Yuanyuan Wan</dc:creator>
    </item>
    <item>
      <title>Learning Time-Varying Correlation Networks with FDR Control via Time-Varying P-values</title>
      <link>https://arxiv.org/abs/2512.10467</link>
      <description>arXiv:2512.10467v2 Announce Type: replace-cross 
Abstract: This paper presents a systematic framework for controlling false discovery rate in learning time-varying correlation networks from high-dimensional, non-linear, non-Gaussian and non-stationary time series with an increasing number of potential abrupt change points in means. We propose a bootstrap-assisted approach to derive dependent and time-varying P-values from a robust estimate of time-varying correlation functions, which are not sensitive to change points. Our procedure is based on a new high-dimensional Gaussian approximation result for the uniform approximation of P-values across time and different coordinates. Moreover, we establish theoretically guaranteed Benjamini--Hochberg and Benjamini--Yekutieli procedures for the dependent and time-varying P-values, which can achieve uniform false discovery rate control. The proposed methods are supported by rigorous mathematical proofs and simulation studies. We also illustrate the real-world application of our framework using both brain electroencephalogram and financial time series data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10467v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bufan Li, Lujia Bai, Weichi Wu</dc:creator>
    </item>
  </channel>
</rss>
