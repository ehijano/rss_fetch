<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Nov 2024 02:35:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Anonymous Distributed Localisation via Spatial Population Protocols</title>
      <link>https://arxiv.org/abs/2411.08434</link>
      <description>arXiv:2411.08434v1 Announce Type: new 
Abstract: In the distributed localization problem (DLP), n anonymous robots (agents) A0, A1, ..., A(n-1) begin at arbitrary positions p0, ..., p(n-1) in S, where S is a Euclidean space. The primary goal in DLP is for agents to reach a consensus on a unified coordinate system that accurately reflects the relative positions of all points, p0, ... , p(n-1), in S. Extensive research on DLP has primarily focused on the feasibility and complexity of achieving consensus when agents have limited access to inter-agent distances, often due to missing or imprecise data. In this paper, however, we examine a minimalist, computationally efficient model of distributed computing in which agents have access to all pairwise distances, if needed. Specifically, we introduce a novel variant of population protocols, referred to as the spatial population protocols model. In this variant each agent can memorise one or a fixed number of coordinates, and when agents A(i) and A(j) interact, they can not only exchange their current knowledge but also either determine the distance d(i,j) between them in S (distance query model) or obtain the vector v(i,j) spanning points p(i) and p(j) (vector query model).
  We examine three DLP scenarios:
  - Self-stabilising localisation protocol with distance queries We propose and analyse self-stabilising localisation protocol based on pairwise distance adjustment. We also discuss several hard instances in this scenario, and suggest possible improvements for the considered protocol,
  - Leader-based localisation protocol with distance queries We propose and analyse several leader-based protocols which stabilise in o(n) parallel time. These protocols rely on efficient solution to multi-contact epidemic, and
  - Self-stabilising localisation protocol with vector queries We propose and analyse superfast self-stabilising DLP protocol which stabilises in O(log n) parallel time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08434v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leszek G\k{a}sieniec, {\L}ukasz Kuszner, Ehsan Latif, Ramviyas Parasuraman, Paul Spirakis, Grzegorz Stachowiak</dc:creator>
    </item>
    <item>
      <title>LSH-MoE: Communication-efficient MoE Training via Locality-Sensitive Hashing</title>
      <link>https://arxiv.org/abs/2411.08446</link>
      <description>arXiv:2411.08446v1 Announce Type: new 
Abstract: Larger transformer models always perform better on various tasks but require more costs to scale up the model size. To efficiently enlarge models, the mixture-of-experts (MoE) architecture is widely adopted, which consists of a gate network and a series of experts and keep the training cost constant by routing the input data to a fixed number of experts instead of all. In existing large-scale MoE training systems, experts would be distributed among different GPUs for parallelization, and thus input data requires additional all-to-all communications to access the target experts and conduct corresponding computations. However, upon evaluating the training process of three mainstream MoE models on commonly used GPU clusters, we found that the all-to-all communication ratio averaged around 45%, which significantly hinders the efficiency and scalability of training MoE models.
  In this paper, we propose LSH-MoE, a communication-efficient MoE training framework using locality-sensitive hashing (LSH). We first present the problems of scaling MoE training in existing systems and highlight the potential of exploiting token similarity to facilitate data compression. Then, we introduce an efficient LSH-based compression technique, which utilizes the cross-polytope hashing for rapid clustering and implements a residual-based error compensation scheme to alleviate the adverse impact of compression. To verify the effectiveness of our methods, we conduct experiments on both language models (e.g., RoBERTa, GPT, and T5) and vision models (e.g., Swin) for pre-training and fine-tuning tasks. The results demonstrate that our method substantially outperforms its counterparts across different tasks by 1.28x - 2.2x of speedup.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08446v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaonan Nie, Qibin Liu, Fangcheng Fu, Shenhan Zhu, Xupeng Miao, Xiaoyang Li, Yang Zhang, Shouda Liu, Bin Cui</dc:creator>
    </item>
    <item>
      <title>In Serverless, OS Scheduler Choice Costs Money: A Hybrid Scheduling Approach for Cheaper FaaS</title>
      <link>https://arxiv.org/abs/2411.08448</link>
      <description>arXiv:2411.08448v1 Announce Type: new 
Abstract: In Function-as-a-Service (FaaS) serverless, large applications are split into short-lived stateless functions. Deploying functions is mutually profitable: users need not be concerned with resource management, while providers can keep their servers at high utilization rates running thousands of functions concurrently on a single machine. It is exactly this high concurrency that comes at a cost. The standard Linux Completely Fair Scheduler (CFS) switches often between tasks, which leads to prolonged execution times. We present evidence that relying on the default Linux CFS scheduler increases serverless workloads cost by up to 10X.
  In this article, we raise awareness and make a case for rethinking the OS-level scheduling in Linux for serverless workloads composed of many short-lived processes. To make serverless more affordable we introduce a hybrid two-level scheduling approach that relies on FaaS characteristics. Short-running functions are executed in FIFO fashion without preemption, while longer-running functions are passed to CFS after a certain time period. We show that tailor-made OS scheduling is able to significantly reduce user-facing costs without adding any provider-facing overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08448v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3652892.3700757</arxiv:DOI>
      <dc:creator>Yuxuan Zhao, Weikang Weng, Rob van Nieuwpoort, Alexandru Uta</dc:creator>
    </item>
    <item>
      <title>Flo: a Semantic Foundation for Progressive Stream Processing</title>
      <link>https://arxiv.org/abs/2411.08274</link>
      <description>arXiv:2411.08274v1 Announce Type: cross 
Abstract: Streaming systems are present throughout modern applications, processing continuous data in real-time. Existing streaming languages have a variety of semantic models and guarantees that are often incompatible. Yet all these languages are considered "streaming" -- what do they have in common? In this paper, we identify two general yet precise semantic properties: streaming progress and eager execution. Together, they ensure that streaming outputs are deterministic and kept fresh with respect to streaming inputs. We formally define these properties in the context of Flo, a parameterized streaming language that abstracts over dataflow operators and the underlying structure of streams. It leverages a lightweight type system to distinguish bounded streams, which allow operators to block on termination, from unbounded ones. Furthermore, Flo provides constructs for dataflow composition and nested graphs with cycles. To demonstrate the generality of our properties, we show how key ideas from representative streaming and incremental computation systems -- Flink, LVars, and DBSP -- have semantics that can be modeled in Flo and guarantees that map to our properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08274v1</guid>
      <category>cs.PL</category>
      <category>cs.DC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shadaj Laddad, Alvin Cheung, Joseph M. Hellerstein, Mae Milano</dc:creator>
    </item>
    <item>
      <title>Federated Graph Learning with Graphless Clients</title>
      <link>https://arxiv.org/abs/2411.08374</link>
      <description>arXiv:2411.08374v1 Announce Type: cross 
Abstract: Federated Graph Learning (FGL) is tasked with training machine learning models, such as Graph Neural Networks (GNNs), for multiple clients, each with its own graph data. Existing methods usually assume that each client has both node features and graph structure of its graph data. In real-world scenarios, however, there exist federated systems where only a part of the clients have such data while other clients (i.e. graphless clients) may only have node features. This naturally leads to a novel problem in FGL: how to jointly train a model over distributed graph data with graphless clients? In this paper, we propose a novel framework FedGLS to tackle the problem in FGL with graphless clients. In FedGLS, we devise a local graph learner on each graphless client which learns the local graph structure with the structure knowledge transferred from other clients. To enable structure knowledge transfer, we design a GNN model and a feature encoder on each client. During local training, the feature encoder retains the local graph structure knowledge together with the GNN model via knowledge distillation, and the structure knowledge is transferred among clients in global update. Our extensive experiments demonstrate the superiority of the proposed FedGLS over five baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08374v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingbo Fu, Song Wang, Yushun Dong, Binchi Zhang, Chen Chen, Jundong Li</dc:creator>
    </item>
    <item>
      <title>Multiplexed bi-layered realization of fault-tolerant quantum computation over optically networked trapped-ion modules</title>
      <link>https://arxiv.org/abs/2411.08616</link>
      <description>arXiv:2411.08616v1 Announce Type: cross 
Abstract: We study an architecture for fault-tolerant measurement-based quantum computation (FT-MBQC) over optically-networked trapped-ion modules. The architecture is implemented with a finite number of modules and ions per module, and leverages photonic interactions for generating remote entanglement between modules and local Coulomb interactions for intra-modular entangling gates. We focus on generating the topologically protected Raussendorf-Harrington-Goyal (RHG) lattice cluster state, which is known to be robust against lattice bond failures and qubit noise, with the modules acting as lattice sites. To ensure that the remote entanglement generation rates surpass the bond-failure tolerance threshold of the RHG lattice, we employ spatial and temporal multiplexing. For realistic system timing parameters, we estimate the code cycle time of the RHG lattice and the ion resources required in a bi-layered implementation, where the number of modules matches the number of sites in two lattice layers, and qubits are reinitialized after measurement. For large distances between modules, we incorporate quantum repeaters between sites and analyze the benefits in terms of cumulative resource requirements. Finally, we derive and analyze a qubit noise-tolerance threshold inequality for the RHG lattice generation in the proposed architecture that accounts for noise from various sources. This includes the depolarizing noise arising from the photonically-mediated remote entanglement generation between modules due to finite optical detection efficiency, limited visibility, and the presence of dark clicks, in addition to the noise from imperfect gates and measurements, and memory decoherence with time. Our work thus underscores the hardware and channel threshold requirements to realize distributed FT-MBQC in a leading qubit platform today -- trapped ions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08616v1</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nitish K. Chandra, Saikat Guha, Kaushik P. Seshadreesan</dc:creator>
    </item>
    <item>
      <title>On-demand Cold Start Frequency Reduction with Off-Policy Reinforcement Learning in Serverless Computing</title>
      <link>https://arxiv.org/abs/2308.07541</link>
      <description>arXiv:2308.07541v2 Announce Type: replace 
Abstract: Function-as-a-Service (FaaS) is a cloud computing paradigm offering an event-driven execution model to applications. It features serverless attributes by eliminating resource management responsibilities from developers, and offers transparent and on-demand scalability of applications. To provide seamless on-demand scalability, new function instances are prepared to serve the incoming workload in the absence or unavailability of function instances. However, FaaS platforms are known to suffer from cold starts, where this function provisioning process introduces a non-negligible delay in function response and reduces the end-user experience. Therefore, the presented work focuses on reducing the frequent, on-demand cold starts on the platform by using Reinforcement Learning(RL). The proposed approach uses model-free Q-learning that consider function metrics such as CPU utilization, existing function instances, and response failure rate, to proactively initialize functions, in advance, based on the expected demand. The proposed solution is implemented on Kubeless and evaluated using an open-source function invocation trace applied to a matrix multiplication function. The evaluation results demonstrate a favourable performance of the RL-based agent when compared to Kubeless' default policy and a function keep-alive policy by improving throughput by up to 8.81% and reducing computation load and resource wastage by up to 55% and 37%, respectively, that is a direct outcome of reduced cold starts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.07541v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>International Conference on Computational Intelligence and Data Analytics (ICCIDA 2024, Springer, Singapore), Hyderabad, India, June 28-29, 2024</arxiv:journal_reference>
      <dc:creator>Siddharth Agarwal, Maria A. Rodriguez, Rajkumar Buyya</dc:creator>
    </item>
    <item>
      <title>Automated MPI-X code generation for scalable finite-difference solvers</title>
      <link>https://arxiv.org/abs/2312.13094</link>
      <description>arXiv:2312.13094v4 Announce Type: replace 
Abstract: Partial differential equations (PDEs) are crucial in modeling diverse phenomena across scientific disciplines, including seismic and medical imaging, computational fluid dynamics, image processing, and neural networks. Solving these PDEs at scale is an intricate and time-intensive process that demands careful tuning. This paper introduces automated code-generation techniques specifically tailored for distributed memory parallelism (DMP) to execute explicit finite-difference (FD) stencils at scale, a fundamental challenge in numerous scientific applications. These techniques are implemented and integrated into the Devito DSL and compiler framework, a well-established solution for automating the generation of FD solvers based on a high-level symbolic math input. Users benefit from modeling simulations for real-world applications at a high-level symbolic abstraction and effortlessly harnessing HPC-ready distributed-memory parallelism without altering their source code. This results in drastic reductions both in execution time and developer effort. A comprehensive performance evaluation of Devito's DMP via MPI demonstrates highly competitive strong and weak scaling on CPU and GPU clusters, proving its effectiveness and capability to meet the demands of large-scale scientific simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13094v4</guid>
      <category>cs.DC</category>
      <category>cs.MS</category>
      <category>cs.PF</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>George Bisbas, Rhodri Nelson, Mathias Louboutin, Fabio Luporini, Paul H. J. Kelly, Gerard Gorman</dc:creator>
    </item>
    <item>
      <title>Data-Centric Design: Introducing An Informatics Domain Model And Core Data Ontology For Computational Systems</title>
      <link>https://arxiv.org/abs/2409.19653</link>
      <description>arXiv:2409.19653v2 Announce Type: replace 
Abstract: The Core Data Ontology (CDO) and the Informatics Domain Model represent a transformative approach to computational systems, shifting from traditional node-centric designs to a data-centric paradigm. This paper introduces a framework where data is categorized into four modalities: objects, events, concepts, and actions. This quadrimodal structure enhances data security, semantic interoperability, and scalability across distributed data ecosystems. The CDO offers a comprehensive ontology that supports AI development, role-based access control, and multimodal data management. By focusing on the intrinsic value of data, the Informatics Domain Model redefines system architectures to prioritize data security, provenance, and auditability, addressing vulnerabilities in current models. The paper outlines the methodology for developing the CDO, explores its practical applications in fields such as AI, robotics, and legal compliance, and discusses future directions for scalable, decentralized, and interoperable data ecosystems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19653v2</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5121/csit.2024.141720</arxiv:DOI>
      <dc:creator>Paul Knowles, Bart Gajderowicz, Keith Dugas</dc:creator>
    </item>
    <item>
      <title>Data movement limits to frontier model training</title>
      <link>https://arxiv.org/abs/2411.01137</link>
      <description>arXiv:2411.01137v2 Announce Type: replace 
Abstract: We present a theoretical model of distributed training, and use it to analyze how far dense and sparse training runs can be scaled. Under our baseline assumptions, given a three month training duration, data movement bottlenecks begin to significantly lower hardware utilization for training runs exceeding about $10^{28}$ FLOP, two orders of magnitude above the largest training run to date, suggesting the arrival of fundamental barriers to scaling in three years given recent rates of growth. A training run exceeding about $10^{31}$ FLOP is infeasible even at low utilization. However, more aggressive batch size scaling and/or shorter and fatter model shapes, if achievable, have the potential to permit much larger training runs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01137v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ege Erdil, David Schneider-Joseph</dc:creator>
    </item>
    <item>
      <title>Two Sides of the Same Coin: Large-scale Measurements of Builder and Rollup after EIP-4844</title>
      <link>https://arxiv.org/abs/2411.03892</link>
      <description>arXiv:2411.03892v2 Announce Type: replace 
Abstract: Web3 is reshaping decentralized ecosystems through innovations like Ethereum. Recently, EIP-4844 is implemented in Ethereum to support its Layer-2 scaling solutions, which introduces a new 128 KB data structure called blob. This upgrade incorporates type-3 transactions with blobs to verify data availability and reduce gas costs for rollups, significantly affecting the strategies of both builders and rollups. In this paper, we present an in-depth study of emerging strategies in builder and rollup markets after EIP-4844, containing hundred million transactions. We find that the efficiency of builder and rollup strategies is interdependent, akin to two sides of the same coin -- both cannot be optimized simultaneously. That is, when builders operate efficiently, rollups tend to overpay in fees, conversely, when rollups optimize their costs, builders may incur losses in inefficient transaction selection. From the side of builders, our results show that 29.48% of these blocks have been constructed inefficiently, which does not produce sufficient profits for builders. Through our evaluation from the side of rollups, we find that over 72.53% of type-3 transactions pay unnecessary fees, leading to notable economic costs of rollups. Our work provides critical insights into optimizing block construction and transaction strategies, advancing the economic efficiency and data scalability of Web3 infrastructures, yet, much like balancing a seesaw, the efficiency of builders and rollups cannot be optimized concurrently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03892v2</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yue Huang, Shuzheng Wang, Yuming Huang, Jing Tang</dc:creator>
    </item>
    <item>
      <title>ADI: Adversarial Dominating Inputs in Vertical Federated Learning Systems</title>
      <link>https://arxiv.org/abs/2201.02775</link>
      <description>arXiv:2201.02775v4 Announce Type: replace-cross 
Abstract: Vertical federated learning (VFL) system has recently become prominent as a concept to process data distributed across many individual sources without the need to centralize it. Multiple participants collaboratively train models based on their local data in a privacy-aware manner. To date, VFL has become a de facto solution to securely learn a model among organizations, allowing knowledge to be shared without compromising privacy of any individuals. Despite the prosperous development of VFL systems, we find that certain inputs of a participant, named adversarial dominating inputs (ADIs), can dominate the joint inference towards the direction of the adversary's will and force other (victim) participants to make negligible contributions, losing rewards that are usually offered regarding the importance of their contributions in federated learning scenarios. We conduct a systematic study on ADIs by first proving their existence in typical VFL systems. We then propose gradient-based methods to synthesize ADIs of various formats and exploit common VFL systems. We further launch greybox fuzz testing, guided by the saliency score of ``victim'' participants, to perturb adversary-controlled inputs and systematically explore the VFL attack surface in a privacy-preserving manner. We conduct an in-depth study on the influence of critical parameters and settings in synthesizing ADIs. Our study reveals new VFL attack opportunities, promoting the identification of unknown threats before breaches and building more secure VFL systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.02775v4</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/SP46215.2023.10179446</arxiv:DOI>
      <dc:creator>Qi Pang, Yuanyuan Yuan, Shuai Wang, Wenting Zheng</dc:creator>
    </item>
    <item>
      <title>DAGER: Exact Gradient Inversion for Large Language Models</title>
      <link>https://arxiv.org/abs/2405.15586</link>
      <description>arXiv:2405.15586v2 Announce Type: replace-cross 
Abstract: Federated learning works by aggregating locally computed gradients from multiple clients, thus enabling collaborative training without sharing private client data. However, prior work has shown that the data can actually be recovered by the server using so-called gradient inversion attacks. While these attacks perform well when applied on images, they are limited in the text domain and only permit approximate reconstruction of small batches and short input sequences. In this work, we propose DAGER, the first algorithm to recover whole batches of input text exactly. DAGER leverages the low-rank structure of self-attention layer gradients and the discrete nature of token embeddings to efficiently check if a given token sequence is part of the client data. We use this check to exactly recover full batches in the honest-but-curious setting without any prior on the data for both encoder- and decoder-based architectures using exhaustive heuristic search and a greedy approach, respectively. We provide an efficient GPU implementation of DAGER and show experimentally that it recovers full batches of size up to 128 on large language models (LLMs), beating prior attacks in speed (20x at same batch size), scalability (10x larger batches), and reconstruction quality (ROUGE-1/2 &gt; 0.99).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15586v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivo Petrov, Dimitar I. Dimitrov, Maximilian Baader, Mark Niklas M\"uller, Martin Vechev</dc:creator>
    </item>
    <item>
      <title>Average-case optimization analysis for distributed consensus algorithms on regular graphs</title>
      <link>https://arxiv.org/abs/2409.00605</link>
      <description>arXiv:2409.00605v3 Announce Type: replace-cross 
Abstract: The consensus problem in distributed computing involves a network of agents aiming to compute the average of their initial vectors through local communication, represented by an undirected graph. This paper focuses on the studying of this problem using an average-case analysis approach, particularly over regular graphs. Traditional algorithms for solving the consensus problem often rely on worst-case performance evaluation scenarios, which may not reflect typical performance in real-world applications. Instead, we apply average-case analysis, focusing on the expected spectral distribution of eigenvalues to obtain a more realistic view of performance. Key contributions include deriving the optimal method for consensus on regular graphs, showing its relation to the Heavy Ball method, analyzing its asymptotic convergence rate, and comparing it to various first-order methods through numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00605v3</guid>
      <category>math.OC</category>
      <category>cs.DC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nhat Trung Nguyen, Alexander Rogozin, Alexander Gasnikov</dc:creator>
    </item>
  </channel>
</rss>
