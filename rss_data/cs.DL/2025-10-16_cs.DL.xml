<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Oct 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Paper Copilot: Tracking the Evolution of Peer Review in AI Conferences</title>
      <link>https://arxiv.org/abs/2510.13201</link>
      <description>arXiv:2510.13201v1 Announce Type: cross 
Abstract: The rapid growth of AI conferences is straining an already fragile peer-review system, leading to heavy reviewer workloads, expertise mismatches, inconsistent evaluation standards, superficial or templated reviews, and limited accountability under compressed timelines. In response, conference organizers have introduced new policies and interventions to preserve review standards. Yet these ad-hoc changes often create further concerns and confusion about the review process, leaving how papers are ultimately accepted - and how practices evolve across years - largely opaque. We present Paper Copilot, a system that creates durable digital archives of peer reviews across a wide range of computer-science venues, an open dataset that enables researchers to study peer review at scale, and a large-scale empirical analysis of ICLR reviews spanning multiple years. By releasing both the infrastructure and the dataset, Paper Copilot supports reproducible research on the evolution of peer review. We hope these resources help the community track changes, diagnose failure modes, and inform evidence-based improvements toward a more robust, transparent, and reliable peer-review system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13201v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <category>cs.LG</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Yang, Qiyao Wei, Jiaxin Pei</dc:creator>
    </item>
    <item>
      <title>Mapping the gender attrition gap in academic psychology</title>
      <link>https://arxiv.org/abs/2510.13273</link>
      <description>arXiv:2510.13273v1 Announce Type: cross 
Abstract: Although more women than men enter social science disciplines, they are underrepresented at senior levels. To investigate this leaky pipeline, this study analyzed the career trajectories of 78,216 psychology researchers using large-scale bibliometric data. Despite overall constituting over 60\% of these researchers, women experienced consistently higher attrition rates than men, particularly in the early years following their first publication. Academic performance, particularly first-authored publications, was strongly associated with early-career retention -- more so than collaboration networks or institutional environment. After controlling for gender differences in publication-, collaboration-, and institution-level factors, women remained more likely to leave academia, especially in early-career stages, pointing to persistent barriers that hinder women's academic careers. These findings suggest that in psychology and potentially other social science disciplines, the core challenge lies in retention rather than recruitment, underscoring the need for targeted, early-career interventions to promote long-term gender equity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13273v1</guid>
      <category>cs.SI</category>
      <category>cs.DL</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xinyi Zhao, Anna I. Thoma, Ralph Hertwig, Dirk U. Wulff</dc:creator>
    </item>
    <item>
      <title>Position: The Artificial Intelligence and Machine Learning Community Should Adopt a More Transparent and Regulated Peer Review Process</title>
      <link>https://arxiv.org/abs/2502.00874</link>
      <description>arXiv:2502.00874v3 Announce Type: replace 
Abstract: The rapid growth of submissions to top-tier Artificial Intelligence (AI) and Machine Learning (ML) conferences has prompted many venues to transition from closed to open review platforms. Some have fully embraced open peer reviews, allowing public visibility throughout the process, while others adopt hybrid approaches, such as releasing reviews only after final decisions or keeping reviews private despite using open peer review systems. In this work, we analyze the strengths and limitations of these models, highlighting the growing community interest in transparent peer review. To support this discussion, we examine insights from Paper Copilot, a website launched two years ago to aggregate and analyze AI / ML conference data while engaging a global audience. The site has attracted over 200,000 early-career researchers, particularly those aged 18-34 from 177 countries, many of whom are actively engaged in the peer review process. Drawing on our findings, this position paper advocates for a more transparent, open, and well-regulated peer review aiming to foster greater community involvement and propel advancements in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00874v3</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Yang</dc:creator>
    </item>
    <item>
      <title>A Formalization of the Ionescu-Tulcea Theorem in Mathlib</title>
      <link>https://arxiv.org/abs/2506.18616</link>
      <description>arXiv:2506.18616v3 Announce Type: replace-cross 
Abstract: We describe the formalization of the Ionescu-Tulcea theorem, showing the existence of a probability measure on the space of trajectories of a Markov chain, in the proof assistant Lean using the integrated library Mathlib. We first present a mathematical proof before exposing the difficulties which arise when trying to formalize it, and how they were overcome. We then build on this work to formalize the construction of the product of an arbitrary family of probability measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18616v3</guid>
      <category>math.PR</category>
      <category>cs.DL</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Etienne Marion (ENS de Lyon)</dc:creator>
    </item>
  </channel>
</rss>
