<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Feb 2026 05:00:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bayes, E-values and Testing</title>
      <link>https://arxiv.org/abs/2602.04146</link>
      <description>arXiv:2602.04146v1 Announce Type: new 
Abstract: This paper studies relationships between Kolmogorov complexity, Shannon entropy, Bayes factors, E-values, and exchangeability testing. The focus is on negative log marginal or predictive probabilities -- what I.J.~Good termed the ``weight of evidence'' -- as a common evidence statistic linking coding, prediction, and sequential testing. The paper reviews the relevant information-theoretic and martingale tools, and discusses exchangeability testing via conformal e-prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04146v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nick Polson, Vadim Sokolov, Daniel Zantedeschi</dc:creator>
    </item>
    <item>
      <title>Phase Transition of Spectral Fluctuations in Large Gram Matrices with a Variance Profile: A Unified Framework for Sparse CLTs</title>
      <link>https://arxiv.org/abs/2602.04302</link>
      <description>arXiv:2602.04302v1 Announce Type: new 
Abstract: We study the asymptotic spectral behavior of high-dimensional random Gram matrices with sparsity and a given variance profile, motivated by applications in wireless communication. Specifically, we consider the Gram matrices $\mathbf S_n=\mathbf Y_n\mathbf Y_n^*$, where the entries of $\mathbf Y_n$ are independent, centered, heteroscedastic, and sparse through Bernoulli masking. The sparsity level is parameterized as $s=q^2/n$, with $q$ ranging from polynomial order to order $n^{1/2}$.
  We investigate two asymptotic regimes in a high-dimensional framework: a moderate-sparsity regime with fixed $s\in(0,1]$, and a high-sparsity regime where $s\to0$. In both regimes, we establish the convergence of the empirical spectral distribution of $\mathbf S_n$ to a deterministic limit, and further derive central limit theorems for linear spectral statistics using resolvent techniques and martingale difference arguments. Our analysis reveals a phase transition in the fluctuation behavior across the two regimes. In the high-sparsity regime, the asymptotic fluctuations are governed by fourth-moment effects, with sparsity-scaled contributions being suppressed. Moreover, a mismatch between the scaling of the mean and variance, of different orders in $q$, necessitates an explicit correction in the centering of the linear spectral statistic. The theory applies to both Gaussian and non-Gaussian entries, and its statistical utility is illustrated through applications to hypothesis testing and outage probability analysis in large-scale MIMO systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04302v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Wang, Guangming Pan, Dandan Jiang</dc:creator>
    </item>
    <item>
      <title>Universality of General Spiked Tensor Models</title>
      <link>https://arxiv.org/abs/2602.04472</link>
      <description>arXiv:2602.04472v1 Announce Type: new 
Abstract: We study the rank-one spiked tensor model in the high-dimensional regime, where the noise entries are independent and identically distributed with zero mean, unit variance, and finite fourth moment.This setting extends the classical Gaussian framework to a substantially broader class of noise distributions.Focusing on asymmetric tensors of order $d$ ($\ge 3$), we analyze the maximum likelihood estimator of the best rank-one approximation.Under a mild assumption isolating informative critical points of the associated optimization landscape, we show that the empirical spectral distribution of a suitably defined block-wise tensor contraction converges almost surely to a deterministic limit that coincides with the Gaussian case.As a consequence, the asymptotic singular value and the alignments between the estimated and true spike directions admit explicit characterizations identical to those obtained under Gaussian noise. These results establish a universality principle for spiked tensor models, demonstrating that their high-dimensional spectral behavior and statistical limits are robust to non-Gaussian noise.
  Our analysis relies on resolvent methods from random matrix theory, cumulant expansions valid under finite moment assumptions, and variance bounds based on Efron-Stein-type arguments. A key challenge in the proof is how to handle the statistical dependence between the signal term and the noise term.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04472v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yanjin Xiang, Zhihua Zhang</dc:creator>
    </item>
    <item>
      <title>Estimation of reliability and accuracy of models of $\varphi$-sub-Gaussian process using generating functions of polynomial expansions</title>
      <link>https://arxiv.org/abs/2602.04668</link>
      <description>arXiv:2602.04668v1 Announce Type: new 
Abstract: Stochastic processes are often represented through orthonormal series expansions, a framework originating in the classical works of Lo\`eve and Karhunen and widely used for simulation and numerical approximation. While truncation error in such expansions has been extensively studied, practical models frequently involve an additional source of error arising from the approximation of coefficient functions when closed-form expressions are unavailable. The combined effect of these two errors remains insufficiently addressed in the literature. Building on the author's earlier work on reliability and accuracy estimates for $\varphi$-sub-Gaussian processes, this paper extends the methodology to orthonormal polynomial systems that do not possess normalized generating functions in analytical form, including the Legendre, generalized Laguerre, and Gegenbauer families. New bounds are derived for models in $L_p(T)$ and $C([0,T])$ that simultaneously account for truncation and coefficient approximation. The resulting criteria provide practical guidance for selecting the number of series terms required to achieve prescribed levels of reliability and accuracy across a broader class of polynomial-based stochastic process models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04668v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oleksandr Mokliachuk</dc:creator>
    </item>
    <item>
      <title>Statistical inference for the stochastic wave equation based on discrete observations</title>
      <link>https://arxiv.org/abs/2602.04708</link>
      <description>arXiv:2602.04708v1 Announce Type: new 
Abstract: The wave speed of a stochastic wave equation driven by Riesz noise on the unbounded multidimensional spatial domain is estimated based on discrete measurements. Central limit theorems for second-order variations of the observations in space, time, and space-time are established. Under general assumptions on the spatial and temporal sampling frequencies, the resulting method-of-moments estimators are asymptotically normally distributed. The covariance structure of the discrete increments admits a closed-form representation involving two different Fej\'er-type kernels, enabling a precise analysis of the interplay between spatial and temporal contributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04708v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anton Tiepner, Mathias Trabs, Eric Ziebell</dc:creator>
    </item>
    <item>
      <title>Adaptive estimation of Sobolev-type energy functionals on the sphere</title>
      <link>https://arxiv.org/abs/2602.04823</link>
      <description>arXiv:2602.04823v1 Announce Type: new 
Abstract: We study the estimation of quadratic Sobolev-type integral functionals of an unknown density on the unit sphere. The functional is defined through fractional powers of the Laplace--Beltrami operator and provides a global measure of smoothness and spectral energy. Our approach relies on spherical needlet frames, which yield a localized multiscale decomposition while preserving tight frame properties in the natural square-integrable function space on the sphere.
  We construct unbiased estimators of suitably truncated versions of the functional and derive sharp oracle risk bounds through an explicit bias--variance analysis. When the smoothness of the density is unknown, we propose a Lepski-type data-driven selection of the resolution level. The resulting adaptive estimator achieves minimax-optimal rates over Sobolev classes, without resorting to nonlinear or sparsity-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04823v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Claudio Durastanti</dc:creator>
    </item>
    <item>
      <title>Byzantine Machine Learning: MultiKrum and an optimal notion of robustness</title>
      <link>https://arxiv.org/abs/2602.03899</link>
      <description>arXiv:2602.03899v1 Announce Type: cross 
Abstract: Aggregation rules are the cornerstone of distributed (or federated) learning in the presence of adversaries, under the so-called Byzantine threat model. They are also interesting mathematical objects from the point of view of robust mean estimation. The Krum aggregation rule has been extensively studied, and endowed with formal robustness and convergence guarantees. Yet, MultiKrum, a natural extension of Krum, is often preferred in practice for its superior empirical performance, even though no theoretical guarantees were available until now. In this work, we provide the first proof that MultiKrum is a robust aggregation rule, and bound its robustness coefficient. To do so, we introduce $\kappa^\star$, the optimal *robustness coefficient* of an aggregation rule, which quantifies the accuracy of mean estimation in the presence of adversaries in a tighter manner compared with previously adopted notions of robustness. We then construct an upper and a lower bound on MultiKrum's robustness coefficient. As a by-product, we also improve on the best-known bounds on Krum's robustness coefficient. We show that MultiKrum's bounds are never worse than Krum's, and better in realistic regimes. We illustrate this analysis by an experimental investigation on the quality of the lower bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03899v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Gilles Bareilles, Wassim Bouaziz, Julien Fageot, El-Mahdi El-Mhamdi</dc:creator>
    </item>
    <item>
      <title>Tsallis Entropy derived from the Chaitin-Kolmogorov Informational Entropy</title>
      <link>https://arxiv.org/abs/2602.03919</link>
      <description>arXiv:2602.03919v1 Announce Type: cross 
Abstract: We provide a rigorous first-principle derivation of the non-additive Tsallis' entropy by employing the Chaitin-Kolmogorov algorithmic information theory. By applying non-local restrictive rules on the string formation (grammar), we show that the algorithmic cost follows a power-law of the string length, instead of the linear behaviour obtained in the classical theory. As a result, the Tsallis entropy governs the increase of information. We explore the result showing, through Landauer's limit, that the heat dissipation in systems with long-range correlations is diminished. The $\Omega_q$ number, which remains incompressible, now offers the possibility of a continuous increase of complexity, measured by the parameter $q$. We show the consistency of the results by a numerical simulation, and discuss Zipf's law in light of the new findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03919v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Airton Deppman</dc:creator>
    </item>
    <item>
      <title>Privacy utility trade offs for parameter estimation in degree heterogeneous higher order networks</title>
      <link>https://arxiv.org/abs/2602.03948</link>
      <description>arXiv:2602.03948v1 Announce Type: cross 
Abstract: In sensitive applications involving relational datasets, protecting information about individual links from adversarial queries is of paramount importance. In many such settings, the available data are summarized solely through the degrees of the nodes in the network. We adopt the $\beta$ model, which is the prototypical statistical model adopted for this form of aggregated relational information, and study the problem of minimax-optimal parameter estimation under both local and central differential privacy constraints. We establish finite sample minimax lower bounds that characterize the precise dependence of the estimation risk on the network size and the privacy parameters, and we propose simple estimators that achieve these bounds up to constants and logarithmic factors under both local and central differential privacy frameworks. Our results provide the first comprehensive finite sample characterization of privacy utility trade offs for parameter estimation in $\beta$ models, addressing the classical graph case and extending the analysis to higher order hypergraph models. We further demonstrate the effectiveness of our methods through experiments on synthetic data and a real world communication network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03948v1</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bibhabasu Mandal, Sagnik Nandy</dc:creator>
    </item>
    <item>
      <title>Statistical Guarantees for Reasoning Probes on Looped Boolean Circuits</title>
      <link>https://arxiv.org/abs/2602.03970</link>
      <description>arXiv:2602.03970v1 Announce Type: cross 
Abstract: We study the statistical behaviour of reasoning probes in a stylized model of looped reasoning, given by Boolean circuits whose computational graph is a perfect $\nu$-ary tree ($\nu\ge 2$) and whose output is appended to the input and fed back iteratively for subsequent computation rounds. A reasoning probe has access to a sampled subset of internal computation nodes, possibly without covering the entire graph, and seeks to infer which $\nu$-ary Boolean gate is executed at each queried node, representing uncertainty via a probability distribution over a fixed collection of $\mathtt{m}$ admissible $\nu$-ary gates. This partial observability induces a generalization problem, which we analyze in a realizable, transductive setting.
  We show that, when the reasoning probe is parameterized by a graph convolutional network (GCN)-based hypothesis class and queries $N$ nodes, the worst-case generalization error attains the optimal rate $\mathcal{O}(\sqrt{\log(2/\delta)}/\sqrt{N})$ with probability at least $1-\delta$, for $\delta\in (0,1)$. Our analysis combines snowflake metric embedding techniques with tools from statistical optimal transport. A key insight is that this optimal rate is achievable independently of graph size, owing to the existence of a low-distortion one-dimensional snowflake embedding of the induced graph metric. As a consequence, our results provide a sharp characterization of how structural properties of the computational graph govern the statistical efficiency of reasoning under partial access.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03970v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>math.MG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasis Kratsios, Giulia Livieri, A. Martina Neuman</dc:creator>
    </item>
    <item>
      <title>Functional Stochastic Localization</title>
      <link>https://arxiv.org/abs/2602.03999</link>
      <description>arXiv:2602.03999v1 Announce Type: cross 
Abstract: Eldan's stochastic localization is a probabilistic construction that has proved instrumental to modern breakthroughs in high-dimensional geometry and the design of sampling algorithms. Motivated by sampling under non-Euclidean geometries and the mirror descent algorithm in optimization, we develop a functional generalization of Eldan's process that replaces Gaussian regularization with regularization by any positive integer multiple of a log-Laplace transform. We further give a mixing time bound on the Markov chain induced by our localization process, which holds if our target distribution satisfies a functional Poincar\'e inequality. Finally, we apply our framework to differentially private convex optimization in $\ell_p$ norms for $p \in [1, 2)$, where we improve state-of-the-art query complexities in a zeroth-order model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.03999v1</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anming Gu, Bobby Shi, Kevin Tian</dc:creator>
    </item>
    <item>
      <title>A Note on Physical Dependence and Mixing Conditions for Triangular Arrays</title>
      <link>https://arxiv.org/abs/2602.04250</link>
      <description>arXiv:2602.04250v1 Announce Type: cross 
Abstract: Under mild structural assumptions and regularity conditions on the marginal and conditional densities, an explicit bound on the $\beta$-mixing coefficients in terms of the physical dependence measure is provided. Consequently, weak physical dependence implies $\beta$-mixing and strong mixing for triangular arrays, complementing Hill (2025), who proved the converse implication under moment assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04250v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Heinrichs</dc:creator>
    </item>
    <item>
      <title>Accurate and Efficient Approximation of the Null Distribution of Rao's Spacing Test</title>
      <link>https://arxiv.org/abs/2602.04318</link>
      <description>arXiv:2602.04318v1 Announce Type: cross 
Abstract: Rao's spacing test is a widely used nonparametric method for assessing uniformity on the circle. However, its broader applicability in practical settings has been limited because the null distribution is not easily calculated. As a result, practitioners have traditionally depended on pre-tabulated critical values computed for a limited set of sample sizes, which restricts the flexibility and generality of the method. In this paper, we address this limitation by recursively computing higher-order moments of the Rao's spacing test statistic and employing the Gram-Charlier expansion to derive an accurate approximation to its null distribution. This approach allows for the efficient and direct computation of p-values for arbitrary sample sizes, thereby eliminating the dependency on existing critical value tables. Moreover, we confirm that our method remains accurate and effective even for large sample sizes that are not represented in current tables, thus overcoming a significant practical limitation. Comparative evaluations with published critical values and saddlepoint approximations demonstrate that our method achieves a high degree of accuracy across a wide range of sample sizes. These findings greatly improve the practicality and usability of Rao's spacing test in both theoretical investigations and applied statistical analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04318v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshiki Kinoshita, Aya Shinozaki, Toshinari Kamakura</dc:creator>
    </item>
    <item>
      <title>Unit Shiha Distribution and its Applications to Engineering and Medical Data</title>
      <link>https://arxiv.org/abs/2602.04400</link>
      <description>arXiv:2602.04400v1 Announce Type: cross 
Abstract: There is a growing need for flexible statistical distributions that can accurately model data defined on the unit interval. This paper introduces a new unit distribution, termed the unit Shiha (USh) distribution, which is derived from the original Shiha (Sh) distribution through an inverse exponential transformation. The probability density function of the USh distribution is sufficiently flexible to model both left- and right-skewed data, while its hazard rate function is capable of capturing various failure-rate patterns, including increasing, bathtub-shaped, and J-shaped forms. Several statistical properties of the proposed distribution are investigated, including moments and related measures, the quantile function, entropy, and stress-strength reliability. Parameter estimation is carried out using the maximum likelihood method, and its performance is evaluated through a simulation study. The practical usefulness of the USh distribution is demonstrated using four real-life data sets, and its performance is compared with several well-known competing unit distributions. The comparative results indicate that the proposed model fits the data better than the competitive models applied in this study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04400v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>F. A. Shiha</dc:creator>
    </item>
    <item>
      <title>Performative Learning Theory</title>
      <link>https://arxiv.org/abs/2602.04402</link>
      <description>arXiv:2602.04402v1 Announce Type: cross 
Abstract: Performative predictions influence the very outcomes they aim to forecast. We study performative predictions that affect a sample (e.g., only existing users of an app) and/or the whole population (e.g., all potential app users). This raises the question of how well models generalize under performativity. For example, how well can we draw insights about new app users based on existing users when both of them react to the app's predictions? We address this question by embedding performative predictions into statistical learning theory. We prove generalization bounds under performative effects on the sample, on the population, and on both. A key intuition behind our proofs is that in the worst case, the population negates predictions, while the sample deceptively fulfills them. We cast such self-negating and self-fulfilling predictions as min-max and min-min risk functionals in Wasserstein space, respectively. Our analysis reveals a fundamental trade-off between performatively changing the world and learning from it: the more a model affects data, the less it can learn from it. Moreover, our analysis results in a surprising insight on how to improve generalization guarantees by retraining on performatively distorted samples. We illustrate our bounds in a case study on prediction-informed assignments of unemployed German residents to job trainings, drawing upon administrative labor market records from 1975 to 2017 in Germany.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04402v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian Rodemann, Unai Fischer-Abaigar, James Bailie, Krikamol Muandet</dc:creator>
    </item>
    <item>
      <title>Locally Gentle State Certification for High Dimensional Quantum Systems</title>
      <link>https://arxiv.org/abs/2602.04550</link>
      <description>arXiv:2602.04550v1 Announce Type: cross 
Abstract: Standard approaches to quantum statistical inference rely on measurements that induce a collapse of the wave function, effectively consuming the quantum state to extract information. In this work, we investigate the fundamental limits of \emph{locally-gentle} quantum state certification, where the learning algorithm is constrained to perturb the state by at most $\alpha$ in trace norm, thereby allowing for the reuse of samples. We analyze the hypothesis testing problem of distinguishing whether an unknown state $\rho$ is equal to a reference $\rho_0$ or $\epsilon$-far from it. We derive the minimax sample complexity for this problem, quantifying the information-theoretic price of non-destructive measurements. Specifically, by constructing explicit measurement operators, we show that the constraint of $\alpha$-gentleness imposes a sample size penalty of $\frac{d}{\alpha^2}$, yielding a total sample complexity of $n = \Theta(\frac{d^3}{\epsilon^2 \alpha^2})$. Our results clarify the trade-off between information extraction and state disturbance, and highlight deep connections between physical measurement constraints and privacy mechanisms in quantum learning. Crucially, we find that the sample size penalty incurred by enforcing $\alpha$-gentleness scales linearly with the Hilbert-space dimension $d$ rather than the number of parameters $d^2-1$ typical for high-dimensional private estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.04550v1</guid>
      <category>quant-ph</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cristina Butucea, Jan Johannes, Henning Stein</dc:creator>
    </item>
    <item>
      <title>The German Tank Problem with Multiple Factories</title>
      <link>https://arxiv.org/abs/2403.14881</link>
      <description>arXiv:2403.14881v4 Announce Type: replace 
Abstract: During the Second World War, estimates of the number of tanks deployed by Germany were critically needed. The Allies adopted a successful statistical approach to estimate this information: assume that the tanks are sequentially numbered starting from, say, 1, and ending at an unknown positive integer $N$. If we observe the numbers of $k$ tanks, then the best linear unbiased estimator for $N$ is $M(1+1/k)-1$ where $M$ is the maximum observed serial number. While this approach was successful, there are many more adversarial situations where the approach for the original German Tank Problem falls short. Typically the number of ``factories'' is a possibly unknown $l&gt;1$, and tanks produced by different factories may have serial numbers in disjoint ranges that are often separated by unknown amounts.
  Clark, Gonye and Miller (CGM) presented an unbiased estimator for $N$ when the minimum serial number is unknown. So if one can identify which samples correspond to which factory, one can then estimate each factory's range using CGM's method, and sum them for an estimate of the rival's total productivity. We present a procedure to estimate the total productivity and prove that it is effective when $\log l/\log k$ is sufficiently small. In the final section, we show that if we have a small number of samples, we can make an estimator that performs orders of magnitude better when given additional information about the size of the gaps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14881v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Steven J. Miller, Kishan Sharma, Andrew K. Yang</dc:creator>
    </item>
    <item>
      <title>Convex relaxation for the generalized maximum-entropy sampling problem</title>
      <link>https://arxiv.org/abs/2404.01390</link>
      <description>arXiv:2404.01390v4 Announce Type: replace 
Abstract: The generalized maximum-entropy sampling problem (GMESP) is to select an order-$s$ principal submatrix from an order-$n$ covariance matrix, to maximize the product of its $t$ greatest eigenvalues, $0&lt;t\leq s &lt;n$. Introduced more than 25 years ago, GMESP is a natural generalization of two fundamental problems in statistical design theory: (i) maximum-entropy sampling problem (MESP); (ii) binary D-optimality (D-Opt). In the general case, it can be motivated by a selection problem in the context of principal component analysis (PCA).
  We introduce the first convex-optimization based relaxation for GMESP, study its behavior, compare it to an earlier spectral bound, and demonstrate its use in a branch-and-bound scheme. We find that such an approach is practical when $s-t$ is very small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01390v4</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Ponte, Marcia Fampa, Jon Lee</dc:creator>
    </item>
    <item>
      <title>P-Tensors: a General Formalism for Constructing Higher Order Message Passing Networks</title>
      <link>https://arxiv.org/abs/2306.10767</link>
      <description>arXiv:2306.10767v2 Announce Type: replace-cross 
Abstract: Several recent papers have proposed increasing the expressive power of graph neural networks by exploiting subgraphs or other topological structures. In parallel, researchers have investigated higher order permutation equivariant networks. In this paper we tie these two threads together by providing a general framework for higher order permutation equivariant message passing in subgraph neural networks. In this paper we introduce a new type of mathematical object called $P$-tensors, which provide a simple way to define the most general form of permutation equivariant message passing in both the above two categories of networks. We show that the P-Tensors paradigm can achieve state-of-the-art performance on benchmark molecular datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10767v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proc. AISTATS, PMLR 238:424-432, 2024</arxiv:journal_reference>
      <dc:creator>Andrew Hands, Tianyi Sun, Risi Kondor</dc:creator>
    </item>
    <item>
      <title>On the Mean-Field limit of diffusive games through the master equation: $L^{\infty}$ estimates and extreme value behavior</title>
      <link>https://arxiv.org/abs/2410.18869</link>
      <description>arXiv:2410.18869v3 Announce Type: replace-cross 
Abstract: We consider an $N$-player game where the states of the players evolve with time as Stochastic Differential Equations (SDEs) with interaction only in the drift terms. Each player controls the drift of the SDE satisfied by her state process, aiming to minimize the expected value of a cost that depends on the paths of the player's state and the empirical measure of the states of all the players until a terminal time. When $N \to \infty$, previous works have established Central Limit Theorems and Large Deviation Principles for the state processes when the game is in Nash Equilibrium (the Nash states), by using the Master Equation to construct approximations of those processes that evolve with time as SDEs with classical Mean-Field interaction. Staying in this framework, we improve an existing $L^{1}$ estimate for the total error of approximating all the Nash states to an $L^{\infty}$ one, and we also establish the $N \to \infty$ asymptotic behavior of the upper order statistics of the Nash states. The latter initiates the development of an Extreme Value Theory for Stochastic Differential Games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18869v3</guid>
      <category>math.PR</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>q-fin.MF</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erhan Bayraktar, Nikolaos Kolliopoulos</dc:creator>
    </item>
    <item>
      <title>Minimax and Bayes Optimal Best-Arm Identification</title>
      <link>https://arxiv.org/abs/2506.24007</link>
      <description>arXiv:2506.24007v4 Announce Type: replace-cross 
Abstract: This study investigates minimax and Bayes optimal strategies for fixed-budget best-arm identification. We consider an adaptive procedure consisting of a sampling phase followed by a recommendation phase, and we design an adaptive experiment within this framework to efficiently identify the best arm, defined as the one with the highest expected outcome. In our proposed strategy, the sampling phase consists of two stages. The first stage is a pilot phase, in which we allocate samples uniformly across arms to eliminate clearly suboptimal arms and to estimate outcome variances. Before entering the second stage, we solve a Gaussian minimax game, which yields a sampling ratio and a decision rule. In the second stage, samples are allocated according to this sampling ratio. After the sampling phase, the procedure enters the recommendation phase, where we select an arm using the decision rule. We prove that this single strategy is simultaneously asymptotically minimax and Bayes optimal for the simple regret, and we establish upper bounds that coincide exactly with our lower bounds, including the constant terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24007v4</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>DP-SPRT: Differentially Private Sequential Probability Ratio Tests</title>
      <link>https://arxiv.org/abs/2508.06377</link>
      <description>arXiv:2508.06377v2 Announce Type: replace-cross 
Abstract: We revisit Wald's celebrated Sequential Probability Ratio Test for sequential tests of two simple hypotheses, under privacy constraints. We propose DP-SPRT, a wrapper that can be calibrated to achieve desired error probabilities and privacy constraints, addressing a significant gap in previous work. DP-SPRT relies on a private mechanism that processes a sequence of queries and stops after privately determining when the query results fall outside a predefined interval. This OutsideInterval mechanism improves upon naive composition of existing techniques like AboveThreshold, achieving a factor-of-2 privacy improvement and thus potentially benefiting other continual monitoring procedures. We prove generic upper bounds on the error and sample complexity of DP-SPRT that can accommodate various noise distributions based on the practitioner's privacy needs. We exemplify them in two settings: Laplace noise (pure Differential Privacy) and Gaussian noise (R\'enyi differential privacy). In the former setting, by providing a lower bound on the sample complexity of any $\varepsilon$-DP test with prescribed type I and type II errors, we show that DP-SPRT is near optimal when both errors are small and the two hypotheses are close. Moreover, we conduct an experimental study revealing its good practical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06377v2</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Michel, Debabrota Basu, Emilie Kaufmann</dc:creator>
    </item>
    <item>
      <title>When Do Credal Sets Stabilize? Fixed-Point Theorems for Credal Set Updates</title>
      <link>https://arxiv.org/abs/2510.04769</link>
      <description>arXiv:2510.04769v2 Announce Type: replace-cross 
Abstract: Many machine learning algorithms rely on iterative updates of uncertainty representations, ranging from variational inference and expectation-maximization, to reinforcement learning, continual learning, and multi-agent learning. In the presence of imprecision and ambiguity, credal sets -- closed, convex sets of probability distributions -- have emerged as a popular framework for representing imprecise probabilistic beliefs. Under such imprecision, many learning problems in imprecise probabilistic machine learning (IPML) may be viewed as processes involving successive applications of update rules on credal sets. This naturally raises the question of whether this iterative process converges to stable fixed points -- or, more generally, under what conditions on the updating mechanism such fixed points exist, and whether they can be attained. We provide the first analysis of this problem, and illustrate our findings using Credal Bayesian Deep Learning as a concrete example. Our work demonstrates that incorporating imprecision into the learning process not only enriches the representation of uncertainty, but also reveals structural conditions under which stability emerges, thereby offering new insights into the dynamics of iterative learning under imprecision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04769v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michele Caprio, Siu Lun Chau, Krikamol Muandet</dc:creator>
    </item>
    <item>
      <title>Inference-Time Alignment for Diffusion Models via Variationally Stable Doob's Matching</title>
      <link>https://arxiv.org/abs/2601.06514</link>
      <description>arXiv:2601.06514v2 Announce Type: replace-cross 
Abstract: Inference-time alignment for diffusion models aims to adapt a pre-trained reference diffusion model toward a target distribution without retraining the reference score network, thereby preserving the generative capacity of the reference model while enforcing desired properties at the inference time. A central mechanism for achieving such alignment is guidance, which modifies the sampling dynamics through an additional drift term. In this work, we introduce variationally stable Doob's matching, a novel framework for provable guidance estimation grounded in Doob's $h$-transform. Our approach formulates guidance as the gradient of logarithm of an underlying Doob's $h$-function and employs gradient-regularized regression to simultaneously estimate both the $h$-function and its gradient, resulting in a consistent estimator of the guidance. Theoretically, we establish non-asymptotic convergence rates for the estimated guidance. Moreover, we analyze the resulting controllable diffusion processes and prove non-asymptotic convergence guarantees for the generated distributions in the 2-Wasserstein distance. Finally, we show that variationally stable guidance estimators are adaptive to unknown low dimensionality, effectively mitigating the curse of dimensionality under low-dimensional subspace assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06514v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Chenguang Duan, Yuling Jiao, Yi Xu, Jerry Zhijian Yang</dc:creator>
    </item>
    <item>
      <title>Low-Dimensional Adaptation of Rectified Flow: A Diffusion and Stochastic Localization Perspective</title>
      <link>https://arxiv.org/abs/2601.15500</link>
      <description>arXiv:2601.15500v2 Announce Type: replace-cross 
Abstract: In recent years, Rectified flow (RF) has gained considerable popularity largely due to its generation efficiency and state-of-the-art performance. In this paper, we investigate the degree to which RF automatically adapts to the intrinsic low dimensionality of the support of the target distribution to accelerate sampling. We show that, using a carefully designed choice of the time-discretization scheme and with sufficiently accurate drift estimates, the RF sampler enjoys an iteration complexity of order $O(k/\varepsilon)$ (up to log factors), where $\varepsilon$ is the precision in total variation distance and $k$ is the intrinsic dimension of the target distribution. In addition, we show that the denoising diffusion probabilistic model (DDPM) procedure is equivalent to a stochastic version of RF by establishing a novel connection between these processes and stochastic localization. Building on this connection, we further design a stochastic RF sampler that also adapts to the low-dimensionality of the target distribution under milder requirements on the accuracy of the drift estimates, and also with a specific time schedule. We illustrate with simulations on the synthetic data and text-to-image data experiments the improved performance of the proposed samplers implementing the newly designed time-discretization schedules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15500v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saptarshi Roy, Alessandro Rinaldo, Purnamrita Sarkar</dc:creator>
    </item>
  </channel>
</rss>
