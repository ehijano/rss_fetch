<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Apr 2024 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Fine-grained Classification of Subquadratic Patterns for Subgraph Listing and Friends</title>
      <link>https://arxiv.org/abs/2404.04369</link>
      <description>arXiv:2404.04369v1 Announce Type: new 
Abstract: In an $m$-edge host graph $G$, all triangles can be listed in time $O(m^{1.5})$ [Itai, Rodeh '78], and all $k$-cycles can be listed in time $O(m^{2-1/{\lceil k/2 \rceil}} + t)$ where $t$ is the output size [Alon, Yuster, Zwick '97]. These classic results also hold for the colored problem variant, where the nodes of the host graph $G$ are colored by nodes in the pattern graph $H$, and we are only interested in subgraphs of $G$ that are isomorphic to the pattern $H$ and respect the colors. We study the problem of listing all $H$-subgraphs in the colored setting, for fixed pattern graphs $H$.
  As our main result, we determine all pattern graphs $H$ such that all $H$-subgraphs can be listed in subquadratic time $O(m^{2-\varepsilon} + t)$, where $t$ is the output size. Moreover, for each such subquadratic pattern $H$ we determine the smallest exponent $c(H)$ such that all $H$-subgraphs can be listed in time $O(m^{c(H)} + t)$. This is a vast generalization of the classic results on triangles and cycles.
  To prove this result, we design new listing algorithms and prove conditional lower bounds based on standard hypotheses from fine-grained complexity theory. In our algorithms, we use a new ingredient that we call hyper-degree splitting, where we split tuples of nodes into high degree and low degree depending on their number of common neighbors.
  We also show the same results for two related problems: finding an $H$-subgraph of minimum total edge-weight in time $O(m^{c(H)})$, and enumerating all $H$-subgraphs in $O(m^{c(H)})$ preprocessing time and constant delay. Again we determine all pattern graphs $H$ that have complexity $c(H) &lt; 2$, and for each such subquadratic pattern we determine the optimal complexity $c(H)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04369v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karl Bringmann, Egor Gorbachev</dc:creator>
    </item>
    <item>
      <title>Fast and Simple Sorting Using Partial Information</title>
      <link>https://arxiv.org/abs/2404.04552</link>
      <description>arXiv:2404.04552v1 Announce Type: new 
Abstract: We consider the problem of sorting a set of items having an unknown total order by doing binary comparisons of the items, given the outcomes of some pre-existing comparisons. We present a simple algorithm with a running time of $O(m+n+\log T)$, where $n$, $m$, and $T$ are the number of items, the number of pre-existing comparisons, and the number of total orders consistent with the outcomes of the pre-existing comparisons, respectively. The algorithm does $O(\log T)$ comparisons.
  Our running time and comparison bounds are best possible up to constant factors, thus resolving a problem that has been studied intensely since 1976 (Fredman, Theoretical Computer Science). The best previous algorithm with a bound of $O(\log T)$ on the number of comparisons has a time bound of $O(n^{2.5})$ and is significantly more complicated. Our algorithm combines three classic algorithms: topological sort, heapsort with the right kind of heap, and efficient insertion into a sorted list.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04552v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bernhard Haeupler, Richard Hlad\'ik, John Iacono, Vaclav Rozhon, Robert Tarjan, Jakub T\v{e}tek</dc:creator>
    </item>
    <item>
      <title>Spectral Independence Beyond Total Influence on Trees and Related Graphs</title>
      <link>https://arxiv.org/abs/2404.04668</link>
      <description>arXiv:2404.04668v1 Announce Type: new 
Abstract: We study how to establish $\textit{spectral independence}$, a key concept in sampling, without relying on total influence bounds, by applying an $\textit{approximate inverse}$ of the influence matrix. Our method gives constant upper bounds on spectral independence for two foundational Gibbs distributions known to have unbounded total influences:
  $\bullet$ The monomer-dimer model on graphs with large girth (including trees). Prior to our work, such results were only known for graphs with constant maximum degrees or infinite regular trees, as shown by Chen, Liu, and Vigoda (STOC '21).
  $\bullet$ The hardcore model on trees with fugacity $\lambda &lt; \mathrm{e}^2$. This remarkably surpasses the well-known $\lambda_r&gt;\mathrm{e}-1$ lower bound for the reconstruction threshold on trees, significantly improving upon the current threshold $\lambda &lt; 1.3$, established in a prior work by Efthymiou, Hayes, \v{S}tefankovi\v{c}, and Vigoda (RANDOM '23).
  Consequently, we establish optimal $\Omega(n^{-1})$ spectral gaps of the Glauber dynamics for these models on arbitrary trees, regardless of the maximum degree $\Delta$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04668v1</guid>
      <category>cs.DS</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyu Chen, Xiongxin Yang, Yitong Yin, Xinyuan Zhang</dc:creator>
    </item>
    <item>
      <title>Approximating Unrelated Machine Weighted Completion Time Using Iterative Rounding and Computer Assisted Proofs</title>
      <link>https://arxiv.org/abs/2404.04773</link>
      <description>arXiv:2404.04773v1 Announce Type: new 
Abstract: We revisit the unrelated machine scheduling problem with the weighted completion time objective. It is known that independent rounding achieves a 1.5 approximation for the problem, and many prior algorithms improve upon this ratio by leveraging strong negative correlation schemes. On each machine $i$, these schemes introduce strong negative correlation between events that some pairs of jobs are assigned to $i$, while maintaining non-positive correlation for all pairs.
  Our algorithm deviates from this methodology by relaxing the pairwise non-positive correlation requirement. On each machine $i$, we identify many groups of jobs. For a job $j$ and a group $B$ not containing $j$, we only enforce non-positive correlation between $j$ and the group as a whole, allowing $j$ to be positively-correlated with individual jobs in $B$. This relaxation suffices to maintain the 1.5-approximation, while enabling us to obtain a much stronger negative correlation within groups using an iterative rounding procedure: at most one job from each group is scheduled on $i$.
  We prove that the algorithm achieves a $(1.36 + \epsilon)$-approximation, improving upon the previous best approximation ratio of $1.4$ due to Harris. While the improvement may not be substantial, the significance of our contribution lies in the relaxed non-positive correlation condition and the iterative rounding framework. Due to the simplicity of our algorithm, we are able to derive a closed form for the weighted completion time our algorithm achieves with a clean analysis. Unfortunately, we could not provide a good analytical analysis for the quantity; instead, we rely on a computer assisted proof.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04773v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shi Li</dc:creator>
    </item>
    <item>
      <title>Range Longest Increasing Subsequence and its Relatives: Beating Quadratic Barrier and Approaching Optimality</title>
      <link>https://arxiv.org/abs/2404.04795</link>
      <description>arXiv:2404.04795v1 Announce Type: new 
Abstract: In this work, we present a plethora of results for the range longest increasing subsequence problem (Range-LIS) and its variants. The input to Range-LIS is a sequence $\mathcal{S}$ of $n$ real numbers and a collection $\mathcal{Q}$ of $m$ query ranges and for each query in $\mathcal{Q}$, the goal is to report the LIS of the sequence $\mathcal{S}$ restricted to that query. Our two main results are for the following generalizations of the Range-LIS problem:
  $\bullet$ 2D Range Queries: In this variant of the Range-LIS problem, each query is a pair of ranges, one of indices and the other of values, and we provide an algorithm with running time $\tilde{O}(mn^{1/2}+ n^{3/2} +k)$, where $k$ is the cumulative length of the $m$ output subsequences. This breaks the quadratic barrier of $\tilde{O}(mn)$ when $m=\Omega(\sqrt{n})$. Previously, the only known result breaking the quadratic barrier was of Tiskin [SODA'10] which could only handle 1D range queries (i.e., each query was a range of indices) and also just outputted the length of the LIS (instead of reporting the subsequence achieving that length).
  $\bullet$ Colored Sequences: In this variant of the Range-LIS problem, each element in $\mathcal{S}$ is colored and for each query in $\mathcal{Q}$, the goal is to report a monochromatic LIS contained in the sequence $\mathcal{S}$ restricted to that query. For 2D queries, we provide an algorithm for this colored version with running time $\tilde{O}(mn^{2/3}+ n^{5/3} +k)$. Moreover, for 1D queries, we provide an improved algorithm with running time $\tilde{O}(mn^{1/2}+ n^{3/2} +k)$. Thus, we again break the quadratic barrier of $\tilde{O}(mn)$. Additionally, we prove that assuming the well-known Combinatorial Boolean Matrix Multiplication Hypothesis, that the runtime for 1D queries is essentially tight for combinatorial algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04795v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karthik C. S., Saladi Rahul</dc:creator>
    </item>
    <item>
      <title>Chromatic number in $1.9999^n$ time? Fast deterministic set partitioning under the asymptotic rank conjecture</title>
      <link>https://arxiv.org/abs/2404.04987</link>
      <description>arXiv:2404.04987v1 Announce Type: new 
Abstract: In this paper we further explore the recently discovered connection by Bj\"{o}rklund and Kaski [STOC 2024] and Pratt [STOC 2024] between the asymptotic rank conjecture of Strassen [Progr. Math. 1994] and the three-way partitioning problem. We show that under the asymptotic rank conjecture, the chromatic number of an $n$-vertex graph can be computed deterministically in $O(1.99982^n)$ time, thus giving a conditional answer to a question of Zamir [ICALP 2021], and questioning the optimality of the $2^n\operatorname{poly}(n)$ time algorithm for chromatic number by Bj\"{o}rklund, Husfeldt, and Koivisto [SICOMP 2009].
  Our technique is a combination of earlier algorithms for detecting $k$-colorings for small $k$ and enumerating $k$-colorable subgraphs, with an extension and derandomisation of Pratt's tensor-based algorithm for balanced three-way partitioning to the unbalanced case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04987v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Bj\"orklund, Radu Curticapean, Thore Husfeldt, Petteri Kaski, Kevin Pratt</dc:creator>
    </item>
    <item>
      <title>Directed Buy-at-Bulk Spanners</title>
      <link>https://arxiv.org/abs/2404.05172</link>
      <description>arXiv:2404.05172v1 Announce Type: new 
Abstract: We present a framework that unifies directed buy-at-bulk network design and directed spanner problems, namely, \emph{buy-at-bulk spanners}. The goal is to find a minimum-cost routing solution for network design problems that capture economies at scale, while satisfying demands and distance constraints for terminal pairs. A more restricted version of this problem was shown to be $O(2^{{\log^{1-\ep} n}})$-hard to approximate, where $n$ is the number of vertices, under a standard complexity assumption, due to Elkin and Peleg (Theory of Computing Systems, 2007).
  To the best of our knowledge, our results are the first sublinear factor approximation algorithms for directed buy-at-bulk spanners. Furthermore, these results hold even when we allow the edge lengths to be \emph{negative}, unlike the previous literature for spanners. Our approximation ratios match the state-of-the-art ratios in special cases, namely, buy-at-bulk network design by Antonakopoulos (WAOA, 2010) and weighted spanners by Grigorescu, Kumar, and Lin (APPROX 2023).
  Our results are based on new approximation algorithms for the following two problems that are of independent interest: \emph{minimum-density distance-constrained junction trees} and \emph{resource-constrained shortest path with negative consumption}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05172v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elena Grigorescu, Nithish Kumar, Young-San Lin</dc:creator>
    </item>
    <item>
      <title>Scheduling Multi-Server Jobs is Not Easy</title>
      <link>https://arxiv.org/abs/2404.05271</link>
      <description>arXiv:2404.05271v1 Announce Type: new 
Abstract: The problem of online scheduling of multi-server jobs is considered, where there are a total of $K$ servers, and each job requires concurrent service from multiple servers for it to be processed. Each job on its arrival reveals its processing time, the number of servers from which it needs concurrent service and an online algorithm has to make scheduling decisions using only causal information, with the goal of minimizing the response/flow time. The worst case input model is considered and the performance metric is the competitive ratio. For the case, when all job processing time (sizes) are the same, we show that the competitive ratio of any deterministic/randomized algorithm is at least $\Omega(K)$ and propose an online algorithm whose competitive ratio is at most $K+1$. With equal job sizes, we also consider the resource augmentation regime where an online algorithm has access to more servers than an optimal offline algorithm. With resource augmentation, we propose a simple algorithm and show that it has a competitive ratio of $1$ when provided with $2K$ servers with respect to an optimal offline algorithm with $K$ servers. With unequal job sizes, we propose an online algorithm whose competitive ratio is at most $2K \log (K w_{\max})$, where $w_{\max}$ is the maximum size of any job.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05271v1</guid>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rahul Vaze</dc:creator>
    </item>
    <item>
      <title>Combinatorial Correlation Clustering</title>
      <link>https://arxiv.org/abs/2404.05433</link>
      <description>arXiv:2404.05433v1 Announce Type: new 
Abstract: Correlation Clustering is a classic clustering objective arising in numerous machine learning and data mining applications. Given a graph $G=(V,E)$, the goal is to partition the vertex set into clusters so as to minimize the number of edges between clusters plus the number of edges missing within clusters.
  The problem is APX-hard and the best known polynomial time approximation factor is 1.73 by Cohen-Addad, Lee, Li, and Newman [FOCS'23]. They use an LP with $|V|^{1/\epsilon^{\Theta(1)}}$ variables for some small $\epsilon$. However, due to the practical relevance of correlation clustering, there has also been great interest in getting more efficient sequential and parallel algorithms.
  The classic combinatorial pivot algorithm of Ailon, Charikar and Newman [JACM'08] provides a 3-approximation in linear time. Like most other algorithms discussed here, this uses randomization. Recently, Behnezhad, Charikar, Ma and Tan [FOCS'22] presented a $3+\epsilon$-approximate solution for solving problem in a constant number of rounds in the Massively Parallel Computation (MPC) setting. Very recently, Cao, Huang, Su [SODA'24] provided a 2.4-approximation in a polylogarithmic number of rounds in the MPC model and in $\tilde{O}(|E|^{1.5})$ time in the classic sequential setting. They asked whether it is possible to get a better than 3-approximation in near-linear time?
  We resolve this problem with an efficient combinatorial algorithm providing a drastically better approximation factor. It achieves a $\sim 2-2/13 &lt; 1.847$-approximation in sub-linear ($\tilde O(|V|)$) sequential time or in sub-linear ($\tilde O(|V|)$) space in the streaming setting, and it uses only a constant number of rounds in the MPC model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05433v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Cohen-Addad, David Rasmussen Lolck, Marcin Pilipczuk, Mikkel Thorup, Shuyi Yan, Hanwen Zhang</dc:creator>
    </item>
    <item>
      <title>BFS versus DFS for random targets in ordered trees</title>
      <link>https://arxiv.org/abs/2404.05664</link>
      <description>arXiv:2404.05664v1 Announce Type: new 
Abstract: Consider a search from the root of an ordered tree with $n$ edges to some target node at a fixed distance $\ell$ from that root. We compare the average time complexity of the breadth-first search (BFS) and depth-first search (DFS) algorithms, when the target node is selected uniformly at random among all nodes at level $\ell$ in the ordered trees with $n$ edges. Intuition suggests that BFS should have better average performance when $\ell$ is small, while DFS must have an advantage when $\ell$ is large. But where exactly is the threshold, as a function of $n$, and is it unique? We obtain explicit formulas for the expected number of steps of both BFS and DFS, by using results on the occupation measure of Brownian excursions, as well as a combinatorial proof of an identity related to lattice paths. This allows us to show that there exists a unique constant $\lambda\approx 0.789004$, such that in expectation BFS is asymptotically faster than DFS if and only if $\ell\leq \lambda\sqrt{n}$. Furthermore, we find the asymptotic average time complexity of BFS in the given setting for any class of Galton$\unicode{x2013}$Watson trees, including binary trees and ordered trees. Finally, we introduce the truncated DFS algorithm, which performs better than both BFS and DFS when $\ell$ is known in advance, and we find a formula evaluating the average time complexity of this algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05664v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stoyan Dimitrov, Martin Minchev, Yan Zhuang</dc:creator>
    </item>
    <item>
      <title>Even Faster Knapsack via Rectangular Monotone Min-Plus Convolution and Balancing</title>
      <link>https://arxiv.org/abs/2404.05681</link>
      <description>arXiv:2404.05681v1 Announce Type: new 
Abstract: We present a pseudopolynomial-time algorithm for the Knapsack problem that has running time $\widetilde{O}(n + t\sqrt{p_{\max}})$, where $n$ is the number of items, $t$ is the knapsack capacity, and $p_{\max}$ is the maximum item profit. This improves over the $\widetilde{O}(n + t \, p_{\max})$-time algorithm based on the convolution and prediction technique by Bateni et al.~(STOC 2018). Moreover, we give some evidence, based on a strengthening of the Min-Plus Convolution Hypothesis, that our running time might be optimal.
  Our algorithm uses two new technical tools, which might be of independent interest. First, we generalize the $\widetilde{O}(n^{1.5})$-time algorithm for bounded monotone min-plus convolution by Chi et al.~(STOC 2022) to the \emph{rectangular} case where the range of entries can be different from the sequence length. Second, we give a reduction from general knapsack instances to \emph{balanced} instances, where all items have nearly the same profit-to-weight ratio, up to a constant factor.
  Using these techniques, we can also obtain algorithms that run in time $\widetilde{O}(n + OPT\sqrt{w_{\max}})$, $\widetilde{O}(n + (nw_{\max}p_{\max})^{1/3}t^{2/3})$, and $\widetilde{O}(n + (nw_{\max}p_{\max})^{1/3} OPT^{2/3})$, where $OPT$ is the optimal total profit and $w_{\max}$ is the maximum item weight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05681v1</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karl Bringmann, Anita D\"urr, Adam Polak</dc:creator>
    </item>
    <item>
      <title>Quantum Speedup for Some Geometric 3SUM-Hard Problems and Beyond</title>
      <link>https://arxiv.org/abs/2404.04535</link>
      <description>arXiv:2404.04535v1 Announce Type: cross 
Abstract: The classical 3SUM conjecture states that the class of 3SUM-hard problems does not admit a truly subquadratic $O(n^{2-\delta})$-time algorithm, where $\delta &gt;0$, in classical computing. The geometric 3SUM-hard problems have widely been studied in computational geometry and recently, these problems have been examined under the quantum computing model. For example, Ambainis and Larka [TQC'20] designed a quantum algorithm that can solve many geometric 3SUM-hard problems in $O(n^{1+o(1)})$-time, whereas Buhrman [ITCS'22] investigated lower bounds under quantum 3SUM conjecture that claims there does not exist any sublinear $O(n^{1-\delta})$-time quantum algorithm for the 3SUM problem. The main idea of Ambainis and Larka is to formulate a 3SUM-hard problem as a search problem, where one needs to find a point with a certain property over a set of regions determined by a line arrangement in the plane. The quantum speed-up then comes from the application of the well-known quantum search technique called Grover search over all regions.
  This paper further generalizes the technique of Ambainis and Larka for some 3SUM-hard problems when a solution may not necessarily correspond to a single point or the search regions do not immediately correspond to the subdivision determined by a line arrangement. Given a set of $n$ points and a positive number $q$, we design $O(n^{1+o(1)})$-time quantum algorithms to determine whether there exists a triangle among these points with an area at most $q$ or a unit disk that contains at least $q$ points. We also give an $O(n^{1+o(1)})$-time quantum algorithm to determine whether a given set of intervals can be translated so that it becomes contained in another set of given intervals and discuss further generalizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04535v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. Mark Keil, Fraser McLeod, Debajyoti Mondal</dc:creator>
    </item>
    <item>
      <title>Aleph Filter: To Infinity in Constant Time</title>
      <link>https://arxiv.org/abs/2404.04703</link>
      <description>arXiv:2404.04703v1 Announce Type: cross 
Abstract: Filter data structures are widely used in various areas of computer science to answer approximate set-membership queries. In many applications, the data grows dynamically, requiring their filters to expand along with the data that they represent. However, existing methods for expanding filters cannot maintain stable performance, memory footprint, and false positive rate at the same time. We address this problem with Aleph Filter, which makes the following contributions. (1) It supports all operations (insertions, queries, deletes, etc.) in constant time, no matter how much the data grows. (2) Given any rough estimate of how much the data will ultimately grow, Aleph Filter provides far superior memory vs. false positive rate trade-offs, even if the estimate is off by orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04703v1</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niv Dayan, Ioana Bercea, Rasmus Pagh</dc:creator>
    </item>
    <item>
      <title>Faster Algorithms for Fair Max-Min Diversification in $\mathbb{R}^d$</title>
      <link>https://arxiv.org/abs/2404.04713</link>
      <description>arXiv:2404.04713v1 Announce Type: cross 
Abstract: The task of extracting a diverse subset from a dataset, often referred to as maximum diversification, plays a pivotal role in various real-world applications that have far-reaching consequences. In this work, we delve into the realm of fairness-aware data subset selection, specifically focusing on the problem of selecting a diverse set of size $k$ from a large collection of $n$ data points (FairDiv).
  The FairDiv problem is well-studied in the data management and theory community. In this work, we develop the first constant approximation algorithm for FairDiv that runs in near-linear time using only linear space. In contrast, all previously known constant approximation algorithms run in super-linear time (with respect to $n$ or $k$) and use super-linear space. Our approach achieves this efficiency by employing a novel combination of the Multiplicative Weight Update method and advanced geometric data structures to implicitly and approximately solve a linear program. Furthermore, we improve the efficiency of our techniques by constructing a coreset. Using our coreset, we also propose the first efficient streaming algorithm for the FairDiv problem whose efficiency does not depend on the distribution of data points. Empirical evaluation on million-sized datasets demonstrates that our algorithm achieves the best diversity within a minute. All prior techniques are either highly inefficient or do not generate a good solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04713v1</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3654940</arxiv:DOI>
      <arxiv:journal_reference>SIGMOD 2024</arxiv:journal_reference>
      <dc:creator>Yash Kurkure, Miles Shamo, Joseph Wiseman, Sainyam Galhotra, Stavros Sintos</dc:creator>
    </item>
    <item>
      <title>Spanners in Planar Domains via Steiner Spanners and non-Steiner Tree Covers</title>
      <link>https://arxiv.org/abs/2404.05045</link>
      <description>arXiv:2404.05045v1 Announce Type: cross 
Abstract: We study spanners in planar domains, including polygonal domains, polyhedral terrain, and planar metrics. Previous work showed that for any constant $\epsilon\in (0,1)$, one could construct a $(2+\epsilon)$-spanner with $O(n\log(n))$ edges (SICOMP 2019), and there is a lower bound of $\Omega(n^2)$ edges for any $(2-\epsilon)$-spanner (SoCG 2015). The main open question is whether a linear number of edges suffices and the stretch can be reduced to $2$. We resolve this problem by showing that for stretch $2$, one needs $\Omega(n\log n)$ edges, and for stretch $2+\epsilon$ for any fixed $\epsilon \in (0,1)$, $O(n)$ edges are sufficient. Our lower bound is the first super-linear lower bound for stretch $2$.
  En route to achieve our result, we introduce the problem of constructing non-Steiner tree covers for metrics, which is a natural variant of the well-known Steiner point removal problem for trees (SODA 2001). Given a tree and a set of terminals in the tree, our goal is to construct a collection of a small number of dominating trees such that for every two points, at least one tree in the collection preserves their distance within a small stretch factor. Here, we identify an unexpected threshold phenomenon around $2$ where a sharp transition from $n$ trees to $\Theta(\log n)$ trees and then to $O(1)$ trees happens. Specifically, (i) for stretch $ 2-\epsilon$, one needs $\Omega(n)$ trees; (ii) for stretch $2$, $\Theta(\log n)$ tree is necessary and sufficient; and (iii) for stretch $2+\epsilon$, a constant number of trees suffice. Furthermore, our lower bound technique for the non-Steiner tree covers of stretch $2$ has further applications in proving lower bounds for two related constructions in tree metrics: reliable spanners and locality-sensitive orderings. Our lower bound for locality-sensitive orderings matches the best upper bound (STOC 2022).</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05045v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sujoy Bhore, Bal\'azs Keszegh, Andrey Kupavskii, Hung Le, Alexandre Louvet, D\"om\"ot\"or P\'alv\"olgyi, Csaba D. T\'oth</dc:creator>
    </item>
    <item>
      <title>Efficient Distributed Data Structures for Future Many-core Architectures</title>
      <link>https://arxiv.org/abs/2404.05515</link>
      <description>arXiv:2404.05515v1 Announce Type: cross 
Abstract: We study general techniques for implementing distributed data structures on top of future many-core architectures with non cache-coherent or partially cache-coherent memory. With the goal of contributing towards what might become, in the future, the concurrency utilities package in Java collections for such architectures, we end up with a comprehensive collection of data structures by considering different variants of these techniques. To achieve scalability, we study a generic scheme which makes all our implementations hierarchical. We consider a collection of known techniques for improving the scalability of concurrent data structures and we adjust them to work in our setting. We have performed experiments which illustrate that some of these techniques have indeed high impact on achieving scalability. Our experiments also reveal the performance and scalability power of the hierarchical approach. We finally present experiments to study energy consumption aspects of the proposed techniques by using an energy model recently proposed for such architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05515v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Panagiota Fatourou, Nikolaos D. Kallimanis, Eleni Kanellou, Odysseas Makridakis, Christi Symeonidou</dc:creator>
    </item>
    <item>
      <title>Coresets for Kernel Clustering</title>
      <link>https://arxiv.org/abs/2110.02898</link>
      <description>arXiv:2110.02898v5 Announce Type: replace 
Abstract: We devise coresets for kernel $k$-Means with a general kernel, and use them to obtain new, more efficient, algorithms. Kernel $k$-Means has superior clustering capability compared to classical $k$-Means, particularly when clusters are non-linearly separable, but it also introduces significant computational challenges. We address this computational issue by constructing a coreset, which is a reduced dataset that accurately preserves the clustering costs.
  Our main result is a coreset for kernel $k$-Means that works for a general kernel and has size $\mathrm{poly}(k\epsilon^{-1})$. Our new coreset both generalizes and greatly improves all previous results; moreover, it can be constructed in time near-linear in $n$. This result immediately implies new algorithms for kernel $k$-Means, such as a $(1+\epsilon)$-approximation in time near-linear in $n$, and a streaming algorithm using space and update time $\mathrm{poly}(k \epsilon^{-1} \log n)$.
  We validate our coreset on various datasets with different kernels. Our coreset performs consistently well, achieving small errors while using very few points. We show that our coresets can speed up kernel $k$-Means++ (the kernelized version of the widely used $k$-Means++ algorithm), and we further use this faster kernel $k$-Means++ for spectral clustering. In both applications, we achieve significant speedup and a better asymptotic growth while the error is comparable to baselines that do not use coresets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.02898v5</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaofeng H. -C. Jiang, Robert Krauthgamer, Jianing Lou, Yubo Zhang</dc:creator>
    </item>
    <item>
      <title>Time complexity of the Analyst's Traveling Salesman algorithm</title>
      <link>https://arxiv.org/abs/2202.10314</link>
      <description>arXiv:2202.10314v2 Announce Type: replace 
Abstract: The Analyst's Traveling Salesman Problem asks for conditions under which a (finite or infinite) subset of $\mathbb{R}^N$ is contained on a curve of finite length. We show that for finite sets, the algorithm constructed by Schul (2007)and Badger-Naples-Vellis (2019) that solves the Analyst's Traveling Salesman Problem has polynomial time complexity and we determine the sharp exponent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.10314v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <category>math.MG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anthony Ramirez, Vyron Vellis</dc:creator>
    </item>
    <item>
      <title>Packing $K_r$s in bounded degree graphs</title>
      <link>https://arxiv.org/abs/2209.03684</link>
      <description>arXiv:2209.03684v2 Announce Type: replace 
Abstract: We study the problem of finding a maximum-cardinality set of $r$-cliques in an undirected graph of fixed maximum degree $\Delta$, subject to the cliques in that set being either vertex-disjoint or edge-disjoint. It is known for $r=3$ that the vertex-disjoint (edge-disjoint) problem is solvable in linear time if $\Delta=3$ ($\Delta=4$) but APX-hard if $\Delta \geq 4$ ($\Delta \geq 5$).
  We generalise these results to an arbitrary but fixed $r \geq 3$, and provide a complete complexity classification for both the vertex- and edge-disjoint variants in graphs of maximum degree $\Delta$.
  Specifically, we show that the vertex-disjoint problem is solvable in linear time if $\Delta &lt; 3r/2 - 1$, solvable in polynomial time if $\Delta &lt; 5r/3 - 1$, and APX-hard if $\Delta \geq \lceil 5r/3 \rceil - 1$. We also show that if $r\geq 6$ then the above implications also hold for the edge-disjoint problem. If $r \leq 5$, then the edge-disjoint problem is solvable in linear time if $\Delta &lt; 3r/2 - 1$, solvable in polynomial time if $\Delta \leq 2r - 2$, and APX-hard if $\Delta &gt; 2r - 2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.03684v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael McKay, David Manlove</dc:creator>
    </item>
    <item>
      <title>MANTRA: Temporal Betweenness Centrality Approximation through Sampling</title>
      <link>https://arxiv.org/abs/2304.08356</link>
      <description>arXiv:2304.08356v4 Announce Type: replace 
Abstract: We present MANTRA, a framework for approximating the temporal betweenness centrality of all nodes in a temporal graph. Our method can compute probabilistically guaranteed high-quality temporal betweenness estimates (of nodes and temporal edges) under all the feasible temporal path optimalities, presented in the work of Bu{\ss} et al. (KDD, 2020). We provide a sample-complexity analysis of our method and speed up the temporal betweenness computation using a state-of-the-art progressive sampling approach based on Monte Carlo Empirical Rademacher Averages. Additionally, we provide an efficient sampling algorithm to approximate the temporal diameter, average path length, and other fundamental temporal graph characteristic quantities within a small error $\varepsilon$ with high probability. The running time of such approximation algorithm is $\tilde{\mathcal{O}}(\frac{\log n}{\varepsilon^2}\cdot |\mathcal{E}|)$, where $n$ is the number of nodes and $|\mathcal{E}|$ is the number of temporal edges in the temporal graph. We support our theoretical results with an extensive experimental analysis on several real-world networks and provide empirical evidence that the MANTRA framework improves the current state of the art in speed, sample size, and required space while maintaining high accuracy of the temporal betweenness estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.08356v4</guid>
      <category>cs.DS</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Antonio Cruciani</dc:creator>
    </item>
    <item>
      <title>A simpler and parallelizable $O(\sqrt{\log n})$-approximation algorithm for Sparsest Cut</title>
      <link>https://arxiv.org/abs/2307.00115</link>
      <description>arXiv:2307.00115v4 Announce Type: replace 
Abstract: Currently, the best known tradeoff between approximation ratio and complexity for the Sparsest Cut problem is achieved by the algorithm in [Sherman, FOCS 2009]: it computes $O(\sqrt{(\log n)/\varepsilon})$-approximation using $O(n^\varepsilon\log^{O(1)}n)$ maxflows for any $\varepsilon\in[\Theta(1/\log n),\Theta(1)]$. It works by solving the SDP relaxation of [Arora-Rao-Vazirani, STOC 2004] using the Multiplicative Weights Update algorithm (MW) of [Arora-Kale, JACM 2016]. To implement one MW step, Sherman approximately solves a multicommodity flow problem using another application of MW. Nested MW steps are solved via a certain ``chaining'' algorithm that combines results of multiple calls to the maxflow algorithm. We present an alternative approach that avoids solving the multicommodity flow problem and instead computes ``violating paths''. This simplifies Sherman's algorithm by removing a need for a nested application of MW, and also allows parallelization: we show how to compute $O(\sqrt{(\log n)/\varepsilon})$-approximation via $O(\log^{O(1)}n)$ maxflows using $O(n^\varepsilon)$ processors. We also revisit Sherman's chaining algorithm, and present a simpler version together with a new analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.00115v4</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Kolmogorov</dc:creator>
    </item>
    <item>
      <title>Sandpile Prediction on Undirected Graphs</title>
      <link>https://arxiv.org/abs/2307.07711</link>
      <description>arXiv:2307.07711v3 Announce Type: replace 
Abstract: The $\textit{Abelian Sandpile}$ model is a well-known model used in exploring $\textit{self-organized criticality}$. Despite a large amount of work on other aspects of sandpiles, there have been limited results in efficiently computing the terminal state, known as the $\textit{sandpile prediction}$ problem.
  On graphs with special structures, we present algorithms that compute the terminal configurations for sandpile instances in $O(n \log n)$ time on trees and $O(n)$ time on paths, where $n$ is the number of vertices. Our algorithms improve the previous best runtime of $O(n \log^5 n)$ on trees [Ramachandran-Schild SODA '17] and $O(n \log n)$ on paths [Moore-Nilsson '99]. To do so, we move beyond the simulation of individual events by directly computing the number of firings for each vertex. The computation is accelerated using splittable binary search trees. In addition, we give algorithms in $O(n)$ time on cliques and $O(n \log^2 n)$ time on pseudotrees.
  On general graphs, we propose a fast algorithm under the setting where the number of chips $N$ could be arbitrarily large. We obtain a $\log N$ dependency, improving over the $\mathtt{poly}(N)$ dependency in purely simulation-based algorithms. Our algorithm also achieves faster performance on various types of graphs, including regular graphs, expander graphs, and hypercubes. We also provide a reduction that enables us to decompose the input sandpile into several smaller instances and solve them separately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.07711v3</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruinian Chang, Jingbang Chen, Ian Munro, Richard Peng, Qingyu Shi, Zeyu Zheng</dc:creator>
    </item>
    <item>
      <title>Almost Tight Bounds for Differentially Private Densest Subgraph</title>
      <link>https://arxiv.org/abs/2308.10316</link>
      <description>arXiv:2308.10316v3 Announce Type: replace 
Abstract: We study the Densest Subgraph (DSG) problem under the additional constraint of differential privacy. DSG is a fundamental theoretical question which plays a central role in graph analytics, and so privacy is a natural requirement. All known private algorithms for Densest Subgraph lose constant multiplicative factors, despite the existence of non-private exact algorithms. We show that, perhaps surprisingly, this loss is not necessary: in both the classic differential privacy model and the LEDP model (local edge differential privacy, introduced recently by Dhulipala et al. [FOCS 2022]), we give $(\epsilon, \delta)$-differentially private algorithms with no multiplicative loss whatsoever. In other words, the loss is \emph{purely additive}. Moreover, our additive losses match or improve the best-known previous additive loss (in any version of differential privacy) when $1/\delta$ is polynomial in $n$, and are almost tight: in the centralized setting, our additive loss is $O(\log n /\epsilon)$ while there is a known lower bound of $\Omega(\sqrt{\log n / \epsilon})$.
  We also give a number of extensions. First, we show how to extend our techniques to both the node-weighted and the directed versions of the problem. Second, we give a separate algorithm with pure differential privacy (as opposed to approximate DP) but with worse approximation bounds. And third, we give a new algorithm for privately computing the optimal density which implies a separation between the structural problem of privately computing the densest subgraph and the numeric problem of privately computing the density of the densest subgraph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.10316v3</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Dinitz, Satyen Kale, Silvio Lattanzi, Sergei Vassilvitskii</dc:creator>
    </item>
    <item>
      <title>Embedding Probability Distributions into Low Dimensional $\ell_1$: Tree Ising Models via Truncated Metrics</title>
      <link>https://arxiv.org/abs/2312.02435</link>
      <description>arXiv:2312.02435v2 Announce Type: replace 
Abstract: Given an arbitrary set of high dimensional points in $\ell_1$, there are known negative results that preclude the possibility of always mapping them to a low dimensional $\ell_1$ space while preserving distances with small multiplicative distortion. This is in stark contrast with dimension reduction in Euclidean space ($\ell_2$) where such mappings are always possible. While the first non-trivial lower bounds for $\ell_1$ dimension reduction were established almost 20 years ago, there has been limited progress in understanding what sets of points in $\ell_1$ are conducive to a low-dimensional mapping.
  In this work, we study a new characterization of $\ell_1$ metrics that are conducive to dimension reduction in $\ell_1$. Our characterization focuses on metrics that are defined by the disagreement of binary variables over a probability distribution -- any $\ell_1$ metric can be represented in this form. We show that, for configurations of $n$ points in $\ell_1$ obtained from tree Ising models, we can reduce dimension to $\mathrm{polylog}(n)$ with constant distortion. In doing so, we develop technical tools for embedding truncated metrics which have been studied because of their applications in computer vision, and are objects of independent interest in metric geometry. Among other tools, we show how any $\ell_1$ metric can be truncated with $O(1)$ distortion and $O(\log(n))$ blowup in dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02435v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moses Charikar, Spencer Compton, Chirag Pabbaraju</dc:creator>
    </item>
    <item>
      <title>A faster FPRAS for #NFA</title>
      <link>https://arxiv.org/abs/2312.13320</link>
      <description>arXiv:2312.13320v2 Announce Type: replace 
Abstract: Given a non-deterministic finite automaton (NFA) A with m states, and a natural number n (presented in unary), the #NFA problem asks to determine the size of the set L(A_n) of words of length n accepted by A. While the corresponding decision problem of checking the emptiness of L(A_n) is solvable in polynomial time, the #NFA problem is known to be #P-hard. Recently, the long-standing open question -- whether there is an FPRAS (fully polynomial time randomized approximation scheme) for #NFA -- was resolved in \cite{ACJR19}. The FPRAS due to \cite{ACJR19} relies on the interreducibility of counting and sampling, and computes, for each pair of state q and natural number i &lt;= n, a set of O(\frac{m^7 n^7}{epsilon^7}) many uniformly chosen samples from the set of words of length i that have a run ending at q (\epsilon is the error tolerance parameter of the FPRAS). This informative measure -- the number of samples maintained per state and length -- also affects the overall time complexity with a quadratic dependence.
  Given the prohibitively high time complexity, in terms of each of the input parameters, of the FPRAS due to \cite{ACJR19}, and considering the widespread application of approximate counting (and sampling) in various tasks in Computer Science, a natural question arises: Is there a faster FPRAS for #NFA that can pave the way for the practical implementation of approximate #NFA tools? In this work, we demonstrate that significant improvements in time complexity are achievable. Specifically, we have reduced the number of samples required for each state to be independent of m, with significantly less dependence on $n$ and $\epsilon$, maintaining only \widetilde{O}(\frac{n^4}{epsilon^2}) samples per state.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13320v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kuldeep S. Meel, Sourav Chakraborty, Umang Mathur</dc:creator>
    </item>
    <item>
      <title>Approximating Partition in Near-Linear Time</title>
      <link>https://arxiv.org/abs/2402.11426</link>
      <description>arXiv:2402.11426v2 Announce Type: replace 
Abstract: We propose an $\widetilde{O}(n + 1/\eps)$-time FPTAS (Fully Polynomial-Time Approximation Scheme) for the classical Partition problem. This is the best possible (up to a polylogarithmic factor) assuming SETH (Strong Exponential Time Hypothesis) [Abboud, Bringmann, Hermelin, and Shabtay'22]. Prior to our work, the best known FPTAS for Partition runs in $\widetilde{O}(n + 1/\eps^{5/4})$ time [Deng, Jin and Mao'23, Wu and Chen'22]. Our result is obtained by solving a more general problem of weakly approximating Subset Sum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11426v2</guid>
      <category>cs.DS</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Chen, Jiayi Lian, Yuchen Mao, Guochuan Zhang</dc:creator>
    </item>
    <item>
      <title>Optimum Noise Mechanism for Differentially Private Queries in Discrete Finite Sets</title>
      <link>https://arxiv.org/abs/2111.11661</link>
      <description>arXiv:2111.11661v3 Announce Type: replace-cross 
Abstract: The Differential Privacy (DP) literature often centers on meeting privacy constraints by introducing noise to the query, typically using a pre-specified parametric distribution model with one or two degrees of freedom. However, this emphasis tends to neglect the crucial considerations of response accuracy and utility, especially in the context of categorical or discrete numerical database queries, where the parameters defining the noise distribution are finite and could be chosen optimally. This paper addresses this gap by introducing a novel framework for designing an optimal noise Probability Mass Function (PMF) tailored to discrete and finite query sets. Our approach considers the modulo summation of random noise as the DP mechanism, aiming to present a tractable solution that not only satisfies privacy constraints but also minimizes query distortion. Unlike existing approaches focused solely on meeting privacy constraints, our framework seeks to optimize the noise distribution under an arbitrary $(\epsilon, \delta)$ constraint, thereby enhancing the accuracy and utility of the response. We demonstrate that the optimal PMF can be obtained through solving a Mixed-Integer Linear Program (MILP). Additionally, closed-form solutions for the optimal PMF are provided, minimizing the probability of error for two specific cases. Numerical experiments highlight the superior performance of our proposed optimal mechanisms compared to state-of-the-art methods. This paper contributes to the DP literature by presenting a clear and systematic approach to designing noise mechanisms that not only satisfy privacy requirements but also optimize query distortion. The framework introduced here opens avenues for improved privacy-preserving database queries, offering significant enhancements in response accuracy and utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.11661v3</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sachin Kadam, Anna Scaglione, Nikhil Ravi, Sean Peisert, Brent Lunghino, Aram Shumavon</dc:creator>
    </item>
    <item>
      <title>Online Sorting and Translational Packing of Convex Polygons</title>
      <link>https://arxiv.org/abs/2112.03791</link>
      <description>arXiv:2112.03791v2 Announce Type: replace-cross 
Abstract: We investigate several online packing problems in which convex polygons arrive one by one and have to be placed irrevocably into a container, while the aim is to minimize the used space. Among other variants, we consider strip packing and bin packing, where the container is the infinite horizontal strip $[0,\infty)\times [0,1]$ or a collection of $1 \times 1$ bins, respectively.
  We draw interesting connections to the following online sorting problem OnlineSorting$[\gamma,n]$: We receive a stream of real numbers $s_1,\ldots,s_n$, $s_i\in[0,1]$, one by one. Each real must be placed in an array $A$ with $\gamma n$ initially empty cells without knowing the subsequent reals. The goal is to minimize the sum of differences of consecutive reals in $A$. The offline optimum is to place the reals in sorted order so the cost is at most $1$. We show that for any $\Delta$-competitive online algorithm of OnlineSorting$[\gamma,n]$, it holds that $\gamma \Delta \in\Omega(\log n/\log \log n)$.
  We use this lower bound to prove the non-existence of competitive algorithms for various online translational packing problems of convex polygons, among them strip packing, bin packing and perimeter packing. This also implies that there exists no online algorithm that can pack all streams of pieces of diameter and total area at most $\delta$ into the unit square. These results are in contrast to the case when the pieces are restricted to rectangles, for which competitive algorithms are known. Likewise, the offline versions of packing convex polygons have constant factor approximation algorithms.
  As a complement, we also include algorithms for both online sorting and translation-only online strip packing with non-trivial competitive ratios. Our algorithm for strip packing relies on a new technique for recursively subdividing the strip into parallelograms of varying height, thickness and slope.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.03791v2</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anders Aamand, Mikkel Abrahamsen, Lorenzo Beretta, Linda Kleist</dc:creator>
    </item>
    <item>
      <title>Computable Bounds and Monte Carlo Estimates of the Expected Edit Distance</title>
      <link>https://arxiv.org/abs/2211.07644</link>
      <description>arXiv:2211.07644v2 Announce Type: replace-cross 
Abstract: The edit distance is a metric of dissimilarity between strings, widely applied in computational biology, speech recognition, and machine learning. Let $e_k(n)$ denote the average edit distance between random, independent strings of $n$ characters from an alphabet of size $k$. For $k \geq 2$, it is an open problem how to efficiently compute the exact value of $\alpha_{k}(n) = e_k(n)/n$ as well as of $\alpha_{k} = \lim_{n \to \infty} \alpha_{k}(n)$, a limit known to exist.
  This paper shows that $\alpha_k(n)-Q(n) \leq \alpha_k \leq \alpha_k(n)$, for a specific $Q(n)=\Theta(\sqrt{\log n / n})$, a result which implies that $\alpha_k$ is computable. The exact computation of $\alpha_k(n)$ is explored, leading to an algorithm running in time $T=\mathcal{O}(n^2k\min(3^n,k^n))$, a complexity that makes it of limited practical use.
  An analysis of statistical estimates is proposed, based on McDiarmid's inequality, showing how $\alpha_k(n)$ can be evaluated with good accuracy, high confidence level, and reasonable computation time, for values of $n$ say up to a quarter million. Correspondingly, 99.9\% confidence intervals of width approximately $10^{-2}$ are obtained for $\alpha_k$.
  Combinatorial arguments on edit scripts are exploited to analytically characterize an efficiently computable lower bound $\beta_k^*$ to $\alpha_k$, such that $ \lim_{k \to \infty} \beta_k^*=1$. In general, $\beta_k^* \leq \alpha_k \leq 1-1/k$; for $k$ greater than a few dozens, computing $\beta_k^*$ is much faster than generating good statistical estimates with confidence intervals of width $1-1/k-\beta_k^*$.
  The techniques developed in the paper yield improvements on most previously published numerical values as well as results for alphabet sizes and string lengths not reported before.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.07644v2</guid>
      <category>cs.FL</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gianfranco Bilardi, Michele Schimd</dc:creator>
    </item>
    <item>
      <title>Black-Box Identity Testing of Noncommutative Rational Formulas in Deterministic Quasipolynomial Time</title>
      <link>https://arxiv.org/abs/2309.15647</link>
      <description>arXiv:2309.15647v3 Announce Type: replace-cross 
Abstract: Rational Identity Testing (RIT) is the decision problem of determining whether or not a noncommutative rational formula computes zero in the free skew field. It admits a deterministic polynomial-time white-box algorithm [Garg, Gurvits, Oliveira, and Wigderson (2016); Ivanyos, Qiao, Subrahmanyam (2018); Hamada and Hirai (2021)], and a randomized polynomial-time algorithm [Derksen and Makam (2017)] in the black-box setting, via singularity testing of linear matrices over the free skew field. Indeed, a randomized NC algorithm for RIT in the white-box setting follows from the result of Derksen and Makam (2017).
  Designing an efficient deterministic black-box algorithm for RIT and understanding the parallel complexity of RIT are major open problems in this area. Despite being open since the work of Garg, Gurvits, Oliveira, and Wigderson (2016), these questions have seen limited progress. In fact, the only known result in this direction is the construction of a quasipolynomial-size hitting set for rational formulas of only inversion height two [Arvind, Chatterjee, Mukhopadhyay (2022)].
  In this paper, we significantly improve the black-box complexity of this problem and obtain the first quasipolynomial-size hitting set for all rational formulas of polynomial size. Our construction also yields the first deterministic quasi-NC upper bound for RIT in the white-box setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15647v3</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>V. Arvind, Abhranil Chatterjee, Partha Mukhopadhyay</dc:creator>
    </item>
  </channel>
</rss>
