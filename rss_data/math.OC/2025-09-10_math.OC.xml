<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Sep 2025 04:01:40 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Inertial accelerated primal-dual algorithms for non-smooth convex optimization problems with linear equality constraints</title>
      <link>https://arxiv.org/abs/2509.07306</link>
      <description>arXiv:2509.07306v1 Announce Type: new 
Abstract: This paper is devoted to the study of an inertial accelerated primal-dual algorithm, which is based on a second-order differential system with time scaling, for solving a non-smooth convex optimization problem with linear equality constraints. We first introduce a second-order differential system with time scaling associated with the non-smooth convex optimization problem, and then obtain fast convergence rates for the primal-dual gap, the feasibility violation, and the objective residual along the trajectory generated by this system. Subsequently, based on the setting of the parameters involved, we propose an inertial accelerated primal-dual algorithm from the time discretization of this system. We also establish fast convergence rates for the primal-dual gap, the feasibility violation, and the objective residual. Furthermore, we demonstrate the efficacy of the proposed algorithm through numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07306v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huan Zhang, Xiangkai Sun, Shengjie Li, Kok Lay Teo</dc:creator>
    </item>
    <item>
      <title>Optimal Multi-Mode Propulsion Mission Design Using Direct Collocation</title>
      <link>https://arxiv.org/abs/2509.07336</link>
      <description>arXiv:2509.07336v1 Announce Type: new 
Abstract: The problem of minimizing the transfer time between periodic orbits in the Earth-Moon elliptic restricted three-body problem using a multi-mode propulsion system is considered. By employing the true anomaly on the primary orbit as the independent variable and introducing normalized time as an additional state, the need to repeatedly solve Kepler's equation at arbitrary epochs is eliminated. Furthermore, a propellant constraint is imposed on the high-thrust mode to activate the multi-mode capabilities of the system and balance efficiency with maneuverability. The minimum-time optimal control problem is formulated as a three-phase trajectory consisting of a coast along the initial periodic orbit, a controlled transfer, and a coast along the terminal periodic orbit. The three-phase optimal control problem is then solved using an adaptive Gaussian quadrature direct collocation method. Case studies are presented for transfers from an L2 southern halo orbit to a near-rectilinear halo orbit, analyzing the impact of different single- and multi-mode propulsion architectures and varying propellant constraint values. Finally, the methodology developed in this paper provides a systematic framework for generating periodic orbit transfers in three-body systems using single- and multi-mode propulsion systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07336v1</guid>
      <category>math.OC</category>
      <category>astro-ph.EP</category>
      <category>astro-ph.IM</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>George V. Haman III, Anil V. Rao</dc:creator>
    </item>
    <item>
      <title>Dynamic Redeployment of Nurses Across Hospitals: A Sample Robust Optimization Approach</title>
      <link>https://arxiv.org/abs/2509.07387</link>
      <description>arXiv:2509.07387v1 Announce Type: new 
Abstract: Problem definition: We study a workforce redeployment problem in hospital networks, where clinical staff, such as nurses, are temporarily reassigned from overstaffed to understaffed sites to address short-term imbalances. This practice of ``internal travel,'' which gained traction during the COVID-19 pandemic to tackle nurse shortages, presents new operational challenges that require tailored analytical support. Key requirements such as advance notice and short-term secondments must be incorporated. Moreover, in rapidly evolving environments, reliance on historical data leads to unreliable forecasts, limiting the effectiveness of traditional sample-based methods. Methodology: We formulate the problem as a stochastic dynamic program and incorporate demand uncertainty via a sample robust optimization (SRO) framework. Using linear decision rule approximation, we reformulate the problem as a tractable linear program. Results: We evaluate the impact of key network design components on system performance. Network connectivity has the largest effect in reducing the total cost, number of redeployments, and travel distance, but its benefits depend on aligning the secondment duration with the network structure. Full connectivity without proper secondments can be counterproductive. The SRO approach outperforms the traditional sample-average method in the presence of demand surges or under-forecasts by better anticipating emergency redeployments. Managerial implications: Internal travel programs offer a promising strategy to alleviate workforce shortages in healthcare systems. Our results highlight the importance of network design, aligning secondment durations with the network structure, and adopting planning methods that are robust to demand surges or inaccurate predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07387v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Liu, Tianchun Li, Mengshi Lu, Pengyi Shi</dc:creator>
    </item>
    <item>
      <title>Reinforcement learning for online hyperparameter tuning in convex quadratic programming</title>
      <link>https://arxiv.org/abs/2509.07404</link>
      <description>arXiv:2509.07404v1 Announce Type: new 
Abstract: Quadratic programming is a workhorse of modern nonlinear optimization, control, and data science. Although regularized methods offer convergence guarantees under minimal assumptions on the problem data, they can exhibit the slow tail-convergence typical of first-order schemes, thus requiring many iterations to achieve high-accuracy solutions. Moreover, hyperparameter tuning significantly impacts on the solver performance but how to find an appropriate parameter configuration remains an elusive research question. To address these issues, we explore how data-driven approaches can accelerate the solution process. Aiming at high-accuracy solutions, we focus on a stabilized interior-point solver and carefully handle its two-loop flow and control parameters. We will show that reinforcement learning can make a significant contribution to facilitating the solver tuning and to speeding up the optimization process. Numerical experiments demonstrate that, after a lightweight training, the learned policy generalizes well to different problem classes with varying dimensions and to various solver configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07404v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeremy Bertoncini, Alberto De Marchi, Matthias Gerdts, Simon Gottschalk</dc:creator>
    </item>
    <item>
      <title>First-order SDSOS-convex semi-algebraic optimization and exact SOCP relaxations</title>
      <link>https://arxiv.org/abs/2509.07418</link>
      <description>arXiv:2509.07418v1 Announce Type: new 
Abstract: In this paper, we define a new type of nonsmooth convex function, called {\em first-order SDSOS-convex semi-algebraic function}, which is an extension of the previously proposed first-order SDSOS-convex polynomials (Chuong et al. in J Global Optim 75:885--919, 2019). This class of nonsmooth convex functions contains many well-known functions, such as the Euclidean norm, the $\ell_1$-norm commonly used in compressed sensing and sparse optimization, and the least squares function frequently employed in machine learning and regression analysis. We show that, under suitable assumptions, the optimal value and optimal solutions of first-order SDSOS-convex semi-algebraic programs can be found by exactly solving an associated second-order cone programming problem. Finally, an application to robust optimization with first-order SDSOS-convex polynomials is discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07418v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengmiao Yang, Liguo Jiao, Jae Hyoung Lee</dc:creator>
    </item>
    <item>
      <title>DKFNet: Differentiable Kalman Filter for Field Inversion and Machine Learning</title>
      <link>https://arxiv.org/abs/2509.07474</link>
      <description>arXiv:2509.07474v1 Announce Type: new 
Abstract: The Kalman filter is a fundamental tool for state estimation in dynamical systems. While originally developed for linear Gaussian settings, it has been extended to nonlinear problems through approaches such as the extended and unscented Kalman filters. Despite its broad use, a persistent limitation is that the underlying approximate model is fixed, which can lead to significant deviations from the true system dynamics. To address this limitation, we introduce the differentiable Kalman filter (DKF), an adjoint-based two-level optimization framework designed to reduce the mismatch between approximate and true dynamics. Within this framework, a field inversion step first uncovers the discrepancy, after which a closure model is trained to capture the discovered dynamics, allowing the filter to adapt flexibly and scale efficiently. We illustrate the capabilities of the DKF using two representative examples: a rocket dynamics model and the Allen-Cahn boundary value problem. In both cases, and across a range of noise levels, the DKF consistently reduces state reconstruction error by at least 90% compared to the classical Kalman filter, while also maintaining robust uncertainty quantification. These results demonstrate that the DKF not only improves estimation accuracy by large margins but also enhances interpretability and scalability, offering a principled pathway for combining data assimilation with modern machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07474v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Wu, Sicheng He</dc:creator>
    </item>
    <item>
      <title>Differential Dynamic Programming for the Optimal Control Problem with an Ellipsoidal Target Set and Its Statistical Inference</title>
      <link>https://arxiv.org/abs/2509.07546</link>
      <description>arXiv:2509.07546v1 Announce Type: new 
Abstract: This work addresses an extended class of optimal control problems where a target for a system state has the form of an ellipsoid rather than a fixed, single point. As a computationally affordable method for resolving the extended problem, we present a revised version of the differential dynamic programming (DDP), termed the differential dynamic programming with ellipsoidal target set (ETS-DDP). To this end, the problem with an ellipsoidal target set is reformulated into an equivalent form with the orthogonal projection operator, yielding that the resulting cost functions turn out to be discontinuous at some points. As the DDP usually requires the differentiability of cost functions, in the ETS-DDP formulation we locally approximate the (nonsmooth) cost functions to smoothed ones near the path generated at the previous iteration, by utilizing the explicit form of the orthogonal projection operator. Moreover, a statistical inference method is also presented for designing the ellipsoidal target set, based on data on admissible target points collected by expert demonstrations. Via a simulation on autonomous parking of a vehicle, it is seen that the proposed ETS-DDP efficiently derives an admissible state trajectory while running much faster than the point-targeted DDP, at the expense of optimality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07546v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sungjun Eom, Gyunghoon Park</dc:creator>
    </item>
    <item>
      <title>On the Characterization of gH-partial derivatives and gH-Product for Interval-Valued Functions</title>
      <link>https://arxiv.org/abs/2509.07565</link>
      <description>arXiv:2509.07565v1 Announce Type: new 
Abstract: In this paper, we show by a counterexample that the gH-partial derivative of interval-valued functions (IVFs) may exist even when the partial derivative of the end point functions do not. Next, we introduce the gH-partial derivative in terms of gH-derivative and discuss its complete characterization. Furthermore, we introduce the gH-product of a vector with an n-tuples of intervals and illustrate by a suitable example that our definition refines the definition existing in the literature. To illustrate and validate these definitions, we provide several non-trivial examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07565v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Suhail,  Tauheed, Akhlad Iqbal</dc:creator>
    </item>
    <item>
      <title>Optimizing a Worldwide-Scale Shipper Transportation Planning in a Carmaker Inbound Supply Chain</title>
      <link>https://arxiv.org/abs/2509.07576</link>
      <description>arXiv:2509.07576v1 Announce Type: new 
Abstract: We study the shipper-side design of large-scale inbound transportation networks, motivated by Renault's global supply chain. We introduce the Shipper Transportation Design Problem, which integrates consolidation, routing, and regularity constraints, and propose a tailored Iterated Local Search (ILS) metaheuristic. The algorithm combines large-neighborhood search with MILP-based perturbations and exploits bundle-specific decompositions and giant container bounds to obtain scalable lower bounds and effective benchmarks. Computational experiments on real industrial data show that the ILS achieves an average gap of 7.9% to the best available lower bound on world-scale instances with more than 700,000 commodities and 1,200,000 arcs, improving Renault's current planning solutions by 23.2%. To our knowledge, this is the first approach to solve shipper-side transportation design problems at such scale. Our analysis further yields managerial insights: accurate bin-packing models are essential for realistic consolidation, highly regular plans offer the best balance between cost and operational stability, and outsourcing is only attractive in low-volume contexts, while large-scale networks benefit from in-house planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07576v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathis Brichet, Maximilian Schiffer, Axel Parmentier</dc:creator>
    </item>
    <item>
      <title>On Global Rates for Regularization Methods based on Secant Derivative Approximations</title>
      <link>https://arxiv.org/abs/2509.07580</link>
      <description>arXiv:2509.07580v1 Announce Type: new 
Abstract: An inexact framework for high-order adaptive regularization methods is presented, in which approximations may be used for the $p$th-order tensor, based on lower-order derivatives. Between each recalculation of the $p$th-order derivative approximation, a high-order secant equation can be used to update the $p$th-order tensor as proposed in (Welzel 2024) or the approximation can be kept constant in a lazy manner. When refreshing the $p$th-order tensor approximation after $m$ steps, an exact evaluation of the tensor or a finite difference approximation can be used with an explicit discretization stepsize. For all the newly adaptive regularization variants, we prove an $\mathcal{O}\left( \max[ \epsilon_1^{-(p+1)/p}, \, \epsilon_2^{(-p+1)/(p-1)} ] \right)$ bound on the number of iterations needed to reach an $(\epsilon_1, \, \epsilon_2)$ second-order stationary points. Discussions on the number of oracle calls for each introduced variant are also provided.
  When $p=2$, we obtain a second-order method that uses quasi-Newton approximations with an $\mathcal{O}\left(\max[\epsilon_1^{-3/2}, \, \, \epsilon_2^{-3}]\right)$ iteration bound to achieve approximate second-order stationarity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07580v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Coralia Cartis, Sadok Jerad</dc:creator>
    </item>
    <item>
      <title>Decentralized Online Riemannian Optimization Beyond Hadamard Manifolds</title>
      <link>https://arxiv.org/abs/2509.07779</link>
      <description>arXiv:2509.07779v1 Announce Type: new 
Abstract: We study decentralized online Riemannian optimization over manifolds with possibly positive curvature, going beyond the Hadamard manifold setting. Decentralized optimization techniques rely on a consensus step that is well understood in Euclidean spaces because of their linearity. However, in positively curved Riemannian spaces, a main technical challenge is that geodesic distances may not induce a globally convex structure. In this work, we first analyze a curvature-aware Riemannian consensus step that enables a linear convergence beyond Hadamard manifolds. Building on this step, we establish a $O(\sqrt{T})$ regret bound for the decentralized online Riemannian gradient descent algorithm. Then, we investigate the two-point bandit feedback setup, where we employ computationally efficient gradient estimators using smoothing techniques, and we demonstrate the same $O(\sqrt{T})$ regret bound through the subconvexity analysis of smoothed objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07779v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emre Sahinoglu, Shahin Shahrampour</dc:creator>
    </item>
    <item>
      <title>Trust-Region Method for Optimization of Set-Valued Maps Given by Finitely Many Functions</title>
      <link>https://arxiv.org/abs/2509.07836</link>
      <description>arXiv:2509.07836v1 Announce Type: new 
Abstract: In this article, we develop a trust-region technique to find critical points of unconstrained set optimization problems with the objective set-valued map defined by finitely many twice continuously differentiable functions. The technique is globally convergent and has the descent property. To ensure the descent property, a new rule of trust-region reduction ratio is introduced for the considered set-valued maps. In the derived method, to find the sequence of iteration points, we need to perform one iteration of a different vector optimization problem at each iteration. Thus, the derived technique is found to be not a straight extension of that for vector optimization. The effectiveness of the proposed algorithm is reported through performance profiles of the proposed approach with the existing methods on various test examples. A list of test problems for set optimization is also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07836v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suprova Ghosh, Debdas Ghosh, Christiane Tammer, Xiaopeng Zhao</dc:creator>
    </item>
    <item>
      <title>A Monte Carlo Approach to Nonsmooth Convex Optimization via Proximal Splitting Algorithms</title>
      <link>https://arxiv.org/abs/2509.07914</link>
      <description>arXiv:2509.07914v1 Announce Type: new 
Abstract: Operator splitting algorithms are a cornerstone of modern first-order optimization, relying critically on proximal operators as their fundamental building blocks. However, explicit formulas for proximal operators are available only for limited classes of functions, restricting the applicability of these methods. Recent work introduced HJ-Prox, a zeroth-order Monte Carlo approximation of the proximal operator derived from Hamilton-Jacobi PDEs, which circumvents the need for closed-form solutions. In this work, we extend the scope of HJ-Prox by establishing that it can be seamlessly incorporated into operator splitting schemes while preserving convergence guarantees. In particular, we show that replacing exact proximal steps with HJ-Prox approximations in algorithms such as proximal gradient descent, Douglas-Rachford splitting, Davis-Yin splitting, and the primal-dual hybrid gradient method still ensures convergence under mild conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07914v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Di, Eric C. Chi, Samy Wu Fung</dc:creator>
    </item>
    <item>
      <title>Learning Mixtures of Spherical Gaussians via Fourier Analysis</title>
      <link>https://arxiv.org/abs/2004.05813</link>
      <description>arXiv:2004.05813v2 Announce Type: cross 
Abstract: Suppose that we are given independent, identically distributed samples $x_l$ from a mixture $\mu$ of no more than $k$ of $d$-dimensional spherical gaussian distributions $\mu_i$ with variance $1$, such that the minimum $\ell_2$ distance between two distinct centers $y_l$ and $y_j$ is greater than $\sqrt{d} \Delta$ for some $c \leq \Delta $, where $c\in (0,1)$ is a small positive universal constant. We develop a randomized algorithm that learns the centers $y_l$ of the gaussians, to within an $\ell_2$ distance of $\delta &lt; \frac{\Delta\sqrt{d}}{2}$ and the weights $w_l$ to within $cw_{min}$ with probability greater than $1 - \exp(-k/c)$. The number of samples and the computational time is bounded above by $poly(k, d, \frac{1}{\delta})$. Such a bound on the sample and computational complexity was previously unknown when $\omega(1) \leq d \leq O(\log k)$. When $d = O(1)$, this follows from work of Regev and Vijayaraghavan. These authors also show that the sample complexity of learning a random mixture of gaussians in a ball of radius $\Theta(\sqrt{d})$ in $d$ dimensions, when $d$ is $\Theta( \log k)$ is at least $poly(k, \frac{1}{\delta})$, showing that our result is tight in this case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2004.05813v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-981-97-7989-5_8</arxiv:DOI>
      <arxiv:journal_reference>Ind.Appl.Math.Springer (2025) 149-181</arxiv:journal_reference>
      <dc:creator>Somnath Chakraborty, Hariharan Narayanan</dc:creator>
    </item>
    <item>
      <title>Efficient Defection: Overage-Proportional Rationing Attains the Cooperative Frontier</title>
      <link>https://arxiv.org/abs/2509.07145</link>
      <description>arXiv:2509.07145v1 Announce Type: cross 
Abstract: We study a noncooperative $n$-player game of slack allocation in which each player $j$ has entitlement $L_j&gt;0$ and chooses a claim $C_j\ge0$. Let $v_j=(C_j-L_j)_+$ (overage) and $s_j=(L_j-C_j)_+$ (slack); set $X=\sum_j v_j$ and $I=\sum_j s_j$. At the end of the period an overage-proportional clearing rule allocates cooperative surplus $I$ to defectors in proportion to $v_j$; cooperators receive $C_j$. We show: (i) the selfish outcome reproduces the cooperative payoff vector $(L_1,\dots,L_n)$; (ii) with bounded actions, defection is a weakly dominant strategy; (iii) within the $\alpha$-power family, the linear rule ($\alpha=1$) is the unique boundary-continuous member; and (iv) the dominant-strategy outcome is Strong Nash under transferable utility and hence coalition-proof (Bernheim et al., 1987). We give a policy interpretation for carbon rationing with a penalty collar.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07145v1</guid>
      <category>econ.TH</category>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Florian Lengyel</dc:creator>
    </item>
    <item>
      <title>Resolvent Compositions for Positive Linear Operators</title>
      <link>https://arxiv.org/abs/2509.07251</link>
      <description>arXiv:2509.07251v1 Announce Type: cross 
Abstract: Resolvent compositions were recently introduced as monotonicity-preserving operations that combine a set-valued monotone operator and a bounded linear operator. They generalize in particular the notion of a resolvent average. We analyze the resolvent compositions when the monotone operator is a strictly positive linear operator. We establish several new properties, including L\"owner partial order relations and asymptotic behavior. In addition, we show that the resolvent composition operations are nonexpansive with respect to the Thompson metric. We also introduce a new form of geometric interpolation and explore its connections to resolvent compositions. Finally, we study two nonlinear equations based on resolvent compositions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07251v1</guid>
      <category>math.FA</category>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Diego J. Cornejo</dc:creator>
    </item>
    <item>
      <title>Mean field control with absorption</title>
      <link>https://arxiv.org/abs/2509.07877</link>
      <description>arXiv:2509.07877v1 Announce Type: cross 
Abstract: In this paper we study a mean field control problem in which particles are absorbed when they reach the boundary of a smooth domain. The value of the N-particle problem is described by a hierarchy of Hamilton-Jacobi equations which are coupled through their boundary conditions. The value function of the limiting problem; meanwhile, solves a Hamilton-Jacobi equation set on the space of sub-probability measures on the smooth domain, i.e. the space of non-negative measures with total mass at most one. Our main contributions are (i) to establish a comparison principle for this novel infinite-dimensional Hamilton-Jacobi equation and (ii) to prove that the value of the N-particle problem converges in a suitable sense towards the value of the limiting problem as N tends to infinity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07877v1</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre Cardaliaguet, Joe Jackson, Panagiotis E. Souganidis</dc:creator>
    </item>
    <item>
      <title>A Modular Algorithm for Non-Stationary Online Convex-Concave Optimization</title>
      <link>https://arxiv.org/abs/2509.07901</link>
      <description>arXiv:2509.07901v1 Announce Type: cross 
Abstract: This paper investigates the problem of Online Convex-Concave Optimization, which extends Online Convex Optimization to two-player time-varying convex-concave games. The goal is to minimize the dynamic duality gap (D-DGap), a critical performance measure that evaluates players' strategies against arbitrary comparator sequences. Existing algorithms fail to deliver optimal performance, particularly in stationary or predictable environments. To address this, we propose a novel modular algorithm with three core components: an Adaptive Module that dynamically adjusts to varying levels of non-stationarity, a Multi-Predictor Aggregator that identifies the best predictor among multiple candidates, and an Integration Module that effectively combines their strengths. Our algorithm achieves a minimax optimal D-DGap upper bound, up to a logarithmic factor, while also ensuring prediction error-driven D-DGap bounds. The modular design allows for the seamless replacement of components that regulate adaptability to dynamic environments, as well as the incorporation of components that integrate ``side knowledge'' from multiple predictors. Empirical results further demonstrate the effectiveness and adaptability of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07901v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qing-xin Meng, Xia Lei, Jian-wei Liu</dc:creator>
    </item>
    <item>
      <title>Theoretical Analysis on how Learning Rate Warmup Accelerates Convergence</title>
      <link>https://arxiv.org/abs/2509.07972</link>
      <description>arXiv:2509.07972v1 Announce Type: cross 
Abstract: Learning rate warmup is a popular and practical technique in training large-scale deep neural networks. Despite the huge success in practice, the theoretical advantages of this strategy of gradually increasing the learning rate at the beginning of the training process have not been fully understood. To resolve this gap between theory and practice, we first propose a novel family of generalized smoothness assumptions, and validate its applicability both theoretically and empirically. Under the novel smoothness assumption, we study the convergence properties of gradient descent (GD) in both deterministic and stochastic settings. It is shown that learning rate warmup consistently accelerates GD, and GD with warmup can converge at most $\Theta(T)$ times faster than with a non-increasing learning rate schedule in some specific cases, providing insights into the benefits of this strategy from an optimization theory perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07972v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuxing Liu, Yuze Ge, Rui Pan, An Kang, Tong Zhang</dc:creator>
    </item>
    <item>
      <title>Upper bound hierarchies for noncommutative polynomial optimization</title>
      <link>https://arxiv.org/abs/2402.02126</link>
      <description>arXiv:2402.02126v3 Announce Type: replace 
Abstract: This work focuses on minimizing the eigenvalue of a noncommutative polynomial subject to a finite number of noncommutative polynomial inequality constraints.
  Based on the Helton-McCullough Positivstellensatz, the noncommutative analog of Lasserre's moment-sum of squares hierarchy provides a sequence of lower bounds converging to the minimal eigenvalue, under mild assumptions on the constraint set. Each lower bound can be obtained by solving a semidefinite program.
  We derive complementary converging hierarchies of upper bounds. They are noncommutative analogues of the upper bound hierarchies due to Lasserre for minimizing polynomials over compact sets. Each upper bound can be obtained by solving a generalized eigenvalue problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02126v3</guid>
      <category>math.OC</category>
      <category>math.FA</category>
      <category>quant-ph</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Igor Klep, Victor Magron, Ga\"el Mass\'e, Jurij Vol\v{c}i\v{c}</dc:creator>
    </item>
    <item>
      <title>Improved Physics-informed neural networks loss function regularization with a variance-based term</title>
      <link>https://arxiv.org/abs/2412.13993</link>
      <description>arXiv:2412.13993v3 Announce Type: replace 
Abstract: In machine learning and statistical modeling, the mean square or absolute error is commonly used as an error metric, also called a "loss function." While effective in reducing the average error, this approach may fail to address localized outliers, leading to significant inaccuracies in regions with sharp gradients or discontinuities. This issue is particularly evident in physics-informed neural networks (PINNs), where such localized errors are expected and affect the overall solution. To overcome this limitation, we propose a novel loss function that combines the mean and the standard deviation of the chosen error metric. By minimizing this combined loss function, the method ensures a more uniform error distribution and reduces the impact of localized high-error regions. The proposed loss function is easy to implement and tested on problems of varying complexity: the 1D Poisson equation, the unsteady Burgers' equation, 2D linear elastic solid mechanics, and 2D steady Navier-Stokes equations. Results demonstrate improved solution quality and lower maximum error compared to the standard mean-based loss, with minimal impact on computational time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13993v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John M. Hanna, Hugues Talbot, Irene E. Vignon-Clementel</dc:creator>
    </item>
    <item>
      <title>A universal convergence theorem for primal-dual penalty and augmented Lagrangian methods</title>
      <link>https://arxiv.org/abs/2412.14269</link>
      <description>arXiv:2412.14269v2 Announce Type: replace 
Abstract: We present a so-called universal convergence theorem for inexact primal-dual penalty and augmented Lagrangian methods that can be applied to a large number of such methods and reduces their convergence analysis to verification of some simple conditions on sequences generated by these methods. If these conditions are verified, then both primal and dual convergence follow directly from the universal convergence theorem. This theorem allows one not only to derive standard convergence theorems for many existing primal-dual penalty and augmented Lagrangian methods in a unified and straightforward manner, but also to strengthen and generalize some of these theorems. In particular, we show how with the use of the universal convergence theorem one can significantly improve some existing results on convergence of a primal-dual rounded weighted $\ell_1$-penalty method, an augmented Lagrangian method for cone constrained optimization, and some other primal-dual methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14269v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. V. Dolgopolik</dc:creator>
    </item>
    <item>
      <title>Hybrid Data-enabled Predictive Control: Incorporating model knowledge into the DeePC</title>
      <link>https://arxiv.org/abs/2502.12467</link>
      <description>arXiv:2502.12467v4 Announce Type: replace 
Abstract: Predictive control can either be data-based (e.g. data-enabled predictive control, or DeePC) or model-based (model predictive control). In this paper we aim to bridge the gap between the two by investigating the case where only a partial model is available, i.e. incorporating model knowledge into DeePC. In our formulation, the partial knowledge takes the form of known state and output equations that are a subset of the complete model equations. We formulate an approach to take advantage of partial model knowledge which we call hybrid data-enabled predictive control (HDeePC). We prove feasible set equivalence and equivalent closed-loop behavior in the noiseless, LTI case. As we show, this has potential advantages over a purely data-based approach in terms of computational expense and robustness to noise in some cases. Furthermore, this allows applications to certain linear time-varying and nonlinear systems. Finally, a number of case studies, including the control of an energy storage system in a microgrid, a triple-mass system, and a larger power system, illustrate the potential of HDeePC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12467v4</guid>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeremy D. Watson</dc:creator>
    </item>
    <item>
      <title>Nonlinear Separation Theorems for Co-Radiant Sets and Optimality Conditions for Approximate and Proper Approximate Solutions in Vector Optimization</title>
      <link>https://arxiv.org/abs/2503.10193</link>
      <description>arXiv:2503.10193v3 Announce Type: replace 
Abstract: This paper deals with $\varepsilon$-efficient and $\varepsilon$-properly efficient points with respect to a co-radiant set in vector optimization problems. In the first part of the paper, we establish a new nonlinear separation theorem for co-radiant sets in normed spaces. Subsequently, we obtain necessary and sufficient conditions, via scalarization, for both $\varepsilon$-efficient and $\varepsilon$-properly efficient points in a general framework, without requiring any assumptions on the co-radiant set or convexity conditions on the sets under consideration. Consequently, our results are applicable in a broader range of settings than those previously addressed in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10193v3</guid>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fernando Garc\'ia-Casta\~no, Miguel \'Angel Melguizo-Padial</dc:creator>
    </item>
    <item>
      <title>Convergence of Momentum-Based Optimization Algorithms with Time-Varying Parameters</title>
      <link>https://arxiv.org/abs/2506.11904</link>
      <description>arXiv:2506.11904v2 Announce Type: replace 
Abstract: In this paper, we present a unified algorithm for stochastic optimization that makes use of a "momentum" term; in other words, the stochastic gradient depends not only on the current true gradient of the objective function, but also on the true gradient at the previous iteration. Our formulation includes the Stochastic Heavy Ball (SHB) and the Stochastic Nesterov Accelerated Gradient (SNAG) algorithms as special cases. In addition, in our formulation, the momentum term is allowed to vary as a function of time (i.e., the iteration counter). The assumptions on the stochastic gradient are the most general in the literature, in that it can be biased, and have a conditional variance that grows in an unbounded fashion as a function of time. This last feature is crucial in order to make the theory applicable to "zero-order" methods, where the gradient is estimated using just two function evaluations.
  We present a set of sufficient conditions for the convergence of the unified algorithm. These conditions are natural generalizations of the familiar Robbins-Monro and Kiefer-Wolfowitz-Blum conditions for standard stochastic gradient descent. We also analyze another method from the literature for the SHB algorithm with a time-varying momentum parameter, and show that it is impracticable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11904v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathukumalli Vidyasagar</dc:creator>
    </item>
    <item>
      <title>Computational Methods and Verification Theorem for Portfolio-Consumption Optimization under Exponential O-U Dynamics</title>
      <link>https://arxiv.org/abs/2508.06491</link>
      <description>arXiv:2508.06491v3 Announce Type: replace 
Abstract: In this paper, we focus on the problem of optimal portfolio-consumption policies in a multi-asset financial market, where the n risky assets follow Exponential Ornstein-Uhlenbeck processes, along with one risk-free bond. The investor's preferences are modeled using Constant Relative Risk Aversion utility with state-dependent stochastic discounting. The problem can be formulated as a high-dimensional stochastic optimal control problem, wherein the associated value function satisfies a Hamilton-Jacobi-Bellman (HJB) equation, which constitutes a necessary condition for optimality. We apply a variable separation technique to transform the HJB equation to a system of ordinary differential equations (ODEs). Then a class of hybrid numerical approaches that integrate exponential Rosenbrock-type methods with Runge-Kutta methods is proposed to solve the ODE system. More importantly, we establish a rigorous verification theorem that provides sufficient conditions for the existence of value function and admissible optimal control, which can be verified numerically. A series of experiments are performed, demonstrating that our proposed method outperforms the conventional grid-based method in both accuracy and computational cost. Furthermore, the numerically derived optimal policy achieves superior performance over all other considered admissible policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06491v3</guid>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaoxiang Zhong, Haiming Song</dc:creator>
    </item>
    <item>
      <title>Revisiting Randomized Smoothing: Nonsmooth Nonconvex Optimization Beyond Global Lipschitz Continuity</title>
      <link>https://arxiv.org/abs/2508.13496</link>
      <description>arXiv:2508.13496v3 Announce Type: replace 
Abstract: Randomized smoothing is a widely adopted technique for optimizing nonsmooth objective functions. However, its efficiency analysis typically relies on global Lipschitz continuity, a condition rarely met in practical applications. To address this limitation, we introduce a new subgradient growth condition that naturally encompasses a wide range of locally Lipschitz functions, with the classical global Lipschitz function as a special case. Under this milder condition, we prove that randomized smoothing yields a differentiable function that satisfies certain generalized smoothness properties. To optimize such functions, we propose novel randomized smoothing gradient algorithms that, with high probability, converge to $(\delta, \epsilon)$-Goldstein stationary points and achieve a sample complexity of $\tilde{\mathcal{O}}(d^{5/2}\delta^{-1}\epsilon^{-4})$. By incorporating variance reduction techniques, we further improve the sample complexity to $\tilde{\mathcal{O}}(d^{3/2}\delta^{-1}\epsilon^{-3})$, matching the optimal $\epsilon$-bound under the global Lipschitz assumption, up to a logarithmic factor. Experimental results validate the effectiveness of our proposed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13496v3</guid>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingfan Xia, Zhenwei Lin, Qi Deng</dc:creator>
    </item>
    <item>
      <title>Linearly involved Generalized Moreau Enhanced Model with Non-quadratic Smooth Convex Data Fidelity Functions</title>
      <link>https://arxiv.org/abs/2509.03258</link>
      <description>arXiv:2509.03258v2 Announce Type: replace 
Abstract: In this paper, we introduce an overall convex model incorporating a nonconvex regularizer. The proposed model is designed by extending the least squares term in the constrained LiGME model [Yata Yamagishi Yamada 2022] to fairly general smooth convex functions for flexible utilization of non-quadratic data fidelity functions. Under an overall convexity condition for the proposed model, we present sufficient conditions for the existence of a minimizer of the proposed model and an inner-loop free algorithm with guaranteed convergence to a global minimizer of the proposed model. To demonstrate the effectiveness of the proposed model and algorithm, we conduct numerical experiments in scenarios of Poisson denoising problem and simultaneous declipping and denoising problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03258v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wataru Yata, Keita Kume, Isao Yamada</dc:creator>
    </item>
    <item>
      <title>Some Remarks on the $l_1$-Robust Solution of LexRank Problem</title>
      <link>https://arxiv.org/abs/2509.04131</link>
      <description>arXiv:2509.04131v2 Announce Type: replace 
Abstract: Graph-based ranking methods, such as LexRank, are fundamental in Natural Language Processing (NLP) applications like text summarization, as they measure the relative importance of textual units. Building on recent advances in ranking methods for growing and dynamic graphs, we develop a robust variant of LexRank that operates on stochastic similarity graphs with uncertain and expanding structure. Our approach introduces a novel $l_1$-based formulation that captures ambiguity in both transition probabilities and graph size, while maintaining sparsity. The resulting non-convex problem is upper-bounded by a linear program, providing a tractable and interpretable approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04131v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anna Timonina-Farkas</dc:creator>
    </item>
    <item>
      <title>$\ell_0$-Norm Multiobjective Optimization Models Motivated by Applications to Proton Therapy</title>
      <link>https://arxiv.org/abs/2509.06833</link>
      <description>arXiv:2509.06833v2 Announce Type: replace 
Abstract: The paper is devoted to investigating single-objective and multiobjective optimization problems involving the $\ell_0$-norm function, which is nonconvex and nondifferentiable. Our motivation comes from proton beam therapy models in cancer research. The developed approach uses subdifferential tools of variational analysis and the Gerstewitz (Tammer) scalarization function in multiobjective optimization. Based on this machinery, we propose several algorithms of the subgradient type and conduct their convergence analysis. The obtained results are illustrated by numerical examples, which reveal some characteristic features of the proposed algorithms and their interactions with the gradient descent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06833v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoda Cong, Xuanfeng Ding, Boris Mordukhovich, Anh Vu Nguyen, Lewei Zhao</dc:creator>
    </item>
    <item>
      <title>Solving Truly Massive Budgeted Monotonic POMDPs with Oracle-Guided Meta-Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2408.07192</link>
      <description>arXiv:2408.07192v2 Announce Type: replace-cross 
Abstract: Monotonic Partially Observable Markov Decision Processes (POMDPs), where the system state progressively decreases until a restorative action is performed, can be used to model sequential repair problems effectively. This paper considers the problem of solving budget-constrained multi-component monotonic POMDPs, where a finite budget limits the maximal number of restorative actions. For a large number of components, solving such a POMDP using current methods is computationally intractable due to the exponential growth in the state space with an increasing number of components. To address this challenge, we propose a two-step approach. Since the individual components of a budget-constrained multi-component monotonic POMDP are only connected via the shared budget, we first approximate the optimal budget allocation among these components using an approximation of each component POMDP's optimal value function which is obtained through a random forest model. Subsequently, we introduce an oracle-guided meta-trained Proximal Policy Optimization (PPO) algorithm to solve each of the independent budget-constrained single-component monotonic POMDPs. The oracle policy is obtained by performing value iteration on the corresponding monotonic Markov Decision Process (MDP). This two-step method provides scalability in solving truly massive multi-component monotonic POMDPs. To demonstrate the efficacy of our approach, we consider a real-world maintenance scenario that involves inspection and repair of an administrative building by a team of agents within a maintenance budget. Finally, we perform a computational complexity analysis for a varying number of components to show the scalability of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07192v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manav Vora, Jonas Liang, Melkior Ornik</dc:creator>
    </item>
    <item>
      <title>Quadratic-form Optimal Transport</title>
      <link>https://arxiv.org/abs/2501.04658</link>
      <description>arXiv:2501.04658v4 Announce Type: replace-cross 
Abstract: We introduce the framework of quadratic-form optimal transport (QOT), whose transport cost has the form $\iint c\,\mathrm{d}\pi \otimes\mathrm{d}\pi$ for some coupling $\pi$ between two marginals. Interesting examples of quadratic-form transport cost and their optimization include inequality measurement, the variance of a bivariate function, covariance, Kendall's tau, the Gromov--Wasserstein distance, quadratic assignment problems, and quadratic regularization of classic optimal transport. QOT leads to substantially different mathematical structures compared to classic transport problems and many technical challenges. We illustrate the fundamental properties of QOT and provide several cases where explicit solutions are obtained. For a wide class of cost functions, including the rectangular cost functions, the QOT problem is solved by a new coupling called the diamond transport, whose copula is supported on a diamond in the unit square.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04658v4</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruodu Wang, Zhenyuan Zhang</dc:creator>
    </item>
    <item>
      <title>Nonlinear Bandwidth and Bode Diagrams based on Scaled Relative Graphs</title>
      <link>https://arxiv.org/abs/2504.01585</link>
      <description>arXiv:2504.01585v2 Announce Type: replace-cross 
Abstract: Scaled Relative Graphs (SRGs) provide a novel graphical frequency-domain method for the analysis of Nonlinear (NL) systems. In this paper, we restrict the SRG to particular input spaces to compute frequency-dependent incremental gain bounds for nonlinear systems. This leads to a NL generalization of the Bode diagram, where the sinusoidal, harmonic, and subharmonic inputs are considered separately. When applied to the analysis of the NL loop transfer and sensitivity, we define a notion of bandwidth for both the open-loop and closed-loop, compatible with the Linear Time-Invariant (LTI) definitions. We illustrate the power of our method on the analysis of a DC motor with a parasitic nonlinearity and verify our results in simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01585v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julius P. J. Krebbekx, Roland T\'oth, Amritam Das</dc:creator>
    </item>
    <item>
      <title>Inexact Column Generation for Bayesian Network Structure Learning via Difference-of-Submodular Optimization</title>
      <link>https://arxiv.org/abs/2505.11089</link>
      <description>arXiv:2505.11089v2 Announce Type: replace-cross 
Abstract: In this paper, we consider a score-based Integer Programming (IP) approach for solving the Bayesian Network Structure Learning (BNSL) problem. State-of-the-art BNSL IP formulations suffer from the exponentially large number of variables and constraints. A standard approach in IP to address such challenges is to employ row and column generation techniques, which dynamically generate rows and columns, while the complex pricing problem remains a computational bottleneck for BNSL. For the general class of $\ell_0$-penalized likelihood scores, we show how the pricing problem can be reformulated as a difference of submodular optimization problem, and how the Difference of Convex Algorithm (DCA) can be applied as an inexact method to efficiently solve the pricing problems. Empirically, we show that, for continuous Gaussian data, our row and column generation approach yields solutions with higher quality than state-of-the-art score-based approaches, especially when the graph density increases, and achieves comparable performance against benchmark constraint-based and hybrid approaches, even when the graph size increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11089v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Yang, Rui Chen</dc:creator>
    </item>
    <item>
      <title>An Efficient Network-aware Direct Search Method for Influence Maximization</title>
      <link>https://arxiv.org/abs/2508.12164</link>
      <description>arXiv:2508.12164v2 Announce Type: replace-cross 
Abstract: Influence Maximization (IM) is a pivotal concept in social network analysis, involving the identification of influential nodes within a network to maximize the number of influenced nodes, and has a wide variety of applications that range from viral marketing and information dissemination to public health campaigns. IM can be modeled as a combinatorial optimization problem with a black-box objective function, where the goal is to select $B$ seed nodes that maximize the expected influence spread. Direct search methods, which do not require gradient information, are well-suited for such problems. Unlike gradient-based approaches, direct search algorithms, in fact, only evaluate the objective function at a suitably chosen set of trial points around the current solution to guide the search process. However, these methods often suffer from scalability issues due to the high cost of function evaluations, especially when applied to combinatorial problems like IM. This work, therefore, proposes the Network-aware Direct Search (NaDS) method, an innovative direct search approach that integrates the network structure into its neighborhood formulation and is used to tackle a mixed-integer programming formulation of the IM problem, the so-called General Information Propagation model. We tested our method on large-scale networks, comparing it to existing state-of-the-art approaches for the IM problem, including direct search methods and various greedy techniques and heuristics. The results of the experiments empirically confirm the assumptions underlying NaDS, demonstrating that exploiting the graph structure of the IM problem in the algorithmic framework can significantly improve its computational efficiency in the considered context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12164v2</guid>
      <category>cs.SI</category>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Bergamaschi, Sara Venturini, Francesco Tudisco, Francesco Rinaldi</dc:creator>
    </item>
    <item>
      <title>Max-Min and 1-Bounded Space Algorithms for the Bin Packing Problem</title>
      <link>https://arxiv.org/abs/2508.18718</link>
      <description>arXiv:2508.18718v2 Announce Type: replace-cross 
Abstract: In the (1-dimensional) bin packing problem, we are asked to pack all the given items into bins, each of capacity one, so that the number of non-empty bins is minimized. Zhu~[Chaos, Solitons \&amp; Fractals 2016] proposed an approximation algorithm $MM$ that sorts the item sequence in a non-increasing order by size at the beginning, and then repeatedly packs, into the current single open bin, first as many of the largest items in the remaining sequence as possible and then as many of the smallest items in the remaining sequence as possible. In this paper we prove that the asymptotic approximation ratio of $MM$ is at most 1.5. Next, focusing on the fact that $MM$ is at the intersection of two algorithm classes, max-min algorithms and 1-bounded space algorithms, we comprehensively analyze the theoretical performance bounds of each subclass derived from the two classes. Our results include a lower bound of 1.25 for the intersection of the two classes. Furthermore, we extend the theoretical analysis over algorithm classes to the cardinality constrained bin packing problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18718v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroshi Fujiwara, Rina Atsumi, Hiroaki Yamamoto</dc:creator>
    </item>
    <item>
      <title>Maximally Resilient Controllers under Temporal Logic Specifications</title>
      <link>https://arxiv.org/abs/2509.01777</link>
      <description>arXiv:2509.01777v2 Announce Type: replace-cross 
Abstract: In this paper, we consider the notion of resilience of a dynamical system, defined by the maximum disturbance a controlled dynamical system can withstand while satisfying given temporal logic specifications. Given a dynamical system and a specification, the objective is to synthesize the controller such that the closed-loop system satisfies this specification while maximizing its resilience. The problem is formulated as a robust optimization program where the objective is to compute the maximum resilience while simultaneously synthesizing the corresponding controller parameters. For linear systems and linear controllers, exact solutions are provided for the class of time-varying polytopic specifications. For the case of nonlinear systems, nonlinear controllers and more general specifications, we leverage tools from the scenario optimization approach, offering a probabilistic guarantee of the solution as well as computational feasibility. Different case studies are presented to illustrate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01777v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youssef Ait Si, Ratnangshu Das, Negar Monir, Sadegh Soudjani, Pushpak Jagtap, Adnane Saoud</dc:creator>
    </item>
  </channel>
</rss>
