<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 14 Apr 2025 04:00:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Autocorrelation functions for point-process time series</title>
      <link>https://arxiv.org/abs/2504.08070</link>
      <description>arXiv:2504.08070v1 Announce Type: new 
Abstract: This article introduces autocorrelograms for time series of point processes. The method is computationally simple, based on binning rather than smoothing. The ability of the method to detect common time series patterns is shown by simulation, and two examples of application to temporal and spatial point processes series are given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08070v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Gervini</dc:creator>
    </item>
    <item>
      <title>Predictive biomarker graphical approach (PRIME) for Precision medicine</title>
      <link>https://arxiv.org/abs/2504.08087</link>
      <description>arXiv:2504.08087v1 Announce Type: new 
Abstract: Precision medicine is an evolving area in the medical field and rely on biomarkers to make patient enrichment decisions, thereby providing drug development direction. A traditional statistical approach is to find the cut-off that leads to the minimum p-value of the interaction between the biomarker dichotomized at that cut-off and treatment. Such an approach does not incorporate clinical significance and the biomarker is not evaluated on a continuous scale. We are proposing to evaluate the biomarker in a continuous manner from a predicted risk standpoint, based on the model that includes the interaction between the biomarker and treatment. The predicted risk can be graphically displayed to explain the relationship between the outcome and biomarker, whereby suggesting a cut-off for biomarker positive/negative groups. We adapt the TreatmentSelection approach and extend it to account for covariates via G-computation. Other features include biomarker comparisons using net gain summary measures and calibration to assess the model fit. The PRIME (Predictive biomarker graphical approach) approach is flexible in the type of outcome and covariates considered. A R package is available and examples will be demonstrated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08087v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gina D'Angelo, Xiaowen Tian, Chuyu Deng, Xian Zhou</dc:creator>
    </item>
    <item>
      <title>On Anticipation Effect in Stepped Wedge Cluster Randomized Trials</title>
      <link>https://arxiv.org/abs/2504.08158</link>
      <description>arXiv:2504.08158v1 Announce Type: new 
Abstract: In stepped wedge cluster randomized trials (SW-CRTs), the intervention is rolled out to clusters over multiple periods. A standard approach for analyzing SW-CRTs utilizes the linear mixed model where the treatment effect is only present after the treatment adoption, under the assumption of no anticipation. This assumption, however, may not always hold in practice because stakeholders, providers, or individuals who are aware of the treatment adoption timing (especially when blinding is challenging or infeasible) can inadvertently change their behaviors in anticipation of the intervention for maximizing potential benefits. We provide an analytical framework to address the anticipation effect in SW-CRTs and study its impact when the treatment effect may or may not depend on the exposure time. We derive expectations of the estimators based on a collection of linear mixed models and demonstrate that when the anticipation effect is ignored, these estimators give biased estimates of the treatment effect. We also provide updated sample size formulas that explicitly account for anticipation effects, exposure-time heterogeneity, or both in SW-CRTs and illustrate how failing to account for these effects when they exist may lead to an underpowered study. Through simulation studies and empirical analyses, we compare the treatment effect estimators under considerations and discuss practical considerations for addressing anticipation in SW-CRTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08158v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Wang, Xinyuan Chen, Katherine R. Courtright, Scott D. Halpern, Michael O. Harhay, Monica Taljaard, Fan Li</dc:creator>
    </item>
    <item>
      <title>Covariance meta regression, with application to mixtures of chemical exposures</title>
      <link>https://arxiv.org/abs/2504.08220</link>
      <description>arXiv:2504.08220v1 Announce Type: new 
Abstract: The motivation of this article is to improve inferences on the covariation in environmental exposures, motivated by data from a study of Toddlers Exposure to SVOCs in Indoor Environments (TESIE). The challenge is that the sample size is limited, so empirical covariance provides a poor estimate. In related applications, Bayesian factor models have been popular; these approaches express the covariance as low rank plus diagonal and can infer the number of factors adaptively. However, they have the disadvantage of shrinking towards a diagonal covariance, often under estimating important covariation patterns in the data. Alternatively, the dimensionality problem is addressed by collapsing the detailed exposure data within chemical classes, potentially obscuring important information. We apply a covariance meta regression extension of Bayesian factor analysis, which improves performance by including information from features summarizing properties of the different exposures. This approach enables shrinkage to more flexible covariance structures, reducing the over-shrinkage problem, as we illustrate in the TESIE data using various chemical features as meta covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08220v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elizabeth Bersson, Kate Hoffman, Heather M. Stapleton, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>A roadmap for systematic identification and analysis of multiple biases in causal inference</title>
      <link>https://arxiv.org/abs/2504.08263</link>
      <description>arXiv:2504.08263v1 Announce Type: new 
Abstract: Observational studies examining causal effects rely on unverifiable causal assumptions, the violation of which can induce multiple biases. Quantitative bias analysis (QBA) methods examine the sensitivity of findings to such violations, generally by producing bias-adjusted estimates under alternative assumptions. Common strategies for QBA address either a single source of bias or multiple sources one at a time, thus not informing the overall impact of the potential biases. We propose a systematic approach (roadmap) for identifying and analysing multiple biases together. Briefly, this consists of (i) articulating the assumptions underlying the primary analysis through specification and emulation of the "ideal trial" that defines the causal estimand of interest and depicting these assumptions using casual diagrams; (ii) depicting alternative assumptions under which biases arise using causal diagrams; (iii) obtaining a single estimate simultaneously adjusted for all biases under the alternative assumptions. We illustrate the roadmap in an investigation of the effect of breastfeeding on risk of childhood asthma. We further use simulations to evaluate a recent simultaneous adjustment approach and illustrate the need for simultaneous rather than one-at-a-time adjustment to examine the overall impact of biases. The proposed roadmap should facilitate the conduct of high-quality multiple bias analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08263v1</guid>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rushani Wijesuriya, Rachael A. Hughes, John B. Carlin, Rachel L. Peters, Jennifer J. Koplin, Margarita Moreno-Betancur</dc:creator>
    </item>
    <item>
      <title>Causal attribution with confidence</title>
      <link>https://arxiv.org/abs/2504.08294</link>
      <description>arXiv:2504.08294v1 Announce Type: new 
Abstract: To answer questions of "causes of effects", the probability of necessity is introduced for assessing whether or not an observed outcome was caused by an earlier treatment. However, the statistical inference for probability of necessity is understudied due to several difficulties, which hinders its application in practice. The evaluation of the probability of necessity involves the joint distribution of potential outcomes, and thus it is in general not point identified and one can at best obtain lower and upper bounds even in randomized experiments, unless certain monotonicity assumptions on potential outcomes are made. Moreover, these bounds are non-smooth functionals of the observed data distribution and standard estimation and inference methods cannot be directly applied. In this paper, we investigate the statistical inference for the probability of necessity in general situations where it may not be point identified. We introduce a mild margin condition to tackle the non-smoothness, under which the bounds become pathwise differentiable. We establish the semiparametric efficiency theory and propose novel asymptotically efficient estimators of the bounds, and further construct confidence intervals for the probability of necessity based on the proposed bounds estimators. The resultant confidence intervals are less conservative than existing methods and can effectively make use of the observed covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08294v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ping Zhang, Ruoyu Wang, Wang Miao</dc:creator>
    </item>
    <item>
      <title>Enhanced Marginal Sensitivity Model and Bounds</title>
      <link>https://arxiv.org/abs/2504.08301</link>
      <description>arXiv:2504.08301v1 Announce Type: new 
Abstract: Sensitivity analysis is important to assess the impact of unmeasured confounding in causal inference from observational studies. The marginal sensitivity model (MSM) provides a useful approach in quantifying the influence of unmeasured confounders on treatment assignment and leading to tractable sharp bounds of common causal parameters. In this paper, to tighten MSM sharp bounds, we propose the enhanced MSM (eMSM) by incorporating another sensitivity constraint that quantifies the influence of unmeasured confounders on outcomes. We derive sharp population bounds of expected potential outcomes under eMSM, which are always narrower than the MSM sharp bounds in a simple and interpretable way. We further discuss desirable specifications of sensitivity parameters related to the outcome sensitivity constraint, and obtain both doubly robust point estimation and confidence intervals for the eMSM population bounds. The effectiveness of eMSM is also demonstrated numerically through two real-data applications. Our development represents, for the first time, a satisfactory extension of MSM to exploit both treatment and outcome sensitivity constraints on unmeasured confounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08301v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yi Zhang, Wenfu Xu, Zhiqiang Tan</dc:creator>
    </item>
    <item>
      <title>High-dimensional Clustering and Signal Recovery under Block Signals</title>
      <link>https://arxiv.org/abs/2504.08332</link>
      <description>arXiv:2504.08332v1 Announce Type: new 
Abstract: This paper studies computationally efficient methods and their minimax optimality for high-dimensional clustering and signal recovery under block signal structures. We propose two sets of methods, cross-block feature aggregation PCA (CFA-PCA) and moving average PCA (MA-PCA), designed for sparse and dense block signals, respectively. Both methods adaptively utilize block signal structures, applicable to non-Gaussian data with heterogeneous variances and non-diagonal covariance matrices. Specifically, the CFA method utilizes a block-wise U-statistic to aggregate and select block signals non-parametrically from data with unknown cluster labels. We show that the proposed methods are consistent for both clustering and signal recovery under mild conditions and weaker signal strengths than the existing methods without considering block structures of signals. Furthermore, we derive both statistical and computational minimax lower bounds (SMLB and CMLB) for high-dimensional clustering and signal recovery under block signals, where the CMLBs are restricted to algorithms with polynomial computation complexity. The minimax boundaries partition signals into regions of impossibility and possibility. No algorithm (or no polynomial time algorithm) can achieve consistent clustering or signal recovery if the signals fall into the statistical (or computational) region of impossibility. We show that the proposed CFA-PCA and MA-PCA methods can achieve the CMLBs for the sparse and dense block signal regimes, respectively, indicating the proposed methods are computationally minimax optimal. A tuning parameter selection method is proposed based on post-clustering signal recovery results. Simulation studies are conducted to evaluate the proposed methods. A case study on global temperature change demonstrates their utility in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08332v1</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wu Su, Yumou Qiu</dc:creator>
    </item>
    <item>
      <title>Standardization of Weighted Ranking Correlation Coefficients</title>
      <link>https://arxiv.org/abs/2504.08428</link>
      <description>arXiv:2504.08428v1 Announce Type: new 
Abstract: A relevant problem in statistics is defining the correlation of two rankings of a list of items. Kendall's tau and Spearman's rho are two well established correlation coefficients, characterized by a symmetric form that ensures zero expected value between two pairs of rankings randomly chosen with uniform probability. However, in recent years, several weighted versions of the original Spearman and Kendall coefficients have emerged that take into account the greater importance of top ranks compared to low ranks, which is common in many contexts. The weighting schemes break the symmetry, causing a non-zero expected value between two random rankings. This issue is very relevant, as it undermines the concept of uncorrelation between rankings. In this paper, we address this problem by proposing a standardization function $g(x)$ that maps a correlation ranking coefficient $\Gamma$ in a standard form $g(\Gamma)$ that has zero expected value, while maintaining the relevant statistical properties of $\Gamma$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08428v1</guid>
      <category>stat.ME</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierangelo Lombardo</dc:creator>
    </item>
    <item>
      <title>A dependent and censored first hitting-time model with compound Poisson processes</title>
      <link>https://arxiv.org/abs/2504.08483</link>
      <description>arXiv:2504.08483v1 Announce Type: new 
Abstract: We consider a bivariate first hitting-time model in which durations are the crossing times of dependent compound Poisson processes with fixed thresholds. The identifiability of the model is discussed, and likelihood estimators of the model parameters are proposed. We obtain the asymptotic properties of the estimators and underline their finite sample performance with a simulation study on synthetic data. The practical applicability of our approach is demonstrated by an application using data from patients suffering from mushroom poisoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08483v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mikael Escobar-Bach, Alexandre Popier, Malo Sahin</dc:creator>
    </item>
    <item>
      <title>Bayesian optimization for mixed variables using an adaptive dimension reduction process: applications to aircraft design</title>
      <link>https://arxiv.org/abs/2504.08682</link>
      <description>arXiv:2504.08682v1 Announce Type: new 
Abstract: Multidisciplinary design optimization methods aim at adapting numerical optimization techniques to the design of engineering systems involving multiple disciplines. In this context, a large number of mixed continuous, integer and categorical variables might arise during the optimization process and practical applications involve a large number of design variables. Recently, there has been a growing interest in mixed variables constrained Bayesian optimization but most existing approaches severely increase the number of the hyperparameters related to the surrogate model. In this paper, we address this issue by constructing surrogate models using less hyperparameters. The reduction process is based on the partial least squares method. An adaptive procedure for choosing the number of hyperparameters is proposed. The performance of the proposed approach is confirmed on analytical tests as well as two real applications related to aircraft design. A significant improvement is obtained compared to genetic algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08682v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.2514/6.2022-0082</arxiv:DOI>
      <dc:creator>Paul Saves, Nathalie Bartoli, Youssef Diouane, Thierry Lefebvre, Joseph Morlier, Christophe David, Eric Nguyen Van, S\'ebastien Defoort</dc:creator>
    </item>
    <item>
      <title>ICBM community cancer registry analysis: a focus on Non-Hodgkin Lymphoma cases in missileers</title>
      <link>https://arxiv.org/abs/2504.08004</link>
      <description>arXiv:2504.08004v1 Announce Type: cross 
Abstract: This study investigates the incidence and age at diagnosis of Non-Hodgkin Lymphoma (NHL) among missileers stationed at Malmstrom Air Force Base (MAFB) compared to national benchmarks. The analysis was motivated by reports of elevated cancer diagnoses within the Intercontinental Ballistic Missile (ICBM) community, specifically targeting NHL cases due to initial media focus and data collection through the Torchlight Initiative. The methodology integrates simulation-based estimation of expected diagnoses using incomplete data and expert knowledge on the underlying population. Statistical tests, including the Standardized Incidence Ratio (SIR) and a nonparametric Sign Test, were used to evaluate both the rate of diagnosis and age at diagnosis. The results demonstrate a statistically significant increase in NHL diagnoses among missileers in the later decades, with observed rates surpassing expected benchmarks. The study also finds that the median age of diagnosis is significantly younger for the study population compared to national averages. Key methodological contributions include estimating the population size and service start ages when comprehensive cohort data is unavailable, incorporating uncertainty quantification, and applying multiple hypothesis testing to identify temporal patterns. While the study acknowledges limitations such as small sample size and estimation uncertainty, as well as recognizing the study subjects as self-reported diagnoses, the findings highlight the effectiveness of these statistical techniques in identifying significant deviations from expected rates. Future studies should refine and build upon these methods, incorporating survival analysis and additional covariates to improve the robustness and scope of the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08004v1</guid>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dawn L. Sanderson, Richard L. Smith</dc:creator>
    </item>
    <item>
      <title>Optimal Transport-Based Generative Models for Bayesian Posterior Sampling</title>
      <link>https://arxiv.org/abs/2504.08214</link>
      <description>arXiv:2504.08214v1 Announce Type: cross 
Abstract: We investigate the problem of sampling from posterior distributions with intractable normalizing constants in Bayesian inference. Our solution is a new generative modeling approach based on optimal transport (OT) that learns a deterministic map from a reference distribution to the target posterior through constrained optimization. The method uses structural constraints from OT theory to ensure uniqueness of the solution and allows efficient generation of many independent, high-quality posterior samples. The framework supports both continuous and mixed discrete-continuous parameter spaces, with specific adaptations for latent variable models and near-Gaussian posteriors. Beyond computational benefits, it also enables new inferential tools based on OT-derived multivariate ranks and quantiles for Bayesian exploratory analysis and visualization. We demonstrate the effectiveness of our approach through multiple simulation studies and a real-world data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08214v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ke Li, Wei Han, Yuexi Wang, Yun Yang</dc:creator>
    </item>
    <item>
      <title>An Introduction to Double/Debiased Machine Learning</title>
      <link>https://arxiv.org/abs/2504.08324</link>
      <description>arXiv:2504.08324v1 Announce Type: cross 
Abstract: This paper provides a practical introduction to Double/Debiased Machine Learning (DML). DML provides a general approach to performing inference about a target parameter in the presence of nuisance parameters. The aim of DML is to reduce the impact of nuisance parameter estimation on estimators of the parameter of interest. We describe DML and its two essential components: Neyman orthogonality and cross-fitting. We highlight that DML reduces functional form dependence and accommodates the use of complex data types, such as text data. We illustrate its application through three empirical examples that demonstrate DML's applicability in cross-sectional and panel settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08324v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Achim Ahrens, Victor Chernozhukov, Christian Hansen, Damian Kozbur, Mark Schaffer, Thomas Wiemann</dc:creator>
    </item>
    <item>
      <title>Multi-resolution filters via linear projection for large spatio-temporal datasets</title>
      <link>https://arxiv.org/abs/2401.05315</link>
      <description>arXiv:2401.05315v3 Announce Type: replace 
Abstract: Advances in compact sensing devices mounted on satellites have facilitated the collection of large spatio-temporal datasets with coordinates. Since such datasets are often incomplete and noisy, it is useful to create the prediction surface of a spatial field. To this end, we consider an online filtering inference by using the Kalman filter based on linear Gaussian state-space models. However, the Kalman filter is impractically time-consuming when the number of locations in spatio-temporal datasets is large. To address this problem, we propose a multi-resolution filter via linear projection (MRF-lp), a fast computation method for online filtering inference. In the MRF-lp, by carrying out a multi-resolution approximation via linear projection (MRA-lp), the forecast covariance matrix can be approximated while capturing both the large- and small-scale spatial variations. As a result of this approximation, our proposed MRF-lp preserves a block-sparse structure of some matrices appearing in the MRF-lp through time, which leads to the scalability of this algorithm. Additionally, we discuss extensions of the MRF-lp to a nonlinear and non-Gaussian case. Simulation studies and real data analysis for total precipitable water vapor demonstrate that our proposed approach performs well compared with the related methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05315v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toshihiro Hirano, Tsunehiro Ishihara</dc:creator>
    </item>
    <item>
      <title>Nonparametric efficient causal estimation of the intervention-specific expected number of recurrent events with continuous-time targeted maximum likelihood and highly adaptive lasso estimation</title>
      <link>https://arxiv.org/abs/2404.01736</link>
      <description>arXiv:2404.01736v4 Announce Type: replace 
Abstract: Longitudinal settings involving outcome, competing risks and censoring events occurring and recurring in continuous time are common in medical research, but are often analyzed with methods that do not allow for taking post-baseline information into account. In this work, we define statistical and causal target parameters via the g-computation formula by carrying out interventions directly on the product integral representing the observed data distribution in a continuous-time counting process model framework. In recurrent events settings our target parameter identifies the expected number of recurrent events also in settings where the censoring mechanism or post-baseline treatment decisions depend on past information of post-baseline covariates such as the recurrent event process. We propose a flexible estimation procedure based on targeted maximum likelihood estimation coupled with highly adaptive lasso estimation to provide a novel approach for double robust and nonparametric inference for the considered target parameter. We illustrate the methods in a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01736v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Helene C. W. Rytgaard, Mark J. van der Laan</dc:creator>
    </item>
    <item>
      <title>Non-parametric estimation of transition intensities in interval censored Markov multi-state models without loops</title>
      <link>https://arxiv.org/abs/2409.07176</link>
      <description>arXiv:2409.07176v2 Announce Type: replace 
Abstract: Interval-censored multi state data is collected when the state of a subject is observed periodically. The analysis of such data using non-parametric multi-state models was not possible until recently, but is very desirable as it allows for more flexibility than its parametric counterparts. The single available result to date has some unique drawbacks. We propose a non-parametric estimator of the transition intensities for interval-censored multi state data using an Expectation Maximisation algorithm. The method allows for a mix of interval-censored and right-censored (exactly observed) transitions. A condition to check for the convergence of the algorithm is given. A simulation study comparing the proposed estimator to a consistent estimator is performed, and shown to yield near identical estimates at smaller computational cost. A data set on the emergence of teeth in children is analysed. Software to perform the analyses is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07176v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Gomon, Hein Putter</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Epidemic Models</title>
      <link>https://arxiv.org/abs/2410.11743</link>
      <description>arXiv:2410.11743v2 Announce Type: replace 
Abstract: Epidemic models describe the evolution of a communicable disease over time. These models are often modified to include the effects of interventions (control measures) such as vaccination, social distancing, school closings etc. Many such models were proposed during the COVID-19 epidemic. Inevitably these models are used to answer the question: What is the effect of the intervention on the epidemic? These models can either be interpreted as data generating models describing observed random variables or as causal models for counterfactual random variables. These two interpretations are often conflated in the literature. We discuss the difference between these two types of models, and then we discuss how to estimate the parameters of the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11743v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejong Bong, Val\'erie Ventura, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Conditional Extremes with Graphical Models</title>
      <link>https://arxiv.org/abs/2411.17013</link>
      <description>arXiv:2411.17013v2 Announce Type: replace 
Abstract: Multivariate extreme value analysis quantifies the probability and magnitude of joint extreme events. River discharges from the upper Danube River basin provide a challenging dataset for such analysis because the data, which is measured on a spatial network, exhibits both asymptotic dependence and asymptotic independence. To account for both features, we extend the conditional multivariate extreme value model (CMEVM) with a new approach for the residual distribution. This allows sparse (graphical) dependence structures and fully parametric prediction. Our approach fills a current gap in statistical methodology by extending graphical extremes models to asymptotically independent random variables. Further, the model can be used to learn the graphical dependence structure when it is unknown a priori. To support inference in high dimensions, we propose a stepwise inference procedure that is computationally efficient and loses no information or predictive power. We show our method is flexible and accurately captures the extremal dependence for the upper Danube River basin discharges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17013v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aiden Farrell, Emma F. Eastoe, Clement Lee</dc:creator>
    </item>
    <item>
      <title>A partial likelihood approach to tree-based density modeling and its application in Bayesian inference</title>
      <link>https://arxiv.org/abs/2412.11692</link>
      <description>arXiv:2412.11692v5 Announce Type: replace 
Abstract: Tree-based priors for probability distributions are usually specified using a predetermined, data-independent collection of candidate recursive partitions of the sample space. To characterize an unknown target density in detail over the entire sample space, candidate partitions must have the capacity to expand deeply into all areas of the sample space with potential non-zero sampling probability. Such an expansive system of partitions often incurs prohibitive computational costs and makes inference prone to overfitting, especially in regions with little probability mass. Thus, existing models typically make a compromise and rely on relatively shallow trees. This hampers one of the most desirable features of trees, their ability to characterize local features, and results in reduced statistical efficiency. Traditional wisdom suggests that this compromise is inevitable to ensure coherent likelihood-based reasoning in Bayesian inference, as a data-dependent partition system that allows deeper expansion only in regions with more observations would induce double dipping of the data. We propose a simple strategy to restore coherency while allowing the candidate partitions to be data-dependent, using Cox's partial likelihood. Our partial likelihood approach is broadly applicable to existing likelihood-based methods and, in particular, to Bayesian inference on tree-based models. We give examples in density estimation in which the partial likelihood is endowed with existing priors on tree-based models and compare with the standard, full-likelihood approach. The results show substantial gains in estimation accuracy and computational efficiency from adopting the partial likelihood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11692v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Ma, Benedetta Bruni</dc:creator>
    </item>
    <item>
      <title>RITHMS : An advanced stochastic framework for the simulation of transgenerational hologenomic data</title>
      <link>https://arxiv.org/abs/2502.07366</link>
      <description>arXiv:2502.07366v2 Announce Type: replace 
Abstract: A holobiont is made up of a host organism together with its microbiota. In the context of animal breeding, as the holobiont can be viewed as the single unit upon which selection operates, integrating microbiota data into genomic prediction models may be a promising approach to improve predictions of phenotypic and genetic values. Nevertheless, there is a paucity of hologenomic transgenerational data to address this hypothesis, and thus to fill this gap, we propose a new simulation framework. Our approach, an R Implementation of a Transgenerational Hologenomic Model-based Simulator (RITHMS) is an open-source package, builds upon the MoBPS package and incorporates distinctive characteristics of the microbiota, notably vertical and horizontal transmission as well as modulation due to the environment and host genetics. In addition, RITHMS can account for a variety of selection strategies and is adaptable to different genetic architectures. We simulated transgenerational hologenomic data using RITHMS under a wide variety of scenarios, varying heritability, microbiability, and microbiota heritability. We found that simulated data accurately reflected expected characteristics, notably based on microbial diversity metrics, correlation between taxa, modulation of vertical and horizontal transmission, response to environmental effects and the evolution of phenotypic values depending on selection strategy. Our results support the relevance of our simulation framework and illustrate its possible use for building a selection index balancing genetic gain and microbial diversity. RITHMS is an advanced, flexible tool for generating transgenerational hologenomic data that incorporate the complex interplay between genetics, microbiota and environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07366v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sol\`ene Pety (MaIAGE, GABI), Mahendra Mariadassou (MaIAGE), Ingrid David (GenPhySE), Andrea Rau (GABI)</dc:creator>
    </item>
    <item>
      <title>Auditing Differential Privacy in the Black-Box Setting</title>
      <link>https://arxiv.org/abs/2503.12045</link>
      <description>arXiv:2503.12045v2 Announce Type: replace 
Abstract: This paper introduces a novel theoretical framework for auditing differential privacy (DP) in a black-box setting. Leveraging the concept of $f$-differential privacy, we explicitly define type I and type II errors and propose an auditing mechanism based on conformal inference. Our approach robustly controls the type I error rate under minimal assumptions. Furthermore, we establish a fundamental impossibility result, demonstrating the inherent difficulty of simultaneously controlling both type I and type II errors without additional assumptions. Nevertheless, under a monotone likelihood ratio (MLR) assumption, our auditing mechanism effectively controls both errors. We also extend our method to construct valid confidence bands for the trade-off function in the finite-sample regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12045v2</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaining Shi, Cong Ma</dc:creator>
    </item>
    <item>
      <title>Using directed acyclic graphs to determine whether multiple imputation or subsample multiple imputation estimates of an exposure-outcome association are unbiased</title>
      <link>https://arxiv.org/abs/2503.24035</link>
      <description>arXiv:2503.24035v2 Announce Type: replace 
Abstract: Missing data is a pervasive problem in epidemiology, with multiple imputation (MI) a commonly used analysis method. MI is valid when data are missing at random (MAR). However, definitions of MAR with multiple incomplete variables are not easily interpretable and graphical model-based conditions are not accessible to applied researchers. Previous literature shows that MI may be valid in subsamples, even if not in the full dataset. Practical guidance on applying MI with multiple incomplete variables is lacking. We present an algorithm using directed acyclic graphs to determine when MI will estimate an exposure-outcome coefficient without bias. We extend the algorithm to assess whether MI in a subsample of the data, in which some variables are complete, and the remaining are imputed, will be valid and unbiased for the same coefficient. We apply the algorithm to several simple exemplars, and in a more complex real-life example highlight that only subsample MI of the outcome would be valid. Our algorithm provides researchers with the tools to decide whether (and how) to use MI in practice when there are multiple incomplete variables. Further work could focus on the likely size and direction of biases, and the impact of different missing data patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24035v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paul Madley-Dowd, Rachael A. Hughes, Maya B. Mathur, Jon Heron, Kate Tilling</dc:creator>
    </item>
    <item>
      <title>Online Bayesian changepoint detection for network Poisson processes with community structure</title>
      <link>https://arxiv.org/abs/2407.04138</link>
      <description>arXiv:2407.04138v2 Announce Type: replace-cross 
Abstract: Network point processes often exhibit latent structure that govern the behaviour of the sub-processes. It is not always reasonable to assume that this latent structure is static, and detecting when and how this driving structure changes is often of interest. In this paper, we introduce a novel online methodology for detecting changes within the latent structure of a network point process. We focus on block-homogeneous Poisson processes, where latent node memberships determine the rates of the edge processes. We propose a scalable variational procedure which can be applied on large networks in an online fashion via a Bayesian forgetting factor applied to sequential variational approximations to the posterior distribution. The proposed framework is tested on simulated and real-world data, and it rapidly and accurately detects changes to the latent edge process rates, and to the latent node group memberships, both in an online manner. In particular, in an application on the Santander Cycles bike-sharing network in central London, we detect changes within the network related to holiday periods and lockdown restrictions between 2019 and 2020.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04138v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11222-025-10606-w</arxiv:DOI>
      <arxiv:journal_reference>Stat Comput 35, 75 (2025)</arxiv:journal_reference>
      <dc:creator>Joshua Corneck, Edward A. K. Cohen, James S. Martin, Francesco Sanna Passino</dc:creator>
    </item>
    <item>
      <title>Microfoundation Inference for Strategic Prediction</title>
      <link>https://arxiv.org/abs/2411.08998</link>
      <description>arXiv:2411.08998v2 Announce Type: replace-cross 
Abstract: Often in prediction tasks, the predictive model itself can influence the distribution of the target variable, a phenomenon termed performative prediction. Generally, this influence stems from strategic actions taken by stakeholders with a vested interest in predictive models. A key challenge that hinders the widespread adaptation of performative prediction in machine learning is that practitioners are generally unaware of the social impacts of their predictions. To address this gap, we propose a methodology for learning the distribution map that encapsulates the long-term impacts of predictive models on the population. Specifically, we model agents' responses as a cost-adjusted utility maximization problem and propose estimates for said cost. Our approach leverages optimal transport to align pre-model exposure (ex ante) and post-model exposure (ex post) distributions. We provide a rate of convergence for this proposed estimate and assess its quality through empirical demonstrations on a credit-scoring dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08998v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Bracale, Subha Maity, Felipe Maia Polo, Seamus Somerstep, Moulinath Banerjee, Yuekai Sun</dc:creator>
    </item>
  </channel>
</rss>
