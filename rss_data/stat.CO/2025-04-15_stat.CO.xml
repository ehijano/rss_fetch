<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Apr 2025 01:58:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Stochastic Claims Reserving Using State Space Modeling</title>
      <link>https://arxiv.org/abs/2504.09292</link>
      <description>arXiv:2504.09292v1 Announce Type: new 
Abstract: Claims reserving, also known as Incurred But Not Reported (IBNR) claims prediction, is an important issue in general insurance. State space modeling is widely recognized as a statistically robust method for addressing this problem. In state space model-based claims reserving, the Kalman filter and Kalman smoother algorithms are employed for model fitting, diagnostics, and deriving reserve estimates. Additionally, the simulation smoother algorithm is used to obtain the sampling distribution of the derived reserve estimate. The integration of these three algorithms results in an elegant and transparent claim reserving process. Various state space models (SSMs) have been proposed in the literature for claims reserving. This article outlines a step-by-step process for computing the SSM-based reserve estimate and its associated sampling distribution for any proposed SSM. A brief discussion on model selection is also included. The claims reserving computations are demonstrated using a real-life data set. The state space modeling computations in the illustrations are performed by using the CSSM procedure in SAS Viya/Econometrics software. The SAS code for reproducing the output in the illustrations is provided in the supplementary material.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09292v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajesh Selukar</dc:creator>
    </item>
    <item>
      <title>Adaptive Robustness of Hypergrid Johnson-Lindenstrauss</title>
      <link>https://arxiv.org/abs/2504.09331</link>
      <description>arXiv:2504.09331v1 Announce Type: new 
Abstract: Johnson and Lindenstrauss (Contemporary Mathematics, 1984) showed that for $n &gt; m$, a scaled random projection $\mathbf{A}$ from $\mathbb{R}^n$ to $\mathbb{R}^m$ is an approximate isometry on any set $S$ of size at most exponential in $m$. If $S$ is larger, however, its points can contract arbitrarily under $\mathbf{A}$. In particular, the hypergrid $([-B, B] \cap \mathbb{Z})^n$ is expected to contain a point that is contracted by a factor of $\kappa_{\mathsf{stat}} = \Theta(B)^{-1/\alpha}$, where $\alpha = m/n$.
  We give evidence that finding such a point exhibits a statistical-computational gap precisely up to $\kappa_{\mathsf{comp}} = \widetilde{\Theta}(\sqrt{\alpha}/B)$. On the algorithmic side, we design an online algorithm achieving $\kappa_{\mathsf{comp}}$, inspired by a discrepancy minimization algorithm of Bansal and Spencer (Random Structures &amp; Algorithms, 2020). On the hardness side, we show evidence via a multiple overlap gap property (mOGP), which in particular captures online algorithms; and a reduction-based lower bound, which shows hardness under standard worst-case lattice assumptions.
  As a cryptographic application, we show that the rounded Johnson-Lindenstrauss embedding is a robust property-preserving hash function (Boyle, Lavigne and Vaikuntanathan, TCC 2019) on the hypergrid for the Euclidean metric in the computationally hard regime. Such hash functions compress data while preserving $\ell_2$ distances between inputs up to some distortion factor, with the guarantee that even knowing the hash function, no computationally bounded adversary can find any pair of points that violates the distortion bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09331v1</guid>
      <category>stat.CO</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrej Bogdanov, Alon Rosen, Neekon Vafa, Vinod Vaikuntanathan</dc:creator>
    </item>
    <item>
      <title>Particle Hamiltonian Monte Carlo</title>
      <link>https://arxiv.org/abs/2504.09875</link>
      <description>arXiv:2504.09875v1 Announce Type: new 
Abstract: In Bayesian inference, Hamiltonian Monte Carlo (HMC) is a popular Markov Chain Monte Carlo (MCMC) algorithm known for its efficiency in sampling from complex probability distributions. However, its application to models with latent variables, such as state-space models, poses significant challenges. These challenges arise from the need to compute gradients of the log-posterior of the latent variables, and the likelihood may be intractable due to the complexity of the underlying model. In this paper, we propose Particle Hamiltonian Monte Carlo (PHMC), an algorithm specifically designed for state-space models. PHMC leverages Sequential Monte Carlo (SMC) methods to estimate the marginal likelihood, infer latent variables (as in particle Metropolis-Hastings), and compute gradients of the log-posterior of model parameters. Importantly, PHMC avoids the need to calculate gradients of the log-posterior for latent variables, which addresses a major limitation of traditional HMC approaches. We assess the performance of Particle HMC on both simulated datasets and a real-world dataset involving crowdsourced cycling activities data. The results demonstrate that Particle HMC outperforms particle marginal Metropolis-Hastings with a Gaussian random walk, particularly in scenarios involving a large number of parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09875v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alaa Amri, V\'ictor Elvira, Amy L. Wilson</dc:creator>
    </item>
    <item>
      <title>Finite Mixture Cox Model for Heterogeneous Time-dependent Right-Censored Data</title>
      <link>https://arxiv.org/abs/2504.08908</link>
      <description>arXiv:2504.08908v1 Announce Type: cross 
Abstract: In this study, we address the challenge of survival analysis within heterogeneous patient populations, where traditional reliance on a single regression model such as the Cox proportional hazards (Cox PH) model often falls short. Recognizing that such populations frequently exhibit varying covariate effects, resulting in distinct subgroups, we argue for the necessity of using separate regression models for each subgroup to avoid the biases and inaccuracies inherent in a uniform model. To address subgroup identification and component selection in survival analysis, we propose a novel approach that integrates the Cox PH model with dynamic penalty functions, specifically the smoothly clipped absolute deviation (SCAD) and the minimax concave penalty (MCP). These modifications provide a more flexible and theoretically sound method for determining the optimal number of mixture components, which is crucial for accurately modeling heterogeneous datasets. Through a modified expectation--maximization (EM) algorithm for parameter estimation and component selection, supported by simulation studies and two real data analyses, our method demonstrates improved precision in risk prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08908v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmad Talafha</dc:creator>
    </item>
    <item>
      <title>Robust Bayesian methods using amortized simulation-based inference</title>
      <link>https://arxiv.org/abs/2504.09475</link>
      <description>arXiv:2504.09475v1 Announce Type: cross 
Abstract: Bayesian simulation-based inference (SBI) methods are used in statistical models where simulation is feasible but the likelihood is intractable. Standard SBI methods can perform poorly in cases of model misspecification, and there has been much recent work on modified SBI approaches which are robust to misspecified likelihoods. However, less attention has been given to the issue of inappropriate prior specification, which is the focus of this work. In conventional Bayesian modelling, there will often be a wide range of prior distributions consistent with limited prior knowledge expressed by an expert. Choosing a single prior can lead to an inappropriate choice, possibly conflicting with the likelihood information. Robust Bayesian methods, where a class of priors is considered instead of a single prior, can address this issue. For each density in the prior class, a posterior can be computed, and the range of the resulting inferences is informative about posterior sensitivity to the prior imprecision. We consider density ratio classes for the prior and implement robust Bayesian SBI using amortized neural methods developed recently in the literature. We also discuss methods for checking for conflict between a density ratio class of priors and the likelihood, and sequential updating methods for examining conflict between different groups of summary statistics. The methods are illustrated for several simulated and real examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09475v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wang Yuyan, Michael Evans, David J. Nott</dc:creator>
    </item>
    <item>
      <title>RANSAC Revisited: An Improved Algorithm for Robust Subspace Recovery under Adversarial and Noisy Corruptions</title>
      <link>https://arxiv.org/abs/2504.09648</link>
      <description>arXiv:2504.09648v1 Announce Type: cross 
Abstract: In this paper, we study the problem of robust subspace recovery (RSR) in the presence of both strong adversarial corruptions and Gaussian noise. Specifically, given a limited number of noisy samples -- some of which are tampered by an adaptive and strong adversary -- we aim to recover a low-dimensional subspace that approximately contains a significant fraction of the uncorrupted samples, up to an error that scales with the Gaussian noise. Existing approaches to this problem often suffer from high computational costs or rely on restrictive distributional assumptions, limiting their applicability in truly adversarial settings. To address these challenges, we revisit the classical random sample consensus (RANSAC) algorithm, which offers strong robustness to adversarial outliers, but sacrifices efficiency and robustness against Gaussian noise and model misspecification in the process. We propose a two-stage algorithm, RANSAC+, that precisely pinpoints and remedies the failure modes of standard RANSAC. Our method is provably robust to both Gaussian and adversarial corruptions, achieves near-optimal sample complexity without requiring prior knowledge of the subspace dimension, and is more efficient than existing RANSAC-type methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09648v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>stat.CO</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guixian Chen, Jianhao Ma, Salar Fattahi</dc:creator>
    </item>
    <item>
      <title>BLAST: Bayesian online change-point detection with structured image data</title>
      <link>https://arxiv.org/abs/2504.09783</link>
      <description>arXiv:2504.09783v1 Announce Type: cross 
Abstract: The prompt online detection of abrupt changes in image data is essential for timely decision-making in broad applications, from video surveillance to manufacturing quality control. Existing methods, however, face three key challenges. First, the high-dimensional nature of image data introduces computational bottlenecks for efficient real-time monitoring. Second, changes often involve structural image features, e.g., edges, blurs and/or shapes, and ignoring such structure can lead to delayed change detection. Third, existing methods are largely non-Bayesian and thus do not provide a quantification of monitoring uncertainty for confident detection. We address this via a novel Bayesian onLine Structure-Aware change deTection (BLAST) method. BLAST first leverages a deep Gaussian Markov random field prior to elicit desirable image structure from offline reference data. With this prior elicited, BLAST employs a new Bayesian online change-point procedure for image monitoring via its so-called posterior run length distribution. This posterior run length distribution can be computed in an online fashion using $\mathcal{O}(p^2)$ work at each time-step, where $p$ is the number of image pixels; this facilitates scalable Bayesian online monitoring of large images. We demonstrate the effectiveness of BLAST over existing methods in a suite of numerical experiments and in two applications, the first on street scene monitoring and the second on real-time process monitoring for metal additive manufacturing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09783v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaojun Zheng, Simon Mak</dc:creator>
    </item>
    <item>
      <title>Bayesian optimal experimental design with Wasserstein information criteria</title>
      <link>https://arxiv.org/abs/2504.10092</link>
      <description>arXiv:2504.10092v1 Announce Type: cross 
Abstract: Bayesian optimal experimental design (OED) provides a principled framework for selecting the most informative observational settings in experiments. With rapid advances in computational power, Bayesian OED has become increasingly feasible for inference problems involving large-scale simulations, attracting growing interest in fields such as inverse problems. In this paper, we introduce a novel design criterion based on the expected Wasserstein-$p$ distance between the prior and posterior distributions. Especially, for $p=2$, this criterion shares key parallels with the widely used expected information gain (EIG), which relies on the Kullback--Leibler divergence instead. First, the Wasserstein-2 criterion admits a closed-form solution for Gaussian regression, a property which can be also leveraged for approximative schemes. Second, it can be interpreted as maximizing the information gain measured by the transport cost incurred when updating the prior to the posterior. Our main contribution is a stability analysis of the Wasserstein-1 criterion, where we provide a rigorous error analysis under perturbations of the prior or likelihood. We partially extend this study also to the Wasserstein-2 criterion. In particular, these results yield error rates when empirical approximations of priors are used. Finally, we demonstrate the computability of the Wasserstein-2 criterion and demonstrate our approximation rates through simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10092v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tapio Helin, Youssef Marzouk, Jose Rodrigo Rojo-Garcia</dc:creator>
    </item>
    <item>
      <title>Conditional Distribution Compression via the Kernel Conditional Mean Embedding</title>
      <link>https://arxiv.org/abs/2504.10139</link>
      <description>arXiv:2504.10139v1 Announce Type: cross 
Abstract: Existing distribution compression methods, like Kernel Herding (KH), were originally developed for unlabelled data. However, no existing approach directly compresses the conditional distribution of labelled data. To address this gap, we first introduce the Average Maximum Conditional Mean Discrepancy (AMCMD), a natural metric for comparing conditional distributions. We then derive a consistent estimator for the AMCMD and establish its rate of convergence. Next, we make a key observation: in the context of distribution compression, the cost of constructing a compressed set targeting the AMCMD can be reduced from $\mathcal{O}(n^3)$ to $\mathcal{O}(n)$. Building on this, we extend the idea of KH to develop Average Conditional Kernel Herding (ACKH), a linear-time greedy algorithm that constructs a compressed set targeting the AMCMD. To better understand the advantages of directly compressing the conditional distribution rather than doing so via the joint distribution, we introduce Joint Kernel Herding (JKH), a straightforward adaptation of KH designed to compress the joint distribution of labelled data. While herding methods provide a simple and interpretable selection process, they rely on a greedy heuristic. To explore alternative optimisation strategies, we propose Joint Kernel Inducing Points (JKIP) and Average Conditional Kernel Inducing Points (ACKIP), which jointly optimise the compressed set while maintaining linear complexity. Experiments show that directly preserving conditional distributions with ACKIP outperforms both joint distribution compression (via JKH and JKIP) and the greedy selection used in ACKH. Moreover, we see that JKIP consistently outperforms JKH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10139v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominic Broadbent, Nick Whiteley, Robert Allison, Tom Lovett</dc:creator>
    </item>
    <item>
      <title>Learned Reference-based Diffusion Sampling for multi-modal distributions</title>
      <link>https://arxiv.org/abs/2410.19449</link>
      <description>arXiv:2410.19449v3 Announce Type: replace-cross 
Abstract: Over the past few years, several approaches utilizing score-based diffusion have been proposed to sample from probability distributions, that is without having access to exact samples and relying solely on evaluations of unnormalized densities. The resulting samplers approximate the time-reversal of a noising diffusion process, bridging the target distribution to an easy-to-sample base distribution. In practice, the performance of these methods heavily depends on key hyperparameters that require ground truth samples to be accurately tuned. Our work aims to highlight and address this fundamental issue, focusing in particular on multi-modal distributions, which pose significant challenges for existing sampling methods. Building on existing approaches, we introduce Learned Reference-based Diffusion Sampler (LRDS), a methodology specifically designed to leverage prior knowledge on the location of the target modes in order to bypass the obstacle of hyperparameter tuning. LRDS proceeds in two steps by (i) learning a reference diffusion model on samples located in high-density space regions and tailored for multimodality, and (ii) using this reference model to foster the training of a diffusion-based sampler. We experimentally demonstrate that LRDS best exploits prior knowledge on the target distribution compared to competing algorithms on a variety of challenging distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19449v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maxence Noble, Louis Grenioux, Marylou Gabri\'e, Alain Oliviero Durmus</dc:creator>
    </item>
    <item>
      <title>CriteoPrivateAds: A Real-World Bidding Dataset to Design Private Advertising Systems</title>
      <link>https://arxiv.org/abs/2502.12103</link>
      <description>arXiv:2502.12103v3 Announce Type: replace-cross 
Abstract: In the past years, many proposals have emerged in order to address online advertising use-cases without access to third-party cookies. All these proposals leverage some privacy-enhancing technologies such as aggregation or differential privacy. Yet, no public and rich-enough ground truth is currently available to assess the relevancy of aforementioned private advertising frameworks. We are releasing the largest, in terms of number of features, bidding dataset specifically built in alignment with the design of major browser vendors proposals such as Chrome Privacy Sandbox. This dataset, coined CriteoPrivateAds, stands for an anonymised version of Criteo production logs and provides sufficient data to learn bidding models commonly used in online advertising under many privacy constraints (delayed reports, display and user-level differential privacy, user signal quantisation or aggregated reports). We ensured that this dataset, while being anonymised, is able to provide offline results close to production performance of adtech companies including Criteo - making it a relevant ground truth to design private advertising systems. The dataset is available in Hugging Face: https://huggingface.co/datasets/criteo/CriteoPrivateAd.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12103v3</guid>
      <category>cs.CR</category>
      <category>stat.CO</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mehdi Sebbar, Corentin Odic, Mathieu L\'echine, Alo\"is Bissuel, Nicolas Chrysanthos, Anthony D'Amato, Alexandre Gilotte, Fabian H\"oring, Sarah Nogueira, Maxime Vono</dc:creator>
    </item>
    <item>
      <title>Semiparametric Growth-Curve Modeling in Hierarchical, Longitudinal Studies</title>
      <link>https://arxiv.org/abs/2503.03550</link>
      <description>arXiv:2503.03550v3 Announce Type: replace-cross 
Abstract: Modeling of growth (or decay) curves arises in many fields such as microbiology, epidemiology, marketing, and econometrics. Parametric forms like Logistic and Gompertz are often used for modeling such monotonic patterns. While useful for compact description, the real-life growth curves rarely follow these parametric forms perfectly. Therefore, the curve estimation methods that strike a balance between prior information in the parametric form and fidelity with the observed data are preferred. In hierarchical, longitudinal studies the interest lies in comparing the growth curves of different groups while accounting for the differences between the within-group subjects. This article describes a flexible state space modeling framework that enables semiparametric growth curve modeling for the data generated from hierarchical, longitudinal studies. The methodology, a type of functional mixed effects modeling, is illustrated with a real-life example of bacterial growth in different settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03550v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajesh Selukar</dc:creator>
    </item>
  </channel>
</rss>
