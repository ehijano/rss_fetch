<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 23 Jan 2026 05:01:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Distributional Balancing for Causal Inference: A Unified Framework via Characteristic Function Distance</title>
      <link>https://arxiv.org/abs/2601.15449</link>
      <description>arXiv:2601.15449v1 Announce Type: new 
Abstract: Weighting methods are essential tools for estimating causal effects in observational studies, with the goal of balancing pre-treatment covariates across treatment groups. Traditional approaches pursue this objective indirectly, for example, via inverse propensity score weighting or by matching a finite number of covariate moments, and therefore do not guarantee balance of the full joint covariate distributions. Recently, distributional balancing methods have emerged as robust, nonparametric alternatives that directly target alignment of entire covariate distributions, but they lack a unified framework, formal theoretical guarantees, and valid inferential procedures. We introduce a unified framework for nonparametric distributional balancing based on the characteristic function distance (CFD) and show that widely used discrepancy measures, including the maximum mean discrepancy and energy distance, arise as special cases. Our theoretical analysis establishes conditions under which the resulting CFD-based weighting estimator achieves $\sqrt{n}$-consistency. Since the standard bootstrap may fail for this estimator, we propose subsampling as a valid alternative for inference. We further extend our approach to an instrumental variable setting to address potential unmeasured confounding. Finally, we evaluate the performance of our method through simulation studies and a real-world application, where the proposed estimator performs well and exhibits results consistent with our theoretical predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15449v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Diptanil Santra, Guanhua Chen, Chan Park</dc:creator>
    </item>
    <item>
      <title>Model-Free Inference for Characterizing Protein Mutations through a Coevolutionary Lens</title>
      <link>https://arxiv.org/abs/2601.15566</link>
      <description>arXiv:2601.15566v1 Announce Type: new 
Abstract: Multiple sequence alignment (MSA) data play a crucial role in the study of protein mutations, with contact prediction being a notable application. Existing methods are often model-based or algorithmic and typically do not incorporate statistical inference to quantify the uncertainty of the prediction outcomes. To address this, we propose a novel framework that transforms the task of contact prediction into a statistical testing problem. Our approach is motivated by the partial correlation for continuous random variables. With one-hot encoding of MSA data, we are able to construct a partial correlation graph for multivariate categorical variables. In this framework, two connected nodes in the graph indicate that the corresponding positions on the protein form a contact. A new spectrum-based test statistic is introduced to test whether two positions are partially correlated. Moreover, the new framework enables the identification of amino acid combinations that contribute to the correlation within the identified contacts, an important but largely unexplored aspect of protein mutations. Numerical experiments demonstrate that our proposed method is valid in terms of controlling Type I errors and powerful in general. Real data applications on various protein families further validate the practical utility of our approach in coevolution and mutation analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15566v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fan Yang, Zhao Ren, Wen Zhou, Kejue Jia, Robert Jernigan</dc:creator>
    </item>
    <item>
      <title>Learning Functional Graphs with Nonlinear Sufficient Dimension Reduction</title>
      <link>https://arxiv.org/abs/2601.15696</link>
      <description>arXiv:2601.15696v1 Announce Type: new 
Abstract: Functional graphical models have undergone extensive development during the recent years, leading to a variety models such as the functional Gaussian graphical model, the functional copula Gaussian graphical model, the functional Bayesian graphical model, the nonparametric functional additive graphical model, and the conditional functional graphical model. These models rely either on some parametric form of distributions on random functions, or on additive conditional independence, a criterion that is different from probabilistic conditional independence. In this paper we introduce a nonparametric functional graphical model based on functional sufficient dimension reduction. Our method not only relaxes the Gaussian or copula Gaussian assumptions, but also enhances estimation accuracy by avoiding the ``curse of dimensionality''. Moreover, it retains the probabilistic conditional independence as the criterion to determine the absence of edges. By doing simulation study and analysis of the f-MRI dataset, we demonstrate the advantages of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15696v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyongwon Kim, Bing Li</dc:creator>
    </item>
    <item>
      <title>A two-sample pseudo-observation-based regression approach for the relative treatment effect</title>
      <link>https://arxiv.org/abs/2601.15880</link>
      <description>arXiv:2601.15880v1 Announce Type: new 
Abstract: The relative treatment effect is an effect measure for the order of two sample-specific outcome variables. It has the interpretation of a probability and also a connection to the area under the ROC curve. In the literature it has been considered for both ordinal or right-censored time-to-event outcomes. For both cases, the present paper introduces a distribution-free regression model that relates the relative treatment effect to a linear combination of covariates. To fit the model, we develop a pseudo-observation-based procedure yielding consistent and asymptotically normal coefficient estimates. In addition, we propose bootstrap-based hypothesis tests to infer the effects of the covariates on the relative treatment effect. A simulation study compares the novel method to Cox regression, demonstrating that the proposed hypothesis tests have high power and keep up with the z-test of the Cox model even in scenarios where the latter is specified correctly. The new methods are used to re-analyze data from the SUCCESS-A trial for progression-free survival of breast cancer patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15880v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dennis Dobler, Alina Schenk, Matthias Schmid</dc:creator>
    </item>
    <item>
      <title>Leave-one-out testing for node-level differences in Gaussian graphical models</title>
      <link>https://arxiv.org/abs/2601.15896</link>
      <description>arXiv:2601.15896v1 Announce Type: new 
Abstract: We study two-sample equality testing in Gaussian graphical models. Classical likelihood ratio tests on decomposable graphs admit clique-wise factorizations, offering limited localization and unstable finite-sample behaviour. We propose node-level inference via a leave-one-out Bartlett-adjusted test on a fully connected graph. The resulting increments have standard chi-square null limits, enabling calibrated significance for single nodes and fixed-size subsets. Simulations confirm validity, and a case study shows practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15896v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davide Benussi, Ester Alongi, Erika Banzato</dc:creator>
    </item>
    <item>
      <title>A Hierarchical Bayesian Framework for Model-based Prognostics</title>
      <link>https://arxiv.org/abs/2601.15942</link>
      <description>arXiv:2601.15942v1 Announce Type: new 
Abstract: In prognostics and health management (PHM) of engineered systems, maintenance decisions are ideally informed by predictions of a system's remaining useful life (RUL) based on operational data. Model-based prognostics algorithms rely on a parametric model of the system degradation process. The model parameters are learned from real-time operational data collected on the system. However, there can be valuable information in data from similar systems or components, which is not typically utilized in PHM. In this contribution, we propose a hierarchical Bayesian modeling (HBM) framework for PHM that integrates both operational data and run-to-failure data from similar systems or components. The HBM framework utilizes hyperparameter distributions learned from data of similar systems or components as priors. It enables efficient updates of predictions as more information becomes available, allowing for increasingly accurate assessments of the degradation process and its associated variability. The effectiveness of the proposed framework is demonstrated through two experimental applications involving real-world data from crack growth and lithium battery degradation. Results show significant improvements in RUL prediction accuracy and demonstrate how the framework facilitates uncertainty management through predictive distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15942v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyu Jia, Iason Papaioannou, Daniel Straub</dc:creator>
    </item>
    <item>
      <title>A Fast Monte Carlo Newton-Raphson Algorithm to Estimate Generalized Linear Mixed Models with Dense Covariance</title>
      <link>https://arxiv.org/abs/2601.16022</link>
      <description>arXiv:2601.16022v1 Announce Type: new 
Abstract: Estimation of Generalised linear mixed models (GLMM) including spatial Gaussian process models is often considered computationally impractical for even moderately sized datasets. In this article, we propose a fast Monte Carlo maximum likelihood (MCML) algorithm for the estimation of GLMMs. The algorithm is a stochastic Newton-Raphson method, which approximates the expected Hessian and gradient of the log-likelihood by drawing samples of the random effects. We propose a new stopping criterion for efficient termination and preventing long runs of sampling in the stationary post-convergence phase of the algorithm and discuss Monte Carlo sample size choice. We run a series of simulation comparisons of spatial statistical models alongside the popular integrated nested Laplacian approximation method and demonstrate potential for similar or improved estimator performance and reduced running times. We also consider scaling of the algorithms to large datasets and demonstrate a greater than 100-fold reduction in running times using modern GPU hardware to illustrate the feasibility of full maximum likelihood methods with big spatial datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16022v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Samuel I. Watson, Yixin Wang, Emanuele Giorgi</dc:creator>
    </item>
    <item>
      <title>On the spherical cardioid distribution and its goodness-of-fit</title>
      <link>https://arxiv.org/abs/2601.16095</link>
      <description>arXiv:2601.16095v1 Announce Type: new 
Abstract: In this paper, we study the spherical cardioid distribution, a higher-dimensional and higher-order generalization of the circular cardioid distribution. This distribution is rotationally symmetric and generates unimodal, multimodal, axial, and girdle-like densities. We show several characteristics of the spherical cardioid that make it highly tractable: simple density evaluation, closedness under convolution, explicit expressions for vectorized moments, and efficient simulation. The moments of the spherical cardioid up to a given order coincide with those of the uniform distribution on the sphere, highlighting its closeness to the latter. We derive estimators by the method of moments and maximum likelihood, their asymptotic distributions, and their asymptotic relative efficiencies. We give the machinery for a bootstrap goodness-of-fit test based on the projected-ecdf approach, including the projected distribution and closed-form expressions for test statistics. An application to modeling the orbits of long-period comets shows the usefulness of the spherical cardioid distribution in real data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16095v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Eduardo Garc\'ia-Portugu\'es</dc:creator>
    </item>
    <item>
      <title>Inference on the Significance of Modalities in Multimodal Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2601.16196</link>
      <description>arXiv:2601.16196v1 Announce Type: new 
Abstract: Despite the popular of multimodal statistical models, there lacks rigorous statistical inference tools for inferring the significance of a single modality within a multimodal model, especially in high-dimensional models. For high-dimensional multimodal generalized linear models, we propose a novel entropy-based metric, called the expected relative entropy, to quantify the information gain of one modality in addition to all other modalities in the model. We propose a deviance-based statistic to estimate the expected relative entropy, prove that it is consistent and its asymptotic distribution can be approximated by a non-central chi-squared distribution. That enables the calculation of confidence intervals and p-values to assess the significance of the expected relative entropy for a given modality. We numerically evaluate the empirical performance of our proposed inference tool by simulations and apply it to a multimodal neuroimaging dataset to demonstrate its good performance on various high-dimensional multimodal generalized linear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16196v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wanting Jin, Guorong Wu, Quefeng Li</dc:creator>
    </item>
    <item>
      <title>Robust X-Learner: Breaking the Curse of Imbalance and Heavy Tails via Robust Cross-Imputation</title>
      <link>https://arxiv.org/abs/2601.15360</link>
      <description>arXiv:2601.15360v1 Announce Type: cross 
Abstract: Estimating Heterogeneous Treatment Effects (HTE) in industrial applications such as AdTech and healthcare presents a dual challenge: extreme class imbalance and heavy-tailed outcome distributions. While the X-Learner framework effectively addresses imbalance through cross-imputation, we demonstrate that it is fundamentally vulnerable to "Outlier Smearing" when reliant on Mean Squared Error (MSE) minimization. In this failure mode, the bias from a few extreme observations ("whales") in the minority group is propagated to the entire majority group during the imputation step, corrupting the estimated treatment effect structure. To resolve this, we propose the Robust X-Learner (RX-Learner). This framework integrates a redescending {\gamma}-divergence objective -- structurally equivalent to the Welsch loss under Gaussian assumptions -- into the gradient boosting machinery. We further stabilize the non-convex optimization using a Proxy Hessian strategy grounded in Majorization-Minimization (MM) principles. Empirical evaluation on a semi-synthetic Criteo Uplift dataset demonstrates that the RX-Learner reduces the Precision in Estimation of Heterogeneous Effect (PEHE) metric by 98.6% compared to the standard X-Learner, effectively decoupling the stable "Core" population from the volatile "Periphery".</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15360v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eichi Uehara</dc:creator>
    </item>
    <item>
      <title>Community-Size Biases in Statistical Inference of Communities in Temporal Networks</title>
      <link>https://arxiv.org/abs/2601.15635</link>
      <description>arXiv:2601.15635v1 Announce Type: cross 
Abstract: In the study of time-dependent (i.e., temporal) networks, researchers often examine the evolution of communities, which are sets of densely connected sets of nodes that are connected sparsely to other nodes. An increasingly prominent approach to studying community structure in temporal networks is statistical inference. In the present paper, we study the performance of a class of statistical-inference methods for community detection in temporal networks. We represent temporal networks as multilayer networks, with each layer encoding a time step, and we illustrate that statistical-inference models that generate community assignments via either a uniform distribution on community assignments or discrete-time Markov processes are biased against generating communities with large or small numbers of nodes. In particular, we demonstrate that statistical-inference methods that use such generative models tend to poorly identify community structure in networks with large or small communities. To rectify this issue, we introduce a novel statistical model that generates the community assignments of the nodes in given layer (i.e., at a given time) using all of the community assignments in the previous layer. We prove results that guarantee that our approach greatly mitigates the bias against large and small communities, so using our generative model is beneficial for studying community structure in networks with large or small communities. Our code is available at https://github.com/tfaust0196/TemporalCommunityComparison.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15635v1</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <category>stat.ME</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Theodore Y. Faust, Arash A. Amini, Mason A. Porter</dc:creator>
    </item>
    <item>
      <title>Fully Functional Weighted Testing for Abrupt and Gradual Location Changes in Functional Time Series</title>
      <link>https://arxiv.org/abs/2601.16058</link>
      <description>arXiv:2601.16058v1 Announce Type: cross 
Abstract: Change point tests for abrupt changes in the mean of functional data, i.e., random elements in infinite-dimensional Hilbert spaces, are either based on dimension reduction techniques, e.g., based on principal components, or directly based on a functional CUSUM (cumulative sum) statistic. The former have often been criticized as not being fully functional and losing too much information. On the other hand, unlike the latter, they take the covariance structure of the data into account by weighting the CUSUM statistics obtained after dimension reduction with the inverse covariance matrix. In this paper, as a middle ground between these two approaches, we propose an alternative statistic that includes the covariance structure with an offset parameter to produce a scale-invariant test procedure and to increase power when the change is not aligned with the first components. We obtain the asymptotic distribution under the null hypothesis for this new test statistic, allowing for time dependence of the data. Furthermore, we introduce versions of all three test statistics for gradual change situations, which have not been previously considered for functional data, and derive their limit distribution. Further results shed light on the asymptotic power behavior for all test statistics under various ground truths for the alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16058v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Claudia Kirch, Hedvika Rano\v{s}ov\'a, Martin Wendler</dc:creator>
    </item>
    <item>
      <title>Synthetic Augmentation in Imbalanced Learning: When It Helps, When It Hurts, and How Much to Add</title>
      <link>https://arxiv.org/abs/2601.16120</link>
      <description>arXiv:2601.16120v1 Announce Type: cross 
Abstract: Imbalanced classification, where one class is observed far less frequently than the other, often causes standard training procedures to prioritize the majority class and perform poorly on rare but important cases. A classic and widely used remedy is to augment the minority class with synthetic examples, but two basic questions remain under-resolved: when does synthetic augmentation actually help, and how many synthetic samples should be generated?
  We develop a unified statistical framework for synthetic augmentation in imbalanced learning, studying models trained on imbalanced data augmented with synthetic minority samples and evaluated under the balanced population risk. Our theory shows that synthetic data is not always beneficial. In a ``local symmetry" regime, imbalance is not the dominant source of error near the balanced optimum, so adding synthetic samples cannot improve learning rates and can even degrade performance by amplifying generator mismatch. When augmentation can help (a ``local asymmetry" regime), the optimal synthetic size depends on generator accuracy and on whether the generator's residual mismatch is directionally aligned with the intrinsic majority-minority shift. This structure can make the best synthetic size deviate from naive full balancing, sometimes by a small refinement and sometimes substantially when generator bias is systematic. Practically, we recommend Validation-Tuned Synthetic Size (VTSS): select the synthetic size by minimizing balanced validation loss over a range centered near the fully balanced baseline, while allowing meaningful departures when the data indicate them. Simulations and a real sepsis prediction study support the theory and illustrate when synthetic augmentation helps, when it cannot, and how to tune its quantity effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16120v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhengchi Ma, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Sequential model confidence sets</title>
      <link>https://arxiv.org/abs/2404.18678</link>
      <description>arXiv:2404.18678v4 Announce Type: replace 
Abstract: In most prediction and estimation situations, scientists consider various statistical models for the same problem, and naturally want to select amongst the best. Hansen et al. (2011) provide a powerful solution to this problem by the so-called model confidence set, a subset of the original set of available models that contains the best models with a given level of confidence. Importantly, model confidence sets respect the underlying selection uncertainty by being flexible in size. However, they presuppose a fixed sample size which stands in contrast to the fact that model selection and forecast evaluation are inherently sequential tasks where we successively collect new data and where the decision to continue or conclude a study may depend on the previous outcomes. In this article, we extend model confidence sets sequentially over time by relying on sequential testing methods. Recently, e-processes and confidence sequences have been introduced as new, safe methods for assessing statistical evidence. Sequential model confidence sets allow to continuously monitor the models' performances and come with time-uniform, nonasymptotic coverage guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18678v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Arnold, Georgios Gavrilopoulos, Benedikt Schulz, Johanna Ziegel</dc:creator>
    </item>
    <item>
      <title>Spectral decomposition-assisted multi-study factor analysis</title>
      <link>https://arxiv.org/abs/2502.14600</link>
      <description>arXiv:2502.14600v2 Announce Type: replace 
Abstract: This article focuses on covariance estimation for multi-study data. Popular approaches employ factor-analytic terms with shared and study-specific loadings that decompose the variance into (i) a shared low-rank component, (ii) study-specific low-rank components, and (iii) a diagonal term capturing idiosyncratic variability. Our proposed methodology estimates the latent factors via spectral decompositions, with a novel approach for separating shared and specific factors, and infers the factor loadings and residual variances via surrogate Bayesian regressions. The resulting posterior has a simple product form across outcomes, bypassing the need for Markov chain Monte Carlo sampling and facilitating parallelization. The proposed methodology has major advantages over current Bayesian competitors in terms of computational speed, scalability and stability while also having strong frequentist guarantees. The theory and methods also add to the rich literature on frequentist methods for factor models with shared and group-specific components of variation. The approximation error decreases as the sample size and the data dimension diverge, formalizing a blessing of dimensionality. We show favorable asymptotic properties, including central limit theorems for point estimators and posterior contraction, and excellent empirical performance in simulations. The methods are applied to integrate three studies on gene associations among immune cells.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14600v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Mauri, Niccol\`o Anceschi, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Inference in pseudo-observation-based regression using (biased) covariance estimation and naive bootstrapping</title>
      <link>https://arxiv.org/abs/2510.06815</link>
      <description>arXiv:2510.06815v2 Announce Type: replace 
Abstract: The pseudo-observation method is regularly applied to time-to-event data. However, to date such analyses have relied on not formally verified statements or ad-hoc methods regarding covariance estimation. This paper strives to close this gap in the literature. To begin with, we demonstrate that the usual Huber-White estimator is not consistent for the limiting covariance of parameter estimates in pseudo-observation regression approaches. By confirming that a plug-in estimator can be used instead, we obtain asymptotically exact and consistent tests for general linear hypotheses in the parameters of the model. Additionally, we confirm that naive bootstrapping can not be used for covariance estimation in the pseudo-observation model either. However, it can be used for hypothesis testing by applying a suitable studentization. Simulations illustrate the good performance of our proposed methods in many scenarios. Finally, we obtain a general uniform law of large numbers for U- and V-statistics, as such statistics are central in the mathematical analysis of the inference procedures developed in this work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06815v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Simon Mack, Morten Overgaard, Dennis Dobler</dc:creator>
    </item>
    <item>
      <title>RESOLVE-IPD: High-Fidelity Individual Patient Data Reconstruction and Uncertainty-Aware Subgroup Meta-Analysis</title>
      <link>https://arxiv.org/abs/2511.01785</link>
      <description>arXiv:2511.01785v2 Announce Type: replace 
Abstract: Individual patient data (IPD) from oncology trials are essential for reliable evidence synthesis but are rarely publicly available, necessitating reconstruction from published Kaplan-Meier (KM) curves. Existing reconstruction methods suffer from digitization errors, unrealistic uniform censoring assumptions, and the inability to recover subgroup-level IPD when only aggregate statistics are available. We developed RESOLVE-IPD, a unified computational framework that enables high-fidelity IPD reconstruction and uncertainty-aware subgroup meta-analysis to address these limitations. RESOLVE-IPD comprises two components. The first component, High-Fidelity IPD Reconstruction, integrates the VEC-KM and CEN-KM modules: VEC-KM extracts precise KM coordinates and explicit censoring marks from vectorized figures, minimizing digitization error, while CEN-KM corrects overlapping censor symbols and eliminates the uniform censoring assumption. The second component, Uncertainty-Aware Subgroup Recovery, employs the MAPLE (Marginal Assignment of Plausible Labels and Evidence Propagation) algorithm to infer patient-level subgroup labels consistent with published summary statistics (e.g., hazard ratio, median overall survival) when subgroup KM curves are unavailable. MAPLE generates ensembles of mathematically valid labelings, facilitating a propagating meta-analysis that quantifies and reflects uncertainty from subgroup reconstruction. RESOLVE-IPD was validated through a subgroup meta-analysis of four trials in advanced esophageal squamous cell carcinoma, focusing on the programmed death ligand 1 (PD-L1)-low population. RESOLVE-IPD enables accurate IPD reconstruction and robust, uncertainty-aware subgroup meta-analyses, strengthening the reliability and transparency of secondary evidence synthesis in precision oncology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01785v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lang Lang, Yao Zhao, Qiuxin Gao, Yanxun Xu</dc:creator>
    </item>
    <item>
      <title>Univariate-Guided Sparse Regression for Biobank-Scale High-Dimensional Omics Data</title>
      <link>https://arxiv.org/abs/2511.22049</link>
      <description>arXiv:2511.22049v4 Announce Type: replace 
Abstract: We present a scalable framework for computing polygenic risk scores (PRS) in high-dimensional genomic settings using the recently introduced Univariate-Guided Sparse Regression (uniLasso). UniLasso is a two-stage penalized regression procedure that leverages univariate coefficients and magnitudes to stabilize feature selection and enhance interpretability. Building on its theoretical and empirical advantages, we adapt uniLasso for application to the UK Biobank, a population-based repository comprising over one million genetic variants measured on hundreds of thousands of individuals from the United Kingdom. We further extend the framework to incorporate external summary statistics to increase predictive accuracy. Our results demonstrate that uniLasso attains predictive performance comparable to standard Lasso while selecting substantially fewer variants, yielding sparser and more interpretable models. Moreover, it exhibits superior performance in estimating PRS relative to its competitors, such as PRS-CS. Integrating external scores further improves prediction while maintaining sparsity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22049v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Richland, Tuomo Kiiskinen, William Wang, Sophia Lu, Balasubramanian Narasimhan, Trevor Hastie, Manuel Rivas, Robert Tibshirani</dc:creator>
    </item>
    <item>
      <title>Recent advances in the Bradley--Terry Model: theory, algorithms, and applications</title>
      <link>https://arxiv.org/abs/2601.14727</link>
      <description>arXiv:2601.14727v2 Announce Type: replace 
Abstract: This article surveys recent progress in the Bradley-Terry (BT) model and its extensions. We focus on the statistical and computational aspects, with emphasis on the regime in which both the number of objects and the volume of comparisons tend to infinity, a setting relevant to large-scale applications. The main topics include asymptotic theory for statistical estimation and inference, along with the associated algorithms. We also discuss applications of these models, including recent work on preference alignment in machine learning. Finally, we discuss several key challenges and outline directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14727v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuxing Fang, Ruijian Han, Yuanhang Luo, Yiming Xu</dc:creator>
    </item>
    <item>
      <title>Life Sequence Transformer: Generative Modelling of Socio-Economic Trajectories from Administrative Data</title>
      <link>https://arxiv.org/abs/2506.01874</link>
      <description>arXiv:2506.01874v2 Announce Type: replace-cross 
Abstract: Generative modelling with Transformer architectures can simulate complex sequential structures across various applications. We extend this line of work to the social sciences by introducing a Transformer-based generative model tailored to longitudinal socio-economic data. Our contributions are: (i) we design a novel encoding method that represents socio-economic life histories as sequences, including overlapping events across life domains; and (ii) we adapt generative modelling techniques to simulate plausible alternative life trajectories conditioned on past histories. Using large-scale data from the Italian social security administration (INPS), we show that the model can be trained at scale, reproduces realistic labour market patterns consistent with known causal relationships, and generates coherent hypothetical life paths. This work demonstrates the feasibility of generative modelling for socio-economic trajectories and opens new opportunities for policy-oriented research, with counterfactual generation as a particularly promising application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01874v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Cabezas, Carlotta Montorsi</dc:creator>
    </item>
    <item>
      <title>Likelihood Matching for Diffusion Models</title>
      <link>https://arxiv.org/abs/2508.03636</link>
      <description>arXiv:2508.03636v2 Announce Type: replace-cross 
Abstract: We propose a Likelihood Matching approach for training diffusion models by first establishing an equivalence between the likelihood of the target data distribution and a likelihood along the sample path of the reverse diffusion. To efficiently compute the reverse sample likelihood, a quasi-likelihood is considered to approximate each reverse transition density by a Gaussian distribution with matched conditional mean and covariance, respectively. The score and Hessian functions for the diffusion generation are estimated by maximizing the quasi-likelihood, ensuring a consistent matching of both the first two transitional moments between every two time points. A stochastic sampler is introduced to facilitate computation that leverages both the estimated score and Hessian information. We establish consistency of the quasi-maximum likelihood estimation, and provide non-asymptotic convergence guarantees for the proposed sampler, quantifying the rates of the approximation errors due to the score and Hessian estimation, dimensionality, and the number of diffusion steps. Empirical and simulation evaluations demonstrate the effectiveness of the proposed Likelihood Matching and validate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03636v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lei Qian, Wu Su, Yanqi Huang, Song Xi Chen</dc:creator>
    </item>
    <item>
      <title>Finite-Sample Inference for Sparsely Permuted Linear Regression</title>
      <link>https://arxiv.org/abs/2601.14872</link>
      <description>arXiv:2601.14872v2 Announce Type: replace-cross 
Abstract: We study a linear observation model with an unknown permutation called \textit{permuted/shuffled linear regression}, where responses and covariates are mismatched and the permutation forms a discrete, factorial-size parameter. The permutation is a key component of the data-generating process, yet its statistical investigation remains challenging due to its discrete nature. We develop a general statistical inference framework on the permutation and regression coefficients. First, we introduce a localization step that reduces the permutation space to a small candidate set building on recent advances in the repro samples method, whose miscoverage decays polynomially with the number of Monte Carlo samples. Then, based on this localized set, we provide statistical inference procedures: a conditional Monte Carlo test of permutation structures with valid finite-sample Type-I error control. We also develop coefficient inference that remains valid under alignment uncertainty of permutations. For computational purposes, we develop a linear assignment problem computable in polynomial time and demonstrate that, with high probability, the solution is equivalent to that of the conventional least squares with large computational cost. Extensions to partially permuted designs and ridge regularization are further discussed. Extensive simulations and an application to air-quality data corroborate finite-sample validity, strong power to detect mismatches, and practical scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14872v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hirofumi Ota, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>Recommending Best Paper Awards for ML/AI Conferences via the Isotonic Mechanism</title>
      <link>https://arxiv.org/abs/2601.15249</link>
      <description>arXiv:2601.15249v2 Announce Type: replace-cross 
Abstract: Machine learning and artificial intelligence conferences such as NeurIPS and ICML now regularly receive tens of thousands of submissions, posing significant challenges to maintaining the quality and consistency of the peer review process. This challenge is particularly acute for best paper awards, which are an important part of the peer review process, yet whose selection has increasingly become a subject of debate in recent years. In this paper, we introduce an author-assisted mechanism to facilitate the selection of best paper awards. Our method employs the Isotonic Mechanism for eliciting authors' assessments of their own submissions in the form of a ranking, which is subsequently utilized to adjust the raw review scores for optimal estimation of the submissions' ground-truth quality. We demonstrate that authors are incentivized to report truthfully when their utility is a convex additive function of the adjusted scores, and we validate this convexity assumption for best paper awards using publicly accessible review data of ICLR from 2019 to 2023 and NeurIPS from 2021 to 2023. Crucially, in the special case where an author has a single quota -- that is, may nominate only one paper -- we prove that truthfulness holds even when the utility function is merely nondecreasing and additive. This finding represents a substantial relaxation of the assumptions required in prior work. For practical implementation, we extend our mechanism to accommodate the common scenario of overlapping authorship. Finally, simulation results demonstrate that our mechanism significantly improves the quality of papers selected for awards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.15249v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>stat.ME</category>
      <pubDate>Fri, 23 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Garrett G. Wen, Buxin Su, Natalie Collina, Zhun Deng, Weijie Su</dc:creator>
    </item>
  </channel>
</rss>
