<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Feb 2026 02:45:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Statistical methods for reference-free single-molecule localisation microscopy</title>
      <link>https://arxiv.org/abs/2602.18727</link>
      <description>arXiv:2602.18727v1 Announce Type: new 
Abstract: MINFLUX (Minimal Photon Flux) is a single-molecule imaging technique capable of resolving fluorophores at a precision of &lt;5 nm. Interpretation of the point patterns generated by this technique presents challenges due to variable emitter density, incomplete bio-labelling of target molecules and their detection, error prone measurement processes, and the presence of spurious (non-structure associated) fluorescent detections. Together, these challenges ensure structural inferences from single-molecule imaging datasets are non-trivial in the absence of strong a priori information, for all but the smallest of point patterns. In addition, current methods often require subjective parameter tuning and presuppose known structural templates, limiting reference-free discovery. We present a statistically grounded, end-to-end analysis framework. Focusing on MINFLUX derived datasets and leveraging Bayesian and spatial statistical methods, a pipeline is presented that demonstrates 1) uncertainty aware clustering of measurements into emitter groups that performs better than current gold standards, 2) rapid identification of molecular structure supergroups, and 3) reconstruction of repeating structures within the dataset without substantial prior knowledge. This pipeline is demonstrated using simulated and real MINFLUX datasets, where emitter clustering and centre detection maintain high performance (emitter subset assignment accuracy &gt; 0.75) across all conditions evaluated, while structural inference achieves reliable discrimination (F1 approx. 0.9) at high labelling efficiency. Template-free reconstruction of Nup96 and DNA-Origami 3x3 grids are achieved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18727v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jack Peyton, Benjamin Davis, Emily Gribbin, Daniel Rolfe, Hannah Mitchell</dc:creator>
    </item>
    <item>
      <title>Prognostics of Multisensor Systems with Unknown and Unlabeled Failure Modes via Bayesian Nonparametric Process Mixtures</title>
      <link>https://arxiv.org/abs/2602.19263</link>
      <description>arXiv:2602.19263v1 Announce Type: new 
Abstract: Modern manufacturing systems often experience multiple and unpredictable failure behaviors, yet most existing prognostic models assume a fixed, known set of failure modes with labeled historical data. This assumption limits the use of digital twins for predictive maintenance, especially in high-mix or adaptive production environments, where new failure modes may emerge, and the failure mode labels may be unavailable.
  To address these challenges, we propose a novel Bayesian nonparametric framework that unifies a Dirichlet process mixture module for unsupervised failure mode discovery with a neural network-based prognostic module. The key innovation lies in an iterative feedback mechanism to jointly learn two modules. These modules iteratively update one another to dynamically infer, expand, or merge failure modes as new data arrive while providing high prognostic accuracy.
  Experiments on both simulation and aircraft engine datasets show that the proposed approach performs competitively with or significantly better than existing approaches. It also exhibits robust online adaptation capabilities, making it well-suited for digital-twin-based system health management in complex manufacturing environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19263v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kani Fu, Sanduni S Disanayaka Mudiyanselage, Chunli Dai, Minhee Kim</dc:creator>
    </item>
    <item>
      <title>Dynamic Elasticity Between Forest Loss and Carbon Emissions: A Subnational Panel Analysis of the United States</title>
      <link>https://arxiv.org/abs/2602.19329</link>
      <description>arXiv:2602.19329v1 Announce Type: new 
Abstract: Accurate quantification of the relationship between forest loss and associated carbon emissions is critical for both environmental monitoring and policy evaluation. Although many studies have documented spatial patterns of forest degradation, there is limited understanding of the dynamic elasticity linking tree cover loss to carbon emissions at subnational scales. In this paper, we construct a comprehensive panel dataset of annual forest loss and carbon emission estimates for U.S. subnational administrative units from 2001 to 2023, based on the Hansen Global Forest Change dataset. We apply fixed effects and dynamic panel regression techniques to isolate within-region variation and account for temporal persistence in emissions. Our results show that forest loss has a significant positive short-run elasticity with carbon emissions, and that emissions exhibit strong persistence over time. Importantly, the estimated long-run elasticity, accounting for autoregressive dynamics, is substantially larger than the short-run effect, indicating cumulative impacts of repeated forest loss events. These findings highlight the importance of modeling temporal dynamics when assessing environmental responses to land cover change. The dynamic elasticity framework proposed here offers a robust and interpretable tool for analyzing environmental change processes, and can inform both regional monitoring systems and carbon accounting frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19329v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keonvin Park</dc:creator>
    </item>
    <item>
      <title>Network-Level Travel Time Prediction Considering The Effects of Weather and Seasonality</title>
      <link>https://arxiv.org/abs/2602.19351</link>
      <description>arXiv:2602.19351v1 Announce Type: new 
Abstract: Accurately predicting travel time information can be helpful for travelers. This study proposes a framework for predicting network-level travel time index (TTI) using machine learning models. A case study was performed on more than 50,000 TTI data collected from the Washington DC area over 6 years. The proposed approach is also able to identify the effects of weather and seasonality. The performances of the machine learning models were assessed and compared with each other. It was shown that the ridge regression model outperformed the other models in both short-term and long-term predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19351v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yufei Ai, Yao Yu, Wenjing Pu, Lu Gao, Yihao Ren</dc:creator>
    </item>
    <item>
      <title>Reliability of stochastic capacity estimates</title>
      <link>https://arxiv.org/abs/2602.19370</link>
      <description>arXiv:2602.19370v1 Announce Type: new 
Abstract: Stochastic traffic capacity is used in traffic modelling and control for unidirectional sections of road infrastructure, although some of the estimation methods have recently proved flawed. However, even sound estimation methods require sufficient data. Because breakdowns are rare, the number of recorded breakdowns effectively determines sample size. This is especially relevant for temporary traffic infrastructure, but also for permanent bottlenecks (e.g., on- and off-ramps), where practitioners must know when estimates are reliable enough for control or design decisions. This paper studies this reliability along with the impact of censored data using synthetic data with a known capacity distribution. A corrected maximum-likelihood estimator is applied to varied samples. In total, 360 artificial measurements are created and used to estimate the capacity distribution, and the deviation from the pre-defined distribution is then quantified. Results indicate that at least 50 recorded breakdowns are necessary; 100-200 are the recommended minimum for temporary measurements. Beyond this, further improvements are marginal, with the expected average relative error below 5 %.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19370v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Igor Mikolasek</dc:creator>
    </item>
    <item>
      <title>Real-time Win Probability and Latent Player Ability via STATS X in Team Sports</title>
      <link>https://arxiv.org/abs/2602.19513</link>
      <description>arXiv:2602.19513v1 Announce Type: new 
Abstract: This study proposes a statistically grounded framework for real-time win probability evaluation and player assessment in score-based team sports, based on minute-by-minute cumulative box-score data. We introduce a continuous dominance indicator (T-score) that maps final scores to real values consistent with win/lose outcomes, and formulate it as a time-evolving stochastic representation (T-process) driven by standardized cumulative statistics. This structure captures temporal game dynamics and enables sequential, analytically tractable updates of in-game win probability. Through this stochastic formulation, competitive advantage is decomposed into interpretable statistical components. Furthermore, we define a latent contribution index, STATS X, which quantifies a player's involvement in favorable dominance intervals identified by the T-process. This allows us to separate a team's baseline strength from game-specific performance fluctuations and provides a coherent, structural evaluation framework for both teams and players. While we do not implement AI methods in this paper, our framework is positioned as a foundational step toward hybrid integration with AI. By providing a structured time-series representation of dominance with an explicit probabilistic interpretation, the framework enables flexible learning mechanisms and incorporation of high-dimensional data, while preserving statistical coherence and interpretability. This work provides a basis for advancing AI-driven sports analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19513v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yasutaka Shimizu, Atsushi Yamanobe</dc:creator>
    </item>
    <item>
      <title>Decomposing Crowd Wisdom: Domain-Specific Calibration Dynamics in Prediction Markets</title>
      <link>https://arxiv.org/abs/2602.19520</link>
      <description>arXiv:2602.19520v1 Announce Type: new 
Abstract: Prediction markets are increasingly used as probability forecasting tools, yet their usefulness depends on calibration, specifically whether a contract trading at 70 cents truly implies a 70% probability. Using 292 million trades across 327,000 binary contracts on Kalshi and Polymarket, this paper shows that calibration is a structured, multidimensional phenomenon. On Kalshi, calibration decomposes into four components (a universal horizon effect, domain-specific biases, domain-by-horizon interactions and a trade-size scale effect) that together explain 87.3% of calibration variance. The dominant pattern is persistent underconfidence in political markets, where prices are chronically compressed toward 50%, and this bias generalises across both exchanges. However, the trade-size scale effect, whereby large trades are associated with amplified underconfidence in politics on Kalshi ($\Delta = 0.53$, 95% confidence interval [0.29, 0.75]), does not replicate on Polymarket ($\Delta = 0.11$, [-0.15, 0.39]), suggesting platform-specific microstructure. A Bayesian hierarchical model confirms the frequentist decomposition with 96.3% posterior predictive coverage. Consumers of prediction market prices who treat them as face-value probabilities will systematically misinterpret them, and the direction of misinterpretation depends on what is being predicted, when and by whom.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19520v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nam Anh Le</dc:creator>
    </item>
    <item>
      <title>Spatio-temporal modeling of urban extreme rainfall events at high resolution</title>
      <link>https://arxiv.org/abs/2602.19774</link>
      <description>arXiv:2602.19774v1 Announce Type: new 
Abstract: Modeling precipitation and its accumulation over time and space is essential for flood risk assessment. We here analyze rainfall data collected over several years through a microscale precipitation sensor network in Montpellier, France, by the OMSEV observatory. A novel spatio-temporal stochastic model is proposed for high-resolution urban rainfall and combines realistic marginal behavior and flexible extremal dependence structure. Rainfall intensities are described by the Extended Generalized Pareto Distribution (EGPD), capturing both moderate and extreme events without threshold selection. Based on spatial extreme-value theory, dependence during extreme episodes is modeled by an r-Pareto process with a non-separable variogram including episode-specific advection, allowing the displacement of rainfall cells to be represented explicitly. Parameters are estimated by a composite likelihood based on joint exceedances, and empirical advection velocities are derived from radar reanalysis. The model accurately reproduces the spatio-temporal structure of extreme rainfall observed in the Montpellier OMSEV network and enables realistic stochastic scenario generation for flood risk assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19774v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chlo\'e Serre-Combe (IMAG, LEMON), Nicolas Meyer (IMAG, LEMON), Thomas Opitz (BioSP), Gwladys Toulemonde (IMAG, LEMON)</dc:creator>
    </item>
    <item>
      <title>A Bayesian Framework for Post-disruption Travel Time Prediction in Metro Networks</title>
      <link>https://arxiv.org/abs/2602.19952</link>
      <description>arXiv:2602.19952v1 Announce Type: new 
Abstract: Disruptions are an inherent feature of transportation systems, occurring unpredictably and with varying durations. Even after an incident is reported as resolved, disruptions can induce irregular train operations that generate substantial uncertainty in passenger waiting and travel times. Accurately forecasting post-disruption travel times therefore remains a critical challenge for transit operators and passenger information systems. This paper develops a Bayesian spatiotemporal modeling framework for post-disruption train travel times that explicitly captures train interactions, headway imbalance, and non-Gaussian distributional characteristics observed during recovery periods. The proposed model decomposes travel times into delay and journey components and incorporates a moving-average error structure to represent dependence between consecutive trains. Skew-normal and skew-$t$ distributions are employed to flexibly accommodate heteroskedasticity, skewness, and heavy-tailed behavior in post-disruption travel times. The framework is evaluated using high-resolution track-occupancy and disruption log data from the Montr\'eal metro system, covering two lines in both travel directions. Empirical results indicate that post-disruption travel times exhibit pronounced distributional asymmetries that vary with traveled distance, as well as significant error dependence across trains. The proposed models consistently outperform baseline specifications in both point prediction accuracy and uncertainty quantification, with the skew-$t$ model demonstrating the most robust performance for longer journeys. These findings underscore the importance of incorporating both distributional flexibility and error dependence when forecasting post-disruption travel times in urban rail systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19952v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shayan Nazemi, Aur\'elie Labbe, Stefan Steiner, Pratheepa Jeganathan, Martin Tr\'epanier, L\'eo R. Belzile</dc:creator>
    </item>
    <item>
      <title>A Two-Step Spatio-Temporal Framework for Turbine-Height Wind Estimation at Unmonitored Sites from Sparse Meteorological Data</title>
      <link>https://arxiv.org/abs/2602.19954</link>
      <description>arXiv:2602.19954v1 Announce Type: new 
Abstract: Accurate estimates of wind speeds at wind turbine hub heights are crucial for both wind resource assessment and day-to-day management of electricity grids with high renewable penetration. In the absence of direct measurements, parametric models are commonly used to extrapolate wind speeds from observed heights to turbine heights. Recent literature has proposed extensions to allow for spatially or temporally varying vertical wind gradients, that is, the rate at which wind speed changes with height. However, these approaches typically assume that reference height and hub height measurements are available at the same locations, which limits their applicability in operational settings where meteorological stations and wind farms are spatially separated. In this paper, we develop a two-step spatio-temporal framework to estimate turbine height wind speeds using only open-access observations from sparse meteorological stations. First, a non-parametric generalized additive model is trained on reanalysis data to perform vertical height extrapolation. Second, a spatial Gaussian process model interpolates these hub-height estimates to wind farm locations while explicitly propagating uncertainty from the height extrapolation stage. The proposed framework enables the construction of high-resolution, sub-hourly turbine-height wind speed time series and spatial wind maps using data available in real time, capabilities not provided by existing reanalysis products. We further provide calibrated uncertainty estimates that account for both vertical extrapolation and spatial interpolation errors. The approach is validated using hub-height measurements from seven operational wind farms in Ireland, demonstrating improved accuracy relative to ERA5 reanalysis while relying solely on real-time, open-access data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19954v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eamonn Organ, Maeve Upton, Denis Allard, Lionel Benoit, James Sweeney</dc:creator>
    </item>
    <item>
      <title>Ostrom-Weighted Bootstrap: A Theoretically Optimal and Provably Complete Framework for Hierarchical Imputation in Multi-Agent Systems</title>
      <link>https://arxiv.org/abs/2602.18442</link>
      <description>arXiv:2602.18442v1 Announce Type: cross 
Abstract: We study the statistical properties of the \emph{Ostrom-Weighted Bootstrap} (OWB), a hierarchical, variance-aware resampling scheme for imputing missing values and estimating archetypes in multi-agent voting data. At Level~1, under mild linear model assumptions, the \emph{ideal} OWB estimator -- with known persona-level (agent-level) variances -- is shown to be the Gauss--Markov best linear unbiased estimator (BLUE) and to strictly dominate uniform weighting whenever persona variances differ. At Level~2, within a canonical hierarchical normal model, the ideal OWB coincides with the conditional Bayesian posterior mean of the archetype. We then analyze the \emph{feasible} OWB, which replaces unknown variances with hierarchically pooled empirical estimates, and show that it can be interpreted as both a feasible generalized least-squares (FGLS) and an empirical-Bayes shrinkage estimator with asymptotically valid weighted bootstrap confidence intervals under mild regularity conditions. Finally, we establish a Zero-NaN Guarantee: as long as each petal has at least one finite observation, the OWB imputation algorithm produces strictly NaN-free completed data using only explicit, non-uniform bootstrap weights and never resorting to uniform sampling or numerical zero-filling.
  To our knowledge, OWB is the first resampling-based method that simultaneously achieves exact BLUE optimality, conditional Bayesian posterior mean interpretation, empirical Bayes shrinkage of precision parameters, asymptotic efficiency via FGLS, consistent weighted bootstrap inference, and provable zero-NaN completion under minimal data assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18442v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hirofumi Wakimoto</dc:creator>
    </item>
    <item>
      <title>Statistical Imaginaries, State Legitimacy: Grappling with the Arrangements Underpinning Quantification in the U.S. Census</title>
      <link>https://arxiv.org/abs/2602.18636</link>
      <description>arXiv:2602.18636v1 Announce Type: cross 
Abstract: Over the last century, the adoption of novel scientific methods for conducting the U.S. census has been met with wide-ranging receptions. Some methods were quietly embraced, while others sparked decades-long controversies. What accounts for these differences? We argue that controversies emerge from $\textit{arrangements of statistical imaginaries}$, putting into tension divergent visions of the census. To analyze these dynamics, we compare reactions to two methods designed to improve data accuracy (imputation and adjustment) and two methods designed to protect confidentiality (swapping and differential privacy), offering insight into how each method reconfigures stakeholder orientations and rhetorical claims. These cases allow us to reflect on how technocratic efforts to improve accuracy and confidentiality can strengthen -- or erode -- trust in data. Our analysis shows how the credibility of the Census Bureau and its data stem not just from empirical evaluations of quantification, but also from how statistical imaginaries are contested and stabilized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18636v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1177/08969205241270898</arxiv:DOI>
      <arxiv:journal_reference>Critical Sociology, 51(6), 1267-1288 (2024)</arxiv:journal_reference>
      <dc:creator>Jayshree Sarathy, danah boyd</dc:creator>
    </item>
    <item>
      <title>Bayesian calendar-time survival analysis with epidemic curve priors and variant-specific infection hazards</title>
      <link>https://arxiv.org/abs/2602.18677</link>
      <description>arXiv:2602.18677v1 Announce Type: cross 
Abstract: In this paper, we develop a Bayesian calendar-time survival model motivated by infectious disease prevention studies occurring during an epidemic, when the risk of infection can change rapidly as the epidemic curve shifts. For studies in which a biomarker is the predictor of interest, we include the option to estimate a threshold of protection for the biomarker. If the intervention is hypothesized to have different associations with several circulating viral variants, or if the infectiousness of the dominant variant(s) changes over the course of the study, we treat infection from different variants as competing risks. We also introduce a novel method for incorporating existing epidemic curve estimates into an informative prior for the baseline hazard function, enabling estimation of the intervention's association with infection risk during periods of calendar time with minimal follow-up in one or more comparator groups. We demonstrate the strengths of this method via simulations, and we apply it to data from an observational COVID-19 vaccine study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18677v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Angela M Dahl, Elizabeth R Brown</dc:creator>
    </item>
    <item>
      <title>Latent Moment Models for Recurrent Binary Outcomes: A Bayesian and Quasi-Distributional Approach</title>
      <link>https://arxiv.org/abs/2602.18988</link>
      <description>arXiv:2602.18988v1 Announce Type: cross 
Abstract: Recurrent binary outcomes within individuals, such as hospital readmissions, often reflect latent risk processes that evolve over time. Conventional methods like generalized linear mixed models and generalized estimating equations estimate average risk but fail to capture temporal changes in variability, asymmetry, and tail behavior. We introduce two statistical frameworks that model each binary event as the outcome of a thresholded value drawn from a time-varying latent distribution defined by its location, scale, skewness, and kurtosis. Rather than treating these four quantities as nonparametric moment estimators, we model them as interpretable latent moments within a flexible latent distributional family. The first, BLaS-Recurrent, is a Bayesian model using the sinh-arcsinh distribution (a parametric family that provides explicit control over asymmetry and tail weight) to estimate latent moment trajectories; the second, QuaD-Recurrent, is a quasi-distributional approach that maps simulated moment vectors to event probabilities using a flexible nonparametric surface. Both models support time-dependent covariates, serial correlation, and multiple membership structures. Simulation studies show improved calibration, interpretability, and robustness over standard models. Applied to ICU readmission data from the MIMIC-IV database, both approaches uncover clinically meaningful patterns in latent risk, such as right-skewed escalation and widening dispersion, that are missed by traditional methods. These models provide interpretable, distribution-sensitive tools for longitudinal binary outcomes in healthcare while explicitly acknowledging that latent "moments" summarize but do not uniquely determine the underlying distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18988v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niloofar Ramezani, Lori P. Selby, Pascal Nitiema, Jeffrey R. Wilson</dc:creator>
    </item>
    <item>
      <title>Adaptive Weighting for Time-to-Event Continual Reassessment Method: Improving Safety in Phase I Dose-Finding Through Data-Driven Delay Distribution Estimation</title>
      <link>https://arxiv.org/abs/2602.19012</link>
      <description>arXiv:2602.19012v1 Announce Type: cross 
Abstract: Background: Phase I dose-finding trials increasingly encounter delayed-onset toxicities, especially with immunotherapies and targeted agents. The time-to-event continual reassessment method (TITE-CRM) handles incomplete follow-up using fixed linear weights, but this ad hoc approach doesn't reflect actual delay patterns and may expose patients to excessive risk during dose escalation.
  Methods: We replace TITE-CRM's fixed weights with adaptive weights, posterior predictive probabilities derived from the evolving toxicity delay distribution. Under a Weibull timing model, we get closed-form weight updates through maximum likelihood estimation, making real-time implementation straightforward. We tested our method (AW-TITE) against TITE-CRM and standard designs (3+3, mTPI, BOIN) across three dose-toxicity scenarios through simulation (N = 30 patients, 2,000 replications). We also examined robustness across varying accrual rates, sample sizes, shape parameters, observation windows, and priors.
  Results: Our AW-TITE reduced patient overdosing by 40.6% compared to TITE-CRM (mean fraction above MTD: 0.202 vs 0.340; 95% CI: -0.210 to -0.067, p &lt; 0.001) while maintaining comparable MTD selection accuracy (mean difference: +0.023, p = 0.21). Against algorithm-based methods, AW-TITE achieved higher MTD identification: +32.6% vs mTPI, +19.8% vs 3+3, and +5.6% vs BOIN. Performance remained robust across all sensitivity analyses.
  Conclusions: Adaptive weighting offers a practical way to improve Phase I trial safety while preserving MTD selection accuracy. The method requires minimal computation and is ready for real-time use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19012v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Amevor, Emmanuel Kubuafor, Dennis Baidoo</dc:creator>
    </item>
    <item>
      <title>Time-Varying Hazard Patterns and Co-Mutation Profiles of KRAS G12C and G12D in Real-World NSCLC</title>
      <link>https://arxiv.org/abs/2602.19295</link>
      <description>arXiv:2602.19295v1 Announce Type: cross 
Abstract: Background: KRAS mutations are the largest oncogenic subset in NSCLC. While KRAS G12C is now targetable, no approved therapies exist for G12D. We examined time-to-next-treatment (TTNT) and overall survival (OS) differences between G12C and G12D, allowing for time-varying hazard effects. Methods: De-identified data from AACR Project GENIE BPC NSCLC v2.0-public were analyzed. TTNT served as a real-world surrogate for progression-free survival. Co-mutations (TP53, STK11, KEAP1, SMARCA4, MET), TMB, and PD-L1 were harmonized. Kaplan-Meier, multivariable Cox, and a pre-specified piecewise Cox model (split at median TTNT = 23 months) were applied. Schoenfeld residuals assessed proportional hazards; bootstrap resampling (B=1000) evaluated stability. Results: Among 162 TTNT-evaluable patients (G12C n=130; G12D n=32), median TTNT was 28.6 versus 32.0 months (log-rank p=0.79). Adjusted Cox regression showed no overall hazard difference (HR=0.85; 95% CI 0.53-1.37; p=0.50), but Schoenfeld testing indicated borderline non-proportionality (p=0.053). Piecewise Cox modeling revealed time-varying effects: early TTNT hazard favored G12D (HR=0.41; 95% CI 0.17-0.97; p=0.043) with significant KRAS x period interaction (HR=3.33; p=0.021) and late-period attenuation (HR=1.38; 95% CI 0.77-2.47; p=0.285). Bootstrap resampling confirmed this pattern (median HRearly=0.39; HRlate=1.41). Among 278 OS-evaluable patients (133 deaths), G12D showed improved OS (adjusted HR=0.63; 95% CI 0.39-0.99; p=0.048). G12C tumors exhibited higher TMB (9.79 vs 7.83 mut/Mb; p=0.002) and greater STK11/KEAP1 enrichment. Conclusions: KRAS G12D demonstrated early TTNT advantage and improved OS. Late-period TTNT differences were non-significant (post-hoc power: 12.3%). These exploratory findings require validation in larger cohorts but support allele-specific therapeutic development for G12D.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19295v1</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Amevor, Dennis Baidoo, Emmanuel Kubuafor</dc:creator>
    </item>
    <item>
      <title>A Causal Framework for Estimating Heterogeneous Effects of On-Demand Tutoring</title>
      <link>https://arxiv.org/abs/2602.19296</link>
      <description>arXiv:2602.19296v1 Announce Type: cross 
Abstract: This paper introduces a scalable causal inference framework for estimating the immediate, session-level effects of on-demand human tutoring embedded within adaptive learning systems. Because students seek assistance at moments of difficulty, conventional evaluation is confounded by self-selection and time-varying knowledge states. We address these challenges by integrating principled analytic sample construction with Deep Knowledge Tracing (DKT) to estimate latent mastery, followed by doubly robust estimation using Causal Forests. Applying this framework to over 5,000 middle-school mathematics tutoring sessions, we find that requesting human tutoring increases next-problem correctness by approximately 4 percentage points and accuracy on the subsequent skill encountered by approximately 3 percentage points, suggesting that the effects of tutoring have proximal transfer across knowledge components. This effect is robust to various forms of model specification and potential unmeasured confounders. Notably, these effects exhibit significant heterogeneity across sessions and students, with session-level effect estimates ranging from $-20.25pp$ to $+19.91pp$. Our follow-up analyses suggest that typical behavioral indicators, such as student talk time, do not consistently correlate with high-impact sessions. Furthermore, treatment effects are larger for students with lower prior mastery and slightly smaller for low-SES students. This framework offers a rigorous, practical template for the evaluation and continuous improvement of on-demand human tutoring, with direct applications for emerging AI tutoring systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19296v1</guid>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kirk Vanacore, Danielle R Thomas, Digory Smith, Bibi Groot, Justin Reich, Rene Kizilcec</dc:creator>
    </item>
    <item>
      <title>Personalized Prediction of Perceived Message Effectiveness Using Large Language Model Based Digital Twins</title>
      <link>https://arxiv.org/abs/2602.19403</link>
      <description>arXiv:2602.19403v1 Announce Type: cross 
Abstract: Perceived message effectiveness (PME) by potential intervention end-users is important for selecting and optimizing personalized smoking cessation intervention messages for mobile health (mHealth) platform delivery. This study evaluates whether large language models (LLMs) can accurately predict PME for smoking cessation messages.
  We evaluated multiple models for predicting PME across three domains: content quality, coping support, and quitting support. The dataset comprised 3010 message ratings (5-point Likert scale) from 301 young adult smokers. We compared (1) supervised learning models trained on labeled data, (2) zero and few-shot LLMs prompted without task-specific fine-tuning, and (3) LLM-based digital twins that incorporate individual characteristics and prior PME histories to generate personalized predictions. Model performance was assessed on three held-out messages per participant using accuracy, Cohen's kappa, and F1.
  LLM-based digital twins outperformed zero and few-shot LLMs (12 percentage points on average) and supervised baselines (13 percentage points), achieving accuracies of 0.49 (content), 0.45 (coping), and 0.49 (quitting), with directional accuracies of 0.75, 0.66, and 0.70 on a simplified 3-point scale. Digital twin predictions showed greater dispersion across rating categories, indicating improved sensitivity to individual differences.
  Integrating personal profiles with LLMs captures person-specific differences in PME and outperforms supervised and zero and few-shot approaches. Improved PME prediction may enable more tailored intervention content in mHealth. LLM-based digital twins show potential for supporting personalization of mobile smoking cessation and other health behavior change interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19403v1</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jasmin Han (Department of Mental Health, Johns Hopkins Bloomberg School of Public Health, Baltimore, MD, USA), Janardan Devkota (Department of Mental Health, Johns Hopkins Bloomberg School of Public Health, Baltimore, MD, USA), Joseph Waring (Department of Mental Health, Johns Hopkins Bloomberg School of Public Health, Baltimore, MD, USA), Amanda Luken (Department of Health Sciences, Towson University, Towson, USA), Felix Naughton (Addiction Research Group, University of East Anglia, Norwich, UK), Roger Vilardaga (Department of Implementation Science, Wake Forest University School of Medicine, Winston-Salem, USA), Jonathan Bricker (Fred Hutchinson Cancer Center, Seattle, USA, Department of Psychology, University of Washington, Seattle, USA), Carl Latkin (Department of Health, Behavior and Society, Johns Hopkins Bloomberg School of Public Health, Baltimore, USA), Meghan Moran (Department of Health, Behavior and Society, Johns Hopkins Bloomberg School of Public Health, Baltimore, USA), Yiqun Chen (Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health, Baltimore, USA, Department of Computer Science, Johns Hopkins Whiting School of Engineering, Baltimore, USA), Johannes Thrul (Department of Mental Health, Johns Hopkins Bloomberg School of Public Health, Baltimore, MD, USA, Sidney Kimmel Comprehensive Cancer Center at Johns Hopkins, Baltimore, USA, Centre for Alcohol Policy Research, La Trobe University, Melbourne, Australia)</dc:creator>
    </item>
    <item>
      <title>Zero Variance Portfolio</title>
      <link>https://arxiv.org/abs/2602.19462</link>
      <description>arXiv:2602.19462v1 Announce Type: cross 
Abstract: When the number of assets is larger than the sample size, the minimum variance portfolio interpolates the training data, delivering pathological zero in-sample variance. We show that if the weights of the zero variance portfolio are learned by a novel ``Ridgelet'' estimator, in a new test data this portfolio enjoys out-of-sample generalizability. It exhibits the double descent phenomenon and can achieve optimal risk in the overparametrized regime when the number of assets dominates the sample size. In contrast, a ``Ridgeless'' estimator which invokes the pseudoinverse fails in-sample interpolation and diverges away from out-of-sample optimality. Extensive simulations and empirical studies demonstrate that the Ridgelet method performs competitively in high-dimensional portfolio optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19462v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Yi Ding, Zhentao Shi, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>A mixed Hinfty-Passivity approach for Leveraging District Heating Systems as Frequency Ancillary Service in Electric Power Systems</title>
      <link>https://arxiv.org/abs/2602.19486</link>
      <description>arXiv:2602.19486v1 Announce Type: cross 
Abstract: This paper introduces a mixed H-infinity-passivity framework that enables district heating systems (DHSs) with heat pumps to support electric-grid frequency regulation. The analysis illustrates how the DHS regulator influences coupled electro-thermal frequency dynamics and provides LMI conditions for efficient controller design. We also present a disturbance-independent temperature regulator that ensures stability and robustness against heat-demand uncertainty. Simulations demonstrate improved frequency-control dynamics in the electrical power grid while maintaining good thermal performance in the DHS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19486v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Yi, Ioannis Lestas</dc:creator>
    </item>
    <item>
      <title>Volatility Spillovers in China's Real Estate Crisis: A Network Approach</title>
      <link>https://arxiv.org/abs/2602.19740</link>
      <description>arXiv:2602.19740v1 Announce Type: cross 
Abstract: Sentiment towards the Chinese real estate sector has deteriorated following the introduction of financing constraints in 2020 with the ''three red lines." Forcing developers to restructure their debt, the policy triggered a cascade of financing troubles, defaults, and reduced housing demand, ultimately culminating in a prolonged real estate crisis. This paper utilizes a network approach in line with Demirer et al. (2018) and Diebold and Yilmaz (2014) to measure daily time-varying connectedness in the stock return volatilities of major Chinese real estate developers throughout the crisis. Focusing on spillover between companies as reflected by market perception, this paper examines how connectedness evolves over time across firms with different regional exposures and state-ownership statuses, filling a gap in the literature to elucidate where property demand and real estate firm trustworthiness have deteriorated most. An event-study analysis of four key moments of the crisis outlines distinct phases of market sentiment: with the introduction of the three red lines, connectedness primarily reflects shared exposure and a uniform shock to the market. Then, the early unrest surrounding Evergrande exposes strong regional differentiation, with firms concentrated in less developed regions receiving significant spillover. By one year into the crisis, previously stable regions receive higher levels of spillover, and there is evidence of a substitution effect towards private developers. Two years into the crisis, the market has much less homogeneity in effects across regions and state-ownership status: major shocks induce minimal network changes, reflecting how investors have already priced in their beliefs. This paper also offers one of the most extensive timelines of the Chinese real estate crisis to date, and a new R package, GephiForR, was created for the network visualization in this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19740v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Julia Manso</dc:creator>
    </item>
    <item>
      <title>Ensemble Machine Learning and Statistical Procedures for Dynamic Predictions of Time-to-Event Outcomes</title>
      <link>https://arxiv.org/abs/2602.19761</link>
      <description>arXiv:2602.19761v1 Announce Type: cross 
Abstract: Dynamic predictions for longitudinal and time-to-event outcomes have become a versatile tool in precision medicine. Our work is motivated by the application of dynamic predictions in the decision-making process for primary biliary cholangitis patients. For these patients, serial biomarker measurements (e.g., bilirubin and alkaline phosphatase levels) are routinely collected to inform treating physicians of the risk of liver failure and guide clinical decision-making. Two popular statistical approaches to derive dynamic predictions are joint modelling and landmarking. However, recently, machine learning techniques have also been proposed. Each approach has its merits, and no single method exists to outperform all others. Consequently, obtaining the best possible survival estimates is challenging. Therefore, we extend the Super Learner framework to combine dynamic predictions from different models and procedures. Super Learner is an ensemble learning technique that allows users to combine different prediction algorithms to improve predictive accuracy and flexibility. It uses cross-validation and different objective functions of performance (e.g., squared loss) that suit specific applications to build the optimally weighted combination of predictions from a library of candidate algorithms. In our work, we pay special attention to appropriate objective functions for Super Learner to obtain the most optimal weighted combination of dynamic predictions. In our primary biliary cholangitis application, Super Learner presented unique benefits due to its ability to flexibly combine outputs from a diverse set of models with varying assumptions for equal or better predictive performance than any model fit separately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19761v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nina van Gerwen, Sten Willemsen, Bettina E. Hansen, Christophe Corpechot, Marco Carbone, Cynthia Levy, Maria-Carlota Lond\~ono, Atsushi Tanaka, Palak Trivedi, Alejandra Villamil, Gideon Hirschfield, Dimitris Rizopoulos</dc:creator>
    </item>
    <item>
      <title>Estimators of different delta coefficients based on the unbiased estimator of the expected proportions of agreements</title>
      <link>https://arxiv.org/abs/2602.20071</link>
      <description>arXiv:2602.20071v1 Announce Type: cross 
Abstract: To measure the degree of agreement between two observers that independently classify $n$ subjects within $K$ categories, it is common to use different kappa type coefficients, the most common of which is the $\kappa_C$ coefficient (Cohen's kappa). As $\kappa_C$ has some weaknesses -such as its poor performance with highly unbalanced marginal distributions-, the $\Delta$ coefficient is sometimes used, based on the $delta$ response model. This model allows us to obtain other parameters like: (a) the $\alpha_i$ contribution of each $i$ category to the value of the global agreement $\Delta=\sum \alpha_i$; and (b) the consistency $\mathcal{S}_i$ in the category $i$ (degree of agreement in the category $i$), a more appropriate parameter than the kappa value obtained by collapsing the data into the category $i$. It has recently been shown that the classic estimator $\hat{\kappa}_C$ underestimates $\kappa_C$, having obtained a new estimator $\hat{\kappa}_{CU}$ which is less biased. This article demonstrates that something similar happens to the known estimators $\hat{\Delta}$, $\hat{\alpha}_i$, and $\hat{\mathcal{S}}_i$ of $\Delta$, $\alpha_i$ and $\mathcal{S}_i$ (respectively), proposes new and less biased estimators $\hat{\Delta}_U$, $\hat{\alpha}_{iU}$, and $\hat{\mathcal{S}}_{iU}$, determines their variances, analyses the behaviour of all estimators, and concludes that the new estimators should be used when $n$ or $K$ are small (at least when $n\leq 50$ or $K\leq 3$). Additionally, the case where one of the raters is a gold standard is contemplated, in which situation two new parameters arise: the $conformity$ (the rater's capability to recognize a subject in the category $i$) and the $predictivity$ (the reliability of a response $i$ by the rater).</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20071v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>A. Mart\'in Andr\'es, M. \'Alvarez Hern\'andez</dc:creator>
    </item>
    <item>
      <title>How Ominous is the Premonition of Future Global Warming?</title>
      <link>https://arxiv.org/abs/2008.11175</link>
      <description>arXiv:2008.11175v2 Announce Type: replace 
Abstract: Global warming, the phenomenon of increasing global average temperature in the recent decades, is receiving wide attention due to its very significant adverse effects on climate. Whether global warming will continue even in the future, is a question that is most important to investigate. In this regard, the so-called general circulation models (GCMs) have attempted to project the future climate, and nearly all of them exhibit alarming rates of global temperature rise in the future.
  Although global warming in the current time frame is undeniable, it is important to assess the validity of the future predictions of the GCMs. In this article, we attempt such a study using our recently-developed Bayesian multiple testing paradigm for model selection in inverse regression problems. The model we assume for the global temperature time series is based on Gaussian process emulation of the black box scenario, realistically treating the dynamic evolution of the time series as unknown.
  We apply our ideas to datasets available from the Intergovernmental Panel on Climate Change (IPCC) website. The best GCM models selected by our method under different assumptions on future climate change scenarios do not convincingly support the present global warming pattern when only the future predictions are considered known. Using our Gaussian process idea, we also forecast the future temperature time series given the current one. Interestingly, our results do not support drastic future global warming predicted by almost all the GCM models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2008.11175v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Debashis Chatterjee, Sourabh Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Compositional data analysis for modelling and forecasting mortality using the {\alpha}-transformation</title>
      <link>https://arxiv.org/abs/2501.01129</link>
      <description>arXiv:2501.01129v3 Announce Type: replace 
Abstract: Mortality forecasting is crucial for demographic planning and actuarial studies, especially for projecting population ageing and longevity risk. Classical approaches largely rely on extrapolative methods, such as the Lee-Carter (LC) model, which use mortality rates as the mortality measure. In recent years, compositional data analysis (CoDA), which respects summability and non-negativity constraints, has gained increasing attention for mortality forecasting. While the centred log-ratio (CLR) transformation is commonly used to map compositional data to real space, the {\alpha}-transformation, a generalisation of log-ratio transformations, offers greater flexibility and adaptability. This study contributes to mortality forecasting by introducing the {\alpha}-transformation as an alternative to the CLR transformation within a non-functional CoDA model that has not been previously investigated in existing literature. To fairly compare the impact of transformation choices on forecast accuracy, zero values in the data are imputed, although the {\alpha}-transformation can inherently handle them. Using age-specific life table death counts for males and females in 31 selected European countries/regions from 1983 to 2018, the proposed method demonstrates comparable performance to the CLR transformation in most cases, with improved forecast accuracy in some instances. These findings highlight the potential of the {\alpha}-transformation for enhancing mortality forecasting within the non-functional CoDA framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01129v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Han Ying Lim, Dharini Pathmanathan, Sophie Dabo-Niang</dc:creator>
    </item>
    <item>
      <title>A Behavioral Scorecard Model Using Survival Analysis</title>
      <link>https://arxiv.org/abs/2503.05023</link>
      <description>arXiv:2503.05023v2 Announce Type: replace 
Abstract: Credit risk assessment is a crucial aspect of financial decision-making, enabling institutions to predict the likelihood of default and make informed lending decisions. Two prominent methodologies in credit risk modeling are logistic regression and survival analysis. Logistic regression is widely used in scorecard development due to its simplicity, interpretability, and effectiveness in estimating the probability of binary outcomes, such as default versus non-default. In contrast, survival analysis -- particularly within the hazard rate framework -- provides insights into the timing of events, such as the time to default. By integrating logistic regression with survival analysis, traditional scorecard models can be enhanced to capture not only the probability of default but also the dynamics of default over time. This combined approach offers a more comprehensive view of credit risk, enabling institutions to manage risk proactively and tailor strategies to individual borrower profiles. This article presents the process of developing a monthly hazard rate model using logistic regression and augmented data with survival analysis techniques to incorporate time-varying risk factors. The process includes data preparation, model construction, and the evaluation of performance metrics. Monthly hazard rates are then converted into default probabilities. Finally, a behavioral scorecard is developed using offset adjustment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05023v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Cheng Lee, Hsi Lee</dc:creator>
    </item>
    <item>
      <title>Enhancing the Accuracy of Spatio-Temporal Models for Wind Speed Prediction by Incorporating Bias-Corrected Crowdsourced Data</title>
      <link>https://arxiv.org/abs/2505.24506</link>
      <description>arXiv:2505.24506v2 Announce Type: replace 
Abstract: Accurate high-resolution spatial and temporal wind speed data is critical for estimating the wind energy potential of a location. For real-time wind speed prediction, statistical models typically depend on high-quality (near) real-time data from official meteorological stations to improve forecasting accuracy. Personal weather stations (PWS) offer an additional source of real-time data and broader spatial coverage than official stations. However, they are not subject to rigorous quality control and may exhibit bias or measurement errors. This paper presents a framework for incorporating PWS data into statistical models for validated official meteorological station data via a two-stage approach. First, bias correction is performed on PWS wind speed data using reanalysis data. Second, we implement a Bayesian hierarchical spatio-temporal model that accounts for varying measurement error in the PWS data. This enables wind speed prediction across a target area, and is particularly beneficial for improving predictions in regions sparse in official monitoring stations. Our results show that including bias-corrected PWS data improves prediction accuracy compared to using meteorological station data alone, with a 5% reduction in prediction error on average across all sites. The results are comparable with popular reanalysis products, but unlike these numerical weather models our approach is available in real-time and offers improved uncertainty quantification. are comparable with popular reanalysis products, but unlike these numerical weather models our approach is available in real-time and offers improved uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24506v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/env.70069</arxiv:DOI>
      <arxiv:journal_reference>Environmetrics 37(2), e70069 (2026)</arxiv:journal_reference>
      <dc:creator>Eamonn Organ, Maeve Upton, Denis Allard, Lionel Benoit, James Sweeney</dc:creator>
    </item>
    <item>
      <title>Design loads for wave impacts -- introducing the Probabilistic Adaptive Screening (PAS) method for predicting extreme non-linear loads on maritime structures</title>
      <link>https://arxiv.org/abs/2511.23156</link>
      <description>arXiv:2511.23156v3 Announce Type: replace 
Abstract: Wave impact loads on maritime structures can cause casualties, damage, pollution and operational delays. Consequently, their extreme values should be accounted for in the design of these structures. However, this is challenging, as wave impact events are both rare and highly complex, requiring both high-fidelity simulations and long analysis durations to reliably quantify the associated design loads. Moreover, existing extreme value prediction methods are neither specifically developed nor adequately validated for wave impact phenomena. We therefore introduce the new Probabilistic Adaptive Screening (PAS) method for predicting extreme non-linear loads on maritime structures. The method integrates copula-based statistical dependence modelling with multi-fidelity screening and adaptive sampling. This framework enables efficient extreme value prediction by statistically mapping low-fidelity indicator variables to high-fidelity impact loads. The method allows for efficient linear potential flow indicators to be used in the low-fidelity stage, even for strongly non-linear cases. Its statistical framework is validated against four non-linear test cases, including non-linear waves, ship vertical bending moments, green water impact loads, and slamming loads. It is concluded that PAS with optimal settings accurately estimates both the short-term distributions and extreme values in these test cases, with most probable maximum (MPM) values within 2-15% of the reference brute-force Monte-Carlo Simulation (MCS) results. In addition, PAS achieves this performance very efficiently, requiring in the order of 1-3% of the high-fidelity simulation time needed for conventional MCS. These results demonstrate that PAS can reliably reproduce the statistics of both weakly and strongly non-linear extreme load problems, while significantly reducing the associated computational cost compared to MCS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23156v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanne M. van Essen, Harleigh C. Seyffert</dc:creator>
    </item>
    <item>
      <title>On the Wisdom of Crowds (of Economists)</title>
      <link>https://arxiv.org/abs/2503.09287</link>
      <description>arXiv:2503.09287v3 Announce Type: replace-cross 
Abstract: We study the properties of macroeconomic survey forecast response averages as the number of survey respondents grows. Such averages are ``portfolios" of forecasts. We characterize the speed and pattern of the gains from diversification as a function of portfolio size (the number of survey respondents) in both (1) the key real-world data-based environment of the U.S. Survey of Professional Forecasters, and (2) the theoretical model-based environment of equicorrelated forecast errors. We proceed by proposing and comparing various direct and model-based ``crowd size signature plots", which summarize the forecasting performance of $k$-average forecasts as a function of $k$, where $k$ is the number of forecasts in the average. We then estimate the equicorrelation model for growth and inflation forecast errors by choosing model parameters to minimize the divergence between direct and model-based signature plots. The results indicate near-perfect equicorrelation model fit for both growth and inflation, which we explicate by showing analytically that, under very weak conditions, the direct and fitted equicorrelation model-based signature plots are identical at a particular model parameter configuration. That parameter configuration immediately suggests an analytic closed-form estimator for the direct signature plot, so that equicorrelation ultimately emerges as a device for convenient calculation of direct signature plots, rather than a separate ``model" producing separate signature plots. In any event we find that the gains from survey diversification are greater for inflation forecasts than for growth forecasts, and that they are largely exhausted with inclusion of 5-10 representative forecasters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.09287v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Francis X. Diebold, Aaron Mora, Minchul Shin</dc:creator>
    </item>
    <item>
      <title>Perception Characteristics Distance: Measuring Stability and Robustness of Perception System in Dynamic Conditions under a Certain Decision Rule</title>
      <link>https://arxiv.org/abs/2506.09217</link>
      <description>arXiv:2506.09217v2 Announce Type: replace-cross 
Abstract: The safety of autonomous driving systems (ADS) depends on accurate perception across distance and driving conditions. The outputs of AI perception algorithms are stochastic, which have a major impact on decision making and safety outcomes, including time-to-collision estimation. However, current perception evaluation metrics do not reflect the stochastic nature of perception algorithms. We introduce the Perception Characteristics Distance (PCD), a novel metric incorporating model output uncertainty as represented by the farthest distance at which an object can be reliably detected. To represent a system's overall perception capability in terms of reliable detection distance, we average PCD values across multiple detection quality and probabilistic thresholds to produce the average PCD (aPCD). For empirical validation, we present the SensorRainFall dataset, collected on the Virginia Smart Road using a sensor-equipped vehicle (cameras, radar, and LiDAR) under different weather (clear and rainy) and illumination conditions (daylight, streetlight, and nighttime). The dataset includes ground-truth distances, bounding boxes, and segmentation masks for target objects. Experiments with state-of-the-art models show that aPCD captures meaningful differences across weather, daylight, and illumination conditions, which traditional evaluation metrics fail to reflect. PCD provides an uncertainty-aware measure of perception performance, supporting safer and more robust ADS operation, while the SensorRainFall dataset offers a valuable benchmark for evaluation. The SensorRainFall dataset is publicly available at https://www.kaggle.com/datasets/datadrivenwheels/sensorrainfall, and the evaluation code is available at https://github.com/datadrivenwheels/PCD_Python.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09217v2</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boyu Jiang, Liang Shi, Zhengzhi Lin, Lanxin Xiang, Loren Stowe, Feng Guo</dc:creator>
    </item>
    <item>
      <title>Non-Linear Model-Based Sequential Decision-Making in Agriculture</title>
      <link>https://arxiv.org/abs/2509.01924</link>
      <description>arXiv:2509.01924v3 Announce Type: replace-cross 
Abstract: Sequential decision-making is central to sustainable agricultural management and precision agriculture, where resource inputs must be optimized under uncertainty and over time. However, such decisions must often be made with limited observations, whereas classical bandit and reinforcement learning approaches typically rely on either linear or black-box reward models that may misrepresent domain knowledge or require large amounts of data. We propose a family of \emph{nonlinear, model-based bandit algorithms} that embed domain-specific response curves directly into the exploration-exploitation loop. By coupling (i) principled uncertainty quantification with (ii) closed-form or rapidly computable profit optima, these algorithms achieve sublinear regret and near-optimal sample complexity while preserving interpretability. Theoretical analysis establishes regret and sample complexity bounds, and extensive simulations emulating real-world fertilizer-rate decisions show consistent improvements over both linear and nonparametric baselines (such as linear UCB and $k$-NN UCB) in the low-sample regime, under both well-specified and shape-compatible misspecified models. Because our approach leverages mechanistic insight rather than large data volumes, it is especially suited to resource-constrained settings, supporting sustainable, inclusive, and transparent sequential decision-making across agriculture, environmental management, and allied applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01924v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sakshi Arya, Wentao Lin</dc:creator>
    </item>
    <item>
      <title>The Price-Pareto growth model of networks with community structure</title>
      <link>https://arxiv.org/abs/2510.13392</link>
      <description>arXiv:2510.13392v2 Announce Type: replace-cross 
Abstract: We introduce a new analytical framework for modelling degree sequences in individual communities of real-world networks, e.g., citations to papers in different fields. Our work is inspired by a recent modification of the Price's model, which assumes that citations are gained partly accidentally, and to some extent preferentially. Our work addresses the need to represent the heterogeneity of various scientific domains, as standard homogeneous models fail to capture the distinct growth ratios and citing cultures of different fields. Extending the model to networks with a community structure allows us to devise the analytical formulae for, amongst others, citation counts in each cluster and their inequality as described by the Gini index. We also show that a citation count distribution in each community tends to a Pareto type II distribution. Thanks to the derived model parameter estimators, the new model can be fitted to real citation and similar networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13392v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>{\L}ukasz Brzozowski, Marek Gagolewski, Grzegorz Siudem, Barbara \.Zoga{\l}a-Siudem</dc:creator>
    </item>
    <item>
      <title>Clust-PSI-PFL: A Population Stability Index Approach for Clustered Non-IID Personalized Federated Learning</title>
      <link>https://arxiv.org/abs/2512.20363</link>
      <description>arXiv:2512.20363v2 Announce Type: replace-cross 
Abstract: Federated learning (FL) supports privacy-preserving, decentralized machine learning (ML) model training by keeping data on client devices. However, non-independent and identically distributed (non-IID) data across clients biases updates and degrades performance. To alleviate these issues, we propose Clust-PSI-PFL, a clustering-based personalized FL framework that uses the Population Stability Index (PSI) to quantify the level of non-IID data. We compute a weighted PSI metric, $WPSI^L$, which we show to be more informative than common non-IID metrics (Hellinger, Jensen-Shannon, and Earth Mover's distance). Using PSI features, we form distributionally homogeneous groups of clients via K-means++; the number of optimal clusters is chosen by a systematic silhouette-based procedure, typically yielding few clusters with modest overhead. Across six datasets (tabular, image, and text modalities), two partition protocols (Dirichlet with parameter $\alpha$ and Similarity with parameter S), and multiple client sizes, Clust-PSI-PFL delivers up to 18% higher global accuracy than state-of-the-art baselines and markedly improves client fairness by a relative improvement of 37% under severe non-IID data. These results establish PSI-guided clustering as a principled, lightweight mechanism for robust PFL under label skew.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20363v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel M. Jimenez-Gutierrez, Mehrdad Hassanzadeh, David Solans, Mohammed Elbamby, Nicolas Kourtellis, Aris Anagnostopoulos, Ioannis Chatzigiannakis, Andrea Vitaletti</dc:creator>
    </item>
    <item>
      <title>Anisotropic local law for non-separable sample covariance matrices</title>
      <link>https://arxiv.org/abs/2602.17960</link>
      <description>arXiv:2602.17960v2 Announce Type: replace-cross 
Abstract: We establish local laws for sample covariance matrices $K = N^{-1}\sum_{i=1}^N \g_i\g_i^*$ where the random vectors $\g_1, \ldots, \g_N \in \R^n$ are independent with common covariance $\Sigma$. Previous work has largely focused on the separable model $\g = \Sigma^{1/2}\w$ with $\w$ having independent entries, but this structure is rarely present in statistical applications involving dependent or nonlinearly transformed data. Under a concentration assumption for quadratic forms $\g^*A\g$, we prove an optimal averaged local law showing that the Stieltjes transform of $K$ converges to its deterministic limit uniformly down to the optimal scale $\eta \geq N^{-1+\eps}$. Under an additional structural assumption on the cumulant tensors of $\g$ -- which interpolates between the highly structured case of independent entries and generic dependence -- we establish the full anisotropic local law, providing entrywise control of the resolvent $(K-zI)^{-1}$ in arbitrary directions. We discuss several classes of non-separable examples satisfying our assumptions, including conditionally mean-zero distributions, the random features model $\g = \sigma(X\w)$ arising in machine learning, and Gaussian measures with nonlinear tilting. The proofs introduce a tensor network framework for analyzing fluctuation averaging in the presence of higher-order cumulant structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17960v2</guid>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhou Fan, Renyuan Ma, Elliot Paquette, Zhichao Wang</dc:creator>
    </item>
  </channel>
</rss>
