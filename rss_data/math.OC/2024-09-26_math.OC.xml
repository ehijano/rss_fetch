<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Sep 2024 04:01:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Willems' Fundamental Lemma for Nonlinear Systems with Koopman Linear Embedding</title>
      <link>https://arxiv.org/abs/2409.16389</link>
      <description>arXiv:2409.16389v1 Announce Type: new 
Abstract: Koopman operator theory and Willems' fundamental lemma both can provide (approximated) data-driven linear representation for nonlinear systems. However, choosing lifting functions for the Koopman operator is challenging, and the quality of the data-driven model from Willems' fundamental lemma has no guarantee for general nonlinear systems. In this paper, we extend Willems' fundamental lemma for a class of nonlinear systems that admit a Koopman linear embedding. We first characterize the relationship between the trajectory space of a nonlinear system and that of its Koopman linear embedding. We then prove that the trajectory space of Koopman linear embedding can be formed by a linear combination of rich-enough trajectories from the nonlinear system. Combining these two results leads to a data-driven representation of the nonlinear system, which bypasses the need for the lifting functions and thus eliminates the associated bias errors. Our results illustrate that both the width (more trajectories) and depth (longer trajectories) of the trajectory library are important to ensure the accuracy of the data-driven model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16389v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xu Shang, Jorge Cort\'es, Yang Zheng</dc:creator>
    </item>
    <item>
      <title>Score-based Neural Ordinary Differential Equations for Computing Mean Field Control Problems</title>
      <link>https://arxiv.org/abs/2409.16471</link>
      <description>arXiv:2409.16471v1 Announce Type: new 
Abstract: Classical neural ordinary differential equations (ODEs) are powerful tools for approximating the log-density functions in high-dimensional spaces along trajectories, where neural networks parameterize the velocity fields. This paper proposes a system of neural differential equations representing first- and second-order score functions along trajectories based on deep neural networks. We reformulate the mean field control (MFC) problem with individual noises into an unconstrained optimization problem framed by the proposed neural ODE system. Additionally, we introduce a novel regularization term to enforce characteristics of viscous Hamilton--Jacobi--Bellman (HJB) equations to be satisfied based on the evolution of the second-order score function. Examples include regularized Wasserstein proximal operators (RWPOs), probability flow matching of Fokker--Planck (FP) equations, and linear quadratic (LQ) MFC problems, which demonstrate the effectiveness and accuracy of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16471v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mo Zhou, Stanley Osher, Wuchen Li</dc:creator>
    </item>
    <item>
      <title>Normalizing flow regularization for photoacoustic tomography</title>
      <link>https://arxiv.org/abs/2409.16564</link>
      <description>arXiv:2409.16564v1 Announce Type: new 
Abstract: Proper regularization is crucial in inverse problems to achieve high-quality reconstruction, even with an ill-conditioned measurement system. This is particularly true for three-dimensional photoacoustic tomography, which is computationally demanding and requires rapid scanning, often leading to incomplete measurements. Deep neural networks, known for their efficiency in handling big data, are anticipated to be adept at extracting underlying information from images sharing certain characteristics, such as specific types of natural or medical images. We introduce a Normalizing Flow Regularization (NFR) method designed to reconstruct images from incomplete and noisy measurements. The method involves training a normalizing flow network to understand the statistical distribution of sample images by mapping them to Gaussian distributions. This well-trained network then acts as a regularization tool within a Bayesian inversion framework. Additionally, we explore the concept of adaptive regularization selection, providing theoretical proof of its admissibility. A significant challenge in three-dimensional image training is the extensive memory and computation requirements. We address this by training the normalizing flow model using only small-size images and applying a patch-based model for reconstructing larger images. Our approach is model-independent, allowing the reuse of a well-trained network as regularization for various imaging systems. Moreover, as a data-driven prior, NFR effectively leverages the available dataset information, outperforming artificial priors. This advantage is demonstrated through numerical simulations of three-dimensional photoacoustic tomography under various conditions of sparsity, noise levels, and limited-view scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16564v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Inverse Problems, 2024</arxiv:journal_reference>
      <dc:creator>Chao Wang, Alexandre H. Thiery</dc:creator>
    </item>
    <item>
      <title>$\mathcal{L}_{1}$ Adaptive Optimizer for Uncertain Time-Varying Convex Optimization</title>
      <link>https://arxiv.org/abs/2409.16583</link>
      <description>arXiv:2409.16583v1 Announce Type: new 
Abstract: We propose an adaptive method for uncertain time-varying (TV) convex optimization, termed as $\mathcal{L}_{1}$ adaptive optimization ($\mathcal{L}_{1}$-AO). The proposed method uses a baseline TV optimizer with a prediction model, designed for the gradient dynamics to exploit the underlying structure of the temporal correlation. Inspired by $\mathcal{L}_{1}$ adaptive control, the proposed method augments an adaptive update law to estimate and compensate for the uncertainty from the inaccurate prediction in the online implementation. The proposed method provides the performance bounds of the error in the optimization variables and cost function, allowing efficient and reliable optimization for uncertain TV problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16583v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinrae Kim, Naira Hovakimyan</dc:creator>
    </item>
    <item>
      <title>Stochastic Shortest Path Problem with Failure Probability</title>
      <link>https://arxiv.org/abs/2409.16672</link>
      <description>arXiv:2409.16672v1 Announce Type: new 
Abstract: We solve a sequential decision-making problem under uncertainty that takes into account the failure probability of a task. This problem cannot be handled by the stochastic shortest path problem, which is the standard model for sequential decision-making. This problem is addressed by introducing dead-ends. Conventionally, we only consider policies that minimize the probability of task failure, so the optimal policy constructed could be overly conservative. In this paper, we address this issue by expanding the search range to a class of policies whose failure probability is less than a desired threshold. This problem can be solved by treating it as a framework of a Bayesian Markov decision process and a two-person zero-sum game. Also, it can be seen that the optimal policy is expressed in the form of a probability distribution on a set of deterministic policies. We also demonstrate the effectiveness of the proposed methods by applying them to a motion planning problem with obstacle avoidance for a moving robot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16672v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ritsusamuel Otsubo</dc:creator>
    </item>
    <item>
      <title>Distributed Robust Optimization Method for AC/MTDC Hybrid Power Systems with DC Network Cognizance</title>
      <link>https://arxiv.org/abs/2409.16750</link>
      <description>arXiv:2409.16750v1 Announce Type: new 
Abstract: AC/multi-terminal DC (MTDC) hybrid power systems have emerged as a solution for the large-scale and longdistance accommodation of power produced by renewable energy systems (RESs). To ensure the optimal operation of such hybrid power systems, this paper addresses three key issues: system operational flexibility, centralized communication limitations, and RES uncertainties. Accordingly, a specific AC/DC optimal power flow (OPF) model and a distributed robust optimization method are proposed. Firstly, we apply a set of linear approximation and convex relaxation techniques to formulate the mixed-integer convex AC/DC OPF model. This model incorporates the DC network-cognizant constraint and enables DC topology reconfiguration. Next, generalized Benders decomposition (GBD) is employed to provide distributed optimization. Enhanced approaches are incorporated into GBD to achieve parallel computation and asynchronous updating. Additionally, the extreme scenario method (ESM) is embedded into the AC/DC OPF model to provide robust decisions to hedge against RES uncertainties. ESM is further extended to align the GBD procedure. Numerical results are finally presented to validate the effectiveness of our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16750v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>SEST 2024 Proceedings</arxiv:journal_reference>
      <dc:creator>Haixiao Li, Aleksandra Leki\'c</dc:creator>
    </item>
    <item>
      <title>Revisiting Extragradient-Type Methods -- Part 1: Generalizations and Sublinear Convergence Rates</title>
      <link>https://arxiv.org/abs/2409.16859</link>
      <description>arXiv:2409.16859v1 Announce Type: new 
Abstract: This paper presents a comprehensive analysis of the well-known extragradient (EG) method for solving both equations and inclusions. First, we unify and generalize EG for [non]linear equations to a wider class of algorithms, encompassing various existing schemes and potentially new variants. Next, we analyze both sublinear ``best-iterate'' and ``last-iterate'' convergence rates for the entire class of algorithms, and derive new convergence results for two well-known instances. Second, we extend our EG framework above to ``monotone'' inclusions, introducing a new class of algorithms and its corresponding convergence results. Third, we also unify and generalize Tseng's forward-backward-forward splitting (FBFS) method to a broader class of algorithms to solve [non]linear inclusions when a weak-Minty solution exists, and establish its ``best-iterate'' convergence rate. Fourth, to complete our picture, we also investigate sublinear rates of two other common variants of EG using our EG analysis framework developed here: the reflected forward-backward splitting and the golden ratio methods. Finally, we conduct an extensive numerical experiment to validate our theoretical findings. Our results demonstrate that several new variants of our proposed algorithms outperform existing schemes in the majority of examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16859v1</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quoc Tran-Dinh, Nghia Nguyen-Trung</dc:creator>
    </item>
    <item>
      <title>Weak Closed-loop Solvability of Linear Quadratic Stochastic Optimal Control Problems with Partial Information</title>
      <link>https://arxiv.org/abs/2409.16924</link>
      <description>arXiv:2409.16924v1 Announce Type: new 
Abstract: This paper investigates a linear quadratic stochastic optimal control (LQSOC) problem with partial information. Firstly, by introducing two Riccati equations and a backward stochastic differential equation (BSDE), we solve this LQSOC problem under standard positive semidefinite assumptions. Secondly, by means of a perturbation approach, we study open-loop solvability of this problem when the weighting matrices in the cost functional are indefinite. Thirdly, we investigate weak closed-loop solvability of this problem and prove the equivalence between open-loop and weak closed-loop solvabilities. Finally, we give an example to illustrate the way for obtaining a weak closed-loop optimal strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16924v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xun Li, Guangchen Wang, Jie Xiong, Heng Zhang</dc:creator>
    </item>
    <item>
      <title>A Single-Loop Stochastic Proximal Quasi-Newton Method for Large-Scale Nonsmooth Convex Optimization</title>
      <link>https://arxiv.org/abs/2409.16971</link>
      <description>arXiv:2409.16971v1 Announce Type: new 
Abstract: We propose a new stochastic proximal quasi-Newton method for minimizing the sum of two convex functions in the particular context that one of the functions is the average of a large number of smooth functions and the other one is nonsmooth. The new method integrates a simple single-loop SVRG (L-SVRG) technique for sampling the gradient and a stochastic limited-memory BFGS (L-BFGS) scheme for approximating the Hessian of the smooth function components. The globally linear convergence rate of the new method is proved under mild assumptions. It is also shown that the new method covers a proximal variant of the L-SVRG as a special case, and it allows for various generalizations through the integration with other variance reduction methods. For example, the L-SVRG can be replaced with the SAGA or SEGA in the proposed new method and thus other new stochastic proximal quasi-Newton methods with rigorously guaranteed convergence can be proposed accordingly. Moreover, we meticulously analyze the resulting nonsmooth subproblem at each iteration and utilize a compact representation of the L-BFGS matrix with the storage of some auxiliary matrices. As a result, we propose a very efficient and easily implementable semismooth Newton solver for solving the involved subproblems, whose arithmetic operation per iteration is merely order of $O(d)$, where d denotes the dimensionality of the problem. With this efficient inner solver, the new method performs well and its numerical efficiency is validated through extensive experiments on a regularized logistic regression problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16971v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yongcun Song, Zimeng Wang, Xiaoming Yuan, Hangrui Yue</dc:creator>
    </item>
    <item>
      <title>Incorporating Shape Knowledge into Regression Models</title>
      <link>https://arxiv.org/abs/2409.17084</link>
      <description>arXiv:2409.17084v1 Announce Type: new 
Abstract: Informed learning is an emerging field in machine learning that aims to compensate for insufficient data with prior knowledge. Shape knowledge covers many types of prior knowledge concerning the relationship of a function's output with respect to input variables, for example, monotonicity, convexity, etc. This shape knowledge -- when formalized into algebraic inequalities (shape constraints) -- can then be incorporated into the training of regression models via a constraint problem formulation. The defined shape-constrained regression problem is, mathematically speaking, a semi-infinite program (SIP). Although off-the-shelf algorithms can be used at this point to solve the SIP, we recommend an adaptive feasible-point algorithm that guarantees optimality up to arbitrary precision and strict fulfillment of the shape constraints. We apply this semi-infinite approach for shape-constrained regression (SIASCOR) to three application examples from manufacturing and one artificial example. One application example has not been considered in a shape-constrained regression setting before, so we used a methodology (ISI) to capture the shape knowledge and define corresponding shape constraints. Finally, we compare the SIASCOR method with a purely data-driven automated machine learning method (AutoML) and another approach for shape-constrained regression (SIAMOR) that uses a different solution algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17084v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Miltiadis Poursanidis, Patrick Link, Jochen Schmid, Uwe Teicher</dc:creator>
    </item>
    <item>
      <title>Non-asymptotic convergence analysis of the stochastic gradient Hamiltonian Monte Carlo algorithm with discontinuous stochastic gradient with applications to training of ReLU neural networks</title>
      <link>https://arxiv.org/abs/2409.17107</link>
      <description>arXiv:2409.17107v1 Announce Type: new 
Abstract: In this paper, we provide a non-asymptotic analysis of the convergence of the stochastic gradient Hamiltonian Monte Carlo (SGHMC) algorithm to a target measure in Wasserstein-1 and Wasserstein-2 distance. Crucially, compared to the existing literature on SGHMC, we allow its stochastic gradient to be discontinuous. This allows us to provide explicit upper bounds, which can be controlled to be arbitrarily small, for the expected excess risk of non-convex stochastic optimization problems with discontinuous stochastic gradients, including, among others, the training of neural networks with ReLU activation function. To illustrate the applicability of our main results, we consider numerical experiments on quantile estimation and on several optimization problems involving ReLU neural networks relevant in finance and artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17107v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luxu Liang, Ariel Neufeld, Ying Zhang</dc:creator>
    </item>
    <item>
      <title>Landscape of Policy Optimization for Finite Horizon MDPs with General State and Action</title>
      <link>https://arxiv.org/abs/2409.17138</link>
      <description>arXiv:2409.17138v1 Announce Type: new 
Abstract: Policy gradient methods are widely used in reinforcement learning. Yet, the nonconvexity of policy optimization imposes significant challenges in understanding the global convergence of policy gradient methods. For a class of finite-horizon Markov Decision Processes (MDPs) with general state and action spaces, we develop a framework that provides a set of easily verifiable assumptions to ensure the Kurdyka-Lojasiewicz (KL) condition of the policy optimization. Leveraging the KL condition, policy gradient methods converge to the globally optimal policy with a non-asymptomatic rate despite nonconvexity. Our results find applications in various control and operations models, including entropy-regularized tabular MDPs, Linear Quadratic Regulator (LQR) problems, stochastic inventory models, and stochastic cash balance problems, for which we show an $\epsilon$-optimal policy can be obtained using a sample size in $\tilde{\mathcal{O}}(\epsilon^{-1})$ and polynomial in terms of the planning horizon by stochastic policy gradient methods. Our result establishes the first sample complexity for multi-period inventory systems with Markov-modulated demands and stochastic cash balance problems in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17138v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xin Chen, Yifan Hu, Minda Zhao</dc:creator>
    </item>
    <item>
      <title>Definition of Cybernetical Neuroscience</title>
      <link>https://arxiv.org/abs/2409.16314</link>
      <description>arXiv:2409.16314v1 Announce Type: cross 
Abstract: A new scientific field is introduced and discussed, named cybernetical neuroscience, which studies mathematical models adopted in computational neuroscience by methods of cybernetics -- the science of control and communication in a living organism, machine and society. It also considers the practical application of the results obtained when studying mathematical models. The main tasks and methods, as well as some results of cybernetic neuroscience are considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16314v1</guid>
      <category>q-bio.NC</category>
      <category>math.OC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Fradkov</dc:creator>
    </item>
    <item>
      <title>Learning Linear Dynamics from Bilinear Observations</title>
      <link>https://arxiv.org/abs/2409.16499</link>
      <description>arXiv:2409.16499v1 Announce Type: cross 
Abstract: We consider the problem of learning a realization of a partially observed dynamical system with linear state transitions and bilinear observations. Under very mild assumptions on the process and measurement noises, we provide a finite time analysis for learning the unknown dynamics matrices (up to a similarity transform). Our analysis involves a regression problem with heavy-tailed and dependent data. Moreover, each row of our design matrix contains a Kronecker product of current input with a history of inputs, making it difficult to guarantee persistence of excitation. We overcome these challenges, first providing a data-dependent high probability error bound for arbitrary but fixed inputs. Then, we derive a data-independent error bound for inputs chosen according to a simple random design. Our main results provide an upper bound on the statistical error rates and sample complexity of learning the unknown dynamics matrices from a single finite trajectory of bilinear observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16499v1</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yahya Sattar, Yassir Jedra, Sarah Dean</dc:creator>
    </item>
    <item>
      <title>Optimal Denial-of-Service Attacks Against Partially-Observable Real-Time Monitoring Systems</title>
      <link>https://arxiv.org/abs/2409.16794</link>
      <description>arXiv:2409.16794v1 Announce Type: cross 
Abstract: In this paper, we investigate the impact of denial-of-service attacks on the status updating of a cyber-physical system with one or more sensors connected to a remote monitor via unreliable channels. We approach the problem from the perspective of an adversary that can strategically jam a subset of the channels. The sources are modeled as Markov chains, and the performance of status updating is measured based on the age of incorrect information at the monitor. Our objective is to derive jamming policies that strike a balance between the degradation of the system's performance and the conservation of the adversary's energy. For a single-source scenario, we formulate the problem as a partially-observable Markov decision process, and rigorously prove that the optimal jamming policy is of a threshold form. We then extend the problem to a multi-source scenario. We formulate this problem as a restless multi-armed bandit, and provide a jamming policy based on the Whittle's index. Our numerical results highlight the performance of our policies compared to baseline policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16794v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saad Kriouile, Mohamad Assaad, Amira Alloum, Touraj Soleymani</dc:creator>
    </item>
    <item>
      <title>Risk-averse learning with delayed feedback</title>
      <link>https://arxiv.org/abs/2409.16866</link>
      <description>arXiv:2409.16866v1 Announce Type: cross 
Abstract: In real-world scenarios, the impacts of decisions may not manifest immediately. Taking these delays into account facilitates accurate assessment and management of risk in real-world environments, thereby ensuring the efficacy of strategies. In this paper, we investigate risk-averse learning using Conditional Value at Risk (CVaR) as risk measure, while incorporating delayed feedback with unknown but bounded delays. We develop two risk-averse learning algorithms that rely on one-point and two-point zeroth-order optimization approaches, respectively. The regret achieved by the algorithms is analyzed in terms of the cumulative delay and the number of total samplings. The results suggest that the two-point risk-averse learning achieves a smaller regret bound than the one-point algorithm. Furthermore, the one-point risk-averse learning algorithm attains sublinear regret under certain delay conditions, and the two-point risk-averse learning algorithm achieves sublinear regret with minimal restrictions on the delay. We provide numerical experiments on a dynamic pricing problem to demonstrate the performance of the proposed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16866v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Siyi Wang, Zifan Wang, Karl Henrik Johansson, Sandra Hirche</dc:creator>
    </item>
    <item>
      <title>Metaheuristic Method for Solving Systems of Equations</title>
      <link>https://arxiv.org/abs/2409.16958</link>
      <description>arXiv:2409.16958v1 Announce Type: cross 
Abstract: This study investigates the effectiveness of Genetic Algorithms (GAs) in solving both linear and nonlinear systems of equations, comparing their performance to traditional methods such as Gaussian Elimination, Newton's Method, and Levenberg-Marquardt. The GA consistently delivered accurate solutions across various test cases, demonstrating its robustness and flexibility. A key advantage of the GA is its ability to explore the solution space broadly, uncovering multiple sets of solutions -- a feat that traditional methods, which typically converge to a single solution, cannot achieve. This feature proved especially beneficial in complex nonlinear systems, where multiple valid solutions exist, highlighting the GA's superiority in navigating intricate solution landscapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16958v1</guid>
      <category>cs.NE</category>
      <category>math.OC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Samson Odan</dc:creator>
    </item>
    <item>
      <title>Locally Regularized Sparse Graph by Fast Proximal Gradient Descent</title>
      <link>https://arxiv.org/abs/2409.17090</link>
      <description>arXiv:2409.17090v1 Announce Type: cross 
Abstract: Sparse graphs built by sparse representation has been demonstrated to be effective in clustering high-dimensional data. Albeit the compelling empirical performance, the vanilla sparse graph ignores the geometric information of the data by performing sparse representation for each datum separately. In order to obtain a sparse graph aligned with the local geometric structure of data, we propose a novel Support Regularized Sparse Graph, abbreviated as SRSG, for data clustering. SRSG encourages local smoothness on the neighborhoods of nearby data points by a well-defined support regularization term. We propose a fast proximal gradient descent method to solve the non-convex optimization problem of SRSG with the convergence matching the Nesterov's optimal convergence rate of first-order methods on smooth and convex objective function with Lipschitz continuous gradient. Extensive experimental results on various real data sets demonstrate the superiority of SRSG over other competing clustering methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17090v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongfang Sun, Yingzhen Yang</dc:creator>
    </item>
    <item>
      <title>Acceleration Methods</title>
      <link>https://arxiv.org/abs/2101.09545</link>
      <description>arXiv:2101.09545v4 Announce Type: replace 
Abstract: This monograph covers some recent advances in a range of acceleration techniques frequently used in convex optimization. We first use quadratic optimization problems to introduce two key families of methods, namely momentum and nested optimization schemes. They coincide in the quadratic case to form the Chebyshev method. We discuss momentum methods in detail, starting with the seminal work of Nesterov and structure convergence proofs using a few master templates, such as that for optimized gradient methods, which provide the key benefit of showing how momentum methods optimize convergence guarantees. We further cover proximal acceleration, at the heart of the Catalyst and Accelerated Hybrid Proximal Extragradient frameworks, using similar algorithmic patterns. Common acceleration techniques rely directly on the knowledge of some of the regularity parameters in the problem at hand. We conclude by discussing restart schemes, a set of simple techniques for reaching nearly optimal convergence rates while adapting to unobserved regularity parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.09545v4</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1561/2400000036</arxiv:DOI>
      <arxiv:journal_reference>Foundations and Trends in Optimization: Vol. 5: No. 1-2, pp 1-245 (2021)</arxiv:journal_reference>
      <dc:creator>Alexandre d'Aspremont, Damien Scieur, Adrien Taylor</dc:creator>
    </item>
    <item>
      <title>A Non-Archimedean Interior Point Method for Solving Lexicographic Multi-Objective Quadratic Programming Problems</title>
      <link>https://arxiv.org/abs/2110.15658</link>
      <description>arXiv:2110.15658v2 Announce Type: replace 
Abstract: This work presents a generalized implementation of the infeasible primal-dual Interior Point Method (IPM) achieved by the use of non-Archimedean values, i.e., infinite and infinitesimal numbers. The extended version, called here non-Archimedean IPM (NA-IPM), is proved to converge in polynomial time to a global optimum and to be able to manage infeasibility and unboundedness transparently, i.e., without considering them as corner cases: by means of a mild embedding (addition of two variables and one constraint) NA-IPM implicitly and transparently manages their possible presence. Moreover, the new algorithm is able to solve a wider variety of linear and quadratic optimization problems than its standard counterpart. Among them, the lexicographic multi-objective one deserves particular attention, since NA-IPM overcomes the issues that standard techniques (such as scalarization or preemptive approach) have. To support the theoretical properties of NA-IPM, the manuscript also shows four linear and quadratic non-Archimedean programming test cases where the effectiveness of the algorithm is verified. This also stresses that NA-IPM is not just a mere symbolic or theoretical algorithm but actually a concrete numerical tool, paving the way for its use in real-world problems in the near future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.15658v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/math10234536</arxiv:DOI>
      <arxiv:journal_reference>Mathematics, 10(23):4536, 2022</arxiv:journal_reference>
      <dc:creator>Lorenzo Fiaschi, Marco Cococcioni</dc:creator>
    </item>
    <item>
      <title>On complexity constants of linear and quadratic models for derivative-free trust-region algorithms</title>
      <link>https://arxiv.org/abs/2205.11358</link>
      <description>arXiv:2205.11358v2 Announce Type: replace 
Abstract: Complexity analysis has become an important tool in the convergence analysis of optimization algorithms. For derivative-free optimization algorithms, it is not different. Interestingly, several constants that appear when developing complexity results hide the dimensions of the problem. This work organizes several results in literature about bounds that appear in derivative-free trust-region algorithms based on linear and quadratic models. All the constants are given explicitly by the quality of the sample set, dimension of the problem and number of sample points. We extend some results to allow "inexact" interpolation sets. We also provide a clearer proof than those already existing in literature for the underdetermined case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.11358v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. E. Schwertner, F. N. C. Sobral</dc:creator>
    </item>
    <item>
      <title>Risk-averse mean field games: exploitability and non-asymptotic analysis</title>
      <link>https://arxiv.org/abs/2301.06930</link>
      <description>arXiv:2301.06930v4 Announce Type: replace 
Abstract: In this paper, we use mean field games (MFGs) to investigate approximations of $N$-player games with uniformly symmetrically continuous heterogeneous closed-loop actions. To incorporate agents' risk aversion (beyond the classical expected utility of total costs), we use an abstract evaluation functional for their performance criteria. Centered around the notion of exploitability, we conduct non-asymptotic analysis on the approximation capability of MFGs from the perspective of state-action distributions without requiring the uniqueness of equilibria. Under suitable assumptions, we first show that scenarios in the $N$-player games with large $N$ and small average exploitabilities can be well approximated by approximate solutions of MFGs with relatively small exploitabilities. We then show that $\delta$-mean field equilibria can be used to construct $\varepsilon$-equilibria in $N$-player games. Furthermore, in this general setting, we prove the existence of mean field equilibria. This proof reveals a possible avenue for incorporating penalization for randomized action into MFGs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.06930v4</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziteng Cheng, Sebastian Jaimungal</dc:creator>
    </item>
    <item>
      <title>Projective Proximal Gradient Descent for A Class of Nonconvex Nonsmooth Optimization Problems: Fast Convergence Without Kurdyka-Lojasiewicz (KL) Property</title>
      <link>https://arxiv.org/abs/2304.10499</link>
      <description>arXiv:2304.10499v2 Announce Type: replace 
Abstract: Nonconvex and nonsmooth optimization problems are important and challenging for statistics and machine learning. In this paper, we propose Projected Proximal Gradient Descent (PPGD) which solves a class of nonconvex and nonsmooth optimization problems, where the nonconvexity and nonsmoothness come from a nonsmooth regularization term which is nonconvex but piecewise convex. In contrast with existing convergence analysis of accelerated PGD methods for nonconvex and nonsmooth problems based on the Kurdyka-\L{}ojasiewicz (K\L{}) property, we provide a new theoretical analysis showing local fast convergence of PPGD. It is proved that PPGD achieves a fast convergence rate of $\cO(1/k^2)$ when the iteration number $k \ge k_0$ for a finite $k_0$ on a class of nonconvex and nonsmooth problems under mild assumptions, which is locally Nesterov's optimal convergence rate of first-order methods on smooth and convex objective function with Lipschitz continuous gradient. Experimental results demonstrate the effectiveness of PPGD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.10499v2</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingzhen Yang, Ping Li</dc:creator>
    </item>
    <item>
      <title>Dynamic Programs on Partially Ordered Sets</title>
      <link>https://arxiv.org/abs/2308.02148</link>
      <description>arXiv:2308.02148v3 Announce Type: replace 
Abstract: We introduce a framework that represents a dynamic program as a family of operators acting on a partially ordered set. We provide an optimality theory based only on order-theoretic assumptions and show how applications across almost all subfields of dynamic programming fit into this framework. These range from traditional dynamic programs to those involving nonlinear recursive preferences, desire for robustness, function approximation, Monte Carlo sampling and distributional dynamic programs. We apply the framework to establish new optimality and algorithmic results for specific applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.02148v3</guid>
      <category>math.OC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas J. Sargent, John Stachurski</dc:creator>
    </item>
    <item>
      <title>Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization</title>
      <link>https://arxiv.org/abs/2310.03234</link>
      <description>arXiv:2310.03234v5 Announce Type: replace 
Abstract: This paper investigates new families of compositional optimization problems, called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC FCCO). There has been a growing interest in FCCO due to its wide-ranging applications in machine learning and AI, as well as its ability to address the shortcomings of stochastic algorithms based on empirical risk minimization. However, current research on FCCO presumes that both the inner and outer functions are smooth, limiting their potential to tackle a more diverse set of problems. Our research expands on this area by examining non-smooth weakly-convex FCCO, where the outer function is weakly convex and non-decreasing, and the inner function is weakly-convex. We analyze a single-loop algorithm and establish its complexity for finding an $\epsilon$-stationary point of the Moreau envelop of the objective function. Additionally, we also extend the algorithm to solving novel non-smooth weakly-convex tri-level finite-sum coupled compositional optimization problems, which feature a nested arrangement of three functions. Lastly, we explore the applications of our algorithms in deep learning for two-way partial AUC maximization and multi-instance two-way partial AUC maximization, using empirical studies to showcase the effectiveness of the proposed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.03234v5</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quanqi Hu, Dixian Zhu, Tianbao Yang</dc:creator>
    </item>
    <item>
      <title>Metric Entropy-Free Sample Complexity Bounds for Sample Average Approximation in Convex Stochastic Programming</title>
      <link>https://arxiv.org/abs/2401.00664</link>
      <description>arXiv:2401.00664v4 Announce Type: replace 
Abstract: This paper studies sample average approximation (SAA) in solving convex or strongly convex stochastic programming (SP) problems. Under some common regularity conditions, we show -- perhaps for the first time -- that SAA's sample complexity can be completely free from any quantification of metric entropy (such as the logarithm of the covering number), leading to a significantly more efficient rate with dimensionality $d$ than most existing results. From the newly established complexity bounds, an important revelation is that SAA and the canonical stochastic mirror descent (SMD) method, two mainstream solution approaches to SP, entail almost identical rates of sample efficiency, rectifying a persistent theoretical discrepancy of SAA from SMD by the order of $O(d)$. Furthermore, this paper explores non-Lipschitzian scenarios where SAA maintains provable efficacy but the corresponding results for SMD remain mostly unexplored, indicating the potential of SAA's better applicability in some irregular settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00664v4</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongcheng Liu, Jindong Tong</dc:creator>
    </item>
    <item>
      <title>An Efficient Two-Sided Sketching Method for Large-Scale Tensor Decomposition Based on Transformed Domains</title>
      <link>https://arxiv.org/abs/2404.16580</link>
      <description>arXiv:2404.16580v4 Announce Type: replace 
Abstract: Large tensors are frequently encountered in various fields such as computer vision, scientific simulations, sensor networks, and data mining. However, these tensors are often too large for convenient processing, transfer, or storage. Fortunately, they typically exhibit a low-rank structure that can be leveraged through tensor decomposition. However, performing large-scale tensor decomposition can be time-consuming. Sketching is a useful technique to reduce the dimensionality of the data. In this paper, we propose a novel two-sided sketching method based on the $\star_{L}$-product decomposition and transformed domains like the discrete cosine transformation. A rigorous theoretical analysis is also conducted to assess the approximation error of the proposed method. Specifically, we improve our method with power iteration to achieve more precise approximate solutions. Extensive numerical experiments and comparisons on low-rank approximation of synthetic large tensors and real-world data like color images and grayscale videos illustrate the efficiency and effectiveness of the proposed approach in terms of both CPU time and approximation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16580v4</guid>
      <category>math.OC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiguang Cheng, Gaohang Yu, Xiaohao Cai, Liqun Qi</dc:creator>
    </item>
    <item>
      <title>Computational analysis on a linkage between generalized logit dynamic and discounted mean field game</title>
      <link>https://arxiv.org/abs/2405.15180</link>
      <description>arXiv:2405.15180v2 Announce Type: replace 
Abstract: Logit dynamics are dynamical systems describing transitions and equilibria of actions of interacting players under uncertainty. An uncertainty is embodied in logit dynamic as a softmax type function often called a logit function originating from a maximization problem subjected to an entropic penalization. This study provides another explanation for the generalized logit dynamic, particularly its logit function and player's heterogeneity, based on a discounted mean field game subjected to the costly decision making of a representative player. A large discount limit of the mean field game is argued to yield a logit dynamic. Further, mean field games that lead to classical and generalized logit dynamics are clarified and their well posedness is discussed. Additionally, numerical methods based on a finite difference discretization for computing generalized logit dynamics and corresponding mean field games are presented. Numerical methods are applied to two problems arising in the management of resources and environment; one involves an inland fisheries management problem with legal and illegal anglers, while the other is a sustainable tourism problem. Particularly, cases that possibly lack the regularity condition to be satisfied for the unique existence of stationary solutions are computationally discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15180v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hidekazu Yoshioka</dc:creator>
    </item>
    <item>
      <title>Distributionally Risk-Receptive and Robust Multistage Stochastic Integer Programs and Interdiction Models</title>
      <link>https://arxiv.org/abs/2406.05256</link>
      <description>arXiv:2406.05256v2 Announce Type: replace 
Abstract: In this paper, we study distributionally risk-receptive and distributionally robust (or risk-averse) multistage stochastic mixed-integer programs (denoted by DRR- and DRO-MSIPs). We present cutting plane-based and reformulation-based approaches for solving DRR- and DRO-MSIPs without and with decision-dependent uncertainty to optimality. We show that these approaches are finitely convergent with probability one. Furthermore, we introduce generalizations of DRR- and DRO-MSIPs by presenting multistage stochastic disjunctive programs and algorithms for solving them. These frameworks are useful for optimization problems under uncertainty where the focus is on analyzing outcomes based on multiple decision-makers' differing perspectives, such as interdiction problems that are attacker-defender games having non-cooperative players. To assess the performance of the algorithms for DRR- and DRO-MSIPs, we consider instances of distributionally ambiguous multistage maximum flow and facility location interdiction problems that are important in their own right. Based on our computational results, we observe that the cutting plane-based approaches are 2800% and 2410% (on average) faster than the reformulation-based approaches for the foregoing instances with distributional risk-aversion and risk-receptiveness, respectively. Additionally, we conducted out-of-sample tests to showcase the significance of the DRR framework in revealing network vulnerabilities and also in mitigating the impact of data corruption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05256v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sumin Kang, Manish Bansal</dc:creator>
    </item>
    <item>
      <title>An implementable proximal-type method for computing critical points to minimization problems with a nonsmooth and nonconvex constraint</title>
      <link>https://arxiv.org/abs/2407.07471</link>
      <description>arXiv:2407.07471v2 Announce Type: replace 
Abstract: This work proposes an implementable proximal-type method for a broad class of optimization problems involving nonsmooth and nonconvex objective and constraint functions. In contrast to existing methods that rely on an ad hoc model approximating the nonconvex functions, our approach can work with a nonconvex model constructed by the pointwise minimum of finitely many convex models. The latter can be chosen with reasonable flexibility to better fit the underlying functions' structure. We provide a unifying framework and analysis covering several subclasses of composite optimization problems and show that our method computes points satisfying certain necessary optimality conditions, which we will call model criticality. Depending on the specific model being used, our general concept of criticality boils down to standard necessary optimality conditions. Numerical experiments on some stochastic reliability-based optimization problems illustrate the practical performance of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07471v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregorio M. Sempere, Welington de Oliveira, Johannes O. Royset</dc:creator>
    </item>
    <item>
      <title>Separable Bregman Framework for Sparsity Constrained Nonlinear Optimization</title>
      <link>https://arxiv.org/abs/2409.12343</link>
      <description>arXiv:2409.12343v2 Announce Type: replace 
Abstract: This paper considers the minimization of a continuously differentiable function over a cardinality constraint. We focus on smooth and relatively smooth functions. These smoothness criteria result in new descent lemmas. Based on the new descent lemmas, novel optimality conditions and algorithms are developed, which extend the previously proposed hard-thresholding algorithms. We give a theoretical analysis of these algorithms and extend previous results on properties of iterative hard thresholding-like algorithms. In particular, we focus on the weighted $\ell_2$ norm, which requires efficient solution of convex subproblems. We apply our algorithms to compressed sensing problems to demonstrate the theoretical findings and the enhancements achieved through the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12343v2</guid>
      <category>math.OC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fatih Selim Aktas, Mustafa Celebi Pinar</dc:creator>
    </item>
    <item>
      <title>An Efficient Benders Decomposition Approach for Optimal Large-Scale Network Slicing</title>
      <link>https://arxiv.org/abs/2306.15247</link>
      <description>arXiv:2306.15247v3 Announce Type: replace-cross 
Abstract: This paper considers the network slicing (NS) problem which attempts to map multiple customized virtual network requests to a common shared network infrastructure and allocate network resources to meet diverse service requirements. This paper proposes an efficient customized Benders decomposition algorithm for globally solving the large-scale NP-hard NS problem. The proposed algorithm decomposes the hard NS problem into two relatively easy function placement (FP) and traffic routing (TR) subproblems and iteratively solves them enabling the information feedback between each other, which makes it particularly suitable to solve large-scale problems. Specifically, the FP subproblem is to place service functions into cloud nodes in the network, and solving it can return a function placement strategy based on which the TR subproblem is defined; and the TR subproblem is to find paths connecting two nodes hosting two adjacent functions in the network, and solving it can either verify that the solution of the FP subproblem is an optimal solution of the original problem, or return a valid inequality to the FP subproblem that cuts off the current infeasible solution. The proposed algorithm is guaranteed to find the globally optimal solution of the NS problem. By taking the special structure of the NS problem into consideration, we successfully develop two families of valid inequalities that render the proposed algorithm converge much more quickly and thus much more efficient. Numerical results demonstrate that the proposed valid inequalities effectively accelerate the convergence of the decomposition algorithm, and the proposed algorithm significantly outperforms the existing algorithms in terms of both solution efficiency and quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15247v3</guid>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei-Kun Chen, Zheyu Wu, Rui-Jin Zhang, Ya-Feng Liu, Yu-Hong Dai, Zhi-Quan Luo</dc:creator>
    </item>
    <item>
      <title>Golden parachutes under the threat of accidents</title>
      <link>https://arxiv.org/abs/2312.02101</link>
      <description>arXiv:2312.02101v2 Announce Type: replace-cross 
Abstract: This paper addresses a continuous-time contracting model that extends the problem introduced by Sannikov and later rigorously analysed by Possama\"{i} and Touzi. In our model, a principal hires a risk-averse agent to carry out a project. Specifically, the agent can perform two different tasks, namely to increase the instantaneous growth rate of the project's value, and to reduce the likelihood of accidents occurring. In order to compensate for these costly actions, the principal offers a continuous stream of payments throughout the entire duration of a contract, which concludes at a random time, potentially resulting in a lump-sum payment. We examine the consequences stemming from the introduction of accidents, modelled by a compound Poisson process that negatively impact the project's value. Furthermore, we investigate whether certain economic scenarii are still characterised by a golden parachute as in Sannikov's model. A golden parachute refers to a situation where the agent stops working and subsequently receives a compensation, which may be either a lump-sum payment leading to termination of the contract or a continuous stream of payments, thereby corresponding to a pension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02101v2</guid>
      <category>math.PR</category>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dylan Possama\"i, Chiara Rossato</dc:creator>
    </item>
    <item>
      <title>Towards Autonomous Supply Chains: Definition, Characteristics, Conceptual Framework, and Autonomy Levels</title>
      <link>https://arxiv.org/abs/2401.14183</link>
      <description>arXiv:2401.14183v2 Announce Type: replace-cross 
Abstract: Recent global disruptions, such as the pandemic and geopolitical conflicts, have profoundly exposed vulnerabilities in traditional supply chains, requiring exploration of more resilient alternatives. Autonomous supply chains (ASCs) have emerged as a potential solution, offering increased visibility, flexibility, and resilience in turbulent trade environments. Despite discussions in industry and academia over several years, ASCs lack well-established theoretical foundations. This paper addresses this research gap by presenting a formal definition of ASC along with its defining characteristics and auxiliary concepts. We propose a layered conceptual framework called the MIISI model. An illustrative case study focusing on the meat supply chain demonstrates an initial ASC implementation based on this conceptual model. Additionally, we introduce a seven-level supply chain autonomy reference model, delineating a trajectory towards achieving a full supply chain autonomy. Recognising that this work represents an initial endeavour, we emphasise the need for continued exploration in this emerging domain. We anticipate that this work will stimulate further research, both theoretical and technical, and contribute to the continual evolution of ASCs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14183v2</guid>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Liming Xu, Stephen Mak, Yaniv Proselkov, Alexandra Brintrup</dc:creator>
    </item>
    <item>
      <title>Fair Mixed Effects Support Vector Machine</title>
      <link>https://arxiv.org/abs/2405.06433</link>
      <description>arXiv:2405.06433v4 Announce Type: replace-cross 
Abstract: To ensure unbiased and ethical automated predictions, fairness must be a core principle in machine learning applications. Fairness in machine learning aims to mitigate biases present in the training data and model imperfections that could lead to discriminatory outcomes. This is achieved by preventing the model from making decisions based on sensitive characteristics like ethnicity or sexual orientation. A fundamental assumption in machine learning is the independence of observations. However, this assumption often does not hold true for data describing social phenomena, where data points are often clustered based. Hence, if the machine learning models do not account for the cluster correlations, the results may be biased. Especially high is the bias in cases where the cluster assignment is correlated to the variable of interest. We present a fair mixed effects support vector machine algorithm that can handle both problems simultaneously. With a reproducible simulation study we demonstrate the impact of clustered data on the quality of fair machine learning predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06433v4</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>math.OC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jo\~ao Vitor Pamplona, Jan Pablo Burgard</dc:creator>
    </item>
  </channel>
</rss>
