<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Sep 2024 01:42:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Practical Challenges of Progressive Data Science in Healthcare</title>
      <link>https://arxiv.org/abs/2409.10537</link>
      <description>arXiv:2409.10537v1 Announce Type: new 
Abstract: The healthcare system collects extensive data, encompassing patient administrative information, clinical measurements, and home-monitored health metrics. To support informed decision-making in patient care and treatment management, it is essential to review and analyze these diverse data sources. Data visualization is a promising solution to navigate healthcare datasets, uncover hidden patterns, and derive actionable insights. However, the process of creating interactive data visualization can be rather challenging due to the size and complexity of these datasets. Progressive data science offers a potential solution, enabling interaction with intermediate results during data exploration. In this paper, we reflect on our experiences with three health data visualization projects employing a progressive data science approach.
  We explore the practical implications and challenges faced at various stages, including data selection, pre-processing, data mining, transformation, and interpretation and evaluation. We highlighted unique challenges and opportunities for three projects, including visualizing surgical outcomes, tracking patient bed transfers, and integrating patient-generated data visualizations into the healthcare setting.
  We identified the following challenges: inconsistent data collection practices, the complexity of adapting to varying data completeness levels, and the need to modify designs for real-world deployment. Our findings underscore the need for careful consideration of using a progressive data science approach when designing visualizations for healthcare settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10537v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Faisal Zaki Roshan, Abhishek Ahuja, Fateme Rajabiyazdi</dc:creator>
    </item>
    <item>
      <title>A Lane Change Assistance System Based on Prediction of Driver Intention</title>
      <link>https://arxiv.org/abs/2409.10551</link>
      <description>arXiv:2409.10551v1 Announce Type: new 
Abstract: Lane change assistance system increase safety by providing warnings and other stability assistance to drivers to avert traffic dangers. In this contribution, lane change intention recognition was performed and applied to generate warnings for drivers to avoid eminent collision. Previous studies have not yet integrated driver's intended lane change actions as an input for determining when to warn drivers about eminent traffic dangers. Thus, if a driver's intended action may result in a collision, the driver should be warned in advance. In this contribution, lane change to left and right and lane keeping intentions were utilized to warn drivers of potential collision using an audio visual interface. The results indicate reduced risk of collision during lane change to left and right except lane keeping maneuvers. Moreover several participant feedback indicate an increased need for improved warnings by additional situational analysis that anticipate other vehicle behaviors such as intended lane changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10551v1</guid>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Foghor Tanshi, Dirk S\"offker</dc:creator>
    </item>
    <item>
      <title>Let's Influence Algorithms Together: How Millions of Fans Build Collective Understanding of Algorithms and Organize Coordinated Algorithmic Actions</title>
      <link>https://arxiv.org/abs/2409.10670</link>
      <description>arXiv:2409.10670v1 Announce Type: new 
Abstract: Previous research pays attention to how users strategically understand and consciously interact with algorithms but mainly focuses on an individual level, making it difficult to explore how users within communities could develop a collective understanding of algorithms and organize collective algorithmic actions. Through a two-year ethnography of online fan activities, this study investigates 43 core fans who always organize large-scale fans' collective actions and their corresponding general fan groups. This study aims to reveal how these core fans mobilize millions of general fans through collective algorithmic actions. These core fans reported the rhetorical strategies used to persuade general fans, the steps taken to build a collective understanding of algorithms, and the collaborative processes that adapt collective actions across platforms and cultures. Our findings highlight the key factors that enable computer-supported collective algorithmic actions and extend collective action research into large-scale domain targeting algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10670v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qing Xiao, Yuhang Zheng, Xianzhe Fan, Bingbing Zhang, Zhicong Lu</dc:creator>
    </item>
    <item>
      <title>Model-in-the-Loop (MILO): Accelerating Multimodal AI Data Annotation with LLMs</title>
      <link>https://arxiv.org/abs/2409.10702</link>
      <description>arXiv:2409.10702v1 Announce Type: new 
Abstract: The growing demand for AI training data has transformed data annotation into a global industry, but traditional approaches relying on human annotators are often time-consuming, labor-intensive, and prone to inconsistent quality. We propose the Model-in-the-Loop (MILO) framework, which integrates AI/ML models into the annotation process. Our research introduces a collaborative paradigm that leverages the strengths of both professional human annotators and large language models (LLMs). By employing LLMs as pre-annotation and real-time assistants, and judges on annotator responses, MILO enables effective interaction patterns between human annotators and LLMs. Three empirical studies on multimodal data annotation demonstrate MILO's efficacy in reducing handling time, improving data quality, and enhancing annotator experiences. We also introduce quality rubrics for flexible evaluation and fine-grained feedback on open-ended annotations. The MILO framework has implications for accelerating AI/ML development, reducing reliance on human annotation alone, and promoting better alignment between human and machine values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10702v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yifan Wang, David Stevens, Pranay Shah, Wenwen Jiang, Miao Liu, Xu Chen, Robert Kuo, Na Li, Boying Gong, Daniel Lee, Jiabo Hu, Ning Zhang, Bob Kamma</dc:creator>
    </item>
    <item>
      <title>"The Data Says Otherwise"-Towards Automated Fact-checking and Communication of Data Claims</title>
      <link>https://arxiv.org/abs/2409.10713</link>
      <description>arXiv:2409.10713v1 Announce Type: new 
Abstract: Fact-checking data claims requires data evidence retrieval and analysis, which can become tedious and intractable when done manually. This work presents Aletheia, an automated fact-checking prototype designed to facilitate data claims verification and enhance data evidence communication. For verification, we utilize a pre-trained LLM to parse the semantics for evidence retrieval. To effectively communicate the data evidence, we design representations in two forms: data tables and visualizations, tailored to various data fact types. Additionally, we design interactions that showcase a real-world application of these techniques. We evaluate the performance of two core NLP tasks with a curated dataset comprising 400 data claims and compare the two representation forms regarding viewers' assessment time, confidence, and preference via a user study with 20 participants. The evaluation offers insights into the feasibility and bottlenecks of using LLMs for data fact-checking tasks, potential advantages and disadvantages of using visualizations over data tables, and design recommendations for presenting data evidence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10713v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3654777.3676359</arxiv:DOI>
      <dc:creator>Yu Fu, Shunan Guo, Jane Hoffswell, Victor S. Bursztyn, Ryan Rossi, John Stasko</dc:creator>
    </item>
    <item>
      <title>Aligning Judgment Using Task Context and Explanations to Improve Human-Recommender System Performance</title>
      <link>https://arxiv.org/abs/2409.10717</link>
      <description>arXiv:2409.10717v1 Announce Type: new 
Abstract: Recommender systems, while a powerful decision making tool, are often operationalized as black box models, such that their AI algorithms are not accessible or interpretable by human operators. This in turn can cause confusion and frustration for the operator and result in unsatisfactory outcomes. While the field of explainable AI has made remarkable strides in addressing this challenge by focusing on interpreting and explaining the algorithms to human operators, there are remaining gaps in the human's understanding of the recommender system. This paper investigates the relative impact of using context, properties of the decision making task and environment, to align human and AI algorithm understanding of the state of the world, i.e. judgment, to improve joint human-recommender performance as compared to utilizing post-hoc algorithmic explanations. We conducted an empirical, between-subjects experiment in which participants were asked to work with an automated recommender system to complete a decision making task. We manipulated the method of transparency (shared contextual information to support shared judgment vs algorithmic explanations) and record the human's understanding of the task, the recommender system, and their overall performance. We found that both techniques yielded equivalent agreement on final decisions. However, those who saw task context had less tendency to over-rely on the recommender system and were able to better pinpoint in what conditions the AI erred. Both methods improved participants' confidence in their own decision making, and increased mental demand equally and frustration negligibly. These results present an alternative approach to improving team performance to post-hoc explanations and illustrate the impact of judgment on human cognition in working with recommender systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10717v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Divya Srivastava, Karen M. Feigh</dc:creator>
    </item>
    <item>
      <title>ArticulatePro: A Comparative Study on a Proactive and Non-Proactive Assistant in a Climate Data Exploration Task</title>
      <link>https://arxiv.org/abs/2409.10797</link>
      <description>arXiv:2409.10797v1 Announce Type: new 
Abstract: Recent advances in Natural Language Interfaces (NLIs) and Large Language Models (LLMs) have transformed our approach to NLP tasks, allowing us to focus more on a Pragmatics-based approach. This shift enables more natural interactions between humans and voice assistants, which have been challenging to achieve. Pragmatics describes how users often talk out of turn, interrupt each other, or provide relevant information without being explicitly asked (maxim of quantity). To explore this, we developed a digital assistant that constantly listens to conversations and proactively generates relevant visualizations during data exploration tasks. In a within-subject study, participants interacted with both proactive and non-proactive versions of a voice assistant while exploring the Hawaii Climate Data Portal (HCDP). Results suggest that the proactive assistant enhanced user engagement and facilitated quicker insights. Our study highlights the potential of Pragmatic, proactive AI in NLIs and identifies key challenges in its implementation, offering insights for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10797v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roderick Tabalba, Christopher J. Lee, Giorgio Tran, Nurit Kirshenbaum, Jason Leigh</dc:creator>
    </item>
    <item>
      <title>Improving Interface Design in Interactive Task Learning for Hierarchical Tasks based on a Qualitative Study</title>
      <link>https://arxiv.org/abs/2409.10826</link>
      <description>arXiv:2409.10826v1 Announce Type: new 
Abstract: Interactive Task Learning (ITL) systems acquire task knowledge from human instructions in natural language interaction. The interaction design of ITL agents for hierarchical tasks stays uncharted. This paper studied Verbal Apprentice Learner(VAL) for gaming, as an ITL example, and qualitatively analyzed the user study data to provide design insights on dialogue language types, task instruction strategies, and error handling. We then proposed an interface design: Editable Hierarchy Knowledge (EHK), as a generic probe for ITL systems for hierarchical tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10826v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3672539.3686326</arxiv:DOI>
      <dc:creator>Jieyu Zhou, Christopher MacLellan</dc:creator>
    </item>
    <item>
      <title>Dark Mode or Light Mode? Exploring the Impact of Contrast Polarity on Visualization Performance Between Age Groups</title>
      <link>https://arxiv.org/abs/2409.10841</link>
      <description>arXiv:2409.10841v1 Announce Type: new 
Abstract: This study examines the impact of positive and negative contrast polarities (i.e., light and dark modes) on the performance of younger adults and people in their late adulthood (PLA). In a crowdsourced study with 134 participants (69 below age 60, 66 aged 60 and above), we assessed their accuracy and time performing analysis tasks across three common visualization types (Bar, Line, Scatterplot) and two contrast polarities (positive and negative). We observed that, across both age groups, the polarity that led to better performance and the resulting amount of improvement varied on an individual basis, with each polarity benefiting comparable proportions of participants. However, the contrast polarity that led to better performance did not always match their preferred polarity. Additionally, we observed that the choice of contrast polarity can have an impact on time similar to that of the choice of visualization type, resulting in an average percent difference of around 36%. These findings indicate that, overall, the effects of contrast polarity on visual analysis performance do not noticeably change with age. Furthermore, they underscore the importance of making visualizations available in both contrast polarities to better-support a broad audience with differing needs. Supplementary materials for this work can be found at \url{https://osf.io/539a4/}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10841v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zack While, Ali Sarvghad</dc:creator>
    </item>
    <item>
      <title>An Exploration of Effects of Dark Mode on University Students: A Human Computer Interface Analysis</title>
      <link>https://arxiv.org/abs/2409.10895</link>
      <description>arXiv:2409.10895v1 Announce Type: new 
Abstract: This research dives into exploring the dark mode effects on students of a university. Research is carried out implementing the dark mode in e-Learning sites and its impact on behavior of the users. Students are spending more time in front of the screen for their studies especially after the pandemic. The blue light from the screen during late hours affects circadian rhythm of the body which negatively impacts the health of humans including eye strain and headache. The difficulty that students faced during the time of interacting with various e-Learning sites especially during late hours was analyzed using different techniques of HCI like survey, interview, evaluation methods and principles of design. Dark mode is an option which creates a pseudo inverted adaptable interface by changing brighter elements of UI into a dim-lit friendly environment. It is said that using dark mode will lessen the amount of blue light emitted and benefit students who suffer from eye strain. Students' interactions with dark mode were investigated using a survey, and an e-learning site with a dark mode theme was created. Based on the students' comments, researchers looked into the effects of dark mode on HCI in e-learning sites. The findings indicate that students have a clear preference for dark mode: 79.7% of survey participants preferred dark mode on their phones, and 61.7% said they would be interested in seeing this feature added to e-learning websites.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10895v1</guid>
      <category>cs.HC</category>
      <category>cs.CE</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Awan Shrestha, Sabil Shrestha, Biplov Paneru, Bishwash Paneru, Sansrit Paudel, Ashish Adhikari, Sanjog Chhetri Sapkota</dc:creator>
    </item>
    <item>
      <title>ASHABot: An LLM-Powered Chatbot to Support the Informational Needs of Community Health Workers</title>
      <link>https://arxiv.org/abs/2409.10913</link>
      <description>arXiv:2409.10913v1 Announce Type: new 
Abstract: Community health workers (CHWs) provide last-mile healthcare services but face challenges due to limited medical knowledge and training. This paper describes the design, deployment, and evaluation of ASHABot, an LLM-powered, experts-in-the-loop, WhatsApp-based chatbot to address the information needs of CHWs in India. Through interviews with CHWs and their supervisors and log analysis, we examine factors affecting their engagement with ASHABot, and ASHABot's role in addressing CHWs' informational needs. We found that ASHABot provided a private channel for CHWs to ask rudimentary and sensitive questions they hesitated to ask supervisors. CHWs trusted the information they received on ASHABot and treated it as an authoritative resource. CHWs' supervisors expanded their knowledge by contributing answers to questions ASHABot failed to answer, but were concerned about demands on their workload and increased accountability. We emphasize positioning LLMs as supplemental fallible resources within the community healthcare ecosystem, instead of as replacements for supervisor support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10913v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Pragnya Ramjee, Mehak Chhokar, Bhuvan Sachdeva, Mahendra Meena, Hamid Abdullah, Aditya Vashistha, Ruchit Nagar, Mohit Jain</dc:creator>
    </item>
    <item>
      <title>Gestalt driven augmented collimator widget for precise 5 dof dental drill tool positioning in 3d space</title>
      <link>https://arxiv.org/abs/2409.10960</link>
      <description>arXiv:2409.10960v1 Announce Type: new 
Abstract: Drill tool positioning in dental implantology is a challenging task requiring 5DOF precision as the rotation around the tool axis is not influential. This work improves the quasi-static visual elements of the state-of-the-art with a novel Augmented Collimation Widget (ACW), an interactive tool of position and angle error visualization based on the gestalt reification, the human ability to group geometric elements. The user can seek in a quick, pre-attentive way the collimation of five (three positional and two rotational) error component widgets (ECWs), taking advantage of three key aspects: component separation and reification, error visual amplification, and dynamic hiding of the collimated components. We compared the ACW with the golden standard in a within-subjects (N=30) user test using 32 implant targets, measuring the time, error, and usability. ACW performed significantly better in positional (+19%) and angular (+47%) precision accuracy and with less mental demand (-6%) and frustration (-13%), but with an expected increase in task time (+59%) and physical demand (+64%). The interview indicated the ACW as the main preference and aesthetically more pleasant than GSW, candidating it as the new golden standard for implantology, but also for other applications where 5DOF positioning is key.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10960v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ISMAR55827.2022.00033</arxiv:DOI>
      <dc:creator>Mine Dastan, Antonio E. Uva, Michele Fiorentino</dc:creator>
    </item>
    <item>
      <title>Virtual Reality for Immersive Education in Orthopedic Surgery Digital Twins</title>
      <link>https://arxiv.org/abs/2409.11014</link>
      <description>arXiv:2409.11014v1 Announce Type: new 
Abstract: Virtual Reality technology, when integrated with Surgical Digital Twins (SDTs), offers significant potential in medical training and surgical planning. We present SurgTwinVR, a VR application that immerses users within an SDT and enables them to navigate a high-fidelity virtual replica of the surgical environment. SurgTwinVR is the first VR application to utilize a dynamic 3D environment that is a clone of a real surgery, encompassing the entire surgical scene, including the surgeon, anatomy, and instruments. Our system utilizes a SDT with important improvements for real-time rendering and features to showcase the potential benefits of such an application in surgical education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11014v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jonas Hein, Jan Grunder, Lilian Calvet, Fr\'ed\'eric Giraud, Nicola Alessandro Cavalcanti, Fabio Carrillo, Philipp F\"urnstahl</dc:creator>
    </item>
    <item>
      <title>Ping! Your Food is Ready: Comparing Different Notification Techniques in 3D AR Cooking Environment</title>
      <link>https://arxiv.org/abs/2409.11357</link>
      <description>arXiv:2409.11357v1 Announce Type: new 
Abstract: Implementing visual and audio notifications on augmented reality devices is a crucial element of intuitive and easy-to-use interfaces. In this paper, we explored creating intuitive interfaces through visual and audio notifications. The study evaluated user performance and preference across three conditions: visual notifications in fixed positions, visual notifications above objects, and no visual notifications with monaural sounds. The users were tasked with cooking and serving customers in an open-source Augmented-Reality sandbox environment called ARtisan Bistro. The results indicated that visual notifications above objects combined with localized audio feedback were the most effective and preferred method by participants. The findings highlight the importance of strategic placement of visual and audio notifications in AR, providing insights for engineers and developers to design intuitive 3D user interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11357v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aditya Raikwar, Lucas Plabst, Anil Ufuk Batmaz, Florian Niebling, Francisco R. Ortega</dc:creator>
    </item>
    <item>
      <title>AI Suggestions Homogenize Writing Toward Western Styles and Diminish Cultural Nuances</title>
      <link>https://arxiv.org/abs/2409.11360</link>
      <description>arXiv:2409.11360v1 Announce Type: new 
Abstract: Large language models (LLMs) are being increasingly integrated into everyday products and services, such as coding tools and writing assistants. As these embedded AI applications are deployed globally, there is a growing concern that the AI models underlying these applications prioritize Western values. This paper investigates what happens when a Western-centric AI model provides writing suggestions to users from a different cultural background. We conducted a cross-cultural controlled experiment with 118 participants from India and the United States who completed culturally grounded writing tasks with and without AI suggestions. Our analysis reveals that AI provided greater efficiency gains for Americans compared to Indians. Moreover, AI suggestions led Indian participants to adopt Western writing styles, altering not just what is written but also how it is written. These findings show that Western-centric AI models homogenize writing toward Western norms, diminishing nuances that differentiate cultural expression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11360v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhruv Agarwal, Mor Naaman, Aditya Vashistha</dc:creator>
    </item>
    <item>
      <title>Personalized Speech Emotion Recognition in Human-Robot Interaction using Vision Transformers</title>
      <link>https://arxiv.org/abs/2409.10687</link>
      <description>arXiv:2409.10687v1 Announce Type: cross 
Abstract: Emotions are an essential element in verbal communication, so understanding individuals' affect during a human-robot interaction (HRI) becomes imperative. This paper investigates the application of vision transformer models, namely ViT (Vision Transformers) and BEiT (BERT Pre-Training of Image Transformers) pipelines, for Speech Emotion Recognition (SER) in HRI. The focus is to generalize the SER models for individual speech characteristics by fine-tuning these models on benchmark datasets and exploiting ensemble methods. For this purpose, we collected audio data from different human subjects having pseudo-naturalistic conversations with the NAO robot. We then fine-tuned our ViT and BEiT-based models and tested these models on unseen speech samples from the participants. In the results, we show that fine-tuning vision transformers on benchmark datasets and and then using either these already fine-tuned models or ensembling ViT/BEiT models gets us the highest classification accuracies per individual when it comes to identifying four primary emotions from their speech: neutral, happy, sad, and angry, as compared to fine-tuning vanilla-ViTs or BEiTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10687v1</guid>
      <category>eess.AS</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <category>cs.SD</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruchik Mishra, Andrew Frye, Madan Mohan Rayguru, Dan O. Popa</dc:creator>
    </item>
    <item>
      <title>A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children</title>
      <link>https://arxiv.org/abs/2409.10710</link>
      <description>arXiv:2409.10710v1 Announce Type: cross 
Abstract: Social-emotional learning (SEL) skills are essential for children to develop to provide a foundation for future relational and academic success. Using art as a medium for creation or as a topic to provoke conversation is a well-known method of SEL learning. Similarly, social robots have been used to teach SEL competencies like empathy, but the combination of art and social robotics has been minimally explored. In this paper, we present a novel child-robot interaction designed to foster empathy and promote SEL competencies via a conversation about art scaffolded by a social robot. Participants (N=11, age range: 7-11) conversed with a social robot about emotional and neutral art. Analysis of video and speech data demonstrated that this interaction design successfully engaged children in the practice of SEL skills, like emotion recognition and self-awareness, and greater rates of empathetic reasoning were observed when children engaged with the robot about emotional art. This study demonstrated that art-based reflection with a social robot, particularly on emotional art, can foster empathy in children, and interactions with a social robot help alleviate discomfort when sharing deep or vulnerable emotions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10710v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isabella Pu, Golda Nguyen, Lama Alsultan, Rosalind Picard, Cynthia Breazeal, Sharifa Alghowinem</dc:creator>
    </item>
    <item>
      <title>Impact Of Emotions on Information Seeking And Sharing Behaviors During Pandemic</title>
      <link>https://arxiv.org/abs/2409.10754</link>
      <description>arXiv:2409.10754v1 Announce Type: cross 
Abstract: We propose a novel approach to assess the public's coping behavior during the COVID-19 outbreak by examining the emotions. Specifically, we explore (1) changes in the public's emotions with the COVID-19 crisis progression and (2) the impacts of the public's emotions on their information-seeking, information-sharing behaviors, and compliance with stay-at-home policies. We base the study on the appraisal tendency framework, detect the public's emotions by fine-tuning a pre-trained RoBERTa model, and cross-analyze third-party behavioral data. We demonstrate the feasibility and reliability of our proposed approach in providing a large-scale examination of the publi's emotions and coping behaviors in a real-world crisis: COVID-19. The approach complements prior crisis communication research, mainly based on self-reported, small-scale experiments and survey data. Our results show that anger and fear are more prominent than other emotions experienced by the public at the pandemic's outbreak stage. Results also show that the extent of low certainty and passive emotions (e.g., sadness, fear) was related to increased information-seeking and information-sharing behaviors. Additionally, high-certainty (e.g., anger) and low-certainty (e.g., sadness, fear) emotions during the outbreak correlated to the public's compliance with stay-at-home orders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10754v1</guid>
      <category>cs.SI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Smitha Muthya Sudheendra, Hao Xu, Jisu Huh, Jaideep Srivastava</dc:creator>
    </item>
    <item>
      <title>Context-Dependent Interactable Graphical User Interface Element Detection for Spatial Computing Applications</title>
      <link>https://arxiv.org/abs/2409.10811</link>
      <description>arXiv:2409.10811v2 Announce Type: cross 
Abstract: In recent years, spatial computing Virtual Reality (VR) has emerged as a transformative technology, offering users immersive and interactive experiences across diversified virtual environments. Users can interact with VR apps through interactable GUI elements (IGEs) on the stereoscopic three-dimensional (3D) graphical user interface (GUI). The accurate recognition of these IGEs is instrumental, serving as the foundation of many software engineering tasks, including automated testing and effective GUI search. The most recent IGE detection approaches for 2D mobile apps typically train a supervised object detection model based on a large-scale manually-labeled GUI dataset, usually with a pre-defined set of clickable GUI element categories like buttons and spinners. Such approaches can hardly be applied to IGE detection in VR apps, due to a multitude of challenges including complexities posed by open-vocabulary and heterogeneous IGE categories, intricacies of context-sensitive interactability, and the necessities of precise spatial perception and visual-semantic alignment for accurate IGE detection results. Thus, it is necessary to embark on the IGE research tailored to VR apps. In this paper, we propose the first zero-shot cOntext-sensitive inteRactable GUI ElemeNT dEtection framework for virtual Reality apps, named Orienter. By imitating human behaviors, Orienter observes and understands the semantic contexts of VR app scenes first, before performing the detection. The detection process is iterated within a feedback-directed validation and reflection loop. Specifically, Orienter contains three components, including (1) Semantic context comprehension, (2) Reflection-directed IGE candidate detection, and (3) Context-sensitive interactability classification. Extensive experiments demonstrate that Orienter is more effective than the state-of-the-art GUI element detection approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10811v2</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuqing Li, Binchang Li, Yepang Liu, Cuiyun Gao, Jianping Zhang, Shing-Chi Cheung, Michael R. Lyu</dc:creator>
    </item>
    <item>
      <title>SIFToM: Robust Spoken Instruction Following through Theory of Mind</title>
      <link>https://arxiv.org/abs/2409.10849</link>
      <description>arXiv:2409.10849v1 Announce Type: cross 
Abstract: Spoken language instructions are ubiquitous in agent collaboration. However, in human-robot collaboration, recognition accuracy for human speech is often influenced by various speech and environmental factors, such as background noise, the speaker's accents, and mispronunciation. When faced with noisy or unfamiliar auditory inputs, humans use context and prior knowledge to disambiguate the stimulus and take pragmatic actions, a process referred to as top-down processing in cognitive science. We present a cognitively inspired model, Speech Instruction Following through Theory of Mind (SIFToM), to enable robots to pragmatically follow human instructions under diverse speech conditions by inferring the human's goal and joint plan as prior for speech perception and understanding. We test SIFToM in simulated home experiments (VirtualHome 2). Results show that the SIFToM model outperforms state-of-the-art speech and language models, approaching human-level accuracy on challenging speech instruction following tasks. We then demonstrate its ability at the task planning level on a mobile manipulator for breakfast preparation tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10849v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lance Ying, Jason Xinyu Liu, Shivam Aarya, Yizirui Fang, Stefanie Tellex, Joshua B. Tenenbaum, Tianmin Shu</dc:creator>
    </item>
    <item>
      <title>Towards Ethical Personal AI Applications: Practical Considerations for AI Assistants with Long-Term Memory</title>
      <link>https://arxiv.org/abs/2409.11192</link>
      <description>arXiv:2409.11192v1 Announce Type: cross 
Abstract: One application area of long-term memory (LTM) capabilities with increasing traction is personal AI companions and assistants. With the ability to retain and contextualize past interactions and adapt to user preferences, personal AI companions and assistants promise a profound shift in how we interact with AI and are on track to become indispensable in personal and professional settings. However, this advancement introduces new challenges and vulnerabilities that require careful consideration regarding the deployment and widespread use of these systems. The goal of this paper is to explore the broader implications of building and deploying personal AI applications with LTM capabilities using a holistic evaluation approach. This will be done in three ways: 1) reviewing the technological underpinnings of LTM in Large Language Models, 2) surveying current personal AI companions and assistants, and 3) analyzing critical considerations and implications of deploying and using these applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11192v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eunhae Lee</dc:creator>
    </item>
    <item>
      <title>A Human-Centered Risk Evaluation of Biometric Systems Using Conjoint Analysis</title>
      <link>https://arxiv.org/abs/2409.11224</link>
      <description>arXiv:2409.11224v1 Announce Type: cross 
Abstract: Biometric recognition systems, known for their convenience, are widely adopted across various fields. However, their security faces risks depending on the authentication algorithm and deployment environment. Current risk assessment methods faces significant challenges in incorporating the crucial factor of attacker's motivation, leading to incomplete evaluations. This paper presents a novel human-centered risk evaluation framework using conjoint analysis to quantify the impact of risk factors, such as surveillance cameras, on attacker's motivation. Our framework calculates risk values incorporating the False Acceptance Rate (FAR) and attack probability, allowing comprehensive comparisons across use cases. A survey of 600 Japanese participants demonstrates our method's effectiveness, showing how security measures influence attacker's motivation. This approach helps decision-makers customize biometric systems to enhance security while maintaining usability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11224v1</guid>
      <category>cs.CV</category>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tetsushi Ohki, Narishige Abe, Hidetsugu Uchida, Shigefumi Yamada</dc:creator>
    </item>
    <item>
      <title>Spontaneous Informal Speech Dataset for Punctuation Restoration</title>
      <link>https://arxiv.org/abs/2409.11241</link>
      <description>arXiv:2409.11241v1 Announce Type: cross 
Abstract: Presently, punctuation restoration models are evaluated almost solely on well-structured, scripted corpora. On the other hand, real-world ASR systems and post-processing pipelines typically apply towards spontaneous speech with significant irregularities, stutters, and deviations from perfect grammar. To address this discrepancy, we introduce SponSpeech, a punctuation restoration dataset derived from informal speech sources, which includes punctuation and casing information. In addition to publicly releasing the dataset, we contribute a filtering pipeline that can be used to generate more data. Our filtering pipeline examines the quality of both speech audio and transcription text. We also carefully construct a ``challenging" test set, aimed at evaluating models' ability to leverage audio information to predict otherwise grammatically ambiguous punctuation. SponSpeech is available at https://github.com/GitHubAccountAnonymous/PR, along with all code for dataset building and model runs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11241v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.33136.88329</arxiv:DOI>
      <arxiv:journal_reference>Recognition Technologies, Inc. Technical Report, 2024</arxiv:journal_reference>
      <dc:creator>Xing Yi Liu, Homayoon Beigi</dc:creator>
    </item>
    <item>
      <title>Interactive AI Alignment: Specification, Process, and Evaluation Alignment</title>
      <link>https://arxiv.org/abs/2311.00710</link>
      <description>arXiv:2311.00710v2 Announce Type: replace 
Abstract: Modern AI enables a high-level, declarative form of interaction: Users describe the intended outcome they wish an AI to produce, but do not actually create the outcome themselves. In contrast, in traditional user interfaces, users invoke specific operations to create the desired outcome. This paper revisits the basic input-output interaction cycle in light of this declarative style of interaction, and connects concepts in AI alignment to define three objectives for interactive alignment of AI: specification alignment (aligning on what to do), process alignment (aligning on how to do it), and evaluation alignment (assisting users in verifying and understanding what was produced). Using existing systems as examples, we show how these user-centered views of AI alignment can be used descriptively, prescriptively, and as an evaluative aid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00710v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Terry, Chinmay Kulkarni, Martin Wattenberg, Lucas Dixon, Meredith Ringel Morris</dc:creator>
    </item>
    <item>
      <title>What Lies Beneath? Exploring the Impact of Underlying AI Model Updates in AI-Infused Systems</title>
      <link>https://arxiv.org/abs/2311.10652</link>
      <description>arXiv:2311.10652v2 Announce Type: replace 
Abstract: AI models are constantly evolving, with new versions released frequently. This raises a key question: how should AI-infused systems integrate updates when the downstream impact on user experience and performance is unclear? Human-AI interaction guidelines encourage notifying users about (changes in) model capabilities, ideally supported by thorough benchmarking. Yet, as AI models integrate into domain-specific workflows, exhaustive benchmarking can become impractical or expensive, often leading to invisible or minimally communicated updates. In this work, we explore the impact of such updates through two complementary studies on facial recognition for historical person identification. First, we conducted an online experiment to understand how users distinguish between models, followed by a diary study examining user perceptions in a real-world deployment. Our findings reveal how model changes impact human-AI performance, downstream user behavior, and the folk theories they develop. Based on these insights, we discuss implications for updating models in AI-infused systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10652v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vikram Mohanty, Jude Lim, Kurt Luther</dc:creator>
    </item>
    <item>
      <title>Information That Matters: Exploring Information Needs of People Affected by Algorithmic Decisions</title>
      <link>https://arxiv.org/abs/2401.13324</link>
      <description>arXiv:2401.13324v5 Announce Type: replace 
Abstract: Every AI system that makes decisions about people has a group of stakeholders that are personally affected by these decisions. However, explanations of AI systems rarely address the information needs of this stakeholder group, who often are AI novices. This creates a gap between conveyed information and information that matters to those who are impacted by the system's decisions, such as domain experts and decision subjects. To address this, we present the "XAI Novice Question Bank," an extension of the XAI Question Bank containing a catalog of information needs from AI novices in two use cases: employment prediction and health monitoring. The catalog covers the categories of data, system context, system usage, and system specifications. We gathered information needs through task-based interviews where participants asked questions about two AI systems to decide on their adoption and received verbal explanations in response. Our analysis showed that participants' confidence increased after receiving explanations but that their understanding faced challenges. These included difficulties in locating information and in assessing their own understanding, as well as attempts to outsource understanding. Additionally, participants' prior perceptions of the systems' risks and benefits influenced their information needs. Participants who perceived high risks sought explanations about the intentions behind a system's deployment, while those who perceived low risks rather asked about the system's operation. Our work aims to support the inclusion of AI novices in explainability efforts by highlighting their information needs, aims, and challenges. We summarize our findings as five key implications that can inform the design of future explanations for lay stakeholder audiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13324v5</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timoth\'ee Schmude, Laura Koesten, Torsten M\"oller, Sebastian Tschiatschek</dc:creator>
    </item>
    <item>
      <title>Situated Understanding of Errors in Older Adults' Interactions with Voice Assistants: A Month-Long, In-Home Study</title>
      <link>https://arxiv.org/abs/2403.02421</link>
      <description>arXiv:2403.02421v2 Announce Type: replace 
Abstract: Our work addresses the challenges older adults face with commercial Voice Assistants (VAs), notably in conversation breakdowns and error handling. Traditional methods of collecting user experiences-usage logs and post-hoc interviews-do not fully capture the intricacies of older adults' interactions with VAs, particularly regarding their reactions to errors. To bridge this gap, we equipped 15 older adults' homes with smart speakers integrated with custom audio recorders to collect ``in-the-wild'' audio interaction data for detailed error analysis. Recognizing the conversational limitations of current VAs, our study also explored the capabilities of Large Language Models (LLMs) to handle natural and imperfect text for improving VAs. Midway through our study, we deployed ChatGPT-powered VA to investigate its efficacy for older adults. Our research suggests leveraging vocal and verbal responses combined with LLMs' contextual capabilities for enhanced error prevention and management in VAs, while proposing design considerations to align VA capabilities with older adults' expectations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02421v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amama Mahmood, Junxiang Wang, Chien-Ming Huang</dc:creator>
    </item>
    <item>
      <title>Visual grounding for desktop graphical user interfaces</title>
      <link>https://arxiv.org/abs/2407.01558</link>
      <description>arXiv:2407.01558v2 Announce Type: replace 
Abstract: Most instance perception and image understanding solutions focus mainly on natural images. However, applications for synthetic images, and more specifically, images of Graphical User Interfaces (GUI) remain limited. This hinders the development of autonomous computer-vision-powered Artificial Intelligence (AI) agents. In this work, we present Instruction Visual Grounding or IVG, a multi-modal solution for object identification in a GUI. More precisely, given a natural language instruction and GUI screen, IVG locates the coordinates of the element on the screen where the instruction would be executed. To this end, we develop two methods. The first method is a three-part architecture that relies on a combination of a Large Language Model (LLM) and an object detection model. The second approach uses a multi-modal foundation model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01558v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tassnim Dardouri, Laura Minkova, Jessica L\'opez Espejel, Walid Dahhane, El Hassane Ettifouri</dc:creator>
    </item>
    <item>
      <title>StuGPTViz: A Visual Analytics Approach to Understand Student-ChatGPT Interactions</title>
      <link>https://arxiv.org/abs/2407.12423</link>
      <description>arXiv:2407.12423v3 Announce Type: replace 
Abstract: The integration of Large Language Models (LLMs), especially ChatGPT, into education is poised to revolutionize students' learning experiences by introducing innovative conversational learning methodologies. To empower students to fully leverage the capabilities of ChatGPT in educational scenarios, understanding students' interaction patterns with ChatGPT is crucial for instructors. However, this endeavor is challenging due to the absence of datasets focused on student-ChatGPT conversations and the complexities in identifying and analyzing the evolutional interaction patterns within conversations. To address these challenges, we collected conversational data from 48 students interacting with ChatGPT in a master's level data visualization course over one semester. We then developed a coding scheme, grounded in the literature on cognitive levels and thematic analysis, to categorize students' interaction patterns with ChatGPT. Furthermore, we present a visual analytics system, StuGPTViz, that tracks and compares temporal patterns in student prompts and the quality of ChatGPT's responses at multiple scales, revealing significant pedagogical insights for instructors. We validated the system's effectiveness through expert interviews with six data visualization instructors and three case studies. The results confirmed StuGPTViz's capacity to enhance educators' insights into the pedagogical value of ChatGPT. We also discussed the potential research opportunities of applying visual analytics in education and developing AI-driven personalized learning solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12423v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zixin Chen, Jiachen Wang, Meng Xia, Kento Shigyo, Dingdong Liu, Rong Zhang, Huamin Qu</dc:creator>
    </item>
    <item>
      <title>Bridging Research and Practice Through Conversation: Reflecting on Our Experience</title>
      <link>https://arxiv.org/abs/2409.05880</link>
      <description>arXiv:2409.05880v2 Announce Type: replace 
Abstract: While some research fields have a long history of collaborating with domain experts outside academia, many quantitative researchers do not have natural avenues to meet experts in areas where the research is later deployed. We explain how conversations -- interviews without a specific research objective -- can bridge research and practice. Using collaborative autoethnography, we reflect on our experience of conducting conversations with practitioners from a range of different backgrounds, including refugee rights, conservation, addiction counseling, and municipal data science. Despite these varied backgrounds, common lessons emerged, including the importance of valuing the knowledge of experts, recognizing that academic research and practice have differing objectives and timelines, understanding the limits of quantification, and avoiding data extractivism. We consider the impact of these conversations on our work, the potential roles we can serve as researchers, and the challenges we anticipate as we move forward in these collaborations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05880v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3689904.3694705</arxiv:DOI>
      <dc:creator>Mayra Russo, Mackenzie Jorgensen, Kristen M. Scott, Wendy Xu, Di H. Nguyen, Jessie Finocchiaro, Matthew Olckers</dc:creator>
    </item>
    <item>
      <title>AACessTalk: Fostering Communication between Minimally Verbal Autistic Children and Parents with Contextual Guidance and Card Recommendation</title>
      <link>https://arxiv.org/abs/2409.09641</link>
      <description>arXiv:2409.09641v2 Announce Type: replace 
Abstract: As minimally verbal autistic (MVA) children communicate with parents through few words and nonverbal cues, parents often struggle to encourage their children to express subtle emotions and needs and to grasp their nuanced signals. We present AACessTalk, a tablet-based, AI-mediated communication system that facilitates meaningful exchanges between an MVA child and a parent. AACessTalk provides real-time guides to the parent to engage the child in conversation and, in turn, recommends contextual vocabulary cards to the child. Through a two-week deployment study with 11 MVA child-parent dyads, we examine how AACessTalk fosters everyday conversation practice and mutual engagement. Our findings show high engagement from all dyads, leading to increased frequency of conversation and turn-taking. AACessTalk also encouraged parents to explore their own interaction strategies and empowered the children to have more agency in communication. We discuss the implications of designing technologies for balanced communication dynamics in parent-MVA child interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09641v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dasom Choi, SoHyun Park, Kyungah Lee, Hwajung Hong, Young-Ho Kim</dc:creator>
    </item>
    <item>
      <title>ExploreSelf: Fostering User-driven Exploration and Reflection on Personal Challenges with Adaptive Guidance by Large Language Models</title>
      <link>https://arxiv.org/abs/2409.09662</link>
      <description>arXiv:2409.09662v2 Announce Type: replace 
Abstract: Expressing stressful experiences in words is proven to improve mental and physical health, but individuals often disengage with writing interventions as they struggle to organize their thoughts and emotions. Reflective prompts have been used to provide direction, and large language models (LLMs) have demonstrated the potential to provide tailored guidance. Current systems often limit users' flexibility to direct their reflections. We thus present ExploreSelf, an LLM-driven application designed to empower users to control their reflective journey. ExploreSelf allows users to receive adaptive support through dynamically generated questions. Through an exploratory study with 19 participants, we examine how participants explore and reflect on personal challenges using ExploreSelf. Our findings demonstrate that participants valued the balance between guided support and freedom to control their reflective journey, leading to deeper engagement and insight. Building on our findings, we discuss implications for designing LLM-driven tools that promote user empowerment through effective reflective practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09662v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Inhwa Song, SoHyun Park, Sachin R. Pendse, Jessica Lee Schleider, Munmun De Choudhury, Young-Ho Kim</dc:creator>
    </item>
    <item>
      <title>Learnings from a Large-Scale Deployment of an LLM-Powered Expert-in-the-Loop Healthcare Chatbot</title>
      <link>https://arxiv.org/abs/2409.10354</link>
      <description>arXiv:2409.10354v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are widely used in healthcare, but limitations like hallucinations, incomplete information, and bias hinder their reliability. To address these, researchers released the Build Your Own expert Bot (BYOeB) platform, enabling developers to create LLM-powered chatbots with integrated expert verification. CataractBot, its first implementation, provides expert-verified responses to cataract surgery questions. A pilot evaluation showed its potential; however the study had a small sample size and was primarily qualitative. In this work, we conducted a large-scale 24-week deployment of CataractBot involving 318 patients and attendants who sent 1,992 messages, with 91.71% of responses verified by seven experts. Analysis of interaction logs revealed that medical questions significantly outnumbered logistical ones, hallucinations were negligible, and experts rated 84.52% of medical answers as accurate. As the knowledge base expanded with expert corrections, system performance improved by 19.02%, reducing expert workload. These insights guide the design of future LLM-powered chatbots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10354v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Bhuvan Sachdeva, Pragnya Ramjee, Geeta Fulari, Kaushik Murali, Mohit Jain</dc:creator>
    </item>
    <item>
      <title>Real or Robotic? Assessing Whether LLMs Accurately Simulate Qualities of Human Responses in Dialogue</title>
      <link>https://arxiv.org/abs/2409.08330</link>
      <description>arXiv:2409.08330v2 Announce Type: replace-cross 
Abstract: Studying and building datasets for dialogue tasks is both expensive and time-consuming due to the need to recruit, train, and collect data from study participants. In response, much recent work has sought to use large language models (LLMs) to simulate both human-human and human-LLM interactions, as they have been shown to generate convincingly human-like text in many settings. However, to what extent do LLM-based simulations \textit{actually} reflect human dialogues? In this work, we answer this question by generating a large-scale dataset of 100,000 paired LLM-LLM and human-LLM dialogues from the WildChat dataset and quantifying how well the LLM simulations align with their human counterparts. Overall, we find relatively low alignment between simulations and human interactions, demonstrating a systematic divergence along the multiple textual properties, including style and content. Further, in comparisons of English, Chinese, and Russian dialogues, we find that models perform similarly. Our results suggest that LLMs generally perform better when the human themself writes in a way that is more similar to the LLM's own style.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08330v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Ivey, Shivani Kumar, Jiayu Liu, Hua Shen, Sushrita Rakshit, Rohan Raju, Haotian Zhang, Aparna Ananthasubramaniam, Junghwan Kim, Bowen Yi, Dustin Wright, Abraham Israeli, Anders Giovanni M{\o}ller, Lechen Zhang, David Jurgens</dc:creator>
    </item>
  </channel>
</rss>
