<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Jun 2025 04:00:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>$whittlehurst$: A Python package implementing Whittle's likelihood estimation of the Hurst exponent</title>
      <link>https://arxiv.org/abs/2506.01985</link>
      <description>arXiv:2506.01985v1 Announce Type: new 
Abstract: This paper presents $whittlehurst$, a Python package implementing Whittle's likelihood method for estimating the Hurst exponent in fractional Brownian motion (fBm). While the theoretical foundations of Whittle's estimator are well-established, practical and computational considerations are critical for effective use. We focus explicitly on assessing our implementation's performance across several numerical approximations of the fractional Gaussian noise (fGn) spectral density, comparing their computational efficiency, accuracy, and consistency across varying input sequence lengths. Extensive empirical evaluations show that our implementation achieves state-of-the-art estimation accuracy and computational speed. Additionally, we benchmark our method against other popular Hurst exponent estimation techniques on synthetic and real-world data, emphasizing practical considerations that arise when applying these estimators to financial and biomedical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01985v1</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>B\'alint Csan\'ady, L\'or\'ant Nagy, Andr\'as Luk\'acs</dc:creator>
    </item>
    <item>
      <title>A label-switching algorithm for fast core-periphery identification</title>
      <link>https://arxiv.org/abs/2506.02069</link>
      <description>arXiv:2506.02069v1 Announce Type: new 
Abstract: Core-periphery (CP) structure is frequently observed in networks where the nodes form two distinct groups: a small, densely interconnected core and a sparse periphery. Borgatti and Everett (2000) proposed one of the most popular methods to identify and quantify CP structure by comparing the observed network with an ``ideal'' CP structure. While this metric has been widely used, an improved algorithm is still needed. In this work, we detail a greedy, label-switching algorithm to identify CP structure that is both fast and accurate. By leveraging a mathematical reformulation of the CP metric, our proposed heuristic offers an order-of-magnitude improvement on the number of operations compared to a naive implementation. We prove that the algorithm converges to a local minimum while consistently yielding solutions within 90\% of the global optimum on small toy networks. On synthetic networks, our algorithm exhibits superior classification accuracies and run-times compared to a popular competing method, and the analysis of real-world networks shows that the proposed method can be nearly 400 times faster than the competition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02069v1</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Yanchenko, Srijan Sengupta</dc:creator>
    </item>
    <item>
      <title>Asymptotically exact variational flows via involutive MCMC kernels</title>
      <link>https://arxiv.org/abs/2506.02162</link>
      <description>arXiv:2506.02162v1 Announce Type: new 
Abstract: Most expressive variational families -- such as normalizing flows -- lack practical convergence guarantees, as their theoretical assurances typically hold only at the intractable global optimum. In this work, we present a general recipe for constructing tuning-free, asymptotically exact variational flows from involutive MCMC kernels. The core methodological component is a novel representation of general involutive MCMC kernels as invertible, measure-preserving iterated random function systems, which act as the flow maps of our variational flows. This leads to three new variational families with provable total variation convergence. Our framework resolves key practical limitations of existing variational families with similar guarantees (e.g., MixFlows), while requiring substantially weaker theoretical assumptions. Finally, we demonstrate the competitive performance of our flows across tasks including posterior approximation, Monte Carlo estimates, and normalization constant estimation, outperforming or matching No-U-Turn sampler (NUTS) and black-box normalizing flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02162v1</guid>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zuheng Xu, Trevor Campbell</dc:creator>
    </item>
    <item>
      <title>Complexity of exact sampling of the first passage of a stable subordinator</title>
      <link>https://arxiv.org/abs/2506.03047</link>
      <description>arXiv:2506.03047v1 Announce Type: new 
Abstract: We consider the exact sampling of the first passage of a stable subordinator across a non-increasing regular barrier. First, the sampling is reduced to one from a bivariate distribution parameterized by the index $\alpha$ of the subordinator and a scalar $z$ independent of the barrier. Then three algorithms are devised for different regions of $(\alpha, z)$, using the acceptance-rejection method without numerical inversion or integration. When combined, the algorithms allow the exact sampling of the first passage to be done with complexity $O(1+|\ln(1-\alpha)|)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03047v1</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyi Chi</dc:creator>
    </item>
    <item>
      <title>Quantization-based Bounds on the Wasserstein Metric</title>
      <link>https://arxiv.org/abs/2506.00976</link>
      <description>arXiv:2506.00976v1 Announce Type: cross 
Abstract: The Wasserstein metric has become increasingly important in many machine learning applications such as generative modeling, image retrieval and domain adaptation. Despite its appeal, it is often too costly to compute. This has motivated approximation methods like entropy-regularized optimal transport, downsampling, and subsampling, which trade accuracy for computational efficiency. In this paper, we consider the challenge of computing efficient approximations to the Wasserstein metric that also serve as strict upper or lower bounds. Focusing on discrete measures on regular grids, our approach involves formulating and exactly solving a Kantorovich problem on a coarse grid using a quantized measure and specially designed cost matrix, followed by an upscaling and correction stage. This is done either in the primal or dual space to obtain valid upper and lower bounds on the Wasserstein metric of the full-resolution inputs. We evaluate our methods on the DOTmark optimal transport images benchmark, demonstrating a 10x-100x speedup compared to entropy-regularized OT while keeping the approximation error below 2%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00976v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Bobrutsky, Amit Moscovich</dc:creator>
    </item>
    <item>
      <title>Generalized Labeled Multi-Bernoulli Filters and Multitarget-Correlation Models</title>
      <link>https://arxiv.org/abs/2506.02772</link>
      <description>arXiv:2506.02772v1 Announce Type: cross 
Abstract: The generalized labeled multi-Bernoulli (GLMB) filter is a theoretically rigorous Bayes-optimal multitarget tracking algorithm with computationally tractable implementations, based on labeled random finite set (LRFS) theory. It presumes that multitarget populations can be approximated using GLMB multitarget probability density functions (p.d.f.'s), which consist of weighted hypotheses regarding the current target-states. A special case of the GLMB p.d.f.-the LMB p.d.f.-presumes that the targets are statistically independent. This paper demonstrates that a) GLMB p.d.f.'s can be interpreted as straightforward generalizations of LMB p.d.f.'s to statistically correlated target populations, given an implicit presumption of "simple labeled correlation" (SLC) models of multitarget correlation; b) the GLMB filter can be reformulated as a SLC-GLMB filter; and c) SLC models seem primarily appropriate for target clusters consisting of small numbers of closely-spaced targets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02772v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronald Mahler</dc:creator>
    </item>
    <item>
      <title>Accelerate Langevin Sampling with Birth-Death Process and Exploration Component</title>
      <link>https://arxiv.org/abs/2305.05529</link>
      <description>arXiv:2305.05529v2 Announce Type: replace 
Abstract: Sampling a probability distribution with known likelihood is a fundamental task in computational science and engineering. Aiming at multimodality, we propose a new sampling method that takes advantage of both birth-death process and exploration component. The main idea of this method is look before you leap. We keep two sets of samplers, one at warmer temperature and one at original temperature. The former one serves as pioneer in exploring new modes and passing useful information to the other, while the latter one samples the target distribution after receiving the information. We derive a mean-field limit and show how the exploration component accelerates the sampling process. Moreover, we prove exponential asymptotic convergence under mild assumption. Finally, we test on experiments from previous literature and compare our methodology to previous ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.05529v2</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lezhi Tan, Jianfeng Lu</dc:creator>
    </item>
    <item>
      <title>Sequential Rank and Preference Learning with the Bayesian Mallows Model</title>
      <link>https://arxiv.org/abs/2412.13644</link>
      <description>arXiv:2412.13644v2 Announce Type: replace 
Abstract: The Bayesian Mallows model is a flexible tool for analyzing data in the form of complete or partial rankings, and transitive or intransitive pairwise preferences. In many potential applications of preference learning, data arrive sequentially and it is of practical interest to update posterior beliefs and predictions efficiently, based on the currently available data. Despite this, most algorithms proposed so far have focused on batch inference. In this paper we present an algorithm for sequentially estimating the posterior distributions of the Bayesian Mallows model using nested sequential Monte Carlo. The algorithm requires minimal user input in the form of tuning parameters, is straightforward to parallelize, and returns the marginal likelihood as a direct byproduct of estimation. We evaluate its performance in simulation experiments, and illustrate a real use case with sequential ranking of Formula 1 drivers throughout three seasons of races.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13644v2</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>{\O}ystein S{\o}rensen, Anja Stein, Waldir Leoncio Netto, David S. Leslie</dc:creator>
    </item>
    <item>
      <title>Taming the Interacting Particle Langevin Algorithm: The Superlinear case</title>
      <link>https://arxiv.org/abs/2403.19587</link>
      <description>arXiv:2403.19587v4 Announce Type: replace-cross 
Abstract: Recent advances in stochastic optimization have yielded the interacting particle Langevin algorithm (IPLA), which leverages the notion of interacting particle systems (IPS) to efficiently sample from approximate posterior densities. This becomes particularly crucial in relation to the framework of Expectation-Maximization (EM), where the E-step is computationally challenging or even intractable. Although prior research has focused on scenarios involving convex cases with gradients of log densities that grow at most linearly, our work extends this framework to include polynomial growth. Taming techniques are employed to produce an explicit discretization scheme that yields a new class of stable, under such non-linearities, algorithms which are called tamed interacting particle Langevin algorithms (tIPLA). We obtain non-asymptotic convergence error estimates in Wasserstein-2 distance for the new class under the best known rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19587v4</guid>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim Johnston, Nikolaos Makras, Sotirios Sabanis</dc:creator>
    </item>
    <item>
      <title>Self-Organizing State-Space Models with Artificial Dynamics</title>
      <link>https://arxiv.org/abs/2409.08928</link>
      <description>arXiv:2409.08928v5 Announce Type: replace-cross 
Abstract: We consider the problem of performing parameter and state inference in a state-space model (SSM) parametrized by a static parameter $\theta$. A popular idea to address this problem consists of incorporating $\theta$ in the state of the system and allowing its time evolution, modelled as a Markov chain $(\theta_t)_{t\geq 1}$. This proxy model defines a so-called self-organizing SSM (SO-SSM) to which one may apply standard particle filters. However, the practical implementation of this idea in a theoretically justified manner has remained an open problem until now. In this paper we fill this gap and in particular show that theoretically consistent SO-SSMs can be defined such that $\|\mathrm{Var}(\theta_{t+1}|\theta_{t})\|\rightarrow 0$ slowly as $t\rightarrow\infty$. This, in turn, leads to particle filter algorithms for online inference in SSMs which we find to be robust in simulation. We also develop constructions of $(\theta_t)_{t\geq 1}$ and associated theoretical guarantees tailored to the application of SO-SSMs to maximum likelihood estimation in SSMs, leading to novel iterated filtering algorithms. The algorithms developed in this work have the advantage of being simple to implement and to require minimal tuning to perform well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08928v5</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Chen, Mathieu Gerber, Christophe Andrieu, Randal Douc</dc:creator>
    </item>
    <item>
      <title>On the Selection Stability of Stability Selection and Its Applications</title>
      <link>https://arxiv.org/abs/2411.09097</link>
      <description>arXiv:2411.09097v3 Announce Type: replace-cross 
Abstract: Stability selection is a widely adopted resampling-based framework for high-dimensional variable selection. This paper seeks to broaden the use of an established stability estimator to evaluate the overall stability of the stability selection results, moving beyond single-variable analysis. We suggest that the stability estimator offers two advantages: it can serve as a reference to reflect the robustness of the results obtained, and it can help identify a Pareto optimal regularization value to improve stability. By determining the regularization value, we calibrate key stability selection parameters, namely, the decision-making threshold and the expected number of falsely selected variables, within established theoretical bounds. In addition, the convergence of stability values over successive sub-samples sheds light on the required number of sub-samples addressing a notable gap in prior studies. The \texttt{stabplot} R package is developed to facilitate the use of the methodology featured in this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09097v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Nouraie, Samuel Muller</dc:creator>
    </item>
    <item>
      <title>Unifying and extending Diffusion Models through PDEs for solving Inverse Problems</title>
      <link>https://arxiv.org/abs/2504.07437</link>
      <description>arXiv:2504.07437v2 Announce Type: replace-cross 
Abstract: Diffusion models have emerged as powerful generative tools with applications in computer vision and scientific machine learning (SciML), where they have been used to solve large-scale probabilistic inverse problems. Traditionally, these models have been derived using principles of variational inference, denoising, statistical signal processing, and stochastic differential equations. In contrast to the conventional presentation, in this study we derive diffusion models using ideas from linear partial differential equations and demonstrate that this approach has several benefits that include a constructive derivation of the forward and reverse processes, a unified derivation of multiple formulations and sampling strategies, and the discovery of a new class of variance preserving models. We also apply the conditional version of these models to solve canonical conditional density estimation problems and challenging inverse problems. These problems help establish benchmarks for systematically quantifying the performance of different formulations and sampling strategies in this study and for future studies. Finally, we identify and implement a mechanism through which a single diffusion model can be applied to measurements obtained from multiple measurement operators. Taken together, the contents of this manuscript provide a new understanding of and several new directions in the application of diffusion models to solving physics-based inverse problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07437v2</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agnimitra Dasgupta, Alexsander Marciano da Cunha, Ali Fardisi, Mehrnegar Aminy, Brianna Binder, Bryan Shaddy, Assad A Oberai</dc:creator>
    </item>
  </channel>
</rss>
