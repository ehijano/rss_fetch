<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SC</link>
    <description>cs.SC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Feb 2026 04:28:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Computing braids from approximate data</title>
      <link>https://arxiv.org/abs/2601.23073</link>
      <description>arXiv:2601.23073v1 Announce Type: cross 
Abstract: We study the theoretical and practical aspects of computing braids described by approximate descriptions of paths in the plane. Exact algorithms rely on the lexicographic ordering of the points in the plane, which is unstable under numerical uncertainty. Instead, we formalize an input model for approximate data, based on a separation predicate. It applies, for example, to paths obtained by tracking the roots of a parametrized polynomial with complex coefficients, thereby connecting certified path tracking outputs to exact braid computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.23073v1</guid>
      <category>cs.CG</category>
      <category>cs.SC</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alexandre Guillemot, Pierre Lairez</dc:creator>
    </item>
    <item>
      <title>Names Don't Matter: Symbol-Invariant Transformer for Open-Vocabulary Learning</title>
      <link>https://arxiv.org/abs/2601.23169</link>
      <description>arXiv:2601.23169v1 Announce Type: cross 
Abstract: Current neural architectures lack a principled way to handle interchangeable tokens, i.e., symbols that are semantically equivalent yet distinguishable, such as bound variables. As a result, models trained on fixed vocabularies often struggle to generalize to unseen symbols, even when the underlying semantics remain unchanged. We propose a novel Transformer-based mechanism that is provably invariant to the renaming of interchangeable tokens. Our approach employs parallel embedding streams to isolate the contribution of each interchangeable token in the input, combined with an aggregated attention mechanism that enables structured information sharing across streams. Experimental results confirm the theoretical guarantees of our method and demonstrate substantial performance gains on open-vocabulary tasks that require generalization to novel symbols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.23169v1</guid>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <category>cs.SC</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>\.Ilker I\c{s}{\i}k, Wenchao Li</dc:creator>
    </item>
  </channel>
</rss>
