<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Oct 2024 02:00:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Analog fast Fourier transforms for scalable and efficient signal processing</title>
      <link>https://arxiv.org/abs/2409.19071</link>
      <description>arXiv:2409.19071v1 Announce Type: new 
Abstract: Edge devices are being deployed at increasing volumes to sense and act on information from the physical world. The discrete Fourier transform (DFT) is often necessary to make this sensed data suitable for further processing $\unicode{x2013}$ such as by artificial intelligence (AI) algorithms $\unicode{x2013}$ and for transmission over communication networks. Analog in-memory computing has been shown to be a fast and energy-efficient solution for processing edge AI workloads, but not for Fourier transforms. This is because of the existence of the fast Fourier transform (FFT) algorithm, which enormously reduces the complexity of the DFT but has so far belonged only to digital processors. Here, we show that the FFT can be mapped to analog in-memory computing systems, enabling them to efficiently scale to arbitrarily large Fourier transforms without requiring large sizes or large numbers of non-volatile memory arrays. We experimentally demonstrate analog FFTs on 1D audio and 2D image signals, using a large-scale charge-trapping memory array with precisely tunable, low-conductance analog states. The scalability of both the new analog FFT approach and the charge-trapping memory device is leveraged to compute a 65,536-point analog DFT, a scale that is otherwise inaccessible by analog systems and which is $&gt;$1000$\times$ larger than any previous analog DFT demonstration. The analog FFT also provides more numerically precise DFTs with greater tolerance to device and circuit non-idealities than a direct matrix-vector multiplication approach. We show that the extension of the FFT algorithm to analog in-memory processors leads to design considerations that differ markedly from digital implementations, and that analog Fourier transforms have a substantial power efficiency advantage at all size scales over FFTs implemented on state-of-the-art digital hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19071v1</guid>
      <category>cs.ET</category>
      <category>eess.SP</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>T. Patrick Xiao, Ben Feinberg, David K. Richardson, Matthew Cannon, Harsha Medu, Vineet Agrawal, Matthew J. Marinella, Sapan Agarwal, Christopher H. Bennett</dc:creator>
    </item>
    <item>
      <title>Nonideality-aware training makes memristive networks more robust to adversarial attacks</title>
      <link>https://arxiv.org/abs/2409.19671</link>
      <description>arXiv:2409.19671v1 Announce Type: new 
Abstract: Neural networks are now deployed in a wide number of areas from object classification to natural language systems. Implementations using analog devices like memristors promise better power efficiency, potentially bringing these applications to a greater number of environments. However, such systems suffer from more frequent device faults and overall, their exposure to adversarial attacks has not been studied extensively. In this work, we investigate how nonideality-aware training - a common technique to deal with physical nonidealities - affects adversarial robustness. We find that adversarial robustness is significantly improved, even with limited knowledge of what nonidealities will be encountered during test time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19671v1</guid>
      <category>cs.ET</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dovydas Joksas, Luis Mu\~noz-Gonz\'alez, Emil Lupu, Adnan Mehonic</dc:creator>
    </item>
    <item>
      <title>Responsible AI in Open Ecosystems: Reconciling Innovation with Risk Assessment and Disclosure</title>
      <link>https://arxiv.org/abs/2409.19104</link>
      <description>arXiv:2409.19104v1 Announce Type: cross 
Abstract: The rapid scaling of AI has spurred a growing emphasis on ethical considerations in both development and practice. This has led to the formulation of increasingly sophisticated model auditing and reporting requirements, as well as governance frameworks to mitigate potential risks to individuals and society. At this critical juncture, we review the practical challenges of promoting responsible AI and transparency in informal sectors like OSS that support vital infrastructure and see widespread use. We focus on how model performance evaluation may inform or inhibit probing of model limitations, biases, and other risks. Our controlled analysis of 7903 Hugging Face projects found that risk documentation is strongly associated with evaluation practices. Yet, submissions (N=789) from the platform's most popular competitive leaderboard showed less accountability among high performers. Our findings can inform AI providers and legal scholars in designing interventions and policies that preserve open-source innovation while incentivizing ethical uptake.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19104v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.SE</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mahasweta Chakraborti, Bert Joseph Prestoza, Nicholas Vincent, Seth Frey</dc:creator>
    </item>
    <item>
      <title>Sharing-Based Channel Access Procedure For Next Generation of Wireless LAN</title>
      <link>https://arxiv.org/abs/2409.19219</link>
      <description>arXiv:2409.19219v1 Announce Type: cross 
Abstract: This paper proposes a new channel access procedure to mitigate the channel access contention in next generation of Wireless Local-Area Networks (WLANs) by allowing cooperation among devices belonging to same network, while maintaining high flexibility in terms of how each device may contend the medium. After introducing the details of the proposed procedure, which is here referred to as sharing-based protocol, an analytical analysis is provided to compare it with the two state-of-art protocols currently adopted in IEEE 802.11 standard, i.e, Enhanced Distributed Channel Access (EDCA)-based and trigger-based protocol. In this regards, closed form expressions are derived to evaluate the success probability of channel access for each protocol. In order to show the merit of the proposed procedure, a comprehensive system level analysis is also provided, which highlights that the proposed procedure outperforms the two state-of-art protocols in terms of mitigating the End-to-End (E2E) delay and allowing a better spectrum utilization by reducing the overall congestion in the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19219v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qing Xia, Salvatore Talarico</dc:creator>
    </item>
    <item>
      <title>Gesture Recognition for Feedback Based Mixed Reality and Robotic Fabrication: A Case Study of the UnLog Tower</title>
      <link>https://arxiv.org/abs/2409.19281</link>
      <description>arXiv:2409.19281v1 Announce Type: cross 
Abstract: Mixed Reality (MR) platforms enable users to interact with three-dimensional holographic instructions during the assembly and fabrication of highly custom and parametric architectural constructions without the necessity of two-dimensional drawings. Previous MR fabrication projects have primarily relied on digital menus and custom buttons as the interface for user interaction with the MR environment. Despite this approach being widely adopted, it is limited in its ability to allow for direct human interaction with physical objects to modify fabrication instructions within the MR environment. This research integrates user interactions with physical objects through real-time gesture recognition as input to modify, update or generate new digital information enabling reciprocal stimuli between the physical and the virtual environment. Consequently, the digital environment is generative of the user's provided interaction with physical objects to allow seamless feedback in the fabrication process. This research investigates gesture recognition for feedback-based MR workflows for robotic fabrication, human assembly, and quality control in the construction of the UnLog Tower.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19281v1</guid>
      <category>cs.HC</category>
      <category>cs.ET</category>
      <category>cs.RO</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-981-99-8405-3_28</arxiv:DOI>
      <dc:creator>Alexander Htet Kyaw, Lawson Spencer, Sasa Zivkovic, Leslie Lok</dc:creator>
    </item>
    <item>
      <title>Analog In-Memory Computing Attention Mechanism for Fast and Energy-Efficient Large Language Models</title>
      <link>https://arxiv.org/abs/2409.19315</link>
      <description>arXiv:2409.19315v1 Announce Type: cross 
Abstract: Transformer neural networks, driven by self-attention mechanisms, are core components of foundational and Large Language Models. In generative transformers, self-attention uses cache memory to store token projections, avoiding recomputation at each time step. However, GPU-stored projections must be loaded into SRAM for each new generation step, causing latency and energy bottlenecks for long sequences. In this work, we propose a fast and energy-efficient hardware implementation of self-attention using analog in-memory computing based on gain cell memories. Volatile gain cell memories can be efficiently written to store new tokens during sequence generation, while performing analog signed weight multiplications to compute the dot-products required for self-attention. We implement Sliding Window Attention, which keeps memory of a finite set of past steps. A charge-to-pulse converter for array readout eliminates the need for analog-to-digital conversion between self-attention stages. Using a co-designed initialization algorithm to adapt pre-trained weights to gain cell non-idealities, we achieve NLP performance comparable to ChatGPT-2 with minimal training iterations, despite hardware constraints. Our end-to-end hardware design includes digital controls, estimating area, latency, and energy. The system reduces attention latency by up to two orders of magnitude and energy consumption by up to five orders compared to GPUs, marking a significant step toward ultra-fast, low-power sequence generation in Large Language Models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19315v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Leroux, Paul-Philipp Manea, Chirag Sudarshan, Jan Finkbeiner, Sebastian Siegel, John Paul Strachan, Emre Neftci</dc:creator>
    </item>
    <item>
      <title>Reprogrammable, in-materia matrix-vector multiplication with floppy modes</title>
      <link>https://arxiv.org/abs/2409.20425</link>
      <description>arXiv:2409.20425v1 Announce Type: cross 
Abstract: Matrix-vector multiplications are a fundamental building block of artificial intelligence; this essential role has motivated their implementation in a variety of physical substrates, from memristor crossbar arrays to photonic integrated circuits. Yet their realization in soft-matter intelligent systems remains elusive. Here, we experimentally demonstrate a reprogrammable elastic metamaterial that computes matrix-vector multiplications using floppy modes -- deformations with near-zero stored elastic energy. Floppy modes allow us to program complex deformations without being hindered by the natural stiffness of the material; but their practical application is challenging, as their existence depends on global topological properties of the system. To overcome this challenge, we introduce a continuously parameterized unit cell design with well-defined compatibility characteristics. This unit cell is then combined to form arbitrary matrix-vector multiplications that can even be reprogrammed after fabrication. Our results demonstrate that floppy modes can act as key enablers for embodied intelligence, smart MEMS devices and in-sensor edge computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20425v1</guid>
      <category>cond-mat.soft</category>
      <category>cs.ET</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Theophile Louvet, Parisa Omidvar, Marc Serra-Garcia</dc:creator>
    </item>
    <item>
      <title>Gumbel-Softmax Discretization Constraint, Differentiable IDS Channel, and an IDS-Correcting Code for DNA Storage</title>
      <link>https://arxiv.org/abs/2407.18929</link>
      <description>arXiv:2407.18929v2 Announce Type: replace-cross 
Abstract: Insertion, deletion, and substitution (IDS) error-correcting codes have garnered increased attention with recent advancements in DNA storage technology. However, a universal method for designing IDS-correcting codes across varying channel settings remains underexplored. We present an autoencoder-based method, THEA-code, aimed at efficiently generating IDS-correcting codes for complex IDS channels. In the work, a Gumbel-Softmax discretization constraint is proposed to discretize the features of the autoencoder, and a simulated differentiable IDS channel is developed as a differentiable alternative for IDS operations. These innovations facilitate the successful convergence of the autoencoder, resulting in channel-customized IDS-correcting codes with commendable performance across complex IDS channels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18929v2</guid>
      <category>cs.IT</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Tue, 01 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alan J. X. Guo, Mengyi Wei, Yufan Dai, Yali Wei, Pengchen Zhang</dc:creator>
    </item>
  </channel>
</rss>
