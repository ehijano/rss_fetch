<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.med-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.med-ph</link>
    <description>physics.med-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.med-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Dec 2024 03:53:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Motion-Guided Deep Image Prior for Cardiac MRI</title>
      <link>https://arxiv.org/abs/2412.04639</link>
      <description>arXiv:2412.04639v1 Announce Type: new 
Abstract: Cardiovascular magnetic resonance imaging is a powerful diagnostic tool for assessing cardiac structure and function. Traditional breath-held imaging protocols, however, pose challenges for patients with arrhythmias or limited breath-holding capacity. We introduce Motion-Guided Deep Image prior (M-DIP), a novel unsupervised reconstruction framework for accelerated real-time cardiac MRI. M-DIP employs a spatial dictionary to synthesize a time-dependent template image, which is further refined using time-dependent deformation fields that model cardiac and respiratory motion. Unlike prior DIP-based methods, M-DIP simultaneously captures physiological motion and frame-to-frame content variations, making it applicable to a wide range of dynamic applications. We validate M-DIP using simulated MRXCAT cine phantom data as well as free-breathing real-time cine and single-shot late gadolinium enhancement data from clinical patients. Comparative analyses against state-of-the-art supervised and unsupervised approaches demonstrate M-DIP's performance and versatility. M-DIP achieved better image quality metrics on phantom data, as well as higher reader scores for in-vivo patient data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04639v1</guid>
      <category>physics.med-ph</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Vornehm, Chong Chen, Muhammad Ahmad Sultan, Syed Murtaza Arshad, Yuchi Han, Florian Knoll, Rizwan Ahmad</dc:creator>
    </item>
    <item>
      <title>Exact Inversion from Space-filling Trajectories in Cone-beam Transmission Tomography</title>
      <link>https://arxiv.org/abs/2412.04669</link>
      <description>arXiv:2412.04669v1 Announce Type: new 
Abstract: This article introduces a new theory of exact inversion in cone-beam transmission tomography where the source point locus is a 2D surface, 3D volume, or something more complex.
  We specialise the theory to the case of the cylinder-shaped source locus, and we describe in detail a functioning practical implementation of the inversion algorithm for this geometry. This locus is accessible to CT scanners which include translational and rotational manipulators (e.g. typical helical scanners). This serves as a concrete instantiation of the theory and as a reference implementation which is immediately applicable to many contemporary scanning apparatuses.
  We illustrate some error characteristics of the algorithm through reconstructions of a simulated dataset. We finish with a reconstruction performed on a set of experimental data acquired with a low-pitch sparsely sampled helical trajectory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04669v1</guid>
      <category>physics.med-ph</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Murdock Grewar, Glenn Myers, Andrew Kingston</dc:creator>
    </item>
    <item>
      <title>Comparison of Deep Learning and Particle Smoother Expectation Maximization Methods for Estimation of Myocardial Perfusion PET Kinetic Parameters</title>
      <link>https://arxiv.org/abs/2412.04706</link>
      <description>arXiv:2412.04706v1 Announce Type: new 
Abstract: Background: Positron emission tomography (PET) is widely used for studying dynamic processes, such as myocardial perfusion, by acquiring data over time frames. Kinetic modeling in PET allows for the estimation of physiological parameters, offering insights into disease characterization. Conventional approaches have notable limitations; for example, graphical methods may reduce accuracy due to linearization, while non-linear least squares (NLLS) methods may converge to local minima. Purpose: This study aims to develop and validate two novel methods for PET kinetic analysis of 82Rb: a particle smoother-based algorithm within an Expectation-Maximization (EM) framework and a convolutional neural network (CNN) approach. Methods: The proposed methods were applied to simulated 82Rb dynamic PET myocardial perfusion studies. Their performance was compared to conventional NLLS methods and a Kalman filter-based Expectation-Maximization (KEM) algorithm. Results: The success rates for parameters F, k3, and k4 were 46.0%, 67.5%, and 54.0% for the particle smoother with EM (PSEM) and 86.5%, 83.0%, and 79.5% for the CNN model, respectively, outperforming the NLLS method. Conclusions: The CNN and PSEM methods showed promising improvements over traditional methods in estimating kinetic parameters in dynamic PET studies, suggesting their potential for enhanced accuracy in disease characterization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04706v1</guid>
      <category>physics.med-ph</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Myungheon Chin, Sarah J Zou, Garry Chinn, Craig S. Levin</dc:creator>
    </item>
    <item>
      <title>Using data collected from structured light plethysmography to differentiate breathing pattern disorder from normal breathing: A study group report</title>
      <link>https://arxiv.org/abs/2412.05141</link>
      <description>arXiv:2412.05141v1 Announce Type: new 
Abstract: This report relates to a study group hosted by the EPSRC funded network, Integrating data-driven BIOphysical models into REspiratory MEdicine (BIOREME), and supported by SofTMech and Innovate UK, Business Connect. This report summarises the work undertaken on a challenge presented by two of the authors, Mathew Bulpett and Dr Emily Fraser. The aim was to identify approaches to analyse data collected using structured light plethysmography (SLP) from (n=31) healthy volunteers and (n=67) patients with Breathing Pattern Disorder (BPD) attributed to "long COVID", i.e. post-acute COVID-19 sequelae. This report explores several approaches including dimensionality reduction techniques on the available data and alternative indices extracted from variation in the time-series data for each measurement. Further proposals are also outlined such as different spatial indices that could be extracted from the SLP data, and the potential to couple to mechanical models of the lungs, chest and abdomen. However, running these latter analyses was beyond the scope of the limited study group timeframe.
  This exploratory analysis did not identify any clear SLP biomarkers of BPD in these cohorts, however recommendations are made for using SLP technologies in future BPD studies based on its findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05141v1</guid>
      <category>physics.med-ph</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bindi S. Brook, Mathew Bulpett, Robin Curnow, Emily Fraser, Eric J. Hall, Shiting Huang, Mariam Mubarak, Carl A. Whitfield</dc:creator>
    </item>
    <item>
      <title>Automatic Tissue Differentiation in Parotidectomy using Hyperspectral Imaging</title>
      <link>https://arxiv.org/abs/2412.04879</link>
      <description>arXiv:2412.04879v1 Announce Type: cross 
Abstract: In head and neck surgery, continuous intraoperative tissue differentiation is of great importance to avoid injury to sensitive structures such as nerves and vessels. Hyperspectral imaging (HSI) with neural network analysis could support the surgeon in tissue differentiation. A 3D Convolutional Neural Network with hyperspectral data in the range of $400-1000$ nm is used in this work. The acquisition system consisted of two multispectral snapshot cameras creating a stereo-HSI-system. For the analysis, 27 images with annotations of glandular tissue, nerve, muscle, skin and vein in 18 patients undergoing parotidectomy are included. Three patients are removed for evaluation following the leave-one-subject-out principle. The remaining images are used for training, with the data randomly divided into a training group and a validation group. In the validation, an overall accuracy of $98.7\%$ is achieved, indicating robust training. In the evaluation on the excluded patients, an overall accuracy of $83.4\%$ has been achieved showing good detection and identification abilities. The results clearly show that it is possible to achieve robust intraoperative tissue differentiation using hyperspectral imaging. Especially the high sensitivity in parotid or nerve tissue is of clinical importance. It is interesting to note that vein was often confused with muscle. This requires further analysis and shows that a very good and comprehensive data basis is essential. This is a major challenge, especially in surgery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04879v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Eric L. Wisotzky, Alexander Schill, Anna Hilsmann, Peter Eisert, Michael Knoke</dc:creator>
    </item>
    <item>
      <title>Kinetic-Diffusion-Rotation Algorithm for Dose Estimation in Radiation Therapy</title>
      <link>https://arxiv.org/abs/2412.05063</link>
      <description>arXiv:2412.05063v1 Announce Type: cross 
Abstract: Monte Carlo methods are state-of-the-art when it comes to dosimetric computations in radiotherapy. However, the execution time of these methods suffers in high-collisional regimes. We address this problem by introducing a kinetic-diffusion particle tracing scheme. This algorithm, first proposed in the context of neutral transport in fusion energy, relies on explicit simulation of the kinetic motion in low-collisional regimes and dynamically switches to motion based on a random walk in high-collisional regimes. The random walk motion maintains the first two moments (mean and variance) of the kinetic motion. We derive an analytic formula for the mean kinetic motion and discuss the addition of a multiple scattering distribution to the algorithm. In contrast to neutral transport, the radiation transfer setting does not readily admit to an analytical expression for the variance of the kinetic motion, and we therefore resort to the use of a lookup table. We test the algorithm for dosimetric computations in radiation therapy on a 2D CT scan of a lung patient. Using a simple particle model, our Python implementation of the algorithm is nearly 33 times faster than an equivalent kinetic simulation at the cost of a small modeling error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05063v1</guid>
      <category>q-bio.QM</category>
      <category>physics.med-ph</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Klaas Willems, Vince Maes, Zhirui Tang, Giovanni Samaey</dc:creator>
    </item>
    <item>
      <title>Reconstructing Quantitative Cerebral Perfusion Images Directly From Measured Sinogram Data Acquired Using C-arm Cone-Beam CT</title>
      <link>https://arxiv.org/abs/2412.05084</link>
      <description>arXiv:2412.05084v1 Announce Type: cross 
Abstract: To shorten the door-to-puncture time for better treating patients with acute ischemic stroke, it is highly desired to obtain quantitative cerebral perfusion images using C-arm cone-beam computed tomography (CBCT) equipped in the interventional suite. However, limited by the slow gantry rotation speed, the temporal resolution and temporal sampling density of typical C-arm CBCT are much poorer than those of multi-detector-row CT in the diagnostic imaging suite. The current quantitative perfusion imaging includes two cascaded steps: time-resolved image reconstruction and perfusion parametric estimation. For time-resolved image reconstruction, the technical challenge imposed by poor temporal resolution and poor sampling density causes inaccurate quantification of the temporal variation of cerebral artery and tissue attenuation values. For perfusion parametric estimation, it remains a technical challenge to appropriately design the handcrafted regularization for better solving the associated deconvolution problem. These two challenges together prevent obtaining quantitatively accurate perfusion images using C-arm CBCT. The purpose of this work is to simultaneously address these two challenges by combining the two cascaded steps into a single joint optimization problem and reconstructing quantitative perfusion images directly from the measured sinogram data. In the developed direct cerebral perfusion parametric image reconstruction technique, TRAINER in short, the quantitative perfusion images have been represented as a subject-specific conditional generative model trained under the constraint of the time-resolved CT forward model, perfusion convolutional model, and the subject's own measured sinogram data. Results shown in this paper demonstrated that using TRAINER, quantitative cerebral perfusion images can be accurately obtained using C-arm CBCT in the interventional suite.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05084v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haotian Zhao, Ruifeng Chen, Jing Yan, Juan Feng, Jun Xiang, Yang Chen, Dong Liang, Yinsheng Li</dc:creator>
    </item>
    <item>
      <title>Effects of skull properties on long-pulsed transcranial focused ultrasound transmission</title>
      <link>https://arxiv.org/abs/2405.08489</link>
      <description>arXiv:2405.08489v4 Announce Type: replace 
Abstract: Transcranial low-intensity focused ultrasound can deliver energy to the brain in a minimally invasive manner for neuromodulation applications. However, long-pulsed sonication through the skull introduces significant wave interactions, complicating precise energy delivery to the target. We present a comprehensive examination of intracranial acoustic fields generated by focused ultrasound transducers and assess the characteristics of cranial bone that affect acoustic transmission. Acoustic field maps were generated at 88 regions of interest across 10 historical and 2 Thiel-embalmed human skull specimens with sonication at frequencies of 220 kHz, 650 kHz, and 1000 kHz. The average peak pressure insertion loss was 3.1$\pm$1.4 dB, 9.0$\pm$1.7 dB, and 14.6$\pm$4.1 dB, and the average power insertion loss was 5.0$\pm$2.4 dB, 14.9$\pm$3.2 dB, and 24.1$\pm$6.3 dB, respectively. The effect of skull thickness, skull density ratio, and skull curvature on intracranial peak pressure and power was investigated and linear fits produced. Our results demonstrate that Thiel-embalmed samples fall within the confidence intervals of fits for historical samples. The effects of angulation and spacing between the transducer and the skull were also investigated. Results indicate that wave superposition resulting from skull and transducer spacing could lead to a 30-40% uncertainty in peak recorded intracranial pressure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08489v4</guid>
      <category>physics.med-ph</category>
      <category>physics.bio-ph</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Han Li (School of Science and Engineering, University of Dundee, Dundee, UK, School of Physics, Engineering and Technology, University of York, UK), Isla Barnard (School of Science and Engineering, University of Dundee, Dundee, UK), Tyler Halliwell (School of Science and Engineering, University of Dundee, Dundee, UK), Xinyu Zhang (School of Medicine, University of Dundee, Dundee, UK), Andreas Melzer (School of Medicine, University of Dundee, Dundee, UK, Institute for Computer Assisted Surgery, University Leipzig, Germany), Zhihong Huang (School of Physics, Engineering and Technology, University of York, UK)</dc:creator>
    </item>
    <item>
      <title>Coherence Based Sound Speed Aberration Correction -- with clinical validation in fetal ultrasound</title>
      <link>https://arxiv.org/abs/2411.16551</link>
      <description>arXiv:2411.16551v2 Announce Type: replace-cross 
Abstract: The purpose of this work is to demonstrate a robust and clinically validated method for correcting sound speed aberrations in medical ultrasound. We propose a correction method that calculates focusing delays directly from the observed two-way distributed average sound speed. The method beamforms multiple coherence images and selects the sound speed that maximizes the coherence for each image pixel. The main contribution of this work is the direct estimation of aberration, without the ill-posed inversion of a local sound speed map, and the proposed processing of coherence images which adapts to in vivo situations where low coherent regions and off-axis scattering represents a challenge. The method is validated in vitro and in silico showing high correlation with ground truth speed of sound maps. Further, the method is clinically validated by being applied to channel data recorded from 172 obstetric Bmode images, and 12 case examples are presented and discussed in detail. The data is recorded with a GE HealthCare Voluson Expert 22 system with an eM6c matrix array probe. The images are evaluated by three expert clinicians, and the results show that the corrected images are preferred or gave equivalent quality to no correction (1540m/s) for 72.5% of the 172 images. In addition, a sharpness metric from digital photography is used to quantify image quality improvement. The increase in sharpness and the change in average sound speed are shown to be linearly correlated with a Pearson Correlation Coefficient of 0.67.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16551v2</guid>
      <category>eess.IV</category>
      <category>physics.med-ph</category>
      <pubDate>Mon, 09 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anders Emil Vr{\aa}lstad, Peter Fosodeder, Karin Ulrike Deibele, Siri Ann Nyrnes, Ole Marius Hoel Rindal, Vibeke Skoura-Torvik, Martin Mienkina, Svein-Erik M{\aa}s{\o}y</dc:creator>
    </item>
  </channel>
</rss>
