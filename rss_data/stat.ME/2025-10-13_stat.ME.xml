<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Oct 2025 04:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Observer-Based Source Localization in Tree Infection Networks via Laplace Transforms</title>
      <link>https://arxiv.org/abs/2510.09828</link>
      <description>arXiv:2510.09828v1 Announce Type: new 
Abstract: We address the problem of localizing the source of infection in an undirected, tree-structured network under a susceptible-infected outbreak model. The infection propagates with independent random time increments (i.e., edge-delays) between neighboring nodes, while only the infection times of a subset of nodes can be observed. We show that a reduced set of observers may be sufficient, in the statistical sense, to localize the source and characterize its identifiability via the joint Laplace transform of the observers' infection times. Using the explicit form of these transforms in terms of the edge-delay probability distributions, we propose scale-invariant least-squares estimators of the source. We evaluate their performance on synthetic trees and on a river network, demonstrating accurate localization under diverse edge-delay models. To conclude, we highlight overlooked technical challenges for observer-based source localization on networks with cycles, where standard spanning-tree reductions may be ill-posed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09828v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>math.PR</category>
      <category>q-bio.PE</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kesler O'Connor, Julia M. Jess, Devlin Costello, Manuel E. Lladser</dc:creator>
    </item>
    <item>
      <title>Bayesian Multivariable Bidirectional Mendelian Randomization</title>
      <link>https://arxiv.org/abs/2510.09991</link>
      <description>arXiv:2510.09991v1 Announce Type: new 
Abstract: Mendelian randomization (MR) is a pivotal tool in genetic epidemiology, leveraging genetic variants as instrumental variables to infer causal relationships between modifiable exposures and health outcomes. Traditional MR methods, while powerful, often rest on stringent assumptions such as the absence of feedback loops, which are frequently violated in complex biological systems. In addition, many popular MR approaches focus on only two variables (i.e., one exposure and one outcome) whereas our motivating applications have many variables. In this article, we introduce a novel Bayesian framework for \emph{multivariable} MR that concurrently addresses \emph{unmeasured confounding} and \emph{feedback loops}. Central to our approach is a sparse conditional cyclic graphical model with a sparse error variance-covariance matrix. Two structural priors are employed to enable the modeling and inference of causal relationships as well as latent confounding structures. Our method is designed to operate effectively with summary-level data, facilitating its application in contexts where individual-level data are inaccessible, e.g., due to privacy concerns. It can also account for horizontal pleiotropy. Through extensive simulations and applications to the GTEx and OneK1K data, we demonstrate the superior performance of our approach in recovering biologically plausible causal relationships in the presence of possible feedback loops and unmeasured confounding. The R package that implements the proposed method is available at \texttt{MR.RGM}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09991v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bitan Sarkar, Yuchao Jiang, Yang Ni</dc:creator>
    </item>
    <item>
      <title>Efficient Prior Sensitivity and Tipping-point Analysis for Medical Research: Revisiting Sampling Importance Resampling</title>
      <link>https://arxiv.org/abs/2510.10034</link>
      <description>arXiv:2510.10034v1 Announce Type: new 
Abstract: Bayesian methods have received increasing attention in medical research, where sensitivity analysis of prior distributions is essential. Such analyses typically require the evaluation of the posterior distribution of a parameter under multiple alternative prior settings. When the posterior distribution of the parameter of interest cannot be derived analytically, the standard approach is to re-fit the Markov chain Monte Carlo (MCMC) algorithm for each setting, which incurs substantial computational costs. This issue is particularly relevant in tipping-point analysis, in which the posterior must be evaluated across gradually changing degrees of borrowing. Sampling importance resampling (SIR) provides an efficient alternative by approximating posterior samples under new settings without MCMC re-fitting. However, to our knowledge , its utility has not been evaluated in scenarios involving repeated MCMC -- such as tipping-point analysis -- or in the application of complex Bayesian models. In this study, we re-evaluate the utility of SIR through two case studies: one involving tipping-point analysis under external data borrowing and another involving sensitivity analysis for a nonparametric Bayesian model in meta-analysis. These examples demonstrate that SIR can significantly reduce computational costs while maintaining a reasonable approximation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10034v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomohiro Ohigashi, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Calibration of answer probabilities in verbal autopsies: Working Paper</title>
      <link>https://arxiv.org/abs/2510.10065</link>
      <description>arXiv:2510.10065v1 Announce Type: new 
Abstract: In most of the world, causes of death are not recorded. Verbal autopsies are structured interviews with people close to the deceased, which are used to estimate the likelihood of various causes of death. Such estimates typically make use of a table of marginal probabilities, called a `probbase', describing the frequency of answers to each interview question conditional on each cause of death. Assembling probbase tables is challenging, since data labelled with verified causes-of-death are not typically available, and is generally done on the basis of expert opinion.
  We propose a method to verify or partially learn a probbase table given only a set of verbal autopsy questionnaires (i.e., unlabelled data). Essentially, we assess how well a probbase can be used to impute answers. Our method requires a mild conditional independence assumption on the joint distribution of questionnaire data and causes of death. More generally, our method serves as a means to assess verbal autopsy algorithms and parameters without the need for external cause-of-death labelling.
  We offer theoretical arguments to support our method, and some brief evaluations on data simulated to resemble realistic verbal autopsy questionnaires. We find moderate promise for the approach in this context, in that we may differentiate probbase values which are too high or too low with around 75% correctness using 1500 verbal autopsy questionnaires.
  This paper serves as an introduction to our approach and a statement of intent, in the spirit of preregistration. We identify a range of theoretical and practical open problems and describe a planned outline of work to evaluate the method. We invite comments and suggestions on our approach and open questions. We stress that our method has not yet been thoroughly tested and we do not endorse its use in a real-world setting at this stage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10065v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nathan Higgins, James Liley, Eilidh Cowan</dc:creator>
    </item>
    <item>
      <title>Generalized Jeffreys's approximate objective Bayes factor: Model-selection consistency, finite-sample accuracy, and statistical evidence in 71,126 clinical trial findings</title>
      <link>https://arxiv.org/abs/2510.10358</link>
      <description>arXiv:2510.10358v1 Announce Type: new 
Abstract: Concerns about the misuse and misinterpretation of p-values and statistical significance have motivated alternatives for quantifying evidence. We define a generalized form of Jeffreys's approximate objective Bayes factor (eJAB), a one-line calculation that is a function of the p-value, sample size, and parameter dimension. We establish conditions under which eJAB is model-selection consistent and verify them for ten statistical tests. We assess finite-sample accuracy by comparing eJAB with Markov chain Monte Carlo computed Bayes factors in 12 simulation studies. We then apply eJAB to 71,126 results from ClinicalTrials.gov (CTG) and find that the proportion of findings with $\text{p-value} \le \alpha$ yet $eJAB_{01}&gt;1$ (favoring the null) closely tracks the significance level $\alpha$, suggesting that such contradictions are pointing to the type I errors. We catalog 4,088 such candidate type I errors and provide details for 131 with reported $\text{p-value} \le 0.01$. We also identify 487 instances of the Jeffreys-Lindley paradox. Finally, we estimate that 75% (6%) of clinical trial plans from CTG set $\alpha \ge 0.05$ as the target evidence threshold, and that 35.5% (0.22%) of results significant at $\alpha =0.05$ correspond to evidence that is no stronger than anecdotal under eJAB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10358v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Puneet Velidi, Zhengxiao Wei, Shreena Nisha Kalaria, Yimeng Liu, C\'eline M. Laumont, Brad H. Nelson, Farouk S. Nathoo</dc:creator>
    </item>
    <item>
      <title>Multiply Robust Estimation of Conditional Survival Probability with Time-Varying Covariates</title>
      <link>https://arxiv.org/abs/2510.10372</link>
      <description>arXiv:2510.10372v1 Announce Type: new 
Abstract: It is often of interest to study the association between covariates and the incidence of a time-to-event outcome, but a common challenge is right-censoring and time-varying covariates measured on a fixed discrete time scale that may explain the censoring afterwards. For example, in vaccine trials, it is of interest to study the association between immune response levels after administering the vaccine and the incidence of the endpoint, but there is loss to follow-up or administrative censoring, and the immune response levels measured at multiple visits may be predictive of the censoring. Existing methods rely on stringent parametric assumptions, only estimate a marginal survival probability, or do not fully use the discrete-time structure of post-treatment covariates.
  In this paper, we propose a nonparametric estimator of the survival probability conditional on covariates with time-varying covariates. We show that the estimator is multiply robust: it is consistent if, within each time window between adjacent visits, at least one of the time-to-event distribution and the censoring distribution is consistently estimated. We demonstrate the superior performance of this estimator in a numerical simulation, and apply the method to a COVID-19 vaccine efficacy trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10372v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongxiang Qiu, Marco Carone, Alex Luedtke, Peter B. Gilbert</dc:creator>
    </item>
    <item>
      <title>Identification and Estimation of Heterogeneous Interference Effects under Unknown Network</title>
      <link>https://arxiv.org/abs/2510.10508</link>
      <description>arXiv:2510.10508v1 Announce Type: new 
Abstract: Interference--in which a unit's outcome is affected by the treatment of other units--poses significant challenges for the identification and estimation of causal effects. Most existing methods for estimating interference effects assume that the interference networks are known. In many practical settings, this assumption is unrealistic as such networks are typically latent. To address this challenge, we propose a novel framework for identifying and estimating heterogeneous group-level interference effects without requiring a known interference network. Specifically, we assume a shared latent community structure between the observed network and the unknown interference network. We demonstrate that interference effects are identifiable if and only if group-level interference effects are heterogeneous, and we establish the consistency and asymptotic normality of the maximum likelihood estimator (MLE). To handle the intractable likelihood function and facilitate the computation, we propose a Bayesian implementation and show that the posterior concentrates around the MLE. A series of simulation studies demonstrate the effectiveness of the proposed method and its superior performance compared with competitors. We apply our proposed framework to the encounter data of stroke patients from the California Department of Healthcare Access and Information (HCAI) and evaluate the causal interference effects of certain intervention in one hospital on the outcomes of other hospitals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10508v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuhua Zhang, Jukka-Pekka Onnela, Shuo Sun, Ruoyu Wang</dc:creator>
    </item>
    <item>
      <title>Robust Clustered Federated Learning for Heterogeneous High-dimensional Data</title>
      <link>https://arxiv.org/abs/2510.10576</link>
      <description>arXiv:2510.10576v1 Announce Type: new 
Abstract: Federated learning has attracted significant attention as a privacy-preserving framework for training personalised models on multi-source heterogeneous data. However, most existing approaches are unable to handle scenarios where subgroup structures coexist alongside within-group heterogeneity. In this paper, we propose a federated learning algorithm that addresses general heterogeneity through adaptive clustering. Specifically, our method partitions tasks into subgroups to address substantial between-group differences while enabling efficient information sharing among similar tasks within each group. Furthermore, we integrate the Huber loss and Iterative Hard Thresholding (IHT) to tackle the challenges of high dimensionality and heavy-tailed distributions. Theoretically, we establish convergence guarantees, derive non-asymptotic error bounds, and provide recovery guarantees for the latent cluster structure. Extensive simulation studies and real-data applications further demonstrate the effectiveness and adaptability of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10576v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changxin Yang, Zhongyi Zhu, Heng Lian</dc:creator>
    </item>
    <item>
      <title>Modelling Territorial Dynamics through Statistical Mechanics: An Application to Resident Foreign Population</title>
      <link>https://arxiv.org/abs/2510.10784</link>
      <description>arXiv:2510.10784v1 Announce Type: new 
Abstract: This study introduces a statistical mechanics framework to analyze the territorial distribution of the Resident Foreign Population across Italian municipalities. The observed percentages of foreign residents are treated as the reference configuration of an interacting system of territorial units. The socio-economic structure of municipalities is summarized through interpretable composite indices and reduced via Principal Components Analysis to construct a univariate external field compatible with the modelling framework. Two complementary approaches are explored: a continuous variant of the Ising model and a stochastic-differential alternative known as Langevin dynamics which are both simulated by adopting Monte Carlo approaches with Simulated Annealing, a stochastic optimization strategy commonly used for exploring local energy minima. Both methods enable efficient local exploration of the configuration space in the neighborhood of the observed state, facilitating the discovery of socio-economic determinants that shape the spatial distribution of resident foreign population. Model adequacy is assessed in terms of energy stability, likelihood, and predictive accuracy. Uncertainty is quantified using model-agnostic Conformal Prediction, which yields adaptive prediction intervals with guaranteed marginal coverage. Mapping the amplitude of these intervals highlights areas of higher uncertainty and supports a nuanced interpretation of territorial socio-economic dynamics. By combining statistical mechanics, multivariate analysis, and uncertainty quantification, the study provides a robust and interpretable general method for modelling complex territorial patterns, with direct relevance for Official Statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10784v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierpaolo Massoli</dc:creator>
    </item>
    <item>
      <title>On function-on-function linear quantile regression</title>
      <link>https://arxiv.org/abs/2510.10792</link>
      <description>arXiv:2510.10792v1 Announce Type: new 
Abstract: We present two innovative functional partial quantile regression algorithms designed to accurately and efficiently estimate the regression coefficient function within the function-on-function linear quantile regression model. Our algorithms utilize functional partial quantile regression decomposition to effectively project the infinite-dimensional response and predictor variables onto a finite-dimensional space. Within this framework, the partial quantile regression components are approximated using a basis expansion approach. Consequently, we approximate the infinite-dimensional function-on-function linear quantile regression model using a multivariate quantile regression model constructed from these partial quantile regression components. To evaluate the efficacy of our proposed techniques, we conduct a series of Monte Carlo experiments and analyze an empirical dataset, demonstrating superior performance compared to existing methods in finite-sample scenarios. Our techniques have been implemented in the ffpqr package in R.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10792v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Muge Mutis, Ufuk Beyaztas, Filiz Karaman, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Small Area Estimation of General Indicators in Off-Census Years</title>
      <link>https://arxiv.org/abs/2510.10812</link>
      <description>arXiv:2510.10812v1 Announce Type: new 
Abstract: We propose small area estimators of general indicators in off-census years, which avoid the use of deprecated census microdata, but are nearly optimal in census years. The procedure is based on replacing the obsolete census file with a larger unit-level survey that adequately covers the areas of interest and contains the values of useful auxiliary variables. However, the minimal data requirement of the proposed method is a single survey with microdata on the target variable and suitable auxiliary variables for the period of interest. We also develop an estimator of the mean squared error (MSE) that accounts for the uncertainty introduced by the large survey used to replace the census of auxiliary information. Our empirical results indicate that the proposed predictors perform clearly better than the alternative predictors when census data are outdated, and are very close to optimal ones when census data are correct. They also illustrate that the proposed total MSE estimator corrects for the bias of purely model-based MSE estimators that do not account for the large survey uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10812v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Acero, Isabel Molina, J. Miguel Mar\'in</dc:creator>
    </item>
    <item>
      <title>Distribution-Free Prediction Sets for Regression under Target Shift</title>
      <link>https://arxiv.org/abs/2510.10985</link>
      <description>arXiv:2510.10985v1 Announce Type: new 
Abstract: In real-world applications, the limited availability of labeled outcomes presents significant challenges for statistical inference due to high collection costs, technical barriers, and other constraints. In this work, we propose a method to construct efficient conformal prediction sets for new target outcomes by leveraging a source distribution that is distinct from the target but related through a distributional shift assumption and provides abundant labeled data. When the target data are fully unlabeled, our predictions rely solely on the source distribution, whereas partial target labels, when available, are integrated to improve efficiency. To address the challenges of data non-exchangeability and distribution non-identifiability, we identify the likelihood ratio by matching the covariate distributions of the source and target domains within a finite B-spline space. To accommodate complex error structures such as asymmetry and multimodality, our method constructs highest predictive density sets using a novel weight-adjusted conditional density estimator. This estimator models the source conditional density along a quantile process and transforms it, through appropriate weighting adjustments, to approximate the target conditional density. We establish the theoretical properties of the proposed method and evaluate its finite-sample performance through simulation studies and a real-data application to the MIMIC-III clinical database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10985v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Menghan Yi, Yanlin Tang, Huixia Judy Wang</dc:creator>
    </item>
    <item>
      <title>A Black-Box Debiasing Framework for Conditional Sampling</title>
      <link>https://arxiv.org/abs/2510.11071</link>
      <description>arXiv:2510.11071v1 Announce Type: new 
Abstract: Conditional sampling is a fundamental task in Bayesian statistics and generative modeling. Consider the problem of sampling from the posterior distribution $P_{X|Y=y^*}$ for some observation $y^*$, where the likelihood $P_{Y|X}$ is known, and we are given $n$ i.i.d. samples $D=\{X_i\}_{i=1}^n$ drawn from an unknown prior distribution $\pi_X$. Suppose that $f(\hat{\pi}_{X^n})$ is the distribution of a posterior sample generated by an algorithm (e.g. a conditional generative model or the Bayes rule) when $\hat{\pi}_{X^n}$ is the empirical distribution of the training data. Although averaging over the randomness of the training data $D$, we have $\mathbb{E}_D\left(\hat{\pi}_{X^n}\right)= \pi_X$, we do not have $\mathbb{E}_D\left\{f(\hat{\pi}_{X^n})\right\}= f(\pi_X)$ due to the nonlinearity of $f$, leading to a bias. In this paper we propose a black-box debiasing scheme that improves the accuracy of such a naive plug-in approach. For any integer $k$ and under boundedness of the likelihood and smoothness of $f$, we generate samples $\hat{X}^{(1)},\dots,\hat{X}^{(k)}$ and weights $w_1,\dots,w_k$ such that $\sum_{i=1}^kw_iP_{\hat{X}^{(i)}}$ is a $k$-th order approximation of $f(\pi_X)$, where the generation process treats $f$ as a black-box. Our generation process achieves higher accuracy when averaged over the randomness of the training data, without degrading the variance, which can be interpreted as improving memorization without compromising generalization in generative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11071v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Han Cui, Jingbo Liu</dc:creator>
    </item>
    <item>
      <title>Data Integration and spatio temporal statistics can quantify relative risk of medico-legal reforms: the example of police emergency mental health responses in Queensland (Australia)</title>
      <link>https://arxiv.org/abs/2510.11101</link>
      <description>arXiv:2510.11101v1 Announce Type: new 
Abstract: This study examined the spatial-temporal dynamics of Emergency Examination Order or Authority (EE-O/A) admissions in Far Northern Queensland (FNQ) from 2009 to 2020, using 13,035 unique police records aggregated across 83 postcodes. A two-stage modelling framework was used: Lasso was used to identify a parsimonious set of socio economic and health-service covariates, and a Conditional Autoregressive (CAR) model incorporated these predictors with structured spatial and temporal random effects. This research demonstrates that socio-economic disadvantage and service accessibility drive EE-O/A incidence, underscoring the need for targeted mental-health interventions and resource allocation in impoverished FNQ communities. Limitations include reliance on cross-sectional census data for covariates and potential ecological bias from data fusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11101v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nidup Dorji, Sourav Das, Richard Stone, Alan R. Clough</dc:creator>
    </item>
    <item>
      <title>Developing an information criterion for spatial data analysis through Bayesian generalized fused lasso</title>
      <link>https://arxiv.org/abs/2510.11172</link>
      <description>arXiv:2510.11172v1 Announce Type: new 
Abstract: In the field of spatial data analysis, spatially varying coefficients (SVC) models, which allow regression coefficients to vary by region and flexibly capture spatial heterogeneity, have continued to be developed in various directions. Moreover, the Bayesian generalized fused lasso is often used as a method that efficiently provides estimation under the natural assumption that regression coefficients of adjacent regions tend to take the same value. In most Bayesian methods, the selection of prior distribution is an essential issue, and in the setting of SVC model with the Bayesian generalized fused lasso, determining the complexity of the class of prior distributions is also a challenging aspect, further amplifying the difficulty of the problem. For example, the widely applicable information criterion (WAIC), which has become standard in Bayesian model selection, does not target determining the complexity. Therefore, in this study, we adapted a criterion called the prior intensified information criterion (PIIC) to this setting. Specifically, under an asymptotic setting that retains the influence of the prior distribution, that is, under an asymptotic setting that deliberately does not provide selection consistency, we derived the asymptotic properties of our generalized fused lasso estimator. Then, based on these properties, we constructed an information criterion as an asymptotically bias-corrected estimator of predictive risk. In numerical experiments, we confirmed that PIIC outperforms WAIC in the sense of reducing the predictive risk, and in a real data analysis, we observed that the two criteria give rise to substantially different results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11172v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuko Kakikawa, Yoshiyuki Ninomiya</dc:creator>
    </item>
    <item>
      <title>Semi-parametric Markov models for multi-type point patterns</title>
      <link>https://arxiv.org/abs/2510.11226</link>
      <description>arXiv:2510.11226v1 Announce Type: new 
Abstract: Multi-type Markov point processes offer a flexible framework for modelling complex multi-type point patterns where it is pertinent to capture both interactions between points as well as large scale trends depending on observed covariates. However, estimation of interaction and covariate effects may be seriously biased in the presence of unobserved spatial confounders. In this paper we introduce a new class of semi-parametric Markov point processes that adjusts for spatial confounding through a non-parametric factor that accommodates effects of latent spatial variables common to all types of points. We introduce a conditional pseudo likelihood for parameter estimation and show that the resulting estimator has desirable asymptotic properties. Our methodology not least has great potential in studies of industry agglomeration and we apply it to study spatial patterns of locations of two types of banks in France.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11226v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ib Thorsgaard Jensen, Jean-Fran\c{c}ois Coeurjolly, Rasmus Waagepetersen</dc:creator>
    </item>
    <item>
      <title>Directional replicability: when can the factor of two be omitted</title>
      <link>https://arxiv.org/abs/2510.11273</link>
      <description>arXiv:2510.11273v1 Announce Type: new 
Abstract: Directional replicability addresses the question of whether an effect studied across $n$ independent studies is present with the same direction in at least $r$ of them, for $r \geq 2$. When the expected direction of the effect is not specified in advance, the state of the art recommends assessing replicability separately by combining one-sided $p$-values for both directions (left and right), and then doubling the smaller of the two resulting combined $p$-values to account for multiple testing. In this work, we show that this multiplicative correction is not always necessary, and give conditions under which it can be safely omitted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11273v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vera Djordjilovi\'c, Tamar Sofer, Jonathan M. Dreyfuss</dc:creator>
    </item>
    <item>
      <title>Iterative Data Curation with Theoretical Guarantees</title>
      <link>https://arxiv.org/abs/2510.11428</link>
      <description>arXiv:2510.11428v1 Announce Type: new 
Abstract: In recent years, more and more large data sets have become available. Data accuracy, the absence of verifiable errors in data, is crucial for these large materials to enable high-quality research, downstream applications, and model training. This results in the problem of how to curate or improve data accuracy in such large and growing data, especially when the data is too large for manual curation to be feasible. This paper presents a unified procedure for iterative and continuous improvement of data sets. We provide theoretical guarantees that data accuracy tests speed up error reduction and, most importantly, that the proposed approach will, asymptotically, eliminate all errors in data with probability one. We corroborate the theoretical results with simulations and a real-world use case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11428v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>V\"ain\"o Yrj\"an\"ainen Johan Jonasson, M{\aa}ns Magnusson</dc:creator>
    </item>
    <item>
      <title>Algorithmic analysis of a complex reliability system subject to multiple events with a preventive maintenance strategy and a Bernoulli vacation policy through MMAPs</title>
      <link>https://arxiv.org/abs/2510.11506</link>
      <description>arXiv:2510.11506v1 Announce Type: new 
Abstract: In this work, a single-unit multi-state system is considered. The system is subject to internal failures, as well as external shocks with multiple consequences. It also incorporates a preventive maintenance strategy and a Bernoulli vacation policy for the repairperson. It is algorithmically modeled in both continuous and discrete time using Marked Markovian Arrival Processes (MMAP). The system's operation/degradation level is divided into an indeterminate number of levels. Upon returning from a vacation period, the repair technician may initiate corrective repair, perform preventive maintenance, replace the unit, remain idle at the workplace, or begin a new vacation period. The decision in the latter two cases is made probabilistically based on the system's operational level. This methodology allows the model and its associated measures to be algorithmically derived in both transient and stationary regimes, presented in a matrix-algorithmic form. Analytical-matrix methods are used to obtain the system's steady-state behaviour as well as various performance measures. Costs and rewards are introduced to analyze when the system becomes profitable. Measures associated with costs over time and in the stationary regime are defined and considered for optimization studies. A numerical example demonstrates the versatility of the model by solving a probabilistic optimization problem using a multi-objective Pareto analysis approach and performing a comparative evaluation of multiple models. Genetic algorithm is applied to find the optimization results in the reduced solution space. All modeling and associated measures have been computationally implemented in Matlab.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11506v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ress.2025.111744</arxiv:DOI>
      <arxiv:journal_reference>Reliability Engineering and System Safety, 266, 111744, 2026</arxiv:journal_reference>
      <dc:creator>Juan Eloy Ruiz-Castro, Hugo Ala\'in Zapata-Ceballos</dc:creator>
    </item>
    <item>
      <title>A Kolmogorov-Smirnov-Type Test for Dependently Double-Truncated Data</title>
      <link>https://arxiv.org/abs/2510.11517</link>
      <description>arXiv:2510.11517v1 Announce Type: new 
Abstract: With double-truncated lifespans, we test the hypothesis of a parametric distribution family for the lifespan. The typical finding from demography is an instationary behaviour of the life expectancy, and a copula models the resulting weak dependence of lifespan and the age at truncation. Our main example is the Farlie-Gumbel-Morgenststern copula. The test is based on Donsker-class arguments and the functional delta method for empirical processes. The assumptions also allow parametric inference, and proofs slightly simplify due to the compact support of the observations. An algorithm with finitely many operations is given for the computation of the test statistic. Simulations becomes necessary for computing the critical value. With the exponential distribution as an example, and for the application to 55{,}000 German double-truncated enterprise lifespans, the constructed Kolmogorov-Smirnov test rejects clearly an age-homogeneous closure hazard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11517v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anne-Marie Toparkus, Rafael Weissbach</dc:creator>
    </item>
    <item>
      <title>A comparison of approaches to incorporate patient-selected and patient-ranked outcomes in clinical trials</title>
      <link>https://arxiv.org/abs/2510.11578</link>
      <description>arXiv:2510.11578v1 Announce Type: new 
Abstract: A key aspect of patient-focused drug development is identifying and measuring outcomes that are important to patients in clinical trials. Many medical conditions affect multiple symptom domains, and a consensus approach to determine the relative importance of the associated multiple outcomes ignores the heterogeneity in individual patient preferences. Patient-selected outcomes offer one way to incorporate individual patient preferences, as proposed in recent regulatory guidance for the treatment for migraine, where each patient selects their most bothersome migraine-associated symptom in addition to pain. Patient-ranked outcomes have also recently been proposed, which go further and consider the full ranking of the relative importance of all the outcomes. This can be assessed using a composite DOOR (Desirability of Outcome Ranking) endpoint. In this paper, we compare the advantages and disadvantages of using patient-selected versus patient-ranked outcomes in the context of a two-arm randomised controlled trial for multiple sclerosis. We compare the power and type I error rate by simulation, and discuss several other important considerations when using the two approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11578v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David S. Robertson, Thomas Jaki</dc:creator>
    </item>
    <item>
      <title>An optimal two-step estimation approach for two-phase studies</title>
      <link>https://arxiv.org/abs/2510.11587</link>
      <description>arXiv:2510.11587v1 Announce Type: new 
Abstract: Two-phase sampling is commonly adopted for reducing cost and improving estimation efficiency. In many two-phase studies, the outcome and some cheap covariates are observed for a large sample in Phase I, and expensive covariates are obtained for a selected subset of the sample in Phase II. As a result, the analysis of the association between the outcome and covariates faces a missing data problem. Complete-case analysis, which relies solely on the Phase II sample, is generally inefficient. In this paper, we study a two-step estimation approach, which first obtains an estimator using the complete data, and then updates it using an asymptotically mean-zero estimator obtained from a working model between the outcome and cheap covariates using the full data. This two-step estimator is asymptotically at least as efficient as the complete-data estimator and is robust to misspecification of the working model. We propose a kernel-based method to construct a two-step estimator that achieves optimal efficiency. Additionally, we develop a simple joint update approach based on multiple working models to approximate the optimal estimator when a fully nonparametric kernel approach is infeasible. We illustrate the proposed methods with various outcome models. We demonstrate their advantages over existing approaches through simulation studies and provide an application to a major cancer genomics study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11587v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qingning Zhou, Kin Yau Wong</dc:creator>
    </item>
    <item>
      <title>The Generalized Word Count in Two-Level Fractional Factorial Designs</title>
      <link>https://arxiv.org/abs/2510.11609</link>
      <description>arXiv:2510.11609v1 Announce Type: new 
Abstract: Unreplicated two-level factorial designs are often used in screening experiments to determine which factors out of a large plausible set are active. A theorem regarding the generalized word count pattern is stated and proved for unreplicated designs. It is shown that a phenomenon regarding optimal designs seen in the recent literature can be explained by the theorem obtained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11609v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xietao Zhou, Steven G. Gilmour</dc:creator>
    </item>
    <item>
      <title>The Role of Congeniality in Multiple Imputation for Doubly Robust Causal Estimation</title>
      <link>https://arxiv.org/abs/2510.11633</link>
      <description>arXiv:2510.11633v1 Announce Type: new 
Abstract: This paper provides clear and practical guidance on the specification of imputation models when multiple imputation is used in conjunction with doubly robust estimation methods for causal inference. Through theoretical arguments and targeted simulations, we show that when a confounder has missing data the corresponding imputation model must include all variables used in either the propensity score model or the outcome model, and that these variables must appear in the same functional form as in the final analysis. Violating these conditions can lead to biased treatment effect estimates, even when both components of the doubly robust estimator are correctly specified. We present a mathematical framework for doubly robust estimation combined with multiple imputation, establish the theoretical requirements for proper imputation in this setting, and demonstrate the consequences of misspecification through simulation. Based on these findings, we offer concrete recommendations to ensure valid inference when using multiple imputation with doubly robust methods in applied causal analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11633v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucy D'Agostino McGowan</dc:creator>
    </item>
    <item>
      <title>Using LLMs to Directly Guess Conditional Expectations Can Improve Efficiency in Causal Estimation</title>
      <link>https://arxiv.org/abs/2510.09684</link>
      <description>arXiv:2510.09684v1 Announce Type: cross 
Abstract: We propose a simple yet effective use of LLM-powered AI tools to improve causal estimation. In double machine learning, the accuracy of causal estimates of the effect of a treatment on an outcome in the presence of a high-dimensional confounder depends on the performance of estimators of conditional expectation functions. We show that predictions made by generative models trained on historical data can be used to improve the performance of these estimators relative to approaches that solely rely on adjusting for embeddings extracted from these models. We argue that the historical knowledge and reasoning capacities associated with these generative models can help overcome curse-of-dimensionality problems in causal inference problems. We consider a case study using a small dataset of online jewelry auctions, and demonstrate that inclusion of LLM-generated guesses as predictors can improve efficiency in estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09684v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chris Engh, P. M. Aronow</dc:creator>
    </item>
    <item>
      <title>A Hybrid Agent-Based and System Dynamics Framework for Modelling Project Execution and Technology Maturity in Early-Stage R&amp;D</title>
      <link>https://arxiv.org/abs/2510.09688</link>
      <description>arXiv:2510.09688v1 Announce Type: cross 
Abstract: This paper presents a hybrid approach to predict the evolution of technological maturity in R and D projects, using the oil and gas sector as an example. Integrating System Dynamics (SD) and Agent Based Modelling (ABM) allows the proposed multi level framework to capture uncertainties in work effort, team size, and project duration, which influence technological progress. While AB SD hybrid models are established in other fields, their use in R and D remains limited. The model combines system level feedback structures governing work phases, rework cycles, and duration with decentralised agents such as team members, tasks, and controllers, whose interactions generate emergent project dynamics. A base case scenario analysed early stage innovation projects with 15 parallel tasks over 156 weeks. A comparative sequential scenario showed an 88 percent reduction in rework duration. A second scenario assessed mixed parallel sequential task structures with varying team sizes. In parallel configurations, increasing team size reduced project duration and improved task completion, with optimal results for teams of four to five members. These findings align with empirical evidence showing that moderate team expansion enhances coordination efficiency without excessive communication overhead. However, larger teams may decrease performance due to communication complexity and management delays. Overall, the model outputs and framework align with expert understanding, supporting their validity as quantitative tools for analysing resource allocation, scheduling efficiency, and technology maturity progression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09688v1</guid>
      <category>cs.MA</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R. W. S. Pessoa, M. H. N{\ae}ss, J. C. Bijos, C. M. Rebello, D. Colombo, L. Schnitman, I. B. R. Nogueira</dc:creator>
    </item>
    <item>
      <title>A Triad of Networks and a Triad of Fusions for the Other Climate Crisis</title>
      <link>https://arxiv.org/abs/2510.09728</link>
      <description>arXiv:2510.09728v1 Announce Type: cross 
Abstract: Shaw and Stevens call for a new paradigm in climate science criticizes Large Scale Determinism in favor of (i) embracing discrepancies, (ii) embracing hierarchies, and (iii) create disruption while keeping interpretability. The last 20 years have seen a plethora of contributions relating complex networks with climate data and climate models. We provide a view of climate networks through a triad of frameworks and associated paradigms: (a) networks of data, where both (geographical) nodes and their links (arcs) are determined according to some metrics and/or statistical criteria; (b) climate data over networks, where the structure of the network (for both vertices and edges) is topologically pre-determined, and the climate variable is continuously defined over the (nonlinear) network; finally, (c) networks for data, referring to the huge machinery based on networks within the realm machine learning and statistics, with specific emphasis on their use for climate data. This paper is not a mere description of each element of the network triad, but rather a manifesto for the creation of three classes of fusions (we term them bridges). We advocate and carefully justify a fusion within to provide a corpus unicuum inside the network triad. We then prove that the fusion within is the starting point for a fusion between, where the network triad becomes a condition sine qua non for the implementation of the Shaw-Stevens agenda. We culminate with a meta fusion that allows for the creation of what we term a Shaw-Stevens network ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09728v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Emilio Porcu, Tobia Filosi, Horst Simon</dc:creator>
    </item>
    <item>
      <title>Kernel Treatment Effects with Adaptively Collected Data</title>
      <link>https://arxiv.org/abs/2510.10245</link>
      <description>arXiv:2510.10245v1 Announce Type: cross 
Abstract: Adaptive experiments improve efficiency by adjusting treatment assignments based on past outcomes, but this adaptivity breaks the i.i.d. assumptions that underpins classical asymptotics. At the same time, many questions of interest are distributional, extending beyond average effects. Kernel treatment effects (KTE) provide a flexible framework by representing counterfactual outcome distributions in an RKHS and comparing them via kernel distances. We present the first kernel-based framework for distributional inference under adaptive data collection. Our method combines doubly robust scores with variance stabilization to ensure asymptotic normality via a Hilbert-space martingale CLT, and introduces a sample-fitted stabilized test with valid type-I error. Experiments show it is well calibrated and effective for both mean shifts and higher-moment differences, outperforming adaptive baselines limited to scalar effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10245v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Houssam Zenati, Bariscan Bozkurt, Arthur Gretton</dc:creator>
    </item>
    <item>
      <title>Neural variational inference for cutting feedback during uncertainty propagation</title>
      <link>https://arxiv.org/abs/2510.10268</link>
      <description>arXiv:2510.10268v1 Announce Type: cross 
Abstract: In many scientific applications, uncertainty of estimates from an earlier (upstream) analysis needs to be propagated in subsequent (downstream) Bayesian analysis, without feedback. Cutting feedback methods, also termed cut-Bayes, achieve this by constructing a cut-posterior distribution that prevents backward information flow. Cutting feedback like nested MCMC is computationally challenging while variational inference (VI) cut-Bayes methods need two variational approximations and require access to the upstream data and model. In this manuscript we propose, NeVI-Cut, a provably accurate and modular neural network-based variational inference method for cutting feedback. We directly utilize samples from the upstream analysis without requiring access to the upstream data or model. This simultaneously preserves modularity of analysis and reduces approximation errors by avoiding a variational approximation for the upstream model. We then use normalizing flows to specify the conditional variational family for the downstream parameters and estimate the conditional cut-posterior as a variational solution of Monte Carlo average loss over all the upstream samples. We provide theoretical guarantees on the NeVI-Cut estimate to approximate any cut-posterior. Our results are in a fixed-data regime and provide convergence rates of the actual variational solution, quantifying how richness of the neural architecture and the complexity of the target cut-posterior dictate the approximation quality. In the process, we establish new results on uniform Kullback-Leibler approximation rates of conditional normalizing flows. Simulation studies and two real-world analyses illustrate how NeVI-Cut achieves significant computational gains over traditional cutting feedback methods and is considerably more accurate than parametric variational cut approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10268v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiafang Song, Sandipan Pramanik, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>Applying non-negative matrix factorization with covariates to label matrix for classification</title>
      <link>https://arxiv.org/abs/2510.10375</link>
      <description>arXiv:2510.10375v1 Announce Type: cross 
Abstract: Non-negative matrix factorization (NMF) is widely used for dimensionality reduction and interpretable analysis, but standard formulations are unsupervised and cannot directly exploit class labels. Existing supervised or semi-supervised extensions usually incorporate labels only via penalties or graph constraints, still requiring an external classifier. We propose \textit{NMF-LAB} (Non-negative Matrix Factorization for Label Matrix), which redefines classification as the inverse problem of non-negative matrix tri-factorization (tri-NMF). Unlike joint NMF methods, which reconstruct both features and labels, NMF-LAB directly factorizes the label matrix $Y$ as the observation, while covariates $A$ are treated as given explanatory variables. This yields a direct probabilistic mapping from covariates to labels, distinguishing our method from label-matrix factorization approaches that mainly model label correlations or impute missing labels. Our inversion offers two key advantages: (i) class-membership probabilities are obtained directly from the factorization without a separate classifier, and (ii) covariates, including kernel-based similarities, can be seamlessly integrated to generalize predictions to unseen samples. In addition, unlabeled data can be encoded as uniform distributions, supporting semi-supervised learning. Experiments on diverse datasets, from small-scale benchmarks to the large-scale MNIST dataset, demonstrate that NMF-LAB achieves competitive predictive accuracy, robustness to noisy or incomplete labels, and scalability to high-dimensional problems, while preserving interpretability. By unifying regression and classification within the tri-NMF framework, NMF-LAB provides a novel, probabilistic, and scalable approach to modern classification tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10375v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenichi Satoh</dc:creator>
    </item>
    <item>
      <title>Transfer Learning with Distance Covariance for Random Forest: Error Bounds and an EHR Application</title>
      <link>https://arxiv.org/abs/2510.10870</link>
      <description>arXiv:2510.10870v1 Announce Type: cross 
Abstract: Random forest is an important method for ML applications due to its broad outperformance over competing methods for structured tabular data. We propose a method for transfer learning in nonparametric regression using a centered random forest (CRF) with distance covariance-based feature weights, assuming the unknown source and target regression functions are different for a few features (sparsely different). Our method first obtains residuals from predicting the response in the target domain using a source domain-trained CRF. Then, we fit another CRF to the residuals, but with feature splitting probabilities proportional to the sample distance covariance between the features and the residuals in an independent sample. We derive an upper bound on the mean square error rate of the procedure as a function of sample sizes and difference dimension, theoretically demonstrating transfer learning benefits in random forests. In simulations, we show that the results obtained for the CRFs also hold numerically for the standard random forest (SRF) method with data-driven feature split selection. Beyond transfer learning, our results also show the benefit of distance-covariance-based weights on the performance of RF in some situations. Our method shows significant gains in predicting the mortality of ICU patients in smaller-bed target hospitals using a large multi-hospital dataset of electronic health records for 200,000 ICU patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10870v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenze Li, Subhadeep Paul</dc:creator>
    </item>
    <item>
      <title>Age-period modeling of mortality gaps: the cases of cancer and circulatory diseases</title>
      <link>https://arxiv.org/abs/2510.10904</link>
      <description>arXiv:2510.10904v1 Announce Type: cross 
Abstract: Understanding and modeling mortality patterns, especially differences in mortality rates between populations, is vital for demographic analysis and public health planning. We compare three statistical models within the age-period framework to examine differences in death counts. The models are based on the double Poisson, bivariate Poisson, and Skellam distributions, each of which provides unique strengths in capturing underlying mortality trends. Focusing on mortality data from 1960 to 2015, we analyze the two leading causes of death in Italy, which exhibit significant temporal and age-related variations. Our results reveal that the Skellam distribution offers superior accuracy and simplicity in capturing mortality differentials. These findings highlight the potential of the Skellam distribution for analyzing mortality gaps effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10904v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Giacomo Lanfiuti Baldi, Andrea Nigri, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Spatial and Temporal Boundaries in Difference-in-Differences: A Framework from Navier-Stokes Equation</title>
      <link>https://arxiv.org/abs/2510.11013</link>
      <description>arXiv:2510.11013v1 Announce Type: cross 
Abstract: This paper develops a unified framework for identifying spatial and temporal boundaries of treatment effects in difference-in-differences designs. Starting from fundamental fluid dynamics equations (Navier-Stokes), we derive conditions under which treatment effects decay exponentially in space and time, enabling researchers to calculate explicit boundaries beyond which effects become undetectable. The framework encompasses both linear (pure diffusion) and nonlinear (advection-diffusion with chemical reactions) regimes, with testable scope conditions based on dimensionless numbers from physics (P\'eclet and Reynolds numbers). We demonstrate the framework's diagnostic capability using air pollution from coal-fired power plants. Analyzing 791 ground-based PM$_{2.5}$ monitors and 189,564 satellite-based NO$_2$ grid cells in the Western United States over 2019-2021, we find striking regional heterogeneity: within 100 km of coal plants, both pollutants show positive spatial decay (PM$_{2.5}$: $\kappa_s = 0.00200$, $d^* = 1,153$ km; NO$_2$: $\kappa_s = 0.00112$, $d^* = 2,062$ km), validating the framework. Beyond 100 km, negative decay parameters correctly signal that urban sources dominate and diffusion assumptions fail. Ground-level PM$_{2.5}$ decays approximately twice as fast as satellite column NO$_2$, consistent with atmospheric transport physics. The framework successfully diagnoses its own validity in four of eight analyzed regions, providing researchers with physics-based tools to assess whether their spatial difference-in-differences setting satisfies diffusion assumptions before applying the estimator. Our results demonstrate that rigorous boundary detection requires both theoretical derivation from first principles and empirical validation of underlying physical assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11013v1</guid>
      <category>econ.EM</category>
      <category>econ.GN</category>
      <category>math.ST</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Policy Robustness &amp; Uncertainty in Model-based Decision Support for the Energy Transition</title>
      <link>https://arxiv.org/abs/2510.11177</link>
      <description>arXiv:2510.11177v1 Announce Type: cross 
Abstract: Climate policy modelling is a key tool for assessing mitigation strategies in complex systems and uncertainty is inherent and unavoidable. We present a general methodology for extensive uncertainty analysis in climate policy modelling. We show how emulators can identify key uncertainties in modelling frameworks and enable policy analysis previously restricted by computational cost. We apply this methodology to FTT:Power to explore uncertainties in the electricity system transition both globally and in India and to assess how robust mitigation strategies are to a vast range of policy and techno-economic scenarios. We find that uncertainties in transition outcomes are significantly larger than previously shown, but strong policy can narrow these ranges. Globally, plant construction and grid connection lead times dominate transition uncertainty, outweighing regional price policies, including policy reversals in the US. Solar PV proves most resilient due to low costs, though still sensitive to financing and infrastructure limits. Wind and other renewables are more vulnerable. In India, we find that policy packages including even partial phaseout instruments have greater robustness to key uncertainties although longer lead times still hinder policy goals. Our results highlight that reducing lead times and phasing out fossil fuels are critical for faster, more robust power sector transitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11177v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ian J. Burton, Femke J. M. M. Nijsse, James M. Salter</dc:creator>
    </item>
    <item>
      <title>Treatment Allocation under Uncertain Costs</title>
      <link>https://arxiv.org/abs/2103.11066</link>
      <description>arXiv:2103.11066v5 Announce Type: replace 
Abstract: We consider the problem of learning how to optimally allocate treatments whose cost is uncertain and can vary with pre-treatment covariates. This setting may arise in medicine if we need to prioritize access to a scarce resource that different patients would use for different amounts of time, or in marketing if we want to target discounts whose cost to the company depends on how much the discounts are used. Here, we show that the optimal treatment allocation rule under budget constraints is a thresholding rule based on priority scores (those with a higher score are treated first), and we propose a number of practical methods for learning these priority scores using data from a randomized trial. Our formal results leverage a statistical connection between our problem and that of learning heterogeneous treatment effects under endogeneity using an instrumental variable. We find our method to perform well in a number of empirical evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.11066v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Sun, Evan Munro, Georgy Kalashnov, Shuyang Du, Stefan Wager</dc:creator>
    </item>
    <item>
      <title>Non-conjugate variational Bayes for pseudo-likelihood mixed effect models</title>
      <link>https://arxiv.org/abs/2206.09444</link>
      <description>arXiv:2206.09444v5 Announce Type: replace 
Abstract: We propose a unified, yet simple to code, non-conjugate variational Bayes algorithm for posterior approximation of generic Bayesian generalized mixed effect models. Specifically, we consider regression models identified by a linear predictor, eventually transformed using a bijective link, where the prediction misfit is measured using, possibly non-differentiable, loss functions. Examples include generalized linear models, quasi-likelihood models, and robust regression. To address the limitations of non-conjugate settings, we employ an efficient message passing optimization strategy under a Gaussian variational approximation of the posterior. The resulting algorithms automatically account for non-conjugate priors and non-smooth losses, without requiring model-specific data-augmented representations. Besides the general formulation, we provide closed-form updates for popular model specifications, including quantile regression and support vector machines. Overall, theoretical and empirical results highlight the effectiveness of the proposed method, demonstrating its computational efficiency and approximation accuracy as an alternative to existing Bayesian techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.09444v5</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/10618600.2025.2527925</arxiv:DOI>
      <arxiv:journal_reference>Journal of Computational and Graphical Statistics, 1-18 (2025)</arxiv:journal_reference>
      <dc:creator>Cristian Castiglione, Mauro Bernardi</dc:creator>
    </item>
    <item>
      <title>Valid post-selection inference in Robust Q-learning</title>
      <link>https://arxiv.org/abs/2208.03233</link>
      <description>arXiv:2208.03233v2 Announce Type: replace 
Abstract: Q-learning facilitates the development of an optimal adaptive treatment strategy through stagewise regression on a pre-specified set of tailoring variables and confounders. Semiparametric robust Q-learning eliminates the residual confounding that can occur when parametric working models for confounding influences are misspecified. However, in the presence of many potential tailoring variables, constructing an optimal adaptive treatment strategy using either approach may lead to including extraneous variables that contribute little or no benefit while increasing implementation costs, thereby placing an undue burden on patients. Using data-driven selection processes to identify a smaller set of informative prognostic factors is straightforward; however, proper statistical inference must account for this selection process. In this paper, we adapt the Universal Post-Selection Inference (UPoSI) procedure to the semiparametric Robust Q-learning method. UPoSI, introduced for use with linear models, allows for very general variable selection mechanisms. Our approach addresses the unique challenges stemming from the use of UPoSI with semiparametric multistage decision methods. Theoretical and simulation results demonstrate the validity of the proposed confidence regions. We illustrate our proposed methods through an application to adaptive treatment strategy estimation for substance abuse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.03233v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jeremiah Jones, Ashkan Ertefaie, James R. McKay, David W. Oslin, Robert L. Strawderman</dc:creator>
    </item>
    <item>
      <title>DeepVARwT: Deep Learning for a VAR Model with Trend</title>
      <link>https://arxiv.org/abs/2209.10587</link>
      <description>arXiv:2209.10587v5 Announce Type: replace 
Abstract: The vector autoregressive (VAR) model has been used to describe the dependence within and across multiple time series. This is a model for stationary time series which can be extended to allow the presence of a deterministic trend in each series. Detrending the data either parametrically or nonparametrically before fitting the VAR model gives rise to more errors in the latter part. In this study, we propose a new approach called DeepVARwT that employs deep learning methodology for maximum likelihood estimation of the trend and the dependence structure at the same time. A Long Short-Term Memory (LSTM) network is used for this purpose. To ensure the stability of the model, we enforce the causality condition on the autoregressive coefficients using the transformation of Ansley &amp; Kohn (1986). We provide a simulation study and an application to real data. In the simulation study, we use realistic trend functions generated from real data and compare the estimates with true function/parameter values. In the real data application, we compare the prediction performance of this model with state-of-the-art models in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.10587v5</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xixi Li, Jingsong Yuan</dc:creator>
    </item>
    <item>
      <title>Integral Fractional Ornstein-Uhlenbeck Process Model for Animal Movement</title>
      <link>https://arxiv.org/abs/2312.09900</link>
      <description>arXiv:2312.09900v2 Announce Type: replace 
Abstract: Modeling the trajectories of animals is challenging due to the complexity of their behaviors, the influence of unpredictable environmental factors, individual variability, and the lack of detailed data on their movements. Additionally, factors such as migration, hunting, reproduction, and social interactions add additional layers of complexity when attempting to accurately forecast their movements. In the literature, various models exits that aim to study animal telemetry, by modeling the velocity of the telemetry, the telemetry itself or both processes jointly through a Markovian process. In this work, we propose to model the velocity of each coordinate axis for animal telemetry data as a fractional Ornstein-Uhlenbeck (fOU) process. Then, the integral fOU process models position data in animal telemetry. Compared to traditional methods, the proposed model is flexible in modeling long-range memory. The Hurst parameter $H \in (0,1)$ is a crucial parameter in integral fOU process, as it determines the degree of dependence or long-range memory. The integral fOU process is nonstationary process. In addition, a higher Hurst parameter ($H &gt; 0.5$) indicates a stronger memory, leading to trajectories with transient trends, while a lower Hurst parameter ($H &lt; 0.5$) implies a weaker memory, resulting in trajectories with recurring trends. When H = 0.5, the process reduces to a standard integral Ornstein-Uhlenbeck process. We develop a fast simulation algorithm of telemetry trajectories using an approach via finite-dimensional distributions. We also develop a maximum likelihood method for parameter estimation and its performance is examined by simulation studies. Finally, we present a telemetry application of Fin Whales that disperse over the Gulf of Mexico.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.09900v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. H. Ramirez-Gonzalez, Ying Sun</dc:creator>
    </item>
    <item>
      <title>Explaining and Connecting Kriging with Gaussian Process Regression</title>
      <link>https://arxiv.org/abs/2408.02331</link>
      <description>arXiv:2408.02331v2 Announce Type: replace 
Abstract: Kriging and Gaussian Process Regression are statistical methods that allow predicting the outcome of a random process or a random field by using a sample of correlated observations. In other words, the random process or random field is partially observed, and by using a sample a prediction is made, pointwise or as a whole, where the latter can be thought as a reconstruction. In addition, the techniques permit to give a measure of uncertainty of the prediction. The methods have different origins. Kriging comes from geostatistics, a field which started to develop around 1950 oriented to mining valuation problems, whereas Gaussian Process Regression has gained popularity in the area of machine learning in the last decade of the previous century. In the literature, the methods are usually presented as being the same technique. However, beyond this affirmation, the techniques have yet not been compared on a thorough mathematical basis and neither explained why and under which conditions this affirmation holds. Furthermore, Kriging has many variants and this affirmation should be precised. In this paper, this gap is filled. It is shown, step by step how both methods are deduced from the first principles -- with a major focus on Kriging, the mathematical connection between them, and which Kriging variant corresponds to which Gaussian Process Regression set up. The three most widely used versions of Kriging are considered: Simple Kriging, Ordinary Kriging and Universal Kriging. It is found, that despite their closeness, the techniques are different in their approach and assumptions, in a similar way the Least Square method, the Best Linear Unbiased Estimator method and the Likelihood method in regression do. I hope this work deepen the understanding of the relation between Kriging and Gaussian Process Regression, and serves as a cohesive introductory resource for researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02331v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marius Marinescu</dc:creator>
    </item>
    <item>
      <title>Data-light Uncertainty Set Merging with Admissibility</title>
      <link>https://arxiv.org/abs/2410.12201</link>
      <description>arXiv:2410.12201v3 Announce Type: replace 
Abstract: This article introduces a Synthetics, Aggregation, and Test inversion (SAT) approach for merging diverse and potentially dependent uncertainty sets into a single unified set. The procedure is data-light, relying only on initial sets and their nominal levels, and it flexibly adapts to user-specified input sets with possibly varying coverage guarantees. SAT is motivated by the challenge of integrating uncertainty sets when only the initial sets and their control levels are available-for example, when merging confidence sets from distributed sites under communication constraints or combining conformal prediction sets generated by different algorithms or data splits. To address this, SAT constructs and aggregates novel synthetic test statistics, and then derive merged sets through test inversion. Our method leverages the duality between set estimation and hypothesis testing, ensuring reliable coverage in dependent scenarios. A key theoretical contribution is a rigorous analysis of SAT's properties, including its admissibility in the context of deterministic set merging. Both theoretical analyses and empirical results confirm the method's finite-sample coverage validity and desirable set sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12201v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shenghao Qin, Jianliang He, Qi Kuang, Bowen Gang, Yin Xia</dc:creator>
    </item>
    <item>
      <title>A New One Parameter Unit Distribution: Median Based Unit Rayleigh (MBUR): Parametric Quantile Regression Model</title>
      <link>https://arxiv.org/abs/2410.14857</link>
      <description>arXiv:2410.14857v3 Announce Type: replace 
Abstract: Parametric quantile regression is illustrated for the one parameter new unit Rayleigh distribution called Median Based Unit Rayleigh distribution (MBUR) distribution. The estimation process using re-parameterized maximum likelihood function is highlighted with real dataset example. The inference and goodness of fit is also explored.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14857v3</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohamed Attia</dc:creator>
    </item>
    <item>
      <title>On "confirmatory" methodological research in statistics and related fields</title>
      <link>https://arxiv.org/abs/2503.08124</link>
      <description>arXiv:2503.08124v4 Announce Type: replace 
Abstract: Empirical substantive research, such as in the life or social sciences, is commonly categorized into the two modes exploratory and confirmatory, both of which are essential to scientific progress. The former is also referred to as hypothesis-generating or data-contingent research, while the latter is also called hypothesis-testing research. In the context of empirical methodological research in statistics, however, the exploratory-confirmatory distinction has received very little attention so far. Our paper aims to fill this gap. First, we revisit the concept of empirical methodological research through the lens of the exploratory-confirmatory distinction. Secondly, we examine current practice with respect to this distinction through a literature survey including 115 articles from the field of biostatistics. Thirdly, we provide practical recommendations towards more appropriate design, interpretation, and reporting of empirical methodological research in light of this distinction. In particular, we argue that both modes of research are crucial to methodological progress, but that most published studies -- even if sometimes disguised as confirmatory -- are essentially exploratory in nature. We emphasize that it may be adequate to consider empirical methodological research as a continuum between "pure" exploration and "strict" confirmation, recommend transparently reporting the mode of conducted research within the spectrum between exploratory and confirmatory, and stress the importance of study protocols written before conducting the study, especially in confirmatory methodological research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08124v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>F. J. D. Lange, Juliane C. Wilcke, Sabine Hoffmann, Moritz Herrmann, Anne-Laure Boulesteix</dc:creator>
    </item>
    <item>
      <title>Supervised Manifold Learning for Functional Data</title>
      <link>https://arxiv.org/abs/2503.17943</link>
      <description>arXiv:2503.17943v2 Announce Type: replace 
Abstract: Classification is a core topic in functional data analysis. A large number of functional classifiers have been proposed in the literature, most of which are based on functional principal component analysis or functional regression. In contrast, we investigate this topic from the perspective of manifold learning. It is assumed that functional data lie on an unknown low-dimensional manifold, and we expect that superior classifiers can be developed based on the manifold structure. To this end, we propose a novel proximity measure that takes the label information into account to learn the low-dimensional representations, also known as the supervised manifold learning outcomes. When the outcomes are coupled with multivariate classifiers, the procedure induces a new family of functional classifiers. In theory, we prove that our functional classifier induced by the $k$-NN classifier is asymptotically optimal. In practice, we show that our method, coupled with several classical multivariate classifiers, achieves highly competitive classification performance compared to existing functional classifiers across both synthetic and real data examples. Supplementary materials are available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17943v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruoxu Tan, Yiming Zang</dc:creator>
    </item>
    <item>
      <title>A burn-in(g) question: How long should an initial equal randomization stage be before Bayesian response-adaptive randomization?</title>
      <link>https://arxiv.org/abs/2503.19795</link>
      <description>arXiv:2503.19795v2 Announce Type: replace 
Abstract: Response-adaptive randomization (RAR) can increase participant benefit in clinical trials, but also complicates statistical analysis. The burn-in period (a non-adaptive initial stage) is commonly used to mitigate this disadvantage, yet guidance on its optimal duration is scarce. To address this critical gap, this paper introduces an exact evaluation approach to investigate how the burn-in length impacts statistical operating characteristics of two-arm binary Bayesian RAR (BRAR) designs. We show that (1) commonly used calibration and asymptotic tests show substantial type I error rate inflation for BRAR designs without a burn-in period, and increasing the total burn-in length to more than half the trial size reduces but does not fully mitigate type I error rate inflation, necessitating exact tests; (2) exact tests conditioning on total successes show the highest average and minimum power up to large burn-in lengths; (3) the burn-in length substantially influences power and participant benefit, which are often not maximized at the maximum or minimum possible burn-in length; (4) the test statistic influences the type I error rate and power; (5) estimation bias decreases quicker in the burn-in length for larger treatment effects and increases for larger trial sizes under the same burn-in length. Our approach is illustrated by re-designing the ARREST trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19795v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edwin Y. N. Tang, Stef Baas, Daniel Kaddaj, Lukas Pin, David S. Robertson, Sof\'ia S. Villar</dc:creator>
    </item>
    <item>
      <title>Density-valued time series: Nonparametric density-on-density regression</title>
      <link>https://arxiv.org/abs/2503.22904</link>
      <description>arXiv:2503.22904v2 Announce Type: replace 
Abstract: This paper is concerned with forecasting probability density functions. Density functions are nonnegative and have a constrained integral; thus, they do not constitute a vector space. Implementing unconstrained functional time-series forecasting methods is problematic for such nonlinear and constrained data. A novel forecasting method is developed based on a nonparametric function-on-function regression, where both the response and the predictor are probability density functions. Asymptotic properties of our nonparametric regression estimator are established, as well as its finite-sample performance through a series of Monte-Carlo simulation studies. Using COVID-19 data from the French department and age-specific period life tables from the United States, we assess and compare the finite-sample forecast accuracy of the proposed method with several existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22904v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fr\'ed\'eric Ferraty, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Predicting data value before collection: A coefficient for prioritizing sources under random distribution shift</title>
      <link>https://arxiv.org/abs/2504.06570</link>
      <description>arXiv:2504.06570v2 Announce Type: replace 
Abstract: Researchers often face choices between multiple data sources that differ in quality, cost, and representativeness. Which sources will most improve predictive performance? We study this data prioritization problem under a random distribution shift model, where candidate sources arise from random perturbations to a target population. We propose the Data Usefulness Coefficient (DUC), which predicts the reduction in prediction error from adding a dataset to training, using only covariate summary statistics and no outcome data. We prove that under random shifts, covariate differences between sources are informative about outcome prediction quality. Through theory and experiments on synthetic and real data, we demonstrate that DUC-based selection outperforms alternative strategies, allowing more efficient resource allocation across heterogeneous data sources. The method provides interpretable rankings between candidate datasets and works for any data modality, including ordinal, categorical, and continuous data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06570v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivy Zhang, Dominik Rothenh\"ausler</dc:creator>
    </item>
    <item>
      <title>Covariate balancing estimation and model selection for difference-in-differences approach</title>
      <link>https://arxiv.org/abs/2504.13057</link>
      <description>arXiv:2504.13057v3 Announce Type: replace 
Abstract: Remarkable progress has been made in difference-in-differences (DID) approaches to causal inference that estimate the average effect of a treatment on the treated (ATT). Of these, the semiparametric DID (SDID) approach incorporates a propensity score analysis into the DID setup. Supposing that the ATT is a function of covariates, we estimate it by weighting the inverse of the propensity score. In this study, as one way to make the estimation robust to the propensity score modeling, we incorporate covariate balancing. Then, by attentively constructing the moment conditions used in the covariate balancing, we show that the proposed estimator is doubly robust. In addition to the estimation, we also address model selection. In practice, covariate selection is an essential task in statistical analysis, but even in the basic setting of the SDID approach, there are no reasonable information criteria. Here, we derive a model selection criterion as an asymptotically bias-corrected estimator of risk based on the loss function used in the SDID estimation. We show that a penalty term can be derived that is considerably different from almost twice the number of parameters that often appears in AIC-type information criteria. Numerical experiments show that the proposed method estimates the ATT more robustly compared with the method using propensity scores given by maximum likelihood estimation, and that the proposed criterion clearly reduces the risk targeted in the SDID approach in comparison with the intuitive generalization of the existing information criterion. In addition, real data analysis confirms that there is a large difference between the results of the proposed method and those of the existing method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13057v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takamichi Baba, Yoshiyuki Ninomiya</dc:creator>
    </item>
    <item>
      <title>A longitudinal Bayesian framework for estimating causal dose-response relationships</title>
      <link>https://arxiv.org/abs/2505.20893</link>
      <description>arXiv:2505.20893v2 Announce Type: replace 
Abstract: Existing causal methods for time-varying exposure and time-varying confounding focus on estimating the average causal effect of a time-varying binary treatment on an end-of-study outcome. Methods for estimating the effects of a time-varying continuous exposure at any dose level on the outcome are limited. We introduce a scalable, non-parametric Bayesian framework for estimating longitudinal causal dose-response relationships with repeated measures.We incorporate the generalized propensity score either as a covariate or through inverse-probability weighting, formulating two Bayesian dose-response estimators. The proposed approach embeds a double non-parametric generalized Bayesian bootstrap which enables a flexible Dirichlet process specification within a generalized estimating equations structure, capturing temporal correlation while making minimal assumptions about the functional form of the continuous exposure. We applied our proposed approach to a motivating study of monthly metro-ridership data and COVID-19 case counts from major international cities, identifying causal relationships and the dynamic dose-response patterns between higher ridership and increased case counts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20893v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Luo, Kuan Liu, Ramandeep Singh, Daniel J. Graham</dc:creator>
    </item>
    <item>
      <title>Causal Inference with Missing Exposures and Missing Outcomes</title>
      <link>https://arxiv.org/abs/2506.03336</link>
      <description>arXiv:2506.03336v2 Announce Type: replace 
Abstract: Missing data are ubiquitous in public health research. When estimating causal effects, there are well-established methods to address bias to due censored outcomes. Commonly, causal estimands are defined under hypothetical interventions to "set" the exposure and to prevent censoring. Identification is evaluated with the sequential backdoor criterion and considerations of data support. Then inverse weighting, standardization, and doubly-robust approaches are applied for statistical estimation and inference. We demonstrate how this framework can be extended to settings with missingness on the exposure of interest as well as the variable defining the population of interest (e.g., persons at risk of the outcome). Our work is motivated by SEARCH-TB's investigation of the effect of alcohol consumption on the risk of incident tuberculosis (TB) infection in rural Uganda. This study posed several real-world challenges: confounding, missingness on the exposure (alcohol use), missingness on the baseline outcome (defining who was at risk of TB), and missingness on the outcome at follow-up (capturing who acquired TB). We present a series of causal models and identification results to demonstrate the handling of missing exposures and outcomes in prospective studies. We highlight the use of TMLE with Super Learner and the real-world consequences of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03336v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kirsten E. Landsiedel, Rachel Abbott, Atukunda Mucunguzi, Florence Mwangwa, Elijah Kakande, Edwin D. Charlebois, Carina Marquez, Moses R. Kamya, Laura B. Balzer</dc:creator>
    </item>
    <item>
      <title>Coefficient Shape Transfer Learning for Functional Linear Regression</title>
      <link>https://arxiv.org/abs/2506.11367</link>
      <description>arXiv:2506.11367v3 Announce Type: replace 
Abstract: The shapes of functions provide highly interpretable summaries of their trajectories. This article develops a novel transfer learning methodology to tackle the challenge of data scarcity in functional linear models. The methodology incorporates samples from the target model (target domain) alongside those from auxiliary models (source domains), transferring knowledge of coefficient shape from the source domains to the target domain. This shape-based transfer learning framework enhances robustness and generalizability: by being invariant to covariate scaling and signal strength, it ensures reliable knowledge transfer even when data from different sources differ in magnitude, and by formalizing the notion of coefficient shape homogeneity, it extends beyond traditional coefficient-equality assumptions to incorporate information from a broader range of source domains. We rigorously analyze the convergence rates of the proposed estimator and examine the minimax optimality. Our findings show that the degree of improvement depends not only on the similarity of coefficient shapes between the target and source domains, but also on coefficient magnitudes and the spectral decay rates of the functional covariates covariance operators. To address situations where only a subset of auxiliary models is informative for the target model, we further develop a data-driven procedure for identifying such informative sources. The effectiveness of the proposed methodology is demonstrated through comprehensive simulation studies and an application to occupation time analysis using physical activity data from the U.S. National Health and Nutrition Examination Survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11367v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuhao Jiao, Ian W. Mckeague, N. -H. Chan</dc:creator>
    </item>
    <item>
      <title>Predictive posteriors under hidden confounding</title>
      <link>https://arxiv.org/abs/2507.05170</link>
      <description>arXiv:2507.05170v2 Announce Type: replace 
Abstract: Predicting outcomes in external domains is challenging due to hidden confounders that potentially influence both predictors and outcomes. Well-established methods frequently rely on stringent assumptions, explicit knowledge about the distribution shift across domains, or bias-inducing regularization schemes to enhance generalization. While recent developments in point prediction under hidden confounding attempt to mitigate these shortcomings, they generally do not provide principled uncertainty quantification. We introduce a Bayesian framework that yields well-calibrated predictive distributions across external domains, supports valid model inference, and achieves posterior contraction rates that improve as the number of observed datasets increases. Simulations and a medical application highlight the remarkable empirical coverage of our approach, nearly unchanged when transitioning from low- to moderate-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05170v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Garc\'ia Meixide, David R\'ios Insua</dc:creator>
    </item>
    <item>
      <title>Best of mini-N in-loop Sampling: A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling</title>
      <link>https://arxiv.org/abs/2510.04087</link>
      <description>arXiv:2510.04087v2 Announce Type: replace 
Abstract: Modern preference alignment techniques, such as Best-of-N (BoN) sampling, rely on reward models trained with pairwise comparison data. While effective at learning relative preferences, this paradigm fails to capture a signal of response acceptability, leaving systems vulnerable to selecting the least bad of many unacceptable options. This is particularly problematic for hard prompts, where the risk of such false acceptances increases with the number of samples. In this paper, we address this critical reliability gap by introducing a new data collection and modeling framework. By augmenting preference data with an outside option, inspired by discrete choice models, we train a reward model that can distinguish not just what is better, but what is good enough. We leverage this capability to create an adaptive inference strategy, best of mini-N in-loop, which partitions the generation budget into sequential loops with a calibrated, early-exit condition. Our experiments show that when tuned as an alignment guardrail, it reduces reliability failures by 70%, and when tuned as an inference accelerator, it improves average inference speed by over 22% in IMDB-sentiment setting. We thus provide a principled and flexible framework for practitioners to explicitly manage the trade-off between reliability and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.04087v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.21203/rs.3.rs-7594024/v1</arxiv:DOI>
      <dc:creator>Hyung Gyu Rho, Sian Lee</dc:creator>
    </item>
    <item>
      <title>Robust mean change point testing in high-dimensional data with heavy tails</title>
      <link>https://arxiv.org/abs/2305.18987</link>
      <description>arXiv:2305.18987v4 Announce Type: replace-cross 
Abstract: We study mean change point testing problems for high-dimensional data, with exponentially- or polynomially-decaying tails. In each case, depending on the $\ell_0$-norm of the mean change vector, we separately consider dense and sparse regimes. We characterise the boundary between the dense and sparse regimes under the above two tail conditions for the first time in the change point literature and propose novel testing procedures that attain optimal rates in each of the four regimes up to a poly-iterated logarithmic factor. By comparing with previous results under Gaussian assumptions, our results quantify the costs of heavy-tailedness on the fundamental difficulty of change point testing problems for high-dimensional data.
  To be specific, when the error distributions possess exponentially-decaying tails, a CUSUM-type statistic is shown to achieve a minimax testing rate up to $\sqrt{\log\log(8n)}$. As for polynomially-decaying tails, admitting bounded $\alpha$-th moments for some $\alpha \geq 4$, we introduce a median-of-means-type test statistic that achieves a near-optimal testing rate in both dense and sparse regimes. In the sparse regime, we further propose a computationally-efficient test to achieve optimality. Our investigation in the even more challenging case of $2 \leq \alpha &lt; 4$, unveils a new phenomenon that the minimax testing rate has no sparse regime, i.e.\ testing sparse changes is information-theoretically as hard as testing dense changes. Finally, we consider various extensions where we also obtain near-optimal performances, including testing against multiple change points, allowing temporal dependence as well as fewer than two finite moments in the data generating mechanisms. We also show how sub-Gaussian rates can be achieved when an additional minimal spacing condition is imposed under the alternative hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.18987v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengchu Li, Yudong Chen, Tengyao Wang, Yi Yu</dc:creator>
    </item>
    <item>
      <title>Discovering and Reasoning of Causality in the Hidden World with Large Language Models</title>
      <link>https://arxiv.org/abs/2402.03941</link>
      <description>arXiv:2402.03941v3 Announce Type: replace-cross 
Abstract: Revealing hidden causal variables alongside the underlying causal mechanisms is essential to the development of science. Despite the progress in the past decades, existing practice in causal discovery (CD) heavily relies on high-quality measured variables, which are usually given by human experts. In fact, the lack of well-defined high-level variables behind unstructured data has been a longstanding roadblock to a broader real-world application of CD. This procedure can naturally benefit from an automated process that can suggest potential hidden variables in the system. Interestingly, Large language models (LLMs) are trained on massive observations of the world and have demonstrated great capability in processing unstructured data. To leverage the power of LLMs, we develop a new framework termed Causal representatiOn AssistanT (COAT) that incorporates the rich world knowledge of LLMs to propose useful measured variables for CD with respect to high-value target variables on their paired unstructured data. Instead of directly inferring causality with LLMs, COAT constructs feedback from intermediate CD results to LLMs to refine the proposed variables. Given the target variable and the paired unstructured data, we first develop COAT-MB that leverages the predictivity of the proposed variables to iteratively uncover the Markov Blanket of the target variable. Built upon COAT-MB, COAT-PAG further extends to uncover a more complete causal graph, i.e., Partial Ancestral Graph, by iterating over the target variables and actively seeking new high-level variables. Moreover, the reliable CD capabilities of COAT also extend the debiased causal inference to unstructured data by discovering an adjustment set. We establish theoretical guarantees for the CD results and verify their efficiency and reliability across realistic benchmarks and real-world case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03941v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenxi Liu, Yongqiang Chen, Tongliang Liu, Mingming Gong, James Cheng, Bo Han, Kun Zhang</dc:creator>
    </item>
    <item>
      <title>Quantifying the Internal Validity of Weighted Estimands</title>
      <link>https://arxiv.org/abs/2404.14603</link>
      <description>arXiv:2404.14603v4 Announce Type: replace-cross 
Abstract: In this paper we study a class of weighted estimands, which we define as parameters that can be expressed as weighted averages of the underlying heterogeneous treatment effects. The popular ordinary least squares (OLS), two-stage least squares (2SLS), and two-way fixed effects (TWFE) estimands are all special cases within our framework. Our focus is on answering two questions concerning weighted estimands. First, under what conditions can they be interpreted as the average treatment effect for some (possibly latent) subpopulation? Second, when these conditions are satisfied, what is the upper bound on the size of that subpopulation, either in absolute terms or relative to a target population of interest? We argue that this upper bound provides a valuable diagnostic for empirical research. When a given weighted estimand corresponds to the average treatment effect for a small subset of the population of interest, we say its internal validity is low. Our paper develops practical tools to quantify the internal validity of weighted estimands. We also apply these tools to revisit a prominent study of the effects of unilateral divorce laws on female suicide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14603v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandre Poirier, Tymon S{\l}oczy\'nski</dc:creator>
    </item>
    <item>
      <title>Generalization Bounds of Surrogate Policies for Combinatorial Optimization Problems</title>
      <link>https://arxiv.org/abs/2407.17200</link>
      <description>arXiv:2407.17200v2 Announce Type: replace-cross 
Abstract: A recent line of structured learning methods has advanced the practical state-of-the-art for combinatorial optimization problems with complex, application-specific objectives. These approaches learn policies that couple a statistical model with a tractable surrogate combinatorial optimization oracle, so as to exploit the distribution of problem instances instead of solving each instance independently. A core obstacle is that the empirical risk is then piecewise constant in the model parameters. This hinders gradient-based optimization and only few theoretical guarantees have been provided so far. We address this issue by analyzing smoothed (perturbed) policies: adding controlled random perturbations to the direction used by the linear oracle yields a differentiable surrogate risk and improves generalization. Our main contribution is a generalization bound that decomposes the excess risk into perturbation bias, statistical estimation error, and optimization error. The analysis hinges on a new Uniform Weak (UW) property capturing the geometric interaction between the statistical model and the normal fan of the feasible polytope; we show it holds under mild assumptions, and automatically when a minimal baseline perturbation is present. The framework covers, in particular, contextual stochastic optimization. We illustrate the scope of the results on applications such as stochastic vehicle scheduling, highlighting how smoothing enables both tractable training and controlled generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17200v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Pierre-Cyril Aubin-Frankowski, Yohann De Castro, Axel Parmentier, Alessandro Rudi</dc:creator>
    </item>
    <item>
      <title>Bayesian Perspective for Orientation Determination in Cryo-EM with Application to Structural Heterogeneity Analysis</title>
      <link>https://arxiv.org/abs/2412.03723</link>
      <description>arXiv:2412.03723v2 Announce Type: replace-cross 
Abstract: Accurate orientation estimation is a crucial component of 3D molecular structure reconstruction, both in single-particle cryo-electron microscopy (cryo-EM) and in the increasingly popular field of cryo-electron tomography (cryo-ET). The dominant approach, which involves searching for the orientation that maximizes cross-correlation relative to given templates, is sub-optimal, particularly under low signal-to-noise conditions. In this work, we propose a Bayesian framework for more accurate and flexible orientation estimation, with the minimum mean square error (MMSE) estimator serving as a key example. Through simulations, we demonstrate that the MMSE estimator consistently outperforms the cross-correlation-based method, especially in challenging low signal-to-noise scenarios, and we provide a theoretical framework that supports these improvements.
  When incorporated into iterative refinement algorithms in the 3D reconstruction pipeline, the MMSE estimator markedly improves reconstruction accuracy, reduces model bias, and enhances robustness to the ``Einstein from Noise'' artifact. Crucially, we demonstrate that orientation estimation accuracy has a decisive effect on downstream structural heterogeneity analysis. In particular, integrating the MMSE-based pose estimator into frameworks for continuous heterogeneity recovery yields accuracy improvements approaching those obtained with ground-truth poses, establishing MMSE-based pose estimation as a key enabler of high-fidelity conformational landscape reconstruction. These findings indicate that the proposed Bayesian framework could substantially advance cryo-EM and cryo-ET by enhancing the accuracy, robustness, and reliability of 3D molecular structure reconstruction, thereby facilitating deeper insights into complex biological systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03723v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sheng Xu, Amnon Balanov, Amit Singer, Tamir Bendory</dc:creator>
    </item>
    <item>
      <title>"Over-optimizing" for Normality: Budget-constrained Uncertainty Quantification for Contextual Decision-making</title>
      <link>https://arxiv.org/abs/2503.12747</link>
      <description>arXiv:2503.12747v3 Announce Type: replace-cross 
Abstract: We study uncertainty quantification for contextual stochastic optimization, focusing on weighted sample average approximation (wSAA), which uses machine-learned relevance weights based on covariates. Although wSAA is widely used for contextual decisions, its uncertainty quantification remains limited. In addition, computational budgets tie sample size to optimization accuracy, creating a coupling that standard analyses often ignore. We establish central limit theorems for wSAA and construct asymptotic-normality-based confidence intervals for optimal conditional expected costs. We analyze the statistical--computational tradeoff under a computational budget, characterizing how to allocate resources between sample size and optimization iterations to balance statistical and optimization errors. These allocation rules depend on structural parameters of the objective; misspecifying them can break the asymptotic optimality of the wSAA estimator. We show that ``over-optimizing'' (running more iterations than the nominal rule) mitigates this misspecification and preserves asymptotic normality, at the expense of a slight slowdown in the convergence rate of the budget-constrained estimator. The common intuition that ``more data is better'' can fail under computational constraints: increasing the sample size may worsen statistical inference by forcing fewer algorithm iterations and larger optimization error. Our framework provides a principled way to quantify uncertainty for contextual decision-making under computational constraints. It offers practical guidance on allocating limited resources between data acquisition and optimization effort, clarifying when to prioritize additional optimization iterations over more data to ensure valid confidence intervals for conditional performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12747v3</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yanyuan Wang, Xiaowei Zhang</dc:creator>
    </item>
  </channel>
</rss>
