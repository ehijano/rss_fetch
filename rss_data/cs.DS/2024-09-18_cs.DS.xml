<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Sep 2024 01:42:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Tie-breaking based Local Search Algorithm for Stable Matching Problems</title>
      <link>https://arxiv.org/abs/2409.10575</link>
      <description>arXiv:2409.10575v1 Announce Type: new 
Abstract: The stable marriage problem with incomplete lists and ties (SMTI) and the hospitals/residents problem with ties (HRT) are important in matching theory with broad practical applications. In this paper, we introduce a tie-breaking based local search algorithm (TBLS) designed to achieve a weakly stable matching of maximum size for both the SMTI and HRT problems. TBLS begins by arbitrarily resolving all ties and iteratively refines the tie-breaking strategy by adjusting the relative order within ties based on preference ranks and the current stable matching. Additionally, we introduce TBLS-E, an equity-focused variant of TBLS, specifically designed for the SMTI problem. This variant maintains the objective of maximizing matching size, while enhancing equity through two simple modifications. In comparison with ten other approximation and local search algorithms, TBLS achieves the highest matching size, while TBLS-E exhibits the lowest sex equality cost. Significantly, TBLS-E preserves a matching size comparable to that of TBLS. Both our algorithms demonstrate faster computational speed than other local search algorithms in solving large-sized instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10575v1</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <category>cs.DM</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyuan Qiu</dc:creator>
    </item>
    <item>
      <title>Clustering with Non-adaptive Subset Queries</title>
      <link>https://arxiv.org/abs/2409.10908</link>
      <description>arXiv:2409.10908v1 Announce Type: new 
Abstract: Recovering the underlying clustering of a set $U$ of $n$ points by asking pair-wise same-cluster queries has garnered significant interest in the last decade. Given a query $S \subset U$, $|S|=2$, the oracle returns yes if the points are in the same cluster and no otherwise. For adaptive algorithms with pair-wise queries, the number of required queries is known to be $\Theta(nk)$, where $k$ is the number of clusters. However, non-adaptive schemes require $\Omega(n^2)$ queries, which matches the trivial $O(n^2)$ upper bound attained by querying every pair of points.
  To break the quadratic barrier for non-adaptive queries, we study a generalization of this problem to subset queries for $|S|&gt;2$, where the oracle returns the number of clusters intersecting $S$. Allowing for subset queries of unbounded size, $O(n)$ queries is possible with an adaptive scheme (Chakrabarty-Liao, 2024). However, the realm of non-adaptive algorithms is completely unknown.
  In this paper, we give the first non-adaptive algorithms for clustering with subset queries. Our main result is a non-adaptive algorithm making $O(n \log k \cdot (\log k + \log\log n)^2)$ queries, which improves to $O(n \log \log n)$ when $k$ is a constant. We also consider algorithms with a restricted query size of at most $s$. In this setting we prove that $\Omega(\max(n^2/s^2,n))$ queries are necessary and obtain algorithms making $\tilde{O}(n^2k/s^2)$ queries for any $s \leq \sqrt{n}$ and $\tilde{O}(n^2/s)$ queries for any $s \leq n$. We also consider the natural special case when the clusters are balanced, obtaining non-adaptive algorithms which make $O(n \log k) + \tilde{O}(k)$ and $O(n\log^2 k)$ queries. Finally, allowing two rounds of adaptivity, we give an algorithm making $O(n \log k)$ queries in the general case and $O(n \log \log k)$ queries when the clusters are balanced.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10908v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hadley Black, Euiwoong Lee, Arya Mazumdar, Barna Saha</dc:creator>
    </item>
    <item>
      <title>Selective algorithm processing of subset sum distributions</title>
      <link>https://arxiv.org/abs/2409.11076</link>
      <description>arXiv:2409.11076v1 Announce Type: new 
Abstract: The efficiency of exact subset sum problem algorithms which compute individual subset sums is defined as $e=min(T/z, 1)$, where $z$ is the number of subset sums computed. $e$ is related to these algorithms' computational complexity. This system maps the sums into $kn$ bins to select its most efficient algorithm for each bin for each input value. These algorithms include additive, subtractive and repeated value dynamic programming. Cases which would otherwise be processed inefficiently (eg: all even values) are handled by modular arithmetic and by dynamically partioning the input values. The system's experimentally validated efficiency corresponds to O(max($T$, $n^2$)) with space complexity O(max($T$, $n$)), for $k=2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11076v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nick Dawes</dc:creator>
    </item>
    <item>
      <title>Tight Bounds for Classical Open Addressing</title>
      <link>https://arxiv.org/abs/2409.11280</link>
      <description>arXiv:2409.11280v1 Announce Type: new 
Abstract: We introduce a classical open-addressed hash table, called rainbow hashing, that supports a load factor of up to $1 - \varepsilon$, while also supporting $O(1)$ expected-time queries, and $O(\log \log \varepsilon^{-1})$ expected-time insertions and deletions. We further prove that this tradeoff curve is optimal: any classical open-addressed hash table that supports load factor $1 - \varepsilon$ must incur $\Omega(\log \log \varepsilon^{-1})$ expected time per operation.
  Finally, we extend rainbow hashing to the setting where the hash table is dynamically resized over time. Surprisingly, the addition of dynamic resizing does not come at any time cost -- even while maintaining a load factor of $\ge 1 - \varepsilon$ at all times, we can support $O(1)$ queries and $O(\log \log \varepsilon^{-1})$ updates.
  Prior to our work, achieving any time bounds of the form $o(\varepsilon^{-1})$ for all of insertions, deletions, and queries simultaneously remained an open question.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11280v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael A. Bender, William Kuszmaul, Renfei Zhou</dc:creator>
    </item>
    <item>
      <title>Provably Efficient Infinite-Horizon Average-Reward Reinforcement Learning with Linear Function Approximation</title>
      <link>https://arxiv.org/abs/2409.10772</link>
      <description>arXiv:2409.10772v1 Announce Type: cross 
Abstract: This paper proposes a computationally tractable algorithm for learning infinite-horizon average-reward linear Markov decision processes (MDPs) and linear mixture MDPs under the Bellman optimality condition. While guaranteeing computational efficiency, our algorithm for linear MDPs achieves the best-known regret upper bound of $\widetilde{\mathcal{O}}(d^{3/2}\mathrm{sp}(v^*)\sqrt{T})$ over $T$ time steps where $\mathrm{sp}(v^*)$ is the span of the optimal bias function $v^*$ and $d$ is the dimension of the feature mapping. For linear mixture MDPs, our algorithm attains a regret bound of $\widetilde{\mathcal{O}}(d\cdot\mathrm{sp}(v^*)\sqrt{T})$. The algorithm applies novel techniques to control the covering number of the value function class and the span of optimistic estimators of the value function, which is of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10772v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Woojin Chae, Dabeen Lee</dc:creator>
    </item>
    <item>
      <title>Online Combinatorial Allocations and Auctions with Few Samples</title>
      <link>https://arxiv.org/abs/2409.11091</link>
      <description>arXiv:2409.11091v1 Announce Type: cross 
Abstract: In online combinatorial allocations/auctions, n bidders sequentially arrive, each with a combinatorial valuation (such as submodular/XOS) over subsets of m indivisible items. The aim is to immediately allocate a subset of the remaining items to maximize the total welfare, defined as the sum of bidder valuations. A long line of work has studied this problem when the bidder valuations come from known independent distributions. In particular, for submodular/XOS valuations, we know 2-competitive algorithms/mechanisms that set a fixed price for each item and the arriving bidders take their favorite subset of the remaining items given these prices. However, these algorithms traditionally presume the availability of the underlying distributions as part of the input to the algorithm. Contrary to this assumption, practical scenarios often require the learning of distributions, a task complicated by limited sample availability. This paper investigates the feasibility of achieving O(1)-competitive algorithms under the realistic constraint of having access to only a limited number of samples from the underlying bidder distributions.
  Our first main contribution shows that a mere single sample from each bidder distribution is sufficient to yield an O(1)-competitive algorithm for submodular/XOS valuations. This result leverages a novel extension of the secretary-style analysis, employing the sample to have the algorithm compete against itself. Although online, this first approach does not provide an online truthful mechanism. Our second main contribution shows that a polynomial number of samples suffices to yield a $(2+\epsilon)$-competitive online truthful mechanism for submodular/XOS valuations and any constant $\epsilon&gt;0$. This result is based on a generalization of the median-based algorithm for the single-item prophet inequality problem to combinatorial settings with multiple items.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11091v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul D\"utting, Thomas Kesselheim, Brendan Lucier, Rebecca Reiffenh\"auser, Sahil Singla</dc:creator>
    </item>
    <item>
      <title>Approximating the shortest path problem with scenarios</title>
      <link>https://arxiv.org/abs/1806.08936</link>
      <description>arXiv:1806.08936v2 Announce Type: replace 
Abstract: This paper discusses the shortest path problem in a general directed graph with $n$ nodes and $K$ cost scenarios (objectives). In order to choose a solution, the min-max criterion is applied. The min-max version of the problem is hard to approximate within $\Omega(\log^{1-\epsilon} K)$ for any $\epsilon&gt;0$ unless NP$\subseteq \text{DTIME}(n^{\text{polylog} \,n})$ even for arc series-parallel graphs and within $\Omega(\log n/\log\log n)$ unless NP$\subseteq \text{ZPTIME}(n^{\log\log n})$ for acyclic graphs. The best approximation algorithm for the min-max shortest path problem in general graphs, known to date, has an approximation ratio of~$K$. In this paper, an $\widetilde{O}(\sqrt{n})$ flow LP-based approximation algorithm for min-max shortest path in general graphs is constructed. It is also shown that the approximation ratio obtained is close to an integrality gap of the corresponding flow LP relaxation.</description>
      <guid isPermaLink="false">oai:arXiv.org:1806.08936v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Kasperski, Pawel Zielinski</dc:creator>
    </item>
    <item>
      <title>Support Testing in the Huge Object Model</title>
      <link>https://arxiv.org/abs/2308.15988</link>
      <description>arXiv:2308.15988v3 Announce Type: replace 
Abstract: The Huge Object model is a distribution testing model in which we are given access to independent samples from an unknown distribution over the set of strings $\{0,1\}^n$, but are only allowed to query a few bits from the samples. We investigate the problem of testing whether a distribution is supported on $m$ elements in this model. It turns out that the behavior of this property is surprisingly intricate, especially when also considering the question of adaptivity.
  We prove lower and upper bounds for both adaptive and non-adaptive algorithms in the one-sided and two-sided error regime. Our bounds are tight when $m$ is fixed to a constant (and the distance parameter $\varepsilon$ is the only variable). For the general case, our bounds are at most $O(\log m)$ apart. In particular, our results show a surprising $O(\log \varepsilon^{-1})$ gap between the number of queries required for non-adaptive testing as compared to adaptive testing. For one sided error testing, we also show that a $O(\log m)$ gap between the number of samples and the number of queries is necessary. Our results utilize a wide variety of combinatorial and probabilistic methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.15988v3</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomer Adar, Eldar Fischer, Amit Levi</dc:creator>
    </item>
    <item>
      <title>An Objective Improvement Approach to Solving Discounted Payoff Games</title>
      <link>https://arxiv.org/abs/2404.04124</link>
      <description>arXiv:2404.04124v2 Announce Type: replace 
Abstract: While discounted payoff games and classic games that reduce to them, like parity and mean-payoff games, are symmetric, their solutions are not. We have taken a fresh view on the properties that optimal solutions need to have, and devised a novel way to converge to them, which is entirely symmetric. We achieve this by building a constraint system that uses every edge to define an inequation, and update the objective function by taking a single outgoing edge for each vertex into account. These edges loosely represent strategies of both players, where the objective function intuitively asks to make the inequation to these edges sharp. In fact, where they are not sharp, there is an `error' represented by the difference between the two sides of the inequation, which is 0 where the inequation is sharp. Hence, the objective is to minimise the sum of these errors. For co-optimal strategies, and only for them, it can be achieved that all selected inequations are sharp or, equivalently, that the sum of these errors is zero. While no co-optimal strategies have been found, we step-wise improve the error by improving the solution for a given objective function or by improving the objective function for a given solution. This also challenges the gospel that methods for solving payoff games are either based on strategy improvement or on value iteration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04124v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Dell'Erba, Arthur Dumas, Sven Schewe</dc:creator>
    </item>
    <item>
      <title>Computing the LZ-End parsing: Easy to implement and practically efficient</title>
      <link>https://arxiv.org/abs/2409.07840</link>
      <description>arXiv:2409.07840v2 Announce Type: replace 
Abstract: The LZ-End parsing [Kreft &amp; Navarro, 2011] of an input string yields compression competitive with the popular Lempel-Ziv 77 scheme, but also allows for efficient random access. Kempa and Kosolobov showed that the parsing can be computed in time and space linear in the input length [Kempa &amp; Kosolobov, 2017], however, the corresponding algorithm is hardly practical. We put the spotlight on their suboptimal algorithm that computes the parsing in time $\mathcal{O}(n \lg\lg n)$. It requires a comparatively small toolset and is therefore easy to implement, but at the same time very efficient in practice. We give a detailed and simplified description with a full listing that incorporates undocumented tricks from the original implementation, but also uses lazy evaluation to reduce the workload in practice and requires less working memory by removing a level of indirection. We legitimize our algorithm in a brief benchmark, obtaining the parsing faster than the state of the art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07840v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Dinklage</dc:creator>
    </item>
    <item>
      <title>Semidefinite programming and linear equations vs. homomorphism problems</title>
      <link>https://arxiv.org/abs/2311.00882</link>
      <description>arXiv:2311.00882v3 Announce Type: replace-cross 
Abstract: We introduce a relaxation for homomorphism problems that combines semidefinite programming with linear Diophantine equations, and propose a framework for the analysis of its power based on the spectral theory of association schemes. We use this framework to establish an unconditional lower bound against the semidefinite programming + linear equations model, by showing that the relaxation does not solve the approximate graph homomorphism problem and thus, in particular, the approximate graph colouring problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00882v3</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Ciardo, Stanislav \v{Z}ivn\'y</dc:creator>
    </item>
    <item>
      <title>Faster algorithms for the alignment of sparse correlated Erd\"os-R\'enyi random graphs</title>
      <link>https://arxiv.org/abs/2405.08421</link>
      <description>arXiv:2405.08421v2 Announce Type: replace-cross 
Abstract: The correlated Erd\"os-R\'enyi random graph ensemble is a probability law on pairs of graphs with $n$ vertices, parametrized by their average degree $\lambda$ and their correlation coefficient $s$. It can be used as a benchmark for the graph alignment problem, in which the labels of the vertices of one of the graphs are reshuffled by an unknown permutation; the goal is to infer this permutation and thus properly match the pairs of vertices in both graphs. A series of recent works has unveiled the role of Otter's constant $\alpha$ (that controls the exponential rate of growth of the number of unlabeled rooted trees as a function of their sizes) in this problem: for $s&gt;\sqrt{\alpha}$ and $\lambda$ large enough it is possible to recover in a time polynomial in $n$ a positive fraction of the hidden permutation. The exponent of this polynomial growth is however quite large and depends on the other parameters, which limits the range of applications of the algorithm. In this work we present a family of faster algorithms for this task, show through numerical simulations that their accuracy is only slightly reduced with respect to the original one, and conjecture that they undergo, in the large $\lambda$ limit, phase transitions at modified Otter's thresholds $\sqrt{\widehat{\alpha}}&gt;\sqrt{\alpha}$, with $\widehat{\alpha}$ related to the enumeration of a restricted family of trees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08421v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Muratori, Guilhem Semerjian</dc:creator>
    </item>
    <item>
      <title>Towards Instance-Optimal Euclidean Spanners</title>
      <link>https://arxiv.org/abs/2409.08227</link>
      <description>arXiv:2409.08227v2 Announce Type: replace-cross 
Abstract: Euclidean spanners are important geometric objects that have been extensively studied since the 1980s. The two most basic "compactness'' measures of a Euclidean spanner $E$ are the size (number of edges) $|E|$ and the weight (sum of edge weights) $\|E\|$. In this paper, we initiate the study of instance optimal Euclidean spanners. Our results are two-fold.
  We demonstrate that the greedy spanner is far from being instance optimal, even when allowing its stretch to grow. More concretely, we design two hard instances of point sets in the plane, where the greedy $(1+x \epsilon)$-spanner (for basically any parameter $x \geq 1$) has $\Omega_x(\epsilon^{-1/2}) \cdot |E_\mathrm{spa}|$ edges and weight $\Omega_x(\epsilon^{-1}) \cdot \|E_\mathrm{light}\|$, where $E_\mathrm{spa}$ and $E_\mathrm{light}$ denote the per-instance sparsest and lightest $(1+\epsilon)$-spanners, respectively, and the $\Omega_x$ notation suppresses a polynomial dependence on $1/x$.
  As our main contribution, we design a new construction of Euclidean spanners, which is inherently different from known constructions, achieving the following bounds: a stretch of $1+\epsilon\cdot 2^{O(\log^*(d/\epsilon))}$ with $O(1) \cdot |E_\mathrm{spa}|$ edges and weight $O(1) \cdot \|E_\mathrm{light}\|$. In other words, we show that a slight increase to the stretch suffices for obtaining instance optimality up to an absolute constant for both sparsity and lightness. Remarkably, there is only a log-star dependence on the dimension in the stretch, and there is no dependence on it whatsoever in the number of edges and weight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08227v2</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Wed, 18 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hung Le, Shay Solomon, Cuong Than, Csaba D. T\'oth, Tianyi Zhang</dc:creator>
    </item>
  </channel>
</rss>
