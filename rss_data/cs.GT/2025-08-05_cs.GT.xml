<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 06 Aug 2025 01:44:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Everyone Contributes! Incentivizing Strategic Cooperation in Multi-LLM Systems via Sequential Public Goods Games</title>
      <link>https://arxiv.org/abs/2508.02076</link>
      <description>arXiv:2508.02076v1 Announce Type: cross 
Abstract: Coordinating multiple large language models (LLMs) to solve complex tasks collaboratively poses a fundamental trade-off between the computation costs and collective performance compared with individual model. We introduce a novel, game-theoretically grounded reinforcement learning (RL) framework, the Multi-Agent Cooperation Sequential Public Goods Game (MAC-SPGG), to systematically incentivize cooperation in multi-LLM ensembles. In MAC-SPGG, LLM agents move in sequence, observing predecessors' outputs and updating beliefs to condition their own contributions. By redesigning the public-goods reward, effortful contributions become the unique Subgame Perfect Nash Equilibrium (SPNE), which eliminates free-riding under traditional SPGG or PGG. Its sequential protocol replaces costly round-based information exchanges with a streamlined decision flow, cutting communication overhead while retaining strategic depth. We prove the existence and uniqueness of the SPNE under realistic parameters, and empirically show that MAC-SPGG-trained ensembles outperform single-agent baselines, chain-of-thought prompting, and other cooperative methods, even achieving comparable performance to large-scale models across reasoning, math, code generation, and NLP tasks. Our results highlight the power of structured, incentive-aligned MAC-SPGG cooperation for scalable and robust multi-agent language generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.02076v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhao Liang, Yuan Qu, Jingyuan Yang, Shaochong Lin, Zuo-Jun Max Shen</dc:creator>
    </item>
    <item>
      <title>Logarithmic Comparison-Based Query Complexity for Fair Division of Indivisible Goods</title>
      <link>https://arxiv.org/abs/2404.18133</link>
      <description>arXiv:2404.18133v2 Announce Type: replace 
Abstract: We study the problem of fairly allocating $m$ indivisible goods to $n$ agents, where agents may have different preferences over the goods. In the traditional setting, agents' valuations are provided as inputs to the algorithm. In this paper, we adopt the query model, which has been widely considered for other similar problems (such as matching [Nis21], graph isomorphism [OS18], and equilibrium in game [Bab16]), and apply it to the fair division problem. In particular, we consider a new \emph{comparison-based query model}, where the algorithm presents two bundles of goods to an agent and the agent responds by telling the algorithm which bundle she prefers. We investigate the query complexity for computing allocations with several fairness notions, including \emph{proportionality up to one good} (PROP1), \emph{envy-freeness up to one good} (EF1), and \emph{maximin share} (MMS). Our main result is an algorithm that computes an allocation that satisfies both PROP1 and $\frac12$-MMS within $O(\log m)$ queries with a constant number of $n$ agents. For identical and additive valuation, we present an algorithm for computing an EF1 allocation within $O(\log m)$ queries with a constant number of $n$ agents. To complement the positive results, we show that the lower bound of the query complexity for any of the three fairness notions is $\Omega(\log m)$ even with two agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18133v2</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaolin Bu, Zihao Li, Shengxin Liu, Jiaxin Song, Biaoshuai Tao</dc:creator>
    </item>
    <item>
      <title>On the Power of Perturbation under Sampling in Solving Extensive-Form Games</title>
      <link>https://arxiv.org/abs/2501.16600</link>
      <description>arXiv:2501.16600v2 Announce Type: replace 
Abstract: We investigate how perturbation does and does not improve the Follow-the-Regularized-Leader (FTRL) algorithm in solving imperfect-information extensive-form games under sampling, where payoffs are estimated from sampled trajectories. While optimistic algorithms are effective under full feedback, they often become unstable in the presence of sampling noise. Payoff perturbation offers a promising alternative for stabilizing learning and achieving \textit{last-iterate convergence}. We present a unified framework for \textit{Perturbed FTRL} algorithms and study two variants: PFTRL-KL (standard KL divergence) and PFTRL-RKL (Reverse KL divergence), the latter featuring an estimator with both unbiasedness and conditional zero variance. While PFTRL-KL generally achieves equivalent or better performance across benchmark games, PFTRL-RKL consistently outperforms it in Leduc poker, whose structure is more asymmetric than the other games in a sense. Given the modest advantage of PFTRL-RKL, we design the second experiment to isolate the effect of conditional zero variance, showing that the variance-reduction property of RKL improve last-iterate performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16600v2</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wataru Masaka, Mitsuki Sakamoto, Kenshi Abe, Kaito Ariu, Tuomas Sandholm, Atsushi Iwasaki</dc:creator>
    </item>
    <item>
      <title>Learning in Structured Stackelberg Games</title>
      <link>https://arxiv.org/abs/2504.09006</link>
      <description>arXiv:2504.09006v2 Announce Type: replace 
Abstract: We study structured Stackelberg games, in which both players (the leader and the follower) observe contextual information about the state of the world at time of play. The leader plays against one of a finite number of followers, but the follower's type is not known until after the game has ended. Importantly, we assume a fixed relationship between the contextual information and the follower's type, thereby allowing the leader to leverage this additional structure when deciding her strategy. Under this setting, we find that standard learning theoretic measures of complexity do not characterize the difficulty of the leader's learning task. Instead, we introduce a new notion of dimension, the Stackelberg-Littlestone dimension, which we show characterizes the instance-optimal regret of the leader in the online setting. Based on this, we also provide a provably optimal learning algorithm. We extend our results to the distributional setting, where we use two new notions of dimension, the $\gamma$-Stackelberg-Natarajan dimension and $\gamma$-Stackelberg-Graph dimension. We prove that these control the sample complexity lower and upper bounds respectively, and we design a simple, improper algorithm that achieves the upper bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09006v2</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria-Florina Balcan, Kiriaki Fragkia, Keegan Harris</dc:creator>
    </item>
    <item>
      <title>Learning to Incentivize in Repeated Principal-Agent Problems with Adversarial Agent Arrivals</title>
      <link>https://arxiv.org/abs/2505.23124</link>
      <description>arXiv:2505.23124v2 Announce Type: replace 
Abstract: We initiate the study of a repeated principal-agent problem over a finite horizon $T$, where a principal sequentially interacts with $K\geq 2$ types of agents arriving in an adversarial order. At each round, the principal strategically chooses one of the $N$ arms to incentivize for an arriving agent of unknown type. The agent then chooses an arm based on its own utility and the provided incentive, and the principal receives a corresponding reward. The objective is to minimize regret against the best incentive in hindsight. Without prior knowledge of agent behavior, we show that the problem becomes intractable, leading to linear regret. We analyze two key settings where sublinear regret is achievable. In the first setting, the principal knows the arm each agent type would select greedily for any given incentive. Under this setting, we propose an algorithm that achieves a regret bound of $O(\min\{\sqrt{KT\log N},K\sqrt{T}\})$ and provide a matching lower bound up to a $\log K$ factor. In the second setting, an agent's response varies smoothly with the incentive and is governed by a Lipschitz constant $L\geq 1$. Under this setting, we show that there is an algorithm with a regret bound of $\tilde{O}((LN)^{1/3}T^{2/3})$ and establish a matching lower bound up to logarithmic factors. Finally, we extend our algorithmic results for both settings by allowing the principal to incentivize multiple arms simultaneously in each round.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.23124v2</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyan Liu, Arnab Maiti, Artin Tajdini, Kevin Jamieson, Lillian J. Ratliff</dc:creator>
    </item>
    <item>
      <title>Polynomial Expectation Property for Max-Polymatrix Games</title>
      <link>https://arxiv.org/abs/2506.01343</link>
      <description>arXiv:2506.01343v2 Announce Type: replace 
Abstract: We address an open problem on the computability of correlated equilibria in a variant of polymatrix where each player's utility is the maximum of their edge payoffs. We demonstrate that this max-variant game has the polynomial expectation property, and the results of \cite{papadimitriou2008computing} can thus be applied. We propose ideas for extending these findings to other variants of polymatrix games, as well as briefly address the broader question of necessity for the polynomial expectation property when computing correlated equilibria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01343v2</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Howard Dai</dc:creator>
    </item>
    <item>
      <title>Towards Replication-Robust Analytics Markets</title>
      <link>https://arxiv.org/abs/2310.06000</link>
      <description>arXiv:2310.06000v4 Announce Type: replace-cross 
Abstract: Despite recent advancements in machine learning, in practice, relevant datasets are often distributed among market competitors who are reluctant to share. To incentivize data sharing, recent works propose analytics markets, where multiple agents share features and are rewarded for improving the predictions of others. These rewards can be computed by treating features as players in a coalitional game, with solution concepts that yield desirable market properties. However, this setup incites agents to strategically replicate their data and act under multiple false identities to increase their own revenue and diminish that of others, limiting the viability of such markets in practice. In this work, we develop an analytics market robust to such strategic replication for supervised learning problems. We adopt Pearl's do-calculus from causal inference to refine the coalitional game by differentiating between observational and interventional conditional probabilities. As a result, we derive rewards that are replication-robust by design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06000v4</guid>
      <category>econ.GN</category>
      <category>cs.GT</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Thomas Falconer, Jalal Kazempour, Pierre Pinson</dc:creator>
    </item>
    <item>
      <title>Two-Person Additively-Separable Sum Games</title>
      <link>https://arxiv.org/abs/2507.19325</link>
      <description>arXiv:2507.19325v3 Announce Type: replace-cross 
Abstract: We consider a sub-class of bi-matrix games which we refer to as two-person (hereafter referred to as two-player) additively-separable sum (TPASS) games, where the sum of the pay-offs of the two players is additively separable. The row player's pay-off at each pair of pure strategies, is the sum of two numbers, the first of which may be dependent on the pure strategy chosen by the column player and the second being independent of the pure strategy chosen by the column player. The column player's pay-off at each pair of pure strategies, is also the sum of two numbers, the first of which may be dependent on the pure strategy chosen by the row player and the second being independent of the pure strategy chosen by the row player. The sum of the inter-dependent components of the pay-offs of the two players is assumed to be zero. We show that a (randomized or mixed) strategy pair is an equilibrium of the game if and only if there exist two other real numbers such that the three together solve a certain linear programming problem. In order to prove this result, we need to appeal to the existence of an equilibrium for the TPASS game. Before proving the desired result concerning the equivalence of the two sets, we provide a simple proof of the existence of equilibrium of TPASS games, using the strong duality theorem and the complementary slackness theorem of linear programming. We also show that any equilibrium for the TPASS game along with appropriate scalars solve the linear programming and its dual that is used to prove the existence result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19325v3</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <pubDate>Tue, 05 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Somdeb Lahiri</dc:creator>
    </item>
  </channel>
</rss>
