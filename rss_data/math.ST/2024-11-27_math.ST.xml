<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Nov 2024 02:55:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Information and Complexity Analysis of Spatial Data</title>
      <link>https://arxiv.org/abs/2411.16871</link>
      <description>arXiv:2411.16871v1 Announce Type: new 
Abstract: Information Theory provides a fundamental basis for analysis, and for a variety of subsequent methodological approaches, in relation to uncertainty quantification. The transversal character of concepts and derived results justifies its omnipresence in scientific research, in almost every area of knowledge, particularly in Physics, Communications, Geosciences, Life Sciences, etc. Information-theoretic aspects underlie modern developments on complexity and risk. A proper use and exploitation of structural characteristics inherent to spatial data motivates, according to the purpose, special considerations in this context. In this paper, some of the most relevant approaches introduced, in particular recent contributions and directions, regarding the informational analysis of spatial data and related aspects concerning complexity analysis, are reviewed under a conceptually connective evolutionary perspective. The discussion involves the cases of spatial data from magnitude measurements and spatial point patterns, with the latter possibly being of a multifractal nature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16871v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spasta.2020.100462</arxiv:DOI>
      <arxiv:journal_reference>Spatial Statistics, Volume 42, 100462 , 2021, ISSN 2211-6753</arxiv:journal_reference>
      <dc:creator>Jose M. Angulo, Francisco J. Esquivel, Ana E. Madrid, Francisco J. Alonso</dc:creator>
    </item>
    <item>
      <title>Optimal Estimation of Shared Singular Subspaces across Multiple Noisy Matrices</title>
      <link>https://arxiv.org/abs/2411.17054</link>
      <description>arXiv:2411.17054v1 Announce Type: new 
Abstract: Estimating singular subspaces from noisy matrices is a fundamental problem with wide-ranging applications across various fields. Driven by the challenges of data integration and multi-view analysis, this study focuses on estimating shared (left) singular subspaces across multiple matrices within a low-rank matrix denoising framework. A common approach for this task is to perform singular value decomposition on the stacked matrix (Stack-SVD), which is formed by concatenating all the individual matrices. We establish that Stack-SVD achieves minimax rate-optimality when the true (left) singular subspaces of the signal matrices are identical. Our analysis reveals some phase transition phenomena in the estimation problem as a function of the underlying signal-to-noise ratio, highlighting how the interplay among multiple matrices collectively determines the fundamental limits of estimation. We then tackle the more complex scenario where the true singular subspaces are only partially shared across matrices. For various cases of partial sharing, we rigorously characterize the conditions under which Stack-SVD remains effective, achieves minimax optimality, or fails to deliver consistent estimates, offering theoretical insights into its practical applicability. To overcome Stack-SVD's limitations in partial sharing scenarios, we propose novel estimators and an efficient algorithm to identify shared and unshared singular vectors, and prove their minimax rate-optimality. Extensive simulation studies and real-world data applications demonstrate the numerous advantages of our proposed approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17054v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhengchi Ma, Rong Ma</dc:creator>
    </item>
    <item>
      <title>Upper and lower bounds on the subgeometric convergence of adaptive Markov chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2411.17084</link>
      <description>arXiv:2411.17084v1 Announce Type: new 
Abstract: We investigate lower bounds on the subgeometric convergence of adaptive Markov chain Monte Carlo under any adaptation strategy. In particular, we prove general lower bounds in total variation and on the weak convergence rate under general adaptation plans. If the adaptation diminishes sufficiently fast, we also develop comparable convergence rate upper bounds that are capable of approximately matching the convergence rate in the subgeometric lower bound. These results provide insight into the optimal design of adaptation strategies and also limitations on the convergence behavior of adaptive Markov chain Monte Carlo. Applications to an adaptive unadjusted Langevin algorithm as well as adaptive Metropolis-Hastings with independent proposals and random-walk proposals are explored.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17084v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Austin Brown, Jeffrey S. Rosenthal</dc:creator>
    </item>
    <item>
      <title>On the Symmetry of Limiting Distributions of M-estimators</title>
      <link>https://arxiv.org/abs/2411.17087</link>
      <description>arXiv:2411.17087v1 Announce Type: new 
Abstract: Many functionals of interest in statistics and machine learning can be written as minimizers of expected loss functions. Such functionals are called $M$-estimands, and can be estimated by $M$-estimators -- minimizers of empirical average losses. Traditionally, statistical inference (e.g., hypothesis tests and confidence sets) for $M$-estimands is obtained by proving asymptotic normality of $M$-estimators centered at the target. However, asymptotic normality is only one of several possible limiting distributions and (asymptotically) valid inference becomes significantly difficult with non-normal limits. In this paper, we provide conditions for the symmetry of three general classes of limiting distributions, enabling inference using HulC (Kuchibhotla et al. (2024)).</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17087v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arunav Bhowmick, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Double robust estimation of functional outcomes with data missing at random</title>
      <link>https://arxiv.org/abs/2411.17224</link>
      <description>arXiv:2411.17224v1 Announce Type: new 
Abstract: We present and study semi-parametric estimators for the mean of functional outcomes in situations where some of these outcomes are missing and covariate information is available on all units. Assuming that the missingness mechanism depends only on the covariates (missing at random assumption), we present two estimators for the functional mean parameter, using working models for the functional outcome given the covariates, and the probability of missingness given the covariates. We contribute by establishing that both these estimators have Gaussian processes as limiting distributions and explicitly give their covariance functions. One of the estimators is double robust in the sense that the limiting distribution holds whenever at least one of the nuisance models is correctly specified. These results allow us to present simultaneous confidence bands for the mean function with asymptotically guaranteed coverage. A Monte Carlo study shows the finite sample properties of the proposed functional estimators and their associated simultaneous inference. The use of the method is illustrated in an application where the mean of counterfactual outcomes is targeted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17224v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xijia Liu, Kreske Ecker, Lina Schelin, Xavier de Luna</dc:creator>
    </item>
    <item>
      <title>Asymptotics for estimating a diverging number of parameters -- with and without sparsity</title>
      <link>https://arxiv.org/abs/2411.17395</link>
      <description>arXiv:2411.17395v1 Announce Type: new 
Abstract: We consider high-dimensional estimation problems where the number of parameters diverges with the sample size. General conditions are established for consistency, uniqueness, and asymptotic normality in both unpenalized and penalized estimation settings. The conditions are weak and accommodate a broad class of estimation problems, including ones with non-convex and group structured penalties. The wide applicability of the results is illustrated through diverse examples, including generalized linear models, multi-sample inference, and stepwise estimation procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17395v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jana Gauss, Thomas Nagler</dc:creator>
    </item>
    <item>
      <title>Improving the Convergence Rates of Forward Gradient Descent with Repeated Sampling</title>
      <link>https://arxiv.org/abs/2411.17567</link>
      <description>arXiv:2411.17567v1 Announce Type: new 
Abstract: Forward gradient descent (FGD) has been proposed as a biologically more plausible alternative of gradient descent as it can be computed without backward pass. Considering the linear model with $d$ parameters, previous work has found that the prediction error of FGD is, however, by a factor $d$ slower than the prediction error of stochastic gradient descent (SGD). In this paper we show that by computing $\ell$ FGD steps based on each training sample, this suboptimality factor becomes $d/(\ell \wedge d)$ and thus the suboptimality of the rate disappears if $\ell \gtrsim d.$ We also show that FGD with repeated sampling can adapt to low-dimensional structure in the input distribution. The main mathematical challenge lies in controlling the dependencies arising from the repeated sampling process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17567v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niklas Dexheimer, Johannes Schmidt-Hieber</dc:creator>
    </item>
    <item>
      <title>Rapid Bayesian Computation and Estimation for Neural Networks via Mixture Distributions</title>
      <link>https://arxiv.org/abs/2411.17667</link>
      <description>arXiv:2411.17667v1 Announce Type: new 
Abstract: This paper presents a Bayesian estimation procedure for single hidden-layer neural networks using $\ell_{1}$ controlled neuron weight vectors. We study the structure of the posterior density that makes it amenable to rapid sampling via Markov Chain Monte Carlo (MCMC), and statistical risk guarantees. Let the neural network have $K$ neurons with internal weights of dimension $d$ and fix the outer weights. With $N$ data observations, use a gain parameter or inverse temperature of $\beta$ in the posterior density.
  The posterior is intrinsically multimodal and not naturally suited to the rapid mixing of MCMC algorithms. For a continuous uniform prior over the $\ell_{1}$ ball, we demonstrate that the posterior density can be written as a mixture density where the mixture components are log-concave. Furthermore, when the number of parameters $Kd$ exceeds a constant times $(\beta N)^{2}\log(\beta N)$, the mixing distribution is also log-concave. Thus, neuron parameters can be sampled from the posterior by only sampling log-concave densities.
  For a discrete uniform prior restricted to a grid, we study the statistical risk (generalization error) of procedures based on the posterior. Using an inverse temperature that is a fractional power of $1/N$, $\beta = C \left[(\log d)/N\right]^{1/4}$, we demonstrate that notions of squared error are on the 4th root order $O(\left[(\log d)/N\right]^{1/4})$. If one further assumes independent Gaussian data with a variance $\sigma^{2} $ that matches the inverse temperature, $\beta = 1/\sigma^{2}$, we show Kullback divergence decays as an improved cube root power $O(\left[(\log d)/N\right]^{1/3})$.
  Future work aims to bridge the sampling ability of the continuous uniform prior with the risk control of the discrete uniform prior, resulting in a polynomial time Bayesian training algorithm for neural networks with statistical risk control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17667v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Curtis McDonald, Andrew R. Barron</dc:creator>
    </item>
    <item>
      <title>Measuring Statistical Evidence: A Short Report</title>
      <link>https://arxiv.org/abs/2411.16831</link>
      <description>arXiv:2411.16831v2 Announce Type: cross 
Abstract: This short text tried to establish a big picture of what evidential statistics is about and how an ideal inference method should behave. Moreover, by examining shortcomings of some of the currently used methods for measuring evidence and utilizing some intuitive principles, we motivated the Relative Belief Ratio as the primary method of characterizing statistical evidence. Number of topics has been omitted for the interest of this text and the reader is strongly advised to refer to (Evans, 2015) as the primary source for further readings of the subject.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16831v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Zamani</dc:creator>
    </item>
    <item>
      <title>Positive Definite Kernels and Random Sequences Connected to Polynomial Hypergroups</title>
      <link>https://arxiv.org/abs/2411.16864</link>
      <description>arXiv:2411.16864v1 Announce Type: cross 
Abstract: This work explores new classes of nonstationary stochastic sequences associated with polynomial hypergroups. Their covariance structures are analyzed through positive definite kernels and corresponding Hilbert spaces. Novel consistent estimators are introduced for deriving covariance structures from sequence realizations. A comprehensive prediction theory is developed, including a fast Levinson-type algorithm for efficiently calculating best linear predictors. Wiener-type theorems are established, enabling the detection of spectral measure atoms via generalized periodograms. Additional advancements, such as prediction with supplementary information, further enhance the scope of this study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16864v1</guid>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Volker H\"osel</dc:creator>
    </item>
    <item>
      <title>Conditional Extremes with Graphical Models</title>
      <link>https://arxiv.org/abs/2411.17013</link>
      <description>arXiv:2411.17013v1 Announce Type: cross 
Abstract: Multivariate extreme value analysis quantifies the probability and magnitude of joint extreme events. River discharges from the upper Danube River basin provide a challenging dataset for such analysis because the data, which is measured on a spatial network, exhibits both asymptotic dependence and asymptotic independence. To account for both features, we extend the conditional multivariate extreme value model (CMEVM) with a new approach for the residual distribution. This allows sparse (graphical) dependence structures and fully parametric prediction. Our approach fills a current gap in statistical methodology for graphical extremes, where existing models require asymptotic independence. Further, the model can be used to learn the graphical dependence structure when it is unknown a priori. To support inference in high dimensions, we propose a stepwise inference procedure that is computationally efficient and loses no information or predictive power. We show our method is flexible and accurately captures the extremal dependence for the upper Danube River basin discharges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17013v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aiden Farrell, Emma F. Eastoe, Clement Lee</dc:creator>
    </item>
    <item>
      <title>On the maximal correlation of some stochastic processes</title>
      <link>https://arxiv.org/abs/2411.17109</link>
      <description>arXiv:2411.17109v1 Announce Type: cross 
Abstract: We consider the maximal correlation coefficient $R(X,Y)$ between two stochastic processes $X$ and $Y$. When $(X,Y)$ is a random walk, the result is a consequence of Cs\'{a}ki-Fischer identity and lower-semi continuity of $\text{Law}(X,Y)\to R(X,Y)$. When $(X,Y)$ are two-dimensional L\'{e}vy processes, we give an expression of $R(X,Y)$ via the covariance $\Sigma$ and the L\'{e}vy measure $\nu$ appeared in the L\'{e}vy-Khinchine formula. As a consequence, for two-dimensional $\alpha$-stable random variables $(X,Y)$ with $0&lt;\alpha&lt;2$, we give an expression of $R(X,Y)$ via the spectral measure of the $\alpha$-stable distribution. We also prove analogs and generalizations of Dembo-Kagan-Shepp-Yu inequality and Madiman-Barron inequality. Roughly speaking, we investigate the maximal correlation coefficient between two random selected ``subvectors'' $Y$ and $Z$ of a common random vector $X$. Besides, by using above new results, we recover several classical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17109v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinshan Chang, Qinwei Chen</dc:creator>
    </item>
    <item>
      <title>Learning Hierarchical Polynomials of Multiple Nonlinear Features with Three-Layer Networks</title>
      <link>https://arxiv.org/abs/2411.17201</link>
      <description>arXiv:2411.17201v1 Announce Type: cross 
Abstract: In deep learning theory, a critical question is to understand how neural networks learn hierarchical features. In this work, we study the learning of hierarchical polynomials of \textit{multiple nonlinear features} using three-layer neural networks. We examine a broad class of functions of the form $f^{\star}=g^{\star}\circ \bp$, where $\bp:\mathbb{R}^{d} \rightarrow \mathbb{R}^{r}$ represents multiple quadratic features with $r \ll d$ and $g^{\star}:\mathbb{R}^{r}\rightarrow \mathbb{R}$ is a polynomial of degree $p$. This can be viewed as a nonlinear generalization of the multi-index model \citep{damian2022neural}, and also an expansion upon previous work that focused only on a single nonlinear feature, i.e. $r = 1$ \citep{nichani2023provable,wang2023learning}.
  Our primary contribution shows that a three-layer neural network trained via layerwise gradient descent suffices for
  \begin{itemize}\item complete recovery of the space spanned by the nonlinear features
  \item efficient learning of the target function $f^{\star}=g^{\star}\circ \bp$ or transfer learning of $f=g\circ \bp$ with a different link function
  \end{itemize} within $\widetilde{\cO}(d^4)$ samples and polynomial time. For such hierarchical targets, our result substantially improves the sample complexity ${\Theta}(d^{2p})$ of the kernel methods, demonstrating the power of efficient feature learning. It is important to highlight that{ our results leverage novel techniques and thus manage to go beyond all prior settings} such as single-index and multi-index models as well as models depending just on one nonlinear feature, contributing to a more comprehensive understanding of feature learning in deep learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17201v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hengyu Fu, Zihao Wang, Eshaan Nichani, Jason D. Lee</dc:creator>
    </item>
    <item>
      <title>Valid Bayesian Inference based on Variance Weighted Projection for High-Dimensional Logistic Regression with Binary Covariates</title>
      <link>https://arxiv.org/abs/2411.17618</link>
      <description>arXiv:2411.17618v1 Announce Type: cross 
Abstract: We address the challenge of conducting inference for a categorical treatment effect related to a binary outcome variable while taking into account high-dimensional baseline covariates. The conventional technique used to establish orthogonality for the treatment effect from nuisance variables in continuous cases is inapplicable in the context of binary treatment. To overcome this obstacle, an orthogonal score tailored specifically to this scenario is formulated which is based on a variance-weighted projection. Additionally, a novel Bayesian framework is proposed to facilitate valid inference for the desired low-dimensional parameter within the complex framework of high-dimensional logistic regression. We provide uniform convergence results, affirming the validity of credible intervals derived from the posterior distribution. The effectiveness of the proposed method is demonstrated through comprehensive simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17618v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhishek Ojha, Naveen N. Narisetty</dc:creator>
    </item>
    <item>
      <title>Confidence surfaces for the mean of locally stationary functional time series</title>
      <link>https://arxiv.org/abs/2109.03641</link>
      <description>arXiv:2109.03641v3 Announce Type: replace 
Abstract: The problem of constructing a simultaneous confidence surface for the 2-dimensional mean function of a non-stationary functional time series is challenging as these bands can not be built on classical limit theory for the maximum absolute deviation between an estimate and the time-dependent regression function. In this paper, we propose a new bootstrap methodology to construct such a region. Our approach is based on a Gaussian approximation for the maximum norm of sparse high-dimensional vectors approximating the maximum absolute deviation which is suitable for nonparametric inference of high-dimensional time series. The elimination of the zero entries produces (besides the time dependence) additional dependencies such that the "classical" multiplier bootstrap is not applicable. To solve this issue we develop a novel multiplier bootstrap, where blocks of the coordinates of the vectors are multiplied with random variables, which mimic the specific structure between the vectors appearing in the Gaussian approximation. We prove the validity of our approach by asymptotic theory, demonstrate good finite sample properties by means of a simulation study and illustrate its applicability by analyzing a data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.03641v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Holger Dette, Weichi Wu</dc:creator>
    </item>
    <item>
      <title>Optimal sub-Gaussian variance proxy for truncated Gaussian and exponential random variables</title>
      <link>https://arxiv.org/abs/2403.08628</link>
      <description>arXiv:2403.08628v2 Announce Type: replace 
Abstract: This paper establishes the optimal sub-Gaussian variance proxy for truncated Gaussian and truncated exponential random variables. The proofs rely on first characterizing the optimal variance proxy as the unique solution to a set of two equations and then observing that for these two truncated distributions, one may find explicit solutions to this set of equations. Moreover, we establish the conditions under which the optimal variance proxy coincides with the variance, thereby characterizing the strict sub-Gaussianity of the truncated random variables. Specifically, we demonstrate that truncated Gaussian variables exhibit strict sub-Gaussian behavior if and only if they are symmetric, meaning their truncation is symmetric with respect to the mean. Conversely, truncated exponential variables are shown to never exhibit strict sub-Gaussian properties. These findings contribute to the understanding of these prevalent probability distributions in statistics and machine learning, providing a valuable foundation for improved and optimal modeling and decision-making processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08628v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathias Barreto, Olivier Marchal, Julyan Arbel</dc:creator>
    </item>
    <item>
      <title>Limiting Spectral Distribution of a Random Commutator Matrix</title>
      <link>https://arxiv.org/abs/2409.16780</link>
      <description>arXiv:2409.16780v2 Announce Type: replace 
Abstract: We study the spectral properties of a class of random matrices of the form $S_n^{-} = n^{-1}(X_1 X_2^* - X_2 X_1^*)$ where $X_k = \Sigma^{1/2}Z_k$, for $k=1,2$, $Z_k$'s are independent $p\times n$ complex-valued random matrices, and $\Sigma$ is a $p\times p$ positive semi-definite matrix, independent of the $Z_k$'s. We assume that $Z_k$'s have independent entries with zero mean and unit variance. The skew-symmetric/skew-Hermitian matrix $S_n^{-}$ will be referred to as a random commutator matrix associated with the samples $X_1$ and $X_2$. We show that, when the dimension $p$ and sample size $n$ increase simultaneously, so that $p/n \to c \in (0,\infty)$, there exists a limiting spectral distribution (LSD) for $S_n^{-}$, supported on the imaginary axis, under the assumptions that the spectral distribution of $\Sigma$ converges weakly and the entries of $Z_k$'s have moments of sufficiently high order. This nonrandom LSD can be described through its Stieltjes transform, which satisfies a coupled Mar\v{c}enko-Pastur-type functional equations. In the special case when $\Sigma = I_p$, we show that the LSD of $S_n^{-}$ is a mixture of a degenerate distribution at zero (with positive mass if $c &gt; 2$), and a continuous distribution with a symmetric density function supported on a compact interval on the imaginary axis. Moreover, we show that the companion matrix $S_n^{+} = \Sigma_n^\frac{1}{2}(Z_1Z_2^* + Z_2Z_1^*)\Sigma_n^\frac{1}{2}$, under identical assumptions, has an LSD supported on the real line, which can be similarly characterized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16780v2</guid>
      <category>math.ST</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Javed Hazarika, Debashis Paul</dc:creator>
    </item>
    <item>
      <title>Label Noise Robustness of Conformal Prediction</title>
      <link>https://arxiv.org/abs/2209.14295</link>
      <description>arXiv:2209.14295v3 Announce Type: replace-cross 
Abstract: We study the robustness of conformal prediction, a powerful tool for uncertainty quantification, to label noise. Our analysis tackles both regression and classification problems, characterizing when and how it is possible to construct uncertainty sets that correctly cover the unobserved noiseless ground truth labels. We further extend our theory and formulate the requirements for correctly controlling a general loss function, such as the false negative proportion, with noisy labels. Our theory and experiments suggest that conformal prediction and risk-controlling techniques with noisy labels attain conservative risk over the clean ground truth labels whenever the noise is dispersive and increases variability. In other adversarial cases, we can also correct for noise of bounded size in the conformal prediction algorithm in order to ensure achieving the correct risk of the ground truth labels without score or data regularity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.14295v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bat-Sheva Einbinder, Shai Feldman, Stephen Bates, Anastasios N. Angelopoulos, Asaf Gendler, Yaniv Romano</dc:creator>
    </item>
    <item>
      <title>Investigating Self-Supervised Image Denoising with Denaturation</title>
      <link>https://arxiv.org/abs/2405.01124</link>
      <description>arXiv:2405.01124v3 Announce Type: replace-cross 
Abstract: Self-supervised learning for image denoising problems in the presence of denaturation for noisy data is a crucial approach in machine learning. However, theoretical understanding of the performance of the approach that uses denatured data is lacking. To provide better understanding of the approach, in this paper, we analyze a self-supervised denoising algorithm that uses denatured data in depth through theoretical analysis and numerical experiments. Through the theoretical analysis, we discuss that the algorithm finds desired solutions to the optimization problem with the population risk, while the guarantee for the empirical risk depends on the hardness of the denoising task in terms of denaturation levels. We also conduct several experiments to investigate the performance of an extended algorithm in practice. The results indicate that the algorithm training with denatured images works, and the empirical performance aligns with the theoretical results. These results suggest several insights for further improvement of self-supervised image denoising that uses denatured data in future directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01124v3</guid>
      <category>stat.ML</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroki Waida, Kimihiro Yamazaki, Atsushi Tokuhisa, Mutsuyo Wada, Yuichiro Wada</dc:creator>
    </item>
    <item>
      <title>Statistical algorithms for low-frequency diffusion data: A PDE approach</title>
      <link>https://arxiv.org/abs/2405.01372</link>
      <description>arXiv:2405.01372v2 Announce Type: replace-cross 
Abstract: We consider the problem of making nonparametric inference in a class of multi-dimensional diffusions in divergence form, from low-frequency data. Statistical analysis in this setting is notoriously challenging due to the intractability of the likelihood and its gradient, and computational methods have thus far largely resorted to expensive simulation-based techniques. In this article, we propose a new computational approach which is motivated by PDE theory and is built around the characterisation of the transition densities as solutions of the associated heat (Fokker-Planck) equation. Employing optimal regularity results from the theory of parabolic PDEs, we prove a novel characterisation for the gradient of the likelihood. Using these developments, for the nonlinear inverse problem of recovering the diffusivity, we then show that the numerical evaluation of the likelihood and its gradient can be reduced to standard elliptic eigenvalue problems, solvable by powerful finite element methods. This enables the efficient implementation of a large class of popular statistical algorithms, including (i) preconditioned Crank-Nicolson and Langevin-type methods for posterior sampling, and (ii) gradient-based descent optimisation schemes to compute maximum likelihood and maximum-a-posteriori estimates. We showcase the effectiveness of these methods via extensive simulation studies in a nonparametric Bayesian model with Gaussian process priors, in which both the proposed optimisation and sampling schemes provide good numerical recovery. The reproducible code is available online at https://github.com/MattGiord/LF-Diffusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01372v2</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Giordano, Sven Wang</dc:creator>
    </item>
    <item>
      <title>Spatial Proportional Hazards Model with Differential Regularization</title>
      <link>https://arxiv.org/abs/2410.13420</link>
      <description>arXiv:2410.13420v3 Announce Type: replace-cross 
Abstract: This paper presents a semiparametric proportional hazards model designed to handle spatially varying covariate functions, applicable to both geostatistical and areal data observed on irregular spatial domains. The model is estimated through the maximization of a penalized partial likelihood, with a roughness penalty term based on a differential of the spatial field over the target domain. The finite element method is employed for efficient estimation, enabling a piecewise polynomial surface representation of the spatial field. We apply this method to analyze response time data from the London Fire Brigade.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13420v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Tedesco</dc:creator>
    </item>
    <item>
      <title>Dense ReLU Neural Networks for Temporal-spatial Model</title>
      <link>https://arxiv.org/abs/2411.09961</link>
      <description>arXiv:2411.09961v3 Announce Type: replace-cross 
Abstract: In this paper, we focus on fully connected deep neural networks utilizing the Rectified Linear Unit (ReLU) activation function for nonparametric estimation. We derive non-asymptotic bounds that lead to convergence rates, addressing both temporal and spatial dependence in the observed measurements. By accounting for dependencies across time and space, our models better reflect the complexities of real-world data, enhancing both predictive performance and theoretical robustness. We also tackle the curse of dimensionality by modeling the data on a manifold, exploring the intrinsic dimensionality of high-dimensional data. We broaden existing theoretical findings of temporal-spatial analysis by applying them to neural networks in more general contexts and demonstrate that our proof techniques are effective for models with short-range dependence. Our empirical simulations across various synthetic response functions underscore the superior performance of our method, outperforming established approaches in the existing literature. These findings provide valuable insights into the strong capabilities of dense neural networks for temporal-spatial modeling across a broad range of function classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09961v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhi Zhang, Carlos Misael Madrid Padilla, Xiaokai Luo, Oscar Hernan Madrid Padilla, Daren Wang</dc:creator>
    </item>
    <item>
      <title>Debiased Regression for Root-N-Consistent Conditional Mean Estimation</title>
      <link>https://arxiv.org/abs/2411.11748</link>
      <description>arXiv:2411.11748v3 Announce Type: replace-cross 
Abstract: This study introduces a debiasing method for regression estimators, including high-dimensional and nonparametric regression estimators. For example, nonparametric regression methods allow for the estimation of regression functions in a data-driven manner with minimal assumptions; however, these methods typically fail to achieve $\sqrt{n}$-consistency in their convergence rates, and many, including those in machine learning, lack guarantees that their estimators asymptotically follow a normal distribution. To address these challenges, we propose a debiasing technique for nonparametric estimators by adding a bias-correction term to the original estimators, extending the conventional one-step estimator used in semiparametric analysis. Specifically, for each data point, we estimate the conditional expected residual of the original nonparametric estimator, which can, for instance, be computed using kernel (Nadaraya-Watson) regression, and incorporate it as a bias-reduction term. Our theoretical analysis demonstrates that the proposed estimator achieves $\sqrt{n}$-consistency and asymptotic normality under a mild convergence rate condition for both the original nonparametric estimator and the conditional expected residual estimator. Notably, this approach remains model-free as long as the original estimator and the conditional expected residual estimator satisfy the convergence rate condition. The proposed method offers several advantages, including improved estimation accuracy and simplified construction of confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11748v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
  </channel>
</rss>
