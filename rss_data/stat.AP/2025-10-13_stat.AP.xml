<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 13 Oct 2025 04:01:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Quantifying Very Extreme Precipitation and Temperature Using Huge Ensembles Generated by Machine Learning-based Climate Model Emulators</title>
      <link>https://arxiv.org/abs/2510.08893</link>
      <description>arXiv:2510.08893v1 Announce Type: new 
Abstract: Weather extremes produce major impacts on society and ecosystems and are likely to change in likelihood and magnitude with climate change. However, very low probability events are hard to characterize statistically using observations or climate model output because of short records/runs. For precipitation, consideration of such events arises in quantifying Probable Maximum Precipitation (PMP), namely estimating extreme precipitation magnitudes for designing and assessing critical infrastructure. A recent National Academies report on modernizing PMP estimation proposed using huge climate model-based ensembles to estimate extreme quantiles, possibly through machine learning-based ensemble boosting. Here we assess such an approach for the contiguous United States using a huge ensemble (10560 years) from a state-of-the-art emulator (ACE2) trained on ERA5 reanalysis. The results indicate that one can practically estimate very extreme precipitation and temperature quantiles using appropriate statistical extreme value techniques. More specifically, the results provide evidence for (1) the use of threshold-exceedance methods with a sufficiently high threshold for reliable estimation (necessary for precipitation), (2) the robustness of results to variations in extremes by season and storm type, and (3) well-constrained statistical uncertainty. Our results also show that the emulator produces extremes outside the range of the ERA5 training data. While this suggests that such emulators have potential for quantifying the climatology of extremes, we do not extensively investigate if this particular emulator is fit for purpose. Our focus is on how to use huge ensembles to estimate very extreme statistics, and we expect the results to be relevant for future improved emulators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08893v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher J. Paciorek, Daniel Cooley</dc:creator>
    </item>
    <item>
      <title>Natural Disaster In Canada (2024)</title>
      <link>https://arxiv.org/abs/2510.08594</link>
      <description>arXiv:2510.08594v1 Announce Type: cross 
Abstract: This paper is a follow-up to our earlier study, Natural Disasters in Canada (2017). We analyze the Canadian Disaster Database (CDD) to examine the frequency and severity of various natural disasters over the past 120 years and to identify emerging trends. We generate annual loss distributions for individual disaster types, as well as an aggregate annual loss distribution across all event types. Our analysis provides evidence that Canada is experiencing warmer and wetter conditions and indicates a substantial likelihood of extreme national-level losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08594v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H. Hao</dc:creator>
    </item>
    <item>
      <title>Mapping Socio-Economic Divides with Urban Mobility Data</title>
      <link>https://arxiv.org/abs/2510.08598</link>
      <description>arXiv:2510.08598v1 Announce Type: cross 
Abstract: The massive digital footprints generated by bike-sharing systems in megacities like Shanghai offer a novel perspective on the urban socio-economic fabric. This study investigates whether these daily mobility patterns can quantitatively map the city's underlying social stratification. To overcome the persistent challenge of acquiring fine-grained socio-economic data, we constructed a multi-layered analytical dataset. We annotated 2,000 raw bike trips with local economic attributes, derived from a novel data enrichment methodology that employs a Large Language Model (LLM), and integrated contextual features of the built environment. A Random Forest model was then utilized as an interpretable framework to determine the key factors governing the relationship between mobility behavior and local economic status. The analysis reveals a compelling and unambiguous finding: a neighborhood's economic level, proxied by housing prices, is the single most dominant predictor of its bike-sharing patterns, substantially outweighing other geographic or temporal factors. This economic determinism manifests in three distinct ways: (1) a spatial clustering of resources, a phenomenon we term the \textit{club effect}, which concentrates mobility infrastructure and usage in affluent areas; (2) a functional dichotomy between necessity-driven, utilitarian usage in lower-income zones and flexible, recreational usage in wealthier ones; and (3) a nuanced inverted U-shaped adoption curve that identifies the urban middle class as the system's primary user base.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08598v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingche Liu, Mengyang Li</dc:creator>
    </item>
    <item>
      <title>A Design-based Solution for Causal Inference with Text: Can a Language Model Be Too Large?</title>
      <link>https://arxiv.org/abs/2510.08758</link>
      <description>arXiv:2510.08758v1 Announce Type: cross 
Abstract: Many social science questions ask how linguistic properties causally affect an audience's attitudes and behaviors. Because text properties are often interlinked (e.g., angry reviews use profane language), we must control for possible latent confounding to isolate causal effects. Recent literature proposes adapting large language models (LLMs) to learn latent representations of text that successfully predict both treatment and the outcome. However, because the treatment is a component of the text, these deep learning methods risk learning representations that actually encode the treatment itself, inducing overlap bias. Rather than depending on post-hoc adjustments, we introduce a new experimental design that handles latent confounding, avoids the overlap issue, and unbiasedly estimates treatment effects. We apply this design in an experiment evaluating the persuasiveness of expressing humility in political communication. Methodologically, we demonstrate that LLM-based methods perform worse than even simple bag-of-words models using our real text and outcomes from our experiment. Substantively, we isolate the causal effect of expressing humility on the perceived persuasiveness of political statements, offering new insights on communication effects for social media platforms, policy makers, and social scientists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08758v1</guid>
      <category>stat.ME</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Graham Tierney, Srikar Katta, Christopher Bail, Sunshine Hillygus, Alexander Volfovsky</dc:creator>
    </item>
    <item>
      <title>Multidimensional Poverty Mapping for Small Areas</title>
      <link>https://arxiv.org/abs/2510.08898</link>
      <description>arXiv:2510.08898v1 Announce Type: cross 
Abstract: Many countries measure poverty based only on income or consumption. However, there is a growing awareness of measuring poverty through multiple dimensions that captures a more reasonable status of poverty. Estimating poverty measure(s) for small geographical areas, commonly referred to as poverty mapping, is challenging due to small or no sample for the small areas. While there is a huge literature available on unidimensional poverty mapping, only a limited effort has been made to address special challenges that arise only in the multidimensional poverty mapping. For example, in multidimensional poverty mapping, a new problem arises involving estimation of relative contributions of different dimensions to overall poverty for small areas. This problem has been grossly ignored in the small area estimation (SAE) literature. We address this issue using a multivariate hierarchical model implemented via a Bayesian method. Moreover, we demonstrate how a multidimensional poverty composite measure can be estimated for small areas. In this paper, we demonstrate our proposed methodology using a survey data specially designed by one of us for multidimensional poverty mapping. This paper adds a new direction to poverty mapping literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08898v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soumojit Das, Dilshanie Deepawansa, Partha Lahiri</dc:creator>
    </item>
    <item>
      <title>The bixplot: A variation on the boxplot suited for bimodal data</title>
      <link>https://arxiv.org/abs/2510.09276</link>
      <description>arXiv:2510.09276v1 Announce Type: cross 
Abstract: Boxplots and related visualization methods are widely used exploratory tools for taking a first look at collections of univariate variables. In this note an extension is provided that is specifically designed to detect and display bimodality and multimodality when the data warrant it. For this purpose a univariate clustering method is constructed that ensures contiguous clusters, meaning that no cluster has members inside another cluster, and such that each cluster contains at least a given number of unique members. The resulting bixplot display facilitates the identification and interpretation of potentially meaningful subgroups underlying the data. The bixplot also displays the individual data values, which can draw attention to isolated points. Implementations of the bixplot are available in both Python and R, and their many options are illustrated on several real datasets. For instance, an external variable can be visualized by color gradations inside the display.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09276v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Camille M. Montalcini, Peter J. Rousseeuw</dc:creator>
    </item>
    <item>
      <title>Integrated Analysis for Electronic Health Records with Structured and Sporadic Missingness</title>
      <link>https://arxiv.org/abs/2506.09208</link>
      <description>arXiv:2506.09208v2 Announce Type: replace 
Abstract: Objectives: We propose a novel imputation method tailored for Electronic Health Records (EHRs) with structured and sporadic missingness. Such missingness frequently arises in the integration of heterogeneous EHR datasets for downstream clinical applications. By addressing these gaps, our method provides a practical solution for integrated analysis, enhancing data utility and advancing the understanding of population health.
  Materials and Methods: We begin by demonstrating structured and sporadic missing mechanisms in the integrated analysis of EHR data. Following this, we introduce a novel imputation framework, Macomss, specifically designed to handle structurally and heterogeneously occurring missing data. We establish theoretical guarantees for Macomss, ensuring its robustness in preserving the integrity and reliability of integrated analyses. To assess its empirical performance, we conduct extensive simulation studies that replicate the complex missingness patterns observed in real-world EHR systems, complemented by validation using EHR datasets from the Duke University Health System (DUHS).
  Results: Simulation studies show that our approach consistently outperforms existing imputation methods. Using datasets from three hospitals within DUHS, Macomss achieves the lowest imputation errors for missing data in most cases and provides superior or comparable downstream prediction performance compared to benchmark methods.
  Conclusions: We provide a theoretically guaranteed and practically meaningful method for imputing structured and sporadic missing data, enabling accurate and reliable integrated analysis across multiple EHR datasets. The proposed approach holds significant potential for advancing research in population health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09208v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jianbin Tan, Yan Zhang, Chuan Hong, T. Tony Cai, Tianxi Cai, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Direct-Assisted Bayesian Unit-level Modeling for Small Area Estimation of Rare Event Prevalence</title>
      <link>https://arxiv.org/abs/2408.16129</link>
      <description>arXiv:2408.16129v3 Announce Type: replace-cross 
Abstract: Small area estimation using survey data can be achieved by using either a design-based or a model-based inferential approach. Design-based direct estimators are generally preferable because of their consistency, asymptotic normality, and reliance on fewer assumptions. However, when data are sparse at the desired area level, as is often the case when measuring rare events, these direct estimators can have extremely large uncertainty, making a model-based approach preferable. A model-based approach with a random spatial effect borrows information from surrounding areas at the cost of inducing shrinkage. As a result, estimates may be over-smoothed and inconsistent with design-based estimates at higher area levels when aggregated. We propose two unit-level Bayesian models for small area estimation of rare event prevalence which use design-based direct estimates at a higher area level to increase consistency in aggregation. This model framework is designed to accommodate sparse data obtained from two-stage stratified cluster sampling, which is particularly relevant to applications in low- and middle-income countries. After introducing the model framework and its implementation, we conduct a simulation study to evaluate its properties and apply it to the estimation of the neonatal mortality rate in Zambia, using 2014 Demographic Health Surveys data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16129v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alana McGovern, Katherine Wilson, Jon Wakefield</dc:creator>
    </item>
    <item>
      <title>A tutorial on optimal dynamic treatment regimes</title>
      <link>https://arxiv.org/abs/2502.16988</link>
      <description>arXiv:2502.16988v2 Announce Type: replace-cross 
Abstract: A dynamic treatment regime is a sequence of treatment decision rules tailored to an individual's evolving status over time. In precision medicine, much focus has been placed on finding an optimal dynamic treatment regime which, if followed by everyone in the population, would yield the best outcome on average; and extensive investigation has been conducted from both methodological and applications standpoints. The aim of this tutorial is to provide readers who are interested in optimal dynamic treatment regimes with a systematic, detailed but accessible introduction, including the formal definition and formulation of this topic within the framework of causal inference, identification assumptions required to link the causal quantity of interest to the observed data, existing statistical models and estimation methods to learn the optimal regime from data, and application of these methods to both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16988v2</guid>
      <category>stat.OT</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chunyu Wang, Brian DM Tom</dc:creator>
    </item>
    <item>
      <title>A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and Opportunities for Further Improvement</title>
      <link>https://arxiv.org/abs/2509.19088</link>
      <description>arXiv:2509.19088v2 Announce Type: replace-cross 
Abstract: Digital representations of individuals ("digital twins") promise to transform social science and decision-making. Yet it remains unclear whether such twins truly mirror the people they emulate. We conducted 19 preregistered studies with a representative U.S. panel and their digital twins, each constructed from rich individual-level data, enabling direct comparisons between human and twin behavior across a wide range of domains and stimuli (including never-seen-before ones). Twins reproduced individual responses with 75% accuracy and seemingly low correlation with human answers (approximately 0.2). However, this apparently high accuracy was no higher than that achieved by generic personas based on demographics only. In contrast, correlation improved when twins incorporated detailed personal information, even outperforming traditional machine learning benchmarks that require additional data. Twins exhibited systematic strengths and weaknesses - performing better in social and personality domains, but worse in political ones - and were more accurate for participants with higher education, higher income, and moderate political views and religious attendance. Together, these findings delineate both the promise and the current limits of digital twins: they capture some relative differences among individuals but not yet the unique judgments of specific people. All data and code are publicly available to support the further development and evaluation of digital twin pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19088v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tiany Peng, George Gui, Daniel J. Merlau, Grace Jiarui Fan, Malek Ben Sliman, Melanie Brucks, Eric J. Johnson, Vicki Morwitz, Abdullah Althenayyan, Silvia Bellezza, Dante Donati, Hortense Fong, Elizabeth Friedman, Ariana Guevara, Mohamed Hussein, Kinshuk Jerath, Bruce Kogut, Akshit Kumar, Kristen Lane, Hannah Li, Patryk Perkowski, Oded Netzer, Olivier Toubia</dc:creator>
    </item>
  </channel>
</rss>
