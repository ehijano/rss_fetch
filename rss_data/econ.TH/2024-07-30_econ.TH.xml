<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.TH</link>
    <description>econ.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 31 Jul 2024 01:47:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Getting the Agent to Wait</title>
      <link>https://arxiv.org/abs/2407.19127</link>
      <description>arXiv:2407.19127v1 Announce Type: new 
Abstract: We examine the strategic interaction between an expert (principal) maximizing engagement and an agent seeking swift information. Our analysis reveals: When priors align, relative patience determines optimal disclosure -- impatient agents induce gradual revelation, while impatient principals cause delayed, abrupt revelation. When priors disagree, catering to the bias often emerges, with the principal initially providing signals aligned with the agent's bias. With private agent beliefs, we observe two phases: one engaging both agents, followed by catering to one type. Comparing personalized and non-personalized strategies, we find faster information revelation in the non-personalized case, but higher quality information in the personalized case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19127v1</guid>
      <category>econ.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maryam Saeedi, Yikang Shen, Ali Shourideh</dc:creator>
    </item>
    <item>
      <title>Certifying Lemons</title>
      <link>https://arxiv.org/abs/2407.19814</link>
      <description>arXiv:2407.19814v1 Announce Type: new 
Abstract: This paper examines an adverse selection environment where a sender with private information (high or low ability) tries to convince a receiver of their high ability. Without commitment or costly signaling, market failure can occur. Certification intermediaries reduce these frictions by enabling signaling through hard information. This paper focuses on a monopolistic certifier and its impact on equilibrium welfare and certificate design. Key findings show that the certifier often provides minimal information, pooling senders of varying abilities and leaving low rents for high-type senders, which typically disadvantages the receiver. However, when precise information is demanded, the certifier screens the sender perfectly, benefiting the receiver. Thus, the monopolistic intermediary has an ambiguous effect on market efficiency. The results emphasize the importance of high certification standards, which drive low-ability senders out of the market. Conditions for such equilibria are characterized, showing how simple threshold strategies by the receiver induce first-best outcomes. Additionally, the relationship between the characteristics of offered certificates and welfare is identified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19814v1</guid>
      <category>econ.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hershdeep Chopra</dc:creator>
    </item>
    <item>
      <title>Unimprovable Students and Inequality in School Choice</title>
      <link>https://arxiv.org/abs/2407.19831</link>
      <description>arXiv:2407.19831v1 Announce Type: new 
Abstract: The Efficiency-Adjusted Deferred Acceptance (EADA) mechanism corrects the Pareto-inefficiency of the celebrated Deferred Acceptance (DA) algorithm, assigning every student to a weakly more preferred school. Nonetheless, it is unclear which and how many students do not improve their DA placement under EADA.
  We show that, despite all its merits, EADA never benefits pupils who are either assigned to their worst-ranked schools or unmatched under DA. It also limits the placement improvement of marginalized students, preserving school segregation. The placement of the worst-off student under EADA may be unreasonably bad, even though significantly more egalitarian allocations are possible. Finally, we provide a bound on the expected number of unimprovable students using a random market approach.
  Our results help to understand why EADA fails to reduce the inequality generated by DA in empirical evaluations of school choice mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19831v1</guid>
      <category>econ.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Josue Ortega, Gabriel Ziegler, R. Pablo Arribillaga</dc:creator>
    </item>
    <item>
      <title>The Transfer Performance of Economic Models</title>
      <link>https://arxiv.org/abs/2202.04796</link>
      <description>arXiv:2202.04796v4 Announce Type: replace 
Abstract: Economists often estimate models using data from a particular domain, e.g. estimating risk preferences in a particular subject pool or for a specific class of lotteries. Whether a model's predictions extrapolate well across domains depends on whether the estimated model has captured generalizable structure. We provide a tractable formulation for this "out-of-domain" prediction problem and define the transfer error of a model based on how well it performs on data from a new domain. We derive finite-sample forecast intervals that are guaranteed to cover realized transfer errors with a user-selected probability when domains are iid, and use these intervals to compare the transferability of economic models and black box algorithms for predicting certainty equivalents. We find that in this application, the black box algorithms we consider outperform standard economic models when estimated and tested on data from the same domain, but the economic models generalize across domains better than the black-box algorithms do.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.04796v4</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaiah Andrews, Drew Fudenberg, Lihua Lei, Annie Liang, Chaofeng Wu</dc:creator>
    </item>
    <item>
      <title>Improving Robust Decisions with Data</title>
      <link>https://arxiv.org/abs/2310.16281</link>
      <description>arXiv:2310.16281v4 Announce Type: replace 
Abstract: A decision-maker faces uncertainty governed by a data-generating process (DGP), which is only known to belong to a set of sequences of independent but possibly non-identical distributions. A robust decision maximizes the expected payoff against the worst possible DGP in this set. This paper characterizes when and how such robust decisions can be improved with data, measured by the expected payoff under the true DGP, no matter which possible DGP is the truth. It further develops novel and simple inference methods to achieve it, as common methods (e.g., maximum likelihood) may fail to deliver such an improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16281v4</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyu Cheng</dc:creator>
    </item>
    <item>
      <title>The Power of Simple Menus in Robust Selling Mechanisms</title>
      <link>https://arxiv.org/abs/2310.17392</link>
      <description>arXiv:2310.17392v2 Announce Type: replace 
Abstract: We study a robust selling problem where a seller attempts to sell one item to a buyer but is uncertain about the buyer's valuation distribution. Existing literature indicates that robust mechanism design provides a stronger theoretical guarantee than robust deterministic pricing. Meanwhile, the superior performance of robust mechanism design comes at the expense of implementation complexity given that the seller offers a menu with an infinite number of options, each coupled with a lottery and a payment for the buyer's selection. In view of this, the primary focus of our research is to find simple selling mechanisms that can effectively hedge against market ambiguity. We show that a selling mechanism with a small menu size (or limited randomization across a finite number of prices) is already capable of deriving significant benefits achieved by the optimal robust mechanism with infinite options. In particular, we develop a general framework to study the robust selling mechanism problem where the seller only offers a finite number of options in the menu. Then we propose a tractable reformulation that addresses a variety of ambiguity sets of the buyer's valuation distribution. Our formulation further enables us to characterize the optimal selling mechanisms and the corresponding competitive ratio for different menu sizes and various ambiguity sets, including support, mean, and quantile information. In light of the closed-form competitive ratios associated with different menu sizes, we provide managerial implications that incorporating a modest menu size already yields a competitive ratio comparable to the optimal robust mechanism with infinite options, which establishes a favorable trade-off between theoretical performance and implementation simplicity. Remarkably, a menu size of merely two can significantly enhance the competitive ratio, compared to the deterministic pricing scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17392v2</guid>
      <category>econ.TH</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shixin Wang</dc:creator>
    </item>
    <item>
      <title>Embracing the Enemy</title>
      <link>https://arxiv.org/abs/2406.09734</link>
      <description>arXiv:2406.09734v4 Announce Type: replace 
Abstract: We study the repeated interactions between two power-hungry agents, the ``friend'', and the ``enemy,'' and one power broker, the principal. All three care about the leading agent's policy choice. The principal, who aligns more with the friend, can influence but not fully control leadership allocation. After an initial cordon sanitaire breaks, the principal embraces the enemy, sometimes promising persistent support: she grants the enemy power in exchange for moderation, which benefits the friend who reciprocates. The closer the principal is to the friend, the more she desires to embrace the enemy, but the harder it is to uphold such promises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09734v4</guid>
      <category>econ.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes Schneider, \'Alvaro Delgado-Vega</dc:creator>
    </item>
    <item>
      <title>Incentive Contracts and Peer Effects in the Workplace</title>
      <link>https://arxiv.org/abs/2406.11712</link>
      <description>arXiv:2406.11712v2 Announce Type: replace 
Abstract: We study the problem of a principal designing wage contracts that simultaneously incentivize and insure workers. Workers' incentives are connected through chains of productivity spillovers, represented by a network of peer-effects. We solve for the optimal linear contract for any network and show that optimal incentives are steeper for more central workers. We link firm profits to organizations' structure via the spectral properties of the coworker network. When production is modular, the incentive allocation rule is sensitive to the link structure across and within modules. When firms can't write personalize contracts, better connected workers extract rents. In this case, unemployment emerges endogenously because large within-group differences in centrality can decrease firm's profits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11712v2</guid>
      <category>econ.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Claveria-Mayol, Pau Mil\'an, Nicol\'as Oviedo-D\'avila</dc:creator>
    </item>
    <item>
      <title>Strategy-proof Selling: a Geometric Approach</title>
      <link>https://arxiv.org/abs/2406.12279</link>
      <description>arXiv:2406.12279v2 Announce Type: replace 
Abstract: We consider one buyer and one seller. For a bundle $(t,q)\in [0,\infty[\times [0,1]=\mathbb{Z}$, $q$ either refers to the wining probability of an object or a share of a good, and $t$ denotes the payment that the buyer makes. We define classical and restricted classical preferences of the buyer on $\mathbb{Z}$; they incorporate quasilinear, non-quasilinear, risk averse preferences with multidimensional pay-off relevant parameters. We define rich single-crossing subsets of the two classes, and characterize strategy-proof mechanisms by using monotonicity of the mechanisms and continuity of the indirect preference correspondences. We also provide a computationally tractable optimization program to compute the optimal mechanism. We do not use revenue equivalence and virtual valuations as tools in our proofs. Our proof techniques bring out the geometric interaction between the single-crossing property and the positions of bundles $(t,q)$s. Our proofs are simple and provide computationally tractable optimization program to compute the optimal mechanism. The extension of the optimization program to the $n-$ buyer environment is immediate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.12279v2</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mridu Prabal Goswami</dc:creator>
    </item>
    <item>
      <title>Optimal Pre-Analysis Plans: Statistical Decisions Subject to Implementability</title>
      <link>https://arxiv.org/abs/2208.09638</link>
      <description>arXiv:2208.09638v3 Announce Type: replace-cross 
Abstract: What is the purpose of pre-analysis plans, and how should they be designed? We model the interaction between an agent who analyzes data and a principal who makes a decision based on agent reports. The agent could be the manufacturer of a new drug, and the principal a regulator deciding whether the drug is approved. Or the agent could be a researcher submitting a research paper, and the principal an editor deciding whether it is published. The agent decides which statistics to report to the principal. The principal cannot verify whether the analyst reported selectively. Absent a pre-analysis message, if there are conflicts of interest, then many desirable decision rules cannot be implemented. Allowing the agent to send a message before seeing the data increases the set of decision rules that can be implemented, and allows the principal to leverage agent expertise. The optimal mechanisms that we characterize require pre-analysis plans. Applying these results to hypothesis testing, we show that optimal rejection rules pre-register a valid test, and make worst-case assumptions about unreported statistics. Optimal tests can be found as a solution to a linear-programming problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.09638v3</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maximilian Kasy, Jann Spiess</dc:creator>
    </item>
    <item>
      <title>Opponent Modeling in Multiplayer Imperfect-Information Games</title>
      <link>https://arxiv.org/abs/2212.06027</link>
      <description>arXiv:2212.06027v4 Announce Type: replace-cross 
Abstract: In many real-world settings agents engage in strategic interactions with multiple opposing agents who can employ a wide variety of strategies. The standard approach for designing agents for such settings is to compute or approximate a relevant game-theoretic solution concept such as Nash equilibrium and then follow the prescribed strategy. However, such a strategy ignores any observations of opponents' play, which may indicate shortcomings that can be exploited. We present an approach for opponent modeling in multiplayer imperfect-information games where we collect observations of opponents' play through repeated interactions. We run experiments against a wide variety of real opponents and exact Nash equilibrium strategies in three-player Kuhn poker and show that our algorithm significantly outperforms all of the agents, including the exact Nash equilibrium strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.06027v4</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>econ.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sam Ganzfried, Kevin A. Wang, Max Chiswick</dc:creator>
    </item>
  </channel>
</rss>
