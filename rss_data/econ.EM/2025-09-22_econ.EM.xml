<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Sep 2025 02:44:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Efficient and Accessible Discrete Choice Experiments: The DCEtool Package for R</title>
      <link>https://arxiv.org/abs/2509.15326</link>
      <description>arXiv:2509.15326v1 Announce Type: new 
Abstract: Discrete Choice Experiments (DCEs) are widely used to elicit preferences for products or services by analyzing choices among alternatives described by their attributes. The quality of the insights obtained from a DCE heavily depends on the properties of its experimental design. While early DCEs often relied on linear criteria such as orthogonality, these approaches were later found to be inappropriate for discrete choice models, which are inherently non-linear. As a result, statistically efficient design methods, based on minimizing the D-error to reduce parameter variance, have become the standard. Although such methods are implemented in several commercial tools, researchers seeking free and accessible solutions often face limitations. This paper presents DCEtool, an R package with a Shiny-based graphical interface designed to support both novice and experienced users in constructing, decoding, and analyzing statistically efficient DCE designs. DCEtool facilitates the implementation of serial DCEs, offers flexible design settings, and enables rapid estimation of discrete choice models. By making advanced design techniques more accessible, DCEtool contributes to the broader adoption of rigorous experimental practices in choice modelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15326v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel P\'erez-Troncoso</dc:creator>
    </item>
    <item>
      <title>Inference on the Distribution of Individual Treatment Effects in Nonseparable Triangular Models</title>
      <link>https://arxiv.org/abs/2509.15401</link>
      <description>arXiv:2509.15401v1 Announce Type: new 
Abstract: In this paper, we develop inference methods for the distribution of heterogeneous individual treatment effects (ITEs) in the nonseparable triangular model with a binary endogenous treatment and a binary instrument of Vuong and Xu (2017) and Feng, Vuong, and Xu (2019). We focus on the estimation of the cumulative distribution function (CDF) of the ITE, which can be used to address a wide range of practically important questions such as inference on the proportion of individuals with positive ITEs, the quantiles of the distribution of ITEs, and the interquartile range as a measure of the spread of the ITEs, as well as comparison of the ITE distributions across sub-populations. Moreover, our CDF-based approach can deliver more precise results than density-based approach previously considered in the literature. We establish weak convergence to tight Gaussian processes for the empirical CDF and quantile function computed from nonparametric ITE estimates of Feng, Vuong, and Xu (2019). Using those results, we develop bootstrap-based nonparametric inferential methods, including uniform confidence bands for the CDF and quantile function of the ITE distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15401v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun Ma, Vadim Marmer, Zhengfei Yu</dc:creator>
    </item>
    <item>
      <title>KRED: Korea Research Economic Database for Macroeconomic Research</title>
      <link>https://arxiv.org/abs/2509.16115</link>
      <description>arXiv:2509.16115v1 Announce Type: new 
Abstract: We introduce KRED (Korea Research Economic Database), a new FRED MD style macroeconomic dataset for South Korea. KRED is constructed by aggregating 88 key monthly time series from multiple official sources (e.g., Bank of Korea ECOS, Statistics Korea KOSIS) into a unified, publicly available database. The dataset is aligned with the FRED MD format, enabling standardized transformations and direct comparability; an Appendix maps each Korean series to its FRED MD counterpart. Using a balanced panel of 80 series from 2009 to 2024, we extract four principal components via PCA that explain approximately 40% of the total variance. These four factors have intuitive economic interpretations, capturing monetary conditions, labor market activity, real output, and housing demand, analogous to diffusion indexes summarizing broad economic movements. Notably, the factor based diffusion indexes derived from KRED clearly trace major macroeconomic fluctuations over the sample period such as the 2020 COVID 19 recession. Our results demonstrate that KRED's factor structure can effectively condense complex economic information into a few informative indexes, yielding new insights into South Korea's business cycles and co movements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16115v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Changryong Baek, Seunghyun Moon, Seunghyeon Lee</dc:creator>
    </item>
    <item>
      <title>Exact P-values for Network Interference</title>
      <link>https://arxiv.org/abs/1506.02084</link>
      <description>arXiv:1506.02084v1 Announce Type: cross 
Abstract: We study the calculation of exact p-values for a large class of non-sharp null hypotheses about treatment effects in a setting with data from experiments involving members of a single connected network. The class includes null hypotheses that limit the effect of one unit's treatment status on another according to the distance between units; for example, the hypothesis might specify that the treatment status of immediate neighbors has no effect, or that units more than two edges away have no effect. We also consider hypotheses concerning the validity of sparsification of a network (for example based on the strength of ties) and hypotheses restricting heterogeneity in peer effects (so that, for example, only the number or fraction treated among neighboring units matters). Our general approach is to define an artificial experiment, such that the null hypothesis that was not sharp for the original experiment is sharp for the artificial experiment, and such that the randomization analysis for the artificial experiment is validated by the design of the original experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:1506.02084v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susan Athey, Dean Eckles, Guido Imbens</dc:creator>
    </item>
    <item>
      <title>Synthetic Difference in Differences</title>
      <link>https://arxiv.org/abs/1812.09970</link>
      <description>arXiv:1812.09970v4 Announce Type: cross 
Abstract: We present a new estimator for causal effects with panel data that builds on insights behind the widely used difference in differences and synthetic control methods. Relative to these methods we find, both theoretically and empirically, that this "synthetic difference in differences" estimator has desirable robustness properties, and that it performs well in settings where the conventional estimators are commonly used in practice. We study the asymptotic behavior of the estimator when the systematic part of the outcome model includes latent unit factors interacted with latent time factors, and we present conditions for consistency and asymptotic normality.</description>
      <guid isPermaLink="false">oai:arXiv.org:1812.09970v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dmitry Arkhangelsky, Susan Athey, David A. Hirshberg, Guido W. Imbens, Stefan Wager</dc:creator>
    </item>
    <item>
      <title>Off-Policy Evaluation via Adaptive Weighting with Data from Contextual Bandits</title>
      <link>https://arxiv.org/abs/2106.02029</link>
      <description>arXiv:2106.02029v2 Announce Type: cross 
Abstract: It has become increasingly common for data to be collected adaptively, for example using contextual bandits. Historical data of this type can be used to evaluate other treatment assignment policies to guide future innovation or experiments. However, policy evaluation is challenging if the target policy differs from the one used to collect data, and popular estimators, including doubly robust (DR) estimators, can be plagued by bias, excessive variance, or both. In particular, when the pattern of treatment assignment in the collected data looks little like the pattern generated by the policy to be evaluated, the importance weights used in DR estimators explode, leading to excessive variance.
  In this paper, we improve the DR estimator by adaptively weighting observations to control its variance. We show that a t-statistic based on our improved estimator is asymptotically normal under certain conditions, allowing us to form confidence intervals and test hypotheses. Using synthetic data and public benchmarks, we provide empirical evidence for our estimator's improved accuracy and inferential properties relative to existing alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.02029v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruohan Zhan, Vitor Hadad, David A. Hirshberg, Susan Athey</dc:creator>
    </item>
    <item>
      <title>Qini Curves for Multi-Armed Treatment Rules</title>
      <link>https://arxiv.org/abs/2306.11979</link>
      <description>arXiv:2306.11979v4 Announce Type: cross 
Abstract: Qini curves have emerged as an attractive and popular approach for evaluating the benefit of data-driven targeting rules for treatment allocation. We propose a generalization of the Qini curve to multiple costly treatment arms, that quantifies the value of optimally selecting among both units and treatment arms at different budget levels. We develop an efficient algorithm for computing these curves and propose bootstrap-based confidence intervals that are exact in large samples for any point on the curve. These confidence intervals can be used to conduct hypothesis tests comparing the value of treatment targeting using an optimal combination of arms with using just a subset of arms, or with a non-targeting assignment rule ignoring covariates, at different budget levels. We demonstrate the statistical performance in a simulation experiment and an application to treatment targeting for election turnout.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11979v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erik Sverdrup, Han Wu, Susan Athey, Stefan Wager</dc:creator>
    </item>
    <item>
      <title>Triply Robust Panel Estimators</title>
      <link>https://arxiv.org/abs/2508.21536</link>
      <description>arXiv:2508.21536v2 Announce Type: cross 
Abstract: This paper studies estimation of causal effects in a panel data setting. We introduce a new estimator, the Triply RObust Panel (TROP) estimator, that combines (i) a flexible model for the potential outcomes based on a low-rank factor structure on top of a two-way-fixed effect specification, with (ii) unit weights intended to upweight units similar to the treated units and (iii) time weights intended to upweight time periods close to the treated time periods. We study the performance of the estimator in a set of simulations designed to closely match several commonly studied real data sets. We find that there is substantial variation in the performance of the estimators across the settings considered. The proposed estimator outperforms two-way-fixed-effect/difference-in-differences, synthetic control, matrix completion and synthetic-difference-in-differences estimators. We investigate what features of the data generating process lead to this performance, and assess the relative importance of the three components of the proposed estimator. We have two recommendations. Our preferred strategy is that researchers use simulations closely matched to the data they are interested in, along the lines discussed in this paper, to investigate which estimators work well in their particular setting. A simpler approach is to use more robust estimators such as synthetic difference-in-differences or the new triply robust panel estimator which we find to substantially outperform two-way fixed effect estimators in many empirically relevant settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21536v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Susan Athey, Guido Imbens, Zhaonan Qu, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>Beyond the Average: Distributional Causal Inference under Imperfect Compliance</title>
      <link>https://arxiv.org/abs/2509.15594</link>
      <description>arXiv:2509.15594v1 Announce Type: cross 
Abstract: We study the estimation of distributional treatment effects in randomized experiments with imperfect compliance. When participants do not adhere to their assigned treatments, we leverage treatment assignment as an instrumental variable to identify the local distributional treatment effect-the difference in outcome distributions between treatment and control groups for the subpopulation of compliers. We propose a regression-adjusted estimator based on a distribution regression framework with Neyman-orthogonal moment conditions, enabling robustness and flexibility with high-dimensional covariates. Our approach accommodates continuous, discrete, and mixed discrete-continuous outcomes, and applies under a broad class of covariate-adaptive randomization schemes, including stratified block designs and simple random sampling. We derive the estimator's asymptotic distribution and show that it achieves the semiparametric efficiency bound. Simulation results demonstrate favorable finite-sample performance, and we demonstrate the method's practical relevance in an application to the Oregon Health Insurance Experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15594v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Undral Byambadalai, Tomu Hirata, Tatsushi Oka, Shota Yasui</dc:creator>
    </item>
    <item>
      <title>Dealing with Logs and Zeros in Regression Models</title>
      <link>https://arxiv.org/abs/2203.11820</link>
      <description>arXiv:2203.11820v3 Announce Type: replace 
Abstract: The log transformation is widely used in linear regression, mainly because coefficients are interpretable as proportional effects. Yet this practice has fundamental limitations, most notably that the log is undefined at zero, creating an identification problem. We propose a new estimator, iterated OLS (iOLS), which targets the normalized average treatment effect, preserving the percentage-change interpretation while addressing these limitations. Our procedure is the theoretically justified analogue of the ad-hoc log(1+Y) transformation and delivers a consistent and asymptotically normal estimator of the parameters of the exponential conditional mean model. iOLS is computationally efficient, globally convergent, and free of the incidental-parameter bias, while extending naturally to endogenous regressors through iterated 2SLS. We illustrate the methods with simulations and revisit three influential publications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.11820v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Benatia, Christophe Bell\'ego, Louis Pape</dc:creator>
    </item>
    <item>
      <title>Modelling with Sensitive Variables</title>
      <link>https://arxiv.org/abs/2403.15220</link>
      <description>arXiv:2403.15220v2 Announce Type: replace 
Abstract: The paper deals with models in which the dependent variable, some explanatory variables, or both represent sensitive data. We introduce a novel discretization method that preserves data privacy when working with such variables. A multiple discretization method is proposed that utilizes information from the different discretization schemes. We show convergence in distribution for the unobserved variable and derive the asymptotic properties of the OLS estimator for linear models. Monte Carlo simulation experiments presented support our theoretical findings. Finally, we contrast our method with a differential privacy method to estimate the Australian gender wage gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15220v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Chan, Laszlo Matyas, Agoston Reguly</dc:creator>
    </item>
    <item>
      <title>Copyright and Competition: Estimating Supply and Demand with Unstructured Data</title>
      <link>https://arxiv.org/abs/2501.16120</link>
      <description>arXiv:2501.16120v2 Announce Type: replace 
Abstract: We study the competitive and welfare effects of copyright in creative industries in the face of cost-reducing technologies such as generative artificial intelligence. Creative products often feature unstructured attributes (e.g., images and text) that are complex and high-dimensional. To address this challenge, we study a stylized design product -- fonts -- using data from the world's largest font marketplace. We construct neural network embeddings to quantify unstructured attributes and measure visual similarity in a manner consistent with human perception. Spatial regression and event-study analyses demonstrate that competition is local in the visual characteristics space. Building on this evidence, we develop a structural model of supply and demand that incorporates embeddings and captures product positioning under copyright-based similarity constraints. Our estimates reveal consumers' heterogeneous design preferences and producers' cost-effective mimicry advantages. Counterfactual analyses show that copyright protection can raise consumer welfare by encouraging product relocation, and that the optimal policy depends on the interaction between copyright and cost-reducing technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16120v2</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sukjin Han, Kyungho Lee</dc:creator>
    </item>
    <item>
      <title>Scenario Analysis with Multivariate Bayesian Machine Learning Models</title>
      <link>https://arxiv.org/abs/2502.08440</link>
      <description>arXiv:2502.08440v3 Announce Type: replace 
Abstract: We present an econometric framework that adapts tools for scenario analysis, such as variants of conditional forecasts and generalized impulse responses, for use with dynamic nonparametric models. The proposed algorithms are based on predictive simulation and sequential Monte Carlo methods. Their utility is demonstrated with three applications: (1) conditional forecasts based on stress test scenarios, measuring (2) macroeconomic risk under varying financial stress, and estimating the (3) asymmetric effects of financial shocks in the US and their international spillovers. Our empirical results indicate the importance of nonlinearities and asymmetries in relationships between macroeconomic and financial variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08440v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Pfarrhofer, Anna Stelzer</dc:creator>
    </item>
  </channel>
</rss>
