<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 May 2024 04:01:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Analysis of reconstruction from noisy discrete generalized Radon data</title>
      <link>https://arxiv.org/abs/2405.13269</link>
      <description>arXiv:2405.13269v1 Announce Type: new 
Abstract: We consider a wide class of generalized Radon transforms $\mathcal R$, which act in $\mathbb{R}^n$ for any $n\ge 2$ and integrate over submanifolds of any codimension $N$, $1\le N\le n-1$. Also, we allow for a fairly general reconstruction operator $\mathcal A$. The main requirement is that $\mathcal A$ be a Fourier integral operator with a phase function, which is linear in the phase variable. We consider the task of image reconstruction from discrete data $g_{j,k} = (\mathcal R f)_{j,k} + \eta_{j,k}$. We show that the reconstruction error $N_\epsilon^{\text{rec}}=\mathcal A \eta_{j,k}$ satisfies $N^{\text{rec}}(\check x;x_0)=\lim_{\epsilon\to0}N_\epsilon^{\text{rec}}(x_0+\epsilon\check x)$, $\check x\in D$. Here $x_0$ is a fixed point, $D\subset\mathbb{R}^n$ is a bounded domain, and $\eta_{j,k}$ are independent, but not necessarily identically distributed, random variables. $N^{\text{rec}}$ and $N_\epsilon^{\text{rec}}$ are viewed as continuous random functions of the argument $\check x$ (random fields), and the limit is understood in the sense of probability distributions. Under some conditions on the first three moments of $\eta_{j,k}$ (and some other not very restrictive conditions on $x_0$ and $\mathcal A$), we prove that $N^{\text{rec}}$ is a zero mean Gaussian random field and explicitly compute its covariance. We also present a numerical experiment with a cone beam transform in $\mathbb{R}^3$, which shows an excellent match between theoretical predictions and simulated reconstructions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13269v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Katsevich</dc:creator>
    </item>
    <item>
      <title>Randomized block coordinate descent method for linear ill-posed problems</title>
      <link>https://arxiv.org/abs/2405.13340</link>
      <description>arXiv:2405.13340v1 Announce Type: new 
Abstract: Consider the linear ill-posed problems of the form $\sum_{i=1}^{b} A_i x_i =y$, where, for each $i$, $A_i$ is a bounded linear operator between two Hilbert spaces $X_i$ and ${\mathcal Y}$. When $b$ is huge, solving the problem by an iterative method using the full gradient at each iteration step is both time-consuming and memory insufficient. Although randomized block coordinate decent (RBCD) method has been shown to be an efficient method for well-posed large-scale optimization problems with a small amount of memory, there still lacks a convergence analysis on the RBCD method for solving ill-posed problems. In this paper, we investigate the convergence property of the RBCD method with noisy data under either {\it a priori} or {\it a posteriori} stopping rules. We prove that the RBCD method combined with an {\it a priori} stopping rule yields a sequence that converges weakly to a solution of the problem almost surely. We also consider the early stopping of the RBCD method and demonstrate that the discrepancy principle can terminate the iteration after finite many steps almost surely. For a class of ill-posed problems with special tensor product form, we obtain strong convergence results on the RBCD method. Furthermore, we consider incorporating the convex regularization terms into the RBCD method to enhance the detection of solution features. To illustrate the theory and the performance of the method, numerical simulations from the imaging modalities in computed tomography and compressive temporal imaging are reported.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13340v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qinian Jin, Duo Liu</dc:creator>
    </item>
    <item>
      <title>A weak Galerkin finite element method for solving the asymptotic lower bound of Maxwell eigenvalue problem</title>
      <link>https://arxiv.org/abs/2405.13423</link>
      <description>arXiv:2405.13423v1 Announce Type: new 
Abstract: In this paper, we propose a weak Galerkin (WG) finite element method for the Maxwell eigenvalue problem. By restricting subspaces, we transform the mixed form of Maxwell eigenvalue problem into simple elliptic equation. Then we give the WG numerical scheme for the Maxwell eigenvalue problem. Furthermore, we obtain the optimal error estimates of arbitrarily high convergence order and prove the lower bound property of numerical solutions for eigenvalues. Numerical experiments show the accuracy of theoretical analysis and the property of lower bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13423v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shusheng Li, Qilong Zhai</dc:creator>
    </item>
    <item>
      <title>The Unisolvence of Lagrange Interpolation with Symmetric Interpolation Space and Nodes in High Dimension</title>
      <link>https://arxiv.org/abs/2405.13430</link>
      <description>arXiv:2405.13430v1 Announce Type: new 
Abstract: High-dimensional Lagrange interpolation plays a pivotal role in finite element methods, where ensuring the unisolvence and symmetry of its interpolation space and nodes set is crucial. In this paper, we leverage group action and group representation theories to precisely delineate the conditions for unisolvence. We establish a necessary condition for unisolvence: the symmetry of the interpolation nodes set is determined by the given interpolation space. Our findings not only contribute to a deeper theoretical understanding but also promise practical benefits by reducing the computational overhead associated with identifying appropriate interpolation nodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13430v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yulin Xie, Yifa Tang</dc:creator>
    </item>
    <item>
      <title>An all Mach number semi-implicit hybrid Finite Volume/Virtual Element method for compressible viscous flows on Voronoi meshes</title>
      <link>https://arxiv.org/abs/2405.13441</link>
      <description>arXiv:2405.13441v1 Announce Type: new 
Abstract: We present a novel high order semi-implicit hybrid finite volume/virtual element numerical scheme for the solution of compressible flows on Voronoi tessellations. The method relies on the flux splitting of the compressible Navier-Stokes equations into three sub-systems: a convective sub-system solved explicitly using a finite volume (FV) scheme, and the viscous and pressure sub-systems which are discretized implicitly at the aid of a virtual element method (VEM). Consequently, the time step restriction of the overall algorithm depends only on the mean flow velocity and not on the fast pressure waves nor on the viscous eigenvalues. As such, the proposed methodology is well suited for the solution of low Mach number flows at all Reynolds numbers. Moreover, the scheme is proven to be globally energy conserving so that shock capturing properties are retrieved in high Mach number flows. To reach high order of accuracy in time and space, an IMEX Runge-Kutta time stepping strategy is employed together with high order spatial reconstructions in terms of CWENO polynomials and virtual element space basis functions. The chosen discretization techniques allow the use of general polygonal grids, a useful tool when dealing with complex domain configurations. The new scheme is carefully validated in both the incompressible limit and the high Mach number regime through a large set of classical benchmarks for fluid dynamics, assessing robustness and accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13441v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Walter Boscheri, Saray Busto, Michael Dumbser</dc:creator>
    </item>
    <item>
      <title>Network Inpainting via Optimal Transport</title>
      <link>https://arxiv.org/abs/2405.13520</link>
      <description>arXiv:2405.13520v1 Announce Type: new 
Abstract: In this work, we present a novel tool for reconstructing networks from corrupted images. The reconstructed network is the result of a minimization problem that has a misfit term with respect to the observed data, and a physics-based regularizing term coming from the theory of optimal transport. Through a range of numerical tests, we demonstrate that our suggested approach can effectively rebuild the primary features of damaged networks, even when artifacts are present.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13520v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enrico Facca, Jan Martin Nordbotten, Erik Andreas Hanson</dc:creator>
    </item>
    <item>
      <title>Bounds on the approximation error for deep neural networks applied to dispersive models: Nonlinear waves</title>
      <link>https://arxiv.org/abs/2405.13566</link>
      <description>arXiv:2405.13566v1 Announce Type: new 
Abstract: We present a comprehensive framework for deriving rigorous and efficient bounds on the approximation error of deep neural networks in PDE models characterized by branching mechanisms, such as waves, Schr\"odinger equations, and other dispersive models. This framework utilizes the probabilistic setting established by Henry-Labord\`ere and Touzi. We illustrate this approach by providing rigorous bounds on the approximation error for both linear and nonlinear waves in physical dimensions $d=1,2,3$, and analyze their respective computational costs starting from time zero. We investigate two key scenarios: one involving a linear perturbative source term, and another focusing on pure nonlinear internal interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13566v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.PR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Claudio Mu\~noz, Nicol\'as Valenzuela</dc:creator>
    </item>
    <item>
      <title>The Jacobi Eigenvalue Algorithm for Computing the Eigenvalues of a Dual Quaternion Hermitian Matrix</title>
      <link>https://arxiv.org/abs/2405.13649</link>
      <description>arXiv:2405.13649v1 Announce Type: new 
Abstract: In this paper, we generalize the Jacobi eigenvalue algorithm to compute all eigenvalues and eigenvectors of a dual quaternion Hermitian matrix and show the convergence. We also propose a three-step Jacobi eigenvalue algorithm to compute the eigenvalues when a dual quaternion Hermitian matrix has two eigenvalues with identical standard parts but different dual parts and prove the convergence. Numerical experiments are presented to illustrate the efficiency and stability of the proposed Jacobi eigenvalue algorithm compaired to the power method and the Rayleigh quotient iteration method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13649v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongjun Chen, Liping Zhang</dc:creator>
    </item>
    <item>
      <title>A Conforming virtual element approximation for the Oseen eigenvalue problem</title>
      <link>https://arxiv.org/abs/2405.13657</link>
      <description>arXiv:2405.13657v1 Announce Type: new 
Abstract: In this paper we analyze a conforming virtual element method to approximate the eigenfunctions and eigenvalues of the two dimensional Oseen eigenvalue problem. We consider the classic velocity-pressure formulation which allows us to consider the divergence-conforming virtual element spaces employed for the Stokes equations. Under standard assumptions on the meshes we derive a priori error estimates for the proposed method with the aid of the compact operators theory. We report some numerical tests to confirm the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13657v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danilo Amigo, Felipe Lepe, Nitesh Verma</dc:creator>
    </item>
    <item>
      <title>Algebraic Conditions for Stability in Runge-Kutta Methods and Their Certification via Semidefinite Programming</title>
      <link>https://arxiv.org/abs/2405.13921</link>
      <description>arXiv:2405.13921v1 Announce Type: new 
Abstract: In this work, we present approaches to rigorously certify $A$- and $A(\alpha)$-stability in Runge-Kutta methods through the solution of convex feasibility problems defined by linear matrix inequalities. We adopt two approaches. The first is based on sum-of-squares programming applied to the Runge-Kutta $E$-polynomial and is applicable to both $A$- and $A(\alpha)$-stability. In the second, we sharpen the algebraic conditions for $A$-stability of Cooper, Scherer, T{\"u}rke, and Wendler to incorporate the Runge-Kutta order conditions. We demonstrate how the theoretical improvement enables the practical use of these conditions for certification of $A$-stability within a computational framework. We then use both approaches to obtain rigorous certificates of stability for several diagonally implicit schemes devised in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13921v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Austin Juhl, David Shirokoff</dc:creator>
    </item>
    <item>
      <title>Nonisothermal Cahn-Hilliard Navier-Stokes system</title>
      <link>https://arxiv.org/abs/2405.13936</link>
      <description>arXiv:2405.13936v1 Announce Type: new 
Abstract: In this research, we introduce and investigate an approximation method that preserves the structural integrity of the non-isothermal Cahn-Hilliard-Navier-Stokes system. Our approach extends a previously proposed technique [1], which utilizes conforming (inf-sup stable) finite elements in space, coupled with implicit time discretization employing convex-concave splitting. Expanding upon this method, we incorporate the unstable P1|P1 pair for the Navier-Stokes contributions, integrating Brezzi-Pitk\"aranta stabilization. Additionally, we improve the enforcement of incompressibility conditions through grad div stabilization. While these techniques are well-established for Navier-Stokes equations, it becomes apparent that for non-isothermal models, they introduce additional coupling terms to the equation governing internal energy. To ensure the conservation of total energy and maintain entropy production, these stabilization terms are appropriately integrated into the internal energy equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13936v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaron Brunk, Dennis Schumann</dc:creator>
    </item>
    <item>
      <title>High order finite-difference ghost-point methods for elliptic problems in domains with curved boundaries</title>
      <link>https://arxiv.org/abs/2405.13986</link>
      <description>arXiv:2405.13986v1 Announce Type: new 
Abstract: In this paper a fourth order finite difference ghost point method for the Poisson equation on regular Cartesian mesh is presented. The method can be considered the high order extension of the second ghost method introduced earlier by the authors. Three different discretizations are considered, which differ in the stencil that discretizes the Laplacian and the source term. It is shown that only two of them provide a stable method. The accuracy of such stable methods are numerically verified on several test problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13986v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Armando Coco, Giovanni Russo</dc:creator>
    </item>
    <item>
      <title>Regularity-Conforming Neural Networks (ReCoNNs) for solving Partial Differential Equations</title>
      <link>https://arxiv.org/abs/2405.14110</link>
      <description>arXiv:2405.14110v1 Announce Type: new 
Abstract: Whilst the Universal Approximation Theorem guarantees the existence of approximations to Sobolev functions -- the natural function spaces for PDEs -- by Neural Networks (NNs) of sufficient size, low-regularity solutions may lead to poor approximations in practice. For example, classical fully-connected feed-forward NNs fail to approximate continuous functions whose gradient is discontinuous when employing strong formulations like in Physics Informed Neural Networks (PINNs). In this article, we propose the use of regularity-conforming neural networks, where a priori information on the regularity of solutions to PDEs can be employed to construct proper architectures. We illustrate the potential of such architectures via a two-dimensional (2D) transmission problem, where the solution may admit discontinuities in the gradient across interfaces, as well as power-like singularities at certain points. In particular, we formulate the weak transmission problem in a PINNs-like strong formulation with interface and continuity conditions. Such architectures are partially explainable; discontinuities are explicitly described, allowing the introduction of novel terms into the loss function. We demonstrate via several model problems in one and two dimensions the advantages of using regularity-conforming architectures in contrast to classical architectures. The ideas presented in this article easily extend to problems in higher dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14110v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jamie M. Taylor, David Pardo, Judit Mu\~noz-Matute</dc:creator>
    </item>
    <item>
      <title>Piecewise rational rotation-minimizing motions via data stream interpolation</title>
      <link>https://arxiv.org/abs/2405.14229</link>
      <description>arXiv:2405.14229v1 Announce Type: new 
Abstract: When a moving frame defined along a space curve is required to keep an axis aligned with the tangent direction of motion, the use of rotation-minimizing frames (RMF) avoids unnecessary rotations in the normal plane. The construction of rigid body motions using a specific subset of quintic curves with rational RMFs (RRMFs) is here considered. In particular, a novel geometric characterization of such subset enables the design of a local algorithm to interpolate an assigned stream of positions, together with an initial frame orientation. To achieve this, the translational part of the motion is described by a parametric $G^1$ spline curve whose segments are quintic RRMFs, with a globally continuous piecewise rational rotation-minimizing frame. A selection of numerical experiments illustrates the performances of the proposed method on synthetic and arbitrary data streams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14229v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Carlotta Giannelli, Lorenzo Sacco, Alessandra Sestioni, Zbyn\v{e}k \v{S}\'ir</dc:creator>
    </item>
    <item>
      <title>Adaptive tempering schedules with approximative intermediate measures for filtering problems</title>
      <link>https://arxiv.org/abs/2405.14408</link>
      <description>arXiv:2405.14408v1 Announce Type: new 
Abstract: Data assimilation algorithms integrate prior information from numerical model simulations with observed data. Ensemble-based filters, regarded as state-of-the-art, are widely employed for large-scale estimation tasks in disciplines such as geoscience and meteorology. Despite their inability to produce the true posterior distribution for nonlinear systems, their robustness and capacity for state tracking are noteworthy. In contrast, Particle filters yield the correct distribution in the ensemble limit but require substantially larger ensemble sizes than ensemble-based filters to maintain stability in higher-dimensional spaces. It is essential to transcend traditional Gaussian assumptions to achieve realistic quantification of uncertainties. One approach involves the hybridisation of filters, facilitated by tempering, to harness the complementary strengths of different filters. A new adaptive tempering method is proposed to tune the underlying schedule, aiming to systematically surpass the performance previously achieved. Although promising numerical results for certain filter combinations in toy examples exist in the literature, the tuning of hyperparameters presents a considerable challenge. A deeper understanding of these interactions is crucial for practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14408v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iris Rammelm\"uller, Gottfried Hastermann, Jana de Wiljes</dc:creator>
    </item>
    <item>
      <title>Novel semi-explicit symplectic schemes for nonseparable stochastic Hamiltonian systems</title>
      <link>https://arxiv.org/abs/2405.14484</link>
      <description>arXiv:2405.14484v1 Announce Type: new 
Abstract: In this manuscript, we propose efficient stochastic semi-explicit symplectic schemes tailored for nonseparable stochastic Hamiltonian systems (SHSs). These semi-explicit symplectic schemes are constructed by introducing augmented Hamiltonians and using symmetric projection. In the case of the artificial restraint in augmented Hamiltonians being zero, the proposed schemes also preserve quadratic invariants, making them suitable for developing semi-explicit charge-preserved multi-symplectic schemes for stochastic cubic Schr\"odinger equations with multiplicative noise. Through numerical experiments that validate theoretical results, we demonstrate that the proposed stochastic semi-explicit symplectic scheme, which features a straightforward Newton iteration solver, outperforms the traditional stochastic midpoint scheme in terms of effectiveness and accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14484v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jialin Hong, Baohui Hou, Liying Sun</dc:creator>
    </item>
    <item>
      <title>Multicontinuum Homogenization for Coupled Flow and Transport Equations</title>
      <link>https://arxiv.org/abs/2405.14572</link>
      <description>arXiv:2405.14572v1 Announce Type: new 
Abstract: In this paper, we present the derivation of a multicontinuum model for the coupled flow and transport equations by applying multicontinuum homogenization. We perform the multicontinuum expansion for both flow and transport solutions and formulate novel coupled constraint cell problems to capture the multiscale property, where oversampled regions are utilized to avoid boundary effects. Assuming the smoothness of macroscopic variables, we obtain a multicontinuum system composed of macroscopic elliptic equations and convection-diffusion-reaction equations with homogenized effective properties. Finally, we present numerical results for various coefficient fields and boundary conditions to validate our proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14572v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dmitry Ammosov, W. T. Leung, Buzheng Shan, Jian Huang</dc:creator>
    </item>
    <item>
      <title>Spectral analysis of block preconditioners for double saddle-point linear systems with application to PDE-constrained optimization</title>
      <link>https://arxiv.org/abs/2405.14605</link>
      <description>arXiv:2405.14605v1 Announce Type: new 
Abstract: In this paper, we describe and analyze the spectral properties of a symmetric positive definite inexact block preconditioner for a class of symmetric, double saddle-point linear systems.
  We develop a spectral analysis of the preconditioned matrix, showing that its eigenvalues can be described in terms of the roots of a cubic polynomial with real coefficients.
  We illustrate the efficiency of the proposed preconditioners, and verify the theoretical bounds, in solving large-scale PDE-constrained optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14605v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Bergamaschi, Angeles Martinez, John Pearson, Andreas Potshcka</dc:creator>
    </item>
    <item>
      <title>Structure preserving finite element schemes for the Navier-Stokes-Cahn-Hilliard system with degenerate mobility</title>
      <link>https://arxiv.org/abs/2405.14763</link>
      <description>arXiv:2405.14763v1 Announce Type: new 
Abstract: In this work we present two new numerical schemes to approximate the Navier-Stokes-Cahn-Hilliard system with degenerate mobility using finite differences in time and finite elements in space. The proposed schemes are conservative, energy-stable and preserve the maximum principle approximately (the amount of the phase variable being outside of the interval [0,1] goes to zero in terms of a truncation parameter). Additionally, we present several numerical results to illustrate the accuracy and the well behavior of the proposed schemes, as well as a comparison with the behavior of the Navier-Stokes-Cahn-Hilliard model with constant mobility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14763v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francisco Guill\'en-Gonz\'alez, Giordano Tierra</dc:creator>
    </item>
    <item>
      <title>Vortex-capturing multiscale spaces for the Ginzburg-Landau equation</title>
      <link>https://arxiv.org/abs/2405.14772</link>
      <description>arXiv:2405.14772v1 Announce Type: new 
Abstract: This paper considers minimizers of the Ginzburg-Landau energy functional in particular multiscale spaces which are based on finite elements. The spaces are constructed by localized orthogonal decomposition techniques and their usage for solving the Ginzburg-Landau equation was first suggested in [D\"orich, Henning, SINUM 2024]. In this work we further explore their approximation properties and give an analytical explanation for why vortex structures of energy minimizers can be captured more accurately in these spaces. We quantify the necessary mesh resolution in terms of the Ginzburg-Landau parameter $\kappa$ and a stabilization parameter $\beta \ge 0$ that is used in the construction of the multiscale spaces. Furthermore, we analyze how $\kappa$ affects the necessary locality of the multiscale basis functions and we prove that the choice $\beta=0$ yields typically the highest accuracy. Our findings are supported by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14772v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria Blum, Christian D\"oding, Patrick Henning</dc:creator>
    </item>
    <item>
      <title>Novel $H^\mathrm{dev}(\mathrm{Curl})$-conforming elements on regular triangulations and Clough--Tocher splits for the planar relaxed micromorphic model</title>
      <link>https://arxiv.org/abs/2405.14849</link>
      <description>arXiv:2405.14849v1 Announce Type: new 
Abstract: In this work we present a consistent reduction of the relaxed micromorphic model to its corresponding two-dimensional planar model, such that its capacity to capture discontinuous dilatation fields is preserved. As a direct consequence of our approach, new conforming finite elements for $H^\mathrm{dev}(\mathrm{Curl},A)$ become necessary. We present two novel $H^\mathrm{dev}(\mathrm{Curl},A)$-conforming finite element spaces, of which one is a macro element based on Clough--Tocher splits, as well as primal and mixed variational formulations of the planar relaxed micromorphic model. Finally, we demonstrate the effectiveness of our approach with two numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14849v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Adam Sky, Michael Neunteufel, Peter Lewintan, Panos Gourgiotis, Andreas Zilian, Patrizio Neff</dc:creator>
    </item>
    <item>
      <title>Gaussian Measures Conditioned on Nonlinear Observations: Consistency, MAP Estimators, and Simulation</title>
      <link>https://arxiv.org/abs/2405.13149</link>
      <description>arXiv:2405.13149v1 Announce Type: cross 
Abstract: The article presents a systematic study of the problem of conditioning a Gaussian random variable $\xi$ on nonlinear observations of the form $F \circ \phi(\xi)$ where $\phi: \mathcal{X} \to \mathbb{R}^N$ is a bounded linear operator and $F$ is nonlinear. Such problems arise in the context of Bayesian inference and recent machine learning-inspired PDE solvers. We give a representer theorem for the conditioned random variable $\xi \mid F\circ \phi(\xi)$, stating that it decomposes as the sum of an infinite-dimensional Gaussian (which is identified analytically) as well as a finite-dimensional non-Gaussian measure. We also introduce a novel notion of the mode of a conditional measure by taking the limit of the natural relaxation of the problem, to which we can apply the existing notion of maximum a posteriori estimators of posterior measures. Finally, we introduce a variant of the Laplace approximation for the efficient simulation of the aforementioned conditioned Gaussian random variables towards uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13149v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Chen, Bamdad Hosseini, Houman Owhadi, Andrew M Stuart</dc:creator>
    </item>
    <item>
      <title>Adaptive coupling of 3D and 2D fluid flow models</title>
      <link>https://arxiv.org/abs/2405.13165</link>
      <description>arXiv:2405.13165v1 Announce Type: cross 
Abstract: Similar to the notion of h-adaptivity, where the discretization resolution is adaptively changed, I propose the notion of model adaptivity, where the underlying model (the governing equations) is adaptively changed in space and time. Specifically, this work introduces a hybrid and adaptive coupling of a 3D bulk fluid flow model with a 2D thin film flow model. As a result, this work extends the applicability of existing thin film flow models to complex scenarios where, for example, bulk flow develops into thin films after striking a surface. At each location in space and time, the proposed framework automatically decides whether a 3D model or a 2D model must be applied. Using a meshless approach for both 3D and 2D models, at each particle, the decision to apply a 2D or 3D model is based on the user-prescribed resolution and a local principal component analysis. When a particle needs to be changed from a 3D model to 2D, or vice versa, the discretization is changed, and all relevant data mapping is done on-the-fly. Appropriate two-way coupling conditions and mass conservation considerations between the 3D and 2D models are also developed. Numerical results show that this model adaptive framework shows higher flexibility and compares well against finely resolved 3D simulations. In an actual application scenario, a 3 factor speed up is obtained, while maintaining the accuracy of the solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13165v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pratik Suchde</dc:creator>
    </item>
    <item>
      <title>Paired Autoencoders for Inverse Problems</title>
      <link>https://arxiv.org/abs/2405.13220</link>
      <description>arXiv:2405.13220v1 Announce Type: cross 
Abstract: We consider the solution of nonlinear inverse problems where the forward problem is a discretization of a partial differential equation. Such problems are notoriously difficult to solve in practice and require minimizing a combination of a data-fit term and a regularization term. The main computational bottleneck of typical algorithms is the direct estimation of the data misfit. Therefore, likelihood-free approaches have become appealing alternatives. Nonetheless, difficulties in generalization and limitations in accuracy have hindered their broader utility and applicability. In this work, we use a paired autoencoder framework as a likelihood-free estimator for inverse problems. We show that the use of such an architecture allows us to construct a solution efficiently and to overcome some known open problems when using likelihood-free estimators. In particular, our framework can assess the quality of the solution and improve on it if needed. We demonstrate the viability of our approach using examples from full waveform inversion and inverse electromagnetic imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13220v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Chung, Emma Hart, Julianne Chung, Bas Peters, Eldad Haber</dc:creator>
    </item>
    <item>
      <title>Convergence analysis of kernel learning FBSDE filter</title>
      <link>https://arxiv.org/abs/2405.13390</link>
      <description>arXiv:2405.13390v1 Announce Type: cross 
Abstract: Kernel learning forward backward SDE filter is an iterative and adaptive meshfree approach to solve the nonlinear filtering problem. It builds from forward backward SDE for Fokker-Planker equation, which defines evolving density for the state variable, and employs KDE to approximate density. This algorithm has shown more superior performance than mainstream particle filter method, in both convergence speed and efficiency of solving high dimension problems.
  However, this method has only been shown to converge empirically. In this paper, we present a rigorous analysis to demonstrate its local and global convergence, and provide theoretical support for its empirical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13390v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>q-fin.MF</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunzheng Lyu, Feng Bao</dc:creator>
    </item>
    <item>
      <title>On the approximation of the von Neumann equation in the semi-classical limit. Part I : numerical algorithm</title>
      <link>https://arxiv.org/abs/2405.13436</link>
      <description>arXiv:2405.13436v1 Announce Type: cross 
Abstract: We propose a new approach to discretize the von Neumann equation, which is efficient in the semi-classical limit. This method is first based on the so called Weyl's variables to address the stiffness associated with the equation. Then, by applying a truncated Hermite expansion of the density operator, we successfully handle this stiffness. Additionally, we develop a finite volume approximation for practical implementation and conduct numerical simulations to illustrate the efficiency of our approach. This asymptotic preserving numerical approximation, combined with the use of Hermite polynomials, provides an efficient tool for solving the von Neumann equation in all regimes, near classical or not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13436v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francis Filbet (IMT), Fran\c{c}ois Golse (CMLS)</dc:creator>
    </item>
    <item>
      <title>New Tight Wavelet Frame Constructions Sharing Responsibility</title>
      <link>https://arxiv.org/abs/2405.13458</link>
      <description>arXiv:2405.13458v1 Announce Type: cross 
Abstract: Tight wavelet frames (TWFs) in $L^2(\mathbb{R}^n)$ are versatile and practical structures that provide the perfect reconstruction property. Nevertheless, existing TWF construction methods exhibit limitations, including a lack of specific methods for generating mother wavelets in extension-based construction, and the necessity to address the sum of squares (SOS) problem even when specific methods for generating mother wavelets are provided in SOS-based construction. It is a common practice for current TWF constructions to begin with a given refinable function. However, this approach places the entire burden on finding suitable mother wavelets. In this paper, we introduce TWF construction methods that spread the burden between both types of functions: refinable functions and mother wavelets. These construction methods offer an alternative approach to circumvent the SOS problem while providing specific techniques for generating mother wavelets. We present examples to illustrate our construction methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13458v1</guid>
      <category>math.FA</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youngmi Hur, Hyojae Lim</dc:creator>
    </item>
    <item>
      <title>Neural Networks-based Random Vortex Methods for Modelling Incompressible Flows</title>
      <link>https://arxiv.org/abs/2405.13691</link>
      <description>arXiv:2405.13691v1 Announce Type: cross 
Abstract: In this paper we introduce a novel Neural Networks-based approach for approximating solutions to the (2D) incompressible Navier--Stokes equations. Our algorithm uses a Physics-informed Neural Network, that approximates the vorticity based on a loss function that uses a computationally efficient formulation of the Random Vortex dynamics. The neural vorticity estimator is then combined with traditional numerical PDE-solvers for the Poisson equation to compute the velocity field. The main advantage of our method compared to standard Physics-informed Neural Networks is that it strictly enforces physical properties, such as incompressibility or boundary conditions, which might otherwise be hard to guarantee with purely Neural Networks-based approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13691v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladislav Cherepanov, Sebastian W. Ertel</dc:creator>
    </item>
    <item>
      <title>Normalizing Basis Functions: Approximate Stationary Models for Large Spatial Data</title>
      <link>https://arxiv.org/abs/2405.13821</link>
      <description>arXiv:2405.13821v1 Announce Type: cross 
Abstract: In geostatistics, traditional spatial models often rely on the Gaussian Process (GP) to fit stationary covariances to data. It is well known that this approach becomes computationally infeasible when dealing with large data volumes, necessitating the use of approximate methods. A powerful class of methods approximate the GP as a sum of basis functions with random coefficients. Although this technique offers computational efficiency, it does not inherently guarantee a stationary covariance. To mitigate this issue, the basis functions can be "normalized" to maintain a constant marginal variance, avoiding unwanted artifacts and edge effects. This allows for the fitting of nearly stationary models to large, potentially non-stationary datasets, providing a rigorous base to extend to more complex problems. Unfortunately, the process of normalizing these basis functions is computationally demanding. To address this, we introduce two fast and accurate algorithms to the normalization step, allowing for efficient prediction on fine grids. The practical value of these algorithms is showcased in the context of a spatial analysis on a large dataset, where significant computational speedups are achieved. While implementation and testing are done specifically within the LatticeKrig framework, these algorithms can be adapted to other basis function methods operating on regular grids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13821v1</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antony Sikorski, Daniel McKenzie, Douglas Nychka</dc:creator>
    </item>
    <item>
      <title>eXmY: A Data Type and Technique for Arbitrary Bit Precision Quantization</title>
      <link>https://arxiv.org/abs/2405.13938</link>
      <description>arXiv:2405.13938v1 Announce Type: cross 
Abstract: eXmY is a novel data type for quantization of ML models. It supports both arbitrary bit widths and arbitrary integer and floating point formats. For example, it seamlessly supports 3, 5, 6, 7, 9 bit formats. For a specific bit width, say 7, it defines all possible formats e.g. e0m6, e1m5, e2m4, e3m3, e4m2, e5m1 and e6m0. For non-power of two bit widths e.g. 5, 6, 7, we created a novel encoding and decoding scheme which achieves perfect compression, byte addressability and is amenable to sharding and vector processing. We implemented libraries for emulation, encoding and decoding tensors and checkpoints in C++, TensorFlow, JAX and PAX. For optimal performance, the codecs use SIMD instructions on CPUs and vector instructions on TPUs and GPUs. eXmY is also a technique and exploits the statistical distribution of exponents in tensors. It can be used to quantize weights, static and dynamic activations, gradients, master weights and optimizer state. It can reduce memory (CPU DRAM and accelerator HBM), network and disk storage and transfers. It can increase multi tenancy and accelerate compute. eXmY has been deployed in production for almost 2 years.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13938v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Agrawal, Matthew Hedlund, Blake Hechtman</dc:creator>
    </item>
    <item>
      <title>Newton Informed Neural Operator for Computing Multiple Solutions of Nonlinear Partials Differential Equations</title>
      <link>https://arxiv.org/abs/2405.14096</link>
      <description>arXiv:2405.14096v1 Announce Type: cross 
Abstract: Solving nonlinear partial differential equations (PDEs) with multiple solutions using neural networks has found widespread applications in various fields such as physics, biology, and engineering. However, classical neural network methods for solving nonlinear PDEs, such as Physics-Informed Neural Networks (PINN), Deep Ritz methods, and DeepONet, often encounter challenges when confronted with the presence of multiple solutions inherent in the nonlinear problem. These methods may encounter ill-posedness issues. In this paper, we propose a novel approach called the Newton Informed Neural Operator, which builds upon existing neural network techniques to tackle nonlinearities. Our method combines classical Newton methods, addressing well-posed problems, and efficiently learns multiple solutions in a single learning process while requiring fewer supervised data points compared to existing neural network methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14096v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenrui Hao, Xinliang Liu, Yahong Yang</dc:creator>
    </item>
    <item>
      <title>A continuous perspective on the inertial corrected primal-dual proximal splitting</title>
      <link>https://arxiv.org/abs/2405.14098</link>
      <description>arXiv:2405.14098v1 Announce Type: cross 
Abstract: We give a continuous perspective on the Inertial Corrected Primal-Dual Proximal Splitting (IC-PDPS) proposed by Valkonen ({\it SIAM J. Optim.}, 30(2): 1391--1420, 2020) for solving saddle-point problems. The algorithm possesses nonergodic convergence rate and admits a tight preconditioned proximal point formulation which involves both inertia and additional correction. Based on new understandings on the relation between the discrete step size and rescaling effect, we rebuild IC-PDPS as a semi-implicit Euler scheme with respect to its iterative sequences and integrated parameters. This leads to two novel second-order ordinary differential equation (ODE) models that are equivalent under proper time transformation, and also provides an alternative interpretation from the continuous point of view. Besides, we present the convergence analysis of the Lagrangian gap along the continuous trajectory by using proper Lyapunov functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14098v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Luo</dc:creator>
    </item>
    <item>
      <title>Automatic Differentiation is Essential in Training Neural Networks for Solving Differential Equations</title>
      <link>https://arxiv.org/abs/2405.14099</link>
      <description>arXiv:2405.14099v1 Announce Type: cross 
Abstract: Neural network-based approaches have recently shown significant promise in solving partial differential equations (PDEs) in science and engineering, especially in scenarios featuring complex domains or the incorporation of empirical data. One advantage of the neural network method for PDEs lies in its automatic differentiation (AD), which necessitates only the sample points themselves, unlike traditional finite difference (FD) approximations that require nearby local points to compute derivatives. In this paper, we quantitatively demonstrate the advantage of AD in training neural networks. The concept of truncated entropy is introduced to characterize the training property. Specifically, through comprehensive experimental and theoretical analyses conducted on random feature models and two-layer neural networks, we discover that the defined truncated entropy serves as a reliable metric for quantifying the residual loss of random feature models and the training speed of neural networks for both AD and FD methods. Our experimental and theoretical analyses demonstrate that, from a training perspective, AD outperforms FD in solving partial differential equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14099v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuqi Chen, Yahong Yang, Yang Xiang, Wenrui Hao</dc:creator>
    </item>
    <item>
      <title>Sparse $L^1$-Autoencoders for Scientific Data Compression</title>
      <link>https://arxiv.org/abs/2405.14270</link>
      <description>arXiv:2405.14270v1 Announce Type: cross 
Abstract: Scientific datasets present unique challenges for machine learning-driven compression methods, including more stringent requirements on accuracy and mitigation of potential invalidating artifacts. Drawing on results from compressed sensing and rate-distortion theory, we introduce effective data compression methods by developing autoencoders using high dimensional latent spaces that are $L^1$-regularized to obtain sparse low dimensional representations. We show how these information-rich latent spaces can be used to mitigate blurring and other artifacts to obtain highly effective data compression methods for scientific data. We demonstrate our methods for short angle scattering (SAS) datasets showing they can achieve compression ratios around two orders of magnitude and in some cases better. Our compression methods show promise for use in addressing current bottlenecks in transmission, storage, and analysis in high-performance distributed computing environments. This is central to processing the large volume of SAS data being generated at shared experimental facilities around the world to support scientific investigations. Our approaches provide general ways for obtaining specialized compression methods for targeted scientific datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14270v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matthias Chung, Rick Archibald, Paul Atzberger, Jack Michael Solomon</dc:creator>
    </item>
    <item>
      <title>An 808 Line Phasor-Based Ddehomogenisation Matlab Code For Multi-Scale Topology Optimisation</title>
      <link>https://arxiv.org/abs/2405.14321</link>
      <description>arXiv:2405.14321v1 Announce Type: cross 
Abstract: This work presents an 808-line Matlab educational code for combined multi-scale topology optimisation and phasor-based dehomogenisation titled deHomTop808. The multi-scale formulation utilises homogenisation of optimal microstructures to facilitate efficient coarse-scale optimisation. Dehomogenisation allows for a high-resolution single-scale reconstruction of the optimised multi-scale structure, achieving minor losses in structural performance, at a fraction of the computational cost, compared to its large-scale topology optimisation counterpart. The presented code utilises stiffness optimal Rank-2 microstructures to minimise the compliance of a single-load case problem, subject to a volume fraction constraint. By exploiting the inherent efficiency benefits of the phasor-based dehomogenisation procedure, on-the-fly dehomogenisation to a single-scale structure is obtained. The presented code includes procedures for structural verification of the final dehomogenised structure by comparison to the multi-scale solution. The code is introduced in terms of the underlying theory and its major components, including examples and potential extensions, and can be downloaded from https://github.com/peterdorffler/deHomTop808.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14321v1</guid>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rebekka Varum Woldseth, Ole Sigmund, Peter D{\o}rffler Ladegaard Jensen</dc:creator>
    </item>
    <item>
      <title>Skew-symmetric schemes for stochastic differential equations with non-Lipschitz drift: an unadjusted Barker algorithm</title>
      <link>https://arxiv.org/abs/2405.14373</link>
      <description>arXiv:2405.14373v1 Announce Type: cross 
Abstract: We propose a new simple and explicit numerical scheme for time-homogeneous stochastic differential equations. The scheme is based on sampling increments at each time step from a skew-symmetric probability distribution, with the level of skewness determined by the drift and volatility of the underlying process. We show that as the step-size decreases the scheme converges weakly to the diffusion of interest. We then consider the problem of simulating from the limiting distribution of an ergodic diffusion process using the numerical scheme with a fixed step-size. We establish conditions under which the numerical scheme converges to equilibrium at a geometric rate, and quantify the bias between the equilibrium distributions of the scheme and of the true diffusion process. Notably, our results do not require a global Lipschitz assumption on the drift, in contrast to those required for the Euler--Maruyama scheme for long-time simulation at fixed step-sizes. Our weak convergence result relies on an extension of the theory of Milstein \&amp; Tretyakov to stochastic differential equations with non-Lipschitz drift, which could also be of independent interest. We support our theoretical results with numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14373v1</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel Livingstone, Nikolas N\"usken, Giorgos Vasdekis, Rui-Yang Zhang</dc:creator>
    </item>
    <item>
      <title>An augmented Lagrangian trust-region method with inexact gradient evaluations to accelerate constrained optimization problems using model hyperreduction</title>
      <link>https://arxiv.org/abs/2405.14827</link>
      <description>arXiv:2405.14827v1 Announce Type: cross 
Abstract: We present an augmented Lagrangian trust-region method to efficiently solve constrained optimization problems governed by large-scale nonlinear systems with application to partial differential equation-constrained optimization. At each major augmented Lagrangian iteration, the expensive optimization subproblem involving the full nonlinear system is replaced by an empirical quadrature-based hyperreduced model constructed on-the-fly. To ensure convergence of these inexact augmented Lagrangian subproblems, we develop a bound-constrained trust-region method that allows for inexact gradient evaluations, and specialize it to our specific setting that leverages hyperreduced models. This approach circumvents a traditional training phase because the models are built on-the-fly in accordance with the requirements of the trust-region convergence theory. Two numerical experiments (constrained aerodynamic shape design) demonstrate the convergence and efficiency of the proposed work. A speedup of 12.7x (for all computational costs, even costs traditionally considered "offline" such as snapshot collection and data compression) relative to a standard optimization approach that does not leverage model reduction is shown.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14827v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianshu Wen, Matthew J. Zahr</dc:creator>
    </item>
    <item>
      <title>Fast, high-order numerical evaluation of volume potentials via polynomial density interpolation</title>
      <link>https://arxiv.org/abs/2209.03844</link>
      <description>arXiv:2209.03844v5 Announce Type: replace 
Abstract: This article presents a high-order accurate numerical method for the evaluation of singular volume integral operators, with attention focused on operators associated with the Poisson and Helmholtz equations in two dimensions. Following the ideas of the density interpolation method for boundary integral operators, the proposed methodology leverages Green's third identity and a local polynomial interpolant of the density function to recast the volume potential as a sum of single- and double-layer potentials and a volume integral with a regularized (bounded or smoother) integrand. The layer potentials can be accurately and efficiently evaluated everywhere in the plane by means of existing methods (e.g. the density interpolation method), while the regularized volume integral can be accurately evaluated by applying elementary quadrature rules. Compared to straightforwardly computing corrections for every singular and nearly-singular volume target, the method significantly reduces the amount of required specialized quadrature by pushing all singular and near-singular corrections to near-singular layer-potential evaluations at target points in a small neighborhood of the domain boundary. Error estimates for the regularization and quadrature approximations are provided. The method is compatible with well-established fast algorithms, being both efficient not only in the online phase but also to set-up. Numerical examples demonstrate the high-order accuracy and efficiency of the proposed methodology; applications to inhomogeneous scattering are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.03844v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jcp.2024.113091</arxiv:DOI>
      <dc:creator>Thomas G. Anderson, Marc Bonnet, Luiz M. Faria, Carlos P\'erez-Arancibia</dc:creator>
    </item>
    <item>
      <title>A two level approach for simulating Bose-Einstein condensates by localized orthogonal decomposition</title>
      <link>https://arxiv.org/abs/2212.07392</link>
      <description>arXiv:2212.07392v3 Announce Type: replace 
Abstract: In this work, we consider the numerical computation of ground states and dynamics of single-component Bose-Einstein condensates (BECs). The corresponding models are spatially discretized with a multiscale finite element approach known as Localized Orthogonal Decomposition (LOD). Despite the outstanding approximation properties of such a discretization in the context of BECs, taking full advantage of it without creating severe computational bottlenecks can be tricky. In this paper, we therefore present two fully-discrete numerical approaches that are formulated in such a way that they take special account of the structure of the LOD spaces. One approach is devoted to the computation of ground states and another one for the computation of dynamics. A central focus of this paper is also the discussion of implementation aspects that are very important for the practical realization of the methods. In particular, we discuss the use of suitable data structures that keep the memory costs economical. The paper concludes with various numerical experiments in 1d, 2d and 3d that investigate convergence rates and approximation properties of the methods and which demonstrate their performance and computational efficiency, also in comparison to spectral and standard finite element approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.07392v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian D\"oding, Patrick Henning, Johan W\"arneg{\aa}rd</dc:creator>
    </item>
    <item>
      <title>Contraction and Convergence Rates for Discretized Kinetic Langevin Dynamics</title>
      <link>https://arxiv.org/abs/2302.10684</link>
      <description>arXiv:2302.10684v5 Announce Type: replace 
Abstract: We provide a framework to analyze the convergence of discretized kinetic Langevin dynamics for $M$-$\nabla$Lipschitz, $m$-convex potentials. Our approach gives convergence rates of $\mathcal{O}(m/M)$, with explicit stepsize restrictions, which are of the same order as the stability threshold for Gaussian targets and are valid for a large interval of the friction parameter. We apply this methodology to various integration schemes which are popular in the molecular dynamics and machine learning communities. Further, we introduce the property ``$\gamma$-limit convergent" (GLC) to characterize underdamped Langevin schemes that converge to overdamped dynamics in the high-friction limit and which have stepsize restrictions that are independent of the friction parameter; we show that this property is not generic by exhibiting methods from both the class and its complement. Finally, we provide asymptotic bias estimates for the BAOAB scheme, which remain accurate in the high-friction limit by comparison to a modified stochastic dynamics which preserves the invariant measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.10684v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1137/23M1556289</arxiv:DOI>
      <arxiv:journal_reference>SIAM Journal on Numerical Analysis, 62(3):1226-1258, 2024</arxiv:journal_reference>
      <dc:creator>Benedict Leimkuhler, Daniel Paulin, Peter A. Whalley</dc:creator>
    </item>
    <item>
      <title>Randomized approximation of summable sequences -- adaptive and non-adaptive</title>
      <link>https://arxiv.org/abs/2308.01705</link>
      <description>arXiv:2308.01705v3 Announce Type: replace 
Abstract: We prove lower bounds for the randomized approximation of the embedding $\ell_1^m \rightarrow \ell_\infty^m$ based on algorithms that use arbitrary linear (hence non-adaptive) information provided by a (randomized) measurement matrix $N \in \mathbb{R}^{n \times m}$. These lower bounds reflect the increasing difficulty of the problem for $m \to \infty$, namely, a term $\sqrt{\log m}$ in the complexity $n$. This result implies that non-compact operators between arbitrary Banach spaces are not approximable using non-adaptive Monte Carlo methods. We also compare these lower bounds for non-adaptive methods with upper bounds based on adaptive, randomized methods for recovery for which the complexity $n$ only exhibits a $(\log\log m)$-dependence. In doing so we give an example of linear problems where the error for adaptive vs. non-adaptive Monte Carlo methods shows a gap of order $n^{1/2} ( \log n)^{-1/2}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01705v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Kunsch, Erich Novak, Marcin Wnuk</dc:creator>
    </item>
    <item>
      <title>A penalized Allen-Cahn equation for the mean curvature flow of thin structures</title>
      <link>https://arxiv.org/abs/2310.10272</link>
      <description>arXiv:2310.10272v2 Announce Type: replace 
Abstract: This paper addresses the approximation of the mean curvature flow of thin structures for which classical phase field methods are not suitable. By thin structures, we mean surfaces that are not domain boundaries, typically higher codimension objects such as 1D curves in 3D, i.e. filaments, or soap films spanning a boundary curve. To approximate the mean curvature flow of such surfaces, we consider a small thickening and we apply to the thickened set an evolution model that combines the classical Allen-Cahn equation with a penalty term that takes on larger values around the skeleton of the set. The novelty of our approach lies in the definition of this penalty term that guarantees a minimal thickness of the evolving set and prevents it from disappearing unexpectedly. We prove a few theoretical properties of our model, provide examples showing the connection with higher codimension mean curvature flow, and introduce a quasi-static numerical scheme with explicit integration of the penalty term. We illustrate the numerical efficiency of the model with accurate approximations of filament structures evolving by mean curvature flow, and we also illustrate its ability to find complex 3D approximations of solutions to the Steiner problem or the Plateau problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.10272v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elie Bretin, Chih-Kang Huang, Simon Masnou</dc:creator>
    </item>
    <item>
      <title>On the data-driven description of lattice materials mechanics</title>
      <link>https://arxiv.org/abs/2310.20056</link>
      <description>arXiv:2310.20056v2 Announce Type: replace 
Abstract: In the emerging field of mechanical metamaterials, using periodic lattice structures as a primary ingredient is relatively frequent. However, the choice of aperiodic lattices in these structures presents unique advantages regarding failure, e.g., buckling or fracture, because avoiding repeated patterns prevents global failures, with local failures occurring in turn that can beneficially delay structural collapse. Therefore, it is expedient to develop models for computing efficiently the effective mechanical properties in lattices from different general features while addressing the challenge of presenting topologies (or graphs) of different sizes. In this paper, we develop a deep learning model to predict energetically-equivalent mechanical properties of linear elastic lattices effectively. Considering the lattice as a graph and defining material and geometrical features on such, we show that Graph Neural Networks provide more accurate predictions than a dense, fully connected strategy, thanks to the geometrically induced bias through graph representation, closer to the underlying equilibrium laws from mechanics solved in the direct problem. Leveraging the efficient forward-evaluation of a vast number of lattices using this surrogate enables the inverse problem, i.e., to obtain a structure having prescribed specific behavior, which is ultimately suitable for multiscale structural optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.20056v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.rineng.2024.102235</arxiv:DOI>
      <dc:creator>Ismael Ben-Yelun, Luis Irastorza-Valera, Luis Saucedo-Mora, Francisco Javier Mont\'ans, Francisco Chinesta</dc:creator>
    </item>
    <item>
      <title>Adjoint Monte Carlo Method</title>
      <link>https://arxiv.org/abs/2401.08361</link>
      <description>arXiv:2401.08361v2 Announce Type: replace 
Abstract: This survey explores the development of adjoint Monte Carlo methods for solving optimization problems governed by kinetic equations, a common challenge in areas such as plasma control and device design. These optimization problems are particularly demanding due to the high dimensionality of the phase space and the randomness in evaluating the objective functional, a consequence of using a forward Monte Carlo solver. To overcome these difficulties, a range of ``adjoint Monte Carlo methods'' have been devised. These methods skillfully combine Monte Carlo gradient estimators with PDE-constrained optimization, introducing innovative solutions tailored for kinetic applications. In this review, we begin by examining three primary strategies for Monte Carlo gradient estimation: the score function approach, the reparameterization trick, and the coupling method. We also delve into the adjoint-state method, an essential element in PDE-constrained optimization. Focusing on applications in the radiative transfer equation and the nonlinear Boltzmann equation, we provide a comprehensive guide on how to integrate Monte Carlo gradient techniques within both the optimize-then-discretize and the discretize-then-optimize frameworks from PDE-constrained optimization. This approach leads to the formulation of effective adjoint Monte Carlo methods, enabling efficient gradient estimation in complex, high-dimensional optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08361v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Russel Caflisch, Yunan Yang</dc:creator>
    </item>
    <item>
      <title>Parallelly Sliced Optimal Transport on Spheres and on the Rotation Group</title>
      <link>https://arxiv.org/abs/2401.16896</link>
      <description>arXiv:2401.16896v2 Announce Type: replace 
Abstract: Sliced optimal transport, which is basically a Radon transform followed by one-dimensional optimal transport, became popular in various applications due to its efficient computation. In this paper, we deal with sliced optimal transport on the sphere $\mathbb{S}^{d-1}$ and on the rotation group SO(3). We propose a parallel slicing procedure of the sphere which requires again only optimal transforms on the line. We analyze the properties of the corresponding parallelly sliced optimal transport, which provides in particular a rotationally invariant metric on the spherical probability measures. For SO(3), we introduce a new two-dimensional Radon transform and develop its singular value decomposition. Based on this, we propose a sliced optimal transport on SO(3).
  As Wasserstein distances were extensively used in barycenter computations, we derive algorithms to compute the barycenters with respect to our new sliced Wasserstein distances and provide synthetic numerical examples on the 2-sphere that demonstrate their behavior for both the free and fixed support setting of discrete spherical measures. In terms of computational speed, they outperform the existing methods for semicircular slicing as well as the regularized Wasserstein barycenters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16896v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Quellmalz, L\'eo Buecher, Gabriele Steidl</dc:creator>
    </item>
    <item>
      <title>Parameterized Wasserstein Gradient Flow</title>
      <link>https://arxiv.org/abs/2404.19133</link>
      <description>arXiv:2404.19133v2 Announce Type: replace 
Abstract: We develop a fast and scalable numerical approach to solve Wasserstein gradient flows (WGFs), particularly suitable for high-dimensional cases. Our approach is to use general reduced-order models, like deep neural networks, to parameterize the push-forward maps such that they can push a simple reference density to the one solving the given WGF. The new dynamical system is called parameterized WGF (PWGF), and it is defined on the finite-dimensional parameter space equipped with a pullback Wasserstein metric. Our numerical scheme can approximate the solutions of WGFs for general energy functionals effectively, without requiring spatial discretization or nonconvex optimization procedures, thus avoiding some limitations of classical numerical methods and more recent deep-learning-based approaches. A comprehensive analysis of the approximation errors measured by Wasserstein distance is also provided in this work. Numerical experiments show promising computational efficiency and verified accuracy on various WGF examples using our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19133v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijie Jin, Shu Liu, Hao Wu, Xiaojing Ye, Haomin Zhou</dc:creator>
    </item>
    <item>
      <title>Improving the convergence analysis of linear subdivision schemes</title>
      <link>https://arxiv.org/abs/2405.09414</link>
      <description>arXiv:2405.09414v2 Announce Type: replace 
Abstract: This work presents several new results concerning the analysis of the convergence of binary, univariate, and linear subdivision schemes, all related to the {\it contractivity factor} of a convergent scheme. First, we prove that a convergent scheme cannot have a contractivity factor lower than half. Since the lower this factor is, the faster is the convergence of the scheme, schemes with contractivity factor $\frac{1}{2}$, such as those generating spline functions, have optimal convergence rate.
  Additionally, we provide further insights and conditions for the convergence of linear schemes and demonstrate their applicability in an improved algorithm for determining the convergence of such subdivision schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09414v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nira Dyn, Nir Sharon</dc:creator>
    </item>
    <item>
      <title>Randomized Householder QR</title>
      <link>https://arxiv.org/abs/2405.10923</link>
      <description>arXiv:2405.10923v2 Announce Type: replace 
Abstract: This paper introduces a randomized Householder QR factorization (RHQR). This factorization can be used to obtain a well conditioned basis of a vector space and thus can be employed in a variety of applications. The RHQR factorization of the input matrix $W$ is equivalent to the standard Householder QR factorization of matrix $\Psi W$, where $\Psi$ is a sketching matrix that can be obtained from any subspace embedding technique. For this reason, the RHQR factorization can also be reconstructed from the Householder QR factorization of the sketched problem, yielding a single-synchronization randomized QR factorization (recRHQR). In most contexts, left-looking RHQR requires a single synchronization per iteration, with half the computational cost of Householder QR, and a similar cost to Randomized Gram-Schmidt (RGS) overall. We discuss the usage of RHQR factorization in the Arnoldi process and then in GMRES, showing thus how it can be used in Krylov subspace methods to solve systems of linear equations. Based on Charles Sheffield's connection between Householder QR and Modified Gram-Schmidt (MGS), a BLAS2-RGS is also derived. A finite precision analysis shows that, under mild probabilistic assumptions, the RHQR factorization of the input matrix $W$ inherits the stability of the Householder QR factorization, producing a well-conditioned basis and a columnwise backward stable factorization, all independently of the condition number of the input $W$, and with the accuracy of the sketching step. We study the subsampled randomized Hadamard transform (SRHT) as a very stable sketching technique.
  Numerical experiments show that RHQR produces a well conditioned basis whose sketch is numerically orthogonal and an accurate factorization, even for the most difficult inputs and with high-dimensional operations made in half-precision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10923v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laura Grigori, Edouard Timsit</dc:creator>
    </item>
    <item>
      <title>Weak convergence rates for temporal numerical approximations of stochastic wave equations with multiplicative noise</title>
      <link>https://arxiv.org/abs/1901.05535</link>
      <description>arXiv:1901.05535v2 Announce Type: replace-cross 
Abstract: In this work we establish weak convergence rates for temporal discretisations of stochastic wave equations with multiplicative noise, in particular, for the hyperbolic Anderson model. For this class of stochastic partial differential equations the weak convergence rates we obtain are indeed twice the known strong rates. To the best of our knowledge, our findings are the first in the scientific literature which provide essentially sharp weak convergence rates for temporal discretisations of stochastic wave equations with multiplicative noise. Key ideas of our proof are a sophisticated splitting of the error and applications of the recently introduced mild It\^{o} formula. We complement our analytical findings by means of numerical simulations in Python for the decay of the weak approximation error for SPDEs for four different test functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:1901.05535v2</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sonja Cox, Arnulf Jentzen, Felix Lindner</dc:creator>
    </item>
    <item>
      <title>Vocabulary for Universal Approximation: A Linguistic Perspective of Mapping Compositions</title>
      <link>https://arxiv.org/abs/2305.12205</link>
      <description>arXiv:2305.12205v2 Announce Type: replace-cross 
Abstract: In recent years, deep learning-based sequence modelings, such as language models, have received much attention and success, which pushes researchers to explore the possibility of transforming non-sequential problems into a sequential form. Following this thought, deep neural networks can be represented as composite functions of a sequence of mappings, linear or nonlinear, where each composition can be viewed as a \emph{word}. However, the weights of linear mappings are undetermined and hence require an infinite number of words. In this article, we investigate the finite case and constructively prove the existence of a finite \emph{vocabulary} $V=\{\phi_i: \mathbb{R}^d \to \mathbb{R}^d | i=1,...,n\}$ with $n=O(d^2)$ for the universal approximation. That is, for any continuous mapping $f: \mathbb{R}^d \to \mathbb{R}^d$, compact domain $\Omega$ and $\varepsilon&gt;0$, there is a sequence of mappings $\phi_{i_1}, ..., \phi_{i_m} \in V, m \in \mathbb{Z}_+$, such that the composition $\phi_{i_m} \circ ... \circ \phi_{i_1} $ approximates $f$ on $\Omega$ with an error less than $\varepsilon$. Our results demonstrate an unusual approximation power of mapping compositions and motivate a novel compositional model for regular languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.12205v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongqiang Cai</dc:creator>
    </item>
    <item>
      <title>Interpolating Parametrized Quantum Circuits using Blackbox Queries</title>
      <link>https://arxiv.org/abs/2310.04396</link>
      <description>arXiv:2310.04396v3 Announce Type: replace-cross 
Abstract: This article focuses on developing classical surrogates for parametrized quantum circuits using interpolation via (trigonometric) polynomials. We develop two algorithms for the construction of such surrogates and prove performance guarantees. The constructions are based on circuit evaluations which are blackbox in the sense that no structural specifics of the circuits are exploited. While acknowledging the limitations of the blackbox approach compared to whitebox evaluations, which exploit specific circuit properties, we demonstrate scenarios in which the blackbox approach might prove beneficial. Sample applications include but are not restricted to the approximation of VQEs and the alleviaton of the barren plateau problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04396v3</guid>
      <category>quant-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lars Simon, Holger Eble, Hagen-Henrik Kowalski, Manuel Radons</dc:creator>
    </item>
    <item>
      <title>Unbiased Kinetic Langevin Monte Carlo with Inexact Gradients</title>
      <link>https://arxiv.org/abs/2311.05025</link>
      <description>arXiv:2311.05025v2 Announce Type: replace-cross 
Abstract: We present an unbiased method for Bayesian posterior means based on kinetic Langevin dynamics that combines advanced splitting methods with enhanced gradient approximations. Our approach avoids Metropolis correction by coupling Markov chains at different discretization levels in a multilevel Monte Carlo approach. Theoretical analysis demonstrates that our proposed estimator is unbiased, attains finite variance, and satisfies a central limit theorem. It can achieve accuracy $\epsilon&gt;0$ for estimating expectations of Lipschitz functions in $d$ dimensions with $\mathcal{O}(d^{1/4}\epsilon^{-2})$ expected gradient evaluations, without assuming warm start. We exhibit similar bounds using both approximate and stochastic gradients, and our method's computational cost is shown to scale independently of the size of the dataset. The proposed method is tested using a multinomial regression problem on the MNIST dataset and a Poisson regression model for soccer scores. Experiments indicate that the number of gradient evaluations per effective sample is independent of dimension, even when using inexact gradients. For product distributions, we give dimension-independent variance bounds. Our results demonstrate that the unbiased algorithm we present can be much more efficient than the ``gold-standard" randomized Hamiltonian Monte Carlo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05025v2</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neil K. Chada, Benedict Leimkuhler, Daniel Paulin, Peter A. Whalley</dc:creator>
    </item>
    <item>
      <title>An $\ell^1$-Plug-and-Play Approach for MPI Using a Zero Shot Denoiser with Evaluation on the 3D Open MPI Dataset</title>
      <link>https://arxiv.org/abs/2401.00275</link>
      <description>arXiv:2401.00275v2 Announce Type: replace-cross 
Abstract: Objective: Magnetic particle imaging (MPI) is an emerging medical imaging modality which has gained increasing interest in recent years. Among the benefits of MPI are its high temporal resolution, and that the technique does not expose the specimen to any kind of ionizing radiation. It is based on the non-linear response of magnetic nanoparticles to an applied magnetic field. From the electric signal measured in receive coils, the particle concentration has to be reconstructed. Due to the ill-posedness of the reconstruction problem, various regularization methods have been proposed for reconstruction ranging from early stopping methods, via classical Tikhonov regularization and iterative methods to modern machine learning approaches. In this work, we contribute to the latter class: we propose a plug-and-play approach based on a generic zero-shot denoiser with an $\ell^1$-prior.
  Approach: We validate the reconstruction parameters of the method on a hybrid dataset and compare it with the baseline Tikhonov, DIP and the previous PP-MPI, which is a plug-and-play method with denoiser trained on MPI-friendly data.
  Main results: We offer a quantitative and qualitative evaluation of the zero-shot plug-and-play approach on the 3D Open MPI dataset. Moreover, we show the quality of the approach with different levels of preprocessing of the data.
  Significance: The proposed method employs a zero-shot denoiser which has not been trained for the MPI task and therefore saves the cost for training. Moreover, it offers a method that can be potentially applied in future MPI contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00275v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vladyslav Gapyak, Corinna Rentschler, Thomas M\"arz, Andreas Weinmann</dc:creator>
    </item>
    <item>
      <title>Space-time unfitted finite elements on moving explicit geometry representations</title>
      <link>https://arxiv.org/abs/2401.12649</link>
      <description>arXiv:2401.12649v2 Announce Type: replace-cross 
Abstract: This work proposes a novel variational approximation of partial differential equations on moving geometries determined by explicit boundary representations. The benefits of the proposed formulation are the ability to handle large displacements of explicitly represented domain boundaries without generating body-fitted meshes and remeshing techniques. For the space discretization, we use a background mesh and an unfitted method that relies on integration on cut cells only. We perform this intersection by using clipping algorithms. To deal with the mesh movement, we pullback the equations to a reference configuration (the spatial mesh at the initial time slab times the time interval) that is constant in time. This way, the geometrical intersection algorithm is only required in 3D, another key property of the proposed scheme. At the end of the time slab, we compute the deformed mesh, intersect the deformed boundary with the background mesh, and consider an exact transfer operator between meshes to compute jump terms in the time discontinuous Galerkin integration. The transfer is also computed using geometrical intersection algorithms. We demonstrate the applicability of the method to fluid problems around rotating (2D and 3D) geometries described by oriented boundary meshes. We also provide a set of numerical experiments that show the optimal convergence of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12649v2</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Santiago Badia, Pere A. Martorell, Francesc Verdugo</dc:creator>
    </item>
    <item>
      <title>Convergence of the deep BSDE method for stochastic control problems formulated through the stochastic maximum principle</title>
      <link>https://arxiv.org/abs/2401.17472</link>
      <description>arXiv:2401.17472v2 Announce Type: replace-cross 
Abstract: It is well-known that decision-making problems from stochastic control can be formulated by means of a forward-backward stochastic differential equation (FBSDE). Recently, the authors of Ji et al. 2022 proposed an efficient deep learning algorithm based on the stochastic maximum principle (SMP). In this paper, we provide a convergence result for this deep SMP-BSDE algorithm and compare its performance with other existing methods. In particular, by adopting a strategy as in Han and Long 2020, we derive a-posteriori estimate, and show that the total approximation error can be bounded by the value of the loss functional and the discretization error. We present numerical examples for high-dimensional stochastic control problems, both in case of drift- and diffusion control, which showcase superior performance compared to existing algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17472v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>q-fin.CP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhipeng Huang, Balint Negyesi, Cornelis W. Oosterlee</dc:creator>
    </item>
    <item>
      <title>FMint: Bridging Human Designed and Data Pretrained Models for Differential Equation Foundation Model</title>
      <link>https://arxiv.org/abs/2404.14688</link>
      <description>arXiv:2404.14688v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a pre-trained foundation model \textbf{FMint} (\textbf{F}oundation \textbf{M}odel based on \textbf{In}i\textbf{t}ialization), designed to speed up large-scale simulations of various differential equations with high accuracy via error correction. Human-designed simulation algorithms excel at capturing the fundamental physics of engineering problems, but often need to balance the trade-off between accuracy and efficiency. While deep learning methods offer innovative solutions across numerous scientific fields, they frequently fall short in domain-specific knowledge. FMint bridges these gaps through conditioning on the initial coarse solutions obtained from conventional human-designed algorithms, and trained to obtain refined solutions for various differential equations. Based on the backbone of large language models, we adapt the in-context learning scheme to learn a universal error correction method for dynamical systems from given prompted sequences of coarse solutions. The model is pre-trained on a corpus of 600K ordinary differential equations (ODEs), and we conduct extensive experiments on both in-distribution and out-of-distribution tasks. FMint outperforms various baselines on large-scale simulation, and demonstrates its capability in generalization to unseen ODEs. Our approach achieves an accuracy improvement of 1 to 2 orders of magnitude over state-of-the-art dynamical system simulators, and delivers a 5X speedup compared to traditional numerical algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14688v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zezheng Song, Jiaxin Yuan, Haizhao Yang</dc:creator>
    </item>
    <item>
      <title>Temporal Evolution of Bradford Curves in Specialized Library Contexts</title>
      <link>https://arxiv.org/abs/2404.19267</link>
      <description>arXiv:2404.19267v2 Announce Type: replace-cross 
Abstract: Bradford's law of bibliographic scattering is a fundamental principle in bibliometrics, offering valuable guidance for academic libraries in literature search and procurement. However, Bradford curves can exhibit various shapes over time, and predicting these shapes remains a challenge due to a lack of causal explanation. This paper attributes the deviations from the theoretical J-shape to integer constraints on the number of journals and articles, extending Leimkuhler and Egghe's formulas to encompass highly productive core journals, where the theoretical journal number falls below one. Using the Simon-Yule model, key parameters of the extended formulas are identified and analyzed. The paper explains the reasons for the Groos Droop and examines the critical points for shape changes. The proposed formulas are validated with empirical data from literature, demonstrating that this method can effectively predict the evolution of Bradford curves, thereby aiding academic libraries in the procurement and utilization of scientific literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19267v2</guid>
      <category>cs.DL</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haobai Xue, Xian Liu</dc:creator>
    </item>
    <item>
      <title>Chordal-NMF with Riemannian Multiplicative Update</title>
      <link>https://arxiv.org/abs/2405.12823</link>
      <description>arXiv:2405.12823v2 Announce Type: replace-cross 
Abstract: Nonnegative Matrix Factorization (NMF) is the problem of approximating a given nonnegative matrix M through the conic combination of two nonnegative low-rank matrices W and H. Traditionally NMF is tackled by optimizing a specific objective function evaluating the quality of the approximation. This assessment is often done based on the Frobenius norm. In this study, we argue that the Frobenius norm as the "point-to-point" distance may not always be appropriate. Due to the nonnegative combination resulting in a polyhedral cone, this conic perspective of NMF may not naturally align with conventional point-to-point distance measures. Hence, a ray-to-ray chordal distance is proposed as an alternative way of measuring the discrepancy between M and WH. This measure is related to the Euclidean distance on the unit sphere, motivating us to employ nonsmooth manifold optimization approaches.
  We apply Riemannian optimization technique to solve chordal-NMF by casting it on a manifold. Unlike existing works on Riemannian optimization that require the manifold to be smooth, the nonnegativity in chordal-NMF is a non-differentiable manifold. We propose a Riemannian Multiplicative Update (RMU) that preserves the convergence properties of Riemannian gradient descent without breaking the smoothness condition on the manifold.
  We showcase the effectiveness of the Chordal-NMF on synthetic datasets as well as real-world multispectral images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12823v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Flavia Esposito, Andersen Ang</dc:creator>
    </item>
  </channel>
</rss>
