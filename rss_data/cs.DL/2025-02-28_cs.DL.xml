<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Feb 2025 05:01:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Multilingual research dissemination: Current practices and implications for bibliometrics</title>
      <link>https://arxiv.org/abs/2502.19663</link>
      <description>arXiv:2502.19663v1 Announce Type: new 
Abstract: English is widely used as a lingua franca in scholarly communication, yet preserving local languages is vital to reaching a broader audience. Disseminating research in multiple languages can help ensure equitable access, a responsibility shared by both publishers and authors. This study examines the practices of both groups to identify any notable differences. Several academic social networks, preprint servers, and repositories are analysed to evaluate the resources currently available and their existing policies. Additionally, journals that actively promote multilingual dissemination are reviewed to understand their implementation strategies and how these align with the standards set by the DOI Registration Agency (DOI RA). From the author's perspective, differing policies across platforms can heavily influence decisions, mainly because not all platforms provide relationship metadata. Publishers face similar challenges, underscoring the urgent need for standardisation. Moreover, the lack of consistency creates opportunities for unethical practices in academia, such as counting total of citations originating from the same article in different languages. This highlights the importance of a more comprehensive approach to evaluating research beyond citation and document counts. Collaboration among publishers, authors, and other stakeholders is essential to fostering greater understanding and preventing misconceptions in the academic landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19663v1</guid>
      <category>cs.DL</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Faizhal Arif Santosa, Barbara S. Lancho Barrantes</dc:creator>
    </item>
    <item>
      <title>Old Experience Helps: Leveraging Survey Methodology to Improve AI Text Annotation Reliability in Social Sciences</title>
      <link>https://arxiv.org/abs/2502.19679</link>
      <description>arXiv:2502.19679v1 Announce Type: new 
Abstract: This paper introduces a framework for assessing the reliability of Large Language Model (LLM) text annotations in social science research by adapting established survey methodology principles. Drawing parallels between survey respondent behavior and LLM outputs, the study implements three key interventions: option randomization, position randomization, and reverse validation. While traditional accuracy metrics may mask model instabilities, particularly in edge cases, our framework provides a more comprehensive reliability assessment. Using the F1000 dataset in biomedical science and three sizes of Llama models (8B, 70B, and 405B parameters), the paper demonstrates that these survey-inspired interventions can effectively identify unreliable annotations that might otherwise go undetected through accuracy metrics alone. The results show that 5-25% of LLM annotations change under these interventions, with larger models exhibiting greater stability. Notably, for rare categories approximately 50% of "correct" annotations demonstrate low reliability when subjected to this framework. The paper introduce an information-theoretic reliability score (R-score) based on Kullback-Leibler divergence that quantifies annotation confidence and distinguishes between random guessing and meaningful annotations at the case level. This approach complements existing expert validation methods by providing a scalable way to assess internal annotation reliability and offers practical guidance for prompt design and downstream analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19679v1</guid>
      <category>cs.DL</category>
      <category>cs.HC</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linzhuo li</dc:creator>
    </item>
    <item>
      <title>BatteryLife: A Comprehensive Dataset and Benchmark for Battery Life Prediction</title>
      <link>https://arxiv.org/abs/2502.18807</link>
      <description>arXiv:2502.18807v2 Announce Type: replace-cross 
Abstract: Battery Life Prediction (BLP), which relies on time series data produced by battery degradation tests, is crucial for battery utilization, optimization, and production. Despite impressive advancements, this research area faces three key challenges. Firstly, the limited size of existing datasets impedes insights into modern battery life data. Secondly, most datasets are restricted to small-capacity lithium-ion batteries tested under a narrow range of diversity in labs, raising concerns about the generalizability of findings. Thirdly, inconsistent and limited benchmarks across studies obscure the effectiveness of baselines and leave it unclear if models popular in other time series fields are effective for BLP. To address these challenges, we propose BatteryLife, a comprehensive dataset and benchmark for BLP. BatteryLife integrates 16 datasets, offering a 2.4 times sample size compared to the previous largest dataset, and provides the most diverse battery life resource with batteries from 8 formats, 80 chemical systems, 12 operating temperatures, and 646 charge/discharge protocols, including both laboratory and industrial tests. Notably, BatteryLife is the first to release battery life datasets of zinc-ion batteries, sodium-ion batteries, and industry-tested large-capacity lithium-ion batteries. With the comprehensive dataset, we revisit the effectiveness of baselines popular in this and other time series fields. Furthermore, we propose CyclePatch, a plug-in technique that can be employed in a series of neural networks. Extensive benchmarking of 18 methods reveals that models popular in other time series fields can be unsuitable for BLP, and CyclePatch consistently improves model performance establishing state-of-the-art benchmarks. Moreover, BatteryLife evaluates model performance across aging conditions and domains. BatteryLife is available at https://github.com/Ruifeng-Tan/BatteryLife.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18807v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ruifeng Tan, Weixiang Hong, Jiayue Tang, Xibin Lu, Ruijun Ma, Xiang Zheng, Jia Li, Jiaqiang Huang, Tong-Yi Zhang</dc:creator>
    </item>
  </channel>
</rss>
