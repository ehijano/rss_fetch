<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.PM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.PM</link>
    <description>q-fin.PM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.PM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Apr 2025 01:43:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Breaking the Dimensional Barrier: A Pontryagin-Guided Direct Policy Optimization for Continuous-Time Multi-Asset Portfolio</title>
      <link>https://arxiv.org/abs/2504.11116</link>
      <description>arXiv:2504.11116v2 Announce Type: replace 
Abstract: Solving large-scale, continuous-time portfolio optimization problems involving numerous assets and state-dependent dynamics has long been challenged by the curse of dimensionality. Traditional dynamic programming and PDE-based methods, while rigorous, typically become computationally intractable beyond few state variables (3~6 limit in prior studies). To overcome this critical barrier, we introduce the Pontryagin-Guided Direct Policy Optimization (PG-DPO) framework. PG-DPO leverages Pontryagin's Maximum Principle (PMP) and backpropagation-through-time (BPTT) to guide neural network policies, handling exogenous states without dense grids. This PMP-guided approach holds potential for a broad class of sufficiently regular continuous-time control problems. Crucially, our computationally efficient "Two-Stage" variant exploits rapidly stabilizing BPTT costate estimates, converting them into near-optimal Pontryagin controls after only a short warm-up, significantly reducing training overhead. This enables a breakthrough in scalability: numerical experiments show PG-DPO successfully tackles problems with dimensions previously considered far out of reach (up to 50 assets and 10 state variables). The framework delivers near-optimal policies, offering a practical and powerful alternative for high-dimensional continuous-time portfolio choice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11116v2</guid>
      <category>q-fin.PM</category>
      <category>q-fin.CP</category>
      <pubDate>Wed, 23 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeonggyu Huh, Jaegi Jeon, Hyeng Keun Koo</dc:creator>
    </item>
  </channel>
</rss>
