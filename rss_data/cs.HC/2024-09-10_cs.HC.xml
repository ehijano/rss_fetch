<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Sep 2024 01:47:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Perception of Stress in Graph Drawings</title>
      <link>https://arxiv.org/abs/2409.04493</link>
      <description>arXiv:2409.04493v1 Announce Type: new 
Abstract: Most of the common graph layout principles (a.k.a. "aesthetics") on which many graph drawing algorithms are based are easy to define and to perceive. For example, the number of pairs of edges that cross each other, how symmetric a drawing looks, the aspect ratio of the bounding box, or the angular resolution at the nodes. The extent to which a graph drawing conforms to these principles can be determined by looking at how it is drawn -- that is, by looking at the marks on the page -- without consideration for the underlying structure of the graph. A key layout principle is that of optimising `stress', the basis for many algorithms such as the popular Kamada \&amp; Kawai algorithm and several force-directed algorithms. The stress of a graph drawing is, loosely speaking, the extent to which the geometric distance between each pair of nodes is proportional to the shortest path between them -- over the whole graph drawing. The definition of stress therefore relies on the underlying structure of the graph (the `paths') in a way that other layout principles do not, making stress difficult to describe to novices unfamiliar with graph drawing principles, and, we believe, difficult to perceive. We conducted an experiment to see whether people (novices as well as experts) can see stress in graph drawings, and found that it is possible to train novices to `see' stress -- even if their perception strategies are not based on the definitional concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04493v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gavin J. Mooney, Helen C. Purchase, Michael Wybrow, Stephen G. Kobourov, Jacob Miller</dc:creator>
    </item>
    <item>
      <title>Toward LLM-Powered Social Robots for Supporting Sensitive Disclosures of Stigmatized Health Conditions</title>
      <link>https://arxiv.org/abs/2409.04508</link>
      <description>arXiv:2409.04508v1 Announce Type: new 
Abstract: Disclosing sensitive health conditions offers significant benefits at both individual and societal levels. However, patients often face challenges due to concerns about stigma. The use of social robots and chatbots to support sensitive disclosures is gaining traction, especially with the emergence of LLM models. Yet, numerous technical, ethical, privacy, safety, efficacy, and reporting concerns must be carefully addressed in this context. In this position paper, we focus on the example of HIV status disclosure, examining key opportunities, technical considerations, and risks associated with LLM-backed social robotics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04508v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alemitu Bezabih, Shadi Nourriz, C. Estelle Smith</dc:creator>
    </item>
    <item>
      <title>Paradoxes of Openness and Trans Experiences in Open Source Software</title>
      <link>https://arxiv.org/abs/2409.04511</link>
      <description>arXiv:2409.04511v1 Announce Type: new 
Abstract: In recent years, concerns have increased over the lack of contributor diversity in open source software (OSS), despite its status as a paragon of open collaboration. OSS is an important form of digital infrastructure and part of a career path for many developers. While there exists a growing body of literature on cisgender women's under-representation in OSS, the experiences of contributors from other marginalized groups are comparatively absent from the literature. Such is the case for trans contributors, a historically influential group in OSS. In this study, we interviewed 21 trans participants to understand and represent their experiences in the OSS literature. From their experiences, we theorize two related paradoxes of openness in OSS: the paradox of openness and display and the paradox of openness and governance. In an increasingly violent world for trans people, we draw on our theorizing to build recommendations for more inclusive and safer OSS projects for contributors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04511v1</guid>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3687047</arxiv:DOI>
      <dc:creator>Hana Frluckaj, James Howison, Laura Dabbish, Nikki Stevens</dc:creator>
    </item>
    <item>
      <title>Developing a Modular Toolkit for Rapid Prototyping of Wearable Vibrotactile Haptic Harness</title>
      <link>https://arxiv.org/abs/2409.04579</link>
      <description>arXiv:2409.04579v1 Announce Type: new 
Abstract: This paper presents a toolkit for rapid harness prototyping. These wearable structures attach vibrotactile actuators to the body using modular elements like 3D printed joints, laser cut or vinyl cutter-based sheets and magnetic clasps. This facilitates easy customization and assembly. The toolkit's primary objective is to simplify the design of haptic wearables, making research in this field easier and more approachable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04579v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sandeep Kollannur (Katie),  Katherine (Katie),  Robertson, Heather Culbertson</dc:creator>
    </item>
    <item>
      <title>From Data Dump to Digestible Chunks: Automated Segmentation and Summarization of Provenance Logs for Communication</title>
      <link>https://arxiv.org/abs/2409.04616</link>
      <description>arXiv:2409.04616v1 Announce Type: new 
Abstract: Communicating one's sensemaking during a complex analysis session to explain thought processes is hard, yet most intelligence occurs in collaborative settings. Team members require a deeper understanding of the work being completed by their peers and subordinates, but little research has fully articulated best practices for analytic provenance consumers. This work proposes an automatic summarization technique that separates an analysis session and summarizes interaction provenance as textual blurbs to allow for meta-analysis of work done. Focusing on the domain of intelligence analysis, we demonstrate our segmentation technique using five datasets, including both publicly available and classified interaction logs. We shared our demonstration with a notoriously inaccessible population of expert reviewers with experience as United States Department of Defense analysts. Our findings indicate that the proposed pipeline effectively generates cards that display key events from interaction logs, facilitating the sharing of analysis progress. Yet, we also hear that there is a need for more prominent justifications and pattern elicitation controls to communicate analysis summaries more effectively. The expert review highlights the potential of automated approaches in addressing the challenges of provenance information in complex domains. We'd like to emphasize the need for further research into provenance communication in other domains.
  A free copy of this paper and all supplemental materials are available at https://osf.io/j4bxt</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04616v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jeremy E. Block, Donald Honeycutt, Brett Benda, Benjamin Rheault, Eric D. Ragan</dc:creator>
    </item>
    <item>
      <title>PAIGE: Examining Learning Outcomes and Experiences with Personalized AI-Generated Educational Podcasts</title>
      <link>https://arxiv.org/abs/2409.04645</link>
      <description>arXiv:2409.04645v1 Announce Type: new 
Abstract: Generative AI is revolutionizing content creation and has the potential to enable real-time, personalized educational experiences. We investigated the effectiveness of converting textbook chapters into AI-generated podcasts and explored the impact of personalizing these podcasts for individual learner profiles. We conducted a 3x3 user study with 180 college students in the United States, comparing traditional textbook reading with both generalized and personalized AI-generated podcasts across three textbook subjects. The personalized podcasts were tailored to students' majors, interests, and learning styles. Our findings show that students found the AI-generated podcast format to be more enjoyable than textbooks and that personalized podcasts led to significantly improved learning outcomes, although this was subject-specific. These results highlight that AI-generated podcasts can offer an engaging and effective modality transformation of textbook material, with personalization enhancing content relevance. We conclude with design recommendations for leveraging AI in education, informed by student feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04645v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tiffany D. Do, Usama Bin Shafqat, Elsie Ling, Nikhil Sarda</dc:creator>
    </item>
    <item>
      <title>Unveiling the Inter-Related Preferences of Crowdworkers: Implications for Personalized and Flexible Platform Design</title>
      <link>https://arxiv.org/abs/2409.04658</link>
      <description>arXiv:2409.04658v1 Announce Type: new 
Abstract: Crowdsourcing platforms have traditionally been designed with a focus on workstation interfaces, restricting the flexibility that crowdworkers need. Recognizing this limitation and the need for more adaptable platforms, prior research has highlighted the diverse work processes of crowdworkers, influenced by factors such as device type and work stage. However, these variables have largely been studied in isolation. Our study is the first to explore the interconnected variabilities among these factors within the crowdwork community. Through a survey involving 150 Amazon Mechanical Turk crowdworkers, we uncovered three distinct groups characterized by their interrelated variabilities in key work aspects. The largest group exhibits a reliance on traditional devices, showing limited interest in integrating smartphones and tablets into their work routines. The second-largest group also primarily uses traditional devices but expresses a desire for supportive tools and scripts that enhance productivity across all devices, particularly smartphones and tablets. The smallest group actively uses and strongly prefers non-workstation devices, especially smartphones and tablets, for their crowdworking activities. We translate our findings into design insights for platform developers, discussing the implications for creating more personalized, flexible, and efficient crowdsourcing environments. Additionally, we highlight the unique work practices of these crowdworker clusters, offering a contrast to those of more traditional and established worker groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04658v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Senjuti Dutta, Rhema Linder, Alex C. Williams, Anastasia Kuzminykh, Scott Ruoti</dc:creator>
    </item>
    <item>
      <title>Exploring Crowdworkers' Perceptions, Current Practices, and Desired Practices Regarding Using Non-Workstation Devices for Crowdwork</title>
      <link>https://arxiv.org/abs/2409.04676</link>
      <description>arXiv:2409.04676v1 Announce Type: new 
Abstract: Despite a plethora of research dedicated to designing HITs for non-workstations, there is a lack of research looking specifically into workers' perceptions of the suitability of these devices for managing and completing work. In this work, we fill this research gap by conducting an online survey of 148 workers on Amazon Mechanical Turk to explore 1. how crowdworkers currently use their non-workstation devices to complete and manage crowdwork, 2. what challenges they face using those devices, and 3. to what extent they wish they could use those devices if their concerns were addressed. Our results show that workers unanimously favor using a desktop to complete and manage crowdwork. While workers occasionally use smartphones or tablets, they find their suitability marginal at best and have little interest in smart speakers and smartwatches, viewing them as unsuitable for crowdwork. When investigating the reason for these views, we find that the key issue is that non workstation devices lack the tooling necessary to automatically find and accept HITs, tooling that workers view as essential in their efforts to compete with bots in accepting high paying work. To address this problem, we propose a new paradigm for finding, accepting, and completing crowdwork that puts crowdworkers on equal footing with bots in these tasks. We also describe future research directions for tailoring HITs to non workstation devices and definitely answering whether smart speakers and smartwatches have a place in crowdwork.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04676v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Senjuti Dutta, Scott Ruoti, Rhema Linder, Alex C. Williams, Anastasia Kuzminykh</dc:creator>
    </item>
    <item>
      <title>XR Prototyping of Mixed Reality Visualizations: Compensating Interaction Latency for a Medical Imaging Robot</title>
      <link>https://arxiv.org/abs/2409.04900</link>
      <description>arXiv:2409.04900v1 Announce Type: new 
Abstract: Researching novel user experiences in medicine is challenging due to limited access to equipment and strict ethical protocols. Extended Reality (XR) simulation technologies offer a cost- and time-efficient solution for developing interactive systems. Recent work has shown Extended Reality Prototyping (XRP)'s potential, but its applicability to specific domains like controlling complex machinery needs further exploration. This paper explores the benefits and limitations of XRP in controlling a mobile medical imaging robot. We compare two XR visualization techniques to reduce perceived latency between user input and robot activation. Our XRP validation study demonstrates its potential for comparative studies, but identifies a gap in modeling human behavior in the analytic XRP validation framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04900v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Hendrik Pl\"umer, Kevin Yu, Ulrich Eck, Denis Kalkofen, Philipp Steininger, Nassir Navab, Markus Tatzgern</dc:creator>
    </item>
    <item>
      <title>How to Align Large Language Models for Teaching English? Designing and Developing LLM based-Chatbot for Teaching English Conversation in EFL, Findings and Limitations</title>
      <link>https://arxiv.org/abs/2409.04987</link>
      <description>arXiv:2409.04987v1 Announce Type: new 
Abstract: This study investigates the design, development, and evaluation of a Large Language Model (LLM)-based chatbot for teaching English conversations in an English as a Foreign Language (EFL) context. Employing the Design and Development Research (DDR), we analyzed needs, established design principles, and iteratively refined a chatbot through experimenting various LLMs and alignment methods. Through both quantitative and qualitative evaluations, we identified the most effective LLM and its prompt combination to generate high-quality, contextually appropriate responses. Interviews with teachers provided insights into desirable system features, potential educational applications, and ethical considerations in the development and deployment of the chatbots. The design iterations yielded the importance of feedback mechanisms and customizable AI personas. Future research should explore adaptive feedback strategies, collaborative approaches with various stakeholders, and the integration of insights from human-computer interaction (HCI) and user experience (UX) design. This study contributes to the growing body of research on applying LLMs in language education, providing insights and recommendations for the design, development, and evaluation of LLM-based chatbots for EFL conversation practice. As the field evolves, ongoing research and collaboration among educators, AI engineers, and other stakeholders will be essential to harness the potential of these technologies to enhance language learning experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04987v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.13490.82883</arxiv:DOI>
      <dc:creator>Jaekwon Park, Jiyoung Bae, Unggi Lee, Taekyung Ahn, Sookbun Lee, Dohee Kim, Aram Choi, Yeil Jeong, Jewoong Moon, Hyeoncheol Kim</dc:creator>
    </item>
    <item>
      <title>Improving Multimodal Emotion Recognition by Leveraging Acoustic Adaptation and Visual Alignment</title>
      <link>https://arxiv.org/abs/2409.05015</link>
      <description>arXiv:2409.05015v2 Announce Type: new 
Abstract: Multimodal Emotion Recognition (MER) aims to automatically identify and understand human emotional states by integrating information from various modalities. However, the scarcity of annotated multimodal data significantly hinders the advancement of this research field. This paper presents our solution for the MER-SEMI sub-challenge of MER 2024. First, to better adapt acoustic modality features for the MER task, we experimentally evaluate the contributions of different layers of the pre-trained speech model HuBERT in emotion recognition. Based on these observations, we perform Parameter-Efficient Fine-Tuning (PEFT) on the layers identified as most effective for emotion recognition tasks, thereby achieving optimal adaptation for emotion recognition with a minimal number of learnable parameters. Second, leveraging the strengths of the acoustic modality, we propose a feature alignment pre-training method. This approach uses large-scale unlabeled data to train a visual encoder, thereby promoting the semantic alignment of visual features within the acoustic feature space. Finally, using the adapted acoustic features, aligned visual features, and lexical features, we employ an attention mechanism for feature fusion. On the MER2024-SEMI test set, the proposed method achieves a weighted F1 score of 88.90%, ranking fourth among all participating teams, validating the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05015v2</guid>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhixian Zhao, Haifeng Chen, Xi Li, Dongmei Jiang, Lei Xie</dc:creator>
    </item>
    <item>
      <title>Using Generative Artificial Intelligence Creatively in the Classroom: Examples and Lessons Learned</title>
      <link>https://arxiv.org/abs/2409.05176</link>
      <description>arXiv:2409.05176v1 Announce Type: new 
Abstract: Although generative artificial intelligence (AI) is not new, recent technological breakthroughs have transformed its capabilities across many domains. These changes necessitate new attention from educators and specialized training within the atmospheric sciences and related fields. Enabling students to use generative AI effectively, responsibly, and ethically is critically important for their academic and professional preparation. Educators can also use generative AI to create engaging classroom activities, such as active learning modules and games, but must be aware of potential pitfalls and biases. There are also ethical implications in using tools that lack transparency, as well as equity concerns for students who lack access to more sophisticated paid versions of generative AI tools. This article is written for students and educators alike, particularly those who want to learn more about generative AI in education, including use cases, ethical concerns, and a brief history of its emergence. Sample user prompts are also provided across numerous applications in education and the atmospheric and related sciences. While we don't have solutions for some broader ethical concerns surrounding the use of generative AI in education, our goal is to start a conversation that could galvanize the education community around shared goals and values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05176v1</guid>
      <category>cs.HC</category>
      <category>physics.ao-ph</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria J. Molina, Amy McGovern, Jhayron S. Perez-Carrasquilla, Robin L. Tanamachi</dc:creator>
    </item>
    <item>
      <title>CARDinality: Interactive Card-shaped Robots with Locomotion and Haptics using Vibration</title>
      <link>https://arxiv.org/abs/2409.05203</link>
      <description>arXiv:2409.05203v1 Announce Type: new 
Abstract: This paper introduces a novel approach to interactive robots by leveraging the form-factor of cards to create thin robots equipped with vibrational capabilities for locomotion and haptic feedback. The system is composed of flat-shaped robots with on-device sensing and wireless control, which offer lightweight portability and scalability. This research introduces a hardware prototype. Applications include augmented card playing, educational tools, and assistive technology, which showcase CARDinality's versatility in tangible interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05203v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3654777.3676421</arxiv:DOI>
      <dc:creator>Aditya Retnanto, Emilie Faracci, Anup Sathya, Yukai Hung, Ken Nakagaki</dc:creator>
    </item>
    <item>
      <title>Don't Leave Me Out: Designing for Device Inclusivity in Mixed Reality Collaboration</title>
      <link>https://arxiv.org/abs/2409.05374</link>
      <description>arXiv:2409.05374v1 Announce Type: new 
Abstract: Modern collaborative Mixed Reality (MR) systems continue to break the boundaries of conventional co-located and remote collaboration and communication. They merge physical and virtual worlds and enable natural interaction, opening up a spectrum of novel opportunities for interpersonal connection. For these connections to be perceived as engaging and positive, collaborators should feel comfortable and experience a sense of belonging. Not having the dedicated devices to smoothly participate in these spaces can hinder this and give users the impression of being left out. To counteract this, we propose to prioritize designing for device inclusivity in MR collaboration, focusing on compensating disadvantages of common non-immersive device classes in cross-device systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05374v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katja Krug, Juli\'an M\'endez, Weizhou Luo, Raimund Dachselt</dc:creator>
    </item>
    <item>
      <title>Visualizing Extensions of Argumentation Frameworks as Layered Graphs</title>
      <link>https://arxiv.org/abs/2409.05457</link>
      <description>arXiv:2409.05457v1 Announce Type: new 
Abstract: The visualization of argumentation frameworks (AFs) is crucial for enabling a wide applicability of argumentative tools. However, their visualization is often considered only as an accompanying part of tools for computing semantics and standard graphical representations are used. We introduce a new visualization technique that draws an AF, together with an extension (as part of the input), as a 3-layer graph layout. Our technique supports the user to more easily explore the visualized AF, better understand extensions, and verify algorithms for computing semantics. To optimize the visual clarity and aesthetics of this layout, we propose to minimize edge crossings in our 3-layer drawing. We do so by an exact ILP-based approach, but also propose a fast heuristic pipeline. Via a quantitative evaluation, we show that the heuristic is feasible even for large instances, while producing at most twice as many crossings as an optimal drawing in most cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05457v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin N\"ollenburg, Christian Pirker, Anna Rapberger, Stefan Woltran, Jules Wulms</dc:creator>
    </item>
    <item>
      <title>Educational Virtual Field Trips based on Social VR and 360{\deg} Spaces</title>
      <link>https://arxiv.org/abs/2409.05496</link>
      <description>arXiv:2409.05496v1 Announce Type: new 
Abstract: Virtual field trips (VFTs) have proven to be valuable learning tools. Such applications are mostly based on 360{\deg} technology and are to be characterized as single-user applications in technological terms. In contrast, Social VR applications are characterized by multi-user capability and user-specific avatars. From a learning perspective, the concepts of collaborative learning and embodiment have long been proposed as conducive to learning. Both concepts might be supported using Social VR. However, little is currently known about the use of Social VR for VFTs. Accordingly, the research questions are to what extent VFTs can be implemented in Social VR environments and how these Social VR-based VFTs are perceived by learners. This article presents an evaluation study on the development and evaluation of a VFT environment using the Social VR platform Mozilla Hubs. It describes the design decisions to create the environment and evaluation results from a mixed-method study (N=16) using a questionnaire and focus group discussions. The study highlighted the opportunities offered by Social VR-based VFTs but also revealed several challenges that need to be addressed to embrace the potential of Social VR-based VFTs to be utilized regularly in education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05496v1</guid>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Surya Kalvakolu, Heinrich S\"obke, Jannicke Baalsrud Hauge, Eckhard Kraft</dc:creator>
    </item>
    <item>
      <title>Enhancing Critical Thinking in Education by means of a Socratic Chatbot</title>
      <link>https://arxiv.org/abs/2409.05511</link>
      <description>arXiv:2409.05511v1 Announce Type: new 
Abstract: While large language models (LLMs) are increasingly playing a pivotal role in education by providing instantaneous, adaptive responses, their potential to promote critical thinking remains understudied. In this paper, we fill such a gap and present an innovative educational chatbot designed to foster critical thinking through Socratic questioning. Unlike traditional intelligent tutoring systems, including educational chatbots, that tend to offer direct answers, the proposed Socratic tutor encourages students to explore various perspectives and engage in self-reflection by posing structured, thought-provoking questions. Our Socratic questioning is implemented by fine and prompt-tuning the open-source pretrained LLM with a specialized dataset that stimulates critical thinking and offers multiple viewpoints. In an effort to democratize access and to protect the students' privacy, the proposed tutor is based on small LLMs (Llama2 7B and 13B-parameter models) that are able to run locally on off-the-shelf hardware. We validate our approach in a battery of experiments consisting of interactions between a simulated student and the chatbot to evaluate its effectiveness in enhancing critical thinking skills. Results indicate that the Socratic tutor supports the development of reflection and critical thinking significantly better than standard chatbots. Our approach opens the door for improving educational outcomes by cultivating active learning and encouraging intellectual autonomy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05511v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucile Favero, Juan Antonio P\'erez-Ortiz, Tanja K\"aser, Nuria Oliver</dc:creator>
    </item>
    <item>
      <title>Citizen-Led Personalization of User Interfaces: Investigating How People Customize Interfaces for Themselves and Others</title>
      <link>https://arxiv.org/abs/2409.05696</link>
      <description>arXiv:2409.05696v1 Announce Type: new 
Abstract: User interface (UI) personalization can improve usability and user experience. However, current systems offer limited opportunities for customization, and third-party solutions often require significant effort and technical skills beyond the reach of most users, impeding the future adoption of interface personalization. In our research, we explore the concept of UI customization for the self and others. We performed a two-week study where nine participants used a custom-designed tool that allows websites' UI customization for oneself and to create and reply to customization assistance requests from others. Results suggest that people enjoy customizing for others more than for themselves. They see requests as challenges to solve and are motivated by the positive feeling of helping others. To customize for themselves, people need help with the creative process. We discuss challenges and opportunities for future research seeking to democratize access to personalized UIs, particularly through community-based approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05696v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3686985</arxiv:DOI>
      <dc:creator>S\'ergio Alves, Ricardo Costa, Kyle Montague, Tiago Guerreiro</dc:creator>
    </item>
    <item>
      <title>The Influence of Task and Group Disparities over Users' Attitudes Toward Using Large Language Models for Psychotherapy</title>
      <link>https://arxiv.org/abs/2409.05703</link>
      <description>arXiv:2409.05703v1 Announce Type: new 
Abstract: The population suffering from mental health disorders has kept increasing in recent years. With the advancements in large language models (LLMs) in diverse fields, LLM-based psychotherapy has also attracted increasingly more attention. However, the factors influencing users' attitudes to LLM-based psychotherapy have rarely been explored. As the first attempt, this paper investigated the influence of task and group disparities on user attitudes toward LLM-based psychotherapy tools. Utilizing the Technology Acceptance Model (TAM) and Automation Acceptance Model (AAM), based on an online survey, we collected and analyzed responses from 222 LLM-based psychotherapy users in mainland China. The results revealed that group disparity (i.e., mental health conditions) can influence users' attitudes toward LLM tools. Further, one of the typical task disparities, i.e., the privacy concern, was not found to have a significant effect on trust and usage intention. These findings can guide the design of future LLM-based psychotherapy services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05703v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qihang He, Jiyao Wang, Dengbo He</dc:creator>
    </item>
    <item>
      <title>What Did My Car Say? Impact of Autonomous Vehicle Explanation Errors and Driving Context On Comfort, Reliance, Satisfaction, and Driving Confidence</title>
      <link>https://arxiv.org/abs/2409.05731</link>
      <description>arXiv:2409.05731v2 Announce Type: new 
Abstract: Explanations for autonomous vehicle (AV) decisions may build trust, however, explanations can contain errors. In a simulated driving study (n = 232), we tested how AV explanation errors, driving context characteristics (perceived harm and driving difficulty), and personal traits (prior trust and expertise) affected a passenger's comfort in relying on an AV, preference for control, confidence in the AV's ability, and explanation satisfaction. Errors negatively affected all outcomes. Surprisingly, despite identical driving, explanation errors reduced ratings of the AV's driving ability. Severity and potential harm amplified the negative impact of errors. Contextual harm and driving difficulty directly impacted outcome ratings and influenced the relationship between errors and outcomes. Prior trust and expertise were positively associated with outcome ratings. Results emphasize the need for accurate, contextually adaptive, and personalized AV explanations to foster trust, reliance, satisfaction, and confidence. We conclude with design, research, and deployment recommendations for trustworthy AV explanation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05731v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Kaufman, Aaron Broukhim, David Kirsh, Nadir Weibel</dc:creator>
    </item>
    <item>
      <title>A Novel Idea Generation Tool using a Structured Conversational AI (CAI) System</title>
      <link>https://arxiv.org/abs/2409.05747</link>
      <description>arXiv:2409.05747v1 Announce Type: new 
Abstract: This paper presents a novel conversational AI-enabled active ideation interface as a creative idea-generation tool to assist novice designers in mitigating the initial latency and ideation bottlenecks that are commonly observed. It is a dynamic, interactive, and contextually responsive approach, actively involving a large language model (LLM) from the domain of natural language processing (NLP) in artificial intelligence (AI) to produce multiple statements of potential ideas for different design problems. Integrating such AI models with ideation creates what we refer to as an Active Ideation scenario, which helps foster continuous dialogue-based interaction, context-sensitive conversation, and prolific idea generation. A pilot study was conducted with thirty novice designers to generate ideas for given problems using traditional methods and the new CAI-based interface. The key parameters of fluency, novelty, and variety were used to compare the outcomes qualitatively by a panel of experts. The findings demonstrated the effectiveness of the proposed tool for generating prolific, diverse and novel ideas. The interface was enhanced by incorporating a prompt-engineered structured dialogue style for each ideation stage to make it uniform and more convenient for the designers. The resulting responses of such a structured CAI interface were found to be more succinct and aligned towards the subsequent design stage, namely conceptualization. The paper thus established the rich potential of using Generative AI (Gen-AI) for the early ill-structured phase of the creative product design process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05747v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>B. Sankar, Dibakar Sen</dc:creator>
    </item>
    <item>
      <title>Creativity and Visual Communication from Machine to Musician: Sharing a Score through a Robotic Camera</title>
      <link>https://arxiv.org/abs/2409.05773</link>
      <description>arXiv:2409.05773v1 Announce Type: new 
Abstract: This paper explores the integration of visual communication and musical interaction by implementing a robotic camera within a "Guided Harmony" musical game. We aim to examine co-creative behaviors between human musicians and robotic systems. Our research explores existing methodologies like improvisational game pieces and extends these concepts to include robotic participation using a PTZ camera. The robotic system interprets and responds to nonverbal cues from musicians, creating a collaborative and adaptive musical experience. This initial case study underscores the importance of intuitive visual communication channels. We also propose future research directions, including parameters for refining the visual cue toolkit and data collection methods to understand human-machine co-creativity further. Our findings contribute to the broader understanding of machine intelligence in augmenting human creativity, particularly in musical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05773v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ross Greer, Laura Fleig, Shlomo Dubnov</dc:creator>
    </item>
    <item>
      <title>Modeling Drivers' Risk Perception via Attention to Improve Driving Assistance</title>
      <link>https://arxiv.org/abs/2409.04738</link>
      <description>arXiv:2409.04738v1 Announce Type: cross 
Abstract: Advanced Driver Assistance Systems (ADAS) alert drivers during safety-critical scenarios but often provide superfluous alerts due to a lack of consideration for drivers' knowledge or scene awareness. Modeling these aspects together in a data-driven way is challenging due to the scarcity of critical scenario data with in-cabin driver state and world state recorded together. We explore the benefits of driver modeling in the context of Forward Collision Warning (FCW) systems. Working with real-world video dataset of on-road FCW deployments, we collect observers' subjective validity rating of the deployed alerts. We also annotate participants' gaze-to-objects and extract 3D trajectories of the ego vehicle and other vehicles semi-automatically. We generate a risk estimate of the scene and the drivers' perception in a two step process: First, we model the movement of vehicles in a given scenario as a joint trajectory forecasting problem. Then, we reason about the drivers' risk perception of the scene by counterfactually modifying the input to the forecasting model to represent the drivers' actual observations of vehicles in the scene. The difference in these behaviours gives us an estimate of driver behaviour that accounts for their actual (inattentive) observations and their downstream effect on overall scene risk. We compare both a learned scene representation as well as a more traditional ``worse-case'' deceleration model to achieve the future trajectory forecast. Our experiments show that using this risk formulation to generate FCW alerts may lead to improved false positive rate of FCWs and improved FCW timing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04738v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Abhijat Biswas, John Gideon, Kimimasa Tamura, Guy Rosman</dc:creator>
    </item>
    <item>
      <title>Using vs. Purchasing Industrial Robots: Adding an Organizational Perspective to Industrial HRI</title>
      <link>https://arxiv.org/abs/2409.05016</link>
      <description>arXiv:2409.05016v1 Announce Type: cross 
Abstract: Purpose: Industrial robots allow manufacturing companies to increase productivity and remain competitive. For robots to be used, they must be accepted by operators on the one hand and bought by decision-makers on the other. The roles involved in such organizational processes have very different perspectives. It is therefore essential for suppliers and robot customers to understand these motives so that robots can successfully be integrated on manufacturing shopfloors. Methodology: We present findings of a qualitative study with operators and decision-makers from two Swiss manufacturing SMEs. Using laddering interviews and means-end analysis, we compare operators' and deciders' relevant elements and how these elements are linked to each other on different abstraction levels. These findings represent drivers and barriers to the acquisition, integration and acceptance of robots in the industry. Findings: We present the differing foci of operators and deciders, and how they can be used by demanders as well as suppliers of robots to achieve robot acceptance and deployment. First, we present a list of relevant attributes, consequences and values that constitute robot acceptance and/or rejection. Second, we provide quantified relevancies for these elements, and how they differ between operators and deciders. And third, we demonstrate how the elements are linked with each other on different abstraction levels, and how these links differ between the two groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05016v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Damian Hostettler</dc:creator>
    </item>
    <item>
      <title>The Influence of Demographic Variation on the Perception of Industrial Robot Movements</title>
      <link>https://arxiv.org/abs/2409.05049</link>
      <description>arXiv:2409.05049v1 Announce Type: cross 
Abstract: The influence of individual differences on the perception and evaluation of interactions with robots has been researched for decades. Some human demographic characteristics have been shown to affect how individuals perceive interactions with robots. Still, it is to-date not clear whether, which and to what extent individual differences influence how we perceive robots, and even less is known about human factors and their effect on the perception of robot movements. In addition, most results on the relevance of individual differences investigate human-robot interactions with humanoid or social robots whereas interactions with industrial robots are underrepresented. We present a literature review on the relationship of robot movements and the influence of demographic variation. Our review reveals a limited comparability of existing findings due to a lack of standardized robot manipulations, various dependent variables used and differing experimental setups including different robot types. In addition, most studies have insufficient sample sizes to derive generalizable results. To overcome these shortcomings, we report the results from a Web-based experiment with 930 participants that studies the effect of demographic characteristics on the evaluation of movement behaviors of an articulated robot arm. Our findings demonstrate that most participants prefer an approach from the side, a large movement range, conventional numbers of rotations, smooth movements and neither fast nor slow movement speeds. Regarding individual differences, most of these preferences are robust to demographic variation, and only gender and age was found to cause slight preference differences between slow and fast movements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05049v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Damian Hostettler</dc:creator>
    </item>
    <item>
      <title>PhysHand: A Hand Simulation Model with Physiological Geometry, Physical Deformation, and Accurate Contact Handling</title>
      <link>https://arxiv.org/abs/2409.05143</link>
      <description>arXiv:2409.05143v1 Announce Type: cross 
Abstract: In virtual Hand-Object Interaction (HOI) scenarios, the authenticity of the hand's deformation is important to immersive experience, such as natural manipulation or tactile feedback. Unrealistic deformation arises from simplified hand geometry, neglect of the different physics attributes of the hand, and penetration due to imprecise contact handling. To address these problems, we propose PhysHand, a novel hand simulation model, which enhances the realism of deformation in HOI. First, we construct a physiologically plausible geometry, a layered mesh with a "skin-flesh-skeleton" structure. Second, to satisfy the distinct physics features of different soft tissues, a constraint-based dynamics framework is adopted with carefully designed layer-corresponding constraints to maintain flesh attached and skin smooth. Finally, we employ an SDF-based method to eliminate the penetration caused by contacts and enhance its accuracy by introducing a novel multi-resolution querying strategy. Extensive experiments have been conducted to demonstrate the outstanding performance of PhysHand in calculating deformations and handling contacts. Compared to existing methods, our PhysHand: 1) can compute both physiologically and physically plausible deformation; 2) significantly reduces the depth and count of penetration in HOI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05143v1</guid>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyang Sun, Dongliang Kou, Ruisheng Yuan, Dingkang Yang, Peng Zhai, Xiao Zhao, Yang Jiang, Xiong Li, Jingchen Li, Lihua Zhang</dc:creator>
    </item>
    <item>
      <title>Exploring Fungal Morphology Simulation and Dynamic Light Containment from a Graphics Generation Perspective</title>
      <link>https://arxiv.org/abs/2409.05171</link>
      <description>arXiv:2409.05171v1 Announce Type: cross 
Abstract: Fungal simulation and control are considered crucial techniques in Bio-Art creation. However, coding algorithms for reliable fungal simulations have posed significant challenges for artists. This study equates fungal morphology simulation to a two-dimensional graphic time-series generation problem. We propose a zero-coding, neural network-driven cellular automaton. Fungal spread patterns are learned through an image segmentation model and a time-series prediction model, which then supervise the training of neural network cells, enabling them to replicate real-world spreading behaviors. We further implemented dynamic containment of fungal boundaries with lasers. Synchronized with the automaton, the fungus successfully spreads into pre-designed complex shapes in reality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05171v1</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kexin Wang, Ivy He, Jinke Li, Ali Asadipour, Yitong Sun</dc:creator>
    </item>
    <item>
      <title>Holonomy: A Virtual Reality Exploration of Hyperbolic Geometry</title>
      <link>https://arxiv.org/abs/2409.05460</link>
      <description>arXiv:2409.05460v1 Announce Type: cross 
Abstract: HOLONOMY is a virtual environment based on the mathematical concept of hyperbolic geometry. Unlike other environments, HOLONOMY allows users to seamlessly explore an infinite hyperbolic space by physically walking. They use their body as the controller, eliminating the need for teleportation or other artificial VR locomotion methods. This paper discusses the development of HOLONOMY, highlighting the technical challenges faced and overcome during its creation, including rendering complex hyperbolic environments, populating the space with objects, and implementing algorithms for finding shortest paths in the underlying non-Euclidean geometry. Furthermore, we present a proof-of-concept implementation in the form of a VR navigation game and some preliminary learning outcomes from this implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05460v1</guid>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3665318.3677149</arxiv:DOI>
      <dc:creator>Martin Skrodzki, Scott Jochems, Joris Rijsdijk, Ravi Snellenberg, Rafael Bidarra</dc:creator>
    </item>
    <item>
      <title>Enhancing Preference-based Linear Bandits via Human Response Time</title>
      <link>https://arxiv.org/abs/2409.05798</link>
      <description>arXiv:2409.05798v1 Announce Type: cross 
Abstract: Binary human choice feedback is widely used in interactive preference learning for its simplicity, but it provides limited information about preference strength. To overcome this limitation, we leverage human response times, which inversely correlate with preference strength, as complementary information. Our work integrates the EZ-diffusion model, which jointly models human choices and response times, into preference-based linear bandits. We introduce a computationally efficient utility estimator that reformulates the utility estimation problem using both choices and response times as a linear regression problem. Theoretical and empirical comparisons with traditional choice-only estimators reveal that for queries with strong preferences ("easy" queries), choices alone provide limited information, while response times offer valuable complementary information about preference strength. As a result, incorporating response times makes easy queries more useful. We demonstrate this advantage in the fixed-budget best-arm identification problem, with simulations based on three real-world datasets, consistently showing accelerated learning when response times are incorporated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05798v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shen Li, Yuyang Zhang, Zhaolin Ren, Claire Liang, Na Li, Julie A. Shah</dc:creator>
    </item>
    <item>
      <title>VFA: Vision Frequency Analysis of Foundation Models and Human</title>
      <link>https://arxiv.org/abs/2409.05817</link>
      <description>arXiv:2409.05817v1 Announce Type: cross 
Abstract: Machine learning models often struggle with distribution shifts in real-world scenarios, whereas humans exhibit robust adaptation. Models that better align with human perception may achieve higher out-of-distribution generalization. In this study, we investigate how various characteristics of large-scale computer vision models influence their alignment with human capabilities and robustness. Our findings indicate that increasing model and data size and incorporating rich semantic information and multiple modalities enhance models' alignment with human perception and their overall robustness. Our empirical analysis demonstrates a strong correlation between out-of-distribution accuracy and human alignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05817v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad-Javad Darvishi-Bayazi, Md Rifat Arefin, Jocelyn Faubert, Irina Rish</dc:creator>
    </item>
    <item>
      <title>Bans vs. Warning Labels: Examining Bystanders' Support for Community-wide Moderation Interventions</title>
      <link>https://arxiv.org/abs/2307.11880</link>
      <description>arXiv:2307.11880v3 Announce Type: replace 
Abstract: Social media platforms like Facebook and Reddit host thousands of user-governed online communities. These platforms sanction communities that frequently violate platform policies; however, public perceptions of such sanctions remain unclear. In a pre-registered survey conducted in the US, I explore bystander perceptions of content moderation for communities that frequently feature hate speech, violent content, and sexually explicit content. Two community-wide moderation interventions are tested: (1) community bans, where all community posts are removed, and (2) community warning labels, where an interstitial warning label precedes access. I examine how third-person effects and support for free speech influence user approval of these interventions on any platform. My regression analyses show that presumed effects on others are a significant predictor of backing for both interventions, while free speech beliefs significantly influence participants' inclination for using warning labels. Analyzing the open-ended responses, I find that community-wide bans are often perceived as too coarse, and users instead value sanctions in proportion to the severity and type of infractions. I report on concerns that norm-violating communities could reinforce inappropriate behaviors and show how users' choice of sanctions is influenced by their perceived effectiveness. I discuss the implications of these results for HCI research on online harms and content moderation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11880v3</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shagun Jhaver</dc:creator>
    </item>
    <item>
      <title>Beyond Words: On Large Language Models Actionability in Mission-Critical Risk Analysis</title>
      <link>https://arxiv.org/abs/2406.10273</link>
      <description>arXiv:2406.10273v5 Announce Type: replace-cross 
Abstract: Context. Risk analysis assesses potential risks in specific scenarios. Risk analysis principles are context-less; the same methodology can be applied to a risk connected to health and information technology security. Risk analysis requires a vast knowledge of national and international regulations and standards and is time and effort-intensive. A large language model can quickly summarize information in less time than a human and can be fine-tuned to specific tasks.
  Aim. Our empirical study aims to investigate the effectiveness of Retrieval-Augmented Generation and fine-tuned LLM in risk analysis. To our knowledge, no prior study has explored its capabilities in risk analysis.
  Method. We manually curated 193 unique scenarios leading to 1283 representative samples from over 50 mission-critical analyses archived by the industrial context team in the last five years. We compared the base GPT-3.5 and GPT-4 models versus their Retrieval-Augmented Generation and fine-tuned counterparts. We employ two human experts as competitors of the models and three other human experts to review the models and the former human experts' analysis. The reviewers analyzed 5,000 scenario analyses.
  Results and Conclusions. Human experts demonstrated higher accuracy, but LLMs are quicker and more actionable. Moreover, our findings show that RAG-assisted LLMs have the lowest hallucination rates, effectively uncovering hidden risks and complementing human expertise. Thus, the choice of model depends on specific needs, with FTMs for accuracy, RAG for hidden risks discovery, and base models for comprehensiveness and actionability. Therefore, experts can leverage LLMs as an effective complementing companion in risk analysis within a condensed timeframe. They can also save costs by averting unnecessary expenses associated with implementing unwarranted countermeasures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10273v5</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Esposito, Francesco Palagiano, Valentina Lenarduzzi, Davide Taibi</dc:creator>
    </item>
    <item>
      <title>Using LLMs to Establish Implicit User Sentiment of Software Desirability</title>
      <link>https://arxiv.org/abs/2408.01527</link>
      <description>arXiv:2408.01527v2 Announce Type: replace-cross 
Abstract: This study explores the use of LLMs for providing quantitative zero-shot sentiment analysis of implicit software desirability, addressing a critical challenge in product evaluation where traditional review scores, though convenient, fail to capture the richness of qualitative user feedback. Innovations include establishing a method that 1) works with qualitative user experience data without the need for explicit review scores, 2) focuses on implicit user satisfaction, and 3) provides scaled numerical sentiment analysis, offering a more nuanced understanding of user sentiment, instead of simply classifying sentiment as positive, neutral, or negative.
  Data is collected using the Microsoft Product Desirability Toolkit (PDT), a well-known qualitative user experience analysis tool. For initial exploration, the PDT metric was given to users of two software systems. PDT data was fed through several LLMs (Claude Sonnet 3 and 3.5, GPT4, and GPT4o) and through a leading transfer learning technique, Twitter-Roberta-Base-Sentiment, and Vader, a leading sentiment analysis tool. Each system was asked to evaluate the data in two ways, by looking at the sentiment expressed in the PDT word/explanation pairs; and by looking at the sentiment expressed by the users in their grouped selection of five words and explanations, as a whole. Each LLM provided a sentiment score, its confidence (low, medium, high) in the score, and an explanation of the score.
  All LLMs tested were able to statistically detect user sentiment from the users' grouped data, whereas TRBS and Vader were not. The confidence and explanation of confidence provided by the LLMs assisted in understanding user sentiment. This study adds deeper understanding of evaluating user experiences, toward the goal of creating a universal tool that quantifies implicit sentiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01527v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sherri Weitl-Harms, John D. Hastings, Jonah Lum</dc:creator>
    </item>
    <item>
      <title>WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild</title>
      <link>https://arxiv.org/abs/2409.03753</link>
      <description>arXiv:2409.03753v2 Announce Type: replace-cross 
Abstract: The increasing availability of real-world conversation data offers exciting opportunities for researchers to study user-chatbot interactions. However, the sheer volume of this data makes manually examining individual conversations impractical. To overcome this challenge, we introduce WildVis, an interactive tool that enables fast, versatile, and large-scale conversation analysis. WildVis provides search and visualization capabilities in the text and embedding spaces based on a list of criteria. To manage million-scale datasets, we implemented optimizations including search index construction, embedding precomputation and compression, and caching to ensure responsive user interactions within seconds. We demonstrate WildVis' utility through three case studies: facilitating chatbot misuse research, visualizing and comparing topic distributions across datasets, and characterizing user-specific conversation patterns. WildVis is open-source and designed to be extendable, supporting additional datasets and customized search and visualization functionalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03753v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuntian Deng, Wenting Zhao, Jack Hessel, Xiang Ren, Claire Cardie, Yejin Choi</dc:creator>
    </item>
  </channel>
</rss>
