<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 01 Oct 2025 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>smallNet: Implementation of a convolutional layer in tiny FPGAs</title>
      <link>https://arxiv.org/abs/2509.25391</link>
      <description>arXiv:2509.25391v1 Announce Type: new 
Abstract: Since current neural network development systems in Xilinx and VLSI require codevelopment with Python libraries, the first stage of a convolutional network has been implemented by developing a convolutional layer entirely in Verilog. This handcoded design, free of IP cores and based on a filter polynomial like structure, enables straightforward deployment not only on low cost FPGAs but also on SoMs, SoCs, and ASICs. We analyze the limitations of numerical representations and compare our implemented architecture, smallNet, with its computer based counterpart, demonstrating a 5.1x speedup, over 81% classification accuracy, and a total power consumption of just 1.5 W. The algorithm is validated on a single-core Cora Z7, demonstrating its feasibility for real time, resource-constrained embedded applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25391v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fernanda Zapata Bascu\~n\'an, Alan Ezequiel Fuster</dc:creator>
    </item>
    <item>
      <title>LLM-Powered Code Analysis and Optimization for Gaussian Splatting Kernels</title>
      <link>https://arxiv.org/abs/2509.25626</link>
      <description>arXiv:2509.25626v1 Announce Type: new 
Abstract: 3D Gaussian splatting (3DGS) is a transformative technique with profound implications on novel view synthesis and real-time rendering. Given its importance, there have been many attempts to improve its performance. However, with the increasing complexity of GPU architectures and the vast search space of performance-tuning parameters, it is a challenging task. Although manual optimizations have achieved remarkable speedups, they require domain expertise and the optimization process can be highly time consuming and error prone. In this paper, we propose to exploit large language models (LLMs) to analyze and optimize Gaussian splatting kernels. To our knowledge, this is the first work to use LLMs to optimize highly specialized real-world GPU kernels. We reveal the intricacies of using LLMs for code optimization and analyze the code optimization techniques from the LLMs. We also propose ways to collaborate with LLMs to further leverage their capabilities. For the original 3DGS code on the MipNeRF360 datasets, LLMs achieve significant speedups, 19% with Deepseek and 24% with GPT-5, demonstrating the different capabilities of different LLMs. By feeding additional information from performance profilers, the performance improvement from LLM-optimized code is enhanced to up to 42% and 38% on average. In comparison, our best-effort manually optimized version can achieve a performance improvement up to 48% and 39% on average, showing that there are still optimizations beyond the capabilities of current LLMs. On the other hand, even upon a newly proposed 3DGS framework with algorithmic optimizations, Seele, LLMs can still further enhance its performance by 6%, showing that there are optimization opportunities missed by domain experts. This highlights the potential of collaboration between domain experts and LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25626v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Hu (North Carolina State University), Huiyang Zhou (North Carolina State University)</dc:creator>
    </item>
    <item>
      <title>SAIL: SRAM-Accelerated LLM Inference System with Lookup-Table-based GEMV</title>
      <link>https://arxiv.org/abs/2509.25853</link>
      <description>arXiv:2509.25853v1 Announce Type: new 
Abstract: Large Language Model (LLM) inference requires substantial computational resources, yet CPU-based inference remains essential for democratizing AI due to the widespread availability of CPUs compared to specialized accelerators. However, efficient LLM inference on CPUs faces two fundamental challenges: (1) existing CPU architectures struggle with low-precision arithmetic required by quantized models, where optimal bit precision varies across models and layers; and (2) the memory-bound nature of the token generation phase creates severe performance bottlenecks. To address these challenges, we propose SAIL (SRAM-Accelerated Inference of LLMs), a CPU-based inference solution that efficiently supports arbitrary bit precisions with minimal overhead. SAIL integrates three key innovations: First, we introduce Batched LUT-based General Matrix-Vector Multiplication (LUT-GEMV) with SRAM-based processing-in-memory, enabling high data reuse through lookup tables and reducing memory movement. Second, our Pattern-Aware LUT optimization identifies and exploits redundancy in input activation patterns, reducing computation cycles by 13.8\%. Third, we develop an in-memory type conversion algorithm that leverages PIM's parallelism for efficient de-/quantization operations, alleviating pressure on CPU's vector units. Our architecture requires only 2\% hardware overhead and a single new instruction, while maintaining dual functionality as both compute and storage units. Experimental evaluations using a modified gem5 simulator demonstrate that SAIL achieves up to 10.7x speedup and 19.9x higher tokens per dollar compared to ARM Neoverse-N1 CPU baselines, and up to 7.04x better cost efficiency than NVIDIA V100 GPUs, establishing a practical path for efficient CPU-based LLM inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25853v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyao Zhang, Jaewoo Park, Jongeun Lee, Elaheh Sadredini</dc:creator>
    </item>
    <item>
      <title>Runtime Energy Monitoring for RISC-V Soft-Cores</title>
      <link>https://arxiv.org/abs/2509.26065</link>
      <description>arXiv:2509.26065v1 Announce Type: new 
Abstract: Energy efficiency is one of the major concern in designing advanced computing infrastructures. From single nodes to large-scale systems (data centers), monitoring the energy consumption of the computing system when applications run is a critical task. Designers and application developers often rely on software tools and detailed architectural models to extract meaningful information and determine the system energy consumption. However, when a design space exploration is required, designers may incur in continuous tuning of the models to match with the system under evaluation. To overcome such limitations, we propose a holistic approach to monitor energy consumption at runtime without the need of running complex (micro-)architectural models. Our approach is based on a measurement board coupled with a FPGA-based System-on-Module. The measuring board captures currents and voltages (up to tens measuring points) driving the FPGA and exposes such values through a specific memory region. A running service reads and computes energy consumption statistics without consuming extra resources on the FPGA device. Our approach is also scalable to monitoring of multi-nodes infrastructures (clusters). We aim to leverage this framework to perform experiments in the context of an aeronautical design application; specifically, we will look at optimizing performance and energy consumption of a shallow artificial neural network on RISC-V based soft-cores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26065v1</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Scionti, Paolo Savio, Francesco Lubrano, Olivier Terzo, Marco Ferretti, Florin Apopei, Juri Bellucci, Ennio Spano, Luca Carriere</dc:creator>
    </item>
    <item>
      <title>On the Shape of Latent Variables in a Denoising VAE-MoG: A Posterior Sampling-Based Study</title>
      <link>https://arxiv.org/abs/2509.25382</link>
      <description>arXiv:2509.25382v1 Announce Type: cross 
Abstract: In this work, we explore the latent space of a denoising variational autoencoder with a mixture-of-Gaussians prior (VAE-MoG), trained on gravitational wave data from event GW150914. To evaluate how well the model captures the underlying structure, we use Hamiltonian Monte Carlo (HMC) to draw posterior samples conditioned on clean inputs, and compare them to the encoder's outputs from noisy data. Although the model reconstructs signals accurately, statistical comparisons reveal a clear mismatch in the latent space. This shows that strong denoising performance doesn't necessarily mean the latent representations are reliable highlighting the importance of using posterior-based validation when evaluating generative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25382v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fernanda Zapata Bascu\~n\'an</dc:creator>
    </item>
    <item>
      <title>EEsizer: LLM-Based AI Agent for Sizing of Analog and Mixed Signal Circuit</title>
      <link>https://arxiv.org/abs/2509.25510</link>
      <description>arXiv:2509.25510v1 Announce Type: cross 
Abstract: The design of Analog and Mixed-Signal (AMS) integrated circuits (ICs) often involves significant manual effort, especially during the transistor sizing process. While Machine Learning techniques in Electronic Design Automation (EDA) have shown promise in reducing complexity and minimizing human intervention, they still face challenges such as numerous iterations and a lack of knowledge about AMS circuit design. Recently, Large Language Models (LLMs) have demonstrated significant potential across various fields, showing a certain level of knowledge in circuit design and indicating their potential to automate the transistor sizing process. In this work, we propose EEsizer, an LLM-based AI agent that integrates large language models with circuit simulators and custom data analysis functions, enabling fully automated, closed-loop transistor sizing without relying on external knowledge. By employing prompt engineering and Chain-of-Thought reasoning, the agent iteratively explores design directions, evaluates performance, and refines solutions with minimal human intervention. We first benchmarked 8 LLMs on six basic circuits and selected three high-performing models to optimize a 20-transistor CMOS operational amplifier, targeting multiple performance metrics, including rail-to-rail operation from 180 nm to 90 nm technology nodes. Notably, OpenAI o3 successfully achieved the user-intended target at 90 nm across three different test groups, with a maximum of 20 iterations, demonstrating adaptability and robustness at advanced nodes. To assess design robustness, we manually designed a bias circuit and performed a variation analysis using Gaussian-distributed variations on transistor dimensions and threshold voltages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25510v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chang Liu, Danial Chitnis</dc:creator>
    </item>
    <item>
      <title>CIMNAS: A Joint Framework for Compute-In-Memory-Aware Neural Architecture Search</title>
      <link>https://arxiv.org/abs/2509.25862</link>
      <description>arXiv:2509.25862v1 Announce Type: cross 
Abstract: To maximize hardware efficiency and performance accuracy in Compute-In-Memory (CIM)-based neural network accelerators for Artificial Intelligence (AI) applications, co-optimizing both software and hardware design parameters is essential. Manual tuning is impractical due to the vast number of parameters and their complex interdependencies. To effectively automate the design and optimization of CIM-based neural network accelerators, hardware-aware neural architecture search (HW-NAS) techniques can be applied. This work introduces CIMNAS, a joint model-quantization-hardware optimization framework for CIM architectures. CIMNAS simultaneously searches across software parameters, quantization policies, and a broad range of hardware parameters, incorporating device-, circuit-, and architecture-level co-optimizations. CIMNAS experiments were conducted over a search space of 9.9x10^85 potential parameter combinations with the MobileNet model as a baseline and RRAM-based CIM architecture. Evaluated on the ImageNet dataset, CIMNAS achieved a reduction in energy-delay-area product (EDAP) ranging from 90.1x to 104.5x, an improvement in TOPS/W between 4.68x and 4.82x, and an enhancement in TOPS/mm^2 from 11.3x to 12.78x relative to various baselines, all while maintaining an accuracy of 73.81%. The adaptability and robustness of CIMNAS are demonstrated by extending the framework to support the SRAM-based ResNet50 architecture, achieving up to an 819.5x reduction in EDAP. Unlike other state-of-the-art methods, CIMNAS achieves EDAP-focused optimization without any accuracy loss, generating diverse software-hardware parameter combinations for high-performance CIM-based neural network designs. The source code of CIMNAS is available at https://github.com/OlgaKrestinskaya/CIMNAS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25862v1</guid>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>cs.NE</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olga Krestinskaya, Mohammed E. Fouda, Ahmed Eltawil, Khaled N. Salama</dc:creator>
    </item>
    <item>
      <title>MUSS-TI: Multi-level Shuttle Scheduling for Large-Scale Entanglement Module Linked Trapped-Ion</title>
      <link>https://arxiv.org/abs/2509.25988</link>
      <description>arXiv:2509.25988v1 Announce Type: cross 
Abstract: Trapped-ion computing is a leading architecture in the pursuit of scalable and high fidelity quantum systems. Modular quantum architectures based on photonic interconnects offer a promising path for scaling trapped ion devices. In this design, multiple Quantum Charge Coupled Device (QCCD) units are interconnected through entanglement module. Each unit features a multi-zone layout that separates functionalities into distinct areas, enabling more efficient and flexible quantum operations. However, achieving efficient and scalable compilation of quantum circuits in such entanglement module linked Quantum Charge-Coupled Device (EML-QCCD) remains a primary challenge for practical quantum applications.
  In this work, we propose a scalable compiler tailored for large-scale trapped-ion architectures, with the goal of reducing the shuttling overhead inherent in EML-QCCD devices. MUSS-TI introduces a multi-level scheduling approach inspired by multi-level memory scheduling in classical computing. This method is designed to be aware of the distinct roles of different zones and to minimize the number of shuttling operations required in EML-QCCD systems. We demonstrate that EML-QCCD architectures are well-suited for executing large-scale applications. Our evaluation shows that MUSS-TI reduces shuttle operations by 41.74% for applications with 30-32 qubits, and by an average of 73.38% and 59.82% for applications with 117-128 qubits and 256-299 qubits, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.25988v1</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xian Wu, Chenghong Zhu, Jingbo Wang, Xin Wang</dc:creator>
    </item>
    <item>
      <title>Enabling Time-Aware Priority Traffic Management over Distributed FPGA Nodes</title>
      <link>https://arxiv.org/abs/2509.26043</link>
      <description>arXiv:2509.26043v1 Announce Type: cross 
Abstract: Network Interface Cards (NICs) greatly evolved from simple basic devices moving traffic in and out of the network to complex heterogeneous systems offloading host CPUs from performing complex tasks on in-transit packets. These latter comprise different types of devices, ranging from NICs accelerating fixed specific functions (e.g., on-the-fly data compression/decompression, checksum computation, data encryption, etc.) to complex Systems-on-Chip (SoC) equipped with both general purpose processors and specialized engines (Smart-NICs). Similarly, Field Programmable Gate Arrays (FPGAs) moved from pure reprogrammable devices to modern heterogeneous systems comprising general-purpose processors, real-time cores and even AI-oriented engines. Furthermore, the availability of high-speed network interfaces (e.g., SFPs) makes modern FPGAs a good choice for implementing Smart-NICs. In this work, we extended the functionalities offered by an open-source NIC implementation (Corundum) by enabling time-aware traffic management in hardware, and using this feature to control the bandwidth associated with different traffic classes. By exposing dedicated control registers on the AXI bus, the driver of the NIC can easily configure the transmission bandwidth of different prioritized queues. Basically, each control register is associated with a specific transmission queue (Corundum can expose up to thousands of transmission and receiving queues), and sets up the fraction of time in a transmission window which the queue is supposed to get access the output port and transmit the packets. Queues are then prioritized and associated to different traffic classes through the Linux QDISC mechanism. Experimental evaluation demonstrates that the approach allows to properly manage the bandwidth reserved to the different transmission flows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26043v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Scionti, Paolo Savio, Francesco Lubrano, Federico Stirano, Antonino Nespola, Olivier Terzo, Corrado De Sio, Luca Sterpone</dc:creator>
    </item>
    <item>
      <title>Benchmarking Deep Learning Convolutions on Energy-constrained CPUs</title>
      <link>https://arxiv.org/abs/2509.26217</link>
      <description>arXiv:2509.26217v1 Announce Type: cross 
Abstract: This work evaluates state-of-the-art convolution algorithms for CPU-based deep learning inference. While most prior studies focus on GPUs or NPUs, CPU implementations remain relatively underoptimized. We benchmark direct, GEMM-based, and Winograd convolutions across modern CPUs from ARM __ , Intel __ , AMD __ , Apple __ , and Nvidia __ , considering both latency and energy efficiency. Our results highlight the key architectural factors that govern CPU efficiency for convolution operations, providing practical guidance for energy-aware embedded deployment. As a main results of this work, the Nvidia __ AGX Orin combined with the GEMM algorithm achieves the best trade-off between inference latency and energy consumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26217v1</guid>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Enrique Galvez (ALSOC), Adrien Cassagne (ALSOC), Alix Munier (ALSOC), Manuel Bouyer</dc:creator>
    </item>
    <item>
      <title>TrackCore-F: Deploying Transformer-Based Subatomic Particle Tracking on FPGAs</title>
      <link>https://arxiv.org/abs/2509.26335</link>
      <description>arXiv:2509.26335v1 Announce Type: cross 
Abstract: The Transformer Machine Learning (ML) architecture has been gaining considerable momentum in recent years. In particular, computational High-Energy Physics tasks such as jet tagging and particle track reconstruction (tracking), have either achieved proper solutions, or reached considerable milestones using Transformers. On the other hand, the use of specialised hardware accelerators, especially FPGAs, is an effective method to achieve online, or pseudo-online latencies. The development and integration of Transformer-based ML to FPGAs is still ongoing and the support from current tools is very limited to non-existent. Additionally, FPGA resources present a significant constraint. Considering the model size alone, while smaller models can be deployed directly, larger models are to be partitioned in a meaningful and ideally, automated way. We aim to develop methodologies and tools for monolithic, or partitioned Transformer synthesis, specifically targeting inference. Our primary use-case involves two machine learning model designs for tracking, derived from the TrackFormers project. We elaborate our development approach, present preliminary results, and provide comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26335v1</guid>
      <category>hep-ex</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arjan Blankestijn, Uraz Odyurt, Amirreza Yousefzadeh</dc:creator>
    </item>
    <item>
      <title>Robust NbN on Si-SiGe hybrid superconducting-semiconducting microwave quantum circuit</title>
      <link>https://arxiv.org/abs/2509.26363</link>
      <description>arXiv:2509.26363v1 Announce Type: cross 
Abstract: Advancing large-scale quantum computing requires superconducting circuits that combine long coherence times with compatibility with semiconductor technology. We investigate niobium nitride (NbN) coplanar waveguide resonators integrated with Si/SiGe quantum wells, creating a hybrid platform designed for CMOS-compatible quantum hardware. Using temperature-dependent microwave spectroscopy in the single-photon regime, we examine resonance frequency and quality factor variations to probe the underlying loss mechanisms. Our analysis identifies the roles of two-level systems, quasiparticles, and scattering processes, and connects these losses to wafer properties and fabrication methods. The devices demonstrate reproducible performance and stable operation maintained for over two years, highlighting their robustness. These results provide design guidelines for developing low-loss, CMOS-compatible superconducting circuits and support progress toward resilient, scalable architectures for quantum information processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26363v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paniz Foshat, Samane Kalhor, Shima Poorgholam-khanjari, Douglas Paul, Martin Weides, Kaveh Delfanazari</dc:creator>
    </item>
    <item>
      <title>Stab-QRAM: An All-Clifford Quantum Random Access Memory for Special Data</title>
      <link>https://arxiv.org/abs/2509.26494</link>
      <description>arXiv:2509.26494v1 Announce Type: cross 
Abstract: Quantum random access memories (QRAMs) are pivotal for data-intensive quantum algorithms, but existing general-purpose and domain-specific architectures are hampered by a critical bottleneck: a heavy reliance on non-Clifford gates (e.g., T-gates), which are prohibitively expensive to implement fault-tolerantly. To address this challenge, we introduce the Stabilizer-QRAM (Stab-QRAM), a domain-specific architecture tailored for data with an affine Boolean structure ($f(\mathbf{x}) = A\mathbf{x} + \mathbf{b}$ over $\mathbb{F}_2$), a class of functions vital for optimization, time-series analysis, and quantum linear systems algorithms. We demonstrate that the gate interactions required to implement the matrix $A$ form a bipartite graph. By applying K\"{o}nig's edge-coloring theorem to this graph, we prove that Stab-QRAM achieves an optimal logical circuit depth of $O(\log N)$ for $N$ data items, matching its $O(\log N)$ space complexity. Critically, the Stab-QRAM is constructed exclusively from Clifford gates (CNOT and X), resulting in a zero $T$-count. This design completely circumvents the non-Clifford bottleneck, eliminating the need for costly magic state distillation and making it exceptionally suited for early fault-tolerant quantum computing platforms. We highlight Stab-QRAM's utility as a resource-efficient oracle for applications in discrete dynamical systems, and as a core component in Quantum Linear Systems Algorithms, providing a practical pathway for executing data-intensive tasks on emerging quantum hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26494v1</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guangyi Li, Yu Gan, Zeguan Wu, Xueyue Zhang, Zheshen Zhang, Junyu Liu</dc:creator>
    </item>
    <item>
      <title>Rearchitecting Datacenter Lifecycle for AI: A TCO-Driven Framework</title>
      <link>https://arxiv.org/abs/2509.26534</link>
      <description>arXiv:2509.26534v1 Announce Type: cross 
Abstract: The rapid rise of large language models (LLMs) has been driving an enormous demand for AI inference infrastructure, mainly powered by high-end GPUs. While these accelerators offer immense computational power, they incur high capital and operational costs due to frequent upgrades, dense power consumption, and cooling demands, making total cost of ownership (TCO) for AI datacenters a critical concern for cloud providers. Unfortunately, traditional datacenter lifecycle management (designed for general-purpose workloads) struggles to keep pace with AI's fast-evolving models, rising resource needs, and diverse hardware profiles. In this paper, we rethink the AI datacenter lifecycle scheme across three stages: building, hardware refresh, and operation. We show how design choices in power, cooling, and networking provisioning impact long-term TCO. We also explore refresh strategies aligned with hardware trends. Finally, we use operation software optimizations to reduce cost. While these optimizations at each stage yield benefits, unlocking the full potential requires rethinking the entire lifecycle. Thus, we present a holistic lifecycle management framework that coordinates and co-optimizes decisions across all three stages, accounting for workload dynamics, hardware evolution, and system aging. Our system reduces the TCO by up to 40\% over traditional approaches. Using our framework we provide guidelines on how to manage AI datacenter lifecycle for the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26534v1</guid>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jovan Stojkovic, Chaojie Zhang, \'I\~nigo Goiri, Ricardo Bianchini</dc:creator>
    </item>
    <item>
      <title>NeuroScalar: A Deep Learning Framework for Fast, Accurate, and In-the-Wild Cycle-Level Performance Prediction</title>
      <link>https://arxiv.org/abs/2509.22410</link>
      <description>arXiv:2509.22410v2 Announce Type: replace 
Abstract: The evaluation of new microprocessor designs is constrained by slow, cycle-accurate simulators that rely on unrepresentative benchmark traces. This paper introduces a novel deep learning framework for high-fidelity, ``in-the-wild'' simulation on production hardware. Our core contribution is a DL model trained on microarchitecture-independent features to predict cycle-level performance for hypothetical processor designs. This unique approach allows the model to be deployed on existing silicon to evaluate future hardware. We propose a complete system featuring a lightweight hardware trace collector and a principled sampling strategy to minimize user impact. This system achieves a simulation speed of 5 MIPS on a commodity GPU, imposing a mere 0.1% performance overhead. Furthermore, our co-designed Neutrino on-chip accelerator improves performance by 85x over the GPU. We demonstrate that this framework enables accurate performance analysis and large-scale hardware A/B testing on a massive scale using real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22410v2</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shayne Wadle, Yanxin Zhang, Vikas Singh, Karthikeyan Sankaralingam</dc:creator>
    </item>
    <item>
      <title>No One-Size-Fits-All: A Workload-Driven Characterization of Bit-Parallel vs. Bit-Serial Data Layouts for Processing-using-Memory</title>
      <link>https://arxiv.org/abs/2509.22980</link>
      <description>arXiv:2509.22980v2 Announce Type: replace 
Abstract: Processing-in-Memory (PIM) is a promising approach to overcoming the memory-wall bottleneck. However, the PIM community has largely treated its two fundamental data layouts, Bit-Parallel (BP) and Bit-Serial (BS), as if they were interchangeable. This implicit "one-layout-fits-all" assumption, often hard-coded into existing evaluation frameworks, creates a critical gap: architects lack systematic, workload-driven guidelines for choosing the optimal data layout for their target applications. To address this gap, this paper presents the first systematic, workload-driven characterization of BP and BS PIM architectures. We develop iso-area, cycle-accurate BP and BS PIM architectural models and conduct a comprehensive evaluation using a diverse set of benchmarks. Our suite includes both fine-grained microworkloads from MIMDRAM to isolate specific operational characteristics, and large-scale applications from the PIMBench suite, such as the VGG network, to represent realistic end-to-end workloads. Our results quantitatively demonstrate that no single layout is universally superior; the optimal choice is strongly dependent on workload characteristics. BP excels on control-flow-intensive tasks with irregular memory access patterns, whereas BS shows substantial advantages in massively parallel, low-precision (e.g., INT4/INT8) computations common in AI. Based on this characterization, we distill a set of actionable design guidelines for architects. This work challenges the prevailing one-size-fits-all view on PIM data layouts and provides a principled foundation for designing next-generation, workload-aware, and potentially hybrid PIM systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22980v2</guid>
      <category>cs.AR</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyao Zhang, Elaheh Sadredini</dc:creator>
    </item>
    <item>
      <title>SAHM: State-Aware Heterogeneous Multicore for Single-Thread Performance</title>
      <link>https://arxiv.org/abs/2509.22405</link>
      <description>arXiv:2509.22405v2 Announce Type: replace-cross 
Abstract: Improving single-thread performance remains a critical challenge in modern processor design, as conventional approaches such as deeper speculation, wider pipelines, and complex out-of-order execution face diminishing returns. This work introduces SAHM-State-Aware Heterogeneous Multicore-a novel architecture that targets performance gains by exploiting fine-grained, time-varying behavioral diversity in single-threaded workloads. Through empirical characterization of performance counter data, we define 16 distinct behavioral states representing different microarchitectural demands. Rather than over-provisioning a monolithic core with all optimizations, SAHM uses a set of specialized cores tailored to specific states and migrates threads at runtime based on detected behavior. This design enables composable microarchitectural enhancements without incurring prohibitive area, power, or complexity costs.
  We evaluate SAHM in both single-threaded and multiprogrammed scenarios, demonstrating its ability to maintain core utilization while improving overall performance through intelligent state-driven scheduling. Experimental results show opportunity for 17% speed up in realistic scenarios. These speed ups are robust against high-cost migration, decreasing by less than 1%. Overall, state-aware core specialization is a new path forward for enhancing single-thread performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.22405v2</guid>
      <category>cs.PF</category>
      <category>cs.AR</category>
      <pubDate>Wed, 01 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shayne Wadle, Karthikeyan Sankaralingam</dc:creator>
    </item>
  </channel>
</rss>
