<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 05 Aug 2024 04:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 05 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Nonparametric Mean and Covariance Estimation for Discretely Observed High-Dimensional Functional Data: Rates of Convergence and Division of Observational Regimes</title>
      <link>https://arxiv.org/abs/2408.01326</link>
      <description>arXiv:2408.01326v1 Announce Type: new 
Abstract: Nonparametric estimation of the mean and covariance parameters for functional data is a critical task, with local linear smoothing being a popular choice. In recent years, many scientific domains are producing high-dimensional functional data for which $p$, the number of curves per subject, is often much larger than the sample size $n$. Much of the methodology developed for such data rely on preliminary nonparametric estimates of the unknown mean functions and the auto- and cross-covariance functions. We investigate the convergence rates of local linear estimators in terms of the maximal error across components and pairs of components for mean and covariance functions, respectively, in both $L^2$ and uniform metrics. The local linear estimators utilize a generic weighting scheme that can adjust for differing numbers of discrete observations $N_{ij}$ across curves $j$ and subjects $i$, where the $N_{ij}$ vary with $n$. Particular attention is given to the equal weight per observation (OBS) and equal weight per subject (SUBJ) weighting schemes. The theoretical results utilize novel applications of concentration inequalities for functional data and demonstrate that, similar to univariate functional data, the order of the $N_{ij}$ relative to $p$ and $n$ divides high-dimensional functional data into three regimes: sparse, dense, and ultra-dense, with the high-dimensional parametric convergence rate of $\left\{\log(p)/n\right\}^{1/2}$ being attainable in the latter two.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01326v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexander Petersen</dc:creator>
    </item>
    <item>
      <title>META-ANOVA: Screening interactions for interpretable machine learning</title>
      <link>https://arxiv.org/abs/2408.00973</link>
      <description>arXiv:2408.00973v1 Announce Type: cross 
Abstract: There are two things to be considered when we evaluate predictive models. One is prediction accuracy,and the other is interpretability. Over the recent decades, many prediction models of high performance, such as ensemble-based models and deep neural networks, have been developed. However, these models are often too complex, making it difficult to intuitively interpret their predictions. This complexity in interpretation limits their use in many real-world fields that require accountability, such as medicine, finance, and college admissions. In this study, we develop a novel method called Meta-ANOVA to provide an interpretable model for any given prediction model. The basic idea of Meta-ANOVA is to transform a given black-box prediction model to the functional ANOVA model. A novel technical contribution of Meta-ANOVA is a procedure of screening out unnecessary interaction before transforming a given black-box model to the functional ANOVA model. This screening procedure allows the inclusion of higher order interactions in the transformed functional ANOVA model without computational difficulties. We prove that the screening procedure is asymptotically consistent. Through various experiments with synthetic and real-world datasets, we empirically demonstrate the superiority of Meta-ANOVA</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00973v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongchan Choi, Seokhun Park, Chanmoo Park, Dongha Kim, Yongdai Kim</dc:creator>
    </item>
    <item>
      <title>Sandwiching Random Geometric Graphs and Erdos-Renyi with Applications: Sharp Thresholds, Robust Testing, and Enumeration</title>
      <link>https://arxiv.org/abs/2408.00995</link>
      <description>arXiv:2408.00995v1 Announce Type: cross 
Abstract: The distribution $\mathsf{RGG}(n,\mathbb{S}^{d-1},p)$ is formed by sampling independent vectors $\{V_i\}_{i = 1}^n$ uniformly on $\mathbb{S}^{d-1}$ and placing an edge between pairs of vertices $i$ and $j$ for which $\langle V_i,V_j\rangle \ge \tau^p_d,$ where $\tau^p_d$ is such that the expected density is $p.$ Our main result is a poly-time implementable coupling between Erd\H{o}s-R\'enyi and $\mathsf{RGG}$ such that $\mathsf{G}(n,p(1 - \tilde{O}(\sqrt{np/d})))\subseteq \mathsf{RGG}(n,\mathbb{S}^{d-1},p)\subseteq \mathsf{G}(n,p(1 + \tilde{O}(\sqrt{np/d})))$ edgewise with high probability when $d\gg np.$ We apply the result to: 1) Sharp Thresholds: We show that for any monotone property having a sharp threshold with respect to the Erd\H{o}s-R\'enyi distribution and critical probability $p^c_n,$ random geometric graphs also exhibit a sharp threshold when $d\gg np^c_n,$ thus partially answering a question of Perkins. 2) Robust Testing: The coupling shows that testing between $\mathsf{G}(n,p)$ and $\mathsf{RGG}(n,\mathbb{S}^{d-1},p)$ with $\epsilon n^2p$ adversarially corrupted edges for any constant $\epsilon&gt;0$ is information-theoretically impossible when $d\gg np.$ We match this lower bound with an efficient (constant degree SoS) spectral refutation algorithm when $d\ll np.$ 3) Enumeration: We show that the number of geometric graphs in dimension $d$ is at least $\exp(dn\log^{-7}n)$, recovering (up to the log factors) the sharp result of Sauermann.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00995v1</guid>
      <category>math.PR</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kiril Bangachev, Guy Bresler</dc:creator>
    </item>
    <item>
      <title>Universality of kernel random matrices and kernel regression in the quadratic regime</title>
      <link>https://arxiv.org/abs/2408.01062</link>
      <description>arXiv:2408.01062v1 Announce Type: cross 
Abstract: Kernel ridge regression (KRR) is a popular class of machine learning models that has become an important tool for understanding deep learning. Much of the focus has been on studying the proportional asymptotic regime, $n \asymp d$, where $n$ is the number of training samples and $d$ is the dimension of the dataset. In this regime, under certain conditions on the data distribution, the kernel random matrix involved in KRR exhibits behavior akin to that of a linear kernel. In this work, we extend the study of kernel regression to the quadratic asymptotic regime, where $n \asymp d^2$. In this regime, we demonstrate that a broad class of inner-product kernels exhibit behavior similar to a quadratic kernel. Specifically, we establish an operator norm approximation bound for the difference between the original kernel random matrix and a quadratic kernel random matrix with additional correction terms compared to the Taylor expansion of the kernel functions. The approximation works for general data distributions under a Gaussian-moment-matching assumption with a covariance structure. This new approximation is utilized to obtain a limiting spectral distribution of the original kernel matrix and characterize the precise asymptotic training and generalization errors for KRR in the quadratic regime when $n/d^2$ converges to a non-zero constant. The generalization errors are obtained for both deterministic and random teacher models. Our proof techniques combine moment methods, Wick's formula, orthogonal polynomials, and resolvent analysis of random matrices with correlated entries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01062v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parthe Pandit, Zhichao Wang, Yizhe Zhu</dc:creator>
    </item>
    <item>
      <title>Parametrized Families of Gibbs Measures and their Statistical Inference</title>
      <link>https://arxiv.org/abs/2408.01104</link>
      <description>arXiv:2408.01104v1 Announce Type: cross 
Abstract: For H\"older continuous functions $f_i$, $i=0,\ldots ,d$, on a subshift of finite type and $\Theta\subset \mathbb \R^d$ we consider a parametrized family of potentials $\{F_\theta= f_0+\sum_{i=1}^d \theta_i f_i : \theta\in \Theta\}$. We show that the maximum likelihood estimator of $\theta$ for a family of Gibbs measures with potentials $F_\theta$ is consistent and determine its asymptotic distribution under the associated shift-invariant distribution. A second part discusses applications; from confidence intervals through testing problems to connections to Bernoulli distributions and stationary Markov chains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01104v1</guid>
      <category>math.DS</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manfred Denker, Marc Ke{\ss}eb\"ohmer, Artur O. Lopes, Silvia R. C. Lopes</dc:creator>
    </item>
    <item>
      <title>Generalized kernel distance covariance in high dimensions: non-null CLTs and power universality</title>
      <link>https://arxiv.org/abs/2106.07725</link>
      <description>arXiv:2106.07725v2 Announce Type: replace 
Abstract: Distance covariance is a popular dependence measure for two random vectors $X$ and $Y$ of possibly different dimensions and types. Recent years have witnessed concentrated efforts in the literature to understand the distributional properties of the sample distance covariance in a high-dimensional setting, with an exclusive emphasis on the null case that $X$ and $Y$ are independent. This paper derives the first non-null central limit theorem for the sample distance covariance, and the more general sample (Hilbert-Schmidt) kernel distance covariance in high dimensions, primarily in the Gaussian case. The new non-null central limit theorem yields an asymptotically exact first-order power formula for the widely used generalized kernel distance correlation test of independence between $X$ and $Y$. The power formula in particular unveils an interesting universality phenomenon: the power of the generalized kernel distance correlation test is completely determined by $n\cdot \text{dcor}^2(X,Y)/\sqrt{2}$ in the high dimensional limit, regardless of a wide range of choices of the kernels and bandwidth parameters. Furthermore, this separation rate is also shown to be optimal in a minimax sense. The key step in the proof of the non-null central limit theorem is a precise expansion of the mean and variance of the sample distance covariance in high dimensions, which shows, among other things, that the non-null Gaussian approximation of the sample distance covariance involves a rather subtle interplay between the dimension-to-sample ratio and the dependence between $X$ and $Y$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.07725v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Qiyang Han, Yandi Shen</dc:creator>
    </item>
    <item>
      <title>Performance of the stochastic MV-PURE estimator in highly noisy settings</title>
      <link>https://arxiv.org/abs/1303.4956</link>
      <description>arXiv:1303.4956v2 Announce Type: replace-cross 
Abstract: The stochastic minimum-variance pseudo-unbiased reduced-rank estimator (stochastic MV-PURE estimator) has been developed to provide linear estimation with robustness against high noise levels, imperfections in model knowledge, and ill-conditioned systems. In this paper, we investigate the theoretical performance of the stochastic MV-PURE estimator under varying levels of additive noise. We prove that the mean-square-error (MSE) of this estimator in the low signal-to-noise (SNR) region is much smaller than that obtained with its full-rank version, the minimum-variance distortionless estimator, and the gap becomes larger as the noise level increases. These results shed light on the excellent performance of the stochastic MV-PURE estimator in highly noisy settings obtained in simulations so far. Furthermore, we extend previous numerical simulations to show how the insight gained from the results of this paper can be used in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:1303.4956v2</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jfranklin.2014.03.012</arxiv:DOI>
      <dc:creator>Tomasz Piotrowski, Isao Yamada</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Interval Estimation for Optimal Policy Evaluation in Online Learning</title>
      <link>https://arxiv.org/abs/2110.15501</link>
      <description>arXiv:2110.15501v4 Announce Type: replace-cross 
Abstract: Evaluating the performance of an ongoing policy plays a vital role in many areas such as medicine and economics, to provide crucial instructions on the early-stop of the online experiment and timely feedback from the environment. Policy evaluation in online learning thus attracts increasing attention by inferring the mean outcome of the optimal policy (i.e., the value) in real-time. Yet, such a problem is particularly challenging due to the dependent data generated in the online environment, the unknown optimal policy, and the complex exploration and exploitation trade-off in the adaptive experiment. In this paper, we aim to overcome these difficulties in policy evaluation for online learning. We explicitly derive the probability of exploration that quantifies the probability of exploring non-optimal actions under commonly used bandit algorithms. We use this probability to conduct valid inference on the online conditional mean estimator under each action and develop the doubly robust interval estimation (DREAM) method to infer the value under the estimated optimal policy in online learning. The proposed value estimator provides double protection for consistency and is asymptotically normal with a Wald-type confidence interval provided. Extensive simulation studies and real data applications are conducted to demonstrate the empirical validity of the proposed DREAM method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.15501v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Shen, Hengrui Cai, Rui Song</dc:creator>
    </item>
    <item>
      <title>Mixed moving average field guided learning for spatio-temporal data</title>
      <link>https://arxiv.org/abs/2301.00736</link>
      <description>arXiv:2301.00736v4 Announce Type: replace-cross 
Abstract: Influenced mixed moving average fields are a versatile modeling class for spatio-temporal data. However, their predictive distribution is not generally known. Under this modeling assumption, we define a novel spatio-temporal embedding and a theory-guided machine learning approach that employs a generalized Bayesian algorithm to make ensemble forecasts. We use Lipschitz predictors and determine fixed-time and any-time PAC Bayesian bounds in the batch learning setting. Performing causal forecast is a highlight of our methodology as its potential application to data with spatial and temporal short and long-range dependence. We then test the performance of our learning methodology by using linear predictors and data sets simulated from a spatio-temporal Ornstein-Uhlenbeck process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.00736v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Imma Valentina Curato, Orkun Furat, Lorenzo Proietti, Bennet Stroeh</dc:creator>
    </item>
    <item>
      <title>Empirical limit theorems for Wiener chaos</title>
      <link>https://arxiv.org/abs/2310.15462</link>
      <description>arXiv:2310.15462v4 Announce Type: replace-cross 
Abstract: We consider empirical measures in a triangular array setup with underlying distributions varying as sample size grows. We study asymptotic properties of multiple integrals with respect to normalized empirical measures. Limit theorems involving series of multiple Wiener-It\^o integrals are established.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.15462v4</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spl.2024.110222</arxiv:DOI>
      <dc:creator>Shuyang Bai, Jiemiao Chen</dc:creator>
    </item>
    <item>
      <title>Moment varieties from inverse Gaussian and gamma distributions</title>
      <link>https://arxiv.org/abs/2312.10433</link>
      <description>arXiv:2312.10433v2 Announce Type: replace-cross 
Abstract: Motivated by previous work on moment varieties for Gaussian distributions and their mixtures, we study moment varieties for two other statistically important two-parameter distributions: the inverse Gaussian and gamma distributions. In particular, we realize the moment varieties as determinantal varieties and find their degrees and singularities. We also provide computational evidence for algebraic identifiability of mixtures, and study the identifiability degree and Euclidean distance degree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10433v2</guid>
      <category>math.AG</category>
      <category>math.AC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oskar Henriksson, Lisa Seccia, Teresa Yu</dc:creator>
    </item>
  </channel>
</rss>
