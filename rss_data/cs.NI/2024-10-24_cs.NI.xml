<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Oct 2024 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>ENWAR: A RAG-empowered Multi-Modal LLM Framework for Wireless Environment Perception</title>
      <link>https://arxiv.org/abs/2410.18104</link>
      <description>arXiv:2410.18104v1 Announce Type: new 
Abstract: Large language models (LLMs) hold significant promise in advancing network management and orchestration in 6G and beyond networks. However, existing LLMs are limited in domain-specific knowledge and their ability to handle multi-modal sensory data, which is critical for real-time situational awareness in dynamic wireless environments. This paper addresses this gap by introducing ENWAR, an ENvironment-aWARe retrieval augmented generation-empowered multi-modal LLM framework. ENWAR seamlessly integrates multi-modal sensory inputs to perceive, interpret, and cognitively process complex wireless environments to provide human-interpretable situational awareness. ENWAR is evaluated on the GPS, LiDAR, and camera modality combinations of DeepSense6G dataset with state-of-the-art LLMs such as Mistral-7b/8x7b and LLaMa3.1-8/70/405b. Compared to general and often superficial environmental descriptions of these vanilla LLMs, ENWAR delivers richer spatial analysis, accurately identifies positions, analyzes obstacles, and assesses line-of-sight between vehicles. Results show that ENWAR achieves key performance indicators of up to 70% relevancy, 55% context recall, 80% correctness, and 86% faithfulness, demonstrating its efficacy in multi-modal perception and interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18104v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmad M. Nazar, Abdulkadir Celik, Mohamed Y. Selim, Asmaa Abdallah, Daji Qiao, Ahmed M. Eltawil</dc:creator>
    </item>
    <item>
      <title>LLM-Slice: Dedicated Wireless Network Slicing for Large Language Models</title>
      <link>https://arxiv.org/abs/2410.18499</link>
      <description>arXiv:2410.18499v1 Announce Type: new 
Abstract: The rapid adoption of large language models (LLMs) presents new challenges for existing network architectures due to significant peak traffic and high communication uncertainty. Traditional wireless networks struggle to support efficiently, leading to intolerable response delays, disconnections, and resource wastage. To address these issues, we propose LLM-Slice, the first system to provide dedicated communication slices for LLMs within a wireless network environment. By creating LLM-specific network slices, LLM-Slice efficiently binds services with communication resources. Based on user equipment (UE) requests and a permissions database, the system registers specific slices to offer controllable LLM services, integrating a downlink resource control module to optimize response speed, enhance resource utilization, and reduce disconnections. By deploying and validating in a real UE-gNB-CN environment, numerical results demonstrate that LLM-Slice significantly improves response speed and resource efficiency, providing a novel solution for fast and controllable LLM access in wireless networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18499v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Boyi Liu, Jingwen Tong, Jun Zhang</dc:creator>
    </item>
    <item>
      <title>Dynamic Content Caching with Waiting Costs via Restless Multi-Armed Bandits</title>
      <link>https://arxiv.org/abs/2410.18627</link>
      <description>arXiv:2410.18627v1 Announce Type: new 
Abstract: We consider a system with a Base Station (BS) associated with local cache, which in turn is connected to the backend server and users. The contents get continuously updated at the backend server, and the BS has a local copy of the subset of the contents. Upon receiving a request from the user, the BS can either fetch a fresh version or, serve the local copy or can wait for additional requests before serving. Fetching content from the BS incurs a fixed fetching cost, serving it locally incurs an aging cost, and for each request waiting at the BS, there will be a waiting for cost per unit time. The aging cost relies on the freshness of the content, which is measured by a metric age of version (AoV). We aim to minimize the average cost subject to cache capacity constraints. We pose the problem as Restless Multi-armed Bandits Problem (RMAB) and propose a Whittle index-based policy that performs very close to the optimal policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18627v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ankita Koley, Chandramani Singh</dc:creator>
    </item>
    <item>
      <title>PASTRAMI: Performance Assessment of SofTware Routers Addressing Measurement Instability</title>
      <link>https://arxiv.org/abs/2410.18644</link>
      <description>arXiv:2410.18644v1 Announce Type: new 
Abstract: Virtualized environments offer a flexible and scalable platform for evaluating network performance, but they can introduce significant variability that complicates accurate measurement. This paper presents PASTRAMI, a methodology designed to assess the stability of software routers, which is critical to accurately evaluate performance metrics such as the Partial Drop Rate at 0.5% (PDR@0.5%). While PDR@0.5% is a key metric to assess packet processing capabilities of a software router, its reliable evaluation depends on consistent router performance with minimal measurement variability. Our research reveals that different Linux versions exhibit distinct behaviors, with some demonstrating non-negligible packet loss even at low loads and high variability in loss measurements, rendering them unsuitable for accurate performance assessments. This paper proposes a systematic approach to differentiate between stable and unstable environments, offering practical guidance on selecting suitable configurations for robust networking performance evaluations in virtualized environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18644v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paolo Lungaroni, Andrea Mayer, Stefano Salsano, Pierpaolo Loreti, Lorenzo Bracciale</dc:creator>
    </item>
    <item>
      <title>5G Replicates TSN: Extending IEEE 802.1CB Capabilities to Integrated 5G/TSN Systems</title>
      <link>https://arxiv.org/abs/2410.18739</link>
      <description>arXiv:2410.18739v1 Announce Type: new 
Abstract: The IEEE 802.1 time-sensitive networking (TSN) standards improve real-time capabilities of the standard Ethernet. TSN and local/private 5G systems are envisaged to co-exist in industrial environments. The IEEE 802.1CB standard provides fault tolerance to TSN systems via frame replication and elimination for reliability (FRER) capabilities. This paper presents X-FRER, a novel framework for extending FRER capabilities to the 3GPP-defined bridge model for 5G and TSN integration. The different embodiments of X-FRER realize FRER-like functionality through multi-path transmissions in a 5G system based on a single or multiple protocol data unit (PDU) sessions. X-FRER also provides enhanced replication and elimination functionality for integrated deployments. Performance evaluation shows that X-FRER empowers a vanilla 5G system with TSN-like capabilities for end-to-end reliability in integrated TSN and 5G deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18739v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adnan Aijaz</dc:creator>
    </item>
    <item>
      <title>Large Generative AI Models meet Open Networks for 6G: Integration, Platform, and Monetization</title>
      <link>https://arxiv.org/abs/2410.18790</link>
      <description>arXiv:2410.18790v1 Announce Type: new 
Abstract: Generative artificial intelligence (GAI) has emerged as a pivotal technology for content generation, reasoning, and decision-making, making it a promising solution on the 6G stage characterized by openness, connected intelligence, and service democratization. This article explores strategies for integrating and monetizing GAI within future open 6G networks, mainly from the perspectives of mobile network operators (MNOs). We propose a novel API-centric telecoms GAI marketplace platform, designed to serve as a central hub for deploying, managing, and monetizing diverse GAI services directly within the network. This platform underpins a flexible and interoperable ecosystem, enhances service delivery, and facilitates seamless integration of GAI capabilities across various network segments, thereby enabling new revenue streams through customer-centric generative services. Results from experimental evaluation in an end-to-end Open RAN testbed, show the latency benefits of this platform for local large language model (LLM) deployment, by comparing token timing for various generated lengths with cloud-based general-purpose LLMs. Lastly, the article discusses key considerations for implementing the GAI marketplace within 6G networks, including monetization strategy, regulatory, management, and service platform aspects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18790v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Peizheng Li, Adri\'an S\'anchez-Momp\'o, Tim Farnham, Aftab Khan, Adnan Aijaz</dc:creator>
    </item>
    <item>
      <title>Adapting MLOps for Diverse In-Network Intelligence in 6G Era: Challenges and Solutions</title>
      <link>https://arxiv.org/abs/2410.18793</link>
      <description>arXiv:2410.18793v1 Announce Type: new 
Abstract: Seamless integration of artificial intelligence (AI) and machine learning (ML) techniques with wireless systems is a crucial step for 6G AInization. However, such integration faces challenges in terms of model functionality and lifecycle management. ML operations (MLOps) offer a systematic approach to tackle these challenges. Existing approaches toward implementing MLOps in a centralized platform often overlook the challenges posed by diverse learning paradigms and network heterogeneity. This article provides a new approach to MLOps targeting the intricacies of future wireless networks. Considering unique aspects of the future radio access network (RAN), we formulate three operational pipelines, namely reinforcement learning operations (RLOps), federated learning operations (FedOps), and generative AI operations (GenOps). These pipelines form the foundation for seamlessly integrating various learning/inference capabilities into networks. We outline the specific challenges and proposed solutions for each operation, facilitating large-scale deployment of AI-Native 6G networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18793v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Peizheng Li, Ioannis Mavromatis, Tim Farnham, Adnan Aijaz, Aftab Khan</dc:creator>
    </item>
    <item>
      <title>Towards Edge General Intelligence via Large Language Models: Opportunities and Challenges</title>
      <link>https://arxiv.org/abs/2410.18125</link>
      <description>arXiv:2410.18125v1 Announce Type: cross 
Abstract: Edge Intelligence (EI) has been instrumental in delivering real-time, localized services by leveraging the computational capabilities of edge networks. The integration of Large Language Models (LLMs) empowers EI to evolve into the next stage: Edge General Intelligence (EGI), enabling more adaptive and versatile applications that require advanced understanding and reasoning capabilities. However, systematic exploration in this area remains insufficient. This survey delineates the distinctions between EGI and traditional EI, categorizing LLM-empowered EGI into three conceptual systems: centralized, hybrid, and decentralized. For each system, we detail the framework designs and review existing implementations. Furthermore, we evaluate the performance and throughput of various Small Language Models (SLMs) that are more suitable for development on edge devices. This survey provides researchers with a comprehensive vision of EGI, offering insights into its vast potential and establishing a foundation for future advancements in this rapidly evolving field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18125v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Handi Chen, Weipeng Deng, Shuo Yang, Jinfeng Xu, Zhihan Jiang, Edith C. H. Ngai, Jiangchuan Liu, Xue Liu</dc:creator>
    </item>
    <item>
      <title>Augmenting Training Data with Vector-Quantized Variational Autoencoder for Classifying RF Signals</title>
      <link>https://arxiv.org/abs/2410.18283</link>
      <description>arXiv:2410.18283v1 Announce Type: cross 
Abstract: Radio frequency (RF) communication has been an important part of civil and military communication for decades. With the increasing complexity of wireless environments and the growing number of devices sharing the spectrum, it has become critical to efficiently manage and classify the signals that populate these frequencies. In such scenarios, the accurate classification of wireless signals is essential for effective spectrum management, signal interception, and interference mitigation. However, the classification of wireless RF signals often faces challenges due to the limited availability of labeled training data, especially under low signal-to-noise ratio (SNR) conditions. To address these challenges, this paper proposes the use of a Vector-Quantized Variational Autoencoder (VQ-VAE) to augment training data, thereby enhancing the performance of a baseline wireless classifier. The VQ-VAE model generates high-fidelity synthetic RF signals, increasing the diversity and fidelity of the training dataset by capturing the complex variations inherent in RF communication signals. Our experimental results show that incorporating VQ-VAE-generated data significantly improves the classification accuracy of the baseline model, particularly in low SNR conditions. This augmentation leads to better generalization and robustness of the classifier, overcoming the constraints imposed by limited real-world data. By improving RF signal classification, the proposed approach enhances the efficacy of wireless communication in both civil and tactical settings, ensuring reliable and secure operations. This advancement supports critical decision-making and operational readiness in environments where communication fidelity is essential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18283v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Srihari Kamesh Kompella, Kemal Davaslioglu, Yalin E. Sagduyu, Sastry Kompella</dc:creator>
    </item>
    <item>
      <title>Advancing Network Security: A Comprehensive Testbed and Dataset for Machine Learning-Based Intrusion Detection</title>
      <link>https://arxiv.org/abs/2410.18332</link>
      <description>arXiv:2410.18332v1 Announce Type: cross 
Abstract: This paper introduces a Testbed designed for generating network traffic, leveraging the capabilities of containers, Kubernetes, and eBPF/XDP technologies. Our Testbed serves as an advanced platform for producing network traffic for machine learning based network experiments. By utilizing this Testbed, we offer small malicious network traffic dataset publically that satisfy ground truth property completely.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18332v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Talaya Farasat, JongWon Kim, Joachim Posegga</dc:creator>
    </item>
    <item>
      <title>Optimizing Edge Offloading Decisions for Object Detection</title>
      <link>https://arxiv.org/abs/2410.18919</link>
      <description>arXiv:2410.18919v1 Announce Type: cross 
Abstract: Recent advances in machine learning and hardware have produced embedded devices capable of performing real-time object detection with commendable accuracy. We consider a scenario in which embedded devices rely on an onboard object detector, but have the option to offload detection to a more powerful edge server when local accuracy is deemed too low. Resource constraints, however, limit the number of images that can be offloaded to the edge. Our goal is to identify which images to offload to maximize overall detection accuracy under those constraints. To that end, the paper introduces a reward metric designed to quantify potential accuracy improvements from offloading individual images, and proposes an efficient approach to make offloading decisions by estimating this reward based only on local detection results. The approach is computationally frugal enough to run on embedded devices, and empirical findings indicate that it outperforms existing alternatives in improving detection accuracy even when the fraction of offloaded images is small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18919v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaming Qiu, Ruiqi Wang, Brooks Hu, Roch Guerin, Chenyang Lu</dc:creator>
    </item>
    <item>
      <title>ServeFlow: A Fast-Slow Model Architecture for Network Traffic Analysis</title>
      <link>https://arxiv.org/abs/2402.03694</link>
      <description>arXiv:2402.03694v2 Announce Type: replace 
Abstract: Network traffic analysis increasingly uses complex machine learning models as the internet consolidates and traffic gets more encrypted. However, over high-bandwidth networks, flows can easily arrive faster than model inference rates. The temporal nature of network flows limits simple scale-out approaches leveraged in other high-traffic machine learning applications. Accordingly, this paper presents ServeFlow, a solution for machine-learning model serving aimed at network traffic analysis tasks, which carefully selects the number of packets to collect and the models to apply for individual flows to achieve a balance between minimal latency, high service rate, and high accuracy. We identify that on the same task, inference time across models can differ by 1.8x - 141.3x, while the inter-packet waiting time is up to 6-8 orders of magnitude higher than the inference time! Based on these insights, we tailor a novel fast-slow model architecture for networking ML pipelines. Flows are assigned to a slower model only when the inferences from the fast model are deemed high uncertainty. ServeFlow is able to make inferences on 76.3% of flows in under 16ms, which is a speed-up of 40.5x on the median end-to-end serving latency while increasing the service rate and maintaining similar accuracy. Even with thousands of features per flow, it achieves a service rate of over 48.5k new flows per second on a 16-core CPU commodity server, which matches the order of magnitude of flow rates observed on city-level network backbones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03694v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shinan Liu, Ted Shaowang, Gerry Wan, Jeewon Chae, Jonatas Marques, Sanjay Krishnan, Nick Feamster</dc:creator>
    </item>
    <item>
      <title>R-ACP: Real-Time Adaptive Collaborative Perception Leveraging Robust Task-Oriented Communications</title>
      <link>https://arxiv.org/abs/2410.04168</link>
      <description>arXiv:2410.04168v3 Announce Type: replace 
Abstract: Collaborative perception enhances sensing in multi-robot and vehicular networks by fusing information from multiple agents, improving perception accuracy and sensing range. However, mobility and non-rigid sensor mounts introduce extrinsic calibration errors, necessitating online calibration, further complicated by limited overlap in sensing regions. Moreover, maintaining fresh information is crucial for timely and accurate sensing. To address calibration errors and ensure timely and accurate perception, we propose a robust task-oriented communication strategy to optimize online self-calibration and efficient feature sharing for Real-time Adaptive Collaborative Perception (R-ACP). Specifically, we first formulate an Age of Perceived Targets (AoPT) minimization problem to capture data timeliness of multi-view streaming. Then, in the calibration phase, we introduce a channel-aware self-calibration technique based on re-identification (Re-ID), which adaptively compresses key features according to channel capacities, effectively addressing calibration issues via spatial and temporal cross-camera correlations. In the streaming phase, we tackle the trade-off between bandwidth and inference accuracy by leveraging an Information Bottleneck (IB) based encoding method to adjust video compression rates based on task relevance, thereby reducing communication overhead and latency. Finally, we design a priority-aware network to filter corrupted features to mitigate performance degradation from packet corruption. Extensive studies demonstrate that our framework outperforms five baselines, improving multiple object detection accuracy (MODA) by 25.49% and reducing communication costs by 51.36% under severely poor channel conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04168v3</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengru Fang, Jingjing Wang, Yanan Ma, Yihang Tao, Yiqin Deng, Xianhao Chen, Yuguang Fang</dc:creator>
    </item>
    <item>
      <title>FlowTracer: A Tool for Uncovering Network Path Usage Imbalance in AI Training Clusters</title>
      <link>https://arxiv.org/abs/2410.17078</link>
      <description>arXiv:2410.17078v2 Announce Type: replace 
Abstract: The increasing complexity of AI workloads, especially distributed Large Language Model (LLM) training, places significant strain on the networking infrastructure of parallel data centers and supercomputing systems. While Equal-Cost Multi- Path (ECMP) routing distributes traffic over parallel paths, hash collisions often lead to imbalanced network resource utilization and performance bottlenecks. This paper presents FlowTracer, a tool designed to analyze network path utilization and evaluate different routing strategies. FlowTracer aids in debugging network inefficiencies by providing detailed visibility into traffic distribution and helping to identify the root causes of performance degradation, such as issues caused by hash collisions. By offering flow-level insights, FlowTracer enables system operators to optimize routing, reduce congestion, and improve the performance of distributed AI workloads. We use a RoCEv2-enabled cluster with a leaf-spine network and 16 400-Gbps nodes to demonstrate how FlowTracer can be used to compare the flow imbalances of ECMP routing against a statically configured network. The example showcases a 30% reduction in imbalance, as measured by a new metric we introduce.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17078v2</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hasibul Jamil, Abdul Alim, Laurent Schares, Pavlos Maniotis, Liran Schour, Ali Sydney, Abdullah Kayi, Tevfik Kosar, Bengi Karacali</dc:creator>
    </item>
  </channel>
</rss>
