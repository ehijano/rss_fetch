<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Sep 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Damage detection in an uncertain nonlinear beam based on stochastic Volterra series: an experimental application</title>
      <link>https://arxiv.org/abs/2409.16305</link>
      <description>arXiv:2409.16305v1 Announce Type: new 
Abstract: The damage detection problem becomes a more difficult task when the intrinsically nonlinear behavior of the structures and the natural data variation are considered in the analysis because both phenomena can be confused with damage if linear and deterministic approaches are implemented. Therefore, this work aims the experimental application of a stochastic version of the Volterra series combined with a novelty detection approach to detect damage in an initially nonlinear system taking into account the measured data variation, caused by the presence of uncertainties. The experimental setup is composed by a cantilever beam operating in a nonlinear regime of motion, even in the healthy condition, induced by the presence of a magnet near to the free extremity. The damage associated with mass changes in a bolted connection (nuts loosed) is detected based on the comparison between linear and nonlinear contributions of the stochastic Volterra kernels in the total response, estimated in the reference and damaged conditions. The experimental measurements were performed on different days to add natural variation to the data measured. The results obtained through the stochastic proposed approach are compared with those obtained by the deterministic version of the Volterra series, showing the advantage of the stochastic model use when we consider the experimental data variation with the capability to detect the presence of the damage with statistical confidence. Besides, the nonlinear metric used presented a higher sensitivity to the occurrence of the damage compared with the linear one, justifying the application of a nonlinear metric when the system exhibits intrinsically nonlinear behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16305v1</guid>
      <category>cs.CE</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ymssp.2019.03.045</arxiv:DOI>
      <arxiv:journal_reference>Mechanical Systems and Signal Processing, vol. 128, pp. 463-478, 2019</arxiv:journal_reference>
      <dc:creator>Luis Gustavo Gioacon Villani, Samuel da Silva, Americo Cunha Jr, Michael D. Todd</dc:creator>
    </item>
    <item>
      <title>A multi-scale probabilistic model for fatigue lifetime characterization of alloys in the presence of random process-induced pores</title>
      <link>https://arxiv.org/abs/2409.16565</link>
      <description>arXiv:2409.16565v1 Announce Type: new 
Abstract: A probabilistic fatigue lifetime model is developed in conjunction with a multi-scale method for structures with pores whose exact distribution, i.e. geometries and locations, is unknown. The model takes into account uncertainty in fatigue lifetimes in structures due to defects at two scales: micro-scale heterogeneity &amp; meso-scale pores. An element-wise probabilistic strain-life model with its criterion modified for taking into account multiaxial loading is developed for taking into account the effect of micro-scale defects on the fatigue lifetime. The effect of meso-scale pores in the structure is taken into account via statistical modelling of the expected pore populations via a finite element method, based on tomographic scans of a small region of porous material used to make the structure. A previously implemented Neuber-type plastic correction algorithm is used for fast full-field approximation of the strain-life criterion around the statistically generated pore fields. The probability of failure of a porous structure is obtained via a weakest link assumption at the level of its constituent finite elements. The fatigue model can be identified via a maximum likelihood estimate on experimental fatigue data of structures containing different types of pore populations. The proposed method is tested on an existing data-set of an aluminium alloy with two levels of porosity. The model requires lesser data for identification than traditional models that consider porous media as a homogeneous material, as the same base material is considered for the two grades of porous material. Numerical studies on synthetically generated data show that the model is capable of taking into account the statistical size effect in fatigue, and demonstrate that fatigue properties of subsurface porous material are lower than that of core porous material, which makes homogenisation of the model non-trivial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16565v1</guid>
      <category>cs.CE</category>
      <category>cond-mat.mtrl-sci</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhishek Palchoudhary, Cristian Ovalle, Vincent Maurel, Pierre Kerfriden</dc:creator>
    </item>
    <item>
      <title>An Integrated Machine Learning and Deep Learning Framework for Credit Card Approval Prediction</title>
      <link>https://arxiv.org/abs/2409.16676</link>
      <description>arXiv:2409.16676v1 Announce Type: new 
Abstract: Credit scoring is vital in the financial industry, assessing the risk of lending to credit card applicants. Traditional credit scoring methods face challenges with large datasets and data imbalance between creditworthy and non-creditworthy applicants. This paper introduces an advanced machine learning and deep learning framework to improve the accuracy and reliability of credit card approval predictions. We utilized extensive datasets of user application records and credit history, implementing a comprehensive preprocessing strategy, feature engineering, and model integration. Our methodology combines neural networks with an ensemble of base models, including logistic regression, support vector machines, k-nearest neighbors, decision trees, random forests, and gradient boosting. The ensemble approach addresses data imbalance using Synthetic Minority Over-sampling Technique (SMOTE) and mitigates overfitting risks. Experimental results show that our integrated model surpasses traditional single-model approaches in precision, recall, F1-score, AUC, and Kappa, providing a robust and scalable solution for credit card approval predictions. This research underscores the potential of advanced machine learning techniques to transform credit risk assessment and financial decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16676v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kejian Tong, Zonglin Han, Yanxin Shen, Yujian Long, Yijing Wei</dc:creator>
    </item>
    <item>
      <title>Beyond Turing Test: Can GPT-4 Sway Experts' Decisions?</title>
      <link>https://arxiv.org/abs/2409.16710</link>
      <description>arXiv:2409.16710v1 Announce Type: new 
Abstract: In the post-Turing era, evaluating large language models (LLMs) involves assessing generated text based on readers' reactions rather than merely its indistinguishability from human-produced content. This paper explores how LLM-generated text impacts readers' decisions, focusing on both amateur and expert audiences. Our findings indicate that GPT-4 can generate persuasive analyses affecting the decisions of both amateurs and professionals. Furthermore, we evaluate the generated text from the aspects of grammar, convincingness, logical coherence, and usefulness. The results highlight a high correlation between real-world evaluation through audience reactions and the current multi-dimensional evaluators commonly used for generative models. Overall, this paper shows the potential and risk of using generated text to sway human decisions and also points out a new direction for evaluating generated text, i.e., leveraging the reactions and decisions of readers. We release our dataset to assist future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16710v1</guid>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takehiro Takayanagi, Hiroya Takamura, Kiyoshi Izumi, Chung-Chi Chen</dc:creator>
    </item>
    <item>
      <title>Stochastic Modelling of Elasticity Tensors</title>
      <link>https://arxiv.org/abs/2409.16714</link>
      <description>arXiv:2409.16714v1 Announce Type: new 
Abstract: We present a novel framework for the probabilistic modelling of random fourth order material tensor fields, with a focus on tensors that are physically symmetric and positive definite (SPD), of which the elasticity tensor is a prime example. Given the critical role that spatial symmetries and invariances play in determining material behaviour, it is essential to incorporate these aspects into the probabilistic description and modelling of material properties. In particular, we focus on spatial point symmetries or invariances under rotations, a classical subject in elasticity. Following this, we formulate a stochastic modelling framework using a Lie algebra representation via a memoryless transformation that respects the requirements of positive definiteness and invariance. With this, it is shown how to generate a random ensemble of elasticity tensors that allows an independent control of strength, eigenstrain, and orientation. The procedure also accommodates the requirement to prescribe specific spatial symmetries and invariances for each member of the whole ensemble, while ensuring that the mean or expected value of the ensemble conforms to a potentially 'higher' class of spatial invariance. Furthermore, it is important to highlight that the set of SPD tensors forms a differentiable manifold, which geometrically corresponds to an open cone within the ambient space of symmetric tensors. Thus, we explore the mathematical structure of the underlying sample space of such tensors, and introduce a new distance measure or metric, called the 'elasticity metric', between the tensors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16714v1</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sharana Kumar Shivanand, Bojana Rosi\'c, Hermann G. Matthies</dc:creator>
    </item>
    <item>
      <title>Analytical assessment of workers' safety concerning direct and indirect ways of getting infected by dangerous pathogen</title>
      <link>https://arxiv.org/abs/2409.16809</link>
      <description>arXiv:2409.16809v1 Announce Type: new 
Abstract: The development of safety policies for protecting large groups of individuals working in indoor environments against disease spreading provides an important and challenging task. To address this issue, we investigate the scenario of workers getting infected by the dangerous airborne pathogen in a close to real-life industrial environment. We present the simple analytical model based on the observations made during the recent pandemic, and business expectations concerning the protection of workers. The model can be tuned to handle other epidemic or non-epidemic threads, including dangerous vapors from industrial processes. In the presented model, we consider direct and indirect ways of getting infected, the first by direct contact with an infected agent, and the second by contact with a contaminated environment, including air in compartments or working surfaces. Our analysis is based on the simplified droplet/aerosol spreading diffusion model, validated by droplets' spreading simulations. The model can be easily applied to new scenarios and has modest computational requirements compared with the simulations. Hence, the model can be applied in an automated protection ecosystem in the industrial environment, where the time for assessing danger is limited, and computation has to be performed almost in real time. Using a simple agent-based model, we confirm the general research conclusion on disease spreading. From our results, we draft a set of countermeasures for infection spreading, which could be used as the basis of the prevention policy, suitable for use in industrial scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16809v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Krzysztof Domino, Arkadiusz Sochan, Jaros{\l}aw Adam Miszczak</dc:creator>
    </item>
    <item>
      <title>A parametric framework for kernel-based dynamic mode decomposition using deep learning</title>
      <link>https://arxiv.org/abs/2409.16817</link>
      <description>arXiv:2409.16817v1 Announce Type: cross 
Abstract: Surrogate modelling is widely applied in computational science and engineering to mitigate computational efficiency issues for the real-time simulations of complex and large-scale computational models or for many-query scenarios, such as uncertainty quantification and design optimisation. In this work, we propose a parametric framework for kernel-based dynamic mode decomposition method based on the linear and nonlinear disambiguation optimization (LANDO) algorithm. The proposed parametric framework consists of two stages, offline and online. The offline stage prepares the essential component for prediction, namely a series of LANDO models that emulate the dynamics of the system with particular parameters from a training dataset. The online stage leverages those LANDO models to generate new data at a desired time instant, and approximate the mapping between parameters and the state with the data using deep learning techniques. Moreover, dimensionality reduction technique is applied to high-dimensional dynamical systems to reduce the computational cost of training. Three numerical examples including Lotka-Volterra model, heat equation and reaction-diffusion equation are presented to demonstrate the efficiency and effectiveness of the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16817v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konstantinos Kevopoulos, Dongwei Ye</dc:creator>
    </item>
    <item>
      <title>A plastic correction algorithm for full-field elasto-plastic finite element simulations : critical assessment of predictive capabilities and improvement by machine learning</title>
      <link>https://arxiv.org/abs/2402.06313</link>
      <description>arXiv:2402.06313v3 Announce Type: replace 
Abstract: This paper introduces a new local plastic correction algorithm that is aimed at accelerating elasto-plastic finite element (FE) simulations for structural problems exhibiting localised plasticity (around e.g. notches, geometrical defects). The proposed method belongs to the category of generalised multi-axial Neuber-type methods, which process the results of an elastic prediction point-wise in order to calculate an approximation of the full elasto-plastic solution. The proposed algorithm relies on a rule of local proportionality, which, in the context of J2 plasticity, allows us to express the plastic plastic correction problem in terms of the amplitude of the full mechanical tensors only. This lightweight correction problem can be solved for numerically using a fully implicit time integrator that shares similarities with the radial return algorithm. The numerical capabilities of the proposed algorithm are demonstrated for a notched structure and a specimen containing a distribution of spherical pores, subjected to monotonic and cyclic loading. As a second point of innovation, we show that the proposed local plastic correction algorithm can be further accelerated by employing a simple meta-modelling strategy, with virtually no added errors. At last, we develop and investigate the merits of a deep-learning-based corrective layer designed to the approximation error of the plastic corrector. A convolutional architecture is used to analyse the neighbourhoods of material points and outputs a scalar correction to the point-wise Neuber-type predictions. This optional brick of the proposed plastic correction methodology relies on the availability of a set of full elasto-plastic finite element solutions to be used as training data-set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06313v3</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhishek Palchoudhary, Simone Peter, Vincent Maurel, Cristian Ovalle, Pierre Kerfriden</dc:creator>
    </item>
    <item>
      <title>Domain decomposition for data-driven reduced modeling of large-scale systems</title>
      <link>https://arxiv.org/abs/2311.00883</link>
      <description>arXiv:2311.00883v2 Announce Type: replace-cross 
Abstract: This paper focuses on the construction of accurate and predictive data-driven reduced models of large-scale numerical simulations with complex dynamics and sparse training datasets. In these settings, standard, single-domain approaches may be too inaccurate or may overfit and hence generalize poorly. Moreover, processing large-scale datasets typically requires significant memory and computing resources which can render single-domain approaches computationally prohibitive. To address these challenges, we introduce a domain decomposition formulation into the construction of a data-driven reduced model. In doing so, the basis functions used in the reduced model approximation become localized in space, which can increase the accuracy of the domain-decomposed approximation of the complex dynamics. The decomposition furthermore reduces the memory and computing requirements to process the underlying large-scale training dataset. We demonstrate the effectiveness and scalability of our approach in a large-scale three-dimensional unsteady rotating detonation rocket engine simulation scenario with over $75$ million degrees of freedom and a sparse training dataset. Our results show that compared to the single-domain approach, the domain-decomposed version reduces both the training and prediction errors for pressure by up to $13 \%$ and up to $5\%$ for other key quantities, such as temperature, and fuel and oxidizer mass fractions. Lastly, our approach decreases the memory requirements for processing by almost a factor of four, which in turn reduces the computing requirements as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00883v2</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.2514/1.J063715</arxiv:DOI>
      <dc:creator>Ionut-Gabriel Farcas, Rayomand P. Gundevia, Ramakanth Munipalli, Karen E. Willcox</dc:creator>
    </item>
    <item>
      <title>High-performance Effective Scientific Error-bounded Lossy Compression with Auto-tuned Multi-component Interpolation</title>
      <link>https://arxiv.org/abs/2311.12133</link>
      <description>arXiv:2311.12133v3 Announce Type: replace-cross 
Abstract: Error-bounded lossy compression has been identified as a promising solution for significantly reducing scientific data volumes upon users' requirements on data distortion. For the existing scientific error-bounded lossy compressors, some of them (such as SPERR and FAZ) can reach fairly high compression ratios and some others (such as SZx, SZ, and ZFP) feature high compression speeds, but they rarely exhibit both high ratio and high speed meanwhile. In this paper, we propose HPEZ with newly-designed interpolations and quality-metric-driven auto-tuning, which features significantly improved compression quality upon the existing high-performance compressors, meanwhile being exceedingly faster than high-ratio compressors. The key contributions lie in the following points: (1) We develop a series of advanced techniques such as interpolation re-ordering, multi-dimensional interpolation, and natural cubic splines to significantly improve compression qualities with interpolation-based data prediction. (2) The auto-tuning module in HPEZ has been carefully designed with novel strategies, including but not limited to block-wise interpolation tuning, dynamic dimension freezing, and Lorenzo tuning. (3) We thoroughly evaluate HPEZ compared with many other compressors on six real-world scientific datasets. Experiments show that HPEZ outperforms other high-performance error-bounded lossy compressors in compression ratio by up to 140% under the same error bound, and by up to 360% under the same PSNR. In parallel data transfer experiments on the distributed database, HPEZ achieves a significant performance gain with up to 40% time cost reduction over the second-best compressor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12133v3</guid>
      <category>cs.DC</category>
      <category>cs.CE</category>
      <category>cs.DB</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyang Liu, Sheng Di, Kai Zhao, Xin Liang, Sian Jin, Zizhe Jian, Jiajun Huang, Shixun Wu, Zizhong Chen, Franck Cappello</dc:creator>
    </item>
    <item>
      <title>Surrogate Modeling of Trajectory Map-matching in Urban Road Networks using Transformer Sequence-to-Sequence Model</title>
      <link>https://arxiv.org/abs/2404.12460</link>
      <description>arXiv:2404.12460v2 Announce Type: replace-cross 
Abstract: Large-scale geolocation telematics data acquired from connected vehicles has the potential to significantly enhance mobility infrastructures and operational systems within smart cities. To effectively utilize this data, it is essential to accurately match the geolocation data to the road segments. However, this matching is often not trivial due to the low sampling rate and errors exacerbated by multipath effects in urban environments. Traditionally, statistical modeling techniques such as Hidden-Markov models incorporating domain knowledge into the matching process have been extensively used for map-matching tasks. However, rule-based map-matching tasks are noise-sensitive and inefficient in processing large-scale trajectory data. Deep learning techniques directly learn the relationship between observed data and road networks from the data, often without the need for hand-crafted rules or domain knowledge. This renders them an efficient approach for map-matching large-scale datasets and more robust to the noise. This paper introduces a deep-learning model, specifically the transformer-based encoder-decoder model, to perform as a surrogate for offline map-matching algorithms. The encoder-decoder architecture initially encodes the series of noisy GPS points into a representation that automatically captures autoregressive behavior and spatial correlations between GPS points. Subsequently, the decoder associates data points with the road network features and thus transforms these representations into a sequence of road segments. The model is trained and evaluated using GPS traces collected in Manhattan, New York. Achieving an accuracy of 75%, transformer-based encoder-decoder models extensively employed in natural language processing presented a promising performance for translating noisy GPS data to the navigated routes in urban road networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12460v2</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sevin Mohammadi, Andrew W. Smyth</dc:creator>
    </item>
    <item>
      <title>A Differentiable Approach to Multi-scale Brain Modeling</title>
      <link>https://arxiv.org/abs/2406.19708</link>
      <description>arXiv:2406.19708v3 Announce Type: replace-cross 
Abstract: We present a multi-scale differentiable brain modeling workflow utilizing BrainPy, a unique differentiable brain simulator that combines accurate brain simulation with powerful gradient-based optimization. We leverage this capability of BrainPy across different brain scales. At the single-neuron level, we implement differentiable neuron models and employ gradient methods to optimize their fit to electrophysiological data. On the network level, we incorporate connectomic data to construct biologically constrained network models. Finally, to replicate animal behavior, we train these models on cognitive tasks using gradient-based learning rules. Experiments demonstrate that our approach achieves superior performance and speed in fitting generalized leaky integrate-and-fire and Hodgkin-Huxley single neuron models. Additionally, training a biologically-informed network of excitatory and inhibitory spiking neurons on working memory tasks successfully replicates observed neural activity and synaptic weight distributions. Overall, our differentiable multi-scale simulation approach offers a promising tool to bridge neuroscience data across electrophysiological, anatomical, and behavioral scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19708v3</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>q-bio.NC</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chaoming Wang, Muyang Lyu, Tianqiu Zhang, Sichao He, Si Wu</dc:creator>
    </item>
    <item>
      <title>LSR-IGRU: Stock Trend Prediction Based on Long Short-Term Relationships and Improved GRU</title>
      <link>https://arxiv.org/abs/2409.08282</link>
      <description>arXiv:2409.08282v2 Announce Type: replace-cross 
Abstract: Stock price prediction is a challenging problem in the field of finance and receives widespread attention. In recent years, with the rapid development of technologies such as deep learning and graph neural networks, more research methods have begun to focus on exploring the interrelationships between stocks. However, existing methods mostly focus on the short-term dynamic relationships of stocks and directly integrating relationship information with temporal information. They often overlook the complex nonlinear dynamic characteristics and potential higher-order interaction relationships among stocks in the stock market. Therefore, we propose a stock price trend prediction model named LSR-IGRU in this paper, which is based on long short-term stock relationships and an improved GRU input. Firstly, we construct a long short-term relationship matrix between stocks, where secondary industry information is employed for the first time to capture long-term relationships of stocks, and overnight price information is utilized to establish short-term relationships. Next, we improve the inputs of the GRU model at each step, enabling the model to more effectively integrate temporal information and long short-term relationship information, thereby significantly improving the accuracy of predicting stock trend changes. Finally, through extensive experiments on multiple datasets from stock markets in China and the United States, we validate the superiority of the proposed LSR-IGRU model over the current state-of-the-art baseline models. We also apply the proposed model to the algorithmic trading system of a financial company, achieving significantly higher cumulative portfolio returns compared to other baseline methods. Our sources are released at https://github.com/ZP1481616577/Baselines_LSR-IGRU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08282v2</guid>
      <category>q-fin.ST</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Zhu, Yuante Li, Yifan Hu, Qinyuan Liu, Dawei Cheng, Yuqi Liang</dc:creator>
    </item>
    <item>
      <title>Analysis of Centrifugal Clutches in Two-Speed Automatic Transmissions with Deep Learning-Based Engagement Prediction</title>
      <link>https://arxiv.org/abs/2409.09755</link>
      <description>arXiv:2409.09755v2 Announce Type: replace-cross 
Abstract: This paper presents a comprehensive numerical analysis of centrifugal clutch systems integrated with a two-speed automatic transmission, a key component in automotive torque transfer. Centrifugal clutches enable torque transmission based on rotational speed without external controls. The study systematically examines various clutch configurations effects on transmission dynamics, focusing on torque transfer, upshifting, and downshifting behaviors under different conditions. A Deep Neural Network (DNN) model predicts clutch engagement using parameters such as spring preload and shoe mass, offering an efficient alternative to complex simulations. The integration of deep learning and numerical modeling provides critical insights for optimizing clutch designs, enhancing transmission performance and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09755v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bo-Yi Lin, Kai Chun Lin</dc:creator>
    </item>
    <item>
      <title>Higher-order-ReLU-KANs (HRKANs) for solving physics-informed neural networks (PINNs) more accurately, robustly and faster</title>
      <link>https://arxiv.org/abs/2409.14248</link>
      <description>arXiv:2409.14248v2 Announce Type: replace-cross 
Abstract: Finding solutions to partial differential equations (PDEs) is an important and essential component in many scientific and engineering discoveries. One of the common approaches empowered by deep learning is Physics-informed Neural Networks (PINNs). Recently, a new type of fundamental neural network model, Kolmogorov-Arnold Networks (KANs), has been proposed as a substitute of Multilayer Perceptions (MLPs), and possesses trainable activation functions. To enhance KANs in fitting accuracy, a modification of KANs, so called ReLU-KANs, using "square of ReLU" as the basis of its activation functions, has been suggested. In this work, we propose another basis of activation functions, namely, Higherorder-ReLU (HR), which is simpler than the basis of activation functions used in KANs, namely, Bsplines; allows efficient KAN matrix operations; and possesses smooth and non-zero higher-order derivatives, essential to physicsinformed neural networks. We name such KANs with Higher-order-ReLU (HR) as their activations, HRKANs. Our detailed experiments on two famous and representative PDEs, namely, the linear Poisson equation and nonlinear Burgers' equation with viscosity, reveal that our proposed Higher-order-ReLU-KANs (HRKANs) achieve the highest fitting accuracy and training robustness and lowest training time significantly among KANs, ReLU-KANs and HRKANs. The codes to replicate our experiments are available at https://github.com/kelvinhkcs/HRKAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14248v2</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <pubDate>Thu, 26 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chi Chiu So, Siu Pang Yung</dc:creator>
    </item>
  </channel>
</rss>
