<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Jul 2025 04:00:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Is Your Model Risk ALARP? Evaluating Prospective Safety-Critical Applications of Complex Models</title>
      <link>https://arxiv.org/abs/2507.10817</link>
      <description>arXiv:2507.10817v1 Announce Type: new 
Abstract: The increasing availability of advanced computational modelling offers new opportunities to improve safety, efficacy, and emissions reductions. Application of complex models to support engineering decisions has been slow in comparison to other sectors, reflecting the higher consequence of unsafe applications. Adopting a complex model introduces a \emph{model risk}, namely the expected consequence of incorrect or otherwise unhelpful outputs. This should be weighed against the prospective benefits that the more sophisticated model can provide, also accounting for the non-zero risk of existing practice. Demonstrating when the model risk of a proposed machine learning application is As Low As Reasonably Practicable (ALARP) can help ensure that safety-critical industries benefit from complex models where appropriate while avoiding their misuse. An example of automated weld radiograph classification is presented to demonstrate how this can be achieved by combining statistical decision analysis, uncertainty quantification, and value of information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10817v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Domenic Di Francesco, Alan Forrest, Fiona McGarry, Nicholas Hall, Adam Sobey</dc:creator>
    </item>
    <item>
      <title>Start from the End: A Framework for Computational Policy Exploration to Inform Effective and Geospatially Consistent Interventions applied to COVID-19 in St. Louis</title>
      <link>https://arxiv.org/abs/2507.10870</link>
      <description>arXiv:2507.10870v1 Announce Type: new 
Abstract: Mathematical models are a powerful tool to study infectious disease dynamics and intervention strategies against them in social systems. However, due to their detailed implementation and steep computational requirements, practitioners and stakeholders are typically only able to explore a small subset of all possible intervention scenarios, a severe limitation when preparing for disease outbreaks. In this work, we propose a parameter exploration framework utilizing emulator models to make uncertainty-aware predictions of high-dimensional parameter spaces and identify large numbers of feasible response strategies. We apply our framework to a case study of a large-scale agent-based disease model of the COVID-19 ``Omicron wave'' in St. Louis, Missouri that took place from December 2021 to February 2022. We identify large numbers of response strategies that would have been estimated to have reduced disease spread by a substantial amount. We also identify policy interventions that would have been able to reduce the geospatial variation in disease spread, which has additional implications for designing thoughtful response strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10870v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David O'Gara, Matt Kasman, Matthew D. Haslam, Ross A. Hammond</dc:creator>
    </item>
    <item>
      <title>Regression modeling for cure factors on uterine cancer data using the reparametrized defective generalized Gompertz distribution</title>
      <link>https://arxiv.org/abs/2507.10902</link>
      <description>arXiv:2507.10902v1 Announce Type: new 
Abstract: Recent advances in medical research have improved survival outcomes for patients with life-threatening diseases. As a result, the existence of long-term survivors from these illnesses is becoming common. However, conventional models in survival analysis assume that all individuals remain at risk of death after the follow-up, disregarding the presence of a cured subpopulation. An important methodological advancement in this context is the use of defective distributions. In the defective models, the survival function converges to a constant value $p \in (0,1)$ as a function of the parameters. Among these models, the defective generalized Gompertz distribution (DGGD) has emerged as a flexible approach. In this work, we introduce a reparametrized version of the DGGD that incorporates the cure parameter and accommodates covariate effects to assess individual-level factors associated with long-term survival. A Bayesian model is presented, with parameter estimation via the Hamiltonian Monte Carlo algorithm. A simulation study demonstrates good asymptotic results of the estimation process under vague prior information. The proposed methodology is applied to a real-world dataset of uterine cancer patients. Our results reveal statistically significant protective effects of surgical intervention, alongside elevated risk associated with age over 50, diagnosis at the metastatic stage, and treatment with chemotherapy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10902v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dionisio Silva Neto, Francisco Louzada Neto, Vera Lucia Tomazella</dc:creator>
    </item>
    <item>
      <title>Evaluating the Predictive Power of Qualifying Performance in Formula One Grand Prix</title>
      <link>https://arxiv.org/abs/2507.10966</link>
      <description>arXiv:2507.10966v1 Announce Type: new 
Abstract: Formula One race weekends are structured around multiple sessions: practices, qualifying, and the Grand Prix itself, each contributing to final race performance. This study analyzes nearly two decades of races, encompassing 7,800 driver-weekend observations, to quantify the predictive value of each session on race outcomes. Expanding on prior research with a larger dataset and a longitudinal perspective, statistical analyses using contingency coefficients and Ordinal Logistic Regression update prior knowledge that qualifying performance is the strongest determinant of final race position, surpassing race start positions and practice results as indicators. Unlike grid positioning at the start of the race, which is susceptible to penalties and external disruptions, qualifying results provide an unbiased measure of driver and car capability. These findings reinforce the strategic emphasis already placed on qualifying, offering an empirical validation of its dominant role in determining race outcomes across varied seasons and eras.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10966v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Weissbock, Shirley Mills</dc:creator>
    </item>
    <item>
      <title>Causal indirect effect of an HIV curative treatment: mediators subject to an assay limit and measurement error</title>
      <link>https://arxiv.org/abs/2507.10984</link>
      <description>arXiv:2507.10984v1 Announce Type: new 
Abstract: Causal mediation analysis decomposes the total effect of a treatment on an outcome into the indirect effect, operating through the mediator, and the direct effect, operating through other pathways. One can estimate only the pure indirect effect/indirect effect relative to no treatment, rather than the total effect by combining a hypothesized treatment effect on the mediator with outcome data without treatment. Furthermore, the mediation formula holds for the pure indirect effect (or the organic indirect effect relative to no treatment) regardless of whether there is an interaction between the treatment and mediator in the outcome model. This methodology holds significant promise in selecting prospective treatments based on their indirect effect for further evaluation in randomized clinical trials.
  We apply this methodology to assess which of two measures of HIV persistence is a more promising target for future HIV curative treatments. We combine a hypothesized treatment effect on two mediators, and outcome data without treatment, to compare the indirect effect of treatments targeting these mediators. Some HIV persistence measurements fall below the assay limit, leading to left-censored mediators. We address this by assuming the outcome model extends to mediators below the assay limit and use maximum likelihood estimation. To address measurement error in the mediators, we adjust our estimates.
  Using data from completed ACTG studies, we estimate the pure or organic indirect effect of potential curative HIV treatments on viral suppression through weeks 4 and 8 after HIV medication interruption, mediated by HIV persistence measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10984v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vindyani Herath, Ronald J. Bosch, Judith J. Lok</dc:creator>
    </item>
    <item>
      <title>Comparative Firm-Level Analysis of ESG and Sentiment Effects on Stock Market Performance: Evidence from 16 Companies Using Retrofitted Sentiment Embeddings</title>
      <link>https://arxiv.org/abs/2507.11485</link>
      <description>arXiv:2507.11485v1 Announce Type: new 
Abstract: This study investigates how emotion-specific sentiment embedded in financial news headlines interacts with firm-level ESG (Environmental, Social, and Governance) ratings to influence stock return behavior. Using data from 16 firms and integrating retrofitted word embeddings with ESG metrics, the analysis evaluates three hypotheses: (1) emotion-specific sentiment independently predicts stock returns; (2) its moderating effect varies across ESG dimensions; and (3) positive (negative) sentiment amplifies (dampens) ESG performance effects.
  The empirical results support Hypotheses 1 and 2. Emotion-laden headlines -- especially those expressing trust, sadness, and anticipation -- significantly correlate with return variability, particularly when aligned with specific ESG dimensions such as Environmental and Social. However, results for Hypothesis 3 are mixed: while sentiment clearly moderates ESG-return relationships, contradictory interactions (e.g., negative sentiment enhancing returns) were more common and exhibited stronger effects than theory-consistent ones.
  Retrofitted word embeddings outperformed the traditional lexicon-based sentiment model (NRC Emotion Lexicon), offering higher explanatory power across interaction models. The study highlights the nuanced and context-dependent nature of ESG-sentiment dynamics, emphasizing the importance of emotional framing in investor behavior and ESG valuation frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11485v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sangdeok Lee</dc:creator>
    </item>
    <item>
      <title>Demographic Distribution Matching between real world and virtual phantom population</title>
      <link>https://arxiv.org/abs/2507.11511</link>
      <description>arXiv:2507.11511v1 Announce Type: new 
Abstract: Virtual imaging trials (VITs) offer scalable and cost-effective tools for evaluating imaging systems and protocols. However, their translational impact depends on rigorous comparability between virtual and real-world populations. This study introduces DISTINCT (Distributional Subsampling for Covariate-Targeted Alignment), a statistical framework for selecting demographically aligned subsamples from large clinical datasets to support robust comparisons with virtual cohorts. We applied DISTINCT to the National Lung Screening Trial (NLST) and a companion virtual trial dataset (VLST). The algorithm jointly aligned typical continuous (age, BMI) and categorical (sex, race, ethnicity) variables by constructing multidimensional bins based on discretized covariates. For a given target size, DISTINCT samples individuals to match the joint demographic distribution of the reference population. We evaluated the demographic similarity between VLST and progressively larger NLST subsamples using Wasserstein and Kolmogorov-Smirnov (K-S) distances to identify the maximal subsample size with acceptable alignment. The algorithm identified a maximal aligned NLST subsample of 9,974 participants, preserving demographic similarity to the VLST population. Receiver operating characteristic (ROC) analysis using risk scores for lung cancer detection showed that area under the curve (AUC) estimates stabilized beyond 6,000 participants, confirming the sufficiency of aligned subsamples for virtual imaging trial evaluation. Stratified AUC analysis revealed substantial performance variation across demographic subgroups, reinforcing the importance of covariate alignment in comparative studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11511v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dhrubajyoti Ghosh, Fakrul Islam Tushar, Lavsen Dahal, Liesbeth Vancoillie, Kyle J. Lafata, Ehsan Samei, Joseph Y. Lo, Sheng Luo</dc:creator>
    </item>
    <item>
      <title>The Shape of Deceit: Behavioral Consistency and Fragility in Money Laundering Patterns</title>
      <link>https://arxiv.org/abs/2507.10608</link>
      <description>arXiv:2507.10608v1 Announce Type: cross 
Abstract: Conventional anti-money laundering (AML) systems predominantly focus on identifying anomalous entities or transactions, flagging them for manual investigation based on statistical deviation or suspicious behavior. This paradigm, however, misconstrues the true nature of money laundering, which is rarely anomalous but often deliberate, repeated, and concealed within consistent behavioral routines. In this paper, we challenge the entity-centric approach and propose a network-theoretic perspective that emphasizes detecting predefined laundering patterns across directed transaction networks. We introduce the notion of behavioral consistency as the core trait of laundering activity, and argue that such patterns are better captured through subgraph structures expressing semantic and functional roles - not solely geometry. Crucially, we explore the concept of pattern fragility: the sensitivity of laundering patterns to small attribute changes and, conversely, their semantic robustness even under drastic topological transformations. We claim that laundering detection should not hinge on statistical outliers, but on preservation of behavioral essence, and propose a reconceptualization of pattern similarity grounded in this insight. This philosophical and practical shift has implications for how AML systems model, scan, and interpret networks in the fight against financial crime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10608v1</guid>
      <category>cs.SI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danny Butvinik, Ofir Yakobi, Michal Einhorn Cohen, Elina Maliarsky</dc:creator>
    </item>
    <item>
      <title>How to rank imputation methods?</title>
      <link>https://arxiv.org/abs/2507.11297</link>
      <description>arXiv:2507.11297v1 Announce Type: cross 
Abstract: Imputation is an attractive tool for dealing with the widespread issue of missing values. Consequently, studying and developing imputation methods has been an active field of research over the last decade. Faced with an imputation task and a large number of methods, how does one find the most suitable imputation? Although model selection in different contexts, such as prediction, has been well studied, this question appears not to have received much attention. In this paper, we follow the concept of Imputation Scores (I-Scores) and develop a new, reliable, and easy-to-implement score to rank missing value imputations for a given data set without access to the complete data. In practice, this is usually done by artificially masking observations to compare imputed to observed values using measures such as the Root Mean Squared Error (RMSE). We discuss how this approach of additionally masking observations can be misleading if not done carefully and that it is generally not valid under MAR. We then identify a new missingness assumption and develop a score that combines a sensible masking of observations with proper scoring rules. As such the ranking is geared towards the imputation that best replicates the distribution of the data, allowing to find imputations that are suitable for a range of downstream tasks. We show the propriety of the score and discuss an estimation algorithm involving energy scores. Finally, we show the efficacy of the new score in simulated data examples, as well as a downstream task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11297v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey N\"af, Krystyna Grzesiak, Erwan Scornet</dc:creator>
    </item>
    <item>
      <title>From Observational Data to Clinical Recommendations: A Causal Framework for Estimating Patient-level Treatment Effects and Learning Policies</title>
      <link>https://arxiv.org/abs/2507.11381</link>
      <description>arXiv:2507.11381v1 Announce Type: cross 
Abstract: We propose a framework for building patient-specific treatment recommendation models, building on the large recent literature on learning patient-level causal models and inspired by the target trial paradigm of Hernan and Robins. We focus on safety and validity, including the crucial issue of causal identification when using observational data. We do not provide a specific model, but rather a way to integrate existing methods and know-how into a practical pipeline. We further provide a real world use-case of treatment optimization for patients with heart failure who develop acute kidney injury during hospitalization. The results suggest our pipeline can improve patient outcomes over the current treatment regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11381v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rom Gutman, Shimon Sheiba, Omer Noy Klien, Naama Dekel Bird, Amit Gruber, Doron Aronson, Oren Caspi, Uri Shalit</dc:creator>
    </item>
    <item>
      <title>Multiscale patterns of migration flows in Austria: regionalization, administrative barriers, and urban-rural divides</title>
      <link>https://arxiv.org/abs/2507.11503</link>
      <description>arXiv:2507.11503v1 Announce Type: cross 
Abstract: Migration is central in various societal problems related to socioeconomic development. While much of the existing research has focused on international migration, migration patterns within a single country remain relatively unexplored. In this work we study internal migration patterns in Austria for a period of over 20 years, obtained from open and high-granularity administrative records. We employ inferential network methods to characterize the flows between municipalities and extract their clustering according to similar target and destination rates. Our methodology reveals significant deviations from commonly assumed relocation patterns modeled by the gravity law. At the same time, we observe unexpected biases of internal migrations that leads to less frequent movements across boundaries at both district and state levels than predictions suggest. This leads to significant regionalization of migration at multiple geographical scales and augmented division between urban and rural areas. These patterns appear to be remarkably persistent across decades of migration data, demonstrating systematic limitations of conventionally used gravity models in migration studies. Our approach presents a robust methodology that can be used to improve such evaluations, and can reveal new phenomena in migration networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11503v1</guid>
      <category>physics.soc-ph</category>
      <category>physics.comp-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Thomas Robiglio, Martina Contisciani, M\'arton Karsai, Tiago P. Peixoto</dc:creator>
    </item>
    <item>
      <title>Bayesian Parameter Inference and Uncertainty Quantification for a Computational Pulmonary Hemodynamics Model Using Gaussian Processes</title>
      <link>https://arxiv.org/abs/2502.14251</link>
      <description>arXiv:2502.14251v2 Announce Type: replace 
Abstract: Subject-specific modeling is a powerful tool in cardiovascular research, providing insights beyond the reach of current clinical diagnostics. Limitations in available clinical data require the incorporation of uncertainty into models to improve guidance for personalized treatments. However, for clinical relevance, such modeling must be computationally efficient. In this study, we used a one-dimensional (1D) fluid dynamics model informed by experimental data from a dog model of chronic thromboembolic pulmonary hypertension (CTEPH), incorporating measurements from multiple subjects under both baseline and CTEPH conditions. Surgical intervention can alleviate CTEPH, yet patients with microvascular disease (e.g., remodeling and narrowing of small vessels) often exhibit persistent pulmonary hypertension, highlighting the importance of assessing microvascular disease severity. Thus, each lung was modeled separately to account for the heterogeneous nature of CTEPH, allowing us to explore lung-specific microvascular narrowing and resistance. We compared inferred parameters between baseline and CTEPH and examined their correlation with clinical markers of disease severity. To accelerate model calibration, we employed Gaussian process (GP) emulators, enabling the estimation of microvascular parameters and their uncertainties within a clinically feasible timeframe. Our results demonstrated that CTEPH leads to heterogeneous microvascular adaptation, reflected in distinct parameter shifts. Notably, the changes in model parameters strongly correlated with disease severity, especially in the lung previously reported to have more advanced disease. This framework provides a rapid, uncertainty-aware method for evaluating microvascular dysfunction in CTEPH and may support more targeted treatment strategies within a timeframe suitable for clinical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14251v2</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <category>physics.bio-ph</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.compbiomed.2025.110552</arxiv:DOI>
      <arxiv:journal_reference>Computers in biology and medicine 2025</arxiv:journal_reference>
      <dc:creator>Amirreza Kachabi, Sofia Altieri Correa, Naomi C. Chesler, Mitchel J. Colebank</dc:creator>
    </item>
    <item>
      <title>Bayesian Joint Modeling for Longitudinal Magnitude Data with Informative Dropout: an Application to Critical Care Data</title>
      <link>https://arxiv.org/abs/2405.19666</link>
      <description>arXiv:2405.19666v2 Announce Type: replace-cross 
Abstract: In various biomedical studies, analysis often focuses on data magnitudes, particularly when algebraic signs are irrelevant or lost. For repeated measures studies involving magnitude outcomes, incorporating random effects is essential as they account for individual heterogeneity, thereby enhancing parameter estimation precision. However, established regression methods specifically designed for magnitude outcomes that incorporate random effects are currently lacking. This article bridges this gap by introducing Bayesian regression modeling approaches for analyzing magnitude data, with a key focus on incorporating random effects. The proposed method is further extended to address multiple causes of informative dropout, a common challenge in repeated measures studies. To tackle this missing data challenge, a joint modeling strategy is developed, building upon the introduced regression techniques. Two numerical simulation studies assess the validity of our method. The chosen simulation scenarios are designed to resemble the conditions of our motivating study. Results demonstrate that the proposed method for magnitude data performs well in terms of estimation accuracy, and the joint models effectively mitigate bias due to missing data. Finally, we apply these models to analyze magnitude data from the motivating study, investigating whether sex impacts the magnitude change in diaphragm thickness over time for ICU patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19666v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wen Teng, Niall D. Ferguson, Ewan C. Goligher, Anna Heath</dc:creator>
    </item>
    <item>
      <title>The GPT Surprise: Offering Large Language Model Chat in a Massive Coding Class Reduced Engagement but Increased Adopters Exam Performances</title>
      <link>https://arxiv.org/abs/2407.09975</link>
      <description>arXiv:2407.09975v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are quickly being adopted in a wide range of learning experiences, especially via ubiquitous and broadly accessible chat interfaces like ChatGPT and Copilot. This type of interface is readily available to students and teachers around the world, yet relatively little research has been done to assess the impact of such generic tools on student learning. Coding education is an interesting test case, both because LLMs have strong performance on coding tasks, and because LLM-powered support tools are rapidly becoming part of the workflow of professional software engineers. To help understand the impact of generic LLM use on coding education, we conducted a large-scale randomized control trial with 5,831 students from 146 countries in an online coding class in which we provided some students with access to a chat interface with GPT-4. We estimate positive benefits on exam performance for adopters, the students who used the tool, but over all students, the advertisement of GPT-4 led to a significant average decrease in exam participation. We observe similar decreases in other forms of course engagement. However, this decrease is modulated by the student's country of origin. Offering access to LLMs to students from low human development index countries increased their exam participation rate on average. Our results suggest there may be promising benefits to using LLMs in an introductory coding class, but also potential harms for engagement, which makes their longer term impact on student success unclear. Our work highlights the need for additional investigations to help understand the potential impact of future adoption and integration of LLMs into classrooms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09975v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Allen Nie, Yash Chandak, Miroslav Suzara, Ali Malik, Juliette Woodrow, Matt Peng, Mehran Sahami, Emma Brunskill, Chris Piech</dc:creator>
    </item>
    <item>
      <title>Forecasting intermittent time series with Gaussian Processes and Tweedie likelihood</title>
      <link>https://arxiv.org/abs/2502.19086</link>
      <description>arXiv:2502.19086v4 Announce Type: replace-cross 
Abstract: We adopt Gaussian Processes (GPs) as latent functions for probabilistic forecasting of intermittent time series. The model is trained in a Bayesian framework that accounts for the uncertainty about the latent function. We couple the latent GP variable with two types of forecast distributions: the negative binomial (NegBinGP) and the Tweedie distribution (TweedieGP). While the negative binomial has already been used in forecasting intermittent time series, this is the first time in which a fully parameterized Tweedie density is used for intermittent time series. We properly evaluate the Tweedie density, which has both a point mass at zero and heavy tails, avoiding simplifying assumptions made in existing models. We test our models on thousands of intermittent count time series. Results show that our models provide consistently better probabilistic forecasts than the competitors. In particular, TweedieGP obtains the best estimates of the highest quantiles, thus showing that it is more flexible than NegBinGP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19086v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 16 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefano Damato, Dario Azzimonti, Giorgio Corani</dc:creator>
    </item>
  </channel>
</rss>
