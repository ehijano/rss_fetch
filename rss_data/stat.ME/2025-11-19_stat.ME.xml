<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Nov 2025 05:02:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On robust Bayesian causal inference</title>
      <link>https://arxiv.org/abs/2511.13895</link>
      <description>arXiv:2511.13895v1 Announce Type: new 
Abstract: This paper develops a Bayesian framework for robust causal inference from longitudinal observational data. Many contemporary methods rely on structural assumptions, such as factor models, to adjust for unobserved confounding, but they can lead to biased causal estimands when mis-specified. We focus on directly estimating time--unit--specific causal effects and use generalised Bayesian inference to quantify model mis-specification and adjust for it, while retaining interpretable posterior inference. We select the learning rate~$\omega$ based on a proper scoring rule that jointly evaluates point and interval accuracy of the causal estimand, thus providing a coherent, decision-theoretic foundation for tuning~$\omega$. Simulation studies and applications to real data demonstrate improved calibration, sharpness, and robustness in estimating causal effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13895v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angelos Alexopoulos, Nikolaos Demiris</dc:creator>
    </item>
    <item>
      <title>State-Space Representation of INGARCH Models and Their Application in Insurance</title>
      <link>https://arxiv.org/abs/2511.14091</link>
      <description>arXiv:2511.14091v1 Announce Type: new 
Abstract: Integer-valued generalized autoregressive conditional heteroskedastic (INGARCH) models are a popular framework for modeling serial dependence in count time-series. While convenient for modeling, prediction, and estimation, INGARCH models lack a clear theoretical justification for the evolution step. This limitation not only makes interpretation difficult and complicates the inclusion of covariates, but can also make the handling of missing data computationally burdensome. Consequently, applying such models in an insurance context, where covariates and missing observations are common, can be challenging. In this paper, we first introduce the marginalized state-space model (M-SSM), defined solely through the marginal distribution of the observations, and show that INGARCH models arise as special cases of this framework. The M-SSM formulation facilitates the natural incorporation of covariates and missing data mechanisms, and this representation in turn provides a coherent way to incorporate these elements within the INGARCH model as well. We then demonstrate that an M-SSM can admit an observation-driven state-space model (O-SSM) representation when suitable assumptions are imposed on the evolution of its conditional mean. This lifting from an M-SSM to an O-SSM provides a natural setting for establishing weak stationarity, even in the presence of heterogeneity and missing observations. The proposed ideas are illustrated through the Poisson and the Negative-Binomial INGARCH(1,1) models, highlighting their applicability in predictive analysis for insurance data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14091v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jae Youn Ahn, Hong Beng Lim, Mario V. W\"uthrich</dc:creator>
    </item>
    <item>
      <title>The Prevalence of Misreporting and Misinterpreting Correlation Coefficients in Biomedical Literature</title>
      <link>https://arxiv.org/abs/2511.14092</link>
      <description>arXiv:2511.14092v1 Announce Type: new 
Abstract: Correlation coefficient is widely used in biomedical and biological literature, yet its frequent misuse and misinterpretation undermine the credibility and reproducibility of the scientific findings. We systematically reviewed 1326 records of correlation analyses across 310 articles published in Science, Nature, and Nature Neuroscience in 2022. Our analysis revealed a troubling pattern of poor statistical reporting and inferring: 58.71% (95% CI: [53.23%, 64.19%], 182/310) of studies did not explicitly report sample sizes, and 98.06% (95% CI: [96.53%, 99.60%], 304/310) failed to provide confidence intervals for correlation coefficients. Among 177 articles inferring correlation strength, 45.25% (95% CI: [38.42%, 53.10%], 81/177) relied solely on point estimates, while 53.63% (95% CI: [46.90%, 61.58%], 96/177) drew conclusions based on null hypothesis significance testing. This widespread omission and misuse highlight a systematic gap in both statistic literacy and editorial standards. We advocate clear reporting guidelines mandating effect sizes and confidence intervals in correlation analyses to enhance the transparency, rigor, and reproducibility of quantitative life sciences research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14092v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiayang Xu, Xintong Chen, Yufeng Liu, Xiaoli Guo, Shanbao Tong</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Covariate-Dependent Discrete Graphical Models and Dynamic Ising Models</title>
      <link>https://arxiv.org/abs/2511.14123</link>
      <description>arXiv:2511.14123v1 Announce Type: new 
Abstract: We propose a covariate-dependent discrete graphical model for capturing dynamic networks among discrete random variables, allowing the dependence structure among vertices to vary with covariates. This discrete dynamic network encompasses the dynamic Ising model as a special case. We formulate a likelihood-based approach for parameter estimation and statistical inference. We achieve efficient parameter estimation in high-dimensional settings through the use of the pseudo-likelihood method. To perform model selection, a birth-and-death Markov chain Monte Carlo algorithm is proposed to explore the model space and select the most suitable model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14123v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lyndsay Roach, Qiong Li, Nanwei Wang, Xin Gao</dc:creator>
    </item>
    <item>
      <title>Covariate Adjustment for the Win Odds: Application to Cardiovascular Outcomes Trials</title>
      <link>https://arxiv.org/abs/2511.14292</link>
      <description>arXiv:2511.14292v1 Announce Type: new 
Abstract: Covariate adjustment can enhance precision and power in clinical trials, yet its application to the win odds remains unclear. The win odds is an extension of the win ratio that includes ties. In their original form, both methods rely on comparing each individual from the treatment group to each individual from the control group in pairwise manner, and count the number of wins, losses, and ties from these pairwise comparisons. A priori, it is not clear how covariate adjustment can be implemented for the win odds. To address this, we establish a connection between the win odds and the marginal probabilistic index, a measure for which covariate adjustment theory is well-developed. Using this connection, we show how covariate adjustment for the win odds is possible, leading to potentially more precise estimators and larger power as compared to the unadjusted win odds. We present the underlying theory for covariate adjustment for the win odds in an accessible way and apply the method on synthetic data based on the CANTOS trial (ClinicalTrials.gov identifier: NCT01327846) characteristics and on simulated data to study the operating characteristics of the method. We observe that there is indeed a potential gain in power when the win odds are adjusted for baseline covariates if the baseline covariates are prognostic for the outcome. This comes at the cost of a slight inflation of the type I error rate for small sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14292v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cyrill Scheidegger, Simon Wandel, Tobias M\"utze</dc:creator>
    </item>
    <item>
      <title>Consistent detection and estimation of multiple structural changes in functional data: unsupervised and supervised approaches</title>
      <link>https://arxiv.org/abs/2511.14353</link>
      <description>arXiv:2511.14353v1 Announce Type: new 
Abstract: We develop algorithms for detecting multiple changepoints in functional data when the number of changepoints is unknown (unsupervised case), when it is specified apriori (supervised case), and when certain bounds are available (semi-supervised case). These algorithms utilize the maximum mean discrepancy (MMD) measure between distributions on Hilbert spaces. We develop an oracle analysis of the changepoint detection problem which reveals an interesting relationship between the true changepoint locations and the local maxima of the oracle MMD curve. The proposed algorithms are shown to detect general distributional changes by exploiting this connection. In the unsupervised case, we test the significance of a potential changepoint and establish its consistency under the single changepoint setting. We investigate the strong consistency of the changepoint estimators in both single and multiple changepoint settings. In both supervised and semi-supervised scenarios, we include a step to merge consecutive groups that are similar to appropriately utilize the prior information about the number of changepoints. In the supervised scenario, the algorithm satisfies an order-preserving property: the estimated changepoints are contained in the true set of changepoints in the underspecified case, while they contain the true set under overspecification. We evaluate the performance of the algorithms on a variety of datasets demonstrating the superiority of the proposed algorithms compared to some of the existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14353v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sourav Chakrabarty, Anirvan Chakraborty, Shyamal K. De</dc:creator>
    </item>
    <item>
      <title>Teaching Longitudinal Linear Mixed Models End-to-End: A Reproducible Case Study in Mouse Body-Weight Growth</title>
      <link>https://arxiv.org/abs/2511.14523</link>
      <description>arXiv:2511.14523v1 Announce Type: new 
Abstract: Background: Linear mixed-effects models are central for analyzing longitudinal continuous data, yet many learners meet them as scattered formulas or software output rather than as a coherent workflow. There is a need for a single, reproducible case study that links questions, model building, diagnostics, and interpretation.
  Methods: We reanalyze a published mouse body-weight experiment with 31 mice in three groups weighed weekly for 12 weeks. After reshaping the data to long format and using profile plots to motivate linear time trends, we fit three random-intercept linear mixed models: a common-slope model, a fully interacted group-by-time model, and a parsimonious model with group-specific intercepts, a shared slope for two groups, and an extra slope for the third. Models are compared using maximum likelihood, AIC, BIC, and likelihood ratio tests, and linear contrasts are used to estimate group differences in weekly means and 12 week gains.
  Results: The parsimonious model fits as well as the fully interacted model and clearly outperforms the common-slope model, revealing small and similar gains in two groups and much steeper growth in the third, with highly significant contrasts for excess weight gain.
  Interpretation: This case study gives a complete, executable workflow for longitudinal linear mixed modeling, from raw data and exploratory plots through model selection, diagnostics, and targeted contrasts. By making explicit the mapping from scientific questions to model terms and estimable contrasts, and by providing R code and a stepwise checklist, it serves as a practical template for teaching and applied work in biostatistics, epidemiology, and related fields</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14523v1</guid>
      <category>stat.ME</category>
      <category>q-bio.OT</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sunday A. Adetunji</dc:creator>
    </item>
    <item>
      <title>A Bayesian INLA-SPDE Approach to Spatio-Temporal Point-Grid Fusion with Change-of-Support and Misaligned Covariates</title>
      <link>https://arxiv.org/abs/2511.14535</link>
      <description>arXiv:2511.14535v1 Announce Type: new 
Abstract: We propose a spatio-temporal data-fusion framework for point data and gridded data with variables observed on different spatial supports. A latent Gaussian field with a Mat\'ern-SPDE prior provides a continuous space representation, while source-specific observation operators map observations to both point measurements and gridded averages, addressing change-of-support and covariate misalignment. Additionally incorporating temporal dependence enables prediction at unknown locations and time points. Inference and prediction are performed using the Integrated Nested Laplace Approximation and the Stochastic Partial Differential Equations approach, which delivers fast computation with uncertainty quantification. Our contributions are: a hierarchical model that jointly fuses multiple data sources of the same variable under different spatial and temporal resolutions and measurement errors, and a practical implementation that incorporates misaligned covariates via the same data fusion framework allowing differing covariate supports. We demonstrate the utility of this framework via simulations calibrated to realistic sensor densities and spatial coverage. Using the simulation framework, we explore the stability and performance of the approach with respect to the number of time points and data/covariate availability, demonstrating gains over single-source models through point and gridded data fusion. We apply our framework to soil moisture mapping in the Elliot Water catchment (Angus, Scotland). We fuse in-situ sensor data with aligned and misaligned covariates, satellite data and elevation data to produce daily high resolution maps with uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14535v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weiyue Zheng, Andrew Elliott, Claire Miller, Marian Scott</dc:creator>
    </item>
    <item>
      <title>PLS-SEM-power: A Shiny App and R package for Computing Required Sample Size and Minimum Detectable Effect Size in PLS-SEMs</title>
      <link>https://arxiv.org/abs/2511.14546</link>
      <description>arXiv:2511.14546v1 Announce Type: new 
Abstract: Despite its evanescent nature, statistical power is crucial for planning Partial Least Squares Structural Equation Modelling (PLS-SEM) studies. This brief paper introduces PLS-SEM-power, a Shiny Application and R package that implements the inverse square root method by Kock and Hadaya (2018) to calculate both the minimum required sample size (a priori analysis) and the Minimum Detectable Effect Size (MDES, sensitivity analysis), given a chosen significance level (alpha level) at 80% power (1 - beta). The application provides an intuitive user interface, facilitating reproducible and easily accessible analyses in diverse research contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14546v1</guid>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alessandro Ansani, Elena Rinallo</dc:creator>
    </item>
    <item>
      <title>Amalgamations in a hierarchy as a way of variable selection in compositional data analysis</title>
      <link>https://arxiv.org/abs/2511.14622</link>
      <description>arXiv:2511.14622v1 Announce Type: new 
Abstract: In certain fields where compositional data are studied, the compositional components, called parts, can be combined into certain subsets, called amalgamations, that are based on domain knowledge. Furthermore, these subsets can form a natural hierarchy of amalgamations subdividing into sub-amalgamations. The authors, a statistician and a biochemist, demonstrate how to create a hierarchy of amalgamations in the context of fatty acid compositions in a sample of marine organisms. Following a tradition in compositional data analysis, these amalgamations are transformed to logratios, and their usefulness as new variables is quantified by the percentage of total logratio variance that they explain. This method is proposed as an alternative method of variable selection in compositional data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14622v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Greenacre, Martin Graeve</dc:creator>
    </item>
    <item>
      <title>Scalable and Efficient Multiple Imputation for Case-Cohort Studies via Influence Function-Based Supersampling</title>
      <link>https://arxiv.org/abs/2511.14692</link>
      <description>arXiv:2511.14692v1 Announce Type: new 
Abstract: Two-phase sampling designs have been widely adopted in epidemiological studies to reduce costs when measuring certain biomarkers is prohibitively expensive. Under these designs, investigators commonly relate survival outcomes to risk factors using the Cox proportional hazards model. To fully utilize covariates collected in phase 1, multiple imputation methods have been developed to impute missing covariates for individuals not included in the phase 2 sample. However, performing multiple imputation on large-scale cohorts can be computationally intensive or even infeasible. To address this issue, Borgan et al. (2023) proposed a random supersampling (RSS) approach that randomly selects a subset of cohort members for imputation, albeit at the cost of reduced efficiency. In this study, we propose an influence function-based supersampling (ISS) approach with weight calibration. The method achieves efficiency comparable to imputing the entire cohort, even with a small supersample, while substantially reducing computational burden. We further demonstrate that the proposed method is particularly advantageous when estimating hazard ratios for high-dimensional expensive biomarkers. Extensive simulation studies are conducted, and a real data application using the National Institutes of Health-American Association of Retired Persons (NIH-AARP) Diet and Health Study is provided to illustrate the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14692v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jooho Kim, Yei Eun Shin</dc:creator>
    </item>
    <item>
      <title>CafeMed: Causal Attention Fusion Enhanced Medication Recommendation</title>
      <link>https://arxiv.org/abs/2511.14064</link>
      <description>arXiv:2511.14064v1 Announce Type: cross 
Abstract: Medication recommendation systems play a crucial role in assisting clinicians with personalized treatment decisions. While existing approaches have made significant progress in learning medication representations, they suffer from two fundamental limitations: (i) treating medical entities as independent features without modeling their synergistic effects on medication selection; (ii) employing static causal relationships that fail to adapt to patient-specific contexts and health states. To address these challenges, we propose CafeMed, a framework that integrates dynamic causal reasoning with cross-modal attention for safe and accurate medication recommendation. CafeMed introduces two key components: the Causal Weight Generator (CWG) that transforms static causal effects into dynamic modulation weights based on individual patient states, and the Channel Harmonized Attention Refinement Module (CHARM) that captures complex interdependencies between diagnoses and procedures. This design enables CafeMed to model how different medical conditions jointly influence treatment decisions while maintaining medication safety constraints. Extensive experiments on MIMIC-III and MIMIC-IV datasets demonstrate that CafeMed significantly outperforms state-of-the-art baselines, achieving superior accuracy in medication prediction while maintaining the lower drug--drug interaction rates. Our results indicate that incorporating dynamic causal relationships and cross-modal synergies leads to more clinically-aligned and personalized medication recommendations. Our code is released publicly at https://github.com/rkl71/CafeMed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14064v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kelin Ren, Chan-Yang Ju, Dong-Ho Lee</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of conditional probability distributions using a generative approach based on conditional push-forward neural networks</title>
      <link>https://arxiv.org/abs/2511.14455</link>
      <description>arXiv:2511.14455v1 Announce Type: cross 
Abstract: We introduce conditional push-forward neural networks (CPFN), a generative framework for conditional distribution estimation. Instead of directly modeling the conditional density $f_{Y|X}$, CPFN learns a stochastic map $\varphi=\varphi(x,u)$ such that $\varphi(x,U)$ and $Y|X=x$ follow approximately the same law, with $U$ a suitable random vector of pre-defined latent variables. This enables efficient conditional sampling and straightforward estimation of conditional statistics through Monte Carlo methods. The model is trained via an objective function derived from a Kullback-Leibler formulation, without requiring invertibility or adversarial training. We establish a near-asymptotic consistency result and demonstrate experimentally that CPFN can achieve performance competitive with, or even superior to, state-of-the-art methods, including kernel estimators, tree-based algorithms, and popular deep learning techniques, all while remaining lightweight and easy to train.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14455v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicola Rares Franco, Lorenzo Tedesco</dc:creator>
    </item>
    <item>
      <title>DeepBlip: Estimating Conditional Average Treatment Effects Over Time</title>
      <link>https://arxiv.org/abs/2511.14545</link>
      <description>arXiv:2511.14545v1 Announce Type: cross 
Abstract: Structural nested mean models (SNMMs) are a principled approach to estimate the treatment effects over time. A particular strength of SNMMs is to break the joint effect of treatment sequences over time into localized, time-specific ``blip effects''. This decomposition promotes interpretability through the incremental effects and enables the efficient offline evaluation of optimal treatment policies without re-computation. However, neural frameworks for SNMMs are lacking, as their inherently sequential g-estimation scheme prevents end-to-end, gradient-based training. Here, we propose DeepBlip, the first neural framework for SNMMs, which overcomes this limitation with a novel double optimization trick to enable simultaneous learning of all blip functions. Our DeepBlip seamlessly integrates sequential neural networks like LSTMs or transformers to capture complex temporal dependencies. By design, our method correctly adjusts for time-varying confounding to produce unbiased estimates, and its Neyman-orthogonal loss function ensures robustness to nuisance model misspecification. Finally, we evaluate our DeepBlip across various clinical datasets, where it achieves state-of-the-art performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14545v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haorui Ma, Dennis Frauen, Stefan Feuerriegel</dc:creator>
    </item>
    <item>
      <title>Nonparametric Uniform Inference in Binary Classification and Policy Values</title>
      <link>https://arxiv.org/abs/2511.14700</link>
      <description>arXiv:2511.14700v1 Announce Type: cross 
Abstract: We develop methods for nonparametric uniform inference in cost-sensitive binary classification, a framework that encompasses maximum score estimation, predicting utility maximizing actions, and policy learning. These problems are well known for slow convergence rates and non-standard limiting behavior, even under point identified parametric frameworks. In nonparametric settings, they may further suffer from failures of identification. To address these challenges, we introduce a strictly convex surrogate loss that point-identifies a representative nonparametric policy function. We then estimate this surrogate policy to conduct inference on both the optimal classification policy and the optimal policy value. This approach enables Gaussian inference, substantially simplifying empirical implementation relative to working directly with the original classification problem. In particular, we establish root-$n$ asymptotic normality for the optimal policy value and derive a Gaussian approximation for the optimal classification policy at the standard nonparametric rate. Extensive simulation studies corroborate the theoretical findings. We apply our method to the National JTPA Study to conduct inference on the optimal treatment assignment policy and its associated welfare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14700v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Nan Liu andYanbo Liu, Yuya Sasaki, Yuanyuan Wan</dc:creator>
    </item>
    <item>
      <title>Distribution-Free Control Charts Based on Runs and Patterns</title>
      <link>https://arxiv.org/abs/1801.06532</link>
      <description>arXiv:1801.06532v2 Announce Type: replace 
Abstract: We propose distribution-free runs-based control charts for detecting location shifts. Using the fact that given the number of total successes, the outcomes of a sequence of Bernoulli trials are random permutations, we are able to control the conditional probability of a signal detected at current time given that there is not alarm before at a pre-determined level. This leads to a desired in-control average run length and data-dependent control limits. Two common runs statistics, the longest run statistic and the scan statitsic, are studied in detail and their exact conditional distributions given the number of total successes are obtained using the finite Markov chain imbedding technique. Numerical results are given to evaluate the performance of the proposed control charts.</description>
      <guid isPermaLink="false">oai:arXiv.org:1801.06532v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tung-Lung Wu</dc:creator>
    </item>
    <item>
      <title>Covariate Adjustment in Randomized Experiments Motivated by Higher-Order Influence Functions</title>
      <link>https://arxiv.org/abs/2411.08491</link>
      <description>arXiv:2411.08491v4 Announce Type: replace 
Abstract: Higher-Order Influence Functions (HOIF), developed in a series of papers over the past twenty years, are a fundamental theoretical device for constructing rate-optimal causal-effect estimators from observational studies. However, the value of HOIF for analyzing well-conducted randomized controlled trials (RCT) has not been explicitly explored. In the recent U.S. Food and Drug Administration and European Medicines Agency guidelines on the practice of covariate adjustment in analyzing RCT, in addition to the simple, unadjusted difference-in-mean estimator, it was also recommended to report the estimator adjusting for baseline covariates via a simple parametric working model, such as a linear model. However, when the number of baseline covariates $p$ is large, the recommendation is somewhat murky. In this paper, we show that HOIF-motivated estimators for the treatment-specific mean have significantly improved statistical properties compared to popular adjusted estimators in practice when $p$ is relatively large relative to the sample size $n$. We also characterize the conditions under which the HOIF-motivated estimator improves upon the unadjusted one. More importantly, we demonstrate that several state-of-the-art adjusted estimators proposed recently can be interpreted as particular HOIF-motivated estimators, thereby placing these estimators in a more unified framework. Numerical and empirical studies are conducted to corroborate our theoretical findings. An accompanying R package can be found on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08491v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sihui Zhao, Xinbo Wang, Lin Liu, Xin Zhang</dc:creator>
    </item>
    <item>
      <title>An Instrumental Variables Framework to Unite Spatial Confounding Methods</title>
      <link>https://arxiv.org/abs/2411.10381</link>
      <description>arXiv:2411.10381v3 Announce Type: replace 
Abstract: Studies investigating the causal effects of spatially varying exposures on outcomes often rely on observational and spatially indexed data. A prevalent challenge is unmeasured spatial confounding, where an unobserved spatially varying variable affects both exposure and outcome, leading to biased estimates and invalid confidence intervals. There is a very large literature on spatial statistics that attempts to address unmeasured spatial confounding bias; most of this literature is not framed in the context of causal inference and relies on strict assumptions. In this paper, we propose an instrumental variables (IV) framework that unifies and extends existing methods for addressing unmeasured spatial confounding bias. This framework reveals that many spatial confounding methods can be viewed as IV methods, in which small-scale spatial variation in exposure operates as the instrument, providing a common theoretical foundation for approaches that previously appeared distinct. The framework clarifies that these methods share a common set of assumptions and differ primarily in how small-scale spatial variation is defined, while offering a general strategy for constructing instruments. It also extends to identify and estimate a broad class of causal effects, including the exposure response curve, without requiring a linear outcome model. We apply our methodology in simulation and to a national data set of 33,255 zip codes to estimate the effect of enforcing air pollution exposure levels below 6-12 $\mu g/m^3$ on all-cause mortality while adjusting for unmeasured spatial confounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10381v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sophie M. Woodward, Mauricio Tec, Francesca Dominici</dc:creator>
    </item>
    <item>
      <title>Multimodal Distributions for Circular Axial Data</title>
      <link>https://arxiv.org/abs/2504.04681</link>
      <description>arXiv:2504.04681v2 Announce Type: replace 
Abstract: The family of circular distributions based on non-negative trigonometric sums (NNTS), developed by Fern\'andez-Dur\'an (2004), is highly flexible for modeling datasets exhibiting multimodality and/or skewness. In this article, we extend the NNTS family to axial data by identifying conditions under which the original NNTS family is suitable for modeling undirected vectors. Since the estimation is performed using maximum likelihood, likelihood ratio tests are developed for characteristics of the density function such as uniformity and symmetry, as well as to compare different axial populations through homogeneity tests. The proposed methodology is applied to real datasets involving orientations of rocks, animals, and plants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04681v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator> Fern\'andez-Dur\'an, J. J.,  Gregorio-Dom\'inguez, M. M</dc:creator>
    </item>
    <item>
      <title>Bayesian information theoretic model-averaging stochastic item selection for computer adaptive testing</title>
      <link>https://arxiv.org/abs/2504.15543</link>
      <description>arXiv:2504.15543v2 Announce Type: replace 
Abstract: Computer Adaptive Testing (CAT) aims to accurately estimate an individual's ability using only a subset of an Item Response Theory (IRT) instrument. For many applications of CAT, one also needs to ensure diverse item exposure across different testing sessions, preventing any single item from being over or underutilized. In CAT, items are selected sequentially based on a running estimate of a respondent's ability. Prior methods almost universally see item selection through an optimization lens, motivating greedy item selection procedures. While efficient, these deterministic methods tend to have poor item exposure. Existing stochastic methods for item selection are ad-hoc, where item sampling weights lack theoretical justification. In this manuscript, we formulate stochastic CAT as a Bayesian model averaging problem. We seek item sampling probabilities, treated in the long run frequentist sense, that perform optimal model averaging for the ability estimate in a Bayesian sense. In doing so we derive a cross-entropy information criterion that yields optimal stochastic mixing. We tested our new method on the eight independent IRT models that comprise the Work Disability Functional Assessment Battery, comparing it to prior art. We found that our stochastic methodology had superior item exposure while not compromising in terms of test accuracy and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15543v2</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tina Su, Edison Choe, Joshua C. Chang</dc:creator>
    </item>
    <item>
      <title>Post-treatment problems: What can we say about the effect of a treatment among sub-groups who (would) respond in some way?</title>
      <link>https://arxiv.org/abs/2505.06754</link>
      <description>arXiv:2505.06754v3 Announce Type: replace 
Abstract: Investigators are often interested in how a treatment affects an outcome for units responding to treatment in a certain way. We may wish to know the effect among units that, for example, meaningfully implemented an intervention, passed an attention check, or demonstrated some important mechanistic response. Simply conditioning on the observed value of the post-treatment variable introduces problematic biases. Further, the identification assumptions required of several existing strategies are often indefensible. We propose the Treatment Reactive Average Causal Effect (TRACE), which we define as the total effect of treatment in the group that, if treated, would realize a particular value of the relevant post-treatment variable. By reasoning about the effect among the "non-reactive" group, we can identify and estimate the range of plausible values for the TRACE. We demonstrate the use of this approach with three examples: (i) learning the effect of police-perceived race on police violence during traffic stops, a case where point identification may be possible; (ii) estimating effects of a community-policing intervention in Liberia, in communities that meaningfully implemented it, and (iii) studying how in-person canvassing affects support for transgender rights, among participants for whom the intervention would result in more positive feelings towards transgender people.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06754v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chad Hazlett, Nina McMurry, Tanvi Shinkre</dc:creator>
    </item>
    <item>
      <title>Robust normality transformation for outlier detection in diverse distributions, with application to functional neuroimaging data</title>
      <link>https://arxiv.org/abs/2505.11806</link>
      <description>arXiv:2505.11806v2 Announce Type: replace 
Abstract: Automatic detection of statistical outliers is facilitated through knowledge of the source distribution of regular observations. Since the population distribution is often unknown in practice, one approach is to apply a transformation to Normality. However, the efficacy of transformation is hindered by the presence of outliers, which can have an outsized influence on transformation parameter(s) and lead to masking of outliers post-transformation. Robust Box-Cox and Yeo-Johnson transformations have been proposed but those transformations are only equipped to deal with skew. Here, we develop a novel robust method for transformation to Normality based on the highly flexible sinh-arcsinh (SHASH) family of distributions, which can accommodate skew, non-Gaussian tail weights, and combinations of both. A critical step is initializing outliers, given their potential influence on the highly flexible SHASH transformation. To this end, we consider conventional robust z-scoring and a novel anomaly detection approach. Through extensive simulation studies and real data analyses representing a wide variety of distribution shapes, we find that SHASH transformation outperforms existing methods, exhibiting high sensitivity to outliers even at heavy contamination levels (20-30\%). We illustrate the utility of SHASH transformation-based outlier detection in the context of noise reduction in functional neuroimaging data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11806v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saranjeet Singh Saluja, Fatma Parlak, Amanda Mejia</dc:creator>
    </item>
    <item>
      <title>Efficient Gibbs Sampling in Cox Regression Models Using Composite Partial Likelihood and P\'olya-Gamma Augmentation</title>
      <link>https://arxiv.org/abs/2506.04675</link>
      <description>arXiv:2506.04675v2 Announce Type: replace 
Abstract: The Cox regression models and their Bayesian extensions are widely used for time-to-event analysis. However, standard Bayesian approaches typically require baseline hazard modeling, and their full conditional distributions lack closed-form expressions, resulting in computational inefficiency and increased vulnerability to bias from baseline hazard misspecification. To address these issues, we propose GS4Cox, a fully Gibbs sampler for Bayesian Cox regression models with four elements: (i) generalized Bayesian framework for avoiding baseline hazard specification, (ii) composite partial likelihood and (iii) P\'olya-Gamma augmentation for closed-form expressions of full conditional distributions, and (iv) affine posterior calibration via the open-faced sandwich adjustment for location and scale adjustment of the posterior distribution. We prove asymptotic unbiasedness of the generalized Bayes estimator under composite partial likelihood and propose an affine posterior transformation that yields higher-order asymptotic agreement with the maximum partial likelihood estimator, while the posterior covariance matches the asymptotic target covariance. We demonstrated that GS4Cox consistently outperformed existing sampling methods through numerical and real-data experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04675v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shu Tamano, Yui Tomo</dc:creator>
    </item>
    <item>
      <title>A Two-step Estimating Approach for Heavy-tailed AR Models with Non-zero Median GARCH-type Noises</title>
      <link>https://arxiv.org/abs/2506.11509</link>
      <description>arXiv:2506.11509v2 Announce Type: replace 
Abstract: This paper develops a novel two-step estimating procedure for heavy-tailed AR models with non-zero median GARCH-type noises, allowing for time-varying volatility. We first establish the self-weighted quantile regression estimator (SQE) across all quantile levels $\tau\in (0,1)$ for the AR parameters $\theta_{0}$. We show that the SQE, less a bias, converges weakly to a Gaussian process at a rate of $n^{-1/2}$. The bias is zero if and only if $\tau$ equals $\tau_{0}$, the probability that the noise is less than zero. Based on the SQE, we propose an approach to estimate $\tau_{0}$ in the second step and {feed the estimated $\tau_0$ back into the SQE to estimate $\theta_0$.} Both the estimated $\tau_{0}$ and $\theta_{0}$ are shown to be consistent and asymptotically normal. A random weighting bootstrap method is developed to approximate the complex distribution. The problem we study is non-standard because $\tau_{0}$ may not be identifiable in conventional quantile regression, and the usual methods cannot verify the existence of the SQE bias. Unlike existing procedures for heavy-tailed time series, our method does not require prior information about the symmetry, tail index, or the parametric form of the noise, nor does it require classical identification conditions, such as zero-mean or zero-median.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11509v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui She, Linlin Dai, Shiqing Ling</dc:creator>
    </item>
    <item>
      <title>Link prediction in ecological networks under extreme taxonomic bias</title>
      <link>https://arxiv.org/abs/2506.23370</link>
      <description>arXiv:2506.23370v2 Announce Type: replace 
Abstract: Ecological networks offer powerful insights into community function, but without first characterizing these networks accurately, our ability to detect and interpret changes under environmental stress is limited. We develop an approach to reduce bias in link prediction in the common scenario in which data are derived from studies focused on a small number of species. Our Extended Covariate-Informed Link Prediction (COIL+) framework employs a latent factor model that flexibly borrows information across species, incorporates species traits and phylogeny, and leverages information from multiple studies to address uncertainty in species occurrence. We also propose a trait-matching procedure that allows heterogeneity in species-level trait-interaction associations. We illustrate the approach with a literature-based dataset of 268 sources reporting Afrotropical frugivory and compare performance with and without correction for occurrence uncertainty. COIL+ substantially improves link prediction and reduces sampling bias, revealing 5637 likely but unobserved frugivory interactions (a median of nine additional interactions per frugivore). Newly predicted interactions are concentrated among poorly sampled frugivores, such as the water chevrotain (Hyemoschus aquaticus, a small forest-dwelling ungulate) and the rufous-bellied helmetshrike (Prionops rufiventris, a passerine bird of East African tropical forests). Additionally, the method improves model discrimination compared to existing methods under strong taxonomic bias and narrow study focus. This framework generalizes to diverse network contexts and provides a useful tool for link prediction in the face of biased interaction data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23370v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jennifer N. Kampe, Camille M. M. DeSisto, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Geometric Decomposition of Statistical Inference through Gradient Flow and Co-Monotonicity Measures</title>
      <link>https://arxiv.org/abs/2511.04599</link>
      <description>arXiv:2511.04599v3 Announce Type: replace 
Abstract: Understanding feature-outcome associations in high-dimensional data remains
  challenging when relationships vary across subpopulations, yet standard
  methods assuming global associations miss context-dependent patterns, reducing
  statistical power and interpretability. We develop a geometric decomposition
  framework offering two strategies for partitioning inference problems into
  regional analyses on data-derived Riemannian graphs. Gradient flow
  decomposition uses path-monotonicity-validated discrete Morse theory to
  partition samples into gradient flow cells where outcomes exhibit monotonic
  behavior. Co-monotonicity decomposition leverages association structure:
  vertex-level coefficients measuring directional concordance between outcome
  and features, or between feature pairs, define embeddings of samples into
  association space. These embeddings induce Riemannian k-NN graphs on which
  biclustering identifies co-monotonicity cells (coherent regions) and feature
  modules. This extends naturally to multi-modal integration across multiple
  feature sets. Both strategies apply independently or jointly, with Bayesian
  posterior sampling providing credible intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04599v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pawel Gajer, Jacques Ravel</dc:creator>
    </item>
    <item>
      <title>Conditioning on posterior samples for flexible frequentist goodness-of-fit testing</title>
      <link>https://arxiv.org/abs/2511.05281</link>
      <description>arXiv:2511.05281v2 Announce Type: replace 
Abstract: Tests of goodness of fit are used in nearly every domain where statistics is applied. One powerful and flexible approach is to sample artificial data sets that are exchangeable with the real data under the null hypothesis (but not under the alternative), as this allows the analyst to conduct a valid test using any test statistic they desire. Such sampling is typically done by conditioning on either an exact or approximate sufficient statistic, but existing methods for doing so have significant limitations, which either preclude their use or substantially reduce their power or computational tractability for many important models. In this paper, we propose to condition on samples from a Bayesian posterior distribution, which constitute a very different type of approximate sufficient statistic than those considered in prior work. Our approach, approximately co-sufficient sampling via Bayes (aCSS-B), considerably expands the scope of this flexible type of goodness-of-fit testing. We prove the approximate validity of the resulting test, and demonstrate its utility on three common null models where no existing methods apply, as well as its outperformance on models where existing methods do apply.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05281v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ritwik Bhaduri, Aabesh Bhattacharyya, Rina Foygel Barber, Lucas Janson</dc:creator>
    </item>
    <item>
      <title>Zeroes and Extrema of Functions via Random Measures</title>
      <link>https://arxiv.org/abs/2511.10293</link>
      <description>arXiv:2511.10293v2 Announce Type: replace 
Abstract: We present methods that provide all zeroes and extrema of a function that do not require differentiation. Using point process theory, we are able to describe the locations of zeroes or maxima, their number, as well as their distribution over a given window of observation. The algorithms in order to accomplish the theoretical development are also provided, and they are exemplified using many illustrative examples, for real and complex functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.10293v2</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Athanasios Christou Micheas</dc:creator>
    </item>
    <item>
      <title>Graphical Model-based Inference on Persistent Homology</title>
      <link>https://arxiv.org/abs/2511.11996</link>
      <description>arXiv:2511.11996v2 Announce Type: replace 
Abstract: Persistent homology is a cornerstone of topological data analysis, offering a multiscale summary of topology with robustness to nuisance transformations, such as rotations and small deformations. Persistent homology has seen broad use across domains such as computer vision and neuroscience. Most statistical treatments, however, use homology primarily as a feature extractor, relying on statistical distance-based tests or simple time-to-event models for inferential tasks. While these approaches can detect global differences, they rarely localize the source of those differences. We address this gap by taking a graphical model-based approach: we associate each vertex with a population latent position in a conic space and model each bar's key events (birth and death times) using an exponential distribution, whose rate is a transformation of the latent positions according to an event occurring on the graph. The low-dimensional bars have simple graph-event representations, such as the formation of a minimum spanning tree or the triangulation of a loop, and thus enjoy tractable likelihoods. Taking a Bayesian approach, we infer latent positions and enable model extensions such as hierarchical models that allow borrowing strength across groups. Applications to a neuroimaging study of Alzheimer's disease demonstrate that our method localizes sources of difference and provides interpretable, model-based analyses of topological structure in complex data. The code is provided and maintained at https://github.com/zitian-wu/graphPH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11996v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zitian Wu, Arkaprava Roy, Leo L. Duan</dc:creator>
    </item>
    <item>
      <title>Transfer learning for high-dimensional Factor-augmented sparse linear model</title>
      <link>https://arxiv.org/abs/2511.12435</link>
      <description>arXiv:2511.12435v2 Announce Type: replace 
Abstract: In this paper, we study transfer learning for high-dimensional factor-augmented sparse linear models, motivated by applications in economics and finance where strongly correlated predictors and latent factor structures pose major challenges for reliable estimation. Our framework simultaneously mitigates the impact of high correlation and removes the additional contributions of latent factors, thereby reducing potential model misspecification in conventional linear modeling. In such settings, the target dataset is often limited, but multiple heterogeneous auxiliary sources may provide additional information. We develop transfer learning procedures that effectively leverage these auxiliary datasets to improve estimation accuracy, and establish non-asymptotic $\ell_1$- and $\ell_2$-error bounds for the proposed estimators. To prevent negative transfer, we introduce a data-driven source detection algorithm capable of identifying informative auxiliary datasets and prove its consistency. In addition, we provide a hypothesis testing framework for assessing the adequacy of the factor model, together with a procedure for constructing simultaneous confidence intervals for the regression coefficients of interest. Numerical studies demonstrate that our methods achieve substantial gains in estimation accuracy and remain robust under heterogeneity across datasets. Overall, our framework offers a theoretical foundation and a practically scalable solution for incorporating heterogeneous auxiliary information in settings with highly correlated features and latent factor structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12435v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bo Fu, Dandan Jiang</dc:creator>
    </item>
    <item>
      <title>On the Identifying Power of Generalized Monotonicity for Average Treatment Effects</title>
      <link>https://arxiv.org/abs/2405.14104</link>
      <description>arXiv:2405.14104v4 Announce Type: replace-cross 
Abstract: In the context of a binary outcome, treatment, and instrument, Balke and Pearl (1993, 1997) es- tablish that the monotonicity condition of Imbens and Angrist (1994) has no identifying power beyond instrument exogeneity for average potential outcomes and average treatment effects in the sense that adding it to instrument exogeneity does not decrease the identified sets for those parameters whenever those restrictions are consistent with the distribution of the observable data. This paper shows that this phenomenon holds in a broader setting with a multi-valued outcome, treatment, and instrument, under an extension of the monotonicity condition that we refer to as generalized monotonicity. We further show that this phenomenon holds for any restriction on treatment response that is stronger than generalized monotonicity provided that these stronger restrictions do not restrict potential outcomes. Importantly, many models of potential treatments previously considered in the literature imply generalized monotonic- ity, including the types of monotonicity restrictions considered by Kline and Walters (2016), Kirkeboen et al. (2016), and Heckman and Pinto (2018), and the restriction that treatment selection is determined by particular classes of additive random utility models. We show through a series of examples that restrictions on potential treatments can provide identifying power beyond instrument exogeneity for av- erage potential outcomes and average treatment effects when the restrictions imply that the generalized monotonicity condition is violated. In this way, our results shed light on the types of restrictions required for help in identifying average potential outcomes and average treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14104v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuehao Bai, Shunzhuang Huang, Sarah Moon, Azeem M. Shaikh, Edward J. Vytlacil</dc:creator>
    </item>
    <item>
      <title>Towards a Unified Theory for Semiparametric Data Fusion with Individual-Level Data</title>
      <link>https://arxiv.org/abs/2409.09973</link>
      <description>arXiv:2409.09973v3 Announce Type: replace-cross 
Abstract: We address the goal of conducting inference about a smooth finite-dimensional parameter by utilizing individual-level data from various independent sources. Recent advancements have led to the development of a comprehensive theory capable of handling scenarios where different data sources align with, possibly distinct subsets of, conditional distributions of a single factorization of the joint target distribution. While this theory proves effective in many significant contexts, it falls short in certain common data fusion problems, such as two-sample instrumental variable analysis, settings that integrate data from epidemiological studies with diverse designs (e.g., prospective cohorts and retrospective case-control studies), and studies with variables prone to measurement error that are supplemented by validation studies. In this paper, we extend the aforementioned comprehensive theory to allow for the fusion of individual-level data from sources aligned with conditional distributions that do not correspond to a single factorization of the target distribution. Assuming conditional and marginal distribution alignments, we provide universal results that characterize the class of all influence functions of regular asymptotically linear estimators and the efficient influence function of any pathwise differentiable parameter, irrespective of the number of data sources, the specific parameter of interest, or the statistical model for the target distribution. This theory paves the way for machine-learning debiased, semiparametric efficient estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09973v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ellen Graham (University of Washington), Marco Carone (University of Washington), Andrea Rotnitzky (University of Washington)</dc:creator>
    </item>
    <item>
      <title>How should we aggregate ratings? Accounting for personal rating scales via Wasserstein barycenters</title>
      <link>https://arxiv.org/abs/2410.00865</link>
      <description>arXiv:2410.00865v2 Announce Type: replace-cross 
Abstract: A common method of comparing items is to collect numerical ratings on a linear scale and compare the average rating for each item. However, averaging ratings does not account for people rating according to differing personal rating scales. With this in mind, we investigate the problem of calculating aggregate numerical ratings from individual numerical ratings and propose a new, non-parametric model for the problem. We show that, with minimal modeling assumptions, the standard average is inconsistent for estimating the quality of items. Analyzing the problem of heterogeneous personal rating scales from the perspective of optimal transport, we derive an alternative rating estimator, which we show is asymptotically consistent almost surely and in L^p for estimating quality, with an optimal rate of convergence. Further, we generalize Kendall's W, a non-parametric coefficient of preference concordance between raters, from the special case of rankings to the more general case of arbitrary numerical ratings. Along the way, we prove Glivenko--Cantelli-type theorems for uniform convergence of the cumulative distribution functions and quantile functions for Wasserstein-2 barycenters on [0,1].</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00865v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Raban</dc:creator>
    </item>
    <item>
      <title>Gradient descent inference in empirical risk minimization</title>
      <link>https://arxiv.org/abs/2412.09498</link>
      <description>arXiv:2412.09498v3 Announce Type: replace-cross 
Abstract: Gradient descent is one of the most widely used iterative algorithms in modern statistical learning. However, its precise algorithmic dynamics in high-dimensional settings remain only partially understood, which has limited its broader potential for statistical inference applications.
  This paper provides a precise, non-asymptotic joint distributional characterization of gradient descent iterates and their debiased statistics in a broad class of empirical risk minimization problems, in the so-called mean-field regime where the sample size is proportional to the signal dimension. Our non-asymptotic state evolution theory holds for both general non-convex loss functions and non-Gaussian data, and reveals the central role of two Onsager correction matrices that precisely characterize the non-trivial dependence among all gradient descent iterates in the mean-field regime.
  Leveraging the joint state evolution characterization, we show that the gradient descent iterate retrieves approximate normality after a debiasing correction via a linear combination of all past iterates, where the debiasing coefficients can be estimated by the proposed gradient descent inference algorithm. This leads to a new algorithmic statistical inference framework based on debiased gradient descent, which (i) applies to a broad class of models with both convex and non-convex losses, (ii) remains valid at each iteration without requiring algorithmic convergence, and (iii) exhibits a certain robustness to possible model misspecification. As a by-product, our framework also provides algorithmic estimates of the generalization error at each iteration. As canonical examples, we demonstrate our theory and inference methods in the single-index regression model and a generalized logistic regression model, where the natural loss functions may exhibit arbitrarily non-convex landscapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09498v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiyang Han, Xiaocong Xu</dc:creator>
    </item>
    <item>
      <title>The purpose of an estimator is what it does: Misspecification, estimands, and over-identification</title>
      <link>https://arxiv.org/abs/2508.13076</link>
      <description>arXiv:2508.13076v4 Announce Type: replace-cross 
Abstract: In over-identified models, misspecification -- the norm rather than exception -- fundamentally changes what estimators estimate. Different estimators imply different estimands rather than different efficiency for the same target. A review of recent applications of generalized method of moments in the American Economic Review suggests widespread acceptance of this fact: There is little formal specification testing and widespread use of estimators that would be inefficient were the model correct, including the use of "hand-selected" moments and weighting matrices. Motivated by these observations, we review and synthesize recent results on estimation under model misspecification, providing guidelines for transparent and robust empirical research. We also provide a new theoretical result, showing that Hansen's J-statistic measures, asymptotically, the range of estimates achievable at a given standard error. Given the widespread use of inefficient estimators and the resulting researcher degrees of freedom, we thus particularly recommend the broader reporting of J-statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13076v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaiah Andrews, Jiafeng Chen, Otavio Tecchio</dc:creator>
    </item>
  </channel>
</rss>
