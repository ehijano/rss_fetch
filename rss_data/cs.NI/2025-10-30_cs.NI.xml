<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 30 Oct 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Deep Reinforcement Learning Approach to QoSAware Load Balancing in 5G Cellular Networks under User Mobility and Observation Uncertainty</title>
      <link>https://arxiv.org/abs/2510.24869</link>
      <description>arXiv:2510.24869v1 Announce Type: new 
Abstract: Efficient mobility management and load balancing are critical to sustaining Quality of Service (QoS) in dense, highly dynamic 5G radio access networks. We present a deep reinforcement learning framework based on Proximal Policy Optimization (PPO) for autonomous, QoS-aware load balancing implemented end-to-end in a lightweight, pure-Python simulation environment. The control problem is formulated as a Markov Decision Process in which the agent periodically adjusts Cell Individual Offset (CIO) values to steer user-cell associations. A multi-objective reward captures key performance indicators (aggregate throughput, latency, jitter, packet loss rate, Jain's fairness index, and handover count), so the learned policy explicitly balances efficiency and stability under user mobility and noisy observations. The PPO agent uses an actor-critic neural network trained from trajectories generated by the Python simulator with configurable mobility (e.g., Gauss-Markov) and stochastic measurement noise. Across 500+ training episodes and stress tests with increasing user density, the PPO policy consistently improves KPI trends (higher throughput and fairness, lower delay, jitter, packet loss, and handovers) and exhibits rapid, stable convergence. Comparative evaluations show that PPO outperforms rule-based ReBuHa and A3 as well as the learning-based CDQL baseline across all KPIs while maintaining smoother learning dynamics and stronger generalization as load increases. These results indicate that PPO's clipped policy updates and advantage-based training yield robust, deployable control for next-generation RAN load balancing using an entirely Python-based toolchain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24869v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mehrshad Eskandarpour, Hossein Soleimani</dc:creator>
    </item>
    <item>
      <title>Performance Evaluation of Multimedia Traffic in Cloud Storage Services over Wi-Fi and LTE Networks</title>
      <link>https://arxiv.org/abs/2510.25079</link>
      <description>arXiv:2510.25079v1 Announce Type: new 
Abstract: The performance of Dropbox, Google Drive, and OneDrive cloud storage services was evaluated under Wi-Fi and LTE network conditions during multimedia file uploads. Traffic was captured using Wireshark, and key metrics (including delay, jitter, bandwidth, and packet loss) were analyzed. Google Drive maintained the most consistent performance across both types of networks, showing low latency and reduced jitter. Dropbox showed efficient bandwidth utilization, but experienced a longer delay over LTE, attributed to a greater number of intermediate hops. OneDrive presented variable behavior, with elevated packet rates and increased sensitivity to fluctuations in the mobile network. A bimodal distribution of packet sizes was observed and modeled using a dual Poisson function. In general, Wi-Fi connections provided greater stability for multimedia transfers, while LTE performance varied depending on platform-specific implementations. The results contribute to a better understanding of traffic behavior in cloud-based storage applications and suggest further analysis with larger datasets and heterogeneous access networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25079v1</guid>
      <category>cs.NI</category>
      <category>cs.MM</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Albert Espinal, V. Sanchez Padilla, Yesenia Cevallos</dc:creator>
    </item>
    <item>
      <title>Learning-Based vs Human-Derived Congestion Control: An In-Depth Experimental Study</title>
      <link>https://arxiv.org/abs/2510.25105</link>
      <description>arXiv:2510.25105v1 Announce Type: new 
Abstract: Learning-based congestion control (CC), including Reinforcement-Learning, promises efficient CC in a fast-changing networking landscape, where evolving communication technologies, applications and traffic workloads pose severe challenges to human-derived, static CC algorithms. Learning-based CC is in its early days and substantial research is required to understand existing limitations, identify research challenges and, eventually, yield deployable solutions for real-world networks. In this paper, we extend our prior work and present a reproducible and systematic study of learning-based CC with the aim to highlight strengths and uncover fundamental limitations of the state-of-the-art. We directly contrast said approaches with widely deployed, human-derived CC algorithms, namely TCP Cubic and BBR (version 3). We identify challenges in evaluating learning-based CC, establish a methodology for studying said approaches and perform large-scale experimentation with learning-based CC approaches that are publicly available. We show that embedding fairness directly into reward functions is effective; however, the fairness properties do not generalise into unseen conditions. We then show that RL learning-based approaches existing approaches can acquire all available bandwidth while largely maintaining low latency. Finally, we highlight that existing the latest learning-based CC approaches under-perform when the available bandwidth and end-to-end latency dynamically change while remaining resistant to non-congestive loss. As with our initial study, our experimentation codebase and datasets are publicly available with the aim to galvanise the research community towards transparency and reproducibility, which have been recognised as crucial for researching and evaluating machine-generated policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25105v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mihai Mazilu, Luca Giacomoni, George Parisis</dc:creator>
    </item>
    <item>
      <title>ML-Based Preamble Collision Detection in the Random Access Procedure of Cellular IoT Networks</title>
      <link>https://arxiv.org/abs/2510.25145</link>
      <description>arXiv:2510.25145v1 Announce Type: new 
Abstract: Preamble collision in the random access channel (RACH) is a major bottleneck in massive machine-type communication (mMTC) scenarios, typical of cellular IoT (CIoT) deployments. This work proposes a machine learning-based mechanism for early collision detection during the random access (RA) procedure. A labeled dataset was generated using the RA procedure messages exchanged between the users and the base station under realistic channel conditions, simulated in MATLAB. We evaluate nine classic classifiers -- including tree ensembles, support vector machines, and neural networks -- across four communication scenarios, varying both channel characteristics (e.g., Doppler spread, multipath) and the cell coverage radius, to emulate realistic propagation, mobility, and spatial conditions. The neural network outperformed all other models, achieving over 98\% balanced accuracy in the in-distribution evaluation (train and test drawn from the same dataset) and sustaining 95\% under out-of-distribution evaluation (train/test from different datasets). To enable deployment on typical base station hardware, we apply post-training quantization. Full integer quantization reduced inference time from 2500 ms to as low as 0.3 ms with negligible accuracy loss. The proposed solution combines high detection accuracy with low-latency inference, making it suitable for scalable, real-time CIoT applications found in real networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25145v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giancarlo Maldonado Cardenas, Diana C. Gonzalez, Judy C. Guevara, Carlos A. Astudillo, Nelson L. S. da Fonseca</dc:creator>
    </item>
    <item>
      <title>Adaptive Design of mmWave Initial Access Codebooks using Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2510.25271</link>
      <description>arXiv:2510.25271v1 Announce Type: new 
Abstract: Initial access (IA) is the process by which user equipment (UE) establishes its first connection with a base station. In 5G systems, particularly at millimeter-wave frequencies, IA integrates beam management to support highly directional transmissions. The base station employs a codebook of beams for the transmission of Synchronization Signal Blocks (SSBs), which are periodically swept to detect and connect users. The design of this SSB codebook is critical for ensuring reliable, wide-area coverage. In current networks, SSB codebooks are meticulously engineered by domain experts. While these expert-defined codebooks provide a robust baseline, they lack flexibility in dynamic or heterogeneous environments where user distributions vary, limiting their overall effectiveness. This paper proposes a hybrid Reinforcement Learning (RL) framework for adaptive SSB codebook design. Building on top of expert knowledge, the RL agent leverages a pool of expert-designed SSB beams and learns to adaptively select or combine them based on real-time feedback. This enables the agent to dynamically tailor codebooks to the actual environment, without requiring explicit user location information, while always respecting practical beam constraints. Simulation results demonstrate that, on average, the proposed approach improves user connectivity by 10.8$\%$ compared to static expert configurations. These findings highlight the potential of combining expert knowledge with data-driven optimization to achieve more intelligent, flexible, and resilient beam management in next-generation wireless networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25271v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sabrine Aroua, Christos Anastasios Bovolis, Bo G\"oransson, Anastasios Giovanidis, Mathieu Leconte, Apostolos Destounis</dc:creator>
    </item>
    <item>
      <title>TCP ROCCET: An RTT-Oriented CUBIC Congestion Control Extension for 5G and Beyond Networks</title>
      <link>https://arxiv.org/abs/2510.25281</link>
      <description>arXiv:2510.25281v1 Announce Type: new 
Abstract: The behavior of loss-based TCP congestion control algorithms like TCP CUBIC continues to be a challenge in modern cellular networks. Due to the large RLC layer buffers required to deal with short-term changes in channel capacity, the behavior of both the Slow Start and congestion avoidance phases may be heavily impacted by the lack of packet losses and the resulting bufferbloat. While existing congestion control algorithms like TCP BBR do tend to perform better even in the presence of large bottleneck buffers, they still tend to fill the buffer more than necessary and can have fairness issues when compared to loss-based algorithms.
  In this paper, we analyze the issues with the use of loss-based congestion control algorithms by analyzing TCP CUBIC, which is currently the most popular variant. To mitigate the issues experienced by TCP CUBIC in cellular networks, we introduce TCP ROCCET, a latency-based extension of TCP CUBIC that responds to network congestion based on round-trip time in addition to packet loss.
  Our findings show that TCP ROCCET can reduce latency and bufferbloat compared to the standard CUBIC implementation, without requiring a specific network architecture. Compared to TCP BBRv3, ROCCET offers similar throughput while maintaining lower overall latency. The evaluation was conducted in real 5G networks, including both stationary and mobile scenarios, confirming ROCCET's improved response to network congestion under varying conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25281v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Prause, Mark Akselrod</dc:creator>
    </item>
    <item>
      <title>Energy consumption assessment of a Virtual Reality Remote Rendering application over 5G networks</title>
      <link>https://arxiv.org/abs/2510.25357</link>
      <description>arXiv:2510.25357v1 Announce Type: new 
Abstract: This paper investigates the energy implications of remote rendering for Virtual Reality (VR) applications within a real 5G testbed. Remote rendering enables lightweight devices to access high-performance graphical content by offloading computationally intensive tasks to Cloud-native Network Functions (CNFs) running on remote servers. However, this approach raises concerns regarding energy consumption across the various network components involved, including the remote computing node, the 5G Core, the Radio Access Network (RAN), and the User Equipment (UE). This work proposes and evaluates two complementary energy monitoring solutions, one hardware-based and one software-based, to measure energy consumption at different system levels. A VR remote renderer, deployed as CNF and leveraging the Media over QUIC (MoQ) protocol, is used as test case for assessing its energy footprint under different multimedia and network configurations. The results provide critical insights into the trade-off between energy consumption and performance of a real-world VR application running in a 5G environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25357v1</guid>
      <category>cs.NI</category>
      <category>cs.MM</category>
      <category>eess.IV</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Roberto Viola, Mikel Irazola, Jos\'e Ram\'on Ju\'arez, Minh Nguyen, Alexander Zoubarev, Alexander Futasz, Louay Bassbouss, Amr A. AbdelNabi, Javier Fern\'andez Hidalgo</dc:creator>
    </item>
    <item>
      <title>Evaluating Learning Congestion control Schemes for LEO Constellations</title>
      <link>https://arxiv.org/abs/2510.25498</link>
      <description>arXiv:2510.25498v1 Announce Type: new 
Abstract: Low Earth Orbit (LEO) satellite networks introduce unique congestion control (CC) challenges due to frequent handovers, rapidly changing round-trip times (RTTs), and non-congestive loss. This paper presents the first comprehensive, emulation-driven evaluation of CC schemes in LEO networks, combining realistic orbital dynamics via the LeoEM framework with targeted Mininet micro-benchmarks. We evaluated representative CC algorithms from three classes, loss-based (Cubic, SaTCP), model-based (BBRv3), and learning-based (Vivace, Sage, Astraea), across diverse single-flow and multi-flow scenarios, including interactions with active queue management (AQM). Our findings reveal that: (1) handover-aware loss-based schemes can reclaim bandwidth but at the cost of increased latency; (2) BBRv3 sustains high throughput with modest delay penalties, yet reacts slowly to abrupt RTT changes; (3) RL-based schemes severely underperform under dynamic conditions, despite being notably resistant to non-congestive loss; (4) fairness degrades significantly with RTT asymmetry and multiple bottlenecks, especially in human-designed CC schemes; and (5) AQM at bottlenecks can restore fairness and boost efficiency. These results expose critical limitations in current CC schemes and provide insight for designing LEO-specific data transport protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25498v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mihai Mazilu, Aiden Valentine, George Parisis</dc:creator>
    </item>
    <item>
      <title>Device to Device Pairs Sharding based on Distance</title>
      <link>https://arxiv.org/abs/2510.25552</link>
      <description>arXiv:2510.25552v1 Announce Type: new 
Abstract: In the conventional cellular system, devices are not allowed to communicate directly with each other in the licensed cellular bandwidth and all communications take place through the base stations. The users requirements has led the technology to become fast and faster. Multimedia rich data exchange, fast service, high quality voice calls, newer and more demanding applications, information at fingertips, everything requires technology and communication between devices. A constant need to increase network capacity for meeting the users growing demands has led to the growth of cellular communication networks from the first generation(1G) to the fifth generation(5G). There will be crores of connected devices in the coming future . A large number of connections are going to be heterogeneous, demanding lesser delays, higher data rates, superior throughput and enhanced system capacity. The available spectrum resources are limited and has to be flexibly used by mobile network operators to cope with the rising demands. An emerging facilitator of the upcoming high data rate demanding next-generation networks are device-to-device(D2D) communication. This paper has developed a model that establishes Device-to-Device (D2D) communication between two end-users without involving the eNB (evolved Node B). We have sharded the UEs and CUs based on the criteria of DISTANCE. To do so, we used the K-means clustering method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25552v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>K Prajwal, Tharun K, Navaneeth P, Ishwar Mandal, Kiran M</dc:creator>
    </item>
    <item>
      <title>Deep Reinforcement Learning-Based Cooperative Rate Splitting for Satellite-to-Underground Communication Networks</title>
      <link>https://arxiv.org/abs/2510.25562</link>
      <description>arXiv:2510.25562v1 Announce Type: new 
Abstract: Reliable downlink communication in satellite-to-underground networks remains challenging due to severe signal attenuation caused by underground soil and refraction in the air-soil interface. To address this, we propose a novel cooperative rate-splitting (CRS)-aided transmission framework, where an aboveground relay decodes and forwards the common stream to underground devices (UDs). Based on this framework, we formulate a max-min fairness optimization problem that jointly optimizes power allocation, message splitting, and time slot scheduling to maximize the minimum achievable rate across UDs. To solve this high-dimensional non-convex problem under uncertain channels, we develop a deep reinforcement learning solution framework based on the proximal policy optimization (PPO) algorithm that integrates distribution-aware action modeling and a multi-branch actor network. Simulation results under a realistic underground pipeline monitoring scenario demonstrate that the proposed approach achieves average max-min rate gains exceeding $167\%$ over conventional benchmark strategies across various numbers of UDs and underground conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25562v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaiqiang Lin, Kangchun Zhao, Yijie Mao</dc:creator>
    </item>
    <item>
      <title>MetaLore: Learning to Orchestrate Communication and Computation for Metaverse Synchronization</title>
      <link>https://arxiv.org/abs/2510.25705</link>
      <description>arXiv:2510.25705v1 Announce Type: new 
Abstract: As augmented and virtual reality evolve, achieving seamless synchronization between physical and digital realms remains a critical challenge, especially for real-time applications where delays affect the user experience. This paper presents MetaLore, a Deep Reinforcement Learning (DRL) based framework for joint communication and computational resource allocation in Metaverse or digital twin environments. MetaLore dynamically shares the communication bandwidth and computational resources among sensors and mobile devices to optimize synchronization, while offering high throughput performance. Special treatment is given in satisfying end-to-end delay guarantees. A key contribution is the introduction of two novel Age of Information (AoI) metrics: Age of Request Information (AoRI) and Age of Sensor Information (AoSI), integrated into the reward function to enhance synchronization quality. An open source simulator has been extended to incorporate and evaluate the approach. The DRL solution is shown to achieve the performance of full-enumeration brute-force solutions by making use of a small, task-oriented observation space of two queue lengths at the network side. This allows the DRL approach the flexibility to effectively and autonomously adapt to dynamic traffic conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25705v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elif Ebru Ohri, Qi Liao, Anastasios Giovanidis, Francesca Fossati, Nour-El-Houda Yellas</dc:creator>
    </item>
    <item>
      <title>Is Protective DNS Blocking the Wild West?</title>
      <link>https://arxiv.org/abs/2510.25352</link>
      <description>arXiv:2510.25352v1 Announce Type: cross 
Abstract: We perform a passive measurement study investigating how a Protective DNS service might perform in a Research &amp; Education Network serving hundreds of member institutions. Utilizing freely-available DNS blocklists consisting of domain names deemed to be threats, we test hundreds of millions of users' real DNS queries, observed over a week's time, to find which answers would be blocked because they involve domain names that are potential threats. We find the blocklists disorderly regarding their names, goals, transparency, and provenance making them quite difficult to compare. Consequently, these Protective DNS underpinnings lack organized oversight, presenting challenges and risks in operation at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25352v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Plonka, Branden Palacio, Debbie Perouli</dc:creator>
    </item>
    <item>
      <title>DSNS: The Deep Space Network Simulator</title>
      <link>https://arxiv.org/abs/2508.04317</link>
      <description>arXiv:2508.04317v3 Announce Type: replace 
Abstract: Simulation tools are commonly used in the development and testing of new protocols or new networks. However, as satellite networks start to grow to encompass thousands of nodes, and as companies and space agencies begin to realize the interplanetary internet, existing satellite and network simulation tools have become impractical for use in this context.
  We therefore present the Deep Space Network Simulator (DSNS): a new network simulator with a focus on large-scale satellite networks. We demonstrate its improved capabilities compared to existing offerings, showcase its flexibility and extensibility through an implementation of existing protocols and the DTN simulation reference scenarios recommended by CCSDS, and evaluate its scalability, showing that it exceeds existing tools while providing better fidelity.
  DSNS provides concrete usefulness to both standards bodies and satellite operators, enabling fast iteration on protocol development and testing of parameters under highly realistic conditions. By removing roadblocks to research and innovation, we can accelerate the development of upcoming satellite networks and ensure that their communication is both fast and secure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04317v3</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Smailes, Filip Futera, Sebastian K\"ohler, Simon Birnbach, Martin Strohmeier, Ivan Martinovic</dc:creator>
    </item>
    <item>
      <title>Towards Trusted Service Monitoring: Verifiable Service Level Agreements</title>
      <link>https://arxiv.org/abs/2510.13370</link>
      <description>arXiv:2510.13370v2 Announce Type: replace-cross 
Abstract: Service Level Agreement (SLA) monitoring in service-oriented environments suffers from inherent trust conflicts when providers self-report metrics, creating incentives to underreport violations. We introduce a framework for generating verifiable SLA violation claims through trusted hardware monitors and zero-knowledge proofs, establishing cryptographic foundations for genuine trustworthiness in service ecosystems. Our approach starts with machine-readable SLA clauses converted into verifiable predicates and monitored within Trusted Execution Environments. These monitors collect timestamped telemetry, organize measurements into Merkle trees, and produce signed attestations. Zero-knowledge proofs aggregate Service-Level Indicators to evaluate compliance, generating cryptographic proofs verifiable by stakeholders, arbitrators, or insurers in disputes, without accessing underlying data. This ensures three security properties: integrity, authenticity, and validity. Our prototype demonstrates linear scaling up to over 1 million events per hour for measurements with near constant-time proof generation and verification for single violation claims, enabling trustless SLA enforcement through cryptographic guarantees for automated compliance verification in service monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13370v2</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Thu, 30 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fernando Castillo, Eduardo Brito, Sebastian Werner, Pille Pullonen-Raudvere, Jonathan Heiss</dc:creator>
    </item>
  </channel>
</rss>
