<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Jun 2025 04:01:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimal Experimental Design Criteria for Data-Consistent Inversion</title>
      <link>https://arxiv.org/abs/2506.12157</link>
      <description>arXiv:2506.12157v1 Announce Type: new 
Abstract: The ability to design effective experiments is crucial for obtaining data that can substantially reduce the uncertainty in the predictions made using computational models. An optimal experimental design (OED) refers to the choice of a particular experiment that optimizes a particular design criteria, e.g., maximizing a utility function, which measures the information content of the data. However, traditional approaches for optimal experimental design typically require solving a large number of computationally intensive inverse problems to find the data that maximizes the utility function. Here, we introduce two novel OED criteria that are specifically crafted for the data consistent inversion (DCI) framework, but do not require solving inverse problems. DCI is a specific approach for solving a class of stochastic inverse problems by constructing a pullback measure on uncertain parameters from an observed probability measure on the outputs of a quantity of interest (QoI) map. While expected information gain (EIG) has been used for both DCI and Bayesian based OED, the characteristics and properties of DCI solutions differ from those of solutions to Bayesian inverse problems which should be reflected in the OED criteria. The new design criteria developed in this study, called the expected scaling effect and the expected skewness effect, leverage the geometric structure of pre-images associated with observable data sets, allowing for an intuitive and computationally efficient approach to OED. These criteria utilize singular value computations derived from sampled and approximated Jacobians of the experimental designs. We present both simultaneous and sequential (greedy) formulations of OED based on these innovative criteria. Numerical results demonstrate the effectiveness in our approach for solving stochastic inverse problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12157v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Troy Butler, John Jakeman, Michael Pilosov, Scott Walsh, Timothy Wildey</dc:creator>
    </item>
    <item>
      <title>A simplified and robust proxy-based approach for overcoming unmeasured confounding in EHR studies</title>
      <link>https://arxiv.org/abs/2506.12177</link>
      <description>arXiv:2506.12177v1 Announce Type: new 
Abstract: Electronic health records (EHR) are used to study treatment effects in clinical settings, yet unmeasured confounding remains a persistent challenge. Indirect measurements of the unmeasured confounder (proxies) offer a potential solution, but existing approaches -- such as proximal inference or full joint modeling -- can be difficult to implement. We propose a two-stage, proxy-based method that is practical, broadly applicable, and robust. In the first stage, we apply factor analysis to proxy and treatment variables, extracting information on latent factors that serve as a surrogate for the unmeasured confounder. In the second stage, we use this model to build covariates that improve causal effect estimation in a standard outcome regression model. Through simulations, we test the method's performance under assumption violations, including non-normal errors, model misspecification, and scenarios where instruments or confounders are incorrectly treated as proxies. We also apply the method to estimate the effect of hospital admission for older adults presenting to the emergency department with chest pain, a setting where standard analyses may fail to recover plausible effects. Our results show that this simplified strategy recovers more reliable estimates than conventional adjustment methods, offering applied researchers a practical tool for addressing unmeasured confounding with proxy variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12177v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haley Colgate Kottler, Amy Cochran</dc:creator>
    </item>
    <item>
      <title>Resilience Measures for the Surrogate Paradox</title>
      <link>https://arxiv.org/abs/2506.12194</link>
      <description>arXiv:2506.12194v1 Announce Type: new 
Abstract: Surrogate markers are often used in clinical trials to evaluate treatment effects when primary outcomes are costly, invasive, or take a long time to observe. However, reliance on surrogates can lead to the surrogate paradox, where a treatment appears beneficial based on the surrogate but is actually harmful with respect to the primary outcome. In this paper, we propose formal measures to assess resilience against the surrogate paradox. Our setting assumes an existing study in which the surrogate marker and primary outcome have been measured (Study A) and a new study (Study B) in which only the surrogate is measured. Rather than assuming transportability of the conditional mean functions across studies, we consider a class of functions for Study B that deviate from those in Study A. Using these, we estimate the distribution of potential treatment effects on the unmeasured primary outcome and define resilience measures including a resilience probability, resilience bound, and resilience set. Our approach complements traditional surrogate validation methods by quantifying the plausibility of the surrogate paradox under controlled deviations from what is known from Study A. We investigate the performance of our proposed measures via a simulation study and application to two distinct HIV clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12194v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emily Hsiao, Lu Tian, Layla Parast</dc:creator>
    </item>
    <item>
      <title>Estimating treatment effects with a unified semi-parametric difference-in-differences approach</title>
      <link>https://arxiv.org/abs/2506.12207</link>
      <description>arXiv:2506.12207v1 Announce Type: new 
Abstract: The difference-in-differences (DID) approach is widely used for estimating causal effects with observational data before and after an intervention. DID is traditionally used to assess an average treatment effect among the treated after making a parallel trends assumption on the means of the outcome. With skewed outcomes, a transformation is often needed; however, the transformation may be difficult to choose, results may be sensitive to the choice, and parallel trends assumptions are made on the transformed scale. More recent DID methods estimate alternative treatment effects, such as quantile treatment effects among the treated, that offer a different understanding of the impact of a treatment and may be preferable with skewed outcomes. However, each alternative DID estimator requires a different parallel trends assumption. We introduce a new DID method that is capable of estimating average, quantile, probability, and novel Mann-Whitney treatment effects among the treated with a single unifying parallel trends assumption. The proposed method uses a semi-parametric cumulative probability model (CPM). The CPM is a linear model for a latent variable on covariates, where the latent variable results from an unspecified transformation of the outcome. Our DID approach makes a universal parallel trends assumption on the expectation of the latent variable conditional on covariates. Hence, our method overcomes challenges surrounding outcomes with complicated, difficult-to-model distributions and avoids the need for separate assumptions and/or approaches for each estimand. We introduce the method; describe identification, estimation, and inference; conduct simulations evaluating its performance; and apply it to real-world data to assess the impact of Medicaid expansion on CD4 cell count at enrollment among people living with HIV.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12207v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julia C. Thome, Andrew J. Spieker, Peter F. Rebeiro, Chun Li, Tong Li, Bryan E. Shepherd</dc:creator>
    </item>
    <item>
      <title>Partial identification via conditional linear programs: estimation and policy learning</title>
      <link>https://arxiv.org/abs/2506.12215</link>
      <description>arXiv:2506.12215v1 Announce Type: new 
Abstract: Many important quantities of interest are only partially identified from observable data: the data can limit them to a set of plausible values, but not uniquely determine them. This paper develops a unified framework for covariate-assisted estimation, inference, and decision making in partial identification problems where the parameter of interest satisfies a series of linear constraints, conditional on covariates. In such settings, bounds on the parameter can be written as expectations of solutions to conditional linear programs that optimize a linear function subject to linear constraints, where both the objective function and the constraints may depend on covariates and need to be estimated from data. Examples include estimands involving the joint distributions of potential outcomes, policy learning with inequality-aware value functions, and instrumental variable settings. We propose two de-biased estimators for bounds defined by conditional linear programs. The first directly solves the conditional linear programs with plugin estimates and uses output from standard LP solvers to de-bias the plugin estimate, avoiding the need for computationally demanding vertex enumeration of all possible solutions for symbolic bounds. The second uses entropic regularization to create smooth approximations to the conditional linear programs, trading a small amount of approximation error for improved estimation and computational efficiency. We establish conditions for asymptotic normality of both estimators, show that both estimators are robust to first-order errors in estimating the conditional constraints and objectives, and construct Wald-type confidence intervals for the partially identified parameters. These results also extend to policy learning problems where the value of a decision policy is only partially identified. We apply our methods to a study on the effects of Medicaid enrollment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12215v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eli Ben-Michael</dc:creator>
    </item>
    <item>
      <title>Inference for microbe--metabolite association networks using a latent graph model</title>
      <link>https://arxiv.org/abs/2506.12275</link>
      <description>arXiv:2506.12275v1 Announce Type: new 
Abstract: Correlation networks are commonly used to infer associations between microbes and metabolites. The resulting p-values are then corrected for multiple comparisons using existing methods such as the Benjamini and Hochberg procedure to control the false discovery rate (FDR). However, most existing methods for FDR control assume the p-values are weakly dependent. Consequently, they can have low power in recovering microbe-metabolite association networks that exhibit important topological features, such as the presence of densely associated modules. We propose a novel inference procedure that is both powerful for detecting significant associations in the microbe-metabolite network and capable of controlling the FDR. Power enhancement is achieved by modeling latent structures in the form of a bipartite stochastic block model. We develop a variational expectation-maximization algorithm to estimate the model parameters and incorporate the learned graph in the testing procedure. In addition to FDR control, this procedure provides a clustering of microbes and metabolites into modules, which is useful for interpretation. We demonstrate the merit of the proposed method in simulations and an application to bacterial vaginosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12275v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jing Ma</dc:creator>
    </item>
    <item>
      <title>Generalizable estimation of conditional average treatment effects using Causal Forest in randomized</title>
      <link>https://arxiv.org/abs/2506.12296</link>
      <description>arXiv:2506.12296v1 Announce Type: new 
Abstract: Generalizing conditional average treatment effects (CATE) estimates in a randomized controlled trial (RCT) to a broader source population can be challenging because of selection bias and high-dimensional covariates. We aim to evaluate CATE estimation approaches using Causal Forest that address selection bias due to trial participation. We propose and compare four CATE estimation approaches using Causal Forest: (1) ignoring selection variables, (2) including selection variables, (3) using inverse probability weighting (IPW) either with (1) or (2). Identifiable condition suggests that including covariates that determine trial selection in CATE-estimating models can yield an unbiased CATE estimate in the source population. However, simulations showed that, in realistic sample sizes in a medical trial, this approach substantially increased variance compared with little gain in bias reduction. IPW-based approaches showed a better performance in most settings by addressing selection bias. Increasing covariates that determine trial participation in Causal Forest estimation can substantially inflate the variance, diminishing benefits of bias reduction. IPW offers a more robust method to adjust for selection bias due to trial participation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12296v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rikuta Hamaya, Etsuji Suzuki, Konan Hara</dc:creator>
    </item>
    <item>
      <title>A Generalized Framework for Approximate Co-Sufficient Sampling</title>
      <link>https://arxiv.org/abs/2506.12334</link>
      <description>arXiv:2506.12334v1 Announce Type: new 
Abstract: Approximate co-sufficient sampling (aCSS) offers a principled route to hypothesis testing when null distributions are unknown, yet current implementations are confined to maximum likelihood estimators with smooth or linear regularization and provide little theoretical insight into power. We present a generalized framework that widens the scope of the aCSS method to embrace nonlinear regularization, such as group lasso and nonconvex penalties, as well as robust and nonparametric estimators. Moreover, we introduce a weighted sampling scheme for enhanced flexibility and propose a generalized aCSS framework that unifies existing conditional sampling methods. Our theoretical analysis rigorously establishes validity and, for the first time, characterizes the power optimality of aCSS procedures in certain high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12334v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Xie, Dongming Huang</dc:creator>
    </item>
    <item>
      <title>A Minimum Distance Estimator Approach for Misspecified Ergodic Processes</title>
      <link>https://arxiv.org/abs/2506.12432</link>
      <description>arXiv:2506.12432v1 Announce Type: new 
Abstract: We propose a minimum distance estimator (MDE) for parameter identification in misspecified models characterized by a sequence of ergodic stochastic processes that converge weakly to the model of interest. The data is generated by the sequence of processes, and we are interested in inferring parameters for the limiting processes. We define a general statistical setting for parameter estimation under such model misspecification and prove the robustness of the MDE. Furthermore, we prove the asymptotic normality of the MDE for multiscale diffusion processes with a well-defined homogenized limit. A tractable numerical implementation of the MDE is provided and realized in the programming language Julia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12432v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaroslav I. Borodavka, Sebastian Krumscheid, Grigorios A. Pavliotis</dc:creator>
    </item>
    <item>
      <title>Truncated Cauchy Combination Test: a Robust and Powerful P-value Combination Method with Arbitrary Correlations</title>
      <link>https://arxiv.org/abs/2506.12489</link>
      <description>arXiv:2506.12489v1 Announce Type: new 
Abstract: Cauchy combination test has been widely used for combining correlated p-values, but it may fail to work under certain scenarios. We propose a truncated Cauchy combination test (TCCT) which focus on combining p-values with arbitrary correlations, and demonstrate that our proposed test solves the limitations of Cauchy combination test and always has higher power. We prove that the tail probability of our test statistic is asymptotically Cauchy distributed, so it is computationally effective to achieve the combined p-value using our proposed TCCT. We show by simulation that our proposed test has accurate type I error rates, and maintain high power when Cauchy combination test fails to work. We finally perform application studies to illustrate the usefulness of our proposed test on GWAS and microbiome sequencing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12489v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bo Chen, Wei Xu, Xin Gao</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for the learning rate in Generalised Bayesian inference</title>
      <link>https://arxiv.org/abs/2506.12532</link>
      <description>arXiv:2506.12532v1 Announce Type: new 
Abstract: In Generalised Bayesian Inference (GBI), the learning rate and hyperparameters of the loss must be estimated. However, these inference-hyperparameters can't be estimated jointly with the other parameters by giving them a prior, as we discuss. Several methods for estimating the learning rate have been given which elicit and minimise a loss based on the goals of the overall inference (in our case, prediction of new data). However, in some settings there exists an unknown ``true'' learning rate about which it is meaningful to have prior belief and it is then possible to use Bayesian inference with held out data to get a posterior for the learning rate. We give conditions under which this posterior concentrates on the optimal rate and suggest hyperparameter estimators derived from this posterior. The new framework supports joint estimation and uncertainty quatification for inference hyperparameters. Experiments show that the resulting GBI-posteriors out-perform Bayesian inference on simulated test data and select optimal or near optimal hyperparameter values in a large real problem of text analysis. Generalised Bayesian inference is particularly useful for combining multiple data sets and most of our examples belong to that setting. As a side note we give asymptotic results for some of the special ``multi-modular'' Generalised Bayes posteriors, which we use in our examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12532v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeong Eun Lee, Sitong Liu, Geoff K. Nicholls</dc:creator>
    </item>
    <item>
      <title>A data-based notion of quantiles on Hadamard spaces</title>
      <link>https://arxiv.org/abs/2506.12534</link>
      <description>arXiv:2506.12534v1 Announce Type: new 
Abstract: This paper defines an alternative notion, described as data-based, of geometric quantiles on Hadamard spaces, in contrast to the existing methodology, described as parameter-based. In addition to having the same desirable properties as parameter-based quantiles, these data-based quantiles are shown to have several theoretical advantages related to large-sample properties like strong consistency and asymptotic normality, breakdown points, extreme quantiles and the gradient of the loss function. Using simulations, we explore some other advantages of the data-based framework, including simpler computation and better adherence to the shape of the distribution, before performing experiments with real diffusion tensor imaging data lying on a manifold of symmetric positive definite matrices. These experiments illustrate some of the uses of these quantiles by testing the equivalence of the generating distributions of different data sets and measuring distributional characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12534v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ha-Young Shin, Hee-Seok Oh</dc:creator>
    </item>
    <item>
      <title>Robust and efficient multiple-unit switchback experimentation</title>
      <link>https://arxiv.org/abs/2506.12654</link>
      <description>arXiv:2506.12654v1 Announce Type: new 
Abstract: User-randomized A/B testing has emerged as the gold standard for online experimentation. However, when this kind of approach is not feasible due to legal, ethical or practical considerations, experimenters have to consider alternatives like item-randomization. Item-randomization is often met with skepticism due to its poor empirical performance. To fill this gap, in this paper we introduce a novel and rich class of experimental designs, "Regular Balanced Switchback Designs" (RBSDs). At their core, RBSDs work by randomly changing treatment assignments over both time and items. After establishing the properties of our designs in a potential outcomes framework, characterizing assumptions and conditions under which corresponding estimators are resilient to the presence of carryover effects, we show empirically via both realistic simulations and real e-commerce data that RBSDs systematically outperform standard item-randomized and non-balanced switchback approaches by yielding much more accurate estimates of the causal effects of interest without incurring any additional bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12654v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Missault, Lorenzo Masoero, Christian Delb\'e, Thomas Richardson, Guido Imbens</dc:creator>
    </item>
    <item>
      <title>Effect Decomposition of Functional-Output Computer Experiments via Orthogonal Additive Gaussian Processes</title>
      <link>https://arxiv.org/abs/2506.12701</link>
      <description>arXiv:2506.12701v1 Announce Type: new 
Abstract: Functional ANOVA (FANOVA) is a widely used variance-based sensitivity analysis tool. However, studies on functional-output FANOVA remain relatively scarce, especially for black-box computer experiments, which often involve complex and nonlinear functional-output relationships with unknown data distribution. Conventional approaches often rely on predefined basis functions or parametric structures that lack the flexibility to capture complex nonlinear relationships. Additionally, strong assumptions about the underlying data distributions further limit their ability to achieve a data-driven orthogonal effect decomposition. To address these challenges, this study proposes a functional-output orthogonal additive Gaussian process (FOAGP) to efficiently perform the data-driven orthogonal effect decomposition. By enforcing a conditional orthogonality constraint on the separable prior process, the proposed functional-output orthogonal additive kernel enables data-driven orthogonality without requiring prior distributional assumptions. The FOAGP framework also provides analytical formulations for local Sobol' indices and expected conditional variance sensitivity indices, enabling comprehensive sensitivity analysis by capturing both global and local effect significance. Validation through two simulation studies and a real case study on fuselage shape control confirms the model's effectiveness in orthogonal effect decomposition and variance decomposition, demonstrating its practical value in engineering applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12701v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Tan, Yongxiang Li, Xiaowu Dai, Kwok-Leung Tsui</dc:creator>
    </item>
    <item>
      <title>Efficient Implementation of a Semiparametric Joint Model for Multivariate Longitudinal Biomarkers and Competing Risks Time-to-Event Data</title>
      <link>https://arxiv.org/abs/2506.12741</link>
      <description>arXiv:2506.12741v1 Announce Type: new 
Abstract: Joint modeling has become increasingly popular for characterizing the association between one or more longitudinal biomarkers and competing risks time-to-event outcomes. However, semiparametric multivariate joint modeling for large-scale data encounter substantial statistical and computational challenges, primarily due to the high dimensionality of random effects and the complexity of estimating nonparametric baseline hazards. These challenges often lead to prolonged computation time and excessive memory usage, limiting the utility of joint modeling for biobank-scale datasets. In this article, we introduce an efficient implementation of a semiparametric multivariate joint model, supported by a normal approximation and customized linear scan algorithms within an expectation-maximization (EM) framework. Our method significantly reduces computation time and memory consumption, enabling the analysis of data from thousands of subjects. The scalability and estimation accuracy of our approach are demonstrated through two simulation studies. We also present an application to the Primary Biliary Cirrhosis (PBC) dataset involving five longitudinal biomarkers as an illustrative example. A user-friendly R package, \texttt{FastJM}, has been developed for the shared random effects joint model with efficient implementation. The package is publicly available on the Comprehensive R Archive Network: https://CRAN.R-project.org/package=FastJM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12741v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shanpeng Li, Emily Ouyang, Jin Zhou, Xinping Cui, Gang Li</dc:creator>
    </item>
    <item>
      <title>A Residual Prediction Test for the Well-Specification of Linear Instrumental Variable Models</title>
      <link>https://arxiv.org/abs/2506.12771</link>
      <description>arXiv:2506.12771v1 Announce Type: new 
Abstract: The linear instrumental variable (IV) model is widely applied in observational studies. The corresponding assumptions are critical for valid causal inference, and hence, it is important to have tools to assess the model's well-specification. The classical Sargan-Hansen J-test is limited to the overidentified setting, where the number of instruments is larger than the number of endogenous variables. Here, we propose a novel and simple test for the well-specification of the linear IV model under the assumption that the structural error is mean independent of the instruments. Importantly, assuming mean independence allows the construction of such a test even in the just-identified setting. We use the idea of residual prediction tests: if the residuals from two-stage least squares can be predicted from the instruments better than randomly, this signals misspecification. We construct a test statistic based on sample splitting and a user-chosen machine learning method. We show asymptotic type I error control. Furthermore, by relying on machine learning tools, our test has good power for detecting alternatives from a broad class of scenarios. We also address heteroskedasticity- and cluster-robust inference. The test is implemented in the R package RPIV and in the ivmodels software package for Python.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12771v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cyrill Scheidegger, Malte Londschien, Peter B\"uhlmann</dc:creator>
    </item>
    <item>
      <title>High-dimensional regression with outcomes of mixed-type using the multivariate spike-and-slab LASSO</title>
      <link>https://arxiv.org/abs/2506.13007</link>
      <description>arXiv:2506.13007v1 Announce Type: new 
Abstract: We consider a high-dimensional multi-outcome regression in which $q,$ possibly dependent, binary and continuous outcomes are regressed onto $p$ covariates. We model the observed outcome vector as a partially observed latent realization from a multivariate linear regression model. Our goal is to estimate simultaneously a sparse matrix ($B$) of latent regression coefficients (i.e., partial covariate effects) and a sparse latent residual precision matrix ($\Omega$), which induces partial correlations between the observed outcomes. To this end, we specify continuous spike-and-slab priors on all entries of $B$ and off-diagonal elements of $\Omega$ and introduce a Monte Carlo Expectation-Conditional Maximization algorithm to compute the maximum a posterior estimate of the model parameters. Under a set of mild assumptions, we derive the posterior contraction rate for our model in the high-dimensional regimes where both $p$ and $q$ diverge with the sample size $n$ and establish a sure screening property, which implies that, as $n$ increases, we can recover all truly non-zero elements of $B$ with probability tending to one. We demonstrate the excellent finite-sample properties of our proposed method, which we call mixed-mSSL, using extensive simulation studies and three applications spanning medicine to ecology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13007v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soham Ghosh, Sameer K. Deshpande</dc:creator>
    </item>
    <item>
      <title>Discussion of "Causal and counterfactual views of missing data models" by Razieh Nabi, Rohit Bhattacharya, Ilya Shpitser, &amp; James M. Robins</title>
      <link>https://arxiv.org/abs/2506.13025</link>
      <description>arXiv:2506.13025v1 Announce Type: new 
Abstract: We congratulate Nabi et al. (2022) on their impressive and insightful paper, which illustrates the benefits of using causal/counterfactual perspectives and tools in missing data problems. This paper represents an important approach to missing-not-at-random (MNAR) problems, exploiting nonparametric independence restrictions for identification, as opposed to parametric/semiparametric models, or resorting to sensitivity analysis. Crucially, the authors represent these restrictions with missing data directed acyclic graphs (m-DAGs), which can be useful to determine identification in complex and interesting MNAR models. In this discussion we consider: (i) how/whether other tools from causal inference could be useful in missing data problems, (ii) problems that combine both missing data and causal inference together, and (iii) some work on estimation in one of the authors' example MNAR models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13025v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex W. Levis, Edward H. Kennedy</dc:creator>
    </item>
    <item>
      <title>The Mixed-Sparse-Smooth-Model Toolbox (MSSM): Efficient Estimation and Selection of Large Multi-Level Statistical Models</title>
      <link>https://arxiv.org/abs/2506.13132</link>
      <description>arXiv:2506.13132v1 Announce Type: new 
Abstract: Additive smooth models, such as Generalized additive models (GAMs) of location, scale, and shape (GAMLSS), are a popular choice for modeling experimental data. However, software available to fit such models is usually not tailored specifically to the estimation of mixed models. As a result, estimation can slow down as the number of random effects increases. Additionally, users often have to provide a substantial amount of problem-specific information in case they are interested in more general non-standard smooth models, such as higher-order derivatives of the likelihood. Here we combined and extended recently proposed strategies to reduce memory requirements and matrix infill into a theoretical framework that supports efficient estimation of general mixed sparse smooth models, including GAMs &amp; GAMLSS, based only on the Gradient and Hessian of the log-likelihood. To make non-standard smooth models more accessible, we developed an approximate estimation algorithm (the L-qEFS update) based on limited-memory quasi-Newton methods. This enables estimation of any general smooth model based only on the log-likelihood function. We also considered the problem of model selection for general mixed smooth models. To facilitate practical application we provide a Python implementation of the theoretical framework, algorithms, and model selection strategies presented here: the Mixed-Sparse-Smooth-Model (MSSM) toolbox. MSSM supports estimation and selection of massive additive multi-level models that are impossible to estimate with alternative software, for example of trial level EEG data. Additionally, when the L-qEFS update is used for estimation, implementing a new non-standard smooth model in MSSM is straightforward. Results from multiple simulation studies and real data examples are presented, showing that the framework implemented in MSSM is both efficient and robust to numerical instabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13132v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua Krause, Jelmer P. Borst, Jacolien van Rij</dc:creator>
    </item>
    <item>
      <title>Fortified Proximal Causal Inference with Many Invalid Proxies</title>
      <link>https://arxiv.org/abs/2506.13152</link>
      <description>arXiv:2506.13152v1 Announce Type: new 
Abstract: Causal inference from observational data often relies on the assumption of no unmeasured confounding, an assumption frequently violated in practice due to unobserved or poorly measured covariates. Proximal causal inference (PCI) offers a promising framework for addressing unmeasured confounding using a pair of outcome and treatment confounding proxies. However, existing PCI methods typically assume all specified proxies are valid, which may be unrealistic and is untestable without extra assumptions. In this paper, we develop a semiparametric approach for a many-proxy PCI setting that accommodates potentially invalid treatment confounding proxies. We introduce a new class of fortified confounding bridge functions and establish nonparametric identification of the population average treatment effect (ATE) under the assumption that at least $\gamma$ out of $K$ candidate treatment confounding proxies are valid, for any $\gamma \leq K$ set by the analyst without requiring knowledge of which proxies are valid. We establish a local semiparametric efficiency bound and develop a class of multiply robust, locally efficient estimators for the ATE. These estimators are thus simultaneously robust to invalid treatment confounding proxies and model misspecification of nuisance parameters. The proposed methods are evaluated through simulation and applied to assess the effect of right heart catheterization in critically ill patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13152v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Myeonghun Yu, Xu Shi, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Joint Quantile Shrinkage: A State-Space Approach toward Non-Crossing Bayesian Quantile Models</title>
      <link>https://arxiv.org/abs/2506.13257</link>
      <description>arXiv:2506.13257v1 Announce Type: new 
Abstract: Crossing of fitted conditional quantiles is a prevalent problem for quantile regression models. We propose a new Bayesian modelling framework that penalises multiple quantile regression functions toward the desired non-crossing space. We achieve this by estimating multiple quantiles jointly with a prior on variation across quantiles, a fused shrinkage prior with quantile adaptivity. The posterior is derived from a decision-theoretic general Bayes perspective, whose form yields a natural state-space interpretation aligned with Time-Varying Parameter (TVP) models. Taken together our approach leads to a Quantile- Varying Parameter (QVP) model, for which we develop efficient sampling algorithms. We demonstrate that our proposed modelling framework provides superior parameter recovery and predictive performance compared to competing Bayesian and frequentist quantile regression estimators in simulated experiments and a real-data application to multivariate quantile estimation in macroeconomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13257v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Kohns, Tibor Szendrei</dc:creator>
    </item>
    <item>
      <title>Exploring Discrete Factor Analysis with the discFA Package in R</title>
      <link>https://arxiv.org/abs/2506.13309</link>
      <description>arXiv:2506.13309v1 Announce Type: new 
Abstract: Literature suggested that using the traditional factor analysis for the count data may be inappropriate. With that in mind, discrete factor analysis builds on fitting systems of dependent discrete random variables to data. The data should be in the form of non-negative counts. Data may also be truncated at some positive integer value. The discFA package in R allows for two distributions: Poisson and Negative Binomial, in combination with possible zero inflation and possible truncation, hence, eight different alternatives. A forward search algorithm is employed to find the model optimal factor model with the lowest AIC. Several different illustrative examples from psychology, agriculture, car industry, and a simulated data will be analyzed at the end.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13309v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Reza Arabi Belaghi, Yasin Asar, Rolf Larsson</dc:creator>
    </item>
    <item>
      <title>Estimating Intractable Posterior Distributions through Gaussian Process regression and Metropolis-adjusted Langevin procedure</title>
      <link>https://arxiv.org/abs/2506.13336</link>
      <description>arXiv:2506.13336v1 Announce Type: new 
Abstract: Numerical simulations are crucial for modeling complex systems, but calibrating them becomes challenging when data are noisy or incomplete and likelihood evaluations are computationally expensive. Bayesian calibration offers an interesting way to handle uncertainty, yet computing the posterior distribution remains a major challenge under such conditions. To address this, we propose a sequential surrogate-based approach that incrementally improves the approximation of the log-likelihood using Gaussian Process Regression. Starting from limited evaluations, the surrogate and its gradient are refined step by step. At each iteration, new evaluations of the expensive likelihood are added only at informative locations, that is to say where the surrogate is most uncertain and where the potential impact on the posterior is greatest. The surrogate is then coupled with the Metropolis-Adjusted Langevin Algorithm, which uses gradient information to efficiently explore the posterior. This approach accelerates convergence, handles relatively high-dimensional settings, and keeps computational costs low. We demonstrate its effectiveness on both a synthetic benchmark and an industrial application involving the calibration of high-speed train parameters from incomplete sensor data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13336v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guillaume Perrin, Romain Jorge Do Marco, Christian Soize, Christine Funfschilling</dc:creator>
    </item>
    <item>
      <title>Penalised spline estimation of covariate-specific time-dependent ROC curves</title>
      <link>https://arxiv.org/abs/2506.13604</link>
      <description>arXiv:2506.13604v1 Announce Type: new 
Abstract: The identification of biomarkers with high predictive accuracy is a crucial task in medical research, as it can aid clinicians in making early decisions, thereby reducing morbidity and mortality in high-risk populations. Time-dependent receiver operating characteristic (ROC) curves are the main tool used to assess the accuracy of prognostic biomarkers for outcomes that evolve over time. Recognising the need to account for patient heterogeneity when evaluating the accuracy of a prognostic biomarker, we introduce a novel penalised-based estimator of the time-dependent ROC curve that accommodates a possible modifying effect of covariates. We consider flexible models for both the hazard function of the event time given the covariates and biomarker and for the location-scale regression model of the biomarker given covariates, enabling the accommodation of non-proportional hazards and nonlinear effects through penalised splines, thus overcoming limitations of earlier methods. The simulation study demonstrates that our approach successfully recovers the true functional form of the covariate-specific time-dependent ROC curve and the corresponding area under the curve across a variety of scenarios. Comparisons with existing methods further show that our approach performs favourably in multiple settings. Our approach is applied to evaluating the Global Registry of Acute Coronary Events risk score's ability to predict mortality after discharge in patients who have suffered an acute coronary syndrome and how this ability may vary with the left ventricular ejection fraction. An R package, CondTimeROC, implementing the proposed method is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13604v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mar\'ia Xos\'e Rodr\'iguez-\'Alvarez, Vanda In\'acio</dc:creator>
    </item>
    <item>
      <title>Parsimonious Compactly Supported Covariance Models in the Gauss Hypergeometric Class: Identifiability, Reparameterizations, and Asymptotic Properties</title>
      <link>https://arxiv.org/abs/2506.13646</link>
      <description>arXiv:2506.13646v1 Announce Type: new 
Abstract: We study the covariance model belonging to the Gauss hypergeometric ($GH$) class, a highly flexible and compactly supported correlation model (kernel). This class includes the well-known Generalized Wendland ($GW$) and Mat\'ern ($MT$) kernels as special cases. First, we provide necessary and sufficient conditions for the validity of the $\mathcal{GH}$ model. We then demonstrate that this family of models suffers from identifiability issues under both increasing and fixed-domain asymptotics.
  To address this problem, we propose a parsimonious version of the model that adheres to an optimality criterion. This approach results in a new class of compactly supported kernels, which can be seen as an improvement over the $\mathcal{GW}$ model. Additionally, we show that two specific compact support reparameterizations allow us to recover the $MT$ model, highlighting the advantages and disadvantages of each reparameterization. Finally, we establish strong consistency and the asymptotic distribution of the maximum likelihood estimator of the microergodic parameter associated with the proposed parsimonious model under fixed-domain asymptotics. The effectiveness of our proposal is illustrated through a simulation study exploring the finite-sample properties of the MLE under both increasing- and fixed-domain asymptotics and
  the analysis of a georeferenced climate dataset,
  using both Gaussian and Tukey-$h$ random fields. In this application, the proposed model outperforms the the $MT$ model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13646v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moreno Bevilacqua, Christian Caama\~no-Carrillo, Tarik Faouzi, Xavier Emery</dc:creator>
    </item>
    <item>
      <title>Do more observations bring more information in rare events?</title>
      <link>https://arxiv.org/abs/2506.13671</link>
      <description>arXiv:2506.13671v1 Announce Type: new 
Abstract: It is generally believed that more observations provide more information. However, we observe that in the independence test for rare events, the power of the test is, surprisingly, determined by the number of rare events rather than the total sample size. Moreover, the correlations tend to shrink to zero even as the total sample size increases, as long as the proportion of rare events decreases. We demonstrate this phenomenon in both fixed and high-dimensional settings. To address these issues, we first rescale the covariances to account for the presence of rare events. We then propose a boosted procedure that uses only a small subset of non-rare events, yet achieves nearly the same power as using the full set of observations. As a result, computational complexity is significantly reduced. The theoretical properties, including asymptotic distribution and local power analysis, are carefully derived for both the rescaled statistic based on the full sample and the boosted test statistic based on subsampling. Furthermore, we extend the theory to multi-class rare events. Extensive simulations and real-world data analyses confirm the effectiveness and computational efficiency of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13671v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danyang Huang, Liyuan Wang, Liping Zhu</dc:creator>
    </item>
    <item>
      <title>Probabilistic patient risk profiling with pair-copula constructions</title>
      <link>https://arxiv.org/abs/2506.13731</link>
      <description>arXiv:2506.13731v1 Announce Type: new 
Abstract: We present, to our knowledge, the first clinical application of vine copula-based classifiers for probabilistic perioperative risk prediction. We obtain full joint probability models for mixed continuous-ordinal variables by fitting a separate vine copula to each outcome class, capturing nonlinear and tail-asymmetric dependence. In a cohort of 767 elective bowel surgeries (81 serious vs. 686 non-serious complications), posterior probabilities from the fitted classification models are used to allocate patients into low-, moderate-, and high-risk groups. Compared to weighted logistic regression and random forests with stratified sampling, the vine copula-based classifiers achieve up to 10% lower class-specific Brier scores and negative log-likelihoods on the out-of-sample. The vine copula-based classifier identifies a larger cohort of true low-risk patients potentially eligible for early discharge. Scenario analyses based on the fitted vine copula models provide interpretable risk profiles, including nonlinear relationships between body mass index, surgery duration, and blood loss, which might remain undetected under linear models. These results demonstrate that vine copula-based classifiers offer a reliable and interpretable framework for individualized, probability-based patient risk profiling. As such, they represent a promising tool for data-driven decision making in perioperative care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13731v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\"Ozge \c{S}ahin</dc:creator>
    </item>
    <item>
      <title>Temporal cross-validation impacts multivariate time series subsequence anomaly detection evaluation</title>
      <link>https://arxiv.org/abs/2506.12183</link>
      <description>arXiv:2506.12183v1 Announce Type: cross 
Abstract: Evaluating anomaly detection in multivariate time series (MTS) requires careful consideration of temporal dependencies, particularly when detecting subsequence anomalies common in fault detection scenarios. While time series cross-validation (TSCV) techniques aim to preserve temporal ordering during model evaluation, their impact on classifier performance remains underexplored. This study systematically investigates the effect of TSCV strategy on the precision-recall characteristics of classifiers trained to detect fault-like anomalies in MTS datasets. We compare walk-forward (WF) and sliding window (SW) methods across a range of validation partition configurations and classifier types, including shallow learners and deep learning (DL) classifiers. Results show that SW consistently yields higher median AUC-PR scores and reduced fold-to-fold performance variance, particularly for deep architectures sensitive to localized temporal continuity. Furthermore, we find that classifier generalization is sensitive to the number and structure of temporal partitions, with overlapping windows preserving fault signatures more effectively at lower fold counts. A classifier-level stratified analysis reveals that certain algorithms, such as random forests (RF), maintain stable performance across validation schemes, whereas others exhibit marked sensitivity. This study demonstrates that TSCV design in benchmarking anomaly detection models on streaming time series and provide guidance for selecting evaluation strategies in temporally structured learning environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12183v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven C. Hespeler, Pablo Moriano, Mingyan Li, Samuel C. Hollifield</dc:creator>
    </item>
    <item>
      <title>The empirical discrete copula process</title>
      <link>https://arxiv.org/abs/2506.12316</link>
      <description>arXiv:2506.12316v1 Announce Type: cross 
Abstract: This paper develops a general inferential framework for discrete copulas on finite supports in any dimension. The copula of a multivariate discrete distribution is defined as Csiszar's I-projection (i.e., the minimum-Kullback-Leibler divergence projection) of its joint probability array onto the polytope of uniform-margins probability arrays of the same size, and its empirical estimator is obtained by applying that same projection to the array of empirical frequencies observed on the sample. Under the assumption of random sampling, strong consistency and root-n-asymptotic normality of the empirical copula array is established, with an explicit "sandwich" form for its covariance. The theory is illustrated by deriving the large-sample distribution of Yule's concordance coefficient (the natural analogue of Spearman's rho for bivariate discrete distributions) and by constructing a test for quasi-independence in multivariate contingency tables. Our results not only complete the foundations of discrete-copula inference but also connect directly to entropically regularised optimal transport and other minimum-divergence problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12316v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Gery Geenens, Ivan Kojadinovic, Tommaso Martini</dc:creator>
    </item>
    <item>
      <title>A Transfer Learning Framework for Multilayer Networks via Model Averaging</title>
      <link>https://arxiv.org/abs/2506.12455</link>
      <description>arXiv:2506.12455v1 Announce Type: cross 
Abstract: Link prediction in multilayer networks is a key challenge in applications such as recommendation systems and protein-protein interaction prediction. While many techniques have been developed, most rely on assumptions about shared structures and require access to raw auxiliary data, limiting their practicality. To address these issues, we propose a novel transfer learning framework for multilayer networks using a bi-level model averaging method. A $K$-fold cross-validation criterion based on edges is used to automatically weight inter-layer and intra-layer candidate models. This enables the transfer of information from auxiliary layers while mitigating model uncertainty, even without prior knowledge of shared structures. Theoretically, we prove the optimality and weight convergence of our method under mild conditions. Computationally, our framework is efficient and privacy-preserving, as it avoids raw data sharing and supports parallel processing across multiple servers. Simulations show our method outperforms others in predictive accuracy and robustness. We further demonstrate its practical value through two real-world recommendation system applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12455v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongqin Qiu, Xinyu Zhang</dc:creator>
    </item>
    <item>
      <title>Moment Restrictions for Nonlinear Panel Data Models with Feedback</title>
      <link>https://arxiv.org/abs/2506.12569</link>
      <description>arXiv:2506.12569v1 Announce Type: cross 
Abstract: Many panel data methods, while allowing for general dependence between covariates and time-invariant agent-specific heterogeneity, place strong a priori restrictions on feedback: how past outcomes, covariates, and heterogeneity map into future covariate levels. Ruling out feedback entirely, as often occurs in practice, is unattractive in many dynamic economic settings. We provide a general characterization of all feedback and heterogeneity robust (FHR) moment conditions for nonlinear panel data models and present constructive methods to derive feasible moment-based estimators for specific models. We also use our moment characterization to compute semiparametric efficiency bounds, allowing for a quantification of the information loss associated with accommodating feedback, as well as providing insight into how to construct estimators with good efficiency properties in practice. Our results apply both to the finite dimensional parameter indexing the parametric part of the model as well as to estimands that involve averages over the distribution of unobserved heterogeneity. We illustrate our methods by providing a complete characterization of all FHR moment functions in the multi-spell mixed proportional hazards model. We compute efficient moment functions for both model parameters and average effects in this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12569v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>St\'ephane Bonhomme, Kevin Dano, Bryan S. Graham</dc:creator>
    </item>
    <item>
      <title>Finite sample-optimal adjustment sets in linear Gaussian causal models</title>
      <link>https://arxiv.org/abs/2506.12869</link>
      <description>arXiv:2506.12869v1 Announce Type: cross 
Abstract: Traditional covariate selection methods for causal inference focus on achieving unbiasedness and asymptotic efficiency. In many practical scenarios, researchers must estimate causal effects from observational data with limited sample sizes or in cases where covariates are difficult or costly to measure. Their needs might be better met by selecting adjustment sets that are finite sample-optimal in terms of mean squared error. In this paper, we aim to find the adjustment set that minimizes the mean squared error of the causal effect estimator, taking into account the joint distribution of the variables and the sample size. We call this finite sample-optimal set the MSE-optimal adjustment set and present examples in which the MSE-optimal adjustment set differs from the asymptotically optimal adjustment set. To identify the MSE-optimal adjustment set, we then introduce a sample size criterion for comparing adjustment sets in linear Gaussian models. We also develop graphical criteria to reduce the search space for this adjustment set based on the causal graph. In experiments with simulated data, we show that the MSE-optimal adjustment set can outperform the asymptotically optimal adjustment set in finite sample size settings, making causal inference more practical in such scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12869v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nadja Rutsch, Sara Magliacane, St\'ephanie van der Pas</dc:creator>
    </item>
    <item>
      <title>Delving Into the Psychology of Machines: Exploring the Structure of Self-Regulated Learning via LLM-Generated Survey Responses</title>
      <link>https://arxiv.org/abs/2506.13384</link>
      <description>arXiv:2506.13384v1 Announce Type: cross 
Abstract: Large language models (LLMs) offer the potential to simulate human-like responses and behaviors, creating new opportunities for psychological science. In the context of self-regulated learning (SRL), if LLMs can reliably simulate survey responses at scale and speed, they could be used to test intervention scenarios, refine theoretical models, augment sparse datasets, and represent hard-to-reach populations. However, the validity of LLM-generated survey responses remains uncertain, with limited research focused on SRL and existing studies beyond SRL yielding mixed results. Therefore, in this study, we examined LLM-generated responses to the 44-item Motivated Strategies for Learning Questionnaire (MSLQ; Pintrich \&amp; De Groot, 1990), a widely used instrument assessing students' learning strategies and academic motivation. Particularly, we used the LLMs GPT-4o, Claude 3.7 Sonnet, Gemini 2 Flash, LLaMA 3.1-8B, and Mistral Large. We analyzed item distributions, the psychological network of the theoretical SRL dimensions, and psychometric validity based on the latent factor structure. Our results suggest that Gemini 2 Flash was the most promising LLM, showing considerable sampling variability and producing underlying dimensions and theoretical relationships that align with prior theory and empirical findings. At the same time, we observed discrepancies and limitations, underscoring both the potential and current constraints of using LLMs for simulating psychological survey data and applying it in educational contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13384v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonie V. D. E. Vogelsmeier, Eduardo Oliveira, Kamila Misiejuk, Sonsoles L\'opez-Pernas, Mohammed Saqr</dc:creator>
    </item>
    <item>
      <title>Digging deeper: deep joint species distribution modeling reveals environmental drivers of Earthworm Communities</title>
      <link>https://arxiv.org/abs/2506.13568</link>
      <description>arXiv:2506.13568v1 Announce Type: cross 
Abstract: Earthworms are key drivers of soil function, influencing organic matter turnover, nutrient cycling, and soil structure. Understanding the environmental controls on their distribution is essential for predicting the impacts of land use and climate change on soil ecosystems. While local studies have identified abiotic drivers of earthworm communities, broad-scale spatial patterns remain underexplored.
  We developed a multi-species, multi-task deep learning model to jointly predict the distribution of 77 earthworm species across metropolitan France, using historical (1960-1970) and contemporary (1990-2020) records. The model integrates climate, soil, and land cover variables to estimate habitat suitability. We applied SHapley Additive exPlanations (SHAP) to identify key environmental drivers and used species clustering to reveal ecological response groups.
  The joint model achieved high predictive performance (TSS &gt;= 0.7) and improved predictions for rare species compared to traditional species distribution models. Shared feature extraction across species allowed for more robust identification of common and contrasting environmental responses. Precipitation variability, temperature seasonality, and land cover emerged as dominant predictors of earthworm distribution. Species clustering revealed distinct ecological strategies tied to climatic and land use gradients.
  Our study advances both the methodological and ecological understanding of soil biodiversity. We demonstrate the utility of interpretable deep learning approaches for large-scale soil fauna modeling and provide new insights into earthworm habitat specialization. These findings support improved soil biodiversity monitoring and conservation planning in the face of global environmental change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13568v1</guid>
      <category>stat.AP</category>
      <category>q-bio.PE</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Si-moussi, Wilfried Thuiller, Esther Galbrun, Thibaud Deca\"ens, Sylvain G\'erard, Daniel F. March\'an, Claire Marsden, Yvan Capowiez, Micka\"el Hedde</dc:creator>
    </item>
    <item>
      <title>From Data-Driven to Purpose-Driven Artificial Intelligence: Systems Thinking for Data-Analytic Automation of Patient Care</title>
      <link>https://arxiv.org/abs/2506.13584</link>
      <description>arXiv:2506.13584v1 Announce Type: cross 
Abstract: In this work, we reflect on the data-driven modeling paradigm that is gaining ground in AI-driven automation of patient care. We argue that the repurposing of existing real-world patient datasets for machine learning may not always represent an optimal approach to model development as it could lead to undesirable outcomes in patient care. We reflect on the history of data analysis to explain how the data-driven paradigm rose to popularity, and we envision ways in which systems thinking and clinical domain theory could complement the existing model development approaches in reaching human-centric outcomes. We call for a purpose-driven machine learning paradigm that is grounded in clinical theory and the sociotechnical realities of real-world operational contexts. We argue that understanding the utility of existing patient datasets requires looking in two directions: upstream towards the data generation, and downstream towards the automation objectives. This purpose-driven perspective to AI system development opens up new methodological opportunities and holds promise for AI automation of patient care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13584v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Daniel Anadria, Roel Dobbe, Anastasia Giachanou, Ruurd Kuiper, Richard Bartels, \'I\~nigo Mart\'inez de Rituerto de Troya, Carmen Z\"urcher, Daniel Oberski</dc:creator>
    </item>
    <item>
      <title>Hybrid Meta-learners for Estimating Heterogeneous Treatment Effects</title>
      <link>https://arxiv.org/abs/2506.13680</link>
      <description>arXiv:2506.13680v1 Announce Type: cross 
Abstract: Estimating conditional average treatment effects (CATE) from observational data involves modeling decisions that differ from supervised learning, particularly concerning how to regularize model complexity. Previous approaches can be grouped into two primary "meta-learner" paradigms that impose distinct inductive biases. Indirect meta-learners first fit and regularize separate potential outcome (PO) models and then estimate CATE by taking their difference, whereas direct meta-learners construct and directly regularize estimators for the CATE function itself. Neither approach consistently outperforms the other across all scenarios: indirect learners perform well when the PO functions are simple, while direct learners outperform when the CATE is simpler than individual PO functions. In this paper, we introduce the Hybrid Learner (H-learner), a novel regularization strategy that interpolates between the direct and indirect regularizations depending on the dataset at hand. The H-learner achieves this by learning intermediate functions whose difference closely approximates the CATE without necessarily requiring accurate individual approximations of the POs themselves. We demonstrate empirically that intentionally allowing suboptimal fits to the POs improves the bias-variance tradeoff in estimating CATE. Experiments conducted on semi-synthetic and real-world benchmark datasets illustrate that the H-learner consistently operates at the Pareto frontier, effectively combining the strengths of both direct and indirect meta-learners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13680v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhongyuan Liang, Lars van der Laan, Ahmed Alaa</dc:creator>
    </item>
    <item>
      <title>Conformal Risk Control</title>
      <link>https://arxiv.org/abs/2208.02814</link>
      <description>arXiv:2208.02814v4 Announce Type: replace 
Abstract: We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an $\mathcal{O}(1/n)$ factor. We also introduce extensions of the idea to distribution shift, quantile risk control, multiple and adversarial risk control, and expectations of U-statistics. Worked examples from computer vision and natural language processing demonstrate the usage of our algorithm to bound the false negative rate, graph distance, and token-level F1-score.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.02814v4</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasios N. Angelopoulos, Stephen Bates, Adam Fisch, Lihua Lei, Tal Schuster</dc:creator>
    </item>
    <item>
      <title>The risks of risk assessment: causal blind spots when using prediction models for treatment decisions</title>
      <link>https://arxiv.org/abs/2402.17366</link>
      <description>arXiv:2402.17366v3 Announce Type: replace 
Abstract: Clinicians increasingly rely on prediction models to guide treatment choices. Most prediction models, however, are developed using observational data that include some patients who have already received the treatment the prediction model is meant to inform. Special attention to the causal role of those earlier treatments is required when interpreting the resulting predictions. We identify 'causal blind spots' in three common approaches to handling treatment when developing a prediction model: including treatment as a predictor, restricting to individuals taking a certain treatment, and ignoring treatment. Through several real examples, we illustrate how the risks obtained from models developed using such approaches may be misinterpreted and can lead to misinformed decision-making. Our discussion covers issues attributable to confounding, selection, mediation and changes in treatment protocols over time. We advocate for an extension of guidelines for the development, reporting and evaluation of prediction models to avoid such misinterpretations. Developers must ensure that the intended target population for the model, and the treatment conditions under which predictions hold, are clearly communicated. When prediction models are intended to inform treatment decisions, they need to provide estimates of risk under the specific treatment (or intervention) options being considered, known as 'prediction under interventions'. Next to suitable data, this requires causal reasoning and causal inference techniques during model development and evaluation. Being clear about what a given prediction model can and cannot be used for prevents misinformed treatment decisions and thereby prevents potential harm to patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17366v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nan van Geloven, Ruth H Keogh, Wouter van Amsterdam, Giovanni Cin\`a, Jesse H. Krijthe, Niels Peek, Kim Luijken, Sara Magliacane, Pawe{\l} Morzywo{\l}ek, Thijs van Ommen, Hein Putter, Matthew Sperrin, Junfeng Wang, Daniala L. Weir, Vanessa Didelez</dc:creator>
    </item>
    <item>
      <title>Simplifying debiased inference via automatic differentiation and probabilistic programming</title>
      <link>https://arxiv.org/abs/2405.08675</link>
      <description>arXiv:2405.08675v3 Announce Type: replace 
Abstract: We introduce an algorithm that simplifies the construction of efficient estimators, making them accessible to a broader audience. 'Dimple' takes as input computer code representing a parameter of interest and outputs an efficient estimator. Unlike standard approaches, it does not require users to derive a functional derivative known as the efficient influence function. Dimple avoids this task by applying automatic differentiation to the statistical functional of interest. Doing so requires expressing this functional as a composition of primitives satisfying a novel differentiability condition. Dimple also uses this composition to determine the nuisances it must estimate. In software, primitives can be implemented independently of one another and reused across different estimation problems. We provide a proof-of-concept Python implementation and showcase through examples how it allows users to go from parameter specification to efficient estimation with just a few lines of code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08675v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Luedtke</dc:creator>
    </item>
    <item>
      <title>An Efficient Sampling from Circular Distributions and its Extension to Toroidal Distributions</title>
      <link>https://arxiv.org/abs/2405.09149</link>
      <description>arXiv:2405.09149v3 Announce Type: replace 
Abstract: Sampling from circular distributions is a fundamental task in directional statistics. A key challenge in acceptance-rejection methods lies in selecting an efficient envelope density, as poor choices can lead to low acceptance rates and increased computational cost, especially in large-scale simulations. To address this, we propose a new sampling framework that utilizes the idea of upper Riemann sums to construct a piecewise envelope. This method ensures validity for any Riemann-integrable target density on a bounded interval. This method exhibits enhanced efficacy relative to the present sampling method for the von Mises distribution. Additionally, we introduce a flexible family of distributions defined on the surface of a curved torus, using its area element. The proposed sampling method is then employed to generate samples from the toroidal model. We explore the maximum entropy characterization and other theoretical properties of one of the marginal distributions arising from this construction for the von Mises distribution. To illustrate the practical utility of our framework, we apply the model to a real dataset on wind direction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09149v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Surojit Biswas, Buddhananda Banerjee</dc:creator>
    </item>
    <item>
      <title>Causal Inference with Outcomes Truncated by Death and Missing Not at Random</title>
      <link>https://arxiv.org/abs/2406.10554</link>
      <description>arXiv:2406.10554v4 Announce Type: replace 
Abstract: In clinical trials, principal stratification analysis is commonly employed to address the issue of truncation by death, where a subject dies before the outcome can be measured. However, in practice, many survivor outcomes may remain uncollected or be missing not at random, posing a challenge to standard principal stratification analyses. In this paper, we explore the identification, estimation, and bounds of the average treatment effect within a subpopulation of individuals who would potentially survive under both treatment and control conditions. We show that the causal parameter of interest can be identified by introducing a proxy variable that affects the outcome only through the principal strata, while requiring that the treatment variable does not directly affect the missingness mechanism. Subsequently, we propose an approach for estimating causal parameters and derive nonparametric bounds in cases where identification assumptions are violated. We illustrate the performance of the proposed method through simulation studies and a real dataset obtained from a Human Immunodeficiency Virus (HIV) study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10554v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Li, Yuan Liu, Shanshan Luo, Zhi Geng</dc:creator>
    </item>
    <item>
      <title>On integral priors for multiple comparison in Bayesian model selection</title>
      <link>https://arxiv.org/abs/2406.14184</link>
      <description>arXiv:2406.14184v4 Announce Type: replace 
Abstract: Noninformative priors constructed for estimation purposes are usually not appropriate for model selection and testing. The methodology of integral priors was developed to get prior distributions for Bayesian model selection when comparing two models, modifying initial improper reference priors. We propose a generalization of this methodology to more than two models. Our approach adds an artificial copy of each model under comparison by compactifying the parametric space and creating an ergodic Markov chain across all models that returns the integral priors as marginals of the stationary distribution. Besides the guarantee of their existence and the lack of paradoxes attached to estimation reference priors, an additional advantage of this methodology is that the simulation of this Markov chain is straightforward as it only requires simulations of imaginary training samples for all models and from the corresponding posterior distributions. This renders its implementation automatic and generic, both in the nested and in the non-nested cases. We present some examples, including situations where other methodologies need specific adjustments or do not produce a satisfactory answer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14184v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diego Salmer\'on, Juan Antonio Cano, Christian P. Robert</dc:creator>
    </item>
    <item>
      <title>A Bayesian joint model of multiple longitudinal and categorical outcomes with application to multiple myeloma using permutation-based variable importance</title>
      <link>https://arxiv.org/abs/2407.14311</link>
      <description>arXiv:2407.14311v3 Announce Type: replace 
Abstract: Joint models have proven to be an effective approach for uncovering potentially hidden connections between various types of outcomes, mainly continuous, time-to-event, and binary. Typically, longitudinal continuous outcomes are characterized by linear mixed-effects models, survival outcomes are described by proportional hazards models, and the link between outcomes are captured by shared random effects. Other modeling variations include generalized linear mixed-effects models for longitudinal data and logistic regression when a binary outcome is present, rather than time until an event of interest. However, in a clinical research setting, one might be interested in modeling the physician's chosen treatment based on the patient's medical history to identify prognostic factors. In this situation, there are often multiple treatment options, requiring the use of a multiclass classification approach. Inspired by this context, we develop a Bayesian joint model for longitudinal and categorical data. In particular, our motivation comes from a multiple myeloma study, in which biomarkers display nonlinear trajectories that are well captured through bi-exponential submodels, where patient-level information is shared with the categorical submodel. We also present a variable importance strategy to rank prognostic factors. We apply our proposal and a competing model to the multiple myeloma data, compare the variable importance and inferential results for both models, and illustrate patient-level interpretations using our joint model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14311v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danilo Alvares, Jessica K. Barrett, Fran\c{c}ois Mercier, Jochen Schulze, Sean Yiu, Felipe Castro, Spyros Roumpanis, Yajing Zhu</dc:creator>
    </item>
    <item>
      <title>Hierarchical Latent Class Models for Mortality Surveillance Using Partially Verified Verbal Autopsies</title>
      <link>https://arxiv.org/abs/2410.09274</link>
      <description>arXiv:2410.09274v3 Announce Type: replace 
Abstract: Monitoring cause-of-death data is an important part of understanding disease burdens and effects of public health interventions. Verbal autopsy (VA) is a well-established method for gathering information about deaths outside of hospitals by conducting an interview to caregivers of a deceased person. It is usually the only tool for cause-of-death surveillance in low-resource settings. A critical limitation with current practices of VA analysis is that all algorithms require either domain knowledge about symptom-cause relationships or large labeled datasets for model training. Therefore, they cannot be easily adopted during public health emergencies when new diseases emerge with rapidly evolving epidemiological patterns. In this paper, we consider estimating the fraction of deaths due to an emerging disease. We develop a novel Bayesian framework using hierarchical latent class models to account for the informative cause-of-death verification process. Our model flexibly captures the joint distribution of symptoms and how they change over time in different sub-populations. We also propose structured priors to improve the precision of the cause-specific mortality estimates for small sub-populations. Our model is motivated by mortality surveillance of COVID-19 related deaths in low-resource settings. We apply our method to a dataset that includes suspected COVID-19 related deaths in Brazil in 2021.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09274v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Zhu, Zehang Richard Li</dc:creator>
    </item>
    <item>
      <title>Sample size and power calculations for causal inference of observational studies</title>
      <link>https://arxiv.org/abs/2501.11181</link>
      <description>arXiv:2501.11181v3 Announce Type: replace 
Abstract: This paper investigates the theoretical foundation and develops analytical formulas for sample size and power calculations for causal inference with observational data. By analyzing the variance of the inverse probability weighting estimator of the average treatment effect, we decompose the power calculations into three components: propensity score distribution, potential outcome distribution, and their correlation. We show that to determine the minimal sample size of an observational study, it is sufficient under mild conditions to have two parameters additional to the standard inputs in the power calculation of randomized trials, which quantify the strength of the confounder-treatment and the confounder-outcome association, respectively. For the former, we propose using the Bhattacharyya coefficient, which measures the covariate overlap and, together with the treatment proportion, leads to a uniquely identifiable and easily computable propensity score distribution. For the latter, we propose a sensitivity parameter bounded by the R-squared statistic of the regression of the outcome on covariates. Utilizing the Lyapunov Central Limit Theorem on the linear combination of covariates, our procedure does not require distributional assumptions on the multivariate covariates. We develop an associated R package PSpower.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11181v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bo Liu, Fan Li</dc:creator>
    </item>
    <item>
      <title>Optimal treatment regimes for the net benefit of a treatment</title>
      <link>https://arxiv.org/abs/2503.22580</link>
      <description>arXiv:2503.22580v2 Announce Type: replace 
Abstract: We develop a mathematical framework to define an optimal individualized treatment rule (ITR) within the context of prioritized outcomes in a randomized controlled trial. Our optimality criterion is based on the framework of generalized pairwise comparisons. We propose two approaches for estimating optimal ITRs on a pairwise basis. The first approach is a variant of the k-nearest neighbors algorithm. The second approach is a meta-learning method based on a randomized bagging scheme, which enables the use of any classification algorithm to construct an ITR. We investigate the theoretical properties of these estimation procedures, evaluate their performance through Monte Carlo simulations, and demonstrate their application to clinical trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22580v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Petit, G\'erard Biau, Rapha\"el Porcher</dc:creator>
    </item>
    <item>
      <title>A Bayesian Multisource Fusion Model for Spatiotemporal PM2.5 in an Urban Setting</title>
      <link>https://arxiv.org/abs/2506.10688</link>
      <description>arXiv:2506.10688v2 Announce Type: replace 
Abstract: Airborne particulate matter (PM2.5) is a major public health concern in urban environments, where population density and emission sources exacerbate exposure risks. We present a novel Bayesian spatiotemporal fusion model to estimate monthly PM2.5 concentrations over Greater London (2014-2019) at 1km resolution. The model integrates multiple PM2.5 data sources, including outputs from two atmospheric air quality dispersion models and predictive variables, such as vegetation and satellite aerosol optical depth, while explicitly modelling a latent spatiotemporal field. Spatial misalignment of the data is addressed through an upscaling approach to predict across the entire area. Building on stochastic partial differential equations (SPDE) within the integrated nested Laplace approximations (INLA) framework, our method introduces spatially- and temporally-varying coefficients to flexibly calibrate datasets and capture fine-scale variability. Model performance and complexity are balanced using predictive metrics such as the predictive model choice criterion and thorough cross-validation. The best performing model shows excellent fit and solid predictive performance, enabling reliable high-resolution spatiotemporal mapping of PM2.5 concentrations with the associated uncertainty. Furthermore, the model outputs, including full posterior predictive distributions, can be used to map exceedance probabilities of regulatory thresholds, supporting air quality management and targeted interventions in vulnerable urban areas, as well as providing refined exposure estimates of PM2.5 for epidemiological applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10688v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abi I. Riley, Marta Blangiardo, Fr\'ed\'eric B. Piel, Andrew Beddows, Sean Beevers, Gary W. Fuller, Paul Agnew, Monica Pirani</dc:creator>
    </item>
    <item>
      <title>CATE Estimation With Potential Outcome Imputation From Local Regression</title>
      <link>https://arxiv.org/abs/2311.03630</link>
      <description>arXiv:2311.03630v2 Announce Type: replace-cross 
Abstract: One of the most significant challenges in Conditional Average Treatment Effect (CATE) estimation is the statistical discrepancy between distinct treatment groups. To address this issue, we propose a model-agnostic data augmentation method for CATE estimation. First, we derive regret bounds for general data augmentation methods suggesting that a small imputation error may be necessary for accurate CATE estimation. Inspired by this idea, we propose a contrastive learning approach that reliably imputes missing potential outcomes for a selected subset of individuals formed using a similarity measure. We augment the original dataset with these reliable imputations to reduce the discrepancy between different treatment groups while inducing minimal imputation error. The augmented dataset can subsequently be employed to train standard CATE estimation models. We provide both theoretical guarantees and extensive numerical studies demonstrating the effectiveness of our approach in improving the accuracy and robustness of numerous CATE estimation models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03630v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmed Aloui, Juncheng Dong, Cat P. Le, Vahid Tarokh</dc:creator>
    </item>
    <item>
      <title>Physics-Informed Priors with Application to Boundary Layer Velocity</title>
      <link>https://arxiv.org/abs/2311.12978</link>
      <description>arXiv:2311.12978v3 Announce Type: replace-cross 
Abstract: One of the most popular recent areas of machine learning predicates the use of neural networks augmented by information about the underlying process in the form of Partial Differential Equations (PDEs). These physics-informed neural networks are obtained by penalizing the inference with a PDE, and have been cast as a minimization problem currently lacking a formal approach to quantify the uncertainty. In this work, we propose a novel model-based framework which regards the PDE as a prior information of a deep Bayesian neural network. The prior is calibrated without data to resemble the PDE solution in the prior mean, while our degree in confidence on the PDE with respect to the data is expressed in terms of the prior variance. The information embedded in the PDE is then propagated to the posterior yielding physics-informed forecasts with uncertainty quantification. We apply our approach to a simulated viscous fluid and to experimentally-obtained turbulent boundary layer velocity in a water tunnel using an appropriately simplified Navier-Stokes equation. Our approach requires very few observations to produce physically-consistent forecasts as opposed to non-physical forecasts stemming from non-informed priors, thereby allowing forecasting complex systems where some amount of data as well as some contextual knowledge is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12978v3</guid>
      <category>physics.flu-dyn</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luca Menicali, David H. Richter, Stefano Castruccio</dc:creator>
    </item>
    <item>
      <title>On importance sampling and independent Metropolis-Hastings with an unbounded weight function</title>
      <link>https://arxiv.org/abs/2411.09514</link>
      <description>arXiv:2411.09514v2 Announce Type: replace-cross 
Abstract: Importance sampling and independent Metropolis-Hastings (IMH) are among the fundamental building blocks of Monte Carlo methods. Both require a proposal distribution that globally approximates the target distribution. The Radon-Nikodym derivative of the target distribution relative to the proposal is called the weight function. Under the assumption that the weight is unbounded but has finite moments under the proposal distribution, we study the approximation error of importance sampling and of the particle independent Metropolis-Hastings algorithm (PIMH), which includes IMH as a special case. For the chains generated by such algorithms, we show that the common random numbers coupling is maximal. Using that coupling we derive bounds on the total variation distance of a PIMH chain to its target distribution. Our results allow a formal comparison of the finite-time biases of importance sampling and IMH, and we find the latter to be have a smaller bias. We further consider bias removal techniques using couplings, and provide conditions under which the resulting unbiased estimators have finite moments. These unbiased estimators provide an alternative to self-normalized importance sampling, implementable in the same settings. We compare their asymptotic efficiency as the number of particles goes to infinity, and consider their use in robust mean estimation techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09514v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>George Deligiannidis (University of Oxford), Pierre E. Jacob (ESSEC Business School), El Mahdi Khribch (ESSEC Business School), Guanyang Wang (Rutgers University)</dc:creator>
    </item>
    <item>
      <title>The Landscape of Causal Discovery Data: Grounding Causal Discovery in Real-World Applications</title>
      <link>https://arxiv.org/abs/2412.01953</link>
      <description>arXiv:2412.01953v2 Announce Type: replace-cross 
Abstract: Causal discovery aims to automatically uncover causal relationships from data, a capability with significant potential across many scientific disciplines. However, its real-world applications remain limited. Current methods often rely on unrealistic assumptions and are evaluated only on simple synthetic toy datasets, often with inadequate evaluation metrics. In this paper, we substantiate these claims by performing a systematic review of the recent causal discovery literature. We present applications in biology, neuroscience, and Earth sciences - fields where causal discovery holds promise for addressing key challenges. We highlight available simulated and real-world datasets from these domains and discuss common assumption violations that have spurred the development of new methods. Our goal is to encourage the community to adopt better evaluation practices by utilizing realistic datasets and more adequate metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01953v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippe Brouillard, Chandler Squires, Jonas Wahl, Konrad P. Kording, Karen Sachs, Alexandre Drouin, Dhanya Sridhar</dc:creator>
    </item>
    <item>
      <title>Robust Conformal Outlier Detection under Contaminated Reference Data</title>
      <link>https://arxiv.org/abs/2502.04807</link>
      <description>arXiv:2502.04807v2 Announce Type: replace-cross 
Abstract: Conformal prediction is a flexible framework for calibrating machine learning predictions, providing distribution-free statistical guarantees. In outlier detection, this calibration relies on a reference set of labeled inlier data to control the type-I error rate. However, obtaining a perfectly labeled inlier reference set is often unrealistic, and a more practical scenario involves access to a contaminated reference set containing a small fraction of outliers. This paper analyzes the impact of such contamination on the validity of conformal methods. We prove that under realistic, non-adversarial settings, calibration on contaminated data yields conservative type-I error control, shedding light on the inherent robustness of conformal methods. This conservativeness, however, typically results in a loss of power. To alleviate this limitation, we propose a novel, active data-cleaning framework that leverages a limited labeling budget and an outlier detection model to selectively annotate data points in the contaminated reference set that are suspected as outliers. By removing only the annotated outliers in this ``suspicious'' subset, we can effectively enhance power while mitigating the risk of inflating the type-I error rate, as supported by our theoretical analysis. Experiments on real datasets validate the conservative behavior of conformal methods under contamination and show that the proposed data-cleaning strategy improves power without sacrificing validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04807v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meshi Bashari, Matteo Sesia, Yaniv Romano</dc:creator>
    </item>
    <item>
      <title>A new and flexible class of sharp asymptotic time-uniform confidence sequences</title>
      <link>https://arxiv.org/abs/2502.10380</link>
      <description>arXiv:2502.10380v2 Announce Type: replace-cross 
Abstract: Confidence sequences are anytime-valid analogues of classical confidence intervals that do not suffer from multiplicity issues under optional continuation of the data collection. As in classical statistics, asymptotic confidence sequences are a nonparametric tool showing under which high-level assumptions asymptotic coverage is achieved so that they also give a certain robustness guarantee against distributional deviations. In this paper, we propose a new flexible class of confidence sequences yielding sharp asymptotic time-uniform confidence sequences under mild assumptions. Furthermore, we highlight the connection to corresponding sequential testing problems and detail the underlying limit theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10380v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spl.2025.110462</arxiv:DOI>
      <arxiv:journal_reference>Statistics &amp; Probability Letters, Volume 226, 2025, 110462, ISSN 0167-7152</arxiv:journal_reference>
      <dc:creator>Felix Gnettner, Claudia Kirch</dc:creator>
    </item>
    <item>
      <title>Survey on Algorithms for multi-index models</title>
      <link>https://arxiv.org/abs/2504.05426</link>
      <description>arXiv:2504.05426v2 Announce Type: replace-cross 
Abstract: We review the literature on algorithms for estimating the index space in a multi-index model. The primary focus is on computationally efficient (polynomial-time) algorithms in Gaussian space, the assumptions under which consistency is guaranteed by these methods, and their sample complexity. In many cases, a gap is observed between the sample complexity of the best known computationally efficient methods and the information-theoretical minimum. We also review algorithms based on estimating the span of gradients using nonparametric methods, and algorithms based on fitting neural networks using gradient descent</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05426v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joan Bruna, Daniel Hsu</dc:creator>
    </item>
    <item>
      <title>Inference of Hierarchical Core-Periphery Structure in Temporal Networks</title>
      <link>https://arxiv.org/abs/2506.10135</link>
      <description>arXiv:2506.10135v2 Announce Type: replace-cross 
Abstract: Networks can have various types of mesoscale structures. One type of mesoscale structure in networks is core-periphery structure, which consists of densely-connected core nodes and sparsely-connected peripheral nodes. The core nodes are connected densely to each other and can be connected to the peripheral nodes, which are connected sparsely to other nodes. There has been much research on core-periphery structure in time-independent networks, but few core-periphery detection methods have been developed for time-dependent (i.e., ``temporal") networks. Using a multilayer-network representation of temporal networks and an inference approach that employs stochastic block models, we generalize a recent method for detecting hierarchical core-periphery structure \cite{Polanco23} from time-independent networks to temporal networks. In contrast to ``onion-like'' nested core-periphery structures (where each node is assigned to a group according to how deeply it is nested in a network's core), hierarchical core-periphery structures encompass networks with nested structures, tree-like structures (where any two groups must either be disjoint or have one as a strict subset of the other), and general non-nested mesoscale structures (where the group assignments of nodes do not have to be nested in any way). To perform statistical inference and thereby identify core-periphery structure, we use a Markov-chain Monte Carlo (MCMC) approach. We illustrate our method for detecting hierarchical core-periphery structure in two real-world temporal networks, and we briefly discuss the structures that we identify in these networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10135v2</guid>
      <category>cs.SI</category>
      <category>math.CO</category>
      <category>physics.data-an</category>
      <category>physics.soc-ph</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Theodore Y. Faust, Mason A. Porter</dc:creator>
    </item>
  </channel>
</rss>
