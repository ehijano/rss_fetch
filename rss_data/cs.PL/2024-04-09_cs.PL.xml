<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Apr 2024 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Predictable Verification using Intrinsic Definitions</title>
      <link>https://arxiv.org/abs/2404.04515</link>
      <description>arXiv:2404.04515v1 Announce Type: new 
Abstract: We propose a novel mechanism of defining data structures using intrinsic definitions that avoids recursion and instead utilizes monadic maps satisfying local conditions. We show that intrinsic definitions are a powerful mechanism that can capture a variety of data structures naturally. We show that they also enable a predictable verification methodology that allows engineers to write ghost code to update monadic maps and perform verification using reduction to decidable logics. We evaluate our methodology using Boogie and prove a suite of data structure manipulating programs correct.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04515v1</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adithya Murali, Cody Rivera, P. Madhusudan</dc:creator>
    </item>
    <item>
      <title>Compilation of Modular and General Sparse Workspaces</title>
      <link>https://arxiv.org/abs/2404.04541</link>
      <description>arXiv:2404.04541v1 Announce Type: new 
Abstract: Recent years have seen considerable work on compiling sparse tensor algebra expressions. This paper addresses a shortcoming in that work, namely how to generate efficient code (in time and space) that scatters values into a sparse result tensor. We address this shortcoming through a compiler design that generates code that uses sparse intermediate tensors (sparse workspaces) as efficient adapters between compute code that scatters and result tensors that do not support random insertion. Our compiler automatically detects sparse scattering behavior in tensor expressions and inserts necessary intermediate workspace tensors. We present an algorithm template for workspace insertion that is the backbone of our code generation algorithm. Our algorithm template is modular by design, supporting sparse workspaces that span multiple user-defined implementations. Our evaluation shows that sparse workspaces can be up to 27.12$\times$ faster than the dense workspaces of prior work. On the other hand, dense workspaces can be up to 7.58$\times$ faster than the sparse workspaces generated by our compiler in other situations, which motivates our compiler design that supports both. Our compiler produces sequential code that is competitive with hand-optimized linear and tensor algebra libraries on the expressions they support, but that generalizes to any other expression. Sparse workspaces are also more memory efficient than dense workspaces as they compress away zeros. This compression can asymptotically decrease memory usage, enabling tensor computations on data that would otherwise run out of memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04541v1</guid>
      <category>cs.PL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Genghan Zhang, Olivia Hsu, Fredrik Kjolstad</dc:creator>
    </item>
    <item>
      <title>IsoPredict: Dynamic Predictive Analysis for Detecting Unserializable Behaviors in Weakly Isolated Data Store Applications</title>
      <link>https://arxiv.org/abs/2404.04621</link>
      <description>arXiv:2404.04621v1 Announce Type: new 
Abstract: This paper presents the first dynamic predictive analysis for data store applications under weak isolation levels, called Isopredict. Given an observed serializable execution of a data store application, Isopredict generates and solves SMT constraints to find an unserializable execution that is a feasible execution of the application. Isopredict introduces novel techniques that handle divergent application behavior; solve mutually recursive sets of constraints; and balance coverage, precision, and performance. An evaluation on four transactional data store benchmarks shows that Isopredict often predicts unserializable behaviors, 99% of which are feasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04621v1</guid>
      <category>cs.PL</category>
      <category>cs.DB</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3656391</arxiv:DOI>
      <arxiv:journal_reference>Proc. ACM Program. Lang., Vol. 8, No. PLDI, Article 161. Publication date: June 2024</arxiv:journal_reference>
      <dc:creator>Chujun Geng, Spyros Blanas, Michael D. Bond, Yang Wang</dc:creator>
    </item>
    <item>
      <title>SAT-DIFF: A Tree Diffing Framework Using SAT Solving</title>
      <link>https://arxiv.org/abs/2404.04731</link>
      <description>arXiv:2404.04731v1 Announce Type: new 
Abstract: Computing differences between tree-structured data is a critical but challenging problem in software analysis. In this paper, we propose a novel tree diffing approach called SatDiff, which reformulates the structural diffing problem into a MaxSAT problem. By encoding the necessary transformations from the source tree to the target tree, SatDiff generates correct, minimal, and type safe low-level edit scripts with formal guarantees. We then synthesize concise high-level edit scripts by effectively merging low-level edits in the appropriate topological order. Our empirical results demonstrate that SatDiff outperforms existing heuristic-based approaches by a significant margin in terms of conciseness while maintaining a reasonable runtime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04731v1</guid>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chuqin Geng, Haolin Ye, Yihan Zhang, Brigitte Pientka, Xujie Si</dc:creator>
    </item>
    <item>
      <title>From Batch to Stream: Automatic Generation of Online Algorithms</title>
      <link>https://arxiv.org/abs/2404.04743</link>
      <description>arXiv:2404.04743v1 Announce Type: new 
Abstract: Online streaming algorithms, tailored for continuous data processing, offer substantial benefits but are often more intricate to design than their offline counterparts. This paper introduces a novel approach for automatically synthesizing online streaming algorithms from their offline versions. In particular, we propose a novel methodology, based on the notion of relational function signature (RFS), for deriving an online algorithm given its offline version. Then, we propose a concrete synthesis algorithm that is an instantiation of the proposed methodology. Our algorithm uses the RFS to decompose the synthesis problem into a set of independent subtasks and uses a combination of symbolic reasoning and search to solve each subproblem. We implement the proposed technique in a new tool called Opera and evaluate it on over 50 tasks spanning two domains: statistical computations and online auctions. Our results show that Opera can automatically derive the online version of the original algorithm for 98% of the tasks. Our experiments also demonstrate that Opera significantly outperforms alternative approaches, including adaptations of SyGuS solvers to this problem as well as two of Opera's own ablations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04743v1</guid>
      <category>cs.PL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3656418</arxiv:DOI>
      <arxiv:journal_reference>Proc. ACM Program. Lang. 8, PLDI, Article 188 (June 2024), 32 pages</arxiv:journal_reference>
      <dc:creator>Ziteng Wang, Shankara Pailoor, Aaryan Prakash, Yuepeng Wang, Isil Dillig</dc:creator>
    </item>
    <item>
      <title>KATch: A Fast Symbolic Verifier for NetKAT</title>
      <link>https://arxiv.org/abs/2404.04760</link>
      <description>arXiv:2404.04760v1 Announce Type: new 
Abstract: We develop new data structures and algorithms for checking verification queries in NetKAT, a domain-specific language for specifying the behavior of network data planes. Our results extend the techniques obtained in prior work on symbolic automata and provide a framework for building efficient and scalable verification tools. We present \KATch, an implementation of these ideas in Scala, featuring an extended set of NetKAT operators that are useful for expressing network-wide specifications, and a verification engine that constructs a bisimulation or generates a counter-example showing that none exists. We evaluate the performance of our implementation on real-world and synthetic benchmarks, verifying properties such as reachability and slice isolation, typically returning a result in well under a second, which is orders of magnitude faster than previous approaches. Our advancements underscore NetKAT's potential as a practical, declarative language for network specification and verification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04760v1</guid>
      <category>cs.PL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark Moeller, Jules Jacobs, Olivier Savary Belanger, David Darais, Cole Schlesinger, Steffen Smolka, Nate Foster, Alexandra Silva</dc:creator>
    </item>
    <item>
      <title>Allo: A Programming Model for Composable Accelerator Design</title>
      <link>https://arxiv.org/abs/2404.04815</link>
      <description>arXiv:2404.04815v1 Announce Type: new 
Abstract: Special-purpose hardware accelerators are increasingly pivotal for sustaining performance improvements in emerging applications, especially as the benefits of technology scaling continue to diminish. However, designers currently lack effective tools and methodologies to construct complex, high-performance accelerator architectures in a productive manner. Existing high-level synthesis (HLS) tools often require intrusive source-level changes to attain satisfactory quality of results. Despite the introduction of several new accelerator design languages (ADLs) aiming to enhance or replace HLS, their advantages are more evident in relatively simple applications with a single kernel. Existing ADLs prove less effective for realistic hierarchical designs with multiple kernels, even if the design hierarchy is flattened.
  In this paper, we introduce Allo, a composable programming model for efficient spatial accelerator design. Allo decouples hardware customizations, including compute, memory, communication, and data type from algorithm specification, and encapsulates them as a set of customization primitives. Allo preserves the hierarchical structure of an input program by combining customizations from different functions in a bottom-up, type-safe manner. This approach facilitates holistic optimizations that span across function boundaries. We conduct comprehensive experiments on commonly-used HLS benchmarks and several realistic deep learning models. Our evaluation shows that Allo can outperform state-of-the-art HLS tools and ADLs on all test cases in the PolyBench. For the GPT2 model, the inference latency of the Allo generated accelerator is 1.7x faster than the NVIDIA A100 GPU with 5.4x higher energy efficiency, demonstrating the capability of Allo to handle large-scale designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04815v1</guid>
      <category>cs.PL</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3656401</arxiv:DOI>
      <dc:creator>Hongzheng Chen, Niansong Zhang, Shaojie Xiang, Zhichen Zeng, Mengjia Dai, Zhiru Zhang</dc:creator>
    </item>
    <item>
      <title>WebPie: A Tiny Slice of Dependent Typing</title>
      <link>https://arxiv.org/abs/2404.05457</link>
      <description>arXiv:2404.05457v1 Announce Type: new 
Abstract: Dependently typed programming languages have become increasingly relevant in recent years. They have been adopted in industrial strength programming languages and have been extremely successful as the basis for theorem provers. There are however, very few entry level introductions to the theory of language constructs for dependently typed languages, and even less sources on didactical implementations. In this paper, we present a small dependently typed programming language called WebPie. The main features of the language are inductive types, recursion and case matching. While none of these features are new, we believe this article can provide a step forward towards the understanding and systematic construction of dependently typed languages for researchers new to dependent types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05457v1</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <category>cs.SC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.400.2</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 400, 2024, pp. 2-27</arxiv:journal_reference>
      <dc:creator>Christophe Scholliers (Ghent University)</dc:creator>
    </item>
    <item>
      <title>A Coq Library of Sets for Teaching Denotational Semantics</title>
      <link>https://arxiv.org/abs/2404.05459</link>
      <description>arXiv:2404.05459v1 Announce Type: new 
Abstract: Sets and relations are very useful concepts for defining denotational semantics. In the Coq proof assistant, curried functions to Prop are used to represent sets and relations, e.g. A -&gt; Prop, A -&gt; B -&gt; Prop, A -&gt; B -&gt; C -&gt; Prop, etc. Further, the membership relation can be encoded by function applications, e.g. X a represents a in X if X: A -&gt; Prop. This is very convenient for developing formal definitions and proofs for professional users, but it makes propositions more difficult to read for non-professional users, e.g. students of a program semantics course. We develop a small Coq library of sets and relations so that standard math notations can be used when teaching denotational semantics of simple imperative languages. This library is developed using Coq's type class system. It brings about zero proof-term overhead comparing with the existing formalization of sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05459v1</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.400.6</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 400, 2024, pp. 79-95</arxiv:journal_reference>
      <dc:creator>Qinxiang Cao, Xiwei Wu, Yalun Liang</dc:creator>
    </item>
    <item>
      <title>Session Types for the Transport Layer: Towards an Implementation of TCP</title>
      <link>https://arxiv.org/abs/2404.05478</link>
      <description>arXiv:2404.05478v1 Announce Type: new 
Abstract: Session types are a typing discipline used to formally describe communication-driven applications with the aim of fewer errors and easier debugging later into the life cycle of the software. Protocols at the transport layer such as TCP, UDP, and QUIC underpin most of the communication on the modern Internet and affect billions of end-users. The transport layer has different requirements and constraints compared to the application layer resulting in different requirements for verification. Despite this, to our best knowledge, no work shows the application of session types at the transport layer. In this work, we discuss how multiparty session types (MPST) can be applied to implement the TCP protocol. We develop an MPST-based implementation of a subset of a TCP server in Rust and test its interoperability against the Linux TCP stack. Our results highlight the differences in assumptions between session type theory and the way transport layer protocols are usually implemented. This work is the first step towards bringing session types into the transport layer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05478v1</guid>
      <category>cs.PL</category>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.401.3</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 401, 2024, pp. 22-36</arxiv:journal_reference>
      <dc:creator>Samuel Cavoj (University of Glasgow), Ivan Nikitin (University of Glasgow), Colin Perkins (University of Glasgow), Ornela Dardha (University of Glasgow)</dc:creator>
    </item>
    <item>
      <title>Behavioural Types for Heterogeneous Systems (Position Paper)</title>
      <link>https://arxiv.org/abs/2404.05479</link>
      <description>arXiv:2404.05479v1 Announce Type: new 
Abstract: Behavioural types provide a promising way to achieve lightweight, language-integrated verification for communication-centric software. However, a large barrier to the adoption of behavioural types is that the current state of the art expects software to be written using the same tools and typing discipline throughout a system, and has little support for components over which a developer has no control.
  This position paper describes the outcomes of a working group discussion at Dagstuhl Seminar 24051 (Next-Generation Protocols for Heterogeneous Systems). We propose a methodology for integrating multiple behaviourally-typed components, written in different languages. Our proposed approach involves an extensible protocol description language, a session IR that can describe data transformations and boundary monitoring and which can be compiled into program-specific session proxies, and finally a session middleware to aid session establishment.
  We hope that this position paper will stimulate discussion on one of the most pressing challenges facing the widespread adoption of behavioural typing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05479v1</guid>
      <category>cs.PL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.401.4</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 401, 2024, pp. 37-48</arxiv:journal_reference>
      <dc:creator>Simon Fowler (University of Glasgow), Philipp Haller (Digital Futures, KTH Royal Institute of Technology), Roland Kuhn (Actyx AG), Sam Lindley (The University of Edinburgh), Alceste Scalas (Technical University of Denmark), Vasco T. Vasconcelos (University of Lisbon)</dc:creator>
    </item>
    <item>
      <title>Three Subtyping Algorithms for Binary Session Types and their Complexity Analyses</title>
      <link>https://arxiv.org/abs/2404.05480</link>
      <description>arXiv:2404.05480v1 Announce Type: new 
Abstract: Session types are a type discipline for describing and specifying communication behaviours of concurrent processes. Session subtyping, firstly introduced by Gay and Hole, is widely used for enlarging typability of session programs. This paper gives the complexity analysis of three algorithms for subtyping of synchronous binary session types. First, we analyse the complexity of the algorithm from the original paper, which is based on an inductive tree search. We then introduce its optimised version, which improves the complexity, but is still exponential against the size of the two types. Finally, we propose a new quadratic algorithm based on a graph search using the concept of XYZW-simulation, recently introduced by Silva et al.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05480v1</guid>
      <category>cs.PL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.401.5</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 401, 2024, pp. 49-60</arxiv:journal_reference>
      <dc:creator>Thien Udomsrirungruang (University of Oxford), Nobuko Yoshida (University of Oxford)</dc:creator>
    </item>
    <item>
      <title>Automated Computer Program Evaluation and Projects -- Our Experiences</title>
      <link>https://arxiv.org/abs/2404.04521</link>
      <description>arXiv:2404.04521v1 Announce Type: cross 
Abstract: This paper provides a few approaches to automating computer programming and project submission tasks, that we have been following for the last six years and have found to be successful. The approaches include using CodeRunner with Learning Management System (LMS) integration for programming practice and evaluation, and Git (GitHub) for project submissions and automatic code evaluation. In this paper, we describe the details of how we set up the tools and customized those for computer science courses. Based on our experiences, we also provide a few insights on using these tools for effective learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04521v1</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>https://www.sxcejournal.com/spe-apr-2023/17.pdf</arxiv:journal_reference>
      <dc:creator>Bama Srinivasan, Mala Nehru, Ranjani Parthasarathi, Saswati Mukherjee, Jeena A Thankachan</dc:creator>
    </item>
    <item>
      <title>Learning Minimal NAP Specifications for Neural Network Verification</title>
      <link>https://arxiv.org/abs/2404.04662</link>
      <description>arXiv:2404.04662v1 Announce Type: cross 
Abstract: Specifications play a crucial role in neural network verification. They define the precise input regions we aim to verify, typically represented as L-infinity norm balls. While recent research suggests using neural activation patterns (NAPs) as specifications for verifying unseen test set data, it focuses on computing the most refined NAPs, often limited to very small regions in the input space. In this paper, we study the following problem: Given a neural network, find a minimal (coarsest) NAP that is sufficient for formal verification of the network's robustness. Finding the minimal NAP specification not only expands verifiable bounds but also provides insights into which neurons contribute to the model's robustness. To address this problem, we propose several exact and approximate approaches. Our exact approaches leverage the verification tool to find minimal NAP specifications in either a deterministic or statistical manner. Whereas the approximate methods efficiently estimate minimal NAPs using adversarial examples and local gradients, without making calls to the verification tool. This allows us to inspect potential causal links between neurons and the robustness of state-of-the-art neural networks, a task for which existing verification frameworks fail to scale. Our experimental results suggest that minimal NAP specifications require much smaller fractions of neurons compared to the most refined NAP specifications, yet they can significantly expand the verifiable boundaries to several orders of magnitude larger.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04662v1</guid>
      <category>cs.LG</category>
      <category>cs.PL</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chuqin Geng, Zhaoyue Wang, Haolin Ye, Saifei Liao, Xujie Si</dc:creator>
    </item>
    <item>
      <title>GATlab: Modeling and Programming with Generalized Algebraic Theories</title>
      <link>https://arxiv.org/abs/2404.04837</link>
      <description>arXiv:2404.04837v1 Announce Type: cross 
Abstract: Categories and categorical structures are increasingly recognized as useful abstractions for modeling in science and engineering. To uniformly implement category-theoretic mathematical models in software, we introduce GATlab, a domain-specific language for algebraic specification embedded in a technical programming language. GATlab is based on generalized algebraic theories (GATs), a logical system extending algebraic theories with dependent types so as to encompass category theory. Using GATlab, the programmer can specify generalized algebraic theories and their models, including both free models, based on symbolic expressions, and computational models, defined by arbitrary code in the host language. Moreover, the programmer can define maps between theories and use them to declaratively migrate models of one theory to models of another. In short, GATlab aims to provide a unified environment for both computer algebra and software interface design with generalized algebraic theories. In this paper, we describe the design, implementation, and applications of GATlab.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04837v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Owen Lynch, Kris Brown, James Fairbanks, Evan Patterson</dc:creator>
    </item>
    <item>
      <title>Quantitative Weakest Hyper Pre: Unifying Correctness and Incorrectness Hyperproperties via Predicate Transformers</title>
      <link>https://arxiv.org/abs/2404.05097</link>
      <description>arXiv:2404.05097v1 Announce Type: cross 
Abstract: We present a novel \emph{weakest pre calculus} for \emph{reasoning about quantitative hyperproperties} over \emph{nondeterministic and probabilistic} programs. Whereas existing calculi allow reasoning about the expected value that a quantity assumes after program termination from a \emph{single initial state}, we do so for \emph{initial sets of states} or \emph{initial probability distributions}. We thus (i)~obtain a weakest pre calculus for hyper Hoare logic and (ii)~enable reasoning about so-called \emph{hyperquantities} which include expected values but also quantities (e.g. variance) out of scope of previous work. As a byproduct, we obtain a novel strongest post for weighted programs that extends both existing strongest and strongest liberal post calculi. Our framework reveals novel dualities between forward and backward transformers, correctness and incorrectness, as well as nontermination and unreachability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05097v1</guid>
      <category>cs.LO</category>
      <category>cs.CR</category>
      <category>cs.FL</category>
      <category>cs.PL</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Linpeng Zhang, Noam Zilberstein, Benjamin Lucien Kaminski, Alexandra Silva</dc:creator>
    </item>
    <item>
      <title>Linear Contextual Metaprogramming and Session Types</title>
      <link>https://arxiv.org/abs/2404.05475</link>
      <description>arXiv:2404.05475v1 Announce Type: cross 
Abstract: We explore the integration of metaprogramming in a call-by-value linear lambda-calculus and sketch its extension to a session type system. We build on a model of contextual modal type theory with multi-level contexts, where contextual values, closing arbitrary terms over a series of variables, may then be boxed and transmitted in messages. Once received, one such value may then be unboxed (with a let-box construct) and locally applied before being run. We present a series of examples where servers prepare and ship code on demand via session typed messages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05475v1</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.401.1</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 401, 2024, pp. 1-10</arxiv:journal_reference>
      <dc:creator>Pedro \^Angelo (LIACC,Faculdade de Ci\^encias da Universidade do Porto, Portugal), Atsushi Igarashi (Kyoto University, Kyoto, Japan), Vasco T. Vasconcelos (LASIGE, Faculdade de Ci\^encias da Universidade de Lisboa, Portugal)</dc:creator>
    </item>
    <item>
      <title>The Argument for Meta-Modeling-Based Approaches to Hardware Generation Languages</title>
      <link>https://arxiv.org/abs/2404.05599</link>
      <description>arXiv:2404.05599v1 Announce Type: cross 
Abstract: The rapid evolution of Integrated Circuit (IC) development necessitates innovative methodologies such as code generation to manage complexity and increase productivity. Using the right methodology for generator development to maximize the capability and, most notably, the feasibility of generators is a crucial part of this work. Meta-Modeling-based approaches drawing on the principles of Model Driven Architecture (MDA) are a promising methodology for generator development. The goal of this paper is to show why such an MDA-based approach can provide extremely powerful generators with minimal implementation effort and to demonstrate that this approach is a superior alternative to the most advanced hardware generation languages such as SpinalHDL and Chisel. For this purpose, this paper provides an in-depth comparison of the Meta-Modeling approach against these hardware generation languages, highlighting the unique advantages of a Meta-Modeling-based approach and summarizes the benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05599v1</guid>
      <category>cs.SE</category>
      <category>cs.AR</category>
      <category>cs.PL</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Schreiner, Daniel Gerl, Robert Kunzelmann, Paritosh Kumar Sinha, Wolfgang Ecker</dc:creator>
    </item>
    <item>
      <title>Stratified Type Theory</title>
      <link>https://arxiv.org/abs/2309.12164</link>
      <description>arXiv:2309.12164v3 Announce Type: replace 
Abstract: A hierarchy of type universes is a rudimentary ingredient in the type theories of many proof assistants to prevent the logical inconsistency resulting from combining dependent functions and the type-in-type rule. In this work, we argue that a universe hierarchy is not the only option for a type theory with a type universe. Taking inspiration from Leivant's Stratified System F, we introduce Stratified Type Theory (StraTT), where rather than stratifying universes by levels, we stratify typing judgements and restrict the domain of dependent functions to strictly lower levels. Even with type-in-type, this restriction suffices to enforce consistency.
  In StraTT, we consider a number of extensions beyond just stratified dependent functions. First, the subsystem subStraTT employs McBride's crude-but-effective stratification (also known as displacement) as a simple form of level polymorphism where global definitions with concrete levels can be displaced uniformly to any higher level. Second, to recover some expressivity lost due to the restriction on dependent function domains, the full StraTT includes a separate nondependent function type with a "floating" domain whose level matches that of the overall function type. Finally, we have implemented a prototype type checker for StraTT extended with datatypes and inference for level and displacement annotations, along with a small core library.
  We have proven subStraTT to be consistent and StraTT to be type safe, but consistency of the full StraTT remains an open problem, largely due to the interaction between floating functions and cumulativity of judgements. Nevertheless, we believe StraTT to be consistent, and as evidence have verified the failure of some well-known type-theoretic paradoxes using our implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.12164v3</guid>
      <category>cs.PL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Chan, Stephanie Weirich</dc:creator>
    </item>
    <item>
      <title>The T-Complexity Costs of Error Correction for Control Flow in Quantum Computation</title>
      <link>https://arxiv.org/abs/2311.12772</link>
      <description>arXiv:2311.12772v2 Announce Type: replace 
Abstract: Numerous quantum algorithms require the use of quantum error correction to overcome the intrinsic unreliability of physical qubits. However, error correction imposes a unique performance bottleneck, known as T-complexity, that can make an implementation of an algorithm as a quantum program run more slowly than on idealized hardware. In this work, we identify that programming abstractions for control flow, such as the quantum if-statement, can introduce polynomial increases in the T-complexity of a program. If not mitigated, this slowdown can diminish the computational advantage of a quantum algorithm.
  To enable reasoning about the costs of control flow, we present a cost model that a developer can use to accurately analyze the T-complexity of a program and pinpoint the sources of slowdown. We also present a set of program-level optimizations, that a developer can use to rewrite a program to reduce its T-complexity, predict the T-complexity of the optimized program using the cost model, and then compile it to an efficient circuit via a straightforward strategy.
  We implement the program-level optimizations in Spire, an extension of the Tower quantum compiler. Using a set of 11 benchmark programs that use control flow, we show that the cost model is accurate, and that Spire's optimizations recover programs that are asymptotically efficient, meaning their runtime T-complexity under error correction is equal to their time complexity on idealized hardware.
  Our results show that optimizing a program before it is compiled to a circuit can yield better results than compiling the program to an inefficient circuit and then invoking a quantum circuit optimizer found in prior work. For our benchmarks, only 2 of 8 tested circuit optimizers recover circuits with asymptotically efficient T-complexity. Compared to these 2 optimizers, Spire uses 54x to 2400x less compile time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12772v2</guid>
      <category>cs.PL</category>
      <category>quant-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3656397</arxiv:DOI>
      <arxiv:journal_reference>Proc. ACM Program. Lang., Vol. 8, No. PLDI, Article 167. Publication date: June 2024</arxiv:journal_reference>
      <dc:creator>Charles Yuan, Michael Carbin</dc:creator>
    </item>
    <item>
      <title>From Algebraic Word Problem to Program: A Formalized Approach</title>
      <link>https://arxiv.org/abs/2003.11517</link>
      <description>arXiv:2003.11517v2 Announce Type: replace-cross 
Abstract: In this paper, we propose a pipeline to convert grade school level algebraic word problem into program of a formal languageA-IMP. Using natural language processing tools, we break the problem into sentence fragments which can then be reduced to functions. The functions are categorized by the head verb of the sentence and its structure, as defined by (Hosseini et al., 2014). We define the function signature and extract its arguments from the text using dependency parsing. We have a working implementation of the entire pipeline which can be found on our github repository.</description>
      <guid isPermaLink="false">oai:arXiv.org:2003.11517v2</guid>
      <category>cs.CL</category>
      <category>cs.PL</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Wiemerslage, Shafiuddin Rehan Ahmed</dc:creator>
    </item>
    <item>
      <title>A faster FPRAS for #NFA</title>
      <link>https://arxiv.org/abs/2312.13320</link>
      <description>arXiv:2312.13320v2 Announce Type: replace-cross 
Abstract: Given a non-deterministic finite automaton (NFA) A with m states, and a natural number n (presented in unary), the #NFA problem asks to determine the size of the set L(A_n) of words of length n accepted by A. While the corresponding decision problem of checking the emptiness of L(A_n) is solvable in polynomial time, the #NFA problem is known to be #P-hard. Recently, the long-standing open question -- whether there is an FPRAS (fully polynomial time randomized approximation scheme) for #NFA -- was resolved in \cite{ACJR19}. The FPRAS due to \cite{ACJR19} relies on the interreducibility of counting and sampling, and computes, for each pair of state q and natural number i &lt;= n, a set of O(\frac{m^7 n^7}{epsilon^7}) many uniformly chosen samples from the set of words of length i that have a run ending at q (\epsilon is the error tolerance parameter of the FPRAS). This informative measure -- the number of samples maintained per state and length -- also affects the overall time complexity with a quadratic dependence.
  Given the prohibitively high time complexity, in terms of each of the input parameters, of the FPRAS due to \cite{ACJR19}, and considering the widespread application of approximate counting (and sampling) in various tasks in Computer Science, a natural question arises: Is there a faster FPRAS for #NFA that can pave the way for the practical implementation of approximate #NFA tools? In this work, we demonstrate that significant improvements in time complexity are achievable. Specifically, we have reduced the number of samples required for each state to be independent of m, with significantly less dependence on $n$ and $\epsilon$, maintaining only \widetilde{O}(\frac{n^4}{epsilon^2}) samples per state.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13320v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kuldeep S. Meel, Sourav Chakraborty, Umang Mathur</dc:creator>
    </item>
    <item>
      <title>IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators</title>
      <link>https://arxiv.org/abs/2403.03894</link>
      <description>arXiv:2403.03894v2 Announce Type: replace-cross 
Abstract: Code understanding and generation have fast become some of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. In particular, most mainstream Code-LMs have been pre-trained on source code files alone. In this work, we investigate the prospect of leveraging readily available compiler intermediate representations (IR) - shared across programming languages - to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer.
  To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source code files coupled with respective intermediate representations. Next, starting from various base Code-LMs (ranging in size from 1.1B to 7.3B parameters), we carry out continued causal language modelling training on SLTrans, forcing the Code-LMs to (1) learn the IR language and (2) align the IR constructs with respective constructs of various programming languages. Our resulting models, dubbed IRCoder, display sizeable and consistent gains across a wide variety of code generation tasks and metrics, including prompt robustness, multilingual code completion, code understanding, and instruction following.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03894v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.PL</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Indraneil Paul, Goran Glava\v{s}, Iryna Gurevych</dc:creator>
    </item>
  </channel>
</rss>
