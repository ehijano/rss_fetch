<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.PM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.PM</link>
    <description>q-fin.PM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.PM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Sep 2025 01:53:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Increase Alpha: Performance and Risk of an AI-Driven Trading Framework</title>
      <link>https://arxiv.org/abs/2509.16707</link>
      <description>arXiv:2509.16707v1 Announce Type: new 
Abstract: There are inefficiencies in financial markets, with unexploited patterns in price, volume, and cross-sectional relationships. While many approaches use large-scale transformers, we take a domain-focused path: feed-forward and recurrent networks with curated features to capture subtle regularities in noisy financial data. This smaller-footprint design is computationally lean and reliable under low signal-to-noise, crucial for daily production at scale. At Increase Alpha, we built a deep-learning framework that maps over 800 U.S. equities into daily directional signals with minimal computational overhead.
  The purpose of this paper is twofold. First, we outline the general overview of the predictive model without disclosing its core underlying concepts. Second, we evaluate its real-time performance through transparent, industry standard metrics. Forecast accuracy is benchmarked against both naive baselines and macro indicators. The performance outcomes are summarized via cumulative returns, annualized Sharpe ratio, and maximum drawdown. The best portfolio combination using our signals provides a low-risk, continuous stream of returns with a Sharpe ratio of more than 2.5, maximum drawdown of around 3\%, and a near-zero correlation with the S\&amp;P 500 market benchmark. We also compare the model's performance through different market regimes, such as the recent volatile movements of the US equity market in the beginning of 2025. Our analysis showcases the robustness of the model and significantly stable performance during these volatile periods.
  Collectively, these findings show that market inefficiencies can be systematically harvested with modest computational overhead if the right variables are considered. This report will emphasize the potential of traditional deep learning frameworks for generating an AI-driven edge in the financial market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16707v1</guid>
      <category>q-fin.PM</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sid Ghatak, Arman Khaledian, Navid Parvini, Nariman Khaledian</dc:creator>
    </item>
    <item>
      <title>Navigating Uncertainty in ESG Investing</title>
      <link>https://arxiv.org/abs/2310.02163</link>
      <description>arXiv:2310.02163v3 Announce Type: replace 
Abstract: The widespread confusion among investors regarding Environmental, Social, and Governance (ESG) rankings assigned by rating agencies has underscored a critical issue in sustainable investing. To address this uncertainty, our research has devised methods that not only recognize this ambiguity but also offer tailored investment strategies for different investor profiles. By developing ESG ensemble strategies and integrating ESG scores into a Reinforcement Learning (RL) model, we aim to optimize portfolios that cater to both financial returns and ESG-focused outcomes. Additionally, by proposing the Double-Mean-Variance model, we classify three types of investors based on their risk preferences. We also introduce ESG-adjusted Capital Asset Pricing Models (CAPMs) to assess the performance of these optimized portfolios. Ultimately, our comprehensive approach provides investors with tools to navigate the inherent ambiguities of ESG ratings, facilitating more informed investment decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02163v3</guid>
      <category>q-fin.PM</category>
      <category>q-fin.ST</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiayue Zhang, Ken Seng Tan, Tony S. Wirjanto, Lysa Porth</dc:creator>
    </item>
    <item>
      <title>Pontryagin-Guided Deep Policy Learning for Constrained Dynamic Portfolio Choice</title>
      <link>https://arxiv.org/abs/2501.12600</link>
      <description>arXiv:2501.12600v5 Announce Type: replace 
Abstract: We present a Pontryagin-Guided Direct Policy Optimization (PG-DPO) framework for \emph{constrained} continuous-time portfolio--consumption problems that scales to hundreds of assets. The method couples neural policies with Pontryagin's Maximum Principle and enforces feasibility via a lightweight log-barrier stagewise solve; a \emph{manifold-projection} variant (P--PGDPO) projects controls onto the PMP/KKT manifold using stabilized adjoints. We prove a barrier--KKT correspondence with $O(\epsilon)$ policy error and $O(\epsilon^2)$ instantaneous Hamiltonian gap, and extend the BPTT--PMP match to constrained settings. On short-sale (nonnegativity, floating cash) and wealth-proportional consumption caps, P--PGDPO reduces risky-weight errors by orders of magnitude versus baseline PG-DPO, while the one-dimensional consumption control shows smaller but consistent gains near binding caps. The approach remains effective when closed-form benchmarks are unavailable, and is readily extensible to transaction costs and interacting limits -- promising even greater benefits under time-varying investment opportunities where classical solutions are scarce.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12600v5</guid>
      <category>q-fin.PM</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeonggyu Huh, Jaegi Jeon, Hyeng Keun Koo, Byung Hwa Lim</dc:creator>
    </item>
    <item>
      <title>Robust Reinforcement Learning with Dynamic Distortion Risk Measures</title>
      <link>https://arxiv.org/abs/2409.10096</link>
      <description>arXiv:2409.10096v3 Announce Type: replace-cross 
Abstract: In a reinforcement learning (RL) setting, the agent's optimal strategy heavily depends on her risk preferences and the underlying model dynamics of the training environment. These two aspects influence the agent's ability to make well-informed and time-consistent decisions when facing testing environments. In this work, we devise a framework to solve robust risk-aware RL problems where we simultaneously account for environmental uncertainty and risk with a class of dynamic robust distortion risk measures. Robustness is introduced by considering all models within a Wasserstein ball around a reference model. We estimate such dynamic robust risk measures using neural networks by making use of strictly consistent scoring functions, derive policy gradient formulae using the quantile representation of distortion risk measures, and construct an actor-critic algorithm to solve this class of robust risk-aware RL problems. We demonstrate the performance of our algorithm on a portfolio allocation example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10096v3</guid>
      <category>cs.LG</category>
      <category>q-fin.CP</category>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anthony Coache, Sebastian Jaimungal</dc:creator>
    </item>
  </channel>
</rss>
