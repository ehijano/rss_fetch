<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Jul 2025 04:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Identifying Neural Connectivity using Bernoulli Autoregressive Partially Linear Additive Models</title>
      <link>https://arxiv.org/abs/2507.18218</link>
      <description>arXiv:2507.18218v1 Announce Type: new 
Abstract: Characterising the interactions between spiking neurons is central to our understanding of cognitive processes such as memory, perception and decision making. In this work, we consider the problem of inferring connectivity in the brain network from non-stationary high-dimensional spike train data. Under a binned spike count representation of these data, we propose a Bernoulli autoregressive partially linear additive (BAPLA) model to identify the effective connectivity of a population of neurons. Estimates of the model parameters are obtained using a regularised maximum likelihood estimator, where an $\ell_1$ penalty is used to find sparse and interpretable estimates of neuronal interactions. We also account for non-stationary firing rates by adding a non-parametric trend to the model and provide an inference procedure to quantify the uncertainty associated with our estimated networks of neuronal interactions. We use synthetic data to assess the performance of the BAPLA method, highlighting its ability to detect both excitatory and inhibitory interactions in various settings. Finally, we apply our method to a neural spiking dataset from the DANDI archive, where we study the interactions of neural processes in reaction to various stimulus-response type neuroscience experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18218v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carla Pinkney, Carolina Euan, Alex Gibberd</dc:creator>
    </item>
    <item>
      <title>On Focusing Statistical Power for Searches and Measurements in Particle Physics</title>
      <link>https://arxiv.org/abs/2507.17831</link>
      <description>arXiv:2507.17831v1 Announce Type: cross 
Abstract: Particle physics experiments rely on the (generalised) likelihood ratio test (LRT) for searches and measurements, which consist of composite hypothesis tests. However, this test is not guaranteed to be optimal, as the Neyman-Pearson lemma pertains only to simple hypothesis tests. Any choice of test statistic thus implicitly determines how statistical power varies across the parameter space. An improvement in the core statistical testing methodology for general settings with composite tests would have widespread ramifications across experiments. We discuss an alternate test statistic that provides the data analyzer an ability to focus the power of the test on physics-motivated regions of the parameter space. We demonstrate the improvement from this technique compared to the LRT on a Higgs $\rightarrow\tau\tau$ dataset simulated by the ATLAS experiment and a dark matter dataset inspired by the LZ experiment. We also employ machine learning to efficiently perform the Neyman construction, which is essential to ensure statistically valid confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17831v1</guid>
      <category>hep-ph</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Carzon, Aishik Ghosh, Rafael Izbicki, Ann Lee, Luca Masserano, Daniel Whiteson</dc:creator>
    </item>
    <item>
      <title>A Two-armed Bandit Framework for A/B Testing</title>
      <link>https://arxiv.org/abs/2507.18118</link>
      <description>arXiv:2507.18118v1 Announce Type: cross 
Abstract: A/B testing is widely used in modern technology companies for policy evaluation and product deployment, with the goal of comparing the outcomes under a newly-developed policy against a standard control. Various causal inference and reinforcement learning methods developed in the literature are applicable to A/B testing. This paper introduces a two-armed bandit framework designed to improve the power of existing approaches. The proposed procedure consists of three main steps: (i) employing doubly robust estimation to generate pseudo-outcomes, (ii) utilizing a two-armed bandit framework to construct the test statistic, and (iii) applying a permutation-based method to compute the $p$-value. We demonstrate the efficacy of the proposed method through asymptotic theories, numerical experiments and real-world data from a ridesharing company, showing its superior performance in comparison to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18118v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinjuan Wang, Qianglin Wen, Yu Zhang, Xiaodong Yan, Chengchun Shi</dc:creator>
    </item>
    <item>
      <title>Regression approaches for modelling genotype-environment interaction and making predictions into unseen environments</title>
      <link>https://arxiv.org/abs/2507.18125</link>
      <description>arXiv:2507.18125v1 Announce Type: cross 
Abstract: In plant breeding and variety testing, there is an increasing interest in making use of environmental information to enhance predictions for new environments. Here, we will review linear mixed models that have been proposed for this purpose. The emphasis will be on predictions and on methods to assess the uncertainty of predictions for new environments. Our point of departure is straight-line regression, which may be extended to multiple environmental covariates and genotype-specific responses. When observable environmental covariates are used, this is also known as factorial regression. Early work along these lines can be traced back to Stringfield &amp; Salter (1934) and Yates &amp; Cochran (1938), who proposed a method nowadays best known as Finlay-Wilkinson regression. This method, in turn, has close ties with regression on latent environmental covariates and factor-analytic variance-covariance structures for genotype-environment interaction. Extensions of these approaches - reduced rank regression, kernel- or kinship-based approaches, random coefficient regression, and extended Finlay-Wilkinson regression - will be the focus of this paper. Our objective is to demonstrate how seemingly disparate methods are very closely linked and fall within a common model-based prediction framework. The framework considers environments as random throughout, with genotypes also modelled as random in most cases. We will discuss options for assessing uncertainty of predictions, including cross validation and model-based estimates of uncertainty. The methods are illustrated using a long-term rice variety trial dataset from Bangladesh.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18125v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maksym Hrachov, Hans-Peter Piepho, Niaz Md. Farhat Rahman, Waqas Ahmed Malik</dc:creator>
    </item>
    <item>
      <title>Index insurance under demand and solvency constraints</title>
      <link>https://arxiv.org/abs/2507.18240</link>
      <description>arXiv:2507.18240v1 Announce Type: cross 
Abstract: Index insurance is often proposed to reduce protection gaps, especially for emerging risks. Unlike traditional insurance, it bases compensation on a measurable index, enabling faster payouts and lower claim management costs. This approach benefits both policyholders, through quick payments, and insurers, through reduced costs and better risk control due to reliable data and robust statistical estimates. An important difference with the concept of Cat Bonds is that the feasibility of such coverage relies on the possibility of mutualization. Mutualization, in turn, is achieved only if a sufficiently high number of policyholders agree to subscribe. The purpose of this paper is to introduce a model for the demand for index insurance and to provide conditions under which the solvency of the portfolio is achieved. From these conditions, we deduce a product that combines index and traditional indemnity insurance in order to benefit from the best of both approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18240v1</guid>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Olivier Lopez (CREST), Daniel Nkameni (CREST)</dc:creator>
    </item>
    <item>
      <title>An omnibus goodness-of-fit test based on trigonometric moments</title>
      <link>https://arxiv.org/abs/2507.18591</link>
      <description>arXiv:2507.18591v1 Announce Type: cross 
Abstract: We present a versatile omnibus goodness-of-fit test based on the first two trigonometric moments of probability-integral-transformed data, which rectifies the covariance scaling errors made by Langholz and Kronmal [J. Amer. Statist. Assoc. 86 (1991), 1077--1084]. Once properly scaled, the quadratic-form statistic asymptotically follows a $\chi_2^2$ distribution under the null hypothesis. The covariance scalings and parameter estimators are provided for $32$ null distribution families, covering heavy-tailed, light-tailed, asymmetric, and bounded-support cases, so the test is ready to be applied directly. Using recent advances in non-degenerate multivariate $U$-statistics with estimated nuisance parameters, we also showcase its asymptotic distribution under local alternatives for three specific examples. Our procedure shows excellent power; in particular, simulations testing the Laplace model against a range of $400$ alternatives reveal that it surpasses all $40$ existing tests for moderate to large sample sizes. A real-data application involving 48-hour-ahead surface temperature forecast errors further demonstrates the practical utility of the test. To ensure full reproducibility, the R code that generated our numerical results is publicly accessible online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18591v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alain Desgagn\'e, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>A Simulated Reconstruction and Reidentification Attack on the 2010 U.S. Census: Full Technical Report</title>
      <link>https://arxiv.org/abs/2312.11283</link>
      <description>arXiv:2312.11283v2 Announce Type: replace 
Abstract: Statistical agencies routinely use different strategies to protect the confidentiality of tabular data from those used to protect the individual records in publicly released microdata. Aggregation is assumed to make the resulting statistics inherently less disclosive than the microdata. The 2010 U.S. Census used different disclosure limitation rules for its tabular and microdata publications. We show that the assumption that these tabular data are inherently less disclosive than their underlying microdata is wrong. The 2010 Census published more than 150 billion statistics in 180 table sets, almost all at the most detailed geographic level -- individual census blocks. Using only 34 of the published table sets, we reconstructed microdata for five variables (census block, sex, age, race, and ethnicity). Using only published data, an attacker using our methods can verify that all records in 70% of all census blocks (97 million people) are perfectly reconstructed. We confirm through reidentification studies that an attacker can, within census blocks with perfect reconstruction accuracy, correctly infer the actual census response on race and ethnicity for 3.4 million vulnerable people (unique persons with race and ethnicity different from the modal person on the census block) with 95\% accuracy. Next, we show that the more robust disclosure limitation framework used for the 2020 U.S. Census defends against attacks that are based on reconstruction. Finally, we show that available alternatives to the 2020 Census Disclosure Avoidance System would either fail to protect confidentiality or overly degrade the statistics' utility for the primary statutory use case: redrawing the boundaries of all the nation's legislative and voting districts in compliance with the 1965 Voting Rights Act. This is the full technical report. For the summary paper see https://doi.org/10.1162/99608f92.4a1ebf70.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11283v2</guid>
      <category>stat.AP</category>
      <category>cs.CR</category>
      <category>econ.EM</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1162/99608f92.4a1ebf70</arxiv:DOI>
      <dc:creator>John M. Abowd, Tamara Adams, Robert Ashmead, David Darais, Sourya Dey, Simson L. Garfinkel, Nathan Goldschlag, Daniel Kifer, Philip Leclerc, Ethan Lew, Scott Moore, Rolando A. Rodr\'iguez, Ramy N. Tadros, Lars Vilhuber</dc:creator>
    </item>
    <item>
      <title>Conditional pathways-based climate attribution</title>
      <link>https://arxiv.org/abs/2409.01396</link>
      <description>arXiv:2409.01396v2 Announce Type: replace 
Abstract: Attribution of climate impacts to natural and anthropogenic source forcings is essential for understanding and addressing climate effects. While standard methods like optimal fingerprinting have been effective for long-term changes, they often struggle in low signal-to-noise regimes typical of short-term forcings or with climate variables loosely related to the forcing. Single-step approaches fail to leverage additional climate information to enhance attribution certainty. To overcome these limitations, we propose a formal statistical framework that incorporates hypothesized physical pathways linking source forcings to downstream impacts. By establishing relationships based on scalar features and simple forcing response models, we create a series of conditional probabilities that describe the likelihood of the final impact. This method captures both primary and secondary processes by which the downstream impact evolves. Through hypothesis testing in a likelihood ratio framework, we demonstrate improved attribution confidence for source magnitudes in low signal-to-noise scenarios. Using the 1991 eruption of Mt. Pinatubo as a case study, we show that incorporating near-surface temperature and stratospheric radiative flux measurements enhances attribution certainty compared to analyses based solely on temperature, even at seasonal and regional scales. This framework holds promise for improving climate attribution assessments for unknown source magnitudes and low signal-to-noise impacts, where traditional methods may falter. Additionally, the formal inclusion of pathways allows for a deeper exploration of complex, multivariate relationships influencing source attribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01396v2</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher R. Wentland, Michael Weylandt, Laura P. Swiler, Diana L. Bull</dc:creator>
    </item>
    <item>
      <title>Active Learning For Repairable Hardware Systems With Partial Coverage</title>
      <link>https://arxiv.org/abs/2503.16315</link>
      <description>arXiv:2503.16315v3 Announce Type: replace 
Abstract: Identifying the optimal diagnostic test and hardware system instance to infer reliability characteristics using field data is challenging, especially when constrained by fixed budgets and minimal maintenance cycles. Active Learning (AL) has shown promise for parameter inference with limited data and budget constraints in machine learning/deep learning tasks. However, AL for reliability model parameter inference remains underexplored for repairable hardware systems. It requires specialized AL Acquisition Functions (AFs) that consider hardware aging and the fact that a hardware system consists of multiple sub-systems, which may undergo only partial testing during a given diagnostic test. To address these challenges, we propose a relaxed Mixed Integer Semidefinite Program (MISDP) AL AF that incorporates Diagnostic Coverage (DC), Fisher Information Matrices (FIMs), and diagnostic testing budgets. Furthermore, we design empirical-based simulation experiments focusing on two diagnostic testing scenarios: (1) partial tests of a hardware system with overlapping subsystem coverage, and (2) partial tests where one diagnostic test fully subsumes the subsystem coverage of another. We evaluate our proposed approach against the most widely used AL AF in the literature (entropy), as well as several intuitive AL AFs tailored for reliability model parameter inference. Our proposed AF ranked best on average among the alternative AFs across 6,000 experimental configurations, with respect to Area Under the Curve (AUC) of the Absolute Total Expected Event Error (ATEER) and Mean Squared Error (MSE) curves, with statistical significance calculated at a 0.05 alpha level using a Friedman hypothesis test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16315v3</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Potter, Beyza Kalkanl{\i}, Deniz Erdo\u{g}mu\c{s}, Michael Everett</dc:creator>
    </item>
    <item>
      <title>Bayesian Active Learning of (small) Quantile Sets through Expected Estimator Modification</title>
      <link>https://arxiv.org/abs/2506.13211</link>
      <description>arXiv:2506.13211v2 Announce Type: replace 
Abstract: Given a multivariate function taking deterministic and uncertain inputs, we consider the problem of estimating a quantile set: a set of deterministic inputs for which the probability that the output belongs to a specific region remains below a given threshold. To solve this problem in the context of expensive-to-evaluate black-box functions, we propose a Bayesian active learning strategy based on Gaussian process modeling. The strategy is driven by a novel sampling criterion, which belongs to a broader principle that we refer to as Expected Estimator Modification (EEM). More specifically, the strategy relies on a novel sampling criterion combined with a sequential Monte Carlo framework that enables the construction of batch-sequential designs for the efficient estimation of small quantile sets. The performance of the strategy is illustrated on several synthetic examples and an industrial application case involving the ROTOR37 compressor model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13211v2</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Romain Ait Abdelmalek-Lomenech (L2S,RT-UQ), Julien Bect (L2S,RT-UQ), Emmanuel Vazquez (L2S,RT-UQ)</dc:creator>
    </item>
    <item>
      <title>Learning Individual Reproductive Behavior from Aggregate Fertility Rates via Neural Posterior Estimation</title>
      <link>https://arxiv.org/abs/2506.22607</link>
      <description>arXiv:2506.22607v2 Announce Type: replace 
Abstract: Age-specific fertility rates (ASFRs) provide the most extensive record of reproductive change, but their aggregate nature obscures the individual-level behavioral mechanisms that drive fertility trends. To bridge this micro-macro divide, we introduce a likelihood-free Bayesian framework that couples a demographically interpretable, individual-level simulation model of the reproductive process with Sequential Neural Posterior Estimation (SNPE). We show that this framework successfully recovers core behavioral parameters governing contemporary fertility, including preferences for family size, reproductive timing, and contraceptive failure, using only ASFRs. The framework's effectiveness is validated on cohorts from four countries with diverse fertility regimes. Most compellingly, the model, estimated solely on aggregate data, successfully predicts out-of-sample distributions of individual-level outcomes, including age at first sex, desired family size, and birth intervals. Because our framework yields complete synthetic life histories, it significantly reduces the data requirements for building microsimulation models and enables behaviorally explicit demographic forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22607v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Fri, 25 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Daniel Ciganda, Ignacio Camp\'on, I\~naki Permanyer, Jakob H Macke</dc:creator>
    </item>
  </channel>
</rss>
