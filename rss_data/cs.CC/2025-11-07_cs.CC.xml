<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Nov 2025 05:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>SVP$_p$ is NP-Hard for all $p &gt; 2$, Even to Approximate Within a Factor of $2^{\log^{1-\varepsilon} n}$</title>
      <link>https://arxiv.org/abs/2511.04125</link>
      <description>arXiv:2511.04125v1 Announce Type: new 
Abstract: We prove that SVP$_p$ is NP-hard to approximate within a factor of $2^{\log^{1 - \varepsilon} n}$, for all constants $\varepsilon &gt; 0$ and $p &gt; 2$, under standard deterministic Karp reductions. This result is also the first proof that \emph{exact} SVP$_p$ is NP-hard in a finite $\ell_p$ norm. Hardness for SVP$_p$ with $p$ finite was previously only known if NP $\not \subseteq$ RP, and under that assumption, hardness of approximation was only known for all constant factors. As a corollary to our main theorem, we show that under the Sliding Scale Conjecture, SVP$_p$ is NP-hard to approximate within a small polynomial factor, for all constants $p &gt; 2$.
  Our proof techniques are surprisingly elementary; we reduce from a \emph{regularized} PCP instance directly to the shortest vector problem by using simple gadgets related to Vandermonde matrices and Hadamard matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04125v1</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaac M. Hair, Amit Sahai</dc:creator>
    </item>
    <item>
      <title>A Compendium of Reductions: reductions.network</title>
      <link>https://arxiv.org/abs/2511.04308</link>
      <description>arXiv:2511.04308v1 Announce Type: new 
Abstract: The website reductions.network serves as a comprehensive database for exploring problems and reductions between them. It presents several complexity classes in the form of an interconnected graph where problems are represented as vertices, while edges represent reductions between them. This graphical perspective allows for identifying problem clusters and simplifying finding problem candidates to reduce from. Moreover, users can easily search for existing problems via a dedicated search bar, and various filters allow them to focus on specific subgraphs of interest. The design of the website enables users to contribute by adding new problems and reductions to the database. Furthermore, the software architecture allows for the integration of additional graphs corresponding to new complexity classes. In the current state, the following networks with their respective complexity classes are included:
  - classical complexity including the classes NP, #P, and SSP-NP
  - parameterized complexity including the classes W[1], W[2]
  - gap-preserving reductions under the PCP-Theorem and the Unique Games Conjecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04308v1</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Gr\"une, Femke Pfaue</dc:creator>
    </item>
    <item>
      <title>Boolean function monotonicity testing requires (almost) $n^(1/2)$ queries</title>
      <link>https://arxiv.org/abs/2511.04558</link>
      <description>arXiv:2511.04558v1 Announce Type: new 
Abstract: We show that for any constant $c&gt;0$, any (two-sided error) adaptive algorithm for testing monotonicity of Boolean functions must have query complexity $\Omega(n^{1/2-c})$. This improves the $\tilde\Omega(n^{1/3})$ lower bound of [CWX17] and almost matches the $\tilde{O}(\sqrt{n})$ upper bound of [KMS18].</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04558v1</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mark Chen, Xi Chen, Hao Cui, William Pires, Jonah Stockwell</dc:creator>
    </item>
    <item>
      <title>Attractors Is All You Need: Parity Games In Polynomial Time</title>
      <link>https://arxiv.org/abs/2511.03752</link>
      <description>arXiv:2511.03752v1 Announce Type: cross 
Abstract: This paper provides a polynomial-time algorithm for solving parity games that runs in $\mathcal{O}(n^{2}\cdot(n + m))$ time-ending a search that has taken decades. Unlike previous attractor-based algorithms, the presented algorithm only removes regions with a determined winner. The paper introduces a new type of attractor that can guarantee finding the minimal dominion of a parity game. The attractor runs in polynomial time and can peel the graph empty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03752v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.FL</category>
      <category>cs.GT</category>
      <category>cs.LO</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rick van der Heijden</dc:creator>
    </item>
    <item>
      <title>Multi-Pass Streaming Lower Bounds for Uniformity Testing</title>
      <link>https://arxiv.org/abs/2511.03960</link>
      <description>arXiv:2511.03960v1 Announce Type: cross 
Abstract: We prove multi-pass streaming lower bounds for uniformity testing over a domain of size $2m$. The tester receives a stream of $n$ i.i.d. samples and must distinguish (i) the uniform distribution on $[2m]$ from (ii) a Paninski-style planted distribution in which, for each pair $(2i-1,2i)$, the probabilities are biased left or right by $\epsilon/2m$. We show that any $\ell$-pass streaming algorithm using space $s$ and achieving constant advantage must satisfy the tradeoff $sn\ell=\tilde{\Omega}(m/\epsilon^2)$. This extends the one-pass lower bound of Diakonikolas, Gouleakis, Kane, and Rao (2019) to multiple passes.
  Our proof has two components. First, we develop a hybrid argument, inspired by Dinur (2020), that reduces streaming to two-player communication problems. This reduction relies on a new perspective on hardness: we identify the source of hardness as uncertainty in the bias directions, rather than the collision locations. Second, we prove a strong lower bound for a basic two-player communication task, in which Alice and Bob must decide whether two random sign vectors $Y^a,Y^b\in\{\pm 1\}^m$ are independent or identical, yet they cannot observe the signs directly--only noisy local views of each coordinate. Our techniques may be of independent use for other streaming problems with stochastic inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03960v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qian Li, Xin Lyu</dc:creator>
    </item>
    <item>
      <title>Quantum Search With Generalized Wildcards</title>
      <link>https://arxiv.org/abs/2511.04669</link>
      <description>arXiv:2511.04669v1 Announce Type: cross 
Abstract: In the search with wildcards problem [Ambainis, Montanaro, Quantum Inf.~Comput.'14], one's goal is to learn an unknown bit-string $x \in \{-1,1\}^n$. An algorithm may, at unit cost, test equality of any subset of the hidden string with a string of its choice. Ambainis and Montanaro showed a quantum algorithm of cost $O(\sqrt{n} \log n)$ and a near-matching lower bound of $\Omega(\sqrt{n})$. Belovs [Comput.~Comp.'15] subsequently showed a tight $O(\sqrt{n})$ upper bound.
  We consider a natural generalization of this problem, parametrized by a subset $\cal{Q} \subseteq 2^{[n]}$, where an algorithm may test whether $x_S = b$ for an arbitrary $S \in \cal{Q}$ and $b \in \{-1,1\}^S$ of its choice, at unit cost. We show near-tight bounds when $\cal{Q}$ is any of the following collections: bounded-size sets, contiguous blocks, prefixes, and only the full set.
  All of these results are derived using a framework that we develop. Using symmetries of the task at hand we show that the quantum query complexity of learning $x$ is characterized, up to a constant factor, by an optimization program, which is succinctly described as follows: `maximize over all odd functions $f : \{-1,1\}^n \to \mathbb{R}$ the ratio of the maximum value of $f$ to the maximum (over $T \in \cal{Q}$) standard deviation of $f$ on a subcube whose free variables are exactly $T$.'
  To the best of our knowledge, ours is the first work to use the primal version of the negative-weight adversary bound (which is a maximization program typically used to show lower bounds) to show new quantum query upper bounds without explicitly resorting to SDP duality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04669v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arjan Cornelissen, Nikhil S. Mande, Subhasree Patro, Nithish Raja, Swagato Sanyal</dc:creator>
    </item>
    <item>
      <title>Logical Expressibility of Syntactic NL for Complementarity, Monotonicity, and Maximization</title>
      <link>https://arxiv.org/abs/2410.04117</link>
      <description>arXiv:2410.04117v2 Announce Type: replace 
Abstract: Syntactic NL or succinctly SNL was first introduced in 2017, analogously to SNP, as a ``syntactically''-defined natural subclass of NL (nondeterministic logarithmic-space complexity class) using a restricted form of logical sentences, starting with second-order ``functional'' existential quantifiers followed by first-order universal quantifiers, in close connection to the so-called linear space hypothesis. We further explore various properties of this complexity class SNL to achieve the better understandings of logical expressibility in NL. For instance, SNL does not enjoy the dichotomy theorem unless L=NL. To express the ``complementary'' problems of SNL problems logically, we introduce $\mu$SNL, which is an extension of SNL by allowing the use of $\mu$-terms. As natural variants of SNL, we further study the computational complexity of monotone and optimization versions of SNL, respectively called MonoSNL and MAXSNL. We further consider maximization problems that are logarithmic-space approximable with only constant approximation ratios. We then introduce a natural subclass of MAXSNL, called MAX$\tau$SNL, which enjoys such limited approximability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04117v2</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomoyuki Yamakami</dc:creator>
    </item>
    <item>
      <title>Monotone Circuit Complexity of Matching</title>
      <link>https://arxiv.org/abs/2507.16105</link>
      <description>arXiv:2507.16105v2 Announce Type: replace 
Abstract: We show that the perfect matching function on $n$-vertex graphs requires monotone circuits of size $\smash{2^{n^{\Omega(1)}}}$. This improves on the $n^{\Omega(\log n)}$ lower bound of Razborov (1985). Our proof uses the standard approximation method together with a new sunflower lemma for matchings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16105v2</guid>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bruno Cavalar, Mika G\"o\"os, Artur Riazanov, Anastasia Sofronova, Dmitry Sokolov</dc:creator>
    </item>
    <item>
      <title>A Little Depth Goes a Long Way: The Expressive Power of Log-Depth Transformers</title>
      <link>https://arxiv.org/abs/2503.03961</link>
      <description>arXiv:2503.03961v3 Announce Type: replace-cross 
Abstract: Recent theoretical results show transformers cannot express sequential reasoning problems over long inputs, intuitively because their computational depth is bounded. However, prior work treats the depth as a constant, leaving it unclear to what degree bounded depth may suffice for solving problems over short inputs, or how increasing the transformer's depth affects its expressive power. We address these questions by analyzing transformers whose depth can grow minimally with context length $n$. We show even highly uniform transformers with depth $\Theta(\log n)$ can express two important problems: recognizing regular languages, which captures state tracking abilities and was known to be expressible only by an unconventional, non-uniform model of transformers, and graph connectivity, which underlies multi-step reasoning. Notably, both of these problems cannot be expressed by fixed-depth transformers under standard complexity conjectures, demonstrating the expressivity benefit of growing depth. Moreover, our theory quantitatively predicts how depth must grow with input length to express these problems, showing that depth scaling is more efficient than scaling width or chain-of-thought steps. Empirically, our detailed experiments designed to bridge the expressivity vs. learnability gap reveal that our theoretical depth requirements for regular language recognition closely match the practical depth requirements for successfully training transformers. Thus, our results clarify how depth affects a transformer's reasoning capabilities, and provide practical guidance for effective depth selection for sequential reasoning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03961v3</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Merrill, Ashish Sabharwal</dc:creator>
    </item>
    <item>
      <title>Exact Expressive Power of Transformers with Padding</title>
      <link>https://arxiv.org/abs/2505.18948</link>
      <description>arXiv:2505.18948v2 Announce Type: replace-cross 
Abstract: Chain of thought is a natural inference-time method for increasing the computational power of transformer-based large language models (LLMs), but comes at the cost of sequential decoding. Are there more efficient alternatives to expand a transformer's expressive power without adding parameters? We consider transformers with padding tokens as a form of parallelizable test-time compute. We show that averaging-hard-attention, masked-pre-norm transformers with polynomial padding recognize precisely the class $\mathsf{FO}$-uniform $\mathsf{TC}^0$ of extremely parallelizable problems. While the $\mathsf{TC}^0$ upper bound was known, proving a matching lower bound had been elusive. Further, our novel analysis reveals the precise expanded power of padded transformers when coupled with another form of inference-time compute, namely dynamically increasing depth via looping. Our core technical contribution is to show how padding helps bring the notions of complete problems and reductions, which have been a cornerstone of classical complexity theory, to the formal study of transformers. Armed with this new tool, we prove that padded transformers with $O(\log^d n)$ looping on inputs of length $n$ recognize exactly the class $\mathsf{FO}$-uniform $\mathsf{TC}^d$ of moderately parallelizable problems. Thus, padding and looping together systematically expand transformers' expressive power: with polylogarithmic looping, polynomially padded transformers recognize precisely the class $\mathsf{FO}$-uniform $\mathsf{NC}$, the best that could be expected without losing parallelism (unless $\mathsf{NC} = \mathsf{P}$). Our results thus motivate further exploration of padding and looping as parallelizable alternatives to chain of thought for test-time compute.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18948v2</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.FL</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Merrill, Ashish Sabharwal</dc:creator>
    </item>
    <item>
      <title>Learning stabilizer structure of quantum states</title>
      <link>https://arxiv.org/abs/2510.05890</link>
      <description>arXiv:2510.05890v2 Announce Type: replace-cross 
Abstract: We consider the task of learning a structured stabilizer decomposition of an arbitrary $n$-qubit quantum state $|\psi\rangle$: for $\epsilon &gt; 0$, output a state $|\phi\rangle$ with stabilizer-rank $\textsf{poly}(1/\epsilon)$ such that $|\psi\rangle=|\phi\rangle+|\phi'\rangle$ where $|\phi'\rangle$ has stabilizer fidelity $&lt; \epsilon$. We first show the existence of such decompositions using the recently established inverse theorem for the Gowers-$3$ norm of states [AD,STOC'25].
  To learn this structure, we initiate the task of self-correction of a state $|\psi\rangle$ with respect to a class of states $S$: given copies of $|\psi\rangle$ which has fidelity $\geq \tau$ with a state in $S$, output $|\phi\rangle \in S$ with fidelity $|\langle \phi | \psi \rangle|^2 \geq \tau^C$ for a constant $C&gt;1$. Assuming the algorithmic polynomial Frieman-Rusza (APFR) conjecture in the high doubling regime (whose combinatorial version was recently resolved [GGMT,Annals of Math.'25]), we give a polynomial-time algorithm for self-correction of stabilizer states. Given access to the state preparation unitary $U_\psi$ for $|\psi\rangle$ and its controlled version $cU_\psi$, we give a polynomial-time protocol that learns a structured decomposition of $|\psi\rangle$. Without assuming APFR, we give a quasipolynomial-time protocol for the same task.
  As our main application, we give learning algorithms for states $|\psi\rangle$ promised to have stabilizer extent $\xi$, given access to $U_\psi$ and $cU_\psi$. We give a protocol that outputs $|\phi\rangle$ which is constant-close to $|\psi\rangle$ in time $\textsf{poly}(n,\xi^{\log \xi})$, which can be improved to polynomial-time assuming APFR. This gives an unconditional learning algorithm for stabilizer-rank $k$ states in time $\textsf{poly}(n,k^{k^2})$. As far as we know, learning arbitrary states with even stabilizer-rank $2$ was unknown.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05890v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Fri, 07 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Srinivasan Arunachalam, Arkopal Dutt</dc:creator>
    </item>
  </channel>
</rss>
