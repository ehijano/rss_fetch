<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Sep 2024 01:49:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Higher-criticism for sparse multi-sensor change-point detection</title>
      <link>https://arxiv.org/abs/2409.15597</link>
      <description>arXiv:2409.15597v1 Announce Type: new 
Abstract: We present a procedure based on higher criticism (Dohono \&amp; Jin 2004) to address the sparse multi-sensor quickest change-point detection problem. Namely, we aim to detect a change in the distribution of the multi-sensor that might affect a few sensors out of potentially many, while those affected sensors, if they exist, are unknown to us in advance. Our procedure involves testing for a change point in individual sensors and combining multiple tests using higher criticism. As a by-product, our procedure also indicates a set of sensors suspected to be affected by the change. We demonstrate the effectiveness of our method compared to other procedures using extensive numerical evaluations. We analyze our procedure under a theoretical framework involving normal data sensors that might experience a change in both mean and variance. We consider individual tests based on the likelihood ratio or the generalized likelihood ratio statistics and show that our procedure attains the information-theoretic limits of detection. These limits coincide with existing litereature when the change is only in the mean.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15597v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tingnan Gong, Alon Kipnis, Yao Xie</dc:creator>
    </item>
    <item>
      <title>BARD: A seamless two-stage dose optimization design integrating backfill and adaptive randomization</title>
      <link>https://arxiv.org/abs/2409.15663</link>
      <description>arXiv:2409.15663v1 Announce Type: new 
Abstract: One common approach for dose optimization is a two-stage design, which initially conducts dose escalation to identify the maximum tolerated dose (MTD), followed by a randomization stage where patients are assigned to two or more doses to further assess and compare their risk-benefit profiles to identify the optimal dose. A limitation of this approach is its requirement for a relatively large sample size. To address this challenge, we propose a seamless two-stage design, BARD (Backfill and Adaptive Randomization for Dose Optimization), which incorporates two key features to reduce sample size and shorten trial duration. The first feature is the integration of backfilling into the stage 1 dose escalation, enhancing patient enrollment and data generation without prolonging the trial. The second feature involves seamlessly combining patients treated in stage 1 with those in stage 2, enabled by covariate-adaptive randomization, to inform the optimal dose and thereby reduce the sample size. Our simulation study demonstrates that BARD reduces the sample size, improves the accuracy of identifying the optimal dose, and maintains covariate balance in randomization, allowing for unbiased comparisons between doses. BARD designs offer an efficient solution to meet the dose optimization requirements set by Project Optimus, with software freely available at www.trialdesign.org.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15663v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yixuan Zhao, Rachael Liu, Jianchang Lin, Ying Yuan</dc:creator>
    </item>
    <item>
      <title>TUNE: Algorithm-Agnostic Inference after Changepoint Detection</title>
      <link>https://arxiv.org/abs/2409.15676</link>
      <description>arXiv:2409.15676v1 Announce Type: new 
Abstract: In multiple changepoint analysis, assessing the uncertainty of detected changepoints is crucial for enhancing detection reliability -- a topic that has garnered significant attention. Despite advancements through selective p-values, current methodologies often rely on stringent assumptions tied to specific changepoint models and detection algorithms, potentially compromising the accuracy of post-detection statistical inference. We introduce TUNE (Thresholding Universally and Nullifying change Effect), a novel algorithm-agnostic approach that uniformly controls error probabilities across detected changepoints. TUNE sets a universal threshold for multiple test statistics, applicable across a wide range of algorithms, and directly controls the family-wise error rate without the need for selective p-values. Through extensive theoretical and numerical analyses, TUNE demonstrates versatility, robustness, and competitive power, offering a viable and reliable alternative for model-agnostic post-detection inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15676v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinxu Jia, Jixuan Liu, Guanghui Wang, Zhaojun Wang, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>A penalized online sequential test of heterogeneous treatment effects for generalized linear models</title>
      <link>https://arxiv.org/abs/2409.15756</link>
      <description>arXiv:2409.15756v1 Announce Type: new 
Abstract: Identification of heterogeneous treatment effects (HTEs) has been increasingly popular and critical in various penalized strategy decisions using the A/B testing approach, especially in the scenario of a consecutive online collection of samples. However, in high-dimensional settings, such an identification remains challenging in the sense of lack of detection power of HTEs with insufficient sample instances for each batch sequentially collected online. In this article, a novel high-dimensional test is proposed, named as the penalized online sequential test (POST), to identify HTEs and select useful covariates simultaneously under continuous monitoring in generalized linear models (GLMs), which achieves high detection power and controls the Type I error. A penalized score test statistic is developed along with an extended p-value process for the online collection of samples, and the proposed POST method is further extended to multiple online testing scenarios, where both high true positive rates and under-controlled false discovery rates are achieved simultaneously. Asymptotic results are established and justified to guarantee properties of the POST, and its performance is evaluated through simulations and analysis of real data, compared with the state-of-the-art online test methods. Our findings indicate that the POST method exhibits selection consistency and superb detection power of HTEs as well as excellent control over the Type I error, which endows our method with the capability for timely and efficient inference for online A/B testing in high-dimensional GLMs framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15756v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiqing Fang, Shuyan Chen, Xin Liu</dc:creator>
    </item>
    <item>
      <title>Robust Inference for Non-Linear Regression Models with Applications in Enzyme Kinetics</title>
      <link>https://arxiv.org/abs/2409.15995</link>
      <description>arXiv:2409.15995v1 Announce Type: new 
Abstract: Despite linear regression being the most popular statistical modelling technique, in real-life we often need to deal with situations where the true relationship between the response and the covariates is nonlinear in parameters. In such cases one needs to adopt appropriate non-linear regression (NLR) analysis, having wider applications in biochemical and medical studies among many others. In this paper we propose a new improved robust estimation and testing methodologies for general NLR models based on the minimum density power divergence approach and apply our proposal to analyze the widely popular Michaelis-Menten (MM) model in enzyme kinetics. We establish the asymptotic properties of our proposed estimator and tests, along with their theoretical robustness characteristics through influence function analysis. For the particular MM model, we have further empirically justified the robustness and the efficiency of our proposed estimator and the testing procedure through extensive simulation studies and several interesting real data examples of enzyme-catalyzed (biochemical) reactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15995v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suryasis Jana, Abhik Ghosh</dc:creator>
    </item>
    <item>
      <title>Easy Conditioning far Beyond Gaussian</title>
      <link>https://arxiv.org/abs/2409.16003</link>
      <description>arXiv:2409.16003v2 Announce Type: new 
Abstract: Estimating and sampling from conditional densities plays a critical role in statistics and data science, with a plethora of applications. Numerous methods are available ranging from simple fitting approaches to sophisticated machine learning algorithms. However, selecting from among these often involves a trade-off between conflicting objectives of efficiency, flexibility and interpretability. Starting from well known easy conditioning results in the Gaussian case, we show, thanks to results pertaining to stability by mixing and marginal transformations, that the latter carry over far beyond the Gaussian case. This enables us to flexibly model multivariate data by accommodating broad classes of multi-modal dependence structures and marginal distributions, while enjoying fast conditioning of fitted joint distributions. In applications, we primarily focus on conditioning via Gaussian versus Gaussian mixture copula models, comparing different fitting implementations for the latter. Numerical experiments with simulated and real data demonstrate the relevance of the approach for conditional sampling, evaluated using multivariate scoring rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16003v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Faul, David Ginsbourger, Ben Spycher</dc:creator>
    </item>
    <item>
      <title>Bayesian Variable Selection and Sparse Estimation for High-Dimensional Graphical Models</title>
      <link>https://arxiv.org/abs/2409.16276</link>
      <description>arXiv:2409.16276v1 Announce Type: new 
Abstract: We introduce a novel Bayesian approach for both covariate selection and sparse precision matrix estimation in the context of high-dimensional Gaussian graphical models involving multiple responses. Our approach provides a sparse estimation of the three distinct sparsity structures: the regression coefficient matrix, the conditional dependency structure among responses, and between responses and covariates. This contrasts with existing methods, which typically focus on any two of these structures but seldom achieve simultaneous sparse estimation for all three. A key aspect of our method is that it leverages the structural sparsity information gained from the presence of irrelevant covariates in the dataset to introduce covariate-level sparsity in the precision and regression coefficient matrices. This is achieved through a Bayesian conditional random field model using a hierarchical spike and slab prior setup. Despite the non-convex nature of the problem, we establish statistical accuracy for points in the high posterior density region, including the maximum-a-posteriori (MAP) estimator. We also present an efficient Expectation-Maximization (EM) algorithm for computing the estimators. Through simulation experiments, we demonstrate the competitive performance of our method, particularly in scenarios with weak signal strength in the precision matrices. Finally, we apply our method to a bike-share dataset, showcasing its predictive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16276v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anwesha Chakravarti, Naveen N. Narishetty, Feng Liang</dc:creator>
    </item>
    <item>
      <title>Identifying Elasticities in Autocorrelated Time Series Using Causal Graphs</title>
      <link>https://arxiv.org/abs/2409.15530</link>
      <description>arXiv:2409.15530v1 Announce Type: cross 
Abstract: The price elasticity of demand can be estimated from observational data using instrumental variables (IV). However, naive IV estimators may be inconsistent in settings with autocorrelated time series. We argue that causal time graphs can simplify IV identification and help select consistent estimators. To do so, we propose to first model the equilibrium condition by an unobserved confounder, deriving a directed acyclic graph (DAG) while maintaining the assumption of a simultaneous determination of prices and quantities. We then exploit recent advances in graphical inference to derive valid IV estimators, including estimators that achieve consistency by simultaneously estimating nuisance effects. We further argue that observing significant differences between the estimates of presumably valid estimators can help to reject false model assumptions, thereby improving our understanding of underlying economic dynamics. We apply this approach to the German electricity market, estimating the price elasticity of demand on simulated and real-world data. The findings underscore the importance of accounting for structural autocorrelation in IV-based analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15530v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Silvana Tiedemann (Centre for Sustainability, Hertie School), Jorge Sanchez Canales (Centre for Sustainability, Hertie School), Felix Schur (Department of Mathematics, ETH Zurich), Raffaele Sgarlato (Centre for Sustainability, Hertie School), Lion Hirth (Centre for Sustainability, Hertie School), Oliver Ruhnau (Department of Economics and Institute of Energy Economics, University of Cologne), Jonas Peters (Department of Mathematics, ETH Zurich)</dc:creator>
    </item>
    <item>
      <title>A theory of generalised coordinates for stochastic differential equations</title>
      <link>https://arxiv.org/abs/2409.15532</link>
      <description>arXiv:2409.15532v1 Announce Type: cross 
Abstract: Stochastic differential equations are ubiquitous modelling tools in physics and the sciences. In most modelling scenarios, random fluctuations driving dynamics or motion have some non-trivial temporal correlation structure, which renders the SDE non-Markovian; a phenomenon commonly known as ``colored'' noise. Thus, an important objective is to develop effective tools for mathematically and numerically studying (possibly non-Markovian) SDEs. In this report, we formalise a mathematical theory for analysing and numerically studying SDEs based on so-called `generalised coordinates of motion'. Like the theory of rough paths, we analyse SDEs pathwise for any given realisation of the noise, not solely probabilistically. Like the established theory of Markovian realisation, we realise non-Markovian SDEs as a Markov process in an extended space. Unlike the established theory of Markovian realisation however, the Markovian realisations here are accurate on short timescales and may be exact globally in time, when flows and fluctuations are analytic. This theory is exact for SDEs with analytic flows and fluctuations, and is approximate when flows and fluctuations are differentiable. It provides useful analysis tools, which we employ to solve linear SDEs with analytic fluctuations. It may also be useful for studying rougher SDEs, as these may be identified as the limit of smoother ones. This theory supplies effective, computationally straightforward methods for simulation, filtering and control of SDEs; amongst others, we re-derive generalised Bayesian filtering, a state-of-the-art method for time-series analysis. Looking forward, this report suggests that generalised coordinates have far-reaching applications throughout stochastic differential equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15532v1</guid>
      <category>math.PR</category>
      <category>math.DS</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lancelot Da Costa, Natha\"el Da Costa, Conor Heins, Johan Medrano, Grigorios A. Pavliotis, Thomas Parr, Ajith Anil Meera, Karl Friston</dc:creator>
    </item>
    <item>
      <title>Smoothing the Conditional Value-at-Risk based Pickands Estimators</title>
      <link>https://arxiv.org/abs/2409.15677</link>
      <description>arXiv:2409.15677v1 Announce Type: cross 
Abstract: We incorporate the conditional value-at-risk (CVaR) quantity into a generalized class of Pickands estimators. By introducing CVaR, the newly developed estimators not only retain the desirable properties of consistency, location, and scale invariance inherent to Pickands estimators, but also achieve a reduction in mean squared error (MSE). To address the issue of sensitivity to the choice of the number of top order statistics used for the estimation, and ensure robust estimation, which are crucial in practice, we first propose a beta measure, which is a modified beta density function, to smooth the estimator. Then, we develop an algorithm to approximate the asymptotic mean squared error (AMSE) and determine the optimal beta measure that minimizes AMSE. A simulation study involving a wide range of distributions shows that our estimators have good and highly stable finite-sample performance and compare favorably with the other estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15677v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizhou Li, Pawel Polak</dc:creator>
    </item>
    <item>
      <title>Linear Contextual Bandits with Interference</title>
      <link>https://arxiv.org/abs/2409.15682</link>
      <description>arXiv:2409.15682v1 Announce Type: cross 
Abstract: Interference, a key concept in causal inference, extends the reward modeling process by accounting for the impact of one unit's actions on the rewards of others. In contextual bandit (CB) settings, where multiple units are present in the same round, potential interference can significantly affect the estimation of expected rewards for different arms, thereby influencing the decision-making process. Although some prior work has explored multi-agent and adversarial bandits in interference-aware settings, the effect of interference in CB, as well as the underlying theory, remains significantly underexplored. In this paper, we introduce a systematic framework to address interference in Linear CB (LinCB), bridging the gap between causal inference and online decision-making. We propose a series of algorithms that explicitly quantify the interference effect in the reward modeling process and provide comprehensive theoretical guarantees, including sublinear regret bounds, finite sample upper bounds, and asymptotic properties. The effectiveness of our approach is demonstrated through simulations and a synthetic data generated based on MovieLens data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15682v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Xu, Wenbin Lu, Rui Song</dc:creator>
    </item>
    <item>
      <title>Adaptive Learn-then-Test: Statistically Valid and Efficient Hyperparameter Selection</title>
      <link>https://arxiv.org/abs/2409.15844</link>
      <description>arXiv:2409.15844v1 Announce Type: cross 
Abstract: We introduce adaptive learn-then-test (aLTT), an efficient hyperparameter selection procedure that provides finite-sample statistical guarantees on the population risk of AI models. Unlike the existing learn-then-test (LTT) technique, which relies on conventional p-value-based multiple hypothesis testing (MHT), aLTT implements sequential data-dependent MHT with early termination by leveraging e-processes. As a result, aLTT can reduce the number of testing rounds, making it particularly well-suited for scenarios in which testing is costly or presents safety risks. Apart from maintaining statistical validity, in applications such as online policy selection for offline reinforcement learning and hyperparameter tuning for engineering systems, aLTT is shown to achieve the same performance as LTT while requiring only a fraction of the testing rounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15844v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matteo Zecchin, Osvaldo Simeone</dc:creator>
    </item>
    <item>
      <title>Stable Survival Extrapolation via Transfer Learning</title>
      <link>https://arxiv.org/abs/2409.16044</link>
      <description>arXiv:2409.16044v1 Announce Type: cross 
Abstract: The mean survival is the key ingredient of the decision process in several applications, including health economic evaluations. It is defined as the area under the complete survival curve thus necessitating extrapolation of the observed data. In this article we employ a Bayesian mortality model and transfer its projections in order to construct the baseline population that acts as an anchor of the survival model. This can be seen as an implicit bias-variance trade-off in unseen data. We then propose extrapolation methods based on flexible parametric polyhazard models which can naturally accommodate diverse shapes, including non-proportional hazards and crossing survival curves while typically maintaining a natural interpretation. We estimate the mean survival and related estimands in three cases, namely breast cancer, cardiac arrhythmia and advanced melanoma. Specifically, we evaluate the survival disadvantage of triple negative breast cancer cases, the efficacy of combining immunotherapy with mRNA cancer therapeutic for advanced melanoma treatment and the suitability of implantable cardioverter defibrilators for cardiac arrhythmia. The latter is conducted in a competing risks context illustrating how working on the cause-specific hazard alone minimizes potential instability. The results suggest that the proposed approach offers a flexible, interpretable and robust approach when survival extrapolation is required.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16044v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anastasios Apsemidis, Nikolaos Demiris</dc:creator>
    </item>
    <item>
      <title>Probability Weighted Clustered Coefficients Regression Models in Complex Survey Sampling</title>
      <link>https://arxiv.org/abs/2210.09339</link>
      <description>arXiv:2210.09339v3 Announce Type: replace 
Abstract: Regression analysis is commonly conducted in survey sampling. However, existing methods fail when the relationships vary across different areas or domains. In this paper, we propose a unified framework to study the group-wise covariate effect under complex survey sampling based on pairwise penalties, and the associated objective function is solved by the alternating direction method of multipliers. Theoretical properties of the proposed method are investigated under some generality conditions. Numerical experiments demonstrate the superiority of the proposed method in terms of identifying groups and estimation efficiency for both linear regression models and logistic regression models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.09339v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingjun Gang, Xin Wang, Zhonglei Wang, Wei Zhong</dc:creator>
    </item>
    <item>
      <title>Elastic Bayesian Model Calibration</title>
      <link>https://arxiv.org/abs/2305.08834</link>
      <description>arXiv:2305.08834v2 Announce Type: replace 
Abstract: Functional data are ubiquitous in scientific modeling. For instance, quantities of interest are modeled as functions of time, space, energy, density, etc. Uncertainty quantification methods for computer models with functional response have resulted in tools for emulation, sensitivity analysis, and calibration that are widely used. However, many of these tools do not perform well when the computer model's parameters control both the amplitude variation of the functional output and its alignment (or phase variation). This paper introduces a framework for Bayesian model calibration when the model responses are misaligned functional data. The approach generates two types of data out of the misaligned functional responses: (1) aligned functions so that the amplitude variation is isolated and (2) warping functions that isolate the phase variation. These two types of data are created for the computer simulation data (both of which may be emulated) and the experimental data. The calibration approach uses both types so that it seeks to match both the amplitude and phase of the experimental data. The framework is careful to respect constraints that arise especially when modeling phase variation, and is framed in a way that it can be done with readily available calibration software. We demonstrate the techniques on two simulated data examples and on two dynamic material science problems: a strength model calibration using flyer plate experiments and an equation of state model calibration using experiments performed on the Sandia National Laboratories' Z-machine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.08834v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Devin Francom, J. Derek Tucker, Gabriel Huerta, Kurtis Shuler, Daniel Ries</dc:creator>
    </item>
    <item>
      <title>A Bayesian joint model for mediation analysis with matrix-valued mediators</title>
      <link>https://arxiv.org/abs/2310.00803</link>
      <description>arXiv:2310.00803v3 Announce Type: replace 
Abstract: Unscheduled treatment interruptions may lead to reduced quality of care in radiation therapy (RT). Identifying the RT prescription dose effects on the outcome of treatment interruptions, mediated through doses distributed into different organs-at-risk (OARs), can inform future treatment planning. The radiation exposure to OARs can be summarized by a matrix of dose-volume histograms (DVH) for each patient. Although various methods for high-dimensional mediation analysis have been proposed recently, few studies investigated how matrix-valued data can be treated as mediators. In this paper, we propose a novel Bayesian joint mediation model for high-dimensional matrix-valued mediators. In this joint model, latent features are extracted from the matrix-valued data through an adaptation of probabilistic multilinear principal components analysis (MPCA), retaining the inherent matrix structure. We derive and implement a Gibbs sampling algorithm to jointly estimate all model parameters, and introduce a Varimax rotation method to identify active indicators of mediation among the matrix-valued data. Our simulation study finds that the proposed joint model has higher efficiency in estimating causal decomposition effects compared to an alternative two-step method, and demonstrates that the mediation effects can be identified and visualized in the matrix form. We apply the method to study the effect of prescription dose on treatment interruptions in anal canal cancer patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00803v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zijin Liu, Zhihui Liu, Ali Hosni, John Kim, Bei Jiang, Olli Saarela</dc:creator>
    </item>
    <item>
      <title>Iterative Methods for Vecchia-Laplace Approximations for Latent Gaussian Process Models</title>
      <link>https://arxiv.org/abs/2310.12000</link>
      <description>arXiv:2310.12000v3 Announce Type: replace 
Abstract: Latent Gaussian process (GP) models are flexible probabilistic non-parametric function models. Vecchia approximations are accurate approximations for GPs to overcome computational bottlenecks for large data, and the Laplace approximation is a fast method with asymptotic convergence guarantees to approximate marginal likelihoods and posterior predictive distributions for non-Gaussian likelihoods. Unfortunately, the computational complexity of combined Vecchia-Laplace approximations grows faster than linearly in the sample size when used in combination with direct solver methods such as the Cholesky decomposition. Computations with Vecchia-Laplace approximations can thus become prohibitively slow precisely when the approximations are usually the most accurate, i.e., on large data sets. In this article, we present iterative methods to overcome this drawback. Among other things, we introduce and analyze several preconditioners, derive new convergence results, and propose novel methods for accurately approximating predictive variances. We analyze our proposed methods theoretically and in experiments with simulated and real-world data. In particular, we obtain a speed-up of an order of magnitude compared to Cholesky-based calculations and a threefold increase in prediction accuracy in terms of the continuous ranked probability score compared to a state-of-the-art method on a large satellite data set. All methods are implemented in a free C++ software library with high-level Python and R packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12000v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pascal K\"undig, Fabio Sigrist</dc:creator>
    </item>
    <item>
      <title>Sampling low-fidelity outputs for estimation of high-fidelity density and its tails</title>
      <link>https://arxiv.org/abs/2402.17984</link>
      <description>arXiv:2402.17984v3 Announce Type: replace 
Abstract: In a multifidelity setting, data are available under the same conditions from two (or more) sources, e.g. computer codes, one being lower-fidelity but computationally cheaper, and the other higher-fidelity and more expensive. This work studies for which low-fidelity outputs, one should obtain high-fidelity outputs, if the goal is to estimate the probability density function of the latter, especially when it comes to the distribution tails and extremes. It is suggested to approach this problem from the perspective of the importance sampling of low-fidelity outputs according to some proposal distribution, combined with special considerations for the distribution tails based on extreme value theory. The notion of an optimal proposal distribution is introduced and investigated, in both theory and simulations. The approach is motivated and illustrated with an application to estimate the probability density function of record extremes of ship motions, obtained through two computer codes of different fidelities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17984v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minji Kim, Kevin O'Connor, Vladas Pipiras, Themistoklis Sapsis</dc:creator>
    </item>
    <item>
      <title>Batch Predictive Inference</title>
      <link>https://arxiv.org/abs/2409.13990</link>
      <description>arXiv:2409.13990v2 Announce Type: replace 
Abstract: Constructing prediction sets with coverage guarantees for unobserved outcomes is a core problem in modern statistics. Methods for predictive inference have been developed for a wide range of settings, but usually only consider test data points one at a time. Here we study the problem of distribution-free predictive inference for a batch of multiple test points, aiming to construct prediction sets for functions -- such as the mean or median -- of any number of unobserved test datapoints. This setting includes constructing simultaneous prediction sets with a high probability of coverage, and selecting datapoints satisfying a specified condition while controlling the number of false claims.
  For the general task of predictive inference on a function of a batch of test points, we introduce a methodology called batch predictive inference (batch PI), and provide a distribution-free coverage guarantee under exchangeability of the calibration and test data. Batch PI requires the quantiles of a rank ordering function defined on certain subsets of ranks. While computing these quantiles is NP-hard in general, we show that it can be done efficiently in many cases of interest, most notably for batch score functions with a compositional structure -- which includes examples of interest such as the mean -- via a dynamic programming algorithm that we develop. Batch PI has advantages over naive approaches (such as partitioning the calibration data or directly extending conformal prediction) in many settings, as it can deliver informative prediction sets even using small calibration sample sizes. We illustrate that our procedures provide informative inference across the use cases mentioned above, through experiments on both simulated data and a drug-target interaction dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13990v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yonghoon Lee, Eric Tchetgen Tchetgen, Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Scalable Expectation Propagation for Mixed-Effects Regression</title>
      <link>https://arxiv.org/abs/2409.14646</link>
      <description>arXiv:2409.14646v2 Announce Type: replace 
Abstract: Mixed-effects regression models represent a useful subclass of regression models for grouped data; the introduction of random effects allows for the correlation between observations within each group to be conveniently captured when inferring the fixed effects. At a time where such regression models are being fit to increasingly large datasets with many groups, it is ideal if (a) the time it takes to make the inferences scales linearly with the number of groups and (b) the inference workload can be distributed across multiple computational nodes in a numerically stable way, if the dataset cannot be stored in one location. Current Bayesian inference approaches for mixed-effects regression models do not seem to account for both challenges simultaneously. To address this, we develop an expectation propagation (EP) framework in this setting that is both scalable and numerically stable when distributed for the case where there is only one grouping factor. The main technical innovations lie in the sparse reparameterisation of the EP algorithm, and a moment propagation (MP) based refinement for multivariate random effect factor approximations. Experiments are conducted to show that this EP framework achieves linear scaling, while having comparable accuracy to other scalable approximate Bayesian inference (ABI) approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14646v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jackson Zhou, John T. Ormerod, Clara Grazian</dc:creator>
    </item>
    <item>
      <title>Perturbation theory for killed Markov processes and quasi-stationary distributions</title>
      <link>https://arxiv.org/abs/2109.13819</link>
      <description>arXiv:2109.13819v2 Announce Type: replace-cross 
Abstract: Motivated by recent developments of quasi-stationary Monte Carlo methods, we investigate the stability of quasi-stationary distributions of killed Markov processes under perturbations of the generator. We first consider a general bounded self-adjoint perturbation operator, and after that, study a particular unbounded perturbation corresponding to truncation of the killing rate. In both scenarios, we quantify the difference between eigenfunctions of the smallest eigenvalue of the perturbed and unperturbed generators in a Hilbert space norm. As a consequence, L1 norm estimates of the difference of the resulting quasi-stationary distributions in terms of the perturbation are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.13819v2</guid>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Rudolf, Andi Q. Wang</dc:creator>
    </item>
    <item>
      <title>High Dimensional Logistic Regression Under Network Dependence</title>
      <link>https://arxiv.org/abs/2110.03200</link>
      <description>arXiv:2110.03200v3 Announce Type: replace-cross 
Abstract: Logistic regression is key method for modeling the probability of a binary outcome based on a collection of covariates. However, the classical formulation of logistic regression relies on the independent sampling assumption, which is often violated when the outcomes interact through an underlying network structure, such as over a temporal/spatial domain or on a social network. This necessitates the development of models that can simultaneously handle both the network `peer-effect' and the effect of high-dimensional covariates. In this paper, we develop a framework for incorporating such dependencies in a high-dimensional logistic regression model by introducing a quadratic interaction term, as in the Ising model, designed to capture the pairwise interactions from the underlying network. The resulting model can also be viewed as an Ising model, where the node-dependent external fields linearly encode the high-dimensional covariates. We propose a penalized maximum pseudo-likelihood method for estimating the network peer-effect and the effect of the covariates (the regression coefficients), which, in addition to handling the high-dimensionality of the parameters, conveniently avoids the computational intractability of the maximum likelihood approach. Under various standard regularity conditions, we show that the corresponding estimate attains the classical high-dimensional rate of consistency. Our results imply that even under network dependence it is possible to consistently estimate the model parameters at the same rate as in classical (independent) logistic regression, when the true parameter is sparse and the underlying network is not too dense. We also develop an efficient algorithm for computing the estimates and validate our theoretical results in numerical experiments. An application to selecting genes in clustering spatial transcriptomics data is also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.03200v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Somabha Mukherjee, Ziang Niu, Sagnik Halder, Bhaswar B. Bhattacharya, George Michailidis</dc:creator>
    </item>
    <item>
      <title>Bayesian Mixtures Models with Repulsive and Attractive Atoms</title>
      <link>https://arxiv.org/abs/2302.09034</link>
      <description>arXiv:2302.09034v3 Announce Type: replace-cross 
Abstract: The study of almost surely discrete random probability measures is an active line of research in Bayesian nonparametrics. The idea of assuming interaction across the atoms of the random probability measure has recently spurred significant interest in the context of Bayesian mixture models. This allows the definition of priors that encourage well-separated and interpretable clusters. In this work, we provide a unified framework for the construction and the Bayesian analysis of random probability measures with interacting atoms, encompassing both repulsive and attractive behaviours. Specifically, we derive closed-form expressions for the posterior distribution, the marginal and predictive distributions, which were not previously available except for the case of measures with i.i.d. atoms. We show how these quantities are fundamental both for prior elicitation and to develop new posterior simulation algorithms for hierarchical mixture models. Our results are obtained without any assumption on the finite point process that governs the atoms of the random measure. Their proofs rely on analytical tools borrowed from the Palm calculus theory, which might be of independent interest. We specialise our treatment to the classes of Poisson, Gibbs, and determinantal point processes, as well as in the case of shot-noise Cox processes. Finally, we illustrate the performance of different modelling strategies on simulated and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09034v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Raffaele Argiento, Federico Camerlenghi, Alessandra Guglielmi</dc:creator>
    </item>
    <item>
      <title>Bayesian Federated Inference for regression models based on non-shared multicenter data sets from heterogeneous populations</title>
      <link>https://arxiv.org/abs/2402.02898</link>
      <description>arXiv:2402.02898v2 Announce Type: replace-cross 
Abstract: To estimate accurately the parameters of a regression model, the sample size must be large enough relative to the number of possible predictors for the model. In practice, sufficient data is often lacking, which can lead to overfitting of the model and, as a consequence, unreliable predictions of the outcome of new patients. Pooling data from different data sets collected in different (medical) centers would alleviate this problem, but is often not feasible due to privacy regulation or logistic problems. An alternative route would be to analyze the local data in the centers separately and combine the statistical inference results with the Bayesian Federated Inference (BFI) methodology. The aim of this approach is to compute from the inference results in separate centers what would have been found if the statistical analysis was performed on the combined data. We explain the methodology under homogeneity and heterogeneity across the populations in the separate centers, and give real life examples for better understanding. Excellent performance of the proposed methodology is shown. An R-package to do all the calculations has been developed and is illustrated in this paper. The mathematical details are given in the Appendix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02898v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marianne A Jonker, Hassan Pazira, Anthony CC Coolen</dc:creator>
    </item>
    <item>
      <title>Does AI help humans make better decisions? A methodological framework for experimental evaluation</title>
      <link>https://arxiv.org/abs/2403.12108</link>
      <description>arXiv:2403.12108v2 Announce Type: replace-cross 
Abstract: The use of Artificial Intelligence (AI), or more generally data-driven algorithms, has become ubiquitous in today's society. Yet, in many cases and especially when stakes are high, humans still make final decisions. The critical question, therefore, is whether AI helps humans make better decisions compared to a human-alone or AI-alone system. We introduce a new methodological framework to experimentally answer this question without additional assumptions. We measure a decision maker's ability to make correct decisions using standard classification metrics based on the baseline potential outcome. We consider a single-blinded experimental design, in which the provision of AI-generated recommendations is randomized across cases with humans making final decisions. Under this experimental design, we show how to compare the performance of three alternative decision-making systems -- human-alone, human-with-AI, and AI-alone. We also show when to provide a human-decision maker with AI recommendations and when they should follow such recommendations. We apply the proposed methodology to the data from our own randomized controlled trial of a pretrial risk assessment instrument. We find that the risk assessment recommendations do not improve the classification accuracy of a judge's decision to impose cash bail. Our analysis also shows that the risk assessment-alone decisions generally perform worse than human decisions with or without algorithmic assistance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12108v2</guid>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 25 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eli Ben-Michael, D. James Greiner, Melody Huang, Kosuke Imai, Zhichao Jiang, Sooahn Shin</dc:creator>
    </item>
  </channel>
</rss>
