<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Oct 2024 04:05:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Longitudinal efficient adjustment sets for time-varying treatment effect estimation in nonparametric causal graphical models</title>
      <link>https://arxiv.org/abs/2410.01000</link>
      <description>arXiv:2410.01000v1 Announce Type: new 
Abstract: Criteria for identifying optimal adjustment sets (i.e., yielding a consistent estimator with minimal asymptotic variance) for estimating average treatment effects in parametric and nonparametric models have recently been established. In a single treatment time point setting, it has been shown that the optimal adjustment set can be identified based on a causal directed acyclic graph alone. In a longitudinal treatment setting, previous work has established graphical rules to compare the asymptotic variance of estimators based on nested time-dependent adjustment sets. However, these rules do not always permit the identification of an optimal time-dependent adjustment set based on a causal graph alone. In this paper, we extend previous results by exploiting conditional independencies that can be read from the graph. We demonstrate theoretically and empirically that our results can yield estimators with even lower asymptotic variance than those allowed by previous results. We conjecture that our new results may even allow the identification of an optimal time-dependent adjustment set based on the causal graph and provide numerical examples supporting this conjecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01000v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Adenyo, Mireille E Schnitzer, David Berger, Jason R Guertin, Bernard Candas, Denis Talbot</dc:creator>
    </item>
    <item>
      <title>Minimax Optimal Probability Matrix Estimation For Graphon With Spectral Decay</title>
      <link>https://arxiv.org/abs/2410.01073</link>
      <description>arXiv:2410.01073v1 Announce Type: new 
Abstract: We study the optimal estimation of probability matrices of random graph models generated from graphons. This problem has been extensively studied in the case of step-graphons and H\"older smooth graphons. In this work, we characterize the regularity of graphons based on the decay rates of their eigenvalues. Our results show that for such classes of graphons, the minimax upper bound is achieved by a spectral thresholding algorithm and matches an information-theoretic lower bound up to a log factor. We provide insights on potential sources of this extra logarithm factor and discuss scenarios where exactly matching bounds can be obtained. This marks a difference from the step-graphon and H\"older smooth settings, because in those settings, there is a known computational-statistical gap where no polynomial time algorithm can achieve the statistical minimax rate. This contrast reflects a deeper observation that the spectral decay is an intrinsic feature of a graphon while smoothness is not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01073v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchen Chen, Jing Lei</dc:creator>
    </item>
    <item>
      <title>High-dimensional logistic regression with missing data: Imputation, regularization, and universality</title>
      <link>https://arxiv.org/abs/2410.01093</link>
      <description>arXiv:2410.01093v1 Announce Type: new 
Abstract: We study high-dimensional, ridge-regularized logistic regression in a setting in which the covariates may be missing or corrupted by additive noise. When both the covariates and the additive corruptions are independent and normally distributed, we provide exact characterizations of both the prediction error as well as the estimation error. Moreover, we show that these characterizations are universal: as long as the entries of the data matrix satisfy a set of independence and moment conditions, our guarantees continue to hold. Universality, in turn, enables the detailed study of several imputation-based strategies when the covariates are missing completely at random. We ground our study by comparing the performance of these strategies with the conjectured performance -- stemming from replica theory in statistical physics -- of the Bayes optimal procedure. Our analysis yields several insights including: (i) a distinction between single imputation and a simple variant of multiple imputation and (ii) that adding a simple ridge regularization term to single-imputed logistic regression can yield an estimator whose prediction error is nearly indistinguishable from the Bayes optimal prediction error. We supplement our findings with extensive numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01093v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kabir Aladin Verchand, Andrea Montanari</dc:creator>
    </item>
    <item>
      <title>Maximum Ideal Likelihood Estimator: An New Estimation and Inference Framework for Latent Variable Models</title>
      <link>https://arxiv.org/abs/2410.01194</link>
      <description>arXiv:2410.01194v1 Announce Type: new 
Abstract: In this paper, a new estimation framework, Maximum Ideal Likelihood Estimator (MILE), is proposed for general parametric models with latent variables and missing values. Instead of focusing on the marginal likelihood of the observed data as in many traditional approaches, the MILE directly considers the joint distribution of the complete dataset by treating the latent variables as parameters (the ideal likelihood). The MILE framework remains valid, even when traditional methods are not applicable, e.g., non-finite conditional expectation of the marginal likelihood function, via different optimization techniques and algorithms. The statistical properties of the MILE, such as the asymptotic equivalence to the Maximum Likelihood Estimation (MLE), are proved under some mild conditions, which facilitate statistical inference and prediction. Simulation studies illustrate that MILE outperforms traditional approaches with computational feasibility and scalability using existing and our proposed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01194v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizhou Cai, Ting Fung Ma</dc:creator>
    </item>
    <item>
      <title>Regularized e-processes: anytime valid inference with knowledge-based efficiency gains</title>
      <link>https://arxiv.org/abs/2410.01427</link>
      <description>arXiv:2410.01427v1 Announce Type: new 
Abstract: Classical statistical methods have theoretical justification when the sample size is predetermined by the data-collection plan. In applications, however, it's often the case that sample sizes aren't predetermined; instead, investigators might use the data observed along the way to make on-the-fly decisions about when to stop data collection. Since those methods designed for static sample sizes aren't reliable when sample sizes are dynamic, there's been a recent surge of interest in e-processes and the corresponding tests and confidence sets that are anytime valid in the sense that their justification holds up for arbitrary dynamic data-collection plans. But if the investigator has relevant-yet-incomplete prior information about the quantity of interest, then there's an opportunity for efficiency gain, but existing approaches can't accommodate this. Here I build a new, regularized e-process framework that features a knowledge-based, imprecise-probabilistic regularization that offers improved efficiency. A generalized version of Ville's inequality is established, ensuring that inference based on the regularized e-process remains anytime valid in a novel, knowledge-dependent sense. In addition to anytime valid hypothesis tests and confidence sets, the proposed regularized e-processes facilitate possibility-theoretic uncertainty quantification with strong frequentist-like calibration properties and other Bayesian-like features: satisfies the likelihood principle, avoids sure-loss, and offers formal decision-making with reliability guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01427v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Efficient Statistics With Unknown Truncation, Polynomial Time Algorithms, Beyond Gaussians</title>
      <link>https://arxiv.org/abs/2410.01656</link>
      <description>arXiv:2410.01656v1 Announce Type: new 
Abstract: We study the estimation of distributional parameters when samples are shown only if they fall in some unknown set $S \subseteq \mathbb{R}^d$. Kontonis, Tzamos, and Zampetakis (FOCS'19) gave a $d^{\mathrm{poly}(1/\varepsilon)}$ time algorithm for finding $\varepsilon$-accurate parameters for the special case of Gaussian distributions with diagonal covariance matrix. Recently, Diakonikolas, Kane, Pittas, and Zarifis (COLT'24) showed that this exponential dependence on $1/\varepsilon$ is necessary even when $S$ belongs to some well-behaved classes. These works leave the following open problems which we address in this work: Can we estimate the parameters of any Gaussian or even extend beyond Gaussians? Can we design $\mathrm{poly}(d/\varepsilon)$ time algorithms when $S$ is a simple set such as a halfspace?
  We make progress on both of these questions by providing the following results:
  1. Toward the first question, we give a $d^{\mathrm{poly}(\ell/\varepsilon)}$ time algorithm for any exponential family that satisfies some structural assumptions and any unknown set $S$ that is $\varepsilon$-approximable by degree-$\ell$ polynomials. This result has two important applications:
  1a) The first algorithm for estimating arbitrary Gaussian distributions from samples truncated to an unknown $S$; and
  1b) The first algorithm for linear regression with unknown truncation and Gaussian features.
  2. To address the second question, we provide an algorithm with runtime $\mathrm{poly}(d/\varepsilon)$ that works for a set of exponential families (containing all Gaussians) when $S$ is a halfspace or an axis-aligned rectangle.
  Along the way, we develop tools that may be of independent interest, including, a reduction from PAC learning with positive and unlabeled samples to PAC learning with positive and negative samples that is robust to certain covariate shifts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01656v1</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jane H. Lee, Anay Mehrotra, Manolis Zampetakis</dc:creator>
    </item>
    <item>
      <title>Revisiting Optimism and Model Complexity in the Wake of Overparameterized Machine Learning</title>
      <link>https://arxiv.org/abs/2410.01259</link>
      <description>arXiv:2410.01259v1 Announce Type: cross 
Abstract: Common practice in modern machine learning involves fitting a large number of parameters relative to the number of observations. These overparameterized models can exhibit surprising generalization behavior, e.g., ``double descent'' in the prediction error curve when plotted against the raw number of model parameters, or another simplistic notion of complexity. In this paper, we revisit model complexity from first principles, by first reinterpreting and then extending the classical statistical concept of (effective) degrees of freedom. Whereas the classical definition is connected to fixed-X prediction error (in which prediction error is defined by averaging over the same, nonrandom covariate points as those used during training), our extension of degrees of freedom is connected to random-X prediction error (in which prediction error is averaged over a new, random sample from the covariate distribution). The random-X setting more naturally embodies modern machine learning problems, where highly complex models, even those complex enough to interpolate the training data, can still lead to desirable generalization performance under appropriate conditions. We demonstrate the utility of our proposed complexity measures through a mix of conceptual arguments, theory, and experiments, and illustrate how they can be used to interpret and compare arbitrary prediction models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01259v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pratik Patil, Jin-Hong Du, Ryan J. Tibshirani</dc:creator>
    </item>
    <item>
      <title>Transformers Handle Endogeneity in In-Context Linear Regression</title>
      <link>https://arxiv.org/abs/2410.01265</link>
      <description>arXiv:2410.01265v1 Announce Type: cross 
Abstract: We explore the capability of transformers to address endogeneity in in-context linear regression. Our main finding is that transformers inherently possess a mechanism to handle endogeneity effectively using instrumental variables (IV). First, we demonstrate that the transformer architecture can emulate a gradient-based bi-level optimization procedure that converges to the widely used two-stage least squares $(\textsf{2SLS})$ solution at an exponential rate. Next, we propose an in-context pretraining scheme and provide theoretical guarantees showing that the global minimizer of the pre-training loss achieves a small excess loss. Our extensive experiments validate these theoretical findings, showing that the trained transformer provides more robust and reliable in-context predictions and coefficient estimates than the $\textsf{2SLS}$ method, in the presence of endogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01265v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haodong Liang, Krishnakumar Balasubramanian, Lifeng Lai</dc:creator>
    </item>
    <item>
      <title>Smaller Confidence Intervals From IPW Estimators via Data-Dependent Coarsening</title>
      <link>https://arxiv.org/abs/2410.01658</link>
      <description>arXiv:2410.01658v1 Announce Type: cross 
Abstract: Inverse propensity-score weighted (IPW) estimators are prevalent in causal inference for estimating average treatment effects in observational studies. Under unconfoundedness, given accurate propensity scores and $n$ samples, the size of confidence intervals of IPW estimators scales down with $n$, and, several of their variants improve the rate of scaling. However, neither IPW estimators nor their variants are robust to inaccuracies: even if a single covariate has an $\varepsilon&gt;0$ additive error in the propensity score, the size of confidence intervals of these estimators can increase arbitrarily. Moreover, even without errors, the rate with which the confidence intervals of these estimators go to zero with $n$ can be arbitrarily slow in the presence of extreme propensity scores (those close to 0 or 1).
  We introduce a family of Coarse IPW (CIPW) estimators that captures existing IPW estimators and their variants. Each CIPW estimator is an IPW estimator on a coarsened covariate space, where certain covariates are merged. Under mild assumptions, e.g., Lipschitzness in expected outcomes and sparsity of extreme propensity scores, we give an efficient algorithm to find a robust estimator: given $\varepsilon$-inaccurate propensity scores and $n$ samples, its confidence interval size scales with $\varepsilon+1/\sqrt{n}$. In contrast, under the same assumptions, existing estimators' confidence interval sizes are $\Omega(1)$ irrespective of $\varepsilon$ and $n$. Crucially, our estimator is data-dependent and we show that no data-independent CIPW estimator can be robust to inaccuracies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01658v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alkis Kalavasis, Anay Mehrotra, Manolis Zampetakis</dc:creator>
    </item>
    <item>
      <title>Paths of Stochastic Processes: a Sudden Turnaround</title>
      <link>https://arxiv.org/abs/2410.01788</link>
      <description>arXiv:2410.01788v1 Announce Type: cross 
Abstract: The commonly accepted definition of paths starts from a random field but ignores the problem of setting joint distributions of infinitely many random variables for defining paths properly afterwards. This paper provides a turnaround that starts with a given covariance function, then defines paths and finally a random field. We show how this approach retains essentially the same properties for Gaussian fields while allowing to construct random fields whose finite dimensional distributions are not Gaussian. Specifically, we start with a kernel $C$ and the associated Reproducing Kernel Hilbert Space ${\cal H}(C)$, and then assign standardized random values to a deterministic orthonormal expansion in ${\cal H}(C)$. This yields paths as random functions with an explicit representation formula. Using Lo\'eve isometry, we prove that pointwise regularity notions like continuity or differentiability hold on functions of ${\cal H}(C)$, paths, and the random field $R_C$ in precisely the same way. Yet, norms of paths as functions behave differently, as we prove that paths are a.s. not in ${\cal H}(C)$, but in certain larger spaces that can partially be characterized. In case of Matern kernels generating Sobolev space $W_2^m(R^d)$, paths lie almost surely in all $W_2^{p}(R^d)$ for $p&lt;m-d/2$, but almost surely not in $W_2^{m-d/2}(R^d)$. This regularity gap between function and paths is explained easily by square summability of expansion coefficients of functions, not of paths. The required orthonormal expansions, well-known in the probabilistic and the deterministic literature, are analyzed and compared with respect to convergence rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01788v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Robert Schaback, Emilio Porcu</dc:creator>
    </item>
    <item>
      <title>Minimax Signal Detection in Sparse Additive Models</title>
      <link>https://arxiv.org/abs/2304.09398</link>
      <description>arXiv:2304.09398v2 Announce Type: replace 
Abstract: Sparse additive models are an attractive choice in circumstances calling for modelling flexibility in the face of high dimensionality. We study the signal detection problem and establish the minimax separation rate for the detection of a sparse additive signal. Our result is nonasymptotic and applicable to the general case where the univariate component functions belong to a generic reproducing kernel Hilbert space. Unlike the estimation theory, the minimax separation rate reveals a nontrivial interaction between sparsity and the choice of function space. We also investigate adaptation to sparsity and establish an adaptive testing rate for a generic function space; adaptation is possible in some spaces while others impose an unavoidable cost. Finally, adaptation to both sparsity and smoothness is studied in the setting of Sobolev space, and we correct some existing claims in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.09398v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subhodh Kotekal, Chao Gao</dc:creator>
    </item>
    <item>
      <title>Time-Uniform Confidence Spheres for Means of Random Vectors</title>
      <link>https://arxiv.org/abs/2311.08168</link>
      <description>arXiv:2311.08168v3 Announce Type: replace 
Abstract: We derive and study time-uniform confidence spheres -- confidence sphere sequences (CSSs) -- which contain the mean of random vectors with high probability simultaneously across all sample sizes. Our results include a dimension-free CSS for log-concave random vectors, a dimension-free CSS for sub-Gaussian random vectors, and CSSs for sub-$\psi$ random vectors (which includes sub-gamma, sub-Poisson, and sub-exponential distributions). For sub-Gaussian distributions we also provide a CSS which tracks a time-varying mean, generalizing Robbins' mixture approach to the multivariate setting. Finally, we provide several CSSs for heavy-tailed random vectors (two moments only). Our bounds hold under a martingale assumption on the mean and do not require that the observations be iid. Our work is based on PAC-Bayesian theory and inspired by an approach of Catoni and Giulini.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.08168v3</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Chugg, Hongjian Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Estimation of on- and off-time distributions in a dynamic Erd\H{o}s-R\'enyi random graph</title>
      <link>https://arxiv.org/abs/2401.14531</link>
      <description>arXiv:2401.14531v2 Announce Type: replace 
Abstract: In this paper we consider a dynamic Erd\H{o}s-R\'enyi graph in which edges, according to an alternating renewal process, change from present to absent and vice versa. The objective is to estimate the on- and off-time distributions while only observing the aggregate number of edges. This inverse problem is dealt with, in a parametric context, by setting up an estimator based on the method of moments. We provide conditions under which the estimator is asymptotically normal, and we point out how the corresponding covariance matrix can be identified. It is also demonstrated how to adapt the estimation procedure if alternative subgraph counts are observed, such as the number of wedges or triangles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14531v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michel Mandjes, Jiesen Wang</dc:creator>
    </item>
    <item>
      <title>On the asymptotic normality of persistent Betti numbers</title>
      <link>https://arxiv.org/abs/1903.03280</link>
      <description>arXiv:1903.03280v4 Announce Type: replace-cross 
Abstract: Persistent Betti numbers are a major tool in persistent homology, a subfield of topological data analysis. Many tools in persistent homology rely on the properties of persistent Betti numbers considered as a two-dimensional stochastic process $ (r,s) \mapsto n^{-1/2} (\beta^{r,s}_q ( \mathcal{K}(n^{1/d} \mathcal{X}_n))-\mathbb{E}[\beta^{r,s}_q ( \mathcal{K}( n^{1/d} \mathcal{X}_n))])$. So far, pointwise limit theorems have been established in different settings. In particular, the pointwise asymptotic normality of (persistent) Betti numbers has been established for stationary Poisson processes and binomial processes with constant intensity function in the so-called critical (or thermodynamic) regime, see Yogeshwaran et al. [2017] and Hiraoka et al. [2018].
  In this contribution, we derive a strong stabilization property (in the spirit of Penrose and Yukich [2001] of persistent Betti numbers and generalize the existing results on the asymptotic normality to the multivariate case and to a broader class of underlying Poisson and binomial processes. Most importantly, we show that the multivariate asymptotic normality holds for all pairs $(r,s)$, $0\le r\le s&lt;\infty$, and that it is not affected by percolation effects in the underlying random geometric graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:1903.03280v4</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Krebs, Wolfgang Polonik</dc:creator>
    </item>
    <item>
      <title>Fitting an ellipsoid to a quadratic number of random points</title>
      <link>https://arxiv.org/abs/2307.01181</link>
      <description>arXiv:2307.01181v2 Announce Type: replace-cross 
Abstract: We consider the problem $(\mathrm{P})$ of fitting $n$ standard Gaussian random vectors in $\mathbb{R}^d$ to the boundary of a centered ellipsoid, as $n, d \to \infty$. This problem is conjectured to have a sharp feasibility transition: for any $\varepsilon &gt; 0$, if $n \leq (1 - \varepsilon) d^2 / 4$ then $(\mathrm{P})$ has a solution with high probability, while $(\mathrm{P})$ has no solutions with high probability if $n \geq (1 + \varepsilon) d^2 /4$. So far, only a trivial bound $n \geq d^2 / 2$ is known on the negative side, while the best results on the positive side assume $n \leq d^2 / \mathrm{polylog}(d)$. In this work, we improve over previous approaches using a key result of Bartl &amp; Mendelson (2022) on the concentration of Gram matrices of random vectors under mild assumptions on their tail behavior. This allows us to give a simple proof that $(\mathrm{P})$ is feasible with high probability when $n \leq d^2 / C$, for a (possibly large) constant $C &gt; 0$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.01181v2</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Afonso S. Bandeira, Antoine Maillard, Shahar Mendelson, Elliot Paquette</dc:creator>
    </item>
    <item>
      <title>Crafting Heavy-Tails in Weight Matrix Spectrum without Gradient Noise</title>
      <link>https://arxiv.org/abs/2406.04657</link>
      <description>arXiv:2406.04657v2 Announce Type: replace-cross 
Abstract: Training strategies for modern deep neural networks (NNs) tend to induce a heavy-tailed (HT) empirical spectral density (ESD) in the layer weights. While previous efforts have shown that the HT phenomenon correlates with good generalization in large NNs, a theoretical explanation of its occurrence is still lacking. Especially, understanding the conditions which lead to this phenomenon can shed light on the interplay between generalization and weight spectra. Our work aims to bridge this gap by presenting a simple, rich setting to model the emergence of HT ESD. In particular, we present a theory-informed analysis for 'crafting' heavy tails in the ESD of two-layer NNs without any gradient noise. This is the first work to analyze a noise-free setting and incorporate optimizer (GD/Adam) dependent (large) learning rates into the HT ESD analysis. Our results highlight the role of learning rates on the Bulk+Spike and HT shape of the ESDs in the early phase of training, which can facilitate generalization in the two-layer NN. These observations shed light on the behavior of large-scale NNs, albeit in a much simpler setting. Last but not least, we present a novel perspective on the ESD evolution dynamics by analyzing the singular vectors of weight matrices and optimizer updates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04657v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vignesh Kothapalli, Tianyu Pang, Shenyang Deng, Zongmin Liu, Yaoqing Yang</dc:creator>
    </item>
    <item>
      <title>A Modified Satterthwaite (1941,1946) Effective Degrees of Freedom Approximation</title>
      <link>https://arxiv.org/abs/2409.14606</link>
      <description>arXiv:2409.14606v3 Announce Type: replace-cross 
Abstract: This study introduces a correction to the approximation of effective degrees of freedom as proposed by Satterthwaite (1941, 1946), specifically addressing scenarios where component degrees of freedom are small. The correction is grounded in analytical results concerning the moments of standard normal random variables. This modification is applicable to complex variance estimates that involve both small and large degrees of freedom, offering an enhanced approximation of the higher moments required by Satterthwaite's framework. Additionally, this correction extends and partially validates the empirically derived adjustment by Johnson &amp; Rust (1992), as it is based on theoretical foundations rather than simulations used to derive empirical transformation constants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14606v3</guid>
      <category>stat.OT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matthias von Davier</dc:creator>
    </item>
  </channel>
</rss>
