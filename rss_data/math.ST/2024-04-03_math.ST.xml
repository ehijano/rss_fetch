<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Apr 2024 04:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Convex relaxation for the generalized maximum-entropy sampling problem</title>
      <link>https://arxiv.org/abs/2404.01390</link>
      <description>arXiv:2404.01390v1 Announce Type: new 
Abstract: The generalized maximum-entropy sampling problem (GMESP) is to select an order-$s$ principal submatrix from an order-$n$ covariance matrix, to maximize the product of its $t$ greatest eigenvalues, $0&lt;t\leq s &lt;n$. It is a problem that specializes to two fundamental problems in statistical design theory:(i) maximum-entropy sampling problem (MESP); (ii) binary D-optimality (D-Opt). In the general case, it is motivated by a selection problem in the context of PCA (principal component analysis).
  We introduce the first convex-optimization based relaxation for GMESP, study its behavior, compare it to an earlier spectral bound, and demonstrate its use in a branch-and-bound scheme. We find that such an approach is practical when $s-t$ is very small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01390v1</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Ponte, Marcia Fampa, Jon Lee</dc:creator>
    </item>
    <item>
      <title>Asymptotics of resampling without replacement in robust and logistic regression</title>
      <link>https://arxiv.org/abs/2404.02070</link>
      <description>arXiv:2404.02070v1 Announce Type: new 
Abstract: This paper studies the asymptotics of resampling without replacement in the proportional regime where dimension $p$ and sample size $n$ are of the same order. For a given dataset $(\bm{X},\bm{y})\in\mathbb{R}^{n\times p}\times \mathbb{R}^n$ and fixed subsample ratio $q\in(0,1)$, the practitioner samples independently of $(\bm{X},\bm{y})$ iid subsets $I_1,...,I_M$ of $\{1,...,n\}$ of size $q n$ and trains estimators $\bm{\hat{\beta}}(I_1),...,\bm{\hat{\beta}}(I_M)$ on the corresponding subsets of rows of $(\bm{X},\bm{y})$. Understanding the performance of the bagged estimate $\bm{\bar{\beta}} = \frac1M\sum_{m=1}^M \bm{\hat{\beta}}(I_1),...,\bm{\hat{\beta}}(I_M)$, for instance its squared error, requires us to understand correlations between two distinct $\bm{\hat{\beta}}(I_m)$ and $\bm{\hat{\beta}}(I_{m'})$ trained on different subsets $I_m$ and $I_{m'}$. In robust linear regression and logistic regression, we characterize the limit in probability of the correlation between two estimates trained on different subsets of the data. The limit is characterized as the unique solution of a simple nonlinear equation. We further provide data-driven estimators that are consistent for estimating this limit. These estimators of the limiting correlation allow us to estimate the squared error of the bagged estimate $\bm{\bar{\beta}}$, and for instance perform parameter tuning to choose the optimal subsample ratio $q$. As a by-product of the proof argument, we obtain the limiting distribution of the bivariate pair $(\bm{x}_i^T \bm{\hat{\beta}}(I_m), \bm{x}_i^T \bm{\hat{\beta}}(I_{m'}))$ for observations $i\in I_m\cap I_{m'}$, i.e., for observations used to train both estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02070v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre C Bellec, Takuya Koriyama</dc:creator>
    </item>
    <item>
      <title>Methods of Stochastic Field Theory in Non-Equilibrium Systems -- Spontaneous Symmetry Breaking of Ergodicity</title>
      <link>https://arxiv.org/abs/2404.01333</link>
      <description>arXiv:2404.01333v1 Announce Type: cross 
Abstract: Recently, a couple of investigations related to symmetry breaking phenomena, 'spontaneous stochasticity' and 'ergodicity breaking' have led to significant impacts in a variety of fields related to the stochastic processes such as economics and finance. We investigate on the origins and effects of those original symmetries in the action from the mathematical and the effective field theory points of view. It is naturally expected that whenever the system respects any symmetry, it would be spontaneously broken once the system falls into a vacuum state which minimizes an effective action of the dynamical system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01333v1</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>econ.GN</category>
      <category>math.ST</category>
      <category>q-fin.EC</category>
      <category>q-fin.GN</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>Expansion of net correlations in terms of partial correlations</title>
      <link>https://arxiv.org/abs/2404.01734</link>
      <description>arXiv:2404.01734v1 Announce Type: cross 
Abstract: Graphical models are usually employed to represent statistical relationships between pairs of variables when all the remaining variables are fixed. In this picture, conditionally independent pairs are disconnected. In the real world, however, strict conditional independence is almost impossible to prove. Here we use a weaker version of the concept of graphical models, in which only the linear component of the conditional dependencies is represented. This notion enables us to relate the marginal Pearson correlation coefficient (a measure of linear marginal dependence) with the partial correlations (a measure of linear conditional dependence). Specifically, we use the graphical model to express the marginal Pearson correlation $\rho_{ij}$ between variables $X_i$ and $X_j$ as a sum of the efficacies with which messages propagate along all the paths connecting the variables in the graph. The expansion is convergent, and provides a mechanistic interpretation of how global correlations arise from local interactions. Moreover, by weighing the relevance of each path and of each intermediate node, an intuitive way to imagine interventions is enabled, revealing for example what happens when a given edge is pruned, or the weight of an edge is modified. The expansion is also useful to construct minimal equivalent models, in which latent variables are introduced to replace a larger number of marginalised variables. In addition, the expansion yields an alternative algorithm to calculate marginal Pearson correlations, particularly beneficial when partial correlation matrix inversion is difficult. Finally, for Gaussian variables, the mutual information is also related to message-passing efficacies along paths in the graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01734v1</guid>
      <category>stat.ME</category>
      <category>cond-mat.stat-mech</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bautista Arenaza, Sebasti\'an Risau-Gusman, In\'es Samengo</dc:creator>
    </item>
    <item>
      <title>Least Squares Inference for Data with Network Dependency</title>
      <link>https://arxiv.org/abs/2404.01977</link>
      <description>arXiv:2404.01977v1 Announce Type: cross 
Abstract: We address the inference problem concerning regression coefficients in a classical linear regression model using least squares estimates. The analysis is conducted under circumstances where network dependency exists across units in the sample. Neglecting the dependency among observations may lead to biased estimation of the asymptotic variance and often inflates the Type I error in coefficient inference. In this paper, we first establish a central limit theorem for the ordinary least squares estimate, with a verifiable dependence condition alongside corresponding neighborhood growth conditions. Subsequently, we propose a consistent estimator for the asymptotic variance of the estimated coefficients, which employs a data-driven method to balance the bias-variance trade-off. We find that the optimal tuning depends on the linear hypothesis under consideration and must be chosen adaptively. The presented theory and methods are illustrated and supported by numerical experiments and a data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01977v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jing Lei, Kehui Chen, Haeun Moon</dc:creator>
    </item>
    <item>
      <title>Edge differentially private estimation in the $\beta$-model via jittering and method of moments</title>
      <link>https://arxiv.org/abs/2112.10151</link>
      <description>arXiv:2112.10151v2 Announce Type: replace 
Abstract: A standing challenge in data privacy is the trade-off between the level of privacy and the efficiency of statistical inference. Here we conduct an in-depth study of this trade-off for parameter estimation in the $\beta$-model (Chatterjee, Diaconis and Sly, 2011) for edge differentially private network data released via jittering (Karwa, Krivitsky and Slavkovi\'{c}, 2017). Unlike most previous approaches based on maximum likelihood estimation for this network model, we proceed via method-of-moments. This choice facilitates our exploration of a substantially broader range of privacy levels - corresponding to stricter privacy - than has been to date. Over this new range we discover our proposed estimator for the parameters exhibits an interesting phase transition, with both its convergence rate and asymptotic variance following one of three different regimes of behavior depending on the level of privacy. Because identification of the operable regime is difficult if not impossible in practice, we devise a novel adaptive bootstrap procedure to construct uniform inference across different phases. In fact, leveraging this bootstrap we are able to provide for simultaneous inference of all parameters in the $\beta$-model (i.e., equal to the number of nodes), which, to our best knowledge, is the first result of its kind. Numerical experiments confirm the competitive and reliable finite sample performance of the proposed inference methods, next to a comparable maximum likelihood method, as well as significant advantages in terms of computational speed and memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.10151v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Qiao Hu, Eric D. Kolaczyk, Qiwei Yao, Fengting Yi</dc:creator>
    </item>
    <item>
      <title>Theory of functional principal component analysis for discretely observed data</title>
      <link>https://arxiv.org/abs/2209.08768</link>
      <description>arXiv:2209.08768v4 Announce Type: replace 
Abstract: Functional data analysis is an important research field in statistics which treats data as random functions drawn from some infinite-dimensional functional space, and functional principal component analysis (FPCA) based on eigen-decomposition plays a central role for data reduction and representation. After nearly three decades of research, there remains a key problem unsolved, namely, the perturbation analysis of covariance operator for diverging number of eigencomponents obtained from noisy and discretely observed data. This is fundamental for studying models and methods based on FPCA, while there has not been substantial progress since Hall, M\"uller and Wang (2006)'s result for a fixed number of eigenfunction estimates. In this work, we aim to establish a unified theory for this problem, obtaining upper bounds for eigenfunctions with diverging indices in both the $\mathcal{L}^2$ and supremum norms, and deriving the asymptotic distributions of eigenvalues for a wide range of sampling schemes. Our results provide insight into the phenomenon when the $\mathcal{L}^{2}$ bound of eigenfunction estimates with diverging indices is minimax optimal as if the curves are fully observed, and reveal the transition of convergence rates from nonparametric to parametric regimes in connection to sparse or dense sampling. We also develop a double truncation technique to handle the uniform convergence of estimated covariance and eigenfunctions. The technical arguments in this work are useful for handling the perturbation series with noisy and discretely observed functional data and can be applied in models or those involving inverse problems based on FPCA as regularization, such as functional linear regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.08768v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hang Zhou, Dongyi Wei, Fang Yao</dc:creator>
    </item>
    <item>
      <title>Statistical inference for pairwise comparison models</title>
      <link>https://arxiv.org/abs/2401.08463</link>
      <description>arXiv:2401.08463v2 Announce Type: replace 
Abstract: Pairwise comparison models have been widely used for utility evaluation and ranking across various fields. The increasing scale of problems today underscores the need to understand statistical inference in these models when the number of subjects diverges, a topic currently lacking in the literature except in a few special instances. To partially address this gap, this paper establishes a near-optimal asymptotic normality result for the maximum likelihood estimator in a broad class of pairwise comparison models, as well as a non-asymptotic convergence rate for each individual subject under comparison. The key idea lies in identifying the Fisher information matrix as a weighted graph Laplacian, which can be studied via a meticulous spectral analysis. Our findings provide a unified theory for performing statistical inference in a wide range of pairwise comparison models beyond the Bradley--Terry model, benefiting practitioners with theoretical guarantees for their use. Simulations utilizing synthetic data are conducted to validate the asymptotic normality result, followed by a hypothesis test using a tennis competition dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08463v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruijian Han, Wenlu Tang, Yiming Xu</dc:creator>
    </item>
    <item>
      <title>Modularity and partially observed graphs</title>
      <link>https://arxiv.org/abs/2112.13190</link>
      <description>arXiv:2112.13190v2 Announce Type: replace-cross 
Abstract: Suppose that there is an unknown underlying graph $G$ on a large vertex set, and we can test only a proportion of the possible edges to check whether they are present in $G$. If $G$ has high modularity, is the observed graph $G'$ likely to have high modularity? We see that this is indeed the case under a mild condition, in a natural model where we test edges at random. We find that $q^*(G') \geq q^*(G)-\varepsilon$ with probability at least $1-\varepsilon$, as long as the expected number edges in $G'$ is large enough. Similarly, $q^*(G') \leq q^*(G)+\varepsilon$ with probability at least $1-\varepsilon$, under the stronger condition that the expected average degree in $G'$ is large enough. Further, under this stronger condition, finding a good partition for $G'$ helps us to find a good partition for $G$.
  We also consider the vertex sampling model for partially observing the underlying graph: we find that for dense underlying graphs we may estimate the modularity by sampling constantly many vertices and observing the corresponding induced subgraph, but this does not hold for underlying graphs with a subquadratic number of edges. Finally we deduce some related results, for example showing that under-sampling tends to lead to overestimation of modularity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.13190v2</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <category>cs.SI</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Colin McDiarmid, Fiona Skerman</dc:creator>
    </item>
  </channel>
</rss>
