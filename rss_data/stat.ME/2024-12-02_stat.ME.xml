<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Dec 2024 05:00:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Temporal Models for Demographic and Global Health Outcomes in Multiple Populations: Introducing the Normal-with-Optional-Shrinkage Data Model Class</title>
      <link>https://arxiv.org/abs/2411.18646</link>
      <description>arXiv:2411.18646v1 Announce Type: new 
Abstract: Statistical models are used to produce estimates of demographic and global health indicators in populations with limited data. Such models integrate multiple data sources to produce estimates and forecasts with uncertainty based on model assumptions. Model assumptions can be divided into assumptions that describe latent trends in the indicator of interest versus assumptions on the data generating process of the observed data, conditional on the latent process value. Focusing on the latter, we introduce a class of data models that can be used to combine data from multiple sources with various reporting issues. The proposed data model accounts for sampling errors and differences in observational uncertainty based on survey characteristics. In addition, the data model employs horseshoe priors to produce estimates that are robust to outlying observations. We refer to the data model class as the normal-with-optional-shrinkage (NOS) set up. We illustrate the use of the NOS data model for the estimation of modern contraceptive use and other family planning indicators at the national level for countries globally, using survey data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18646v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Leontine Alkema, Herbert Susmann, Evan Ray</dc:creator>
    </item>
    <item>
      <title>A Bayesian semi-parametric approach to causal mediation for longitudinal mediators and time-to-event outcomes with application to a cardiovascular disease cohort study</title>
      <link>https://arxiv.org/abs/2411.18739</link>
      <description>arXiv:2411.18739v1 Announce Type: new 
Abstract: Causal mediation analysis of observational data is an important tool for investigating the potential causal effects of medications on disease-related risk factors, and on time-to-death (or disease progression) through these risk factors. However, when analyzing data from a cohort study, such analyses are complicated by the longitudinal structure of the risk factors and the presence of time-varying confounders. Leveraging data from the Atherosclerosis Risk in Communities (ARIC) cohort study, we develop a causal mediation approach, using (semi-parametric) Bayesian Additive Regression Tree (BART) models for the longitudinal and survival data. Our framework allows for time-varying exposures, confounders, and mediators, all of which can either be continuous or binary. We also identify and estimate direct and indirect causal effects in the presence of a competing event. We apply our methods to assess how medication, prescribed to target cardiovascular disease (CVD) risk factors, affects the time-to-CVD death.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18739v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saurabh Bhandari, Michael J. Daniels, Maria Josefsson, Donald M. Lloyd-Jones, Juned Siddique</dc:creator>
    </item>
    <item>
      <title>Difference-in-differences Design with Outcomes Missing Not at Random</title>
      <link>https://arxiv.org/abs/2411.18772</link>
      <description>arXiv:2411.18772v1 Announce Type: new 
Abstract: This paper addresses one of the most prevalent problems encountered by political scientists working with difference-in-differences (DID) design: missingness in panel data. A common practice for handling missing data, known as complete case analysis, is to drop cases with any missing values over time. A more principled approach involves using nonparametric bounds on causal effects or applying inverse probability weighting based on baseline covariates. Yet, these methods are general remedies that often under-utilize the assumptions already imposed on panel structure for causal identification. In this paper, I outline the pitfalls of complete case analysis and propose an alternative identification strategy based on principal strata. To be specific, I impose parallel trends assumption within each latent group that shares the same missingness pattern (e.g., always-respondents, if-treated-respondents) and leverage missingness rates over time to estimate the proportions of these groups. Building on this, I tailor Lee bounds, a well-known nonparametric bounds under selection bias, to partially identify the causal effect within the DID design. Unlike complete case analysis, the proposed method does not require independence between treatment selection and missingness patterns, nor does it assume homogeneous effects across these patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18772v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sooahn Shin</dc:creator>
    </item>
    <item>
      <title>Inference on Dynamic Spatial Autoregressive Models with Change Point Detection</title>
      <link>https://arxiv.org/abs/2411.18773</link>
      <description>arXiv:2411.18773v1 Announce Type: new 
Abstract: We analyze a varying-coefficient dynamic spatial autoregressive model with spatial fixed effects. One salient feature of the model is the incorporation of multiple spatial weight matrices through their linear combinations with varying coefficients, which help solve the problem of choosing the most "correct" one for applied econometricians who often face the availability of multiple expert spatial weight matrices. We estimate and make inferences on the model coefficients and coefficients in basis expansions of the varying coefficients through penalized estimations, establishing the oracle properties of the estimators and the consistency of the overall estimated spatial weight matrix, which can be time-dependent. We further consider two applications of our model in change point detections in dynamic spatial autoregressive models, providing theoretical justifications in consistent change point locations estimation and practical implementations. Simulation experiments demonstrate the performance of our proposed methodology, and a real data analysis is also carried out.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18773v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zetai Cen, Yudong Chen, Clifford Lam</dc:creator>
    </item>
    <item>
      <title>Improved order selection method for hidden Markov models: a case study with movement data</title>
      <link>https://arxiv.org/abs/2411.18826</link>
      <description>arXiv:2411.18826v1 Announce Type: new 
Abstract: Hidden Markov models (HMMs) are a versatile statistical framework commonly used in ecology to characterize behavioural patterns from animal movement data. In HMMs, the observed data depend on a finite number of underlying hidden states, generally interpreted as the animal's unobserved behaviour. The number of states is a crucial parameter, controlling the trade-off between ecological interpretability of behaviours (fewer states) and the goodness of fit of the model (more states). Selecting the number of states, commonly referred to as order selection, is notoriously challenging. Common model selection metrics, such as AIC and BIC, often perform poorly in determining the number of states, particularly when models are misspecified. Building on existing methods for HMMs and mixture models, we propose a double penalized likelihood maximum estimate (DPMLE) for the simultaneous estimation of the number of states and parameters of non-stationary HMMs. The DPMLE differs from traditional information criteria by using two penalty functions on the stationary probabilities and state-dependent parameters. For non-stationary HMMs, forward and backward probabilities are used to approximate stationary probabilities. Using a simulation study that includes scenarios with additional complexity in the data, we compare the performance of our method with that of AIC and BIC. We also illustrate how the DPMLE differs from AIC and BIC using narwhal (Monodon monoceros) movement data. The proposed method outperformed AIC and BIC in identifying the correct number of states under model misspecification. Furthermore, its capacity to handle non-stationary dynamics allowed for more realistic modeling of complex movement data, offering deeper insights into narwhal behaviour. Our method is a powerful tool for order selection in non-stationary HMMs, with potential applications extending beyond the field of ecology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18826v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fanny Dupont, Marianne Marcoux, Nigel Hussey, Marie Auger-M\'eth\'e</dc:creator>
    </item>
    <item>
      <title>Redesigning the ensemble Kalman filter with a dedicated model of epistemic uncertainty</title>
      <link>https://arxiv.org/abs/2411.18864</link>
      <description>arXiv:2411.18864v1 Announce Type: new 
Abstract: The problem of incorporating information from observations received serially in time is widespread in the field of uncertainty quantification. Within a probabilistic framework, such problems can be addressed using standard filtering techniques. However, in many real-world problems, some (or all) of the uncertainty is epistemic, arising from a lack of knowledge, and is difficult to model probabilistically. This paper introduces a possibilistic ensemble Kalman filter designed for this setting and characterizes some of its properties. Using possibility theory to describe epistemic uncertainty is appealing from a philosophical perspective, and it is easy to justify certain heuristics often employed in standard ensemble Kalman filters as principled approaches to capturing uncertainty within it. The possibilistic approach motivates a robust mechanism for characterizing uncertainty which shows good performance with small sample sizes, and can outperform standard ensemble Kalman filters at given sample size, even when dealing with genuinely aleatoric uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18864v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chatchuea Kimchaiwong, Jeremie Houssineau, Adam M. Johansen</dc:creator>
    </item>
    <item>
      <title>Learning treatment effects under covariate dependent left truncation and right censoring</title>
      <link>https://arxiv.org/abs/2411.18879</link>
      <description>arXiv:2411.18879v1 Announce Type: new 
Abstract: In observational studies with delayed entry, causal inference for time-to-event outcomes can be challenging. The challenges arise because, in addition to the potential confounding bias from observational data, the collected data often also suffers from the selection bias due to left truncation, where only subjects with time-to-event (such as death) greater than the enrollment times are included, as well as bias from informative right censoring. To estimate the treatment effects on time-to-event outcomes in such settings, inverse probability weighting (IPW) is often employed. However, IPW is sensitive to model misspecifications, which makes it vulnerable, especially when faced with three sources of biases. Moreover, IPW is inefficient. To address these challenges, we propose a doubly robust framework to handle covariate dependent left truncation and right censoring that can be applied to a wide range of estimation problems, including estimating average treatment effect (ATE) and conditional average treatment effect (CATE). For average treatment effect, we develop estimators that enjoy model double robustness and rate double robustness. For conditional average treatment effect, we develop orthogonal and doubly robust learners that can achieve oracle rate of convergence. Our framework represents the first attempt to construct doubly robust estimators and orthogonal learners for treatment effects that account for all three sources of biases: confounding, selection from covariate-induced dependent left truncation, and informative right censoring. We apply the proposed methods to analyze the effect of midlife alcohol consumption on late-life cognitive impairment, using data from Honolulu Asia Aging Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18879v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuyao Wang, Andrew Ying, Ronghui Xu</dc:creator>
    </item>
    <item>
      <title>Bayesian Cluster Weighted Gaussian Models</title>
      <link>https://arxiv.org/abs/2411.18957</link>
      <description>arXiv:2411.18957v1 Announce Type: new 
Abstract: We introduce a novel class of Bayesian mixtures for normal linear regression models which incorporates a further Gaussian random component for the distribution of the predictor variables. The proposed cluster-weighted model aims to encompass potential heterogeneity in the distribution of the response variable as well as in the multivariate distribution of the covariates for detecting signals relevant to the underlying latent structure. Of particular interest are potential signals originating from: (i) the linear predictor structures of the regression models and (ii) the covariance structures of the covariates. We model these two components using a lasso shrinkage prior for the regression coefficients and a graphical-lasso shrinkage prior for the covariance matrices. A fully Bayesian approach is followed for estimating the number of clusters, by treating the number of mixture components as random and implementing a trans-dimensional telescoping sampler. Alternative Bayesian approaches based on overfitting mixture models or using information criteria to select the number of components are also considered. The proposed method is compared against EM type implementation, mixtures of regressions and mixtures of experts. The method is illustrated using a set of simulation studies and a biomedical dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18957v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Panagiotis Papastamoulis, Konstantinos Perrakis</dc:creator>
    </item>
    <item>
      <title>Algorithmic modelling of a complex redundant multi-state system subject to multiple events, preventive maintenance, loss of units and a multiple vacation policy through a MMAP</title>
      <link>https://arxiv.org/abs/2411.19104</link>
      <description>arXiv:2411.19104v1 Announce Type: new 
Abstract: A complex multi-state redundant system undergoing preventive maintenance and experiencing multiple events is being considered in a continuous time frame. The online unit is susceptible to various types of failures, both internal and external in nature, with multiple degradation levels present, both internally and externally. Random inspections are continuously monitoring these degradation levels, and if they reach a critical state, the unit is directed to a repair facility for preventive maintenance. The repair facility is managed by a single repairperson, who follows a multiple vacation policy dependent on the operational status of the units. The repairperson is responsible for two primary tasks: corrective repairs and preventive maintenance. The time durations within the system follow phase-type distributions, and the model is constructed using Markovian Arrival Processes with marked arrivals. A variety of performance measures, including transient and stationary distributions, are calculated using matrix-analytic methods. This approach enables the expression of key results and overall system behaviour in a matrix-algorithmic format. In order to optimize the model, costs and rewards are integrated into the analysis. A numerical example is presented to showcase the model's flexibility and effectiveness in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19104v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.matcom.2024.11.005</arxiv:DOI>
      <arxiv:journal_reference>Mathematics and Computers in Simulation, Volume 230, April 2025, Pages 165-192</arxiv:journal_reference>
      <dc:creator>Juan Eloy Ruiz-Castro, Hugo Ala\'in Zapata-Ceballos</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation on the circle based on Fej\'er polynomials</title>
      <link>https://arxiv.org/abs/2411.19138</link>
      <description>arXiv:2411.19138v1 Announce Type: new 
Abstract: This paper presents a comprehensive study of nonparametric estimation techniques on the circle using Fej\'er polynomials, which are analogues of Bernstein polynomials for periodic functions. Building upon Fej\'er's uniform approximation theorem, the paper introduces circular density and distribution function estimators based on Fej\'er kernels. It establishes their theoretical properties, including uniform strong consistency and asymptotic expansions. The proposed methods are extended to account for measurement errors by incorporating classical and Berkson error models, adjusting the Fej\'er estimator to mitigate their effects. Simulation studies analyze the finite-sample performance of these estimators under various scenarios, including mixtures of circular distributions and measurement error models. An application to rainfall data demonstrates the practical application of the proposed estimators, demonstrating their robustness and effectiveness in the presence of rounding-induced Berkson errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19138v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bernhard Klar, Bojana Milo\v{s}evi\'c, Marko Obradovi\'c</dc:creator>
    </item>
    <item>
      <title>A Unified Bayesian Framework for Mortality Model Selection</title>
      <link>https://arxiv.org/abs/2411.19176</link>
      <description>arXiv:2411.19176v1 Announce Type: new 
Abstract: In recent years, a wide range of mortality models has been proposed to address the diverse factors influencing mortality rates, which has highlighted the need to perform model selection. Traditional mortality model selection methods, such as AIC and BIC, often require fitting multiple models independently and ranking them based on these criteria. This process can fail to account for uncertainties in model selection, which can lead to overly optimistic prediction interval, and it disregards the potential insights from combining models. To address these limitations, we propose a novel Bayesian model selection framework that integrates model selection and parameter estimation into the same process. This requires creating a model building framework that will give rise to different models by choosing different parametric forms for each term. Inference is performed using the reversible jump Markov chain Monte Carlo algorithm, which is devised to allow for transition between models of different dimensions, as is the case for the models considered here. We develop modelling frameworks for data stratified by age and period and for data stratified by age, period and product. Our results are presented in two case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19176v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Diana, Jackie Wong Siaw Tze, Aniketh Pittea</dc:creator>
    </item>
    <item>
      <title>Flexible space-time models for extreme data</title>
      <link>https://arxiv.org/abs/2411.19184</link>
      <description>arXiv:2411.19184v1 Announce Type: new 
Abstract: Extreme Value Analysis is an essential methodology in the study of rare and extreme events, which hold significant interest in various fields, particularly in the context of environmental sciences. Models that employ the exceedances of values above suitably selected high thresholds possess the advantage of capturing the "sub-asymptotic" dependence of data. This paper presents an extension of spatial random scale mixture models to the spatio-temporal domain. A comprehensive framework for characterizing the dependence structure of extreme events across both dimensions is provided. Indeed, the model is capable of distinguishing between asymptotic dependence and independence, both in space and time, through the use of parametric inference. The high complexity of the likelihood function for the proposed model necessitates a simulation approach based on neural networks for parameter estimation, which leverages summaries of the sub-asymptotic dependence present in the data. The effectiveness of the model in assessing the limiting dependence structure of spatio-temporal processes is demonstrated through both simulation studies and an application to rainfall datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19184v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Dell'Oro, Carlo Gaetan</dc:creator>
    </item>
    <item>
      <title>On the application of Jammalamadaka-Jim\'enez Gamero-Meintanis test for circular regression model assessment</title>
      <link>https://arxiv.org/abs/2411.19205</link>
      <description>arXiv:2411.19205v1 Announce Type: new 
Abstract: We study a circular-circular multiplicative regression model, characterized by an angular error distribution assumed to be wrapped Cauchy. We propose a specification procedure for this model, focusing on adapting a recently proposed goodness-of-fit test for circular distributions. We derive its limiting properties and study the power performance of the test through extensive simulations, including the adaptation of some other well-known goodness-of-fit tests for this type of data. To emphasize the practical relevance of our methodology, we apply it to several small real-world datasets and wind direction measurements in the Black Forest region of southwestern Germany, demonstrating the power and versatility of the presented approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19205v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Katarina Halaj, Bernhard Klar, Bojana Milo\v{s}evi\'c, Mirjana Veljovi\'c</dc:creator>
    </item>
    <item>
      <title>Sparse optimization for estimating the cross-power spectrum in linear inverse models : from theory to the application in brain connectivity</title>
      <link>https://arxiv.org/abs/2411.19225</link>
      <description>arXiv:2411.19225v1 Announce Type: new 
Abstract: In this work we present a computationally efficient linear optimization approach for estimating the cross--power spectrum of an hidden multivariate stochastic process from that of another observed process. Sparsity in the resulting estimator of the cross--power is induced through $\ell_1$ regularization and the Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) is used for computing such an estimator. With respect to a standard implementation, we prove that a proper initialization step is sufficient to guarantee the required symmetric and antisymmetric properties of the involved quantities. Further, we show how structural properties of the forward operator can be exploited within the FISTA update in order to make our approach adequate also for large--scale problems such as those arising in context of brain functional connectivity.
  The effectiveness of the proposed approach is shown in a practical scenario where we aim at quantifying the statistical relationships between brain regions in the context of non-invasive electromagnetic field recordings. Our results show that our method provide results with an higher specificity that classical approaches based on a two--step procedure where first the hidden process describing the brain activity is estimated through a linear optimization step and then the cortical cross--power spectrum is computed from the estimated time--series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19225v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Carini, Isabella Furci, Sara Sommariva</dc:creator>
    </item>
    <item>
      <title>Distribution-Free Calibration of Statistical Confidence Sets</title>
      <link>https://arxiv.org/abs/2411.19368</link>
      <description>arXiv:2411.19368v1 Announce Type: new 
Abstract: Constructing valid confidence sets is a crucial task in statistical inference, yet traditional methods often face challenges when dealing with complex models or limited observed sample sizes. These challenges are frequently encountered in modern applications, such as Likelihood-Free Inference (LFI). In these settings, confidence sets may fail to maintain a confidence level close to the nominal value. In this paper, we introduce two novel methods, TRUST and TRUST++, for calibrating confidence sets to achieve distribution-free conditional coverage. These methods rely entirely on simulated data from the statistical model to perform calibration. Leveraging insights from conformal prediction techniques adapted to the statistical inference context, our methods ensure both finite-sample local coverage and asymptotic conditional coverage as the number of simulations increases, even if n is small. They effectively handle nuisance parameters and provide computationally efficient uncertainty quantification for the estimated confidence sets. This allows users to assess whether additional simulations are necessary for robust inference. Through theoretical analysis and experiments on models with both tractable and intractable likelihoods, we demonstrate that our methods outperform existing approaches, particularly in small-sample regimes. This work bridges the gap between conformal prediction and statistical inference, offering practical tools for constructing valid confidence sets in complex models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19368v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luben M. C. Cabezas, Guilherme P. Soares, Thiago R. Ramos, Rafael B. Stern, Rafael Izbicki</dc:creator>
    </item>
    <item>
      <title>Random Effects Misspecification and its Consequences for Prediction in Generalized Linear Mixed Models</title>
      <link>https://arxiv.org/abs/2411.19384</link>
      <description>arXiv:2411.19384v1 Announce Type: new 
Abstract: When fitting generalized linear mixed models (GLMMs), one important decision to make relates to the choice of the random effects distribution. As the random effects are unobserved, misspecification of this distribution is a real possibility. In this article, we investigate the consequences of random effects misspecification for point prediction and prediction inference in GLMMs, a topic on which there is considerably less research compared to consequences for parameter estimation and inference. We use theory, simulation, and a real application to explore the effect of using the common normality assumption for the random effects distribution when the correct specification is a mixture of normal distributions, focusing on the impacts on point prediction, mean squared prediction errors (MSEPs), and prediction intervals. We found that the optimal shrinkage is different under the two random effect distributions, so is impacted by misspecification. The unconditional MSEPs for the random effects are almost always larger under the misspecified normal random effects distribution, especially when cluster sizes are small. Results for the MSEPs conditional on the random effects are more complicated, but they remain generally larger under the misspecified distribution when the true random effect is close to the mean of one of the component distributions in the true mixture distribution. Results for prediction intervals indicate that overall coverage probability is not greatly impacted by misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19384v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quan Vu, Francis K. C. Hui, Samuel Muller, A. H. Welsh</dc:creator>
    </item>
    <item>
      <title>Bayesian Hierarchical Modeling for Predicting Spatially Correlated Curves in Irregular Domains: A Case Study on PM10 Pollution</title>
      <link>https://arxiv.org/abs/2411.19425</link>
      <description>arXiv:2411.19425v1 Announce Type: new 
Abstract: This study presents a Bayesian hierarchical model for analyzing spatially correlated functional data and handling irregularly spaced observations. The model uses Bernstein polynomial (BP) bases combined with autoregressive random effects, allowing for nuanced modeling of spatial correlations between sites and dependencies of observations within curves. Moreover, the proposed procedure introduces a distinct structure for the random effect component compared to previous works. Simulation studies conducted under various challenging scenarios verify the model's robustness, demonstrating its capacity to accurately recover spatially dependent curves and predict observations at unmonitored locations. The model's performance is further supported by its application to real-world data, specifically PM$_{10}$ particulate matter measurements from a monitoring network in Mexico City. This application is of practical importance, as particles can penetrate the respiratory system and aggravate various health conditions. The model effectively predicts concentrations at unmonitored sites, with uncertainty estimates that reflect spatial variability across the domain. This new methodology provides a flexible framework for the FDA in spatial contexts and addresses challenges in analyzing irregular domains with potential applications in environmental monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19425v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alvaro Alexander Burbano Moreno, Ronaldo Dias</dc:creator>
    </item>
    <item>
      <title>Unsupervised Variable Selection for Ultrahigh-Dimensional Clustering Analysis</title>
      <link>https://arxiv.org/abs/2411.19448</link>
      <description>arXiv:2411.19448v1 Announce Type: new 
Abstract: Compared to supervised variable selection, the research on unsupervised variable selection is far behind. A forward partial-variable clustering full-variable loss (FPCFL) method is proposed for the corresponding challenges. An advantage is that the FPCFL method can distinguish active, redundant, and uninformative variables, which the previous methods cannot achieve. Theoretical and simulation studies show that the performance of a clustering method using all the variables can be worse if many uninformative variables are involved. Better results are expected if the uninformative variables are excluded. The research addresses a previous concern about how variable selection affects the performance of clustering. Rather than many previous methods attempting to select all the relevant variables, the proposed method selects a subset that can induce an equally good result. This phenomenon does not appear in the supervised variable selection problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19448v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tonglin Zhang, Huyunting Huang</dc:creator>
    </item>
    <item>
      <title>Density-Calibrated Conformal Quantile Regression</title>
      <link>https://arxiv.org/abs/2411.19523</link>
      <description>arXiv:2411.19523v1 Announce Type: new 
Abstract: This paper introduces the Density-Calibrated Conformal Quantile Regression (CQR-d) method, a novel approach for constructing prediction intervals that adapts to varying uncertainty across the feature space. Building upon conformal quantile regression, CQR-d incorporates local information through a weighted combination of local and global conformity scores, where the weights are determined by local data density. We prove that CQR-d provides valid marginal coverage at level $1 - \alpha - \epsilon$, where $\epsilon$ represents a small tolerance from numerical optimization. Through extensive simulation studies and an application to the a heteroscedastic dataset available in R, we demonstrate that CQR-d maintains the desired coverage while producing substantially narrower prediction intervals compared to standard conformal quantile regression (CQR). Notably, in our application on heteroscedastic data, CQR-d achieves an $8.6\%$ reduction in average interval width while maintaining comparable coverage. The method's effectiveness is particularly pronounced in settings with clear local uncertainty patterns, making it a valuable tool for prediction tasks in heterogeneous data environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19523v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Lu</dc:creator>
    </item>
    <item>
      <title>Isotropy testing in spatial point patterns: nonparametric versus parametric replication under misspecification</title>
      <link>https://arxiv.org/abs/2411.19633</link>
      <description>arXiv:2411.19633v1 Announce Type: new 
Abstract: Several hypothesis testing methods have been proposed to validate the assumption of isotropy in spatial point patterns. A majority of these methods are characterised by an unknown distribution of the test statistic under the null hypothesis of isotropy. Parametric approaches to approximating the distribution involve simulation of patterns from a user-specified isotropic model. Alternatively, nonparametric replicates of the test statistic under isotropy can be used to waive the need for specifying a model. In this paper, we first develop a general framework which allows for the integration of a selected nonparametric replication method into isotropy testing. We then conduct a large simulation study comprising application-like scenarios to assess the performance of tests with different parametric and nonparametric replication methods. In particular, we explore distortions in test size and power caused by model misspecification, and demonstrate the advantages of nonparametric replication in such scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19633v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakub J. Pypkowski, Adam M. Sykulski, James S. Martin</dc:creator>
    </item>
    <item>
      <title>Adjusting auxiliary variables under approximate neighborhood interference</title>
      <link>https://arxiv.org/abs/2411.19789</link>
      <description>arXiv:2411.19789v1 Announce Type: new 
Abstract: Randomized experiments are the gold standard for causal inference. However, traditional assumptions, such as the Stable Unit Treatment Value Assumption (SUTVA), often fail in real-world settings where interference between units is present. Network interference, in particular, has garnered significant attention. Structural models, like the linear-in-means model, are commonly used to describe interference; but they rely on the correct specification of the model, which can be restrictive. Recent advancements in the literature, such as the Approximate Neighborhood Interference (ANI) framework, offer more flexible approaches by assuming negligible interference from distant units. In this paper, we introduce a general framework for regression adjustment for the network experiments under the ANI assumption. This framework expands traditional regression adjustment by accounting for imbalances in network-based covariates, ensuring precision improvement, and providing shorter confidence intervals. We establish the validity of our approach using a design-based inference framework, which relies solely on randomization of treatment assignments for inference without requiring correctly specified outcome models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19789v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Lu, Yuhao Wang, Zhiheng Zhang</dc:creator>
    </item>
    <item>
      <title>Thompson, Ulam, or Gauss? Multi-criteria recommendations for posterior probability computation methods in Bayesian response-adaptive trials</title>
      <link>https://arxiv.org/abs/2411.19871</link>
      <description>arXiv:2411.19871v1 Announce Type: new 
Abstract: To implement a Bayesian response-adaptive trial it is necessary to evaluate a sequence of posterior probabilities. This sequence is often approximated by simulation due to the unavailability of closed-form formulae to compute it exactly. Approximating these probabilities by simulation can be computationally expensive and impact the accuracy or the range of scenarios that may be explored. An alternative approximation method based on Gaussian distributions can be faster but its accuracy is not guaranteed. The literature lacks practical recommendations for selecting approximation methods and comparing their properties, particularly considering trade-offs between computational speed and accuracy. In this paper, we focus on the case where the trial has a binary endpoint with Beta priors. We first outline an efficient way to compute the posterior probabilities exactly for any number of treatment arms. Then, using exact probability computations, we show how to benchmark calculation methods based on considerations of computational speed, patient benefit, and inferential accuracy. This is done through a range of simulations in the two-armed case, as well as an analysis of the three-armed Established Status Epilepticus Treatment Trial. Finally, we provide practical guidance for which calculation method is most appropriate in different settings, and how to choose the number of simulations if the simulation-based approximation method is used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19871v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Kaddaj, Lukas Pin, Stef Baas, Edwin Y. N. Tang, David S. Robertson, Sof\'ia S. Villar</dc:creator>
    </item>
    <item>
      <title>Transfer Learning for High-dimensional Quantile Regression with Distribution Shift</title>
      <link>https://arxiv.org/abs/2411.19933</link>
      <description>arXiv:2411.19933v1 Announce Type: new 
Abstract: Information from related source studies can often enhance the findings of a target study. However, the distribution shift between target and source studies can severely impact the efficiency of knowledge transfer. In the high-dimensional regression setting, existing transfer approaches mainly focus on the parameter shift. In this paper, we focus on the high-dimensional quantile regression with knowledge transfer under three types of distribution shift: parameter shift, covariate shift, and residual shift. We propose a novel transferable set and a new transfer framework to address the above three discrepancies. Non-asymptotic estimation error bounds and source detection consistency are established to validate the availability and superiority of our method in the presence of distribution shift. Additionally, an orthogonal debiased approach is proposed for statistical inference with knowledge transfer, leading to sharper asymptotic results. Extensive simulation results as well as real data applications further demonstrate the effectiveness of our proposed procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19933v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiqi Bai, Yijiao Zhang, Hanbo Yang, Zhongyi Zhu</dc:creator>
    </item>
    <item>
      <title>Double Descent in Portfolio Optimization: Dance between Theoretical Sharpe Ratio and Estimation Accuracy</title>
      <link>https://arxiv.org/abs/2411.18830</link>
      <description>arXiv:2411.18830v1 Announce Type: cross 
Abstract: We study the relationship between model complexity and out-of-sample performance in the context of mean-variance portfolio optimization. Representing model complexity by the number of assets, we find that the performance of low-dimensional models initially improves with complexity but then declines due to overfitting. As model complexity becomes sufficiently high, the performance improves with complexity again, resulting in a double ascent Sharpe ratio curve similar to the double descent phenomenon observed in artificial intelligence. The underlying mechanisms involve an intricate interaction between the theoretical Sharpe ratio and estimation accuracy. In high-dimensional models, the theoretical Sharpe ratio approaches its upper limit, and the overfitting problem is reduced because there are more parameters than data restrictions, which allows us to choose well-behaved parameters based on inductive bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18830v1</guid>
      <category>q-fin.PM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yonghe Lu, Yanrong Yang, Terry Zhang</dc:creator>
    </item>
    <item>
      <title>ZIPG-SK: A Novel Knockoff-Based Approach for Variable Selection in Multi-Source Count Data</title>
      <link>https://arxiv.org/abs/2411.18986</link>
      <description>arXiv:2411.18986v1 Announce Type: cross 
Abstract: The rapid development of sequencing technology has generated complex, highly skewed, and zero-inflated multi-source count data. This has posed significant challenges in variable selection, which is crucial for uncovering shared disease mechanisms, such as tumor development and metabolic dysregulation. In this study, we propose a novel variable selection method called Zero-Inflated Poisson-Gamma based Simultaneous knockoff (ZIPG-SK) for multi-source count data. To address the highly skewed and zero-inflated properties of count data, we introduce a Gaussian copula based on the ZIPG distribution for constructing knockoffs, while also incorporating the information of covariates. This method successfully detects common features related to the results in multi-source data while controlling the false discovery rate (FDR). Additionally, our proposed method effectively combines e-values to enhance power. Extensive simulations demonstrate the superiority of our method over Simultaneous Knockoff and other existing methods in processing count data, as it improves power across different scenarios. Finally, we validated the method by applying it to two real-world multi-source datasets: colorectal cancer (CRC) and type 2 diabetes (T2D). The identified variable characteristics are consistent with existing studies and provided additional insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18986v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shan Tang, Shanjun Mao, Shourong Ma, Falong Tan</dc:creator>
    </item>
    <item>
      <title>On the Unknowable Limits to Prediction</title>
      <link>https://arxiv.org/abs/2411.19223</link>
      <description>arXiv:2411.19223v1 Announce Type: cross 
Abstract: This short Correspondence critiques the classic dichotomization of prediction error into reducible and irreducible components, noting that certain types of error can be eliminated at differential speeds. We propose an improved analytical framework that better distinguishes epistemic from aleatoric uncertainty, emphasizing that predictability depends on information sets and cautioning against premature claims of unpredictability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19223v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiani Yan, Charles Rahal</dc:creator>
    </item>
    <item>
      <title>Canonical correlation analysis of stochastic trends via functional approximation</title>
      <link>https://arxiv.org/abs/2411.19572</link>
      <description>arXiv:2411.19572v1 Announce Type: cross 
Abstract: This paper proposes a novel canonical correlation analysis for semiparametric inference in $I(1)/I(0)$ systems via functional approximation. The approach can be applied coherently to panels of $p$ variables with a generic number $s$ of stochastic trends, as well as to subsets or aggregations of variables. This study discusses inferential tools on $s$ and on the loading matrix $\psi$ of the stochastic trends (and on their duals $r$ and $\beta$, the cointegration rank and the cointegrating matrix): asymptotically pivotal test sequences and consistent estimators of $s$ and $r$, $T$-consistent, mixed Gaussian and efficient estimators of $\psi$ and $\beta$, Wald tests thereof, and misspecification tests for checking model assumptions. Monte Carlo simulations show that these tools have reliable performance uniformly in $s$ for small, medium and large-dimensional systems, with $p$ ranging from 10 to 300. An empirical analysis of 20 exchange rates illustrates the methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19572v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Massimo Franchi, Iliyan Georgiev, Paolo Paruolo</dc:creator>
    </item>
    <item>
      <title>Nonparametric Estimation for a Log-concave Distribution Function with Interval-censored Data</title>
      <link>https://arxiv.org/abs/2411.19878</link>
      <description>arXiv:2411.19878v1 Announce Type: cross 
Abstract: We consider the nonparametric maximum likelihood estimation for the underlying event time based on mixed-case interval-censored data, under a log-concavity assumption on its distribution function. This generalized framework relaxes the assumptions of a log-concave density function or a concave distribution function considered in the literature. A log-concave distribution function is fulfilled by many common parametric families in survival analysis and also allows for multi-modal and heavy-tailed distributions. We establish the existence, uniqueness and consistency of the log-concave nonparametric maximum likelihood estimator. A computationally efficient procedure that combines an active set algorithm with the iterative convex minorant algorithm is proposed. Numerical studies demonstrate the advantages of incorporating additional shape constraint compared to the unconstrained nonparametric maximum likelihood estimator. The results also show that our method achieves a balance between efficiency and robustness compared to assuming log-concavity in the density. An R package iclogcondist is developed to implement our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19878v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chi Wing Chu, Hok Kan Ling, Chaoyu Yuan</dc:creator>
    </item>
    <item>
      <title>Cross Validation for Penalized Quantile Regression with a Case-Weight Adjusted Solution Path</title>
      <link>https://arxiv.org/abs/1902.07770</link>
      <description>arXiv:1902.07770v2 Announce Type: replace 
Abstract: Cross validation is widely used for selecting tuning parameters in regularization methods, but it is computationally intensive in general. To lessen its computational burden, approximation schemes such as generalized approximate cross validation (GACV) are often employed. However, such approximations may not work well when non-smooth loss functions are involved. As a case in point, approximate cross validation schemes for penalized quantile regression do not work well for extreme quantiles. In this paper, we propose a new algorithm to compute the leave-one-out cross validation scores exactly for quantile regression with ridge penalty through a case-weight adjusted solution path. Resorting to the homotopy technique in optimization, we introduce a case weight for each individual data point as a continuous embedding parameter and decrease the weight gradually from one to zero to link the estimators based on the full data and those with a case deleted. This allows us to design a solution path algorithm to compute all leave-one-out estimators very efficiently from the full-data solution. We show that the case-weight adjusted solution path is piecewise linear in the weight parameter, and using the solution path, we examine case influences comprehensively and observe that different modes of case influences emerge, depending on the specified quantiles, data dimensions and penalty parameter. We further illustrate the utility of the proposed algorithm in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:1902.07770v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shanshan Tu, Yunzhang Zhu, Yoonkyung Lee, Qiuyu Gu, Haozhen Yu</dc:creator>
    </item>
    <item>
      <title>A smoothed-Bayesian approach to frequency recovery from sketched data</title>
      <link>https://arxiv.org/abs/2309.15408</link>
      <description>arXiv:2309.15408v3 Announce Type: replace 
Abstract: We provide a novel statistical perspective on a classical problem at the intersection of computer science and information theory: recovering the empirical frequency of a symbol in a large discrete dataset using only a compressed representation, or sketch, obtained via random hashing. Departing from traditional algorithmic approaches, recent works have proposed Bayesian nonparametric (BNP) methods that can provide more informative frequency estimates by leveraging modeling assumptions about the distribution of the sketched data. In this paper, we propose a smoothed-Bayesian method, inspired by existing BNP approaches but designed in a frequentist framework to overcome the computational limitations of the BNP approaches when dealing with large-scale data from realistic distributions, including those with power-law tail behaviors. For sketches obtained with a single hash function, our approach is supported by rigorous frequentist properties, including unbiasedness and optimality under a squared error loss function within an intuitive class of linear estimators. For sketches with multiple hash functions, we introduce an approach based on multi-view learning to construct computationally efficient frequency estimators. We validate our method on synthetic and real data, comparing its performance to that of existing alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15408v3</guid>
      <category>stat.ME</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Stefano Favaro, Matteo Sesia</dc:creator>
    </item>
    <item>
      <title>Reconstruct Ising Model with Global Optimality via SLIDE</title>
      <link>https://arxiv.org/abs/2310.09257</link>
      <description>arXiv:2310.09257v2 Announce Type: replace 
Abstract: The reconstruction of interaction networks between random events is a critical problem arising from statistical physics and politics, sociology, biology, psychology, and beyond. The Ising model lays the foundation for this reconstruction process, but finding the underlying Ising model from the least amount of observed samples in a computationally efficient manner has been historically challenging for half a century. Using sparsity learning, we present an approach named SLIDE whose sample complexity is globally optimal. Furthermore, a tuning-free algorithm is developed to give a statistically consistent solution of SLIDE in polynomial time with high probability. On extensive benchmarked cases, the SLIDE approach demonstrates dominant performance in reconstructing underlying Ising models, confirming its superior statistical properties. The application on the U.S. senators voting in the last six congresses reveals that both the Republicans and Democrats noticeably assemble in each congress; interestingly, the assembling of Democrats is particularly pronounced in the latest congress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09257v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuanyu Chen, Jin Zhu, Junxian Zhu, Xueqin Wang, Heping Zhang</dc:creator>
    </item>
    <item>
      <title>Robust Multi-Model Subset Selection</title>
      <link>https://arxiv.org/abs/2311.13202</link>
      <description>arXiv:2311.13202v5 Announce Type: replace 
Abstract: Outlying observations can be challenging to handle and adversely affect subsequent analyses, particularly, in complex high-dimensional datasets. Although outliers are not always undesired anomalies in the data and may possess valuable insights, only methods that are robust to outliers are able to accurately identify them and resist their influence. In this paper, we propose a method that generates an ensemble of sparse and diverse predictive models that are resistant to outliers. We show that the ensembles generally outperform single-model sparse and robust methods in high-dimensional prediction tasks. Cross-validation is used to tune model parameters to control levels of sparsity, diversity and resistance to outliers. We establish the finitesample breakdown point of the ensembles and the models that comprise them, and we develop a tailored computing algorithm to learn the ensembles by leveraging recent developments in L0 optimization. Our extensive numerical experiments on synthetic and artificially contaminated real datasets from bioinformatics and cheminformatics demonstrate the competitive advantage of our method over state-of-the-art single-model methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13202v5</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anthony-Alexander Christidis, Gabriela Cohen-Freue</dc:creator>
    </item>
    <item>
      <title>The Mollified (Discrete) Uniform Distribution and its Applications</title>
      <link>https://arxiv.org/abs/2403.00383</link>
      <description>arXiv:2403.00383v2 Announce Type: replace 
Abstract: The mollified uniform distribution is rediscovered, which constitutes a ``soft'' version of the continuous uniform distribution. Important stochastic properties are presented and used to demonstrate potential fields of applications. For example, it constitutes a model covering platykurtic, mesokurtic and leptokurtic shapes. Its cumulative distribution function may also serve as the soft-clipping response function for defining generalized linear models with approximately linear dependence. Furthermore, it might be considered for teaching, as an appealing example for the convolution of random variables. Finally, a discrete type of mollified uniform distribution is briefly discussed as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00383v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian H. Wei{\ss}</dc:creator>
    </item>
    <item>
      <title>Weighted Particle-Based Optimization for Efficient Generalized Posterior Calibration</title>
      <link>https://arxiv.org/abs/2405.04845</link>
      <description>arXiv:2405.04845v4 Announce Type: replace 
Abstract: In the realm of statistical learning, the increasing volume of accessible data and increasing model complexity necessitate robust methodologies. This paper explores two branches of robust Bayesian methods in response to this trend. The first is generalized Bayesian inference, which introduces a learning rate parameter to enhance robustness against model misspecifications. The second is Gibbs posterior inference, which formulates inferential problems using generic loss functions rather than probabilistic models. In such approaches, it is necessary to calibrate the spread of the posterior distribution by selecting a learning rate parameter. The study aims to enhance the generalized posterior calibration (GPC) algorithm proposed by [1]. Their algorithm chooses the learning rate to achieve the nominal frequentist coverage probability, but it is computationally intensive because it requires repeated posterior simulations for bootstrap samples. We propose a more efficient version of the GPC inspired by sequential Monte Carlo (SMC) samplers. A target distribution with a different learning rate is evaluated without posterior simulation as in the reweighting step in SMC sampling. Thus, the proposed algorithm can reach the desirable value within a few iterations. This improvement substantially reduces the computational cost of the GPC. Its efficacy is demonstrated through synthetic and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04845v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICoDSA62899.2024.10651910</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 7th International Conference on Data Science and Its Applications 2024 (ICoDSA), pp. 515-521</arxiv:journal_reference>
      <dc:creator>Masahiro Tanaka</dc:creator>
    </item>
    <item>
      <title>A general approach to fitting multistate cure models based on an extended-long-format data structure</title>
      <link>https://arxiv.org/abs/2409.09865</link>
      <description>arXiv:2409.09865v2 Announce Type: replace 
Abstract: A multistate cure model is a statistical framework used to analyze and represent the transitions individuals undergo between different states over time, accounting for the possibility of being cured by initial treatment. This model is particularly useful in pediatric oncology where a proportion of the patient population achieves cure through treatment and therefore will never experience certain events. Despite its importance, no universal consensus exists on the structure of multistate cure models. Our study provides a novel framework for defining such models through a set of non-cure states. We develops a generalized algorithm based on the extended long data format, an extension of the traditional long data format, where a transition can be divided into two rows, each with a weight assigned reflecting the posterior probability of its cure status. The multistate cure model is built upon the current framework of multistate model and mixture cure model. The proposed algorithm makes use of the Expectation-Maximization (EM) algorithm and weighted likelihood representation such that it is easy to implement with standard packages. Additionally, it facilitates dynamic prediction. The algorithm is applied on data from the European Society for Blood and Marrow Transplantation (EBMT). Standard errors of the estimated parameters in the EM algorithm are obtained via a non-parametric bootstrap procedure, while the method involving the calculation of the second-derivative matrix of the observed log-likelihood is also presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09865v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yilin Jiang, Harm van Tinteren, Marta Fiocco</dc:creator>
    </item>
    <item>
      <title>Parsimonious Dynamic Mode Decomposition: A Robust and Automated Approach for Optimally Sparse Mode Selection in Complex Systems</title>
      <link>https://arxiv.org/abs/2410.16656</link>
      <description>arXiv:2410.16656v2 Announce Type: replace 
Abstract: This paper introduces the Parsimonious Dynamic Mode Decomposition (parsDMD), a novel algorithm designed to automatically select an optimally sparse subset of dynamic modes for both spatiotemporal and purely temporal data. By incorporating time-delay embedding and leveraging Orthogonal Matching Pursuit (OMP), parsDMD ensures robustness against noise and effectively handles complex, nonlinear dynamics. The algorithm is validated on a diverse range of datasets, including standing wave signals, identifying hidden dynamics, fluid dynamics simulations (flow past a cylinder and transonic buffet), and atmospheric sea-surface temperature (SST) data. ParsDMD addresses a significant limitation of the traditional sparsity-promoting DMD (spDMD), which requires manual tuning of sparsity parameters through a rigorous trial-and-error process to balance between single-mode and all-mode solutions. In contrast, parsDMD autonomously determines the optimally sparse subset of modes without user intervention, while maintaining minimal computational complexity. Comparative analyses demonstrate that parsDMD consistently outperforms spDMD by providing more accurate mode identification and effective reconstruction in noisy environments. These advantages render parsDMD an effective tool for real-time diagnostics, forecasting, and reduced-order model construction across various disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16656v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.DS</category>
      <category>physics.data-an</category>
      <category>physics.flu-dyn</category>
      <category>stat.ML</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arpan Das, Pier Marzocca, Oleg Levinski</dc:creator>
    </item>
    <item>
      <title>Efficient inference for differential equation models without numerical solvers</title>
      <link>https://arxiv.org/abs/2411.10494</link>
      <description>arXiv:2411.10494v3 Announce Type: replace 
Abstract: Parameter inference is essential when interpreting observational data using mathematical models. Standard inference methods for differential equation models typically rely on obtaining repeated numerical solutions of the differential equation(s). Recent results have explored how numerical truncation error can have major, detrimental, and sometimes hidden impacts on likelihood-based inference by introducing false local maxima into the log-likelihood function. We present a straightforward approach for inference that eliminates the need for solving the underlying differential equations, thereby completely avoiding the impact of truncation error. Open-access Jupyter notebooks, available on GitHub, allow others to implement this method for a broad class of widely-used models to interpret biological data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10494v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Johnston, Oliver J. Maclaren, Ruth E. Baker, Matthew J. Simpson</dc:creator>
    </item>
    <item>
      <title>Fast post-process Bayesian inference with Variational Sparse Bayesian Quadrature</title>
      <link>https://arxiv.org/abs/2303.05263</link>
      <description>arXiv:2303.05263v3 Announce Type: replace-cross 
Abstract: In applied Bayesian inference scenarios, users may have access to a large number of pre-existing model evaluations, for example from maximum-a-posteriori (MAP) optimization runs. However, traditional approximate inference techniques make little to no use of this available information. We propose the framework of post-process Bayesian inference as a means to obtain a quick posterior approximation from existing target density evaluations, with no further model calls. Within this framework, we introduce Variational Sparse Bayesian Quadrature (VSBQ), a method for post-process approximate inference for models with black-box and potentially noisy likelihoods. VSBQ reuses existing target density evaluations to build a sparse Gaussian process (GP) surrogate model of the log posterior density function. Subsequently, we leverage sparse-GP Bayesian quadrature combined with variational inference to achieve fast approximate posterior inference over the surrogate. We validate our method on challenging synthetic scenarios and real-world applications from computational neuroscience. The experiments show that VSBQ builds high-quality posterior approximations by post-processing existing optimization traces, with no further model evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.05263v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengkun Li, Gr\'egoire Clart\'e, Martin J{\o}rgensen, Luigi Acerbi</dc:creator>
    </item>
    <item>
      <title>Asymptotics for non-degenerate multivariate $U$-statistics with estimated nuisance parameters under the null and local alternative hypotheses</title>
      <link>https://arxiv.org/abs/2401.11272</link>
      <description>arXiv:2401.11272v3 Announce Type: replace-cross 
Abstract: The large-sample behavior of non-degenerate multivariate $U$-statistics of arbitrary degree is investigated under the assumption that their kernel depends on parameters that can be estimated consistently. Mild regularity conditions are provided which guarantee that once properly normalized, such statistics are asymptotically multivariate Gaussian both under the null hypothesis and sequences of local alternatives. The work of Randles (1982, Ann. Statist.) is extended in three ways: the data and the kernel values can be multivariate rather than univariate, the limiting behavior under local alternatives is studied for the first time, and the effect of knowing some of the nuisance parameters is quantified. These results can be applied to a broad range of goodness-of-fit testing contexts, as shown in two specific examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11272v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of Multivariate Analysis (2025)</arxiv:journal_reference>
      <dc:creator>Alain Desgagn\'e, Christian Genest, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>A General Control-Theoretic Approach for Reinforcement Learning: Theory and Algorithms</title>
      <link>https://arxiv.org/abs/2406.14753</link>
      <description>arXiv:2406.14753v3 Announce Type: replace-cross 
Abstract: We devise a control-theoretic reinforcement learning approach to support direct learning of the optimal policy. We establish various theoretical properties of our approach, such as convergence and optimality of our analog of the Bellman operator and Q-learning, a new control-policy-variable gradient theorem, and a specific gradient ascent algorithm based on this theorem within the context of a specific control-theoretic framework. We empirically evaluate the performance of our control theoretic approach on several classical reinforcement learning tasks, demonstrating significant improvements in solution quality, sample complexity, and running time of our approach over state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14753v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weiqin Chen, Mark S. Squillante, Chai Wah Wu, Santiago Paternain</dc:creator>
    </item>
    <item>
      <title>Quantile processes and their applications in finite populations</title>
      <link>https://arxiv.org/abs/2407.21238</link>
      <description>arXiv:2407.21238v2 Announce Type: replace-cross 
Abstract: The weak convergence of the quantile processes, which are constructed based on different estimators of the finite population quantiles, is shown under various well-known sampling designs based on a superpopulation model. The results related to the weak convergence of these quantile processes are applied to find asymptotic distributions of the smooth $L$-estimators and the estimators of smooth functions of finite population quantiles. Based on these asymptotic distributions, confidence intervals are constructed for several finite population parameters like the median, the $\alpha$-trimmed means, the interquartile range and the quantile based measure of skewness. Comparisons of various estimators are carried out based on their asymptotic distributions. We show that the use of the auxiliary information in the construction of the estimators sometimes has an adverse effect on the performances of the smooth $L$-estimators and the estimators of smooth functions of finite population quantiles under several sampling designs. Further, the performance of each of the above-mentioned estimators sometimes becomes worse under sampling designs, which use the auxiliary information, than their performances under simple random sampling without replacement (SRSWOR).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21238v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1214/24-AOS2432</arxiv:DOI>
      <arxiv:journal_reference>The Annals of Statistics, 52(5), 2194-2216 (2024)</arxiv:journal_reference>
      <dc:creator>Anurag Dey, Probal Chaudhuri</dc:creator>
    </item>
    <item>
      <title>Stein transport for Bayesian inference</title>
      <link>https://arxiv.org/abs/2409.01464</link>
      <description>arXiv:2409.01464v2 Announce Type: replace-cross 
Abstract: We introduce $\textit{Stein transport}$, a novel methodology for Bayesian inference designed to efficiently push an ensemble of particles along a predefined curve of tempered probability distributions. The driving vector field is chosen from a reproducing kernel Hilbert space and can be derived either through a suitable kernel ridge regression formulation or as an infinitesimal optimal transport map in the Stein geometry. The update equations of Stein transport resemble those of Stein variational gradient descent (SVGD), but introduce a time-varying score function as well as specific weights attached to the particles. While SVGD relies on convergence in the long-time limit, Stein transport reaches its posterior approximation at finite time $t=1$. Studying the mean-field limit, we discuss the errors incurred by regularisation and finite-particle effects, and we connect Stein transport to birth-death dynamics and Fisher-Rao gradient flows. In a series of experiments, we show that in comparison to SVGD, Stein transport not only often reaches more accurate posterior approximations with a significantly reduced computational budget, but that it also effectively mitigates the variance collapse phenomenon commonly observed in SVGD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01464v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolas N\"usken</dc:creator>
    </item>
    <item>
      <title>Recursive Nested Filtering for Efficient Amortized Bayesian Experimental Design</title>
      <link>https://arxiv.org/abs/2409.05354</link>
      <description>arXiv:2409.05354v2 Announce Type: replace-cross 
Abstract: This paper introduces the Inside-Out Nested Particle Filter (IO-NPF), a novel, fully recursive, algorithm for amortized sequential Bayesian experimental design in the non-exchangeable setting. We frame policy optimization as maximum likelihood estimation in a non-Markovian state-space model, achieving (at most) $\mathcal{O}(T^2)$ computational complexity in the number of experiments. We provide theoretical convergence guarantees and introduce a backward sampling algorithm to reduce trajectory degeneracy. IO-NPF offers a practical, extensible, and provably consistent approach to sequential Bayesian experimental design, demonstrating improved efficiency over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05354v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahel Iqbal, Hany Abdulsamad, Sara P\'erez-Vieites, Simo S\"arkk\"a, Adrien Corenflos</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2411.07978</link>
      <description>arXiv:2411.07978v2 Announce Type: replace-cross 
Abstract: This study introduces a doubly robust (DR) estimator for regression discontinuity (RD) designs. In RD designs, treatment effects are estimated in a quasi-experimental setting where treatment assignment depends on whether a running variable surpasses a predefined cutoff. A common approach in RD estimation is to apply nonparametric regression methods, such as local linear regression. In such an approach, the validity relies heavily on the consistency of nonparametric estimators and is limited by the nonparametric convergence rate, thereby preventing $\sqrt{n}$-consistency. To address these issues, we propose the DR-RD estimator, which combines two distinct estimators for the conditional expected outcomes. If either of these estimators is consistent, the treatment effect estimator remains consistent. Furthermore, due to the debiasing effect, our proposed estimator achieves $\sqrt{n}$-consistency if both regression estimators satisfy certain mild conditions, which also simplifies statistical inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07978v2</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
  </channel>
</rss>
