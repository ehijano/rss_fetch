<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Apr 2025 01:49:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Building Scalable AI-Powered Applications with Cloud Databases: Architectures, Best Practices and Performance Considerations</title>
      <link>https://arxiv.org/abs/2504.18793</link>
      <description>arXiv:2504.18793v1 Announce Type: new 
Abstract: The rapid adoption of AI-powered applications demands high-performance, scalable, and efficient cloud database solutions, as traditional architectures often struggle with AI-driven workloads requiring real-time data access, vector search, and low-latency queries. This paper explores how cloud-native databases enable AI-driven applications by leveraging purpose-built technologies such as vector databases (pgvector), graph databases (AWS Neptune), NoSQL stores (Amazon DocumentDB, DynamoDB), and relational cloud databases (Aurora MySQL and PostgreSQL). It presents architectural patterns for integrating AI workloads with cloud databases, including Retrieval-Augmented Generation (RAG) [1] with LLMs, real-time data pipelines, AI-driven query optimization, and embeddings-based search. Performance benchmarks, scalability considerations, and cost-efficient strategies are evaluated to guide the design of AI-enabled applications. Real-world case studies from industries such as healthcare, finance, and customer experience illustrate how enterprises utilize cloud databases to enhance AI capabilities while ensuring security, governance, and compliance with enterprise and regulatory standards. By providing a comprehensive analysis of AI and cloud database integration, this paper serves as a practical guide for researchers, architects, and enterprises to build next-generation AI applications that optimize performance, scalability, and cost efficiency in cloud environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18793v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Santosh Bhupathi</dc:creator>
    </item>
    <item>
      <title>LiLIS: Enhancing Big Spatial Data Processing with Lightweight Distributed Learned Index</title>
      <link>https://arxiv.org/abs/2504.18883</link>
      <description>arXiv:2504.18883v2 Announce Type: new 
Abstract: The efficient management of big spatial data is crucial for location-based services, particularly in smart cities. However, existing systems such as Simba and Sedona, which incorporate distributed spatial indexing, still incur substantial index construction overheads, rendering them far from optimal for real-time analytics. Recent studies demonstrate that learned indices can achieve high efficiency through well-designed machine learning models, but how to design a learned index for distributed spatial analytics remains unaddressed. In this paper, we present LiLIS, a Lightweight Distributed Learned Index for big spatial data. LiLIS combines machine-learned search strategies with spatial-aware partitioning within a distributed framework, and efficiently implements common spatial queries, including point query, range query, k-nearest neighbors (kNN), and spatial joins. Extensive experimental results over real-world and synthetic datasets show that LiLIS outperforms state-of-the-art big spatial data analytics by $2-3$ orders of magnitude for most spatial queries, and the index building achieves $1.5-2\times$ speed-up. The code is available at https://github.com/SWUFE-DB-Group/learned-index-spark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18883v2</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongpu Chen, Wanjun Hao, Ziang Zeng, Long Shi, Yi Wen, Zhi-Jie Wang, Yu Zhao</dc:creator>
    </item>
    <item>
      <title>Beyond Performance: Measuring the Environmental Impact of Analytical Databases</title>
      <link>https://arxiv.org/abs/2504.18980</link>
      <description>arXiv:2504.18980v1 Announce Type: new 
Abstract: The exponential growth of data is making query processing increasingly critical for modern computing infrastructure, yet the environmental impact of database operations remains poorly understood and largely overlooked. This paper presents ATLAS, a comprehensive methodology for measuring and quantifying the environmental footprint of analytical database systems, considering both operational impacts and manufacturing costs of hardware components. Through extensive empirical evaluation of four distinct database architectures (DuckDB, MonetDB, Hyper, and StarRocks), we uncover how fundamental architectural decisions affect environmental efficiency. Our findings reveal that environmental considerations in database operations are multifaceted, encompassing both immediate operational impacts and long-term sustainability implications. We demonstrate that architectural choices can significantly influence both power consumption and environmental sustainability, while deployment location emerges as a critical factor that can amplify or diminish these architectural advantages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18980v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Bachras, Hans-Arno Jacobsen</dc:creator>
    </item>
    <item>
      <title>BQSched: A Non-intrusive Scheduler for Batch Concurrent Queries via Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2504.19142</link>
      <description>arXiv:2504.19142v1 Announce Type: new 
Abstract: Most large enterprises build predefined data pipelines and execute them periodically to process operational data using SQL queries for various tasks. A key issue in minimizing the overall makespan of these pipelines is the efficient scheduling of concurrent queries within the pipelines. Existing tools mainly rely on simple heuristic rules due to the difficulty of expressing the complex features and mutual influences of queries. The latest reinforcement learning (RL) based methods have the potential to capture these patterns from feedback, but it is non-trivial to apply them directly due to the large scheduling space, high sampling cost, and poor sample utilization.
  Motivated by these challenges, we propose BQSched, a non-intrusive Scheduler for Batch concurrent Queries via reinforcement learning. Specifically, BQSched designs an attention-based state representation to capture the complex query patterns, and proposes IQ-PPO, an auxiliary task-enhanced proximal policy optimization (PPO) algorithm, to fully exploit the rich signals of Individual Query completion in logs. Based on the RL framework above, BQSched further introduces three optimization strategies, including adaptive masking to prune the action space, scheduling gain-based query clustering to deal with large query sets, and an incremental simulator to reduce sampling cost. To our knowledge, BQSched is the first non-intrusive batch query scheduler via RL. Extensive experiments show that BQSched can significantly improve the efficiency and stability of batch query scheduling, while also achieving remarkable scalability and adaptability in both data and queries. For example, across all DBMSs and scales tested, BQSched reduces the overall makespan of batch queries on TPC-DS benchmark by an average of 34% and 13%, compared with the commonly used heuristic strategy and the adapted RL-based scheduler, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19142v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenhao Xu, Chunyu Chen, Jinglin Peng, Jiannan Wang, Jun Gao</dc:creator>
    </item>
    <item>
      <title>Representing and querying data tensors in RDF and SPARQL</title>
      <link>https://arxiv.org/abs/2504.19224</link>
      <description>arXiv:2504.19224v1 Announce Type: new 
Abstract: Embedding tensors in databases has recently gained in significance, due to the rapid proliferation of machine learning methods (including LLMs) which produce embeddings in the form of tensors. To support emerging use cases hybridizing machine learning with knowledge graphs, a robust and efficient tensor representation scheme is needed. We introduce a novel approach for representing data tensors as literals in RDF, along with an extension of SPARQL implementing specialized functionalities for handling such literals. The extension includes 36 SPARQL functions and four aggregates. To support this approach, we provide a thoroughly tested, open-source implementation based on Apache Jena, along with an exemplary knowledge graph and query set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19224v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Piotr Marciniak, Piotr Sowinski, Maria Ganzha</dc:creator>
    </item>
    <item>
      <title>Cost-based Selection of Provenance Sketches for Data Skipping</title>
      <link>https://arxiv.org/abs/2504.19252</link>
      <description>arXiv:2504.19252v1 Announce Type: new 
Abstract: Provenance sketches, light-weight indexes that record what data is needed (is relevant) for answering a query, can significantly improve performance of important classes of queries (e.g., HAVING and top-k queries). Given a horizontal partition of a table, a provenance sketch for a query Q records which fragments contain provenance. Once a provenance sketch has been captured for a query, it can be used to speed-up subsequent queries by skipping data that does not belong to a sketch. The size and, thus, also the effectiveness of a provenance sketch is often quite sensitive to the choice of attribute(s) we are partitioning on. In this work, we develop sample-based estimation techniques for the size of provenance sketches akin to a specialized form of approximate query processing. This technique enables the online selection of provenance sketches by estimating the size of sketches for a set of candidate attributes and then creating the sketch that is estimated to yield the largest benefit. We demonstrate experimentally that our estimation is accurate enough to select optimal or near optimal provenance sketches in most cases which in turn leads to a runtime improvement of up to %60 compared to other strategies for selecting provenance sketches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19252v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyu Liu, Boris Glavic</dc:creator>
    </item>
    <item>
      <title>vMODB: Unifying event and data management for distributed asynchronous applications</title>
      <link>https://arxiv.org/abs/2504.19757</link>
      <description>arXiv:2504.19757v1 Announce Type: new 
Abstract: Event-driven architecture (EDA) has emerged as a crucial architectural pattern for scalable cloud applications. However, its asynchronous and decoupled nature introduces challenges for meeting transactional requirements. Database systems, relegated to serving as storage engines for individual components, do not recognize transactions that span multiple components in EDAs. In contrast, messaging systems are unaware of the components' application states. Weaving such asynchronous and independent EDA components forces developers to relinquish transactional guarantees, resulting in data consistency issues. To address this challenge, we design vMODB, a distributed framework that enables the implementation of highly consistent and scalable cloud applications without compromising the envisioned benefits of EDA. We propose Virtual Micro Service (VMS), a novel programming model that provides familiar constructs to enable developers to specify the data model, constraints, and concurrency semantics of components, as well as transactions and data dependencies that span across components. vMODB leverages VMS semantics to enforce ACID properties by transparently unifying event logs and state management into a common event-driven execution framework. Our experiments using two benchmarks show that vMODB outperforms a widely adopted state-of-the-art competing framework that only offers eventual consistency by up to 3X. With its high performance, familiar programming constructs, and ACID properties, vMODB will significantly simplify the development of highly consistent and efficient EDAs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19757v1</guid>
      <category>cs.DB</category>
      <category>cs.SE</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rodrigo Laigner, Yongluan Zhou</dc:creator>
    </item>
    <item>
      <title>MINT: Multi-Vector Search Index Tuning</title>
      <link>https://arxiv.org/abs/2504.20018</link>
      <description>arXiv:2504.20018v1 Announce Type: new 
Abstract: Vector search plays a crucial role in many real-world applications. In addition to single-vector search, multi-vector search becomes important for multi-modal and multi-feature scenarios today. In a multi-vector database, each row is an item, each column represents a feature of items, and each cell is a high-dimensional vector. In multi-vector databases, the choice of indexes can have a significant impact on performance. Although index tuning for relational databases has been extensively studied, index tuning for multi-vector search remains unclear and challenging. In this paper, we define multi-vector search index tuning and propose a framework to solve it. Specifically, given a multi-vector search workload, we develop algorithms to find indexes that minimize latency and meet storage and recall constraints. Compared to the baseline, our latency achieves 2.1X to 8.3X speedup.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20018v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiongli Zhu, Yue Wang, Bailu Ding, Philip A. Bernstein, Vivek Narasayya, Surajit Chaudhuri</dc:creator>
    </item>
    <item>
      <title>Scalable Substructure Discovery Algorithm For Homogeneous Multilayer Networks</title>
      <link>https://arxiv.org/abs/2504.19328</link>
      <description>arXiv:2504.19328v1 Announce Type: cross 
Abstract: Graph mining analyzes real-world graphs to find core substructures (connected subgraphs) in applications modeled as graphs. Substructure discovery is a process that involves identifying meaningful patterns, structures, or components within a large data set. These substructures can be of various types, such as frequent patterns, motifs, or other relevant features within the data.
  To model complex data sets -- with multiple types of entities and relationships -- multilayer networks (or MLNs) have been shown to be more effective as compared to simple and attributed graphs. Analysis algorithms on MLNs using the decoupling approach have been shown to be both efficient and accurate. Hence, this paper focuses on substructure discovery in homogeneous multilayer networks (one type of MLN) using a novel decoupling-based approach. In this approach, each layer is processed independently, and then the results from two or more layers are composed to identify substructures in the entire MLN. The algorithm is designed and implemented, including the composition part, using one of the distributed processing frameworks (the Map/Reduce paradigm) to provide scalability.
  After establishing the correctness, we analyze the speedup and response time of the proposed algorithm and approach through extensive experimental analysis on large synthetic and real-world data sets with diverse graph characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19328v1</guid>
      <category>cs.SI</category>
      <category>cs.DB</category>
      <category>cs.IR</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arshdeep Singh, Abhishek Santra, Sharma Chakravarthy</dc:creator>
    </item>
    <item>
      <title>TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate</title>
      <link>https://arxiv.org/abs/2504.19874</link>
      <description>arXiv:2504.19874v1 Announce Type: cross 
Abstract: Vector quantization, a problem rooted in Shannon's source coding theory, aims to quantize high-dimensional Euclidean vectors while minimizing distortion in their geometric structure. We propose TurboQuant to address both mean-squared error (MSE) and inner product distortion, overcoming limitations of existing methods that fail to achieve optimal distortion rates. Our data-oblivious algorithms, suitable for online applications, achieve near-optimal distortion rates (within a small constant factor) across all bit-widths and dimensions. TurboQuant achieves this by randomly rotating input vectors, inducing a concentrated Beta distribution on coordinates, and leveraging the near-independence property of distinct coordinates in high dimensions to simply apply optimal scalar quantizers per each coordinate. Recognizing that MSE-optimal quantizers introduce bias in inner product estimation, we propose a two-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL (QJL) transform on the residual, resulting in an unbiased inner product quantizer. We also provide a formal proof of the information-theoretic lower bounds on best achievable distortion rate by any vector quantizer, demonstrating that TurboQuant closely matches these bounds, differing only by a small constant ($\approx 2.7$) factor. Experimental results validate our theoretical findings, showing that for KV cache quantization, we achieve absolute quality neutrality with 3.5 bits per channel and marginal quality degradation with 2.5 bits per channel. Furthermore, in nearest neighbor search tasks, our method outperforms existing product quantization techniques in recall while reducing indexing time to virtually zero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19874v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Zandieh, Majid Daliri, Majid Hadian, Vahab Mirrokni</dc:creator>
    </item>
    <item>
      <title>A Unified Approach for Multi-Granularity Search over Spatial Datasets</title>
      <link>https://arxiv.org/abs/2412.04805</link>
      <description>arXiv:2412.04805v3 Announce Type: replace 
Abstract: There has been increased interest in data search as a means to find relevant datasets or data points in data lakes and repositories. Although approaches have been proposed to support spatial dataset search and data point search, they consider the two types of searches independently. To enable search operations ranging from the coarse-grained dataset level to the fine-grained data point level, we provide an integrated one that supports diverse query types and distance metrics. In this paper, we focus on designing a multi-granularity spatial data search system, called Spadas, that supports both dataset and data point search operations. To address the challenges of the high cost of indexing and susceptibility to outliers, we propose a unified index that can drastically improve query efficiency in various scenarios by organizing data reasonably and removing outliers in datasets. Moreover, to accelerate all data search operations, we propose a set of pruning mechanisms based on the unified index, including fast bound estimation, approximation technique with error bound, and pruning in batch techniques, to effectively filter out non-relevant datasets and points. Finally, we report the results of a detailed experimental evaluation using six spatial data repositories, achieving orders of magnitude faster than the state-of-the-art algorithms and demonstrating the effectiveness by case study. An online spatial data search system of Spadas is also implemented and made accessible to users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04805v3</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenzhe Yang, Sheng Wang, Shixun Huang, Yuyang Liao, Yuan Sun, Juliana Freire, Zhiyong Peng</dc:creator>
    </item>
    <item>
      <title>SQL-Factory: A Multi-Agent Framework for High-Quality and Large-Scale SQL Generation</title>
      <link>https://arxiv.org/abs/2504.14837</link>
      <description>arXiv:2504.14837v3 Announce Type: replace 
Abstract: High quality SQL corpus is essential for intelligent database. For example, Text-to-SQL requires SQL queries and correspond natural language questions as training samples. However, collecting such query corpus remains challenging in practice due to the high cost of manual annotation, which highlights the importance of automatic SQL generation. Despite recent advances, existing generation methods still face limitations in achieving both diversity and cost-effectiveness. Besides, many methods also treat all tables equally, which overlooks schema complexity and leads to under-utilization of structurally rich tables. To address these issues, this paper proposes a multi-agent framework for high-quality and large-scale SQL generation, dubbed SQL-Factory. It decomposes the generation process into three collaborative teams: the Generation Team explores diverse query structures using a powerful language model, the Expansion Team scales promising patterns via a lightweight language model, and the Management Team adaptively schedules the workflow and evaluates the quality of synthesized queries. This modular framework ensures a balanced trade-off between diversity, scalability, and generation cost. We apply SQL-Factory to four widely used benchmarks and generate over 300,000 SQL queries with less than $200 API cost. Our generated queries achieve higher diversity compared to other methods, and extensive experiments demonstrate that the generated queries significantly improve the model performance in various downstream tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14837v3</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiahui Li, Tongwang Wu, Yuren Mao, Yunjun Gao, Yajie Feng, Huaizhong Liu</dc:creator>
    </item>
    <item>
      <title>WikiNER-fr-gold: A Gold-Standard NER Corpus</title>
      <link>https://arxiv.org/abs/2411.00030</link>
      <description>arXiv:2411.00030v2 Announce Type: replace-cross 
Abstract: We address in this article the the quality of the WikiNER corpus, a multilingual Named Entity Recognition corpus, and provide a consolidated version of it. The annotation of WikiNER was produced in a semi-supervised manner i.e. no manual verification has been carried out a posteriori. Such corpus is called silver-standard. In this paper we propose WikiNER-fr-gold which is a revised version of the French proportion of WikiNER. Our corpus consists of randomly sampled 20% of the original French sub-corpus (26,818 sentences with 700k tokens). We start by summarizing the entity types included in each category in order to define an annotation guideline, and then we proceed to revise the corpus. Finally we present an analysis of errors and inconsistency observed in the WikiNER-fr corpus, and we discuss potential future work directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00030v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danrun Cao (IRISA, EXPRESSION), Nicolas B\'echet (IRISA, UBS, EXPRESSION), Pierre-Fran\c{c}ois Marteau (IRISA, UBS, EXPRESSION)</dc:creator>
    </item>
    <item>
      <title>Think2SQL: Reinforce LLM Reasoning Capabilities for Text2SQL</title>
      <link>https://arxiv.org/abs/2504.15077</link>
      <description>arXiv:2504.15077v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown impressive capabilities in transforming natural language questions about relational databases into SQL queries. Despite recent improvements, small LLMs struggle to handle questions involving multiple tables and complex SQL patterns under a Zero-Shot Learning (ZSL) setting. Supervised Fine-Tuning (SFT) partially compensates for the knowledge deficits in pretrained models but falls short while dealing with queries involving multi-hop reasoning. To bridge this gap, different LLM training strategies to reinforce reasoning capabilities have been proposed, ranging from leveraging a thinking process within ZSL, including reasoning traces in SFT, or adopt Reinforcement Learning (RL) strategies. However, the influence of reasoning on Text2SQL performance is still largely unexplored. This paper investigates to what extent LLM reasoning capabilities influence their Text2SQL performance on four benchmark datasets. To this end, it considers the following LLM settings: (1) ZSL, including general-purpose reasoning or not; (2) SFT, with and without task-specific reasoning traces; (3) RL, exploring the use of different rewarding functions, both the established EXecution accuracy (EX) and a mix with fine-grained ones that also account the precision, recall, and cardinality of partially correct answers; (4) SFT+RL, i.e, a two-stage approach that combines SFT and RL. The results show that general-purpose reasoning under ZSL proves to be ineffective in tackling complex Text2SQL cases. Small LLMs benefit from SFT with reasoning much more than larger ones. RL is generally beneficial across all tested models and datasets. The use of the fine-grained metrics turns out to be the most effective RL strategy. Thanks to RL and the novel text2SQL rewards, the 7B Qwen-Coder-2.5 model performs on par with 400+ Billion ones (including gpt-4o) on the Bird dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15077v2</guid>
      <category>cs.LG</category>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Simone Papicchio, Simone Rossi, Luca Cagliero, Paolo Papotti</dc:creator>
    </item>
  </channel>
</rss>
