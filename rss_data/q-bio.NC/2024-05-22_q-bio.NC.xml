<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 May 2024 04:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Implementing feature binding through dendritic networks of a single neuron</title>
      <link>https://arxiv.org/abs/2405.12645</link>
      <description>arXiv:2405.12645v1 Announce Type: new 
Abstract: A single neuron receives an extensive array of synaptic inputs through its dendrites, raising the fundamental question of how these inputs undergo integration and summation, culminating in the initiation of spikes in the soma. Experimental and computational investigations have revealed various modes of integration operations that include linear, superlinear, and sublinear summation. Interestingly, distinct neuron types exhibit diverse patterns of dendritic integration contingent upon the spatial distribution of dendrites. The functional implications of these specific integration modalities remain largely unexplored. In this study, we employ the Purkinje cell as a model system to investigate these intricate questions. Our findings reveal that Purkinje cells (PCs) generally exhibit sublinear summation across their expansive dendrites. The degree of sublinearity is dynamically modulated by both spatial and temporal input. Strong sublinearity necessitates that the synaptic distribution in PCs be globally scattered sensitive, whereas weak sublinearity facilitates the generation of complex firing patterns in PCs. Leveraging dendritic branches characterized by strong sublinearity as computational units, we demonstrate that a neuron can adeptly address the feature-binding problem. Collectively, these results offer a systematic perspective on the functional role of dendritic sublinearity, providing inspiration for a broader understanding of dendritic integration across various neuronal types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12645v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanhong Tang, Shanshan Jia, Tiejun Huang, Zhaofei Yu, Jian K. Liu</dc:creator>
    </item>
    <item>
      <title>Could a Computer Architect Understand our Brain?</title>
      <link>https://arxiv.org/abs/2405.12815</link>
      <description>arXiv:2405.12815v1 Announce Type: new 
Abstract: This paper presents a highly speculative model encompassing the cortex, thalamus, and hippocampus of the mammalian brain. While the majority of computational neuroscience models are founded upon empirical evidence, this model is predicated upon a hardware proposal for a machine learning accelerator. Such a device was designed to perform a specific task, such as speech recognition. The design process employed the principles and techniques typically used by computer architects in the design of devices such as processors. However, it also sought to maintain plausibility with biological systems in accordance with the current understanding of the mammalian brain. In the course of our research, we have identified a functional framework that may help to fill the gaps in current neuroscience, thereby facilitating the explanations for many elusive cognitive-level effects. This paper does not describe the device itself or the rationale behind the design decision, but instead, it presents a concise description of the derived model. In brief, the model provides a functional definition of the cortical column and its structural definition by the minicolumns. It also offers a descriptive model for the corticothalamic and corticostriatal loops, a functional proposal for the hippocampal complex, and a simplified view of the brainstem circuitry involved in auditory processing. The proposed model appears to provide an explanation for a number of cognitive phenomena, including some ERP effects, bottom-up and top-down attention, and the relationship between phenomena such as the cocktail party effect, anterograde and retrograde amnesia following hippocampal complex damage, and so forth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12815v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Valentin Puente-Varona</dc:creator>
    </item>
    <item>
      <title>Metacognitive particles, mental action and the sense of agency</title>
      <link>https://arxiv.org/abs/2405.12941</link>
      <description>arXiv:2405.12941v1 Announce Type: new 
Abstract: This paper articulates metacognition using the language of statistical physics and Bayesian mechanics. Metacognitive beliefs, defined as beliefs about beliefs, find a natural description within this formalism, which allows us to define the dynamics of 'metacognitive particles', i.e., systems possessing metacognitive beliefs. We further unpack this typology of metacognitive systems by distinguishing passive and active metacognitive particles, where active particles are endowed with the capacity for mental actions that update the parameters of other beliefs. We provide arguments for the necessity of this architecture in the emergence of a subjective sense of agency and the experience of being separate from the environment. The motivation is to pave the way towards a mathematical and physical understanding of cognition -- and higher forms thereof -- furthering the study and formalization of cognitive science in the language of mathematical physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12941v1</guid>
      <category>q-bio.NC</category>
      <category>nlin.AO</category>
      <category>physics.bio-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lars Sandved-Smith, Lancelot Da Costa</dc:creator>
    </item>
    <item>
      <title>Do the receptive fields in the primary visual cortex span a variability over the degree of elongation of the receptive fields?</title>
      <link>https://arxiv.org/abs/2404.04858</link>
      <description>arXiv:2404.04858v5 Announce Type: replace 
Abstract: This paper presents results of combining (i) theoretical analysis regarding connections between the orientation selectivity and the elongation of receptive fields for the affine Gaussian derivative model with (ii) biological measurements of orientation selectivity in the primary visual cortex, to investigate if (iii) the receptive fields can be regarded as spanning a variability in the degree of elongation.
  From an in-depth theoretical analysis of idealized models for the receptive fields of simple and complex cells in the primary visual cortex, we have established that the directional selectivity becomes more narrow with increasing elongation of the receptive fields. By comparison with previously established biological results, concerning broad vs. sharp orientation tuning of visual neurons in the primary visual cortex, we demonstrate that those underlying theoretical predictions, in combination with these biological results, are consistent with a previously formulated biological hypothesis, stating that the biological receptive field shapes should span the degrees of freedom in affine image transformations, to support affine covariance over the population of receptive fields in the primary visual cortex.
  Based on this possible indirect support for the working hypothesis concerning affine covariance, we formulate a set of testable predictions that could be used to, with neurophysiological experiments, judge if the receptive fields in the primary visual cortex of higher mammals could be regarded as spanning a variability over the eccentricity or the elongation of the receptive fields, and, if so, then also characterize if such a variability would, in a structured way, be related to the pinwheel structure in the visual cortex.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04858v5</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tony Lindeberg</dc:creator>
    </item>
    <item>
      <title>Statistical Field Theory and Neural Structures Dynamics IV: Field-Theoretic Formalism for Interacting Collective States</title>
      <link>https://arxiv.org/abs/2311.18417</link>
      <description>arXiv:2311.18417v2 Announce Type: replace-cross 
Abstract: Building upon the findings presented in the first three papers of this series, we formulate an effective field theory for interacting collective states. These states consist of a large number of interconnected neurons and are distinguished by their intrinsic activity. The field theory encompasses an infinite set of fields, each of which characterizes the dynamics of a specific type of collective state. Interaction terms within the theory drive transitions between various collective states, allowing us to describe processes such as activation, association, and deactivation of these states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18417v2</guid>
      <category>physics.bio-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>hep-th</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre Gosselin (IF), A\"ileen Lotz</dc:creator>
    </item>
    <item>
      <title>Adapting to time: why nature evolved a diverse set of neurons</title>
      <link>https://arxiv.org/abs/2404.14325</link>
      <description>arXiv:2404.14325v2 Announce Type: replace-cross 
Abstract: Brains have evolved a diverse set of neurons with varying morphologies, physiological properties and rich dynamics that impact their processing of temporal information. By contrast, most neural network models include a homogeneous set of units that only vary in terms of their spatial parameters (weights and biases). To investigate the importance of temporal parameters to neural function, we trained spiking neural networks on tasks of varying temporal complexity, with different subsets of parameters held constant. We find that in a tightly resource constrained setting, adapting conduction delays is essential to solve all test conditions, and indeed that it is possible to solve these tasks using only temporal parameters (delays and time constants) with weights held constant. In the most complex spatio-temporal task we studied, we found that an adaptable bursting parameter was essential. More generally, allowing for adaptation of both temporal and spatial parameters increases network robustness to noise, an important feature for both biological brains and neuromorphic computing systems. In summary, our findings highlight how rich and adaptable dynamics are key to solving temporally structured tasks at a low neural resource cost, which may be part of the reason why biological neurons vary so dramatically in their physiological properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14325v2</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karim G. Habashy, Benjamin D. Evans, Dan F. M. Goodman, Jeffrey S. Bowers</dc:creator>
    </item>
  </channel>
</rss>
