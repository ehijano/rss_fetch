<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Feb 2026 05:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The global structure of the time delay likelihood</title>
      <link>https://arxiv.org/abs/2602.22307</link>
      <description>arXiv:2602.22307v1 Announce Type: new 
Abstract: We identify a fundamental pathology in the likelihood for time delay inference which challenges standard inference methods. By analysing the likelihood for time delay inference with Gaussian process light curve models, we show that it generically develops a boundary-driven "W"-shape with a global maximum at the true delay and gradual rises towards the edges of the observation window. This arises because time delay estimation is intrinsically extrapolative. In practice, global samplers such as nested sampling are steered towards spurious edge modes unless strict convergence criteria are adopted. We demonstrate this with simulations and show that the effect strengthens with higher data density over a fixed time span. To ensure convergence, we provide concrete guidance, notably increasing the number of live points. Further, we show that methods implicitly favouring small delays, for example optimisers and local MCMC, induce a bias towards larger $H_0$. Our results clarify failure modes and offer practical remedies for robust fully Bayesian time delay inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22307v1</guid>
      <category>stat.ME</category>
      <category>astro-ph.CO</category>
      <category>astro-ph.GA</category>
      <category>astro-ph.IM</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Namu Kroupa, Will Handley</dc:creator>
    </item>
    <item>
      <title>Predictive variational inference for flexible regression models</title>
      <link>https://arxiv.org/abs/2602.22582</link>
      <description>arXiv:2602.22582v1 Announce Type: new 
Abstract: A conventional Bayesian approach to prediction uses the posterior distribution to integrate out parameters in a density for unobserved data conditional on the observed data and parameters. When the true posterior is intractable, it is replaced by an approximation; here we focus on variational approximations. Recent work has explored methods that learn posteriors optimized for predictive accuracy under a chosen scoring rule, while regularizing toward the prior or conventional posterior. Our work builds on an existing predictive variational inference (PVI) framework that improves prediction, but also diagnoses model deficiencies through implicit model expansion. In models where the sampling density depends on the parameters through a linear predictor, we improve the interpretability of existing PVI methods as a diagnostic tool. This is achieved by adopting PVI posteriors of Gaussian mixture form (GM-PVI) and establishing connections with plug-in prediction for mixture-of-experts models. We make three main contributions. First, we show that GM-PVI prediction is equivalent to plug-in prediction for certain mixture-of-experts models with covariate-independent weights in generalized linear models and hierarchical extensions of them. Second, we extend standard PVI by allowing GM-PVI posteriors to vary with the prediction covariate and in this case an equivalence to plug-in prediction for mixtures of experts with covariate-dependent weights is established. Third, we demonstrate the diagnostic value of this approach across several examples, including generalized linear models, linear mixed models, and latent Gaussian process models, demonstrating how the parameters of the original model must vary across the covariate space to achieve improvements in prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22582v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Kock, Scott A. Sisson, G. S. Rodrigues, David J. Nott</dc:creator>
    </item>
    <item>
      <title>Modeling Covariate Feedback, Reversal, and Latent Traits in Longitudinal Data: A Joint Hierarchical Framework</title>
      <link>https://arxiv.org/abs/2602.22588</link>
      <description>arXiv:2602.22588v1 Announce Type: new 
Abstract: Time-varying covariates in longitudinal studies frequently evolve through reciprocal feedback, undergo role reversal, and reflect unobserved individual heterogeneity. Standard statistical frameworks often assume fixed covariate roles and exogenous predictors, limiting their utility in systems governed by dynamic behavioral or physiological processes. We develop a hierarchical joint modeling framework that unifies three key features of such systems: (i) bidirectional feedback between a binary and a continuous covariate, (ii) role reversal in which these covariates become jointly modeled outcomes at a prespecified decision phase, and (iii) a shared latent trait influencing both intermediate covariates and a final binary endpoint. The model proceeds in three phases: a feedback-driven longitudinal process, a reversal phase in which the two covariates are jointly modeled conditional on the latent trait, and an outcome model linking a binary, decision-relevant endpoint to observed and latent components. Estimation is carried out using maximum likelihood and Bayesian inference, with Hamiltonian Monte Carlo supporting robust posterior estimation for models with latent structure and mixed outcome types. Simulation studies show that the model yields well calibrated coverage, small bias, and improved predictive performance compared to standard generalized linear mixed models, marginal approaches, and models that ignore feedback or latent traits. In an analysis of nationally representative U.S. panel data, the model captures the co-evolution of physical activity and body mass index and their joint influence, moderated by a latent behavioral resilience factor, on income mobility. The framework offers a flexible, practically implementable tool for analyzing longitudinal decision systems in which feedback, covariate role transition, and unmeasured capacity are central to prediction and intervention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22588v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niloofar Ramezani, Pascal Nitiema, Jeffrey R. Wilson</dc:creator>
    </item>
    <item>
      <title>Beyond Vintage Rotation: Bias-Free Sparse Representation Learning with Oracle Inference</title>
      <link>https://arxiv.org/abs/2602.22590</link>
      <description>arXiv:2602.22590v1 Announce Type: new 
Abstract: Learning low-dimensional latent representations is a central topic in statistics and machine learning, and rotation methods have long been used to obtain sparse and interpretable representations. Despite nearly a century of widespread use across many fields, rigorous guarantees for valid inference for the learned representation remain lacking. In this paper, we identify a surprisingly prevalent phenomenon that suggests a reason for this gap: for a broad class of vintage rotations, the resulting estimators exhibit a non-estimable bias. Because this bias is independent of the data, it fundamentally precludes the development of valid inferential procedures, including the construction of confidence intervals and hypothesis testing. To address this challenge, we propose a novel bias-free rotation method within a general representation learning framework based on latent variables. We establish an oracle inference property for the learned sparse representations: the estimators achieve the same asymptotic variance as in the ideal setting where the latent variables are observed. To bridge the gap between theory and computation, we develop an efficient computational framework and prove that its output estimators retain the same oracle property. Our results provide a rigorous inference procedure for the rotated estimators, yielding statistically valid and interpretable representation learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22590v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengyu Cui, Yunxiao Chen, Jing Ouyang, Gongjun Xu</dc:creator>
    </item>
    <item>
      <title>Feasible Fusion: Constrained Joint Estimation under Structural Non-Overlap</title>
      <link>https://arxiv.org/abs/2602.22612</link>
      <description>arXiv:2602.22612v1 Announce Type: new 
Abstract: Causal inference in modern largescale systems faces growing challenges, including highdimensional covariates, multi-valued treatments, massive observational (OBS) data, and limited randomized controlled trial (RCT) samples due to cost constraints. We formalize treatment-induced structural non-overlap and show that, under this regime, commonly used weighted fusion methods provably fail to satisfy randomized identifying restrictions.To address this issue,we propose a constrained joint estimation framework that minimizes observational risk while enforcing causal validity through orthogonal experimental moment conditions. We further show that structural non-overlap creates a feasibility obstruction for moment enforcement in the original covariate space.We also derive a penalized primaldual algorithm that jointly learns representations and predictors, and establish oracle inequalities decomposing error into overlap recovery, moment violation, and statistical terms.Extensive synthetic experiments demonstrate robust performance under varying degrees of nonoverlap. A largescale ridehailing application shows that our method achieves substantial gains over existing baselines, matching the performance of models trained with significantly more RCT data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22612v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxi Du, Zhiheng Zhang, Haoxuan Li, Cong Fang, Jixing Xu, Peng Zhen, Jiecheng Guo</dc:creator>
    </item>
    <item>
      <title>A General (Non-Markovian) Framework for Covariate Adaptive Randomization: Achieving Balance While Eliminating the Shift</title>
      <link>https://arxiv.org/abs/2602.22648</link>
      <description>arXiv:2602.22648v1 Announce Type: new 
Abstract: Emerging applications increasingly demand flexible covariate adaptive randomization (CAR) methods that support unequal targeted allocation ratios. While existing procedures can achieve covariate balance, they often suffer from the shift problem. This occurs when the allocation ratios of some additional covariates deviate from the target. We show that this problem is equivalent to a mismatch between the conditional average allocation ratio and the target among units sharing specific covariate values, revealing a failure of existing procedures in the long run. To address it, we derive a new form of allocation function by requiring that balancing covariates ensures the ratio matches the target. Based on this form, we design a class of parameterized allocation functions. When the parameter roughly matches certain characteristics of the covariate distribution, the resulting procedure can balance covariates. Thus, we propose a feasible randomization procedure that updates the parameter based on collected covariate information, rendering the procedure non-Markovian. To accommodate this, we introduce a CAR framework that allows non-Markovian procedure. We then establish its key theoretical properties, including the boundedness of covariate imbalance in probability and the asymptotic distribution of the imbalance for additional covariates. Ultimately, we conclude that the feasible randomization procedure can achieve covariate balance and eliminate the shift.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22648v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hengjia Fang, Wei Ma</dc:creator>
    </item>
    <item>
      <title>Learning about Corner Kicks in Soccer by Analysis of Event Times Using a Frailty Model</title>
      <link>https://arxiv.org/abs/2602.22684</link>
      <description>arXiv:2602.22684v1 Announce Type: new 
Abstract: Corner kicks are an important event in soccer because they are often the result of strong attacking play and can be of keen interest to sports fans and bettors. Peng, Hu, and Swartz (2024, Computational Statistics) formulate the mixture feature of corner kick times caused by previous corner kicks, frame the commonly available corner kick data as right-censored event times, and explore patterns of corner kicks. This paper extends their modeling to accommodate the potential correlations between corner kicks by the same teams within the same games. We con- sider a frailty model for event times and apply the Monte Carlo Expec- tation Maximization (MCEM) algorithm to obtain the maximum like- lihood estimates for the model parameters. We compare the proposed model with the model in Peng, Hu, and Swartz (2024) using likelihood ratio tests. The 2019 Chinese Super League (CSL) data are employed throughout the paper for motivation and illustration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22684v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Riley L Isaacs, X. Joan Hu, K. Ken Peng, Tim Swartz</dc:creator>
    </item>
    <item>
      <title>Renewable estimation in linear expectile regression models with streaming data sets</title>
      <link>https://arxiv.org/abs/2602.22687</link>
      <description>arXiv:2602.22687v1 Announce Type: new 
Abstract: Streaming data often exhibit heterogeneity due to heteroscedastic variances or inhomogeneous covariate effects. Online renewable quantile and expectile regression methods provide valuable tools for detecting such heteroscedasticity by combining current data with summary statistics from historical data. However, quantile regression can be computationally demanding because of the non-smooth check function. To address this, we propose a novel online renewable method based on expectile regression, which efficiently updates estimates using both current observations and historical summaries, thereby reducing storage requirements. By exploiting the smoothness of the expectile loss function, our approach achieves superior computational efficiency compared with existing online renewable methods for streaming data with heteroscedastic variances or inhomogeneous covariate effects. We establish the consistency and asymptotic normality of the proposed estimator under mild regularity conditions, demonstrating that it achieves the same statistical efficiency as oracle estimators based on full individual-level data. Numerical experiments and real-data applications demonstrate that our method performs comparably to the oracle estimator while maintaining high computational efficiency and minimal storage costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22687v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Cao, Shanshan Wanga, Xiaoxue Hua</dc:creator>
    </item>
    <item>
      <title>Asymptotic Theory and Sequential Test for General Multi-Armed Bandit Process</title>
      <link>https://arxiv.org/abs/2602.22768</link>
      <description>arXiv:2602.22768v1 Announce Type: new 
Abstract: Multi-armed bandit (MAB) processes constitute a foundational subclass of reinforcement learning problems and represent a central topic in statistical decision theory, but are limited to simultaneous adaptive allocation and sequential test, because of the absence of asymptotic theory under non-i.i.d sequence and sublinear information. To address this open challenge, we propose Urn Bandit (UNB) process to integrate the reinforcement mechanism of urn probabilistic models with MAB principles, ensuring almost sure convergence of resource allocation to optimal arms. We establish the joint functional central limit theorem (FCLT) for consistent estimators of expected rewards under non-i.i.d., non-sub-Gaussian and sublinear reward samples with pairwise correlations across arms. To overcome the limitations of existing methods that focus mainly on cumulative regret, we establish the asymptotic theory along with adaptive allocation that serves powerful sequential test, such as arms comparison, A/B testing, and policy valuation. Simulation studies and real data analysis demonstrate that UNB maintains statistical test performance of equal randomization (ER) design but obtain more average rewards like classical MAB processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22768v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Li Yang, Xiaodong Yan, Dandan Jiang</dc:creator>
    </item>
    <item>
      <title>Rejoinder to the discussants of the two JASA articles `Frequentist Model Averaging' and `The Focused Information Ctierion', by Nils Lid Hjort and Gerda Claeskens</title>
      <link>https://arxiv.org/abs/2602.22803</link>
      <description>arXiv:2602.22803v1 Announce Type: new 
Abstract: We are honoured to have our work read and discussed at such a thorough level by several experts. Words of appreciation and encouragement are gratefully received, while the many supplementary comments, thoughtful reminders, new perspectives and additional themes raised are warmly welcomed and deeply appreciated. Our thanks go also to JASA Editor Francisco Samaniego and his editorial helpers for organising this discussion.
  Space does not allow us answering all of the many worthwhile points raised by our discussants, but in the following we make an attempt to respond to what we perceive of as being the major issues. Our responses are organised by themes rather than by discussants. We shall refer to our two articles as `the FMA paper' (Hjort and Claeskens) and `the FIC paper' (Claeskens and Hjort).</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22803v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nils Lid Hjort, Gerda Claeskens</dc:creator>
    </item>
    <item>
      <title>Projection depth for functional data: Practical issues, computation and applications</title>
      <link>https://arxiv.org/abs/2602.22877</link>
      <description>arXiv:2602.22877v1 Announce Type: new 
Abstract: Statistical analysis of functional data is challenging due to their complex patterns, for which functional depth provides an effective means of reflecting their ordering structure. In this work, we investigate practical aspects of the recently proposed regularized projection depth (RPD), which induces a meaningful ordering of functional data while appropriately accommodating their complex shape features. Specifically, we examine the impact and choice of its tuning parameter, which regulates the degree of effective dimension reduction applied to the data, and propose a random projection-based approach for its efficient computation, supported by theoretical justification. Through comprehensive numerical studies, we explore a wide range of statistical applications of the RPD and demonstrate its particular usefulness in uncovering shape features in functional data analysis. This ability allows the RPD to outperform competing depth-based methods, especially in tasks such as functional outlier detection, classification, and two-sample hypothesis testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22877v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filip Bo\v{c}inec, Stanislav Nagy, Hyemin Yeon</dc:creator>
    </item>
    <item>
      <title>A note on the area under the likelihood and the fake evidence for model selection</title>
      <link>https://arxiv.org/abs/2602.22965</link>
      <description>arXiv:2602.22965v1 Announce Type: new 
Abstract: Improper priors are not allowed for the computation of the Bayesian evidence $Z=p({\bf y})$ (a.k.a., marginal likelihood), since in this case $Z$ is not completely specified due to an arbitrary constant involved in the computation. However, in this work, we remark that they can be employed in a specific type of model selection problem: when we have several (possibly infinite) models belonging to the same parametric family (i.e., for tuning parameters of a parametric model). However, the quantities involved in this type of selection cannot be considered as Bayesian evidences: we suggest to use the name ``fake evidences'' (or ``areas under the likelihood'' in the case of uniform improper priors). We also show that, in this model selection scenario, using a diffuse prior and increasing its scale parameter asymptotically to infinity, we cannot recover the value of the area under the likelihood, obtained with a uniform improper prior. We first discuss it from a general point of view. Then we provide, as an applicative example, all the details for Bayesian regression models with nonlinear bases, considering two cases: the use of a uniform improper prior and the use of a Gaussian prior, respectively. A numerical experiment is also provided confirming and checking all the previous statements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22965v1</guid>
      <category>stat.ME</category>
      <category>cs.CE</category>
      <category>eess.SP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s00180-025-01641-2</arxiv:DOI>
      <arxiv:journal_reference>Computational Statistics, Volume 40, pages 4799-4824, year 2025</arxiv:journal_reference>
      <dc:creator>L. Martino, F. Llorente</dc:creator>
    </item>
    <item>
      <title>permApprox: a general framework for accurate permutation p-value approximation</title>
      <link>https://arxiv.org/abs/2602.22975</link>
      <description>arXiv:2602.22975v1 Announce Type: new 
Abstract: Permutation procedures are common practice in hypothesis testing when distributional assumptions about the test statistic are not met or unknown. With only few permutations, empirical p-values lie on a coarse grid and may even be zero when the observed test statistic exceeds all permuted values. Such zero p-values are statistically invalid and hinder multiple testing correction. Parametric tail modeling with the Generalized Pareto Distribution (GPD) has been proposed to address this issue, but existing implementations can again yield zero p-values when the estimated shape parameter is negative and the fitted distribution has a finite upper bound.
  We introduce a method for accurate and zero-free p-value approximation in permutation testing, embedded in the permApprox workflow and R package. Building on GPD tail modeling, the method enforces a support constraint during parameter estimation to ensure valid extrapolation beyond the observed statistic, thereby strictly avoiding zero p-values. The workflow further integrates robust parameter estimation, data-driven threshold selection, and principled handling of hybrid p-values that are discrete in the bulk and continuous in the extreme tail.
  Extensive simulations using two-sample t-tests and Wilcoxon rank-sum tests show that permApprox produces accurate, robust, and zero-free p-value approximations across a wide range of sample and effect sizes. Applications to single-cell RNA-seq and microbiome data demonstrate its practical utility: permApprox yields smooth and interpretable p-value distributions even with few permutations. By resolving the zero-p-value problem while preserving accuracy and computational efficiency, permApprox enables reliable permutation-based inference in high-dimensional and computationally intensive settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22975v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefanie Peschel, Anne-Laure Boulesteix, Erika von Mutius, Christian L. M\"uller</dc:creator>
    </item>
    <item>
      <title>Testing Partially-Identifiable Causal Queries Using Ternary Tests</title>
      <link>https://arxiv.org/abs/2602.23020</link>
      <description>arXiv:2602.23020v1 Announce Type: new 
Abstract: We consider hypothesis testing of binary causal queries using observational data. Since the mapping of causal models to the observational distribution that they induce is not one-to-one, in general, causal queries are often only partially identifiable. When binary statistical tests are used for testing partially-identifiable causal queries, their results do not translate in a straightforward manner to the causal hypothesis testing problem. We propose using ternary (three-outcome) statistical tests to test partially-identifiable causal queries. We establish testability requirements that ternary tests must satisfy in terms of uniform consistency and present equivalent topological conditions on the hypotheses. To leverage the existing toolbox of binary tests, we prove that obtaining ternary tests by combining binary tests is complete. Finally, we demonstrate how topological conditions serve as a guide to construct ternary tests for two concrete causal hypothesis testing problems, namely testing the instrumental variable (IV) inequalities and comparing treatment efficacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23020v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourbh Bhadane, Joris M. Mooij, Philip Boeken, Onno Zoeter</dc:creator>
    </item>
    <item>
      <title>Semiparametric Joint Inference for Sensitivity and Specificity at the Youden-Optimal Cut-Off</title>
      <link>https://arxiv.org/abs/2602.23045</link>
      <description>arXiv:2602.23045v1 Announce Type: new 
Abstract: Sensitivity and specificity evaluated at an optimal diagnostic cut-off are fundamental measures of classification accuracy when continuous biomarkers are used for disease diagnosis. Joint inference for these quantities is challenging because their estimators are evaluated at a common, data-driven threshold estimated from both diseased and healthy samples, inducing statistical dependence. Existing approaches are largely based on parametric assumptions or fully nonparametric procedures, which may be sensitive to model misspecification or lack efficiency in moderate samples. We propose a semiparametric framework for joint inference on sensitivity and specificity at the Youden-optimal cut-off under the density ratio model. Using maximum empirical likelihood, we derive estimators of the optimal threshold and the corresponding sensitivity and specificity, and establish their joint asymptotic normality. This leads to Wald-type and range-preserving logit-transformed confidence regions. Simulation studies show that the proposed method achieves accurate coverage with improved efficiency relative to existing parametric and nonparametric alternatives across a variety of distributional settings. An analysis of COVID-19 antibody data demonstrates the practical advantages of the proposed approach for diagnostic decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23045v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyan Liu, Qinglong Tian, Chunlin Wang, Pengfei Li</dc:creator>
    </item>
    <item>
      <title>Dimension Reduction in Multivariate Extremes via Latent Linear Factor Models</title>
      <link>https://arxiv.org/abs/2602.23143</link>
      <description>arXiv:2602.23143v1 Announce Type: new 
Abstract: We propose a new and interpretable class of high-dimensional tail dependence models based on latent linear factor structures. Specifically, extremal dependence of an observable vector is assumed to be driven by a lower-dimensional latent $K$-factor model, where $K \ll d$, thereby inducing an explicit form of dimension reduction. Geometrically, this is reflected in the support of the associated spectral dependence measure, whose intrinsic dimension is at most $K-1$. The loading structure may additionally exhibit sparsity, meaning that each component is influenced by only a small number of latent factors, which further enhances interpretability and scalability. Under mild structural assumptions, we establish identifiability of the model parameters and provide a constructive recovery procedure based on a margin-free tail pairwise dependence matrix, which also yields practical rank-based estimation methods. The framework combines naturally with marginal tail models and is particularly well suited to high-dimensional settings. We illustrate its applicability in a spatial wind energy application, where the latent factor structure enables tractable estimation of the risk that a large proportion of turbines simultaneously fall below their cut-in wind speed thresholds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23143v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexis Boulin, Axel B\"ucher</dc:creator>
    </item>
    <item>
      <title>Randomization Tests in Switchback Experiments</title>
      <link>https://arxiv.org/abs/2602.23257</link>
      <description>arXiv:2602.23257v1 Announce Type: new 
Abstract: Switchback experiments--alternating treatment and control over time--are widely used when unit-level randomization is infeasible, outcomes are aggregated, or user interference is unavoidable. In practice, experimentation must support fast product cycles, so teams often run studies for limited durations and make decisions with modest samples. At the same time, outcomes in these time-indexed settings exhibit serial dependence, seasonality, and occasional heavy-tailed shocks, and temporal interference (carryover or anticipation) can render standard asymptotics and naive randomization tests unreliable. In this paper, we develop a randomization-test framework that delivers finite-sample valid, distribution-free p-values for several null hypotheses of interest using only the known assignment mechanism, without parametric assumptions on the outcome process. For causal effects of interests, we impose two primitive conditions--non-anticipation and a finite carryover horizon m--and construct conditional randomization tests (CRTs) based on an ex ante pooling of design blocks into "sections," which yields a tractable conditional assignment law and ensures imputability of focal outcomes. We provide diagnostics for learning the carryover window and assessing non-anticipation, and we introduce studentized CRTs for a session-wise weak null that accommodates within-session seasonality with asymptotic validity. Power approximations under distributed-lag effects with AR(1) noise guide design and analysis choices, and simulations demonstrate favorable size and power relative to common alternatives. Our framework extends naturally to other time-indexed designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23257v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jizhou Liu, Liang Zhong</dc:creator>
    </item>
    <item>
      <title>Identifiability of Treatment Effects with Unobserved Spatially Varying Confounders</title>
      <link>https://arxiv.org/abs/2602.23291</link>
      <description>arXiv:2602.23291v1 Announce Type: new 
Abstract: The study of causal effects in the presence of unmeasured spatially varying confounders has garnered increasing attention. However, a general framework for identifiability, which is critical for reliable causal inference from observational data, has yet to be advanced. In this paper, we study a linear model with various parametric model assumptions on the covariance structure between the unmeasured confounder and the exposure of interest. We establish identifiability of the treatment effect for many commonly 20 used spatial models for both discrete and continuous data, under mild conditions on the structure of observation locations and the exposure-confounder association. We also emphasize models or scenarios where identifiability may not hold, under which statistical inference should be conducted with caution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23291v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tommy Tang, Xinran Li, Bo Li</dc:creator>
    </item>
    <item>
      <title>Data-Efficient Generative Modeling of Non-Gaussian Global Climate Fields via Scalable Composite Transformations</title>
      <link>https://arxiv.org/abs/2602.23311</link>
      <description>arXiv:2602.23311v1 Announce Type: new 
Abstract: Quantifying uncertainty in future climate projections is hindered by the prohibitive computational cost of running physical climate models, which severely limits the availability of training data. We propose a data-efficient framework for emulating the internal variability of global climate fields, specifically designed to overcome these sample-size constraints. Inspired by copula modeling, our approach constructs a highly expressive joint distribution via a composite transformation to a multivariate standard normal space. We combine a nonparametric Bayesian transport map for spatial dependence modeling with flexible, spatially varying marginal models, essential for capturing non-Gaussian behavior and heavy-tailed extremes. These marginals are defined by a parametric model followed by a semi-parametric B-spline correction to capture complex distributional features. The marginal parameters are spatially smoothed using Gaussian-process priors with low-rank approximations, rendering the computational cost linear in the spatial dimension. When applied to global log-precipitation-rate fields at more than 50,000 grid locations, our stochastic surrogate achieves high fidelity, accurately quantifying the climate distribution's spatial dependence and marginal characteristics, including the tails. Using only 10 training samples, it outperforms a state-of-the-art competitor trained on 80 samples, effectively octupling the computational budget for climate research. We provide a Python implementation at https://github.com/jobrachem/ppptm .</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23311v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Brachem, Paul F. V. Wiemann, Matthias Katzfuss</dc:creator>
    </item>
    <item>
      <title>Robust model selection using likelihood as data</title>
      <link>https://arxiv.org/abs/2602.23355</link>
      <description>arXiv:2602.23355v1 Announce Type: new 
Abstract: Model selection is a central task in statistics, but standard methods are not robust in misspecified settings where the true data-generating process (DGP) is not in the set of candidate models. The key limitation is that existing methods -- including information criteria and Bayesian posteriors -- do not quantify uncertainty about how well each candidate model approximates the true DGP. In this paper, we introduce a novel approach to model selection based on modeling the likelihood values themselves. Specifically, given $K$ candidate models and $n$ observations, we view the $n\times K$ matrix of negative log-likelihood values as a random data matrix and observe that the expectation of each row is equal to the vector of Kullback--Leibler divergences between the $K$ models and the true DGP, up to an additive constant. We use a multivariate normal model to estimate and quantify uncertainty in this expectation, providing calibrated inferences for robust model selection under misspecification. The procedure is easy to compute, interpretable, and comes with theoretical guarantees, including consistency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23355v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jongwoo Choi, Neil A. Spencer, Jeffrey W. Miller</dc:creator>
    </item>
    <item>
      <title>Differentially Private Truncation of Unbounded Data via Public Second Moments</title>
      <link>https://arxiv.org/abs/2602.22282</link>
      <description>arXiv:2602.22282v1 Announce Type: cross 
Abstract: Data privacy is important in the AI era, and differential privacy (DP) is one of the golden solutions. However, DP is typically applicable only if data have a bounded underlying distribution. We address this limitation by leveraging second-moment information from a small amount of public data. We propose Public-moment-guided Truncation (PMT), which transforms private data using the public second-moment matrix and applies a principled truncation whose radius depends only on non-private quantities: data dimension and sample size. This transformation yields a well-conditioned second-moment matrix, enabling its inversion with a significantly strengthened ability to resist the DP noise. Furthermore, we demonstrate the applicability of PMT by using penalized and generalized linear regressions. Specifically, we design new loss functions and algorithms, ensuring that solutions in the transformed space can be mapped back to the original domain. We have established improvements in the models' DP estimation through theoretical error bounds, robustness guarantees, and convergence results, attributing the gains to the conditioning effect of PMT. Experiments on synthetic and real datasets confirm that PMT substantially improves the accuracy and stability of DP models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22282v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zilong Cao, Xuan Bi, Hai Zhang</dc:creator>
    </item>
    <item>
      <title>Multilevel Control Functional</title>
      <link>https://arxiv.org/abs/2305.12996</link>
      <description>arXiv:2305.12996v4 Announce Type: replace 
Abstract: Control variates are variance reduction techniques for Monte Carlo estimators. They play a critical role in improving Monte Carlo estimators in scientific and machine learning applications that involve computationally expensive integrals. We introduce multilevel control functionals (MLCFs), a novel and widely applicable extension of control variates that combines non-parametric Stein-based control variates with multi-fidelity methods. We show that when the integrand and the density are smooth, and when the dimensionality is not very high, MLCFs enjoy a faster convergence rate. We provide both theoretical analysis and empirical assessments on differential equation examples, including Bayesian inference for ecological models, to demonstrate the effectiveness of our proposed approach. Furthermore, we extend MLCFs for variational inference, and demonstrate improved performance empirically through Bayesian neural network examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.12996v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaiyu Li, Yiming Yang, Xiaoyuan Cheng, Yi He, Zhuo Sun</dc:creator>
    </item>
    <item>
      <title>An online generalization of the (e-)Benjamini-Hochberg procedure</title>
      <link>https://arxiv.org/abs/2407.20683</link>
      <description>arXiv:2407.20683v5 Announce Type: replace 
Abstract: In online multiple testing, the hypotheses arrive one by one, and at each time we must immediately reject or accept the current hypothesis solely based on the data and hypotheses observed so far. Many online procedures have been proposed, but none of them are generalizations of the Benjamini-Hochberg (BH) procedure based on p-values, or of the e-BH procedure that uses e-values. In this paper, we consider a relaxed problem setup that allows the current hypothesis to be rejected at any later step. We show that this relaxation allows us to define -- what we justify extensively to be -- the natural and appropriate online extension of the BH and e-BH procedures. We show that the FDR guarantees for BH (resp. e-BH) and online BH (resp. online e-BH) are identical under positive, negative or arbitrary dependence, at fixed and stopping times. Further, the online BH (resp. online e-BH) rule recovers the BH (resp. e-BH) rule as a special case when the number of hypotheses is known to be fixed. Of independent interest, our proof techniques also allow us to prove that numerous existing online procedures, which were known to control the FDR at fixed times, also control the FDR at stopping times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20683v5</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lasse Fischer, Ziyu Xu, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Functional Network Autoregressive Models for Panel Data</title>
      <link>https://arxiv.org/abs/2502.13431</link>
      <description>arXiv:2502.13431v2 Announce Type: replace 
Abstract: This study proposes a novel functional vector autoregressive framework for analyzing network interactions of functional outcomes in panel data settings. In this framework, an individual's outcome function is influenced by the outcomes of others through a simultaneous equation system. To estimate the functional parameters of interest, we need to address the endogeneity issue arising from these simultaneous interactions among outcome functions. This issue is carefully handled by developing a novel functional moment-based estimator. We establish the consistency, convergence rate, and pointwise asymptotic normality of the proposed estimator. Additionally, we discuss the estimation of marginal effects and impulse response analysis. As an empirical illustration, we analyze the demand for a bike-sharing service in the U.S. The results reveal statistically significant spatial interactions in bike availability across stations, with interaction patterns varying over the time of day.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13431v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomohiro Ando, Tadao Hoshino</dc:creator>
    </item>
    <item>
      <title>Tutorial on Bayesian Functional Regression Using Stan</title>
      <link>https://arxiv.org/abs/2505.05633</link>
      <description>arXiv:2505.05633v2 Announce Type: replace 
Abstract: This manuscript provides step-by-step instructions for implementing Bayesian functional regression models using Stan. Extensive simulations indicate that the inferential performance of the methods is comparable to that of state-of-the-art frequentist approaches. However, Bayesian approaches allow for more flexible modeling and provide an alternative when frequentist methods are not available or may require additional development. Methods and software are illustrated using the accelerometry data from the National Health and Nutrition Examination Survey (NHANES).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.05633v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziren Jiang, Ciprian Crainiceanu, Erjia Cui</dc:creator>
    </item>
    <item>
      <title>A General Framework for Joint Multi-State Models</title>
      <link>https://arxiv.org/abs/2510.07128</link>
      <description>arXiv:2510.07128v4 Announce Type: replace 
Abstract: Conventional joint modeling approaches generally characterize the relationship between longitudinal biomarkers and discrete event occurrences within terminal, recurring or competing risk settings, thereby offering a limited representation of complex, multi-state trajectories.
  We propose a general multi-state joint modeling framework that unifies longitudinal biomarker dynamics with multi-state time-to-event processes defined on arbitrary directed graphs. The proposed framework also accomodates nonlinear longitudinal submodels and scalable inference via stochastic gradient descent. This formulation encompasses both Markovian and semi-Markovian transition structures, allowing recurrent cycles and terminal absorptions to be naturally represented. The longitudinal and event processes are linked through shared latent structures within nonlinear mixed-effects models, extending classical joint modeling formulations.
  We derive the complete likelihood, model selection criteria, and develop scalable inference procedures based on stochastic gradient descent to enable high-dimensional and large-scale applications. In addition, we formulate a dynamic prediction framework that provides individualized state-transition probabilities and personalized risk assessments along complex event trajectories.
  Through simulation and application to the PAQUID cohort, we demonstrate accurate parameter recovery and individualized prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07128v4</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>F\'elix Laplante, Christophe Ambroise</dc:creator>
    </item>
    <item>
      <title>Calibrating confounding strength in sensitivity models for weighting estimators: a comparative review and a new method</title>
      <link>https://arxiv.org/abs/2510.16560</link>
      <description>arXiv:2510.16560v2 Announce Type: replace 
Abstract: Causal inference is only valid when its underlying assumptions are satisfied, one of the most central being the ignorability or unconfoundedness assumption. However, this hypothesis is often unrealistic in observational studies, as some confounding variables may remain unobserved. To address this limitation, sensitivity models for Inverse Probability Weighting (IPW) estimators, known as Marginal Sensitivity Models, have been introduced, allowing for a controlled relaxation of ignorability. A substantial body of literature has emerged around these models, aiming to derive sharp and robust bounds for both binary and continuous treatment effects. A key element of these approaches is the specification of a sensitivity parameter, referred to as the "confounding strength", which quantifies the extent of deviation from ignorability. Yet, determining an appropriate value for this parameter is challenging, and the final interpretation of sensitivity analyses can be unclear. We believe these difficulties represent major obstacles to the adoption of such methods in practice. Therefore, after introducing sensitivity analyses for IPW estimators, we review different strategies to estimate or lower bound the confounding strength, introduce a new method leveraging negative controls, provide a decision tree with guidelines to choose a suitable approach, and compare the methodologies in an in-depth simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16560v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jean-Baptiste Baitairian, Bernard Sebastien, Rana Jreich, Sandrine Katsahian, Agathe Guilloux</dc:creator>
    </item>
    <item>
      <title>Throwing Vines at the Wall: Structure Learning via Random Search</title>
      <link>https://arxiv.org/abs/2510.20035</link>
      <description>arXiv:2510.20035v2 Announce Type: replace 
Abstract: Vine copulas offer flexible multivariate dependence modeling and have become widely used in machine learning. Yet, structure learning remains a key challenge. Early heuristics, such as Dissmann's greedy algorithm, are still considered the gold standard but are often suboptimal. We propose random search algorithms and a statistical framework based on model confidence sets, to improve structure selection, provide theoretical guarantees on selection probabilities, and serve as a foundation for ensembling. Empirical results on real-world data sets show that our methods consistently outperform state-of-the-art approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20035v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thibault Vatter, Thomas Nagler</dc:creator>
    </item>
    <item>
      <title>Discovering Causal Relationships Between Time Series With Spatial Structure</title>
      <link>https://arxiv.org/abs/2510.26485</link>
      <description>arXiv:2510.26485v2 Announce Type: replace 
Abstract: Causal discovery is the subfield of causal inference concerned with estimating the structure of cause-and-effect relationships in a system of interrelated variables, as opposed to quantifying the strength of causal effects. As interest in causal discovery builds in fields such as ecology, public health, and environmental sciences where data is regularly collected with spatial and temporal structures, approaches must evolve to manage autocorrelation and complex confounding. As it stands, the few proposed causal discovery algorithms for spatiotemporal data require summarizing across locations, ignore spatial autocorrelation, and/or scale poorly to high dimensions. Here, we introduce a developing framework that extends time-series causal discovery to systems with spatial structure, building upon work on causal discovery across contexts and methods for handling spatial confounding in causal effect estimation. We close by outlining remaining gaps in the literature and directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26485v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rebecca F. Supple (School of Mathematics and Statistics, University of St Andrews, Centre for Research into Ecological and Environmental Modelling, University of St Andrews), Hannah Worthington (School of Mathematics and Statistics, University of St Andrews, Centre for Research into Ecological and Environmental Modelling, University of St Andrews), Ben Swallow (School of Mathematics and Statistics, University of St Andrews, Centre for Research into Ecological and Environmental Modelling, University of St Andrews)</dc:creator>
    </item>
    <item>
      <title>Design-based inference for generalized causal effects in randomized experiments</title>
      <link>https://arxiv.org/abs/2602.18383</link>
      <description>arXiv:2602.18383v2 Announce Type: replace 
Abstract: Generalized causal effect estimands, including the Mann-Whitney parameter and causal net benefit, provide flexible summaries of treatment effects in randomized experiments with non-Gaussian or multivariate outcomes. We develop a unified design-based inference framework for regression adjustment and variance estimation of a broad class of generalized causal effect estimands defined through pairwise contrast functions. Leveraging the theory of U-statistics and finite-population asymptotics, we establish the consistency and asymptotic normality of regression estimators constructed from individual pairs and per-unit pair averages, even when the working models are misspecified. Consequently, these estimators are model-assisted rather than model-based. In contrast to classical average treatment effect estimands, we show that for nonlinear contrast functions, covariate adjustment preserves consistency but does not admit a universal efficiency guarantee. For inference, we demonstrate that standard heteroskedasticity-robust and cluster-robust variance estimators are generally inconsistent in this setting. As a remedy, we prove that a complete two-way cluster-robust variance estimator, which fully accounts for pairwise dependence and reverse comparisons, is consistent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18383v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyuan Chen, Fan Li</dc:creator>
    </item>
    <item>
      <title>Bayesian Multinomial Logistic Regression for Numerous Categories</title>
      <link>https://arxiv.org/abs/2208.14537</link>
      <description>arXiv:2208.14537v2 Announce Type: replace-cross 
Abstract: Bayesian multinomial logistic regression provides a principled, interpretable approach to multiclass classification, but posterior sampling becomes increasingly expensive as the model dimension grows. Prior work has studied scalability in the number of subjects and covariates; in contrast, this paper focuses on how computation changes as the number of outcome categories increases. To improve scalability in settings with numerous categories, we adapt a gamma-augmentation strategy to decouple category-specific coefficient updates, so that each category's coefficients can be updated conditional on a single auxiliary variable per subject, rather than on the full set of other categories' coefficients. Because the resulting coefficient conditionals are non-conjugate, we couple this augmentation with either adaptive Metropolis-Hastings or elliptical slice sampling. Through simulation and a real-data example, we compare effective sample size and effective sampling rate across several standard competitors. We find that the best-performing sampler depends on the dimension and imbalance regime, and that the proposed augmentation provides substantial speedups in scenarios with numerous categories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.14537v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jared D. Fisher, Kyle R. McEvoy</dc:creator>
    </item>
    <item>
      <title>Polynomial-Time Near-Optimal Estimation over Certain Type-2 Convex Bodies</title>
      <link>https://arxiv.org/abs/2512.22714</link>
      <description>arXiv:2512.22714v2 Announce Type: replace-cross 
Abstract: We develop polynomial-time algorithms for near-optimal minimax mean estimation under $\ell_2$-squared loss in a Gaussian sequence model under convex constraints. The parameter space is an origin-symmetric, type-2 convex body $K \subset \mathbb{R}^n$, and we assume additional regularity conditions: specifically, we assume $K$ is well-balanced, i.e., there exist known radii $r, R &gt; 0$ such that $r B_2 \subseteq K \subseteq R B_2$, as well as oracle access to the Minkowski gauge of $K$. Under these and some further assumptions on $K$, our procedures achieve the minimax rate up to small factors, depending poly-logarithmically on the dimension, while remaining computationally efficient.
  We further extend our methodology to the linear regression and robust heavy-tailed settings, establishing polynomial-time near-optimal estimators when the constraint set satisfies the regularity conditions above. To the best of our knowledge, these results provide the first general framework for attaining statistically near-optimal performance under such broad geometric constraints while preserving computational tractability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22714v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matey Neykov</dc:creator>
    </item>
    <item>
      <title>Not Just How Much, But Where: Decomposing Epistemic Uncertainty into Per-Class Contributions</title>
      <link>https://arxiv.org/abs/2602.21160</link>
      <description>arXiv:2602.21160v2 Announce Type: replace-cross 
Abstract: In safety-critical classification, the cost of failure is often asymmetric, yet Bayesian deep learning summarises epistemic uncertainty with a single scalar, mutual information (MI), that cannot distinguish whether a model's ignorance involves a benign or safety-critical class. We decompose MI into a per-class vector $C_k(x)=\sigma_k^{2}/(2\mu_k)$, with $\mu_k{=}\mathbb{E}[p_k]$ and $\sigma_k^2{=}\mathrm{Var}[p_k]$ across posterior samples. The decomposition follows from a second-order Taylor expansion of the entropy; the $1/\mu_k$ weighting corrects boundary suppression and makes $C_k$ comparable across rare and common classes. By construction $\sum_k C_k \approx \mathrm{MI}$, and a companion skewness diagnostic flags inputs where the approximation degrades. After characterising the axiomatic properties of $C_k$, we validate it on three tasks: (i) selective prediction for diabetic retinopathy, where critical-class $C_k$ reduces selective risk by 34.7\% over MI and 56.2\% over variance baselines; (ii) out-of-distribution detection on clinical and image benchmarks, where $\sum_k C_k$ achieves the highest AUROC and the per-class view exposes asymmetric shifts invisible to MI; and (iii) a controlled label-noise study in which $\sum_k C_k$ shows less sensitivity to injected aleatoric noise than MI under end-to-end Bayesian training, while both metrics degrade under transfer learning. Across all tasks, the quality of the posterior approximation shapes uncertainty at least as strongly as the choice of metric, suggesting that how uncertainty is propagated through the network matters as much as how it is measured.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21160v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mame Diarra Toure, David A. Stephens</dc:creator>
    </item>
  </channel>
</rss>
