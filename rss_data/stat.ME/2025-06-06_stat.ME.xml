<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Jun 2025 04:01:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On the Spherical Dirichlet Distribution: Corrections and Results</title>
      <link>https://arxiv.org/abs/2506.04441</link>
      <description>arXiv:2506.04441v1 Announce Type: new 
Abstract: This note corrects a technical error in Guardiola (2020, Journal of Statistical Distributions and Applications), presents updated derivations, and offers an extended discussion of the properties of the spherical Dirichlet distribution. Today, data mining and gene expressions are at the forefront of modern data analysis. Here we introduce a novel probability distribution that is applicable in these fields. This paper develops the proposed Spherical-Dirichlet Distribution designed to fit vectors located at the positive orthant of the hypersphere, as it is often the case for data in these fields, avoiding unnecessary probability mass. Basic properties of the proposed distribution, including normalizing constants and moments are developed. Relationships with other distributions are also explored. Estimators based on classical inferential statistics, such as method of moments and maximum likelihood estimators are obtained. Two applications are developed: the first one uses simulated data, and the second uses a real text mining example. Both examples are fitted using the proposed Spherical-Dirichlet Distribution and their results are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04441v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1186/s40488-020-00106-9</arxiv:DOI>
      <arxiv:journal_reference>J. Stat. Distrib. App. 7, 6 (2020)</arxiv:journal_reference>
      <dc:creator>Jose H Guardiola</dc:creator>
    </item>
    <item>
      <title>Robust Estimation in Step-Stress Experiments under Exponential Lifetime Distributions</title>
      <link>https://arxiv.org/abs/2506.04445</link>
      <description>arXiv:2506.04445v1 Announce Type: new 
Abstract: Many modern products exhibit high reliability, often resulting in long times to failure. Consequently, conducting experiments under normal operating conditions may require an impractically long duration to obtain sufficient failure data for reliable statistical inference. As an alternative, accelerated life tests (ALTs) are employed to induce earlier failures and thereby reduce testing time. In step-stress experiments a stress factor that accelerates product degradation is identified and systematically increased to provoke early failures. The stress level is increased at predetermined time points and maintained constant between these intervals. Failure data observed under increased levels of stress is statistically analyzed, and results are then extrapolate to normal operating conditions.
  Classical estimation methods such analysis rely on the maximum likelihood estimator (MLE) which is know to be very efficient, but lack robustness in the presence of outlying data. In this work, Minimum Density Power Divergence Estimators (MDPDEs) are proposed as a robust alternative, demonstrating an appealing compromise between efficiency and robustness. The MDPDE based on mixed distributions is developed, and its theoretical properties, including the expression for the asymptotic distribution of the model parameters, are derived under exponential lifetime assumptions. The good performance of the proposed method is evaluated through simulation studies, and its applicability is demonstrated using real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04445v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mar\'ia Jaenada, Juan Manuel Mill\'an, Leandro Pardo</dc:creator>
    </item>
    <item>
      <title>A Scalable Exponential Random Graph Model: Amortised Hierarchical Sequential Neural Posterior Estimation with Applications in Neuroscience</title>
      <link>https://arxiv.org/abs/2506.04558</link>
      <description>arXiv:2506.04558v1 Announce Type: new 
Abstract: Exponential Random Graph Models (ERGMs) are an inferential model for analysing statistical networks. Recent development in ERGMs uses hierarchical Bayesian setup to jointly model a group of networks, which is called a multiple-network Exponential Random Graph Model (MN-ERGMs). MN-ERGM has been successfully applied on real-world resting-state fMRI data from the Cam-CAN project to infer the brain connectivity on aging. However, conventional Bayesian ERGM estimation approach is computationally intensive and lacks implementation scalability due to intractable ERGM likelihood. We address this key limitation by using neural posterior estimation (NPE), which trains a neural network-based conditional density estimator to infer the posterior.\\ We proposed an Amortised Hierarchical Sequential Neural Posterior Estimation (AHS-NPE) and various ERGM-specific adjustment schemes to target the Bayesian hierarchical structure of MN-ERGMs. Our proposed method contributes to the ERGM literature as a very scalable solution, and we used AHS-NPE to re-show the fitting results on the Cam-CAN data application and further scaled it up to a larger implementation sample size. More importantly, our AHS-NPE contributes to the general NPE literature as a new hierarchical NPE approach that preserves the amortisation and sequential refinement, which can be applied to a variety of study fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04558v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yefeng Fan, Simon Richard White</dc:creator>
    </item>
    <item>
      <title>Transform-Resampled Double Bootstrap Percentile with Applications in System Reliability Assessment</title>
      <link>https://arxiv.org/abs/2506.04573</link>
      <description>arXiv:2506.04573v1 Announce Type: new 
Abstract: System reliability assessment(SRA) is a challenging task due to the limited experimental data and the complex nature of the system structures. Despite a long history dating back to \cite{buehler1957confidence}, exact methods have only been applied to SRA for simple systems. High-order asymptotic methods, such as the Cornish-Fisher expansion, have become popular for balancing computational efficiency with improved accuracy when data are limited, but frequently encounter the "bend-back" problem in high-reliability scenarios and require complex analytical computations. To overcome these limitations, we propose a novel method for SRA by modifying the double bootstrap framework, termed the double bootstrap percentile with transformed resamples. In particular, we design a nested resampling process for log-location-scale lifetime models, eliminating the computational burden caused by the iterative resampling process involved in the conventional double bootstrap. We prove that the proposed method maintains the high-order convergence property, thus providing a highly accurate yet computationally efficient confidence limit for system reliability. Moreover, the proposed procedure is straightforward to implement, involving only a simple resampling operation and efficient moment estimation steps. Numerical studies further demonstrate that our approach outperforms the state-of-the-art SRA methods and, at the same time, is much less susceptible to the bend-back issue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04573v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junpeng Gong, Xu He, Zhaohui Li</dc:creator>
    </item>
    <item>
      <title>Optimized and regularly repeated lattice-based Latin hypercube designs for large-scale computer experiments</title>
      <link>https://arxiv.org/abs/2506.04582</link>
      <description>arXiv:2506.04582v1 Announce Type: new 
Abstract: Computer simulations serve as powerful tools for scientists and engineers to gain insights into complex systems. Less costly than physical experiments, computer experiments sometimes involve large number of trials. Conventional design optimization and model fitting methods for computer experiments are inefficient for large-scale problems. In this paper, we propose new methods to optimize good lattice point sets, using less computation to construct designs with enhanced space-filling properties such as high separation distance, low discrepancy, and high separation distance on projections. These designs show promising performance in uncertainty quantification as well as physics-informed neural networks. We also propose a new type of space-filling design called regularly repeated lattice-based Latin hypercube designs, which contain lots of local space-filling Latin hypercube designs as subdesigns. Such designs facilitate rapid fitting of multiple local Gaussian process models in a moving window type of modeling approach and thus are useful for large-scale emulation problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04582v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xu He, Junpeng Gong, Zhaohui Li</dc:creator>
    </item>
    <item>
      <title>Efficient Gibbs Sampling in Cox Regression Models Using Composite Partial Likelihood and P\'olya-Gamma Augmentation</title>
      <link>https://arxiv.org/abs/2506.04675</link>
      <description>arXiv:2506.04675v1 Announce Type: new 
Abstract: The Cox regression model and its Bayesian extensions are widely used in survival analysis. However, standard Bayesian approaches require modeling of the baseline hazard, and their full conditional distributions lack closed-form expressions. Therefore, the Metropolis-Hastings sampling algorithm is typically employed, whose efficiency is highly sensitive to the choice of proposal distribution. To address these issues, we propose the GS4Cox, an efficient Gibbs sampling algorithm for the Cox regression model based on four key components: (i) general Bayesian framework, (ii) composite partial likelihood, (iii) P\'olya-Gamma augmentation scheme, and (iv) finite corrections. Our experiments on both synthetic and actual datasets demonstrate that the GS4Cox algorithm outperforms existing sampling methods in terms of convergence speed and sampling efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04675v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shu Tamano, Yui Tomo</dc:creator>
    </item>
    <item>
      <title>Distributed lag non-linear models with Laplacian-P-splines for analysis of spatially structured time series</title>
      <link>https://arxiv.org/abs/2506.04814</link>
      <description>arXiv:2506.04814v1 Announce Type: new 
Abstract: Distributed lag non-linear models (DLNM) have gained popularity for modeling nonlinear lagged relationships between exposures and outcomes. When applied to spatially referenced data, these models must account for spatial dependence, a challenge that has yet to be thoroughly explored within the penalized DLNM framework. This gap is mainly due to the complex model structure and high computational demands, particularly when dealing with large spatio-temporal datasets. To address this, we propose a novel Bayesian DLNM-Laplacian-P-splines (DLNM-LPS) approach that incorporates spatial dependence using conditional autoregressive (CAR) priors, a method commonly applied in disease mapping. Our approach offers a flexible framework for capturing nonlinear associations while accounting for spatial dependence. It uses the Laplace approximation to approximate the conditional posterior distribution of the regression parameters, eliminating the need for Markov chain Monte Carlo (MCMC) sampling, often used in Bayesian inference, thus improving computational efficiency. The methodology is evaluated through simulation studies and applied to analyze the relationship between temperature and mortality in London.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04814v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sara Rutten, Bryan Sumalinab, Oswaldo Gressani, Thomas Neyens, Elisa Duarte, Niel Hens, Christel Faes</dc:creator>
    </item>
    <item>
      <title>Bayesian Doubly Robust Causal Inference via Posterior Coupling</title>
      <link>https://arxiv.org/abs/2506.04868</link>
      <description>arXiv:2506.04868v1 Announce Type: new 
Abstract: In observational studies, propensity score methods are central for estimating causal effects while adjusting for confounders. Among them, the doubly robust (DR) estimator has gained considerable attention because it provides consistent estimates when either the propensity score model or the outcome model is correctly specified. Like other propensity score approaches, the DR estimator typically involves two-step estimation: first, estimating the propensity score and outcome models, and then estimating the causal effects using the estimated values. However, this sequential procedure does not naturally align with the Bayesian framework, which centers on updating prior beliefs solely through the likelihood. In this manuscript, we propose novel Bayesian DR estimation via posterior coupling, which incorporates propensity score information via moment conditions directly into the posterior distribution. This design avoids the feedback problem and enables a fully Bayesian interpretation of DR estimation without requiring two-step estimation. We detail the theoretical properties of the proposed method and demonstrate its advantages over existing Bayesian approaches through comprehensive simulation studies and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04868v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shunichiro Orihara, Tomotaka Momozaki, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Goodness-of-fit testing for the stationary density of a size-structured PDE</title>
      <link>https://arxiv.org/abs/2506.05103</link>
      <description>arXiv:2506.05103v1 Announce Type: new 
Abstract: We consider two division models for structured cell populations, where cells can grow, age and divide. These models have been introduced in the literature under the denomination of `mitosis' and `adder' models. In the recent years, there has been an increasing interest in Biology to understand whether the cells divide equally or not, as this can be related to important mechanisms in cellular aging or recovery. We are therefore interested in testing the null hypothesis $H_0$ where the division of a mother cell results into two daughters of equal size or age, against the alternative hypothesis $H_1$ where the division is asymmetric and ruled by a kernel that is absolutely continuous with respect to the Lebesgue measure. The sample consists of i.i.d. observations of cell sizes and ages drawn from the population, and the division is not directly observed. The hypotheses of the test are reformulated as hypotheses on the stationary size and age distributions of the models, which we assume are also the distributions of the observations. We propose a goodness-of-fit test that we study numerically on simulated data before applying it on real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05103v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Van Ha Hoang, Phu Thanh Nguyen, Thanh Mai Pham Ngoc, Vincent Rivoirard, Viet Chi Tran</dc:creator>
    </item>
    <item>
      <title>The Spurious Factor Dilemma: Robust Inference in Heavy-Tailed Elliptical Factor Models</title>
      <link>https://arxiv.org/abs/2506.05116</link>
      <description>arXiv:2506.05116v1 Announce Type: new 
Abstract: Factor models are essential tools for analyzing high-dimensional data, particularly in economics and finance. However, standard methods for determining the number of factors often overestimate the true number when data exhibit heavy-tailed randomness, misinterpreting noise-induced outliers as genuine factors. This paper addresses this challenge within the framework of Elliptical Factor Models (EFM), which accommodate both heavy tails and potential non-linear dependencies common in real-world data. We demonstrate theoretically and empirically that heavy-tailed noise generates spurious eigenvalues that mimic true factor signals. To distinguish these, we propose a novel methodology based on a fluctuation magnification algorithm. We show that under magnifying perturbations, the eigenvalues associated with real factors exhibit significantly less fluctuation (stabilizing asymptotically) compared to spurious eigenvalues arising from heavy-tailed effects. This differential behavior allows the identification and detection of the true and spurious factors. We develop a formal testing procedure based on this principle and apply it to the problem of accurately selecting the number of common factors in heavy-tailed EFMs. Simulation studies and real data analysis confirm the effectiveness of our approach compared to existing methods, particularly in scenarios with pronounced heavy-tailedness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05116v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiang Hu, Jiahui Xie, Yangchun Zhang, Wang Zhou</dc:creator>
    </item>
    <item>
      <title>Estimation of Treatment Effects Under Nonstationarity via Truncated Difference-in-Q's</title>
      <link>https://arxiv.org/abs/2506.05308</link>
      <description>arXiv:2506.05308v1 Announce Type: new 
Abstract: Randomized controlled experiments (''A/B testing'') are fundamental for assessing interventions in dynamic technology-driven environments, such as recommendation systems, online marketplaces, and digital health interventions. In these systems, interventions typically impact not only the current state of the system, but also future states; therefore, accurate estimation of the global average treatment effect (or GATE) from experiments requires accounting for the dynamic temporal behavior of the system. To address this, recent literature has analyzed a range of estimators applied to Bernoulli randomized experiments in stationary environments, ranging from the standard difference-in-means (DM) estimator to methods building on reinforcement learning techniques, such as off-policy evaluation and the recently proposed difference-in-Q's (DQ) estimator. However, all these estimators exhibit high bias and variance when the environment is nonstationary. This paper addresses the challenge of estimation under nonstationarity. We show that a simple extension of the DM estimator using differences in truncated outcome trajectories yields favorable bias and variance in nonstationary Markovian settings. Our theoretical analysis establishes this result by first showing that the truncated estimator is in fact estimating an appropriate policy gradient that can be expressed as a difference in Q-values; thus we refer to our estimator as the truncated DQ estimator (by analogy to the DQ estimator). We then show that the corresponding policy gradient is a first-order approximation to the GATE. Combining these insights yields our bias and variance bounds. We validate our results through synthetic and realistic simulations-including hospital and ride-sharing settings-and show that a well-calibrated truncated DQ estimator achieves low bias and variance even in nonstationary environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05308v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ramesh Johari, Tianyi Peng, Wenqian Xing</dc:creator>
    </item>
    <item>
      <title>On the Wasserstein Geodesic Principal Component Analysis of probability measures</title>
      <link>https://arxiv.org/abs/2506.04480</link>
      <description>arXiv:2506.04480v1 Announce Type: cross 
Abstract: This paper focuses on Geodesic Principal Component Analysis (GPCA) on a collection of probability distributions using the Otto-Wasserstein geometry. The goal is to identify geodesic curves in the space of probability measures that best capture the modes of variation of the underlying dataset. We first address the case of a collection of Gaussian distributions, and show how to lift the computations in the space of invertible linear maps. For the more general setting of absolutely continuous probability measures, we leverage a novel approach to parameterizing geodesics in Wasserstein space with neural networks. Finally, we compare to classical tangent PCA through various examples and provide illustrations on real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04480v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nina Vesseron, Elsa Cazelles, Alice Le Brigant, Thierry Klein</dc:creator>
    </item>
    <item>
      <title>Amortized variational transdimensional inference</title>
      <link>https://arxiv.org/abs/2506.04749</link>
      <description>arXiv:2506.04749v1 Announce Type: cross 
Abstract: The expressiveness of flow-based models combined with stochastic variational inference (SVI) has, in recent years, expanded the application of optimization-based Bayesian inference to include problems with complex data relationships. However, until now, SVI using flow-based models has been limited to problems of fixed dimension. We introduce CoSMIC, normalizing flows (COntextually-Specified Masking for Identity-mapped Components), an extension to neural autoregressive conditional normalizing flow architectures that enables using a single amortized variational density for inference over a transdimensional target distribution. We propose a combined stochastic variational transdimensional inference (VTI) approach to training CoSMIC flows using techniques from Bayesian optimization and Monte Carlo gradient estimation. Numerical experiments demonstrate the performance of VTI on challenging problems that scale to high-cardinality model spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04749v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Laurence Davies, Dan Mackinlay, Rafael Oliveira, Scott A. Sisson</dc:creator>
    </item>
    <item>
      <title>At the edge of Donsker's Theorem: Asymptotics of multiscale scan statistics</title>
      <link>https://arxiv.org/abs/2506.05112</link>
      <description>arXiv:2506.05112v1 Announce Type: cross 
Abstract: For nonparametric inference about a function, multiscale testing procedures resolve the need for bandwidth selection and achieve asymptotically optimal detection performance against a broad range of alternatives. However, critical values strongly depend on the noise distribution, and we argue that existing methods are either statistically infeasible, or asymptotically sub-optimal. To address this methodological challenge, we show how to develop a feasible multiscale test via weak convergence arguments, by replacing the additive multiscale penalty with a multiplicative weighting. This new theoretical foundation preserves the optimal detection properties of multiscale tests and extends their applicability to nonstationary nonlinear time series via a tailored bootstrap scheme. Inference for signal discovery, goodness-of-fit testing of regression functions, and multiple changepoint detection is studied in detail, and we apply the new methodology to analyze the April 2025 power blackout on the Iberian peninsula. Our methodology is enabled by a novel functional central limit in H\"older spaces with critical modulus of continuity, where Donsker's theorem fails to hold due to lack of tightness. Probabilistically, we discover a novel form of thresholded weak convergence that holds only in the upper support of the distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05112v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johann K\"ohne, Fabian Mies</dc:creator>
    </item>
    <item>
      <title>Nonlinear Causal Discovery for Grouped Data</title>
      <link>https://arxiv.org/abs/2506.05120</link>
      <description>arXiv:2506.05120v1 Announce Type: cross 
Abstract: Inferring cause-effect relationships from observational data has gained significant attention in recent years, but most methods are limited to scalar random variables. In many important domains, including neuroscience, psychology, social science, and industrial manufacturing, the causal units of interest are groups of variables rather than individual scalar measurements. Motivated by these applications, we extend nonlinear additive noise models to handle random vectors, establishing a two-step approach for causal graph learning: First, infer the causal order among random vectors. Second, perform model selection to identify the best graph consistent with this order. We introduce effective and novel solutions for both steps in the vector case, demonstrating strong performance in simulations. Finally, we apply our method to real-world assembly line data with partial knowledge of causal ordering among variable groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05120v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Konstantin G\"obler, Tobias Windisch, Mathias Drton</dc:creator>
    </item>
    <item>
      <title>Causal Effect Identification in lvLiNGAM from Higher-Order Cumulants</title>
      <link>https://arxiv.org/abs/2506.05202</link>
      <description>arXiv:2506.05202v1 Announce Type: cross 
Abstract: This paper investigates causal effect identification in latent variable Linear Non-Gaussian Acyclic Models (lvLiNGAM) using higher-order cumulants, addressing two prominent setups that are challenging in the presence of latent confounding: (1) a single proxy variable that may causally influence the treatment and (2) underspecified instrumental variable cases where fewer instruments exist than treatments. We prove that causal effects are identifiable with a single proxy or instrument and provide corresponding estimation methods. Experimental results demonstrate the accuracy and robustness of our approaches compared to existing methods, advancing the theoretical and practical understanding of causal inference in linear systems with latent confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05202v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Tramontano, Yaroslav Kivva, Saber Salehkaleybar Mathias Drton, Negar Kiyavash</dc:creator>
    </item>
    <item>
      <title>Pooling information in likelihood-free inference</title>
      <link>https://arxiv.org/abs/2212.02658</link>
      <description>arXiv:2212.02658v2 Announce Type: replace 
Abstract: Likelihood-free inference (LFI) methods, such as approximate Bayesian computation, have become commonplace for conducting inference in complex models. Many approaches are based on summary statistics or discrepancies derived from synthetic data. However, determining which summary statistics or discrepancies to use for constructing the posterior remains a challenging question, both practically and theoretically. Instead of relying on a single vector of summaries for inference, we propose a new pooled posterior that optimally combines inferences from multiple LFI posteriors. This pooled approach eliminates the need to select a single vector of summaries or even a specific LFI algorithm. Our approach is straightforward to implement and avoids performing a high-dimensional LFI analysis involving all summary statistics. We give theoretical guarantees for the improved performance of the pooled posterior mean in terms of asymptotic frequentist risk and demonstrate the effectiveness of the approach in a number of benchmark examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.02658v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David T. Frazier, Christopher Drovandi, Lucas Kock, David J. Nott</dc:creator>
    </item>
    <item>
      <title>Abnormal component analysis</title>
      <link>https://arxiv.org/abs/2312.16139</link>
      <description>arXiv:2312.16139v2 Announce Type: replace 
Abstract: At the crossway of machine learning and data analysis, anomaly detection aims at identifying observations that exhibit abnormal behaviour. Be it measurement errors, disease development, severe weather, production quality default(s) (items) or failed equipment, financial frauds or crisis events, their on-time identification and isolation constitute an important task in almost any area of industry and science. While a substantial body of literature is devoted to detection of anomalies, little attention is payed to their explanation. This is the case mostly due to intrinsically non-supervised nature of the task and non-robustness of the exploratory methods like principal component analysis (PCA).
  We introduce a new statistical tool dedicated for exploratory analysis of abnormal observations using data depth as a score. Abnormal component analysis (shortly ACA) is a method that searches a low-dimensional data representation that best visualises and explains anomalies. This low-dimensional representation not only allows to distinguish groups of anomalies better than the methods of the state of the art, but as well provides a -- linear in variables and thus easily interpretable -- explanation for anomalies. In a comparative simulation and real-data study, ACA also proves advantageous for anomaly analysis with respect to methods present in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16139v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Romain Valla, Pavlo Mozharovskyi, Florence d'Alch\'e-Buc</dc:creator>
    </item>
    <item>
      <title>Orthogonal calibration via posterior projections with applications to the Schwarzschild model</title>
      <link>https://arxiv.org/abs/2404.03152</link>
      <description>arXiv:2404.03152v3 Announce Type: replace 
Abstract: The orbital superposition method originally developed by Schwarzschild (1979) is used to study the dynamics of growth of a black hole and its host galaxy, and has uncovered new relationships between the galaxy's global characteristics. Scientists are specifically interested in finding optimal parameter choices for this model that best match physical measurements along with quantifying the uncertainty of such procedures. This renders a statistical calibration problem with multivariate outcomes. In this article, we develop a Bayesian method for calibration with multivariate outcomes using orthogonal bias functions thus ensuring parameter identifiability. Our approach is based on projecting the posterior to an appropriate space which allows the user to choose any nonparametric prior on the bias function(s) instead of having to model it (them) with Gaussian processes. We develop a functional projection approach using the theory of Hilbert spaces. A finite-dimensional analogue of the projection problem is also considered. We illustrate the proposed approach using a BART prior and apply it to calibrate the Schwarzschild model illustrating how a multivariate approach may resolve discrepancies resulting from a univariate calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03152v3</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antik Chakraborty, Jonelle B. Walsh, Louis Strigari, Bani K. Mallick, Anirban Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Proximal indirect comparison</title>
      <link>https://arxiv.org/abs/2405.10773</link>
      <description>arXiv:2405.10773v2 Announce Type: replace 
Abstract: We consider the problem of indirect comparison, where a treatment arm of interest is absent by design in one randomized controlled trial but available in the other. The former is the target trial, and the latter is the source trial. The identifiability of the target population average treatment effect often relies on conditional transportability assumptions. However, it is a common concern whether all relevant effect modifiers are measured and controlled for. We give a new proximal identification result in the presence of shifted, unobserved effect modifiers based on proxies: an adjustment proxy in both trials and an additional reweighting proxy in the source trial. We propose an estimator which is doubly-robust against misspecifications of the so-called bridge functions and asymptotically normal under mild consistency of estimators for the bridge functions. We use two weight management trials as a context to illustrate selection of proxies and apply our method to compare the weight loss effect of active treatments from these trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10773v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zehao Su, Helene C. W. Rytgaard, Henrik Ravn, Frank Eriksson</dc:creator>
    </item>
    <item>
      <title>Causal Inference with Outcomes Truncated by Death and Missing Not at Random</title>
      <link>https://arxiv.org/abs/2406.10554</link>
      <description>arXiv:2406.10554v3 Announce Type: replace 
Abstract: In clinical trials, principal stratification analysis is commonly employed to address the issue of truncation by death, where a subject dies before the outcome can be measured. However, in practice, many survivor outcomes may remain uncollected or be missing not at random, posing a challenge to standard principal stratification analyses. In this paper, we explore the identification, estimation, and bounds of the average treatment effect within a subpopulation of individuals who would potentially survive under both treatment and control conditions. We show that the causal parameter of interest can be identified by introducing a proxy variable that affects the outcome only through the principal strata, while requiring that the treatment variable does not directly affect the missingness mechanism. Subsequently, we propose an approach for estimating causal parameters and derive nonparametric bounds in cases where identification assumptions are violated. We illustrate the performance of the proposed method through simulation studies and a real dataset obtained from a Human Immunodeficiency Virus (HIV) study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10554v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Li, Yuan Liu, Shanshan Luo, Zhi Geng</dc:creator>
    </item>
    <item>
      <title>A discrete-time survival model to handle interval-censored covariates, with applications to HIV cohort studies</title>
      <link>https://arxiv.org/abs/2408.07738</link>
      <description>arXiv:2408.07738v2 Announce Type: replace 
Abstract: Methods are lacking to handle the problem of survival analysis in the presence of an interval-censored covariate, specifically the case in which the conditional hazard of the primary event of interest depends on the occurrence of a secondary event, the observation time of which is subject to interval censoring. We propose and study a flexible class of discrete-time parametric survival models that handle the censoring problem through simultaneous modeling of the interval-censored secondary event, the outcome, and the censoring mechanism. We apply this model to the research question that motivated the methodology, estimating the effect of HIV status on all-cause mortality in a prospective cohort study in South Africa. Our model has applicability for many open questions, including estimating the impact of policy decisions on population level HIV-related outcomes and determining causes of morbidity and mortality for which the HIV positive population may be at increased risk. Examples include determining how the large-scale transition from efavirenz-based to dolutegravir-based first-line ART impacted mortality for people living with HIV and determining whether HIV status is associated with increased risk of stroke, diabetes, hypertension, and other non-communicable diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07738v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Avi Kenny, Stephen Olivier, Jianxuan Zang, Jeffrey W. Imai-Eaton, James P. Hughes, Mark J. Siedner</dc:creator>
    </item>
    <item>
      <title>Smoothing Variances Across Time: Adaptive Stochastic Volatility</title>
      <link>https://arxiv.org/abs/2408.11315</link>
      <description>arXiv:2408.11315v4 Announce Type: replace 
Abstract: We introduce a novel Bayesian framework for estimating time-varying volatility by extending the Random Walk Stochastic Volatility (RWSV) model with Dynamic Shrinkage Processes (DSP) in log-variances. Unlike the classical Stochastic Volatility (SV) or GARCH-type models with restrictive parametric stationarity assumptions, our proposed Adaptive Stochastic Volatility (ASV) model provides smooth yet dynamically adaptive estimates of evolving volatility and its uncertainty. We further enhance the model by incorporating a nugget effect, allowing it to flexibly capture small-scale variability while preserving smoothness elsewhere. We derive the theoretical properties of the global-local shrinkage prior DSP. Through simulation studies, we show that ASV exhibits remarkable misspecification resilience and low prediction error across various data-generating processes. Furthermore, ASV's capacity to yield locally smooth and interpretable estimates facilitates a clearer understanding of the underlying patterns and trends in volatility. As an extension, we develop the Bayesian Trend Filter with ASV (BTF-ASV) which allows joint modeling of the mean and volatility with abrupt changes. Finally, our proposed models are applied to time series data from finance, econometrics, and environmental science, highlighting their flexibility and broad applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11315v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason B. Cho, David S. Matteson</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Independence Testing via Maximum and Average Distance Correlations</title>
      <link>https://arxiv.org/abs/2001.01095</link>
      <description>arXiv:2001.01095v3 Announce Type: replace-cross 
Abstract: This paper investigates the utilization of maximum and average distance correlations for multivariate independence testing. We characterize their consistency properties in high-dimensional settings with respect to the number of marginally dependent dimensions, compare the advantages of each test statistic, examine their respective null distributions, and present a fast chi-square-based testing procedure. The resulting tests are non-parametric and applicable to both Euclidean distance and the Gaussian kernel as the underlying metric. To better understand the practical use cases of the proposed tests, we evaluate the empirical performance of the maximum distance correlation, average distance correlation, and the original distance correlation across various multivariate dependence scenarios, as well as conduct a real data experiment to test the presence of various cancer types and peptide levels in human plasma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2001.01095v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cencheng Shen, Yuexiao Dong</dc:creator>
    </item>
    <item>
      <title>Second Order Ensemble Langevin Method for Sampling and Inverse Problems</title>
      <link>https://arxiv.org/abs/2208.04506</link>
      <description>arXiv:2208.04506v3 Announce Type: replace-cross 
Abstract: We propose a sampling method based on an ensemble approximation of second order Langevin dynamics. The log target density is appended with a quadratic term in an auxiliary momentum variable and damped-driven Hamiltonian dynamics introduced; the resulting stochastic differential equation is invariant to the Gibbs measure, with marginal on the position coordinates given by the target. A preconditioner based on covariance under the law of the dynamics does not change this invariance property, and is introduced to accelerate convergence to the Gibbs measure. The resulting mean-field dynamics may be approximated by an ensemble method; this results in a gradient-free and affine-invariant stochastic dynamical system. Numerical results demonstrate its potential as the basis for a numerical sampler in Bayesian inverse problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.04506v3</guid>
      <category>math.DS</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Communications in Mathematical Sciences, Volume 23 (2025) Number 5</arxiv:journal_reference>
      <dc:creator>Ziming Liu, Andrew M. Stuart, Yixuan Wang</dc:creator>
    </item>
    <item>
      <title>Policy learning "without" overlap: Pessimism and generalized empirical Bernstein's inequality</title>
      <link>https://arxiv.org/abs/2212.09900</link>
      <description>arXiv:2212.09900v4 Announce Type: replace-cross 
Abstract: This paper studies offline policy learning, which aims at utilizing observations collected a priori (from either fixed or adaptively evolving behavior policies) to learn an optimal individualized decision rule that achieves the best overall outcomes for a given population. Existing policy learning methods rely on a uniform overlap assumption, i.e., the propensities of exploring all actions for all individual characteristics must be lower bounded. As one has no control over the data collection process, this assumption can be unrealistic in many situations, especially when the behavior policies are allowed to evolve over time with diminishing propensities for certain actions.
  In this paper, we propose Pessimistic Policy Learning (PPL), a new algorithm that optimizes lower confidence bounds (LCBs) -- instead of point estimates -- of the policy values. The LCBs are constructed using knowledge of the behavior policies for collecting the offline data. Without assuming any uniform overlap condition, we establish a data-dependent upper bound for the suboptimality of our algorithm, which only depends on (i) the overlap for the optimal policy, and (ii) the complexity of the policy class we optimize over. As an implication, for adaptively collected data, we ensure efficient policy learning as long as the propensities for optimal actions are lower bounded over time, while those for suboptimal ones are allowed to diminish arbitrarily fast. In our theoretical analysis, we develop a new self-normalized type concentration inequality for inverse-propensity-weighting estimators, generalizing the well-known empirical Bernstein's inequality to unbounded and non-i.i.d. data. We complement our theory with an efficient optimization algorithm via Majorization-Minimization and policy tree search, as well as extensive simulation studies and real-world applications that demonstrate the efficacy of PPL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.09900v4</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ying Jin, Zhimei Ren, Zhuoran Yang, Zhaoran Wang</dc:creator>
    </item>
    <item>
      <title>Learned harmonic mean estimation of the Bayesian evidence with normalizing flows</title>
      <link>https://arxiv.org/abs/2405.05969</link>
      <description>arXiv:2405.05969v2 Announce Type: replace-cross 
Abstract: We present the learned harmonic mean estimator with normalizing flows - a robust, scalable and flexible estimator of the Bayesian evidence for model comparison. Since the estimator is agnostic to sampling strategy and simply requires posterior samples, it can be applied to compute the evidence using any Markov chain Monte Carlo (MCMC) sampling technique, including saved down MCMC chains, or any variational inference approach. The learned harmonic mean estimator was recently introduced, where machine learning techniques were developed to learn a suitable internal importance sampling target distribution to solve the issue of exploding variance of the original harmonic mean estimator. In this article we present the use of normalizing flows as the internal machine learning technique within the learned harmonic mean estimator. Normalizing flows can be elegantly coupled with the learned harmonic mean to provide an approach that is more robust, flexible and scalable than the machine learning models considered previously. We perform a series of numerical experiments, applying our method to benchmark problems and to a cosmological example in up to 21 dimensions. We find the learned harmonic mean estimator is in agreement with ground truth values and nested sampling estimates. The open-source harmonic Python package implementing the learned harmonic mean, now with normalizing flows included, is publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05969v2</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alicja Polanska, Matthew A. Price, Davide Piras, Alessio Spurio Mancini, Jason D. McEwen</dc:creator>
    </item>
    <item>
      <title>Mining Causality: AI-Assisted Search for Instrumental Variables</title>
      <link>https://arxiv.org/abs/2409.14202</link>
      <description>arXiv:2409.14202v3 Announce Type: replace-cross 
Abstract: The instrumental variables (IVs) method is a leading empirical strategy for causal inference. Finding IVs is a heuristic and creative process, and justifying its validity -- especially exclusion restrictions -- is largely rhetorical. We propose using large language models (LLMs) to search for new IVs through narratives and counterfactual reasoning, similar to how a human researcher would. The stark difference, however, is that LLMs can dramatically accelerate this process and explore an extremely large search space. We demonstrate how to construct prompts to search for potentially valid IVs. We contend that multi-step and role-playing prompting strategies are effective for simulating the endogenous decision-making processes of economic agents and for navigating language models through the realm of real-world scenarios, rather than anchoring them within the narrow realm of academic discourses on IVs. We apply our method to three well-known examples in economics: returns to schooling, supply and demand, and peer effects. We then extend our strategy to finding (i) control variables in regression and difference-in-differences and (ii) running variables in regression discontinuity designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14202v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sukjin Han</dc:creator>
    </item>
    <item>
      <title>Isolated Causal Effects of Natural Language</title>
      <link>https://arxiv.org/abs/2410.14812</link>
      <description>arXiv:2410.14812v3 Announce Type: replace-cross 
Abstract: As language technologies become widespread, it is important to understand how changes in language affect reader perceptions and behaviors. These relationships may be formalized as the isolated causal effect of some focal language-encoded intervention (e.g., factual inaccuracies) on an external outcome (e.g., readers' beliefs). In this paper, we introduce a formal estimation framework for isolated causal effects of language. We show that a core challenge of estimating isolated effects is the need to approximate all non-focal language outside of the intervention. Drawing on the principle of omitted variable bias, we provide measures for evaluating the quality of both non-focal language approximations and isolated effect estimates themselves. We find that poor approximation of non-focal language can lead to bias in the corresponding isolated effect estimates due to omission of relevant variables, and we show how to assess the sensitivity of effect estimates to such bias along the two key axes of fidelity and overlap. In experiments on semi-synthetic and real-world data, we validate the ability of our framework to correctly recover isolated effects and demonstrate the utility of our proposed measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14812v3</guid>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael</dc:creator>
    </item>
    <item>
      <title>Spatial-temporal prediction of forest attributes using latent Gaussian models and inventory data</title>
      <link>https://arxiv.org/abs/2503.16691</link>
      <description>arXiv:2503.16691v2 Announce Type: replace-cross 
Abstract: The USDA Forest Inventory and Analysis (FIA) program conducts a national forest inventory for the United States through a network of permanent field plots. FIA produces estimates of area averages and totals for plot-measured forest variables through design-based inference, assuming a fixed population and a probability sample of field plot locations. The fixed-population assumption and characteristics of the FIA sampling scheme make it difficult to estimate change in forest variables over time using design-based inference. We propose spatial-temporal models based on Gaussian processes as a flexible tool for forest inventory data, capable of inferring forest variables and change thereof over arbitrary spatial and temporal domains. It is shown to be beneficial for the covariance function governing the latent Gaussian process to account for variation at multiple scales, separating spatially local variation from ecosystem-scale variation. We demonstrate a model for forest biomass density, inferring 20 years of biomass change within two US National Forests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16691v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 06 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul B. May, Andrew O. Finley</dc:creator>
    </item>
  </channel>
</rss>
