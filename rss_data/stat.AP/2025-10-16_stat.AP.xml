<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Oct 2025 04:03:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Long-Term Spatio-Temporal Forecasting of Monthly Rainfall in West Bengal Using Ensemble Learning Approaches</title>
      <link>https://arxiv.org/abs/2510.13927</link>
      <description>arXiv:2510.13927v1 Announce Type: new 
Abstract: Rainfall forecasting plays a critical role in climate adaptation, agriculture, and water resource management. This study develops long-term forecasts of monthly rainfall across 19 districts of West Bengal using a century-scale dataset spanning 1900-2019. Daily rainfall records are aggregated into monthly series, resulting in 120 years of observations for each district. The forecasting task involves predicting the next 108 months (9 years, 2011-2019) while accounting for temporal dependencies and spatial interactions among districts. To address the nonlinear and complex structure of rainfall dynamics, we propose a hierarchical modeling framework that combines regression-based forecasting of yearly features with multi-layer perceptrons (MLPs) for monthly prediction. Yearly features, such as annual totals, quarterly proportions, variability measures, skewness, and extremes, are first forecasted using regression models that incorporate both own lags and neighboring-district lags. These forecasts are then integrated as auxiliary inputs into an MLP model, which captures nonlinear temporal patterns and spatial dependencies in the monthly series. The results demonstrate that the hierarchical regression-MLP architecture provides robust long-term spatio-temporal forecasts, offering valuable insights for agriculture, irrigation planning, and water conservation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13927v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jishu Adhikary, Raju Maiti</dc:creator>
    </item>
    <item>
      <title>Earthquake Forecasting with ETAS.inlabru</title>
      <link>https://arxiv.org/abs/2510.13930</link>
      <description>arXiv:2510.13930v1 Announce Type: new 
Abstract: The ETAS models are currently the most popular in the field of earthquake forecasting. The MCMC method is time-consuming and limited by parameter correlation while bringing parameter uncertainty. The INLA-based method "inlabru" solves these problems and performs better at Bayesian inference.
  The report introduces the composition of the ETAS model, then provides the model's log-likelihood and approximates it using Taylor expansion and binning strategies. We also present the general procedure of Bayesian inference in inlabru.
  The report follows three experiments. The first one explores the effect of fixing one parameter at its actual or wrong values on the posterior distribution of other parameters. We found that $\alpha$ and $K$ have an apparent mutual influence relationship. At the same time, fixing $\alpha$ or $K$ to its actual value can reduce the model fitting time by more than half.
  The second experiment compares normalised inter-event-time distribution on real data and synthetic catalogues. The distributions of normalised inter-event-time of real data and synthetic catalogues are consistent. Compared with Exp(1), they have more short and long inter-event-time, indicating the existence of clustering. Change on $\mu$ and $p$ will influence the inter-event-time distribution.
  In the last one, we use events before the mainshock to predict events ten weeks after the mainshock. We use the number test and Continuous Ranked Probability Score (CRPS) to measure the accuracy and precision of the predictions. We found that we need at least one mainshock and corresponding offspring to make reliable forecasting. And when we have more mainshocks in our data, our forecasting will be better. Besides, we also figure out what is needed to obtain a good posterior distribution for each parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13930v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziwen Zhong</dc:creator>
    </item>
    <item>
      <title>A Data-Parsimonious Model for Long-Term Risk Assessments of West Nile Virus Spillover</title>
      <link>https://arxiv.org/abs/2510.14011</link>
      <description>arXiv:2510.14011v1 Announce Type: new 
Abstract: Many West Nile virus (WNV) forecasting frameworks incorporate entomological or avian surveillance data, which may be unavailable in some regions. We introduce a novel data-parsimonious probabilistic model to predict both the timing of outbreak onset and the seasonal severity of WNV spillover. Our approach combines a temperature-driven compartmental model of WNV with nonparametric kernel density estimation methods to construct a joint probability density function and a Poisson rate surface as function of mosquito abundance and normalized cumulative temperature. Calibrated on human incidence records, the model produces reliable forecasts several months before the transmission season begins, supporting proactive mitigation efforts. We evaluated the framework across three counties in California (Orange, Los Angeles, and Riverside), two in Texas (Dallas and Harris), and one in Florida (Duval), representing completely different ecology and distinct climatic regimes, and observed strong agreement across multiple performance metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14011v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saman Hosseini, Lee W. Cohnstaedt, Matin Marjani, Caterina Scoglio</dc:creator>
    </item>
    <item>
      <title>Bayes-ically fair: A Bayesian Ranking of the Olympic Medal Table</title>
      <link>https://arxiv.org/abs/2510.14723</link>
      <description>arXiv:2510.14723v1 Announce Type: new 
Abstract: Evaluating a country's sporting success provides insight into its decision-making and infrastructure for developing athletic talent. The Olympic Games serve as a global benchmark, yet conventional medal rankings can be unduly influenced by population size. We propose a Bayesian ranking scheme to rank the performance of National Olympic Committees by their "long-run" medals-to-population ratio. The algorithm aims to mitigate the influence of large populations and reduce the stochastic fluctuations for smaller nations by applying shrinkage. These long-run rankings provide a more stable and interpretable ordering of national sporting performance across games compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14723v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cormac MacDermott, Carl J. Scarrott, John Ferguson</dc:creator>
    </item>
    <item>
      <title>Dynamic Spatial Treatment Effect Boundaries: A Continuous Functional Framework from Navier-Stokes Equations</title>
      <link>https://arxiv.org/abs/2510.14409</link>
      <description>arXiv:2510.14409v1 Announce Type: cross 
Abstract: I develop a comprehensive theoretical framework for dynamic spatial treatment effect boundaries using continuous functional definitions grounded in Navier-Stokes partial differential equations. Rather than discrete treatment effect estimators, the framework characterizes treatment intensity as a continuous function $\tau(\mathbf{x}, t)$ over space-time, enabling rigorous analysis of propagation dynamics, boundary evolution, and cumulative exposure patterns. Building on exact self-similar solutions expressible through Kummer confluent hypergeometric and modified Bessel functions, I establish that treatment effects follow scaling laws $\tau(d, t) = t^{-\alpha} f(d/t^\beta)$ where exponents characterize diffusion mechanisms. Empirical validation using 42 million TROPOMI satellite observations of NO$_2$ pollution from U.S. coal-fired power plants demonstrates strong exponential spatial decay ($\kappa_s = 0.004$ per km, $R^2 = 0.35$) with detectable boundaries at 572 km. Monte Carlo simulations confirm superior performance over discrete parametric methods in boundary detection and false positive avoidance (94\% vs 27\% correct rejection). Regional heterogeneity analysis validates diagnostic capability: positive decay parameters within 100 km confirm coal plant dominance; negative parameters beyond 100 km correctly signal when urban sources dominate. The continuous functional perspective unifies spatial econometrics with mathematical physics, providing theoretically grounded methods for boundary detection, exposure quantification, and policy evaluation across environmental economics, banking, and healthcare applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14409v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatsuru Kikuchi</dc:creator>
    </item>
    <item>
      <title>ROC Analysis with Covariate Adjustment Using Neural Network Models: Evaluating the Role of Age in the Physical Activity-Mortality Association</title>
      <link>https://arxiv.org/abs/2510.14494</link>
      <description>arXiv:2510.14494v1 Announce Type: cross 
Abstract: The receiver operating characteristic (ROC) curve and its summary measure, the Area Under the Curve (AUC), are well-established tools for evaluating the efficacy of biomarkers in biomedical studies. Compared to the traditional ROC curve, the covariate-adjusted ROC curve allows for individual evaluation of the biomarker. However, the use of machine learning models has rarely been explored in this context, despite their potential to develop more powerful and sophisticated approaches for biomarker evaluation. The goal of this paper is to propose a framework for neural network-based covariate-adjusted ROC modeling that allows flexible and nonlinear evaluation of the effectiveness of a biomarker to discriminate between two reference populations. The finite-sample performance of our method is investigated through extensive simulation tests under varying dependency structures between biomarkers, covariates, and referenced populations. The methodology is further illustrated in a clinically case study that assesses daily physical activity - measured as total activity time (TAC), a proxy for daily step count-as a biomarker to predict mortality at three, five and eight years. Analyzes stratified by sex and adjusted for age and BMI reveal distinct covariate effects on mortality outcomes. These results underscore the importance of covariate-adjusted modeling in biomarker evaluation and highlight TAC's potential as a functional capacity biomarker based on specific individual characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14494v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziad Akram Ali Hammouri, Yating Zhou, Rahul Ghosal, Juan C. Vidal, Marcos Matabuena</dc:creator>
    </item>
    <item>
      <title>Additive Density Regression</title>
      <link>https://arxiv.org/abs/2510.14502</link>
      <description>arXiv:2510.14502v1 Announce Type: cross 
Abstract: We present a structured additive regression approach to model conditional densities given scalar covariates, where only samples of the conditional distributions are observed. This links our approach to distributional regression models for scalar data. The model is formulated in a Bayes Hilbert space -- preserving nonnegativity and integration to one under summation and scalar multiplication -- with respect to an arbitrary finite measure. This allows to consider, amongst others, continuous, discrete and mixed densities. Our theoretical results include asymptotic existence, uniqueness, consistency, and asymptotic normality of the penalized maximum likelihood estimator, as well as confidence regions and inference for the (effect) densities. For estimation, we propose to maximize the penalized log-likelihood corresponding to an appropriate multinomial, or equivalently, Poisson regression model, which we show to approximate the original penalized maximum likelihood problem. We apply our framework to a motivating gender economic data set from the German Socio-Economic Panel Study (SOEP), analyzing the distribution of the woman's share in a couple's total labor income given covariate effects for year, place of residence and age of the youngest child. As the income share is a continuous variable having discrete point masses at zero and one for single-earner couples, the corresponding densities are of mixed type.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14502v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eva-Maria Maier, Alexander Fottner, Sonja Greven, Almond St\"ocker</dc:creator>
    </item>
    <item>
      <title>Loss functions arising from the index of agreement</title>
      <link>https://arxiv.org/abs/2510.14714</link>
      <description>arXiv:2510.14714v1 Announce Type: cross 
Abstract: We examine the theoretical properties of the index of agreement loss function $L_W$, the negatively oriented counterpart of Willmott's index of agreement, a common metric in environmental sciences and engineering. We prove that $L_W$ is bounded within [0, 1], translation and scale invariant, and estimates the parameter $\Bbb{E}_{F}[\underline{y}] \pm \Bbb{V}_{F}^{1/2}[\underline{y}]$ when fitting a distribution. We propose $L_{\operatorname{NR}_2}$ as a theoretical improvement, which replaces the denominator of $L_W$ with the sum of Euclidean distances, better aligning with the underlying geometric intuition. This new loss function retains the appealing properties of $L_W$ but also admits closed-form solutions for linear model parameter estimation. We show that as the correlation between predictors and the dependent variable approaches 1, parameter estimates from squared error, $L_{\operatorname{NR}_2}$ and $L_W$ converge. This behavior is mirrored in hydrologic model calibration (a core task in water resources engineering), where performance becomes nearly identical across these loss functions. Finally, we suggest potential improvements for existing $L_p$-norm variants of the index of agreement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14714v1</guid>
      <category>stat.ME</category>
      <category>physics.comp-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hristos Tyralis, Georgia Papacharalampous</dc:creator>
    </item>
    <item>
      <title>A contribution to discern the true impact of covid-19 on human mortality</title>
      <link>https://arxiv.org/abs/2401.03875</link>
      <description>arXiv:2401.03875v2 Announce Type: replace 
Abstract: The years 2020 and 2021 were characterized by the COVID-19 pandemic. The true impact of the pandemic on populations' health and life still has to be fully discerned. The main objective of this work is to discern the true impact of COVID-19 pandemic in the EU countries. The mortality trends are considered and modelled. The excess mortality attributable to COVID-19, on a yearly basis, is estimated via a novel strategy that combines datasets from different official sources. Considering demographic and geographic factors, new indices are also formulated, ranking the pandemic impact on EU countries, and new sociopolitical/economic reflections. This work, which is also in line with the conclusions of previously published authoritative studies on excess mortality, represents an original methodology that, in a timely manner, can be implemented in the services of public decision makers for future emerging needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03875v2</guid>
      <category>stat.AP</category>
      <category>q-bio.PE</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefano Barone</dc:creator>
    </item>
    <item>
      <title>Improving measurement error and representativeness in nonprobability surveys</title>
      <link>https://arxiv.org/abs/2410.18282</link>
      <description>arXiv:2410.18282v2 Announce Type: replace 
Abstract: In the age of big data, nonprobability surveys are becoming increasingly abundant. Data integration techniques involving both probability and nonprobability surveys are being extensively used for providing improved estimates for finite population estimation. While much of the existing research has focused on mitigating selection bias in nonprobability surveys, the issue of measurement error within these surveys remains relatively unexplored. Statistical methods devised with the purpose of reducing selection bias are appropriate for reliable estimation, only under the assumption of accuracy of survey responses. Motivated by a recent case study of Kennedy, Mercer, and Lau (2024), our research addresses bias from both measurement and sampling errors in nonprobability surveys. In this article, we propose a new data integration method that uses multiple probability and nonprobability surveys and leverages machine learning models to construct a composite estimator. The proposed composite estimator integrates probability and nonprobability surveys, when both contain response variables of interest. We analyze the performance of this estimator in comparison to an existing composite estimator in literature, analytically as well as empirically, using multiple survey data from Kennedy et al. (2024). Finally, we identify conditions under which the proposed estimator outperforms estimators based solely on probability surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18282v2</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditi Sen, Partha Lahiri</dc:creator>
    </item>
    <item>
      <title>Partially stochastic deep learning with uncertainty quantification for model predictive heating control</title>
      <link>https://arxiv.org/abs/2504.03350</link>
      <description>arXiv:2504.03350v3 Announce Type: replace 
Abstract: Making the control of building heating systems more energy efficient is crucial for reducing global energy consumption and greenhouse gas emissions. Traditional rule-based control methods use a static, outdoor temperature-dependent heating curve to regulate heat input. This open-loop approach fails to account for both the current state of the system (indoor temperature) and free heat gains, such as solar radiation, often resulting in poor thermal comfort and overheating. Model Predictive Control (MPC) addresses these drawbacks by using predictive modeling to optimize heating based on a building's learned thermal behavior, current system state, and weather forecasts. However, current industrial MPC solutions often employ simplified physics-inspired indoor temperature models, sacrificing accuracy for robustness and interpretability. While purely data-driven models offer superior predictive performance and therefore more accurate control, they face challenges such as a lack of transparency.
  To bridge this gap, we propose a partially stochastic deep learning (DL) architecture, dubbed LSTM+BNN, for building-specific indoor temperature modeling. Unlike most studies that evaluate model performance through simulations or limited test buildings, our experiments across a comprehensive dataset of 100 real-world buildings, under various weather conditions, demonstrate that LSTM+BNN outperforms an industry-proven reference model, reducing the average prediction error measured as RMSE by more than 40% for the 48-hour prediction horizon of interest. Unlike deterministic DL approaches, LSTM+BNN offers a critical advantage by enabling pre-assessment of model competency for control optimization through uncertainty quantification. Thus, the proposed model shows significant potential to improve thermal comfort and energy efficiency achieved with heating MPC solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03350v3</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Hannula, Arttu H\"akkinen, Antti Solonen, Felipe Uribe, Jana de Wiljes, Lassi Roininen</dc:creator>
    </item>
    <item>
      <title>Planning for gold: Hypothesis screening with split samples for valid powerful testing in matched observational studies</title>
      <link>https://arxiv.org/abs/2406.00866</link>
      <description>arXiv:2406.00866v2 Announce Type: replace-cross 
Abstract: Observational studies are valuable tools for inferring causal effects in the absence of controlled experiments. However, these studies may be biased due to the presence of some relevant, unmeasured set of covariates. One approach to mitigate this concern is to identify hypotheses likely to be more resilient to hidden biases by splitting the data into a planning sample for designing the study and an analysis sample for making inferences. We devise a powerful and flexible method for selecting hypotheses in the planning sample when an unknown number of outcomes are affected by the treatment, allowing researchers to gain the benefits of exploratory analysis and still conduct powerful inference under concerns of unmeasured confounding. We investigate the theoretical properties of our method and conduct extensive simulations that demonstrate pronounced benefits, especially at higher levels of allowance for unmeasured confounding. Finally, we demonstrate our method in an observational study of the multi-dimensional impacts of a devastating flood in Bangladesh.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00866v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Bekerman, Abhinandan Dalal, Carlo del Ninno, Dylan S. Small</dc:creator>
    </item>
    <item>
      <title>Replicable Bandits for Digital Health Interventions</title>
      <link>https://arxiv.org/abs/2407.15377</link>
      <description>arXiv:2407.15377v4 Announce Type: replace-cross 
Abstract: Adaptive treatment assignment algorithms, such as bandit algorithms, are increasingly used in digital health intervention clinical trials. Frequently, the data collected from these trials is used to conduct causal inference and related data analyses to decide how to refine the intervention, and whether to roll-out the intervention more broadly. This work studies inference for estimands that depend on the adaptive algorithm itself; a simple example is the mean reward under the adaptive algorithm. Specifically, we investigate the replicability of statistical analyses concerning such estimands when using data from trials deploying adaptive treatment assignment algorithms. We demonstrate that many standard statistical estimators can be inconsistent and fail to be replicable across repetitions of the clinical trial, even as the sample size grows large. We show that this non-replicability is intimately related to properties of the adaptive algorithm itself. We introduce a formal definition of a "replicable bandit algorithm" and prove that under such algorithms, a wide variety of common statistical estimators are guaranteed to be consistent and asymptotically normal. We present both theoretical results and simulation studies based on a mobile health oral health self-care intervention. Our findings underscore the importance of designing adaptive algorithms with replicability in mind, especially for settings like digital health, where deployment decisions rely heavily on replicated evidence. We conclude by discussing open questions on the connections between algorithm design, statistical inference, and experimental replicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15377v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kelly W. Zhang, Nowell Closser, Anna L. Trella, Susan A. Murphy</dc:creator>
    </item>
    <item>
      <title>Calibrated Predictive Lower Bounds on Time-to-Unsafe-Sampling in LLMs</title>
      <link>https://arxiv.org/abs/2506.13593</link>
      <description>arXiv:2506.13593v3 Announce Type: replace-cross 
Abstract: We introduce time-to-unsafe-sampling, a novel safety measure for generative models, defined as the number of generations required by a large language model (LLM) to trigger an unsafe (e.g., toxic) response. While providing a new dimension for prompt-adaptive safety evaluation, quantifying time-to-unsafe-sampling is challenging: unsafe outputs are often rare in well-aligned models and thus may not be observed under any feasible sampling budget. To address this challenge, we frame this estimation problem as one of survival analysis. We build on recent developments in conformal prediction and propose a novel calibration technique to construct a lower predictive bound (LPB) on the time-to-unsafe-sampling of a given prompt with rigorous coverage guarantees. Our key technical innovation is an optimized sampling-budget allocation scheme that improves sample efficiency while maintaining distribution-free guarantees. Experiments on both synthetic and real data support our theoretical results and demonstrate the practical utility of our method for safety risk assessment in generative AI models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13593v3</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 17 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hen Davidov, Gilad Freidkin, Shai Feldman, Yaniv Romano</dc:creator>
    </item>
  </channel>
</rss>
