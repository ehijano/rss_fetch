<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.GN updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.GN</link>
    <description>econ.GN updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.GN" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Feb 2026 05:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Racial Preferences at a Texas Medical School</title>
      <link>https://arxiv.org/abs/2602.18484</link>
      <description>arXiv:2602.18484v1 Announce Type: new 
Abstract: Whether and how race is used in selective admissions remains a central question in higher education and civil rights law. In Students for Fair Admissions v. Harvard (2023), the Supreme Court held that race-based affirmative action in college admissions violates the Equal Protection Clause, purportedly ending the practice. This report examines admissions at a public medical school in the pre-SFFA period. Using applicant-level data on over 11,000 applications to Texas Tech University Health Sciences Center Medical School for the 2021 and 2022 cycles, I relate admission decisions to academic merit (MCAT, GPA, science GPA), race, gender, and situational judgment (Casper) scores. Summary statistics, academic-index decompositions, and logistic regression models provide strong evidence of racial preferences: African American and Hispanic applicants are preferred relative to academically similar White and Asian applicants. Counterfactual and preference-removal analyses quantify the magnitude of these disparities. The findings document the kind of race-based preferences that SFFA was meant to address and establish a baseline for assessing whether admissions practice changed after the decision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18484v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Puelz</dc:creator>
    </item>
    <item>
      <title>Stability Anchors and Risk Amplifiers: Tail Spillovers Across Stablecoin Designs</title>
      <link>https://arxiv.org/abs/2602.18820</link>
      <description>arXiv:2602.18820v1 Announce Type: new 
Abstract: This paper investigates systemic risk transmission across stablecoin markets using Quantile Vector Autoregression (QVAR). Analyzing eight major stablecoins with day data coverage from 2021 to 2025, supplemented by minute-level event studies on three additional coins experiencing major depegs until 2025, we document three findings. First, stabilization mechanism dictates tail-risk behavior: fiat-backed stablecoins function as "stability anchors" with near-zero net spillovers across quantiles, while algorithmic and crypto-collateralized designs become risk amplifiers specifically under extreme market conditions. Second, the theoretical risk isolation between fiat and crypto markets breaks down during stress: direct volatility channels emerge between the US Dollar Index and Bitcoin that bypass stablecoin intermediation. Third, Forbes-Rigobon contagion tests across four depeg events show heterogeneous transmission: after adjusting for volatility, algorithmic stablecoins exhibit significant residual contagion while fiat-backed coins show flight-to-quality effects. These findings imply that uniform stablecoin regulation is inappropriate; regulatory capital buffers for extreme losses should be 2--3x higher for non-fiat-backed stablecoins than median-based measures indicate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18820v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenbin Wu, Can Liu</dc:creator>
    </item>
    <item>
      <title>Fiscal Limits to Protectionism: The 2025 U.S. Tariff Laffer Curve</title>
      <link>https://arxiv.org/abs/2602.18938</link>
      <description>arXiv:2602.18938v1 Announce Type: new 
Abstract: We quantify the Tariff Laffer Curve for the U.S. using a multi-sector Ricardian model calibrated to the 2025 US trade war. We find revenue-maximizing tariffs of 20--30 percent and welfare-maximizing rates of 0--10 percent. We define the Marginal Fiscal Efficiency Index to partition tariffs into welfare-improving, trade-off, and revenue-decreasing regions. Expanding the trade war to more partners raises peak revenue even under retaliation, whereas coordinated retaliation sharply erodes welfare. By January 2026, 20 percent of U.S. tariffs exceed their Laffer peaks. Inverse-optimum estimation reveals diminished U.S. concern for foreign welfare, punitive treatment of China, and rising revenue motives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18938v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pau Pujolas, Jack Rossbach</dc:creator>
    </item>
    <item>
      <title>Political influence and corporate profits: a study of Hungarian firms</title>
      <link>https://arxiv.org/abs/2602.19100</link>
      <description>arXiv:2602.19100v1 Announce Type: new 
Abstract: This paper investigates the extent of political rent seeking in Hungary in the 2010s. Political capitalism--where powerful private interests influence public policy for private gain--creates opportunities for rent seeking that vary across sectors. The analysis is based on a theoretical model assuming rent seeking occurs in a three-stage process: changes in economic institutions granting regulatory privileges, which are enhanced by political-business networks; this leads to scarcities, and increased market power in certain markets; which then generates rents. To quantify this, the study evaluates Hungarian political capitalism by examining the impact of political decisions on firms' rents, analysing the profit trends of the 1,000 largest Hungarian firms (selected annually by net sales) and comparing their mean profit share (earnings before tax) across two periods: 2008-2012 and 2019-2023. A significant increase in a sector's mean profit share was assumed to indicate increased rent seeking. Using Welch's two-sample t-tests, three sectors were identified as potentially experiencing increased rent seeking: agriculture, construction, and financial and insurance activities. Quantitative findings include a 320% increase in mean agricultural profit share (70% in mean ROA), a more than fivefold increase in construction mean profit share (mean ROA from 3.3% to 10.1%), and a more than 6.5 times increase in financial sector mean profit share. Furthermore, a similar Czech analysis showed no significant increases in any sector's profit share, suggesting that the detected rises in Hungarian sectors are linked to domestic activities rather than external factors, which strengthens the findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19100v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10602-026-09505-7</arxiv:DOI>
      <arxiv:journal_reference>Constitutional Political Economy, 2026, Special Issue: Institution and Rent Seeking</arxiv:journal_reference>
      <dc:creator>Zoltan Bartha</dc:creator>
    </item>
    <item>
      <title>Pentecostal Mayors, Sexual Education, and Teenage Pregnancy</title>
      <link>https://arxiv.org/abs/2602.19388</link>
      <description>arXiv:2602.19388v1 Announce Type: new 
Abstract: A growing literature documents how religious institutions shape behavior through social influence, but less is known about what happens when religious movements gain political power and use the tools of government to advance their agenda. We use a regression discontinuity design on close mayoral elections in Brazil to show that mayors from parties institutionally tied to Pentecostal denominations increase teenage fertility 3 per 1,000 higher (a 40% increase). This effect appears for cohorts exposed to middle school during the administration. Consistent with a school-based mechanism, we find that the likelihood that municipal schools offer sexual education programs falls by 12.5 percentage points, with no changes in state schools outside mayoral control. We also find elevated STD rates, and higher middle school dropout rates, while slightly older cohorts show no effects. Results are not explained by changes in contraceptive availability in public clinics, pointing to sexual education as the primary mechanism. We also find no effects from other right-wing parties, indicating the importance of institutional links to Pentecostal parties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19388v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcela Mello, Jo\~ao Garcia</dc:creator>
    </item>
    <item>
      <title>The dynamics of innovation diffusion: A survey of Bass-type models</title>
      <link>https://arxiv.org/abs/2602.19488</link>
      <description>arXiv:2602.19488v1 Announce Type: new 
Abstract: This paper synthesises the existing research on the dynamics of innovation diffusion, with a focus on Bass-type models and their extensions. The theoretical foundation of innovation diffusion proposed by Rogers (1962) and the seminal work of Bass (1969) serve as a starting point for the analysis. We identify and examine various generalizations and stochastic extensions of the Bass model, including counting processes, diffusion processes, and uncertain processes, as well as parameter estimation techniques, from classical statistical techniques to more advanced techniques such as Bayesian filtering and metaheuristic optimisation. We finally explore alternative models of innovation diffusion, with a particular focus on agent-based models. This overview of the evolution of Bass-type models illustrates the progress made in innovation diffusion research over the past decades.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19488v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Langren\'e, Rui Liu, Xiangqin Wu, Tianhao Zhi</dc:creator>
    </item>
    <item>
      <title>Integrating Predictive Models into Two-Sided Recommendations: A Matching-Theoretic Approach</title>
      <link>https://arxiv.org/abs/2602.19689</link>
      <description>arXiv:2602.19689v1 Announce Type: new 
Abstract: Two-sided platforms must recommend users to users, where matches (termed \emph{dates} in this paper) require mutual interest and activity on both sides. Naive ranking by predicted dating probabilities concentrates exposure on a small subset of highly responsive users, generating congestion and overstating efficiency. We model recommendation as a many-to-many matching problem and design integrators that map predicted login, like, and reciprocation probabilities into recommendations under attention constraints. We introduce \emph{effective dates}, a congestion-adjusted metric that discounts matches involving overloaded receivers. We then propose \emph{exposure-constrained deferred acceptance} (ECDA), which limits receiver exposure in terms of expected likes or dates rather than headcount. Using production-grade predictions from a large Japanese dating platform, we show in calibrated simulations that ECDA increases effective dates and receiver-side dating probability despite reducing total dates. A large-scale regional field experiment confirms these effects in practice, indicating that exposure control improves equity and early-stage matching efficiency without harming downstream engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19689v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kazuki Sekiya, Suguru Otani, Yuki Komatsu, Sachio Ohkawa, Shunya Noda</dc:creator>
    </item>
    <item>
      <title>Janus-Faced Technological Progress and the Arms Race in the Education of Humans and Chatbots</title>
      <link>https://arxiv.org/abs/2602.19783</link>
      <description>arXiv:2602.19783v1 Announce Type: new 
Abstract: We study the conditions under which technological advances, in combination with a lognormal wage distribution, incentivize agents into an inefficient educational arms race. Our model emphasizes that lognormal wage distributions imply that agents' wages increase exponentially in the level of their skill as well as in the level of technology. In turn, this exponential relation between skills, technology, and wages pressures agents into an exhausting race for the tails of the economy's skill distribution. Moreover, technological advances and overinvestment in education increase GDP and inequality, while welfare may decline. In an alternative interpretation, our model studies firms that invest in artificial intelligence of their chatbots and AI agents. For a wide range of specifications, firms, just like humans, have an incentive to choose corner solutions where investment is limited only by borrowing constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19783v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wolfgang Kuhle</dc:creator>
    </item>
    <item>
      <title>Marriage and Divorce in Continuous Time</title>
      <link>https://arxiv.org/abs/2602.19798</link>
      <description>arXiv:2602.19798v1 Announce Type: new 
Abstract: This paper reformulates the Greenwood and Guner (2009) marriage and divorce model in continuous time using the HACT methods of Achdou et al. (2022). Replacing the AR(1) match quality process with an Ornstein-Uhlenbeck process yields a tridiagonal generator, reducing the computational complexity of both the value function and stationary distribution calculations from quadratic to linear in the number of grid points. The continuous-time model closely replicates the discrete-time equilibrium across all key outcomes, including the share of married households, the marriage rate, and the divorce rate, while achieving substantial gains in computation time and memory usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19798v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazuharu Yanagimoto</dc:creator>
    </item>
    <item>
      <title>Extension of the fusion power plant costing standard</title>
      <link>https://arxiv.org/abs/2602.19389</link>
      <description>arXiv:2602.19389v1 Announce Type: cross 
Abstract: This paper documents the work of the Clean Air Task Force (CATF) International Working Group (IWG) on Fusion Cost Analysis in 2024-2025, and the methodological extensions implemented in the CATF-supported branch of the pyFECONs fusion power-plant costing framework. Using the standards-aligned chart-of-accounts and physics-to-economics workflow established by ARPA-E. The IWG development reorganizes and deepens the framework around three architecture-defining cost-driver tracks for Magnetic Fusion Energy (MFE), Inertial Fusion Energy (IFE), and Magneto-Inertial Fusion Energy (MIFE). In particular, the generic driver placeholder in Account 22.1.3 is treated as a controlled swap-point and replaced by a full cost-account development for the dominant driver in each class, enabling auditable traceability from requirements and geometry to rolled-up plant costs. On top of this driver-centric foundation, we introduce a probabilistic costing layer that compounds materials price uncertainty, TRL-based maturity uncertainty, and learning-curve uncertainty into cost distributions. We then describe safety-informed costing that enumerates fusion-relevant hazards and maps mitigating systems, structures, and provisions into standardized accounts, together with scenario-parameterized regulatory and financial adders. Finally, we document expanded macroeconomic and finance parameterization and a value-metrics module that complements LCOE with investment and planning measures (NPV, IRR MIRR, revenue requirements, WACC-based annualization, and residual and follow-on value), all computed from the same COA-mapped outputs. Collectively, these additions convert a deterministic, standards-aligned costing backbone into an extensible analysis environment suitable for transparent sensitivity studies, uncertainty propagation, and safety- and finance-coupled interpretation of fusion pilot-plant and NOAK scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19389v1</guid>
      <category>physics.soc-ph</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Simon Woodruff, Alicia Durham, Alex Higginbottom, Chris Raastad</dc:creator>
    </item>
    <item>
      <title>Leap+Verify: Regime-Adaptive Speculative Weight Prediction for Accelerating Neural Network Training</title>
      <link>https://arxiv.org/abs/2602.19580</link>
      <description>arXiv:2602.19580v1 Announce Type: cross 
Abstract: We introduce Leap+Verify, a framework that applies speculative execution -- predicting future model weights and validating predictions before acceptance -- to accelerate neural network training. Inspired by speculative decoding in language model inference and by the Automatically Scalable Computation (ASC) architecture for program execution, Leap+Verify decomposes training into three dynamically detected regimes (chaotic, transition, stable) using activation-space cosine similarity as a real-time Lyapunov proxy signal. Within each regime, analytic weight predictors (momentum, linear, quadratic extrapolation) attempt to forecast model parameters K training steps ahead; predictions are accepted only when validated against a held-out loss criterion. We evaluate Leap+Verify on GPT-2 124M and Qwen 2.5-1.5B trained on WikiText-103 across five random seeds, sweeping prediction depth K in {5, 10, 25, 50, 75, 100}. Momentum-based prediction (Adam moment extrapolation) fails catastrophically at both scales, with predicted losses exceeding actuals by 100-10,000x -- a universal norm explosion in optimizer-state extrapolation. Finite-difference predictors (linear, quadratic) succeed where momentum fails: at 124M, they achieve 24% strict acceptance at K=5 in stable regimes; at 1.5B, they achieve 37% strict acceptance in transition regimes. The scale-dependent finding is in regime distribution: GPT-2 124M spends 34% of training in stable regime, while Qwen 1.5B spends 64% in chaotic regime and reaches stable in only 0-2 of 40 checkpoints. Larger models are more predictable when predictable, but less often predictable -- the practical bottleneck shifts from predictor accuracy to regime availability. Cross-seed results are highly consistent (less than 1% validation loss variance), and the three-regime framework produces identical phase boundaries (plus or minus 50 steps) across seeds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19580v1</guid>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.18739387</arxiv:DOI>
      <dc:creator>Jeremy McEntire</dc:creator>
    </item>
    <item>
      <title>A Mixed-Method Framework for Evaluating the Social Impact of Community Cooperation Projects in Developing Countries</title>
      <link>https://arxiv.org/abs/2602.20009</link>
      <description>arXiv:2602.20009v1 Announce Type: cross 
Abstract: Why do some community-cooperation projects catalyse participation through durable, resilient collaboration networks while others result in negligible impact and leave the local social fabric unchanged? We argue outcomes hinge on participation architecture: simple, visible routines -- onboarding help, templated tasks, lightweight contribution/benefit tracking -- that create easy ``entry portals'' and route work across clusters without heavy hierarchy. We introduce Project Intervention Response Analysis (PIRA), a mixed anthropological-network-analysis framework that compares observed community networks with counterfactual networks absent from project-induced ties. PIRA also adds a new egocentric metric to detect ``architectural alters'' -- latent facilitators and boundary spanners. We begin validating PIRA in a three-month field study in Pomerini, Tanzania, where NGOs coordinated citizens, associations, and specialists. Findings indicate that sociotechnical participation architectures -- not charismatic hubs -- underwrite durable coordination. PIRA offers a reusable method to link organizational design mechanisms to formal network signatures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20009v1</guid>
      <category>cs.SI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Giorgia Samp\`o, Saverio Giallorenzo, Zelda Alice Franceschi</dc:creator>
    </item>
    <item>
      <title>Behavioral Consequences of Sexual Orientation Disclosure in a Large-Scale Digital Environment</title>
      <link>https://arxiv.org/abs/2403.03649</link>
      <description>arXiv:2403.03649v4 Announce Type: replace 
Abstract: Many individuals hesitate to disclose their sexual orientation, anticipating that disclosure may alter how others respond to them. At the same time, concealing one's identity can entail substantial personal and social costs. Understanding how others react to sexual orientation disclosure is therefore central to evaluating the broader consequences of coming out. This paper uses an innovative data set from a popular online video game together with a natural experiment to causally identify behavioral responses to sexual minority disclosure. We exploit exogenous variation in the identity of a playable character to identify the effects of coming out on players' revealed preferences for that character across diverse regions globally. Our findings reveal a substantial and persistent negative impact of coming out.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03649v4</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enzo Brox, Riccardo Di Francesco</dc:creator>
    </item>
    <item>
      <title>Optimal longevity of a dynasty</title>
      <link>https://arxiv.org/abs/2409.15978</link>
      <description>arXiv:2409.15978v5 Announce Type: replace 
Abstract: Standard optimal growth models implicitly impose a ``perpetual existence'' constraint, which can ethically justify infinite misery in stagnant economies. This paper investigates the optimal longevity of a dynasty within a Critical-Level Utilitarian (CLU) framework. By treating the planning horizon as an endogenous choice variable, we establish a structural isomorphism between static population ethics and dynamic growth theory. Our analysis derives closed-form solutions for optimal consumption and longevity in a roundabout production economy. We show that under low productivity, a finite horizon is structurally optimal to avoid the creation of lives not worth living. This result suggests that the termination of a dynasty can be interpreted not as a failure of sustainability, but as an {altruistic termination} to prevent intergenerational suffering. We also highlight an ethical asymmetry: while a finite horizon is optimal for declining economies, growing economies under intergenerational equity demand the ultimate sacrifice from the current generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15978v5</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satoshi Nakano, Kazuhiko Nishimura</dc:creator>
    </item>
    <item>
      <title>Strategic Expression, Popularity Traps, and Welfare in Social Media</title>
      <link>https://arxiv.org/abs/2601.01370</link>
      <description>arXiv:2601.01370v2 Announce Type: replace 
Abstract: Social media platforms systematically reward popularity over authenticity, incentivizing users to strategically tailor their expression for attention. In this paper, we introduce (i) an attention-seeking model, distinct from canonical mechanisms of conformity, learning, persuasion, and (mis)information transmission in social networks literature, and (ii) the first utilitarian framework defined directly over observable social media platform metrics, filling a critical gap in the social media literature. In the model, agents hold fixed heterogeneous authentic opinions and derive (i) utility gains from the popularity of their own posts -- measured by likes received, and (ii) utility gains (losses) from exposure to content that aligns with (diverges from) their authentic opinion. Social media interaction acts as a state-dependent welfare amplifier: light topics generate Pareto improvements, whereas intense topics make everyone worse off in a polarized society (e.g., political debates during elections). Moreover, strategic expression amplifies social media polarization during polarized events while dampening it during unified events (e.g., national celebrations). Consequently, strategic distortions magnify welfare outcomes, expanding aggregate gains in light topics while exacerbating losses in intense, polarized ones. Counterintuitively, strategic agents often face a popularity trap: posting a more popular opinion is individually optimal, yet collective action by similar agents eliminates their authentic opinion from the platform, leaving them worse off than under the authentic-expression benchmark. Preference-based algorithms -- widely used by platforms -- or homophilic exposures discipline popularity-driven behavior, narrowing the popularity trap region and limiting its welfare effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01370v2</guid>
      <category>econ.GN</category>
      <category>cs.SI</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zafer Kanik, Zaruhi Hakobyan</dc:creator>
    </item>
    <item>
      <title>Decarbonization of financial markets: a mean-field game approach</title>
      <link>https://arxiv.org/abs/2301.09163</link>
      <description>arXiv:2301.09163v2 Announce Type: replace-cross 
Abstract: We develop a financial market model in which a large population of firms chooses dynamic emission strategies under climate transition risk, interacting with both environmentally concerned and neutral investors. Firms face a trade-off between financial returns and environmental performance, while their decisions are coupled through an equilibrium stochastic discount factor determined by investors' portfolio allocations. The framework is formulated as a mean-field game, for which we establish existence and uniqueness of a Nash equilibrium among firms. We propose a convergent numerical scheme to compute the equilibrium and use it to study how climate transition risk and green-minded investors affect decarbonization dynamics and asset prices. Our results show that uncertainty about future climate risks and policies increases aggregate emissions and widens valuation spreads between green and brown firms. Although environmentally concerned investors can partially offset these effects by raising the cost of capital for high-emission firms and incentivizing emission reductions, policy uncertainty weakens their impact. Even a large share of green-minded investors is insufficient to reverse emission growth when future climate policies are unclear, highlighting the crucial role of credible and predictable climate policy in enabling financial markets to support decarbonization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.09163v2</guid>
      <category>q-fin.MF</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre Lavigne, Peter Tankov</dc:creator>
    </item>
    <item>
      <title>When AI Democratizes Exploitation: LLM-Assisted Strategic Manipulation of Fair Division Algorithms</title>
      <link>https://arxiv.org/abs/2511.14722</link>
      <description>arXiv:2511.14722v2 Announce Type: replace-cross 
Abstract: Fair resource division algorithms, like those implemented in Spliddit platform, have traditionally been considered difficult for the end users to manipulate due to its complexities. This paper demonstrates how Large Language Models (LLMs) can dismantle these protective barriers by democratizing access to strategic expertise. Through empirical analysis of rent division scenarios on Spliddit algorithms, we show that users can obtain actionable manipulation strategies via simple conversational queries to AI assistants. We present four distinct manipulation scenarios: exclusionary collusion where majorities exploit minorities, defensive counterstrategies that backfire, benevolent subsidization of specific participants, and cost minimization coalitions. Our experiments reveal that LLMs can explain algorithmic mechanics, identify profitable deviations, and generate specific numerical inputs for coordinated preference misreporting--capabilities previously requiring deep technical knowledge. These findings extend algorithmic collective action theory from classification contexts to resource allocation scenarios, where coordinated preference manipulation replaces feature manipulation. The implications reach beyond rent division to any domain using algorithmic fairness mechanisms for resource division. While AI-enabled manipulation poses risks to system integrity, it also creates opportunities for preferential treatment of equity deserving groups. We argue that effective responses must combine algorithmic robustness, participatory design, and equitable access to AI capabilities, acknowledging that strategic sophistication is no longer a scarce resource.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14722v2</guid>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Priyanka Verma, Balagopal Unnikrishnan</dc:creator>
    </item>
  </channel>
</rss>
