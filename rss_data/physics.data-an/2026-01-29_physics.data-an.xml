<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 Jan 2026 05:01:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Evolving beyond collapse: An adaptive particle batch smoother for cryospheric data assimilation</title>
      <link>https://arxiv.org/abs/2601.20049</link>
      <description>arXiv:2601.20049v1 Announce Type: cross 
Abstract: We present a new adaptive particle-based data assimilation scheme for cryospheric applications that leverages promising developments in importance sampling. The proposed approach seeks to combine some of the advantages of two widely used classes of schemes: particle methods and iterative ensemble Kalman methods. Specifically, it extends the PBS that is commonly used in cryospheric data assimilation, with the AMIS algorithm. This adaptive formulation transforms the PBS into an iterative scheme with improved resilience against ensemble collapse and the ability to implement early-stopping strategies. As such, computational cost is automatically adapted to the complexity of the problem at hand, even down to the grid-cell and water year level in distributed multiyear simulations. In homage to the schemes that it builds on, we coin this new algorithm the Adaptive Particle Batch Smoother (AdaPBS) and we test it across a range of scenarios. First, we conducted an intercomparison of some of the most commonly used cryospheric data assimilation algorithms using MCMC simulation as a costly gold-standard benchmark in a simplified temperature index model assimilating snow depth observations. We further evaluated AdaPBS by assimilating snow depth observations from the ESMSnowMIP project at 6 different sites spanning 3 continents, using an ensemble of simulations generated with the more complex FSM2. Our results demonstrate that AdaPBS is a robust and reliable tool, outperforming or at least matching the performance of other commonly used algorithms and successfully handling complex cases with dense observational datasets. All experiments were carried out using the open-source MuSA toolbox, which now includes AdaPBS and MCMC among the growing list of available cryospheric data assimilation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20049v1</guid>
      <category>physics.geo-ph</category>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristoffer Aalstad, Esteban Alonso-Gonz\'alez, Norbert Pirk, Sebastian Westermann, Clarissa Willmes, Ruitang Yang</dc:creator>
    </item>
    <item>
      <title>Higher order moments of scalar within a plume in a turbulent boundary layer</title>
      <link>https://arxiv.org/abs/2601.20470</link>
      <description>arXiv:2601.20470v1 Announce Type: cross 
Abstract: This study examines the statistical nature of instantaneous scalar concentration in an elevated point-source plume (neutral or buoyant) dispersing within a turbulent boundary layer. Using high-frequency long-duration experimental measurements, we extensively validate the gamma distribution as the appropriate probability density function of concentration, particularly at large scalar magnitudes. The two-parameter gamma distribution is shown to capture the PDF at all locations across the plume. The classical similarity of the mean and root-mean-square (RMS) concentration, often expressed through a Gaussian form, is recovered through similarity of the scale and shape parameters of the gamma distribution. In addition, statistics of extreme events, such as the 99th percentile of the instantaneous concentration signal, are also well predicted, and their observed invariance near the plume centreline is reasoned. Further, similarity is observed for the third- and higher-order central moments and standardised central moments from the experimental data. The framework of the gamma distribution is also analytically extended to higher-order statistics. The experimental data are in good agreement with the predicted central moments up to the eighth order. The results emphasise the importance of achieving statistical convergence for the intermittent concentration signal, directly influenced by finite sampling times in a measurement. A secondary result is obtained for the ratio of plume half-widths based on the mean and the RMS concentration to be $1/\sqrt{2}$, consistent with experimental observations. The results establish the gamma distribution as a consistent and unified model for all scalar concentration statistics in elevated point source plumes within a turbulent boundary layer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20470v1</guid>
      <category>physics.flu-dyn</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miaoyan Pang, Krishna M Talluru, Kapil Chauhan</dc:creator>
    </item>
    <item>
      <title>Trigger Optimization and Event Classification for Dark Matter Searches in the CYGNO Experiment Using Machine Learning</title>
      <link>https://arxiv.org/abs/2601.20626</link>
      <description>arXiv:2601.20626v1 Announce Type: cross 
Abstract: The CYGNO experiment employs an optical-readout Time Projection Chamber (TPC) to search for rare low-energy interactions using finely resolved scintillation images. While the optical readout provides rich topological information, it produces large, sparse megapixel images that challenge real-time triggering, data reduction, and background discrimination.
  We summarize two complementary machine-learning approaches developed within CYGNO. First, we present a fast and fully unsupervised strategy for online data reduction based on reconstruction-based anomaly detection. A convolutional autoencoder trained exclusively on pedestal images (i.e. frames acquired with GEM amplification disabled) learns the detector noise morphology and highlights particle-induced structures through localized reconstruction residuals, from which compact Regions of Interest (ROIs) are extracted. On real prototype data, the selected configuration retains (93.0 +/- 0.2)% of reconstructed signal intensity while discarding (97.8 +/- 0.1)% of the image area, with ~25 ms per-frame inference time on a consumer GPU.
  Second, we report a weakly supervised application of the Classification Without Labels (CWoLa) framework to data acquired with an Americium--Beryllium neutron source. Using only mixed AmBe and standard datasets (no event-level labels), a convolutional classifier learns to identify nuclear-recoil-like topologies. The achieved performance approaches the theoretical limit imposed by the mixture composition and isolates a high-score population with compact, approximately circular morphologies consistent with nuclear recoils.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20626v1</guid>
      <category>physics.ins-det</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>F. D. Amaro, R. Antonietti, E. Baracchini, L. Benussi, C. Capoccia, M. Caponero, L. G. M. de Carvalho, G. Cavoto, I. A. Costa, A. Croce, M. D'Astolfo, G. D'Imperio, G. Dho, E. Di Marco, J. M. F. dos Santos, D. Fiorina, F. Iacoangeli, Z. Islam, E. Kemp, H. P. Lima Jr, G. Maccarrone, R. D. P. Mano, D. J. G. Marques, G. Mazzitelli, P. Meloni, A. Messina, C. M. B. Monteiro, R. A. Nobrega, G. M. Oppedisano, I. F. Pains, E. Paoletti, F. Petrucci, S. Piacentini, D. Pierluigi, D. Pinci, F. Renga, A. Russo, G. Saviano, P. A. O. C. Silva, N. J. Spooner, R. Tesauro, S. Tomassini, D. Tozzi</dc:creator>
    </item>
    <item>
      <title>Plotting correlated data</title>
      <link>https://arxiv.org/abs/2601.20805</link>
      <description>arXiv:2601.20805v1 Announce Type: cross 
Abstract: A very common task in data visualization is to plot many data points with some measured y-value as a function of fixed x-values. Uncertainties on the y-values are typically presented as vertical error bars that represent either a Frequentist confidence interval or Bayesian credible interval for each data point. Most of the time, these error bars represent a 68\% confidence/credibility level, which leads to the intuition that a model fits the data reasonably well if its prediction lies within the error bars of roughly two thirds of the data points. Unfortunately, this and other intuitions no longer work when the uncertainties of the data points are correlated. If the error bars only show the square root of diagonal elements of some covariance matrix with non-negligible off-diagonal elements, we simply do not have enough information in the plot to judge whether a drawn model line agrees well with the data or not. In this paper we will demonstrate this problem and discuss ways to add more information to the plots to make it easier to judge the agreement between the data and some model prediction in the plot, as well as glean some insight where the model might be deficient. This is done by explicitly showing the contribution of the first principal component of the uncertainties, and by displaying the conditional uncertainties of all data points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20805v1</guid>
      <category>stat.ME</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Koch</dc:creator>
    </item>
    <item>
      <title>Empirical Growing Networks vs Minimal Models: Evidence and Challenges from Software Heritage and APS Citation Datasets</title>
      <link>https://arxiv.org/abs/2501.10145</link>
      <description>arXiv:2501.10145v5 Announce Type: replace 
Abstract: We investigate the evolution rules and degree distribution properties of the Software Heritage dataset, a large-scale growing network linking software source-code versions from open-source communities. The network spans more than 40 years and includes about 6 billion nodes and edges. Our analysis relies on deterministic temporal and topological partitions of nodes and edges, which account for the multilayer and partially timestamped structure of the main graph. We derive a temporal graph that reveals a mesoscale structure and enables the study of edge dynamics--creation, inheritance, and aging--together with comparisons to minimal models using degree distributions and histograms of edge timestamp differences. The temporal graph also exposes regime shifts that correlate with changes in developer practices, as reflected in the average number of edges per new node. We estimate scaling exponents under the scale-free hypothesis and highlight the sensitivity of the estimation method used to both regime shifts and outliers, while showing that partitioning improves regularity and helps disentangle these effects. We extend the analysis to the APS citation network, which also exhibits a major regime shift, with an accelerated growth regime becoming dominant after 1985. Although both datasets are a priori good candidates for advanced quantitative analysis, our results illustrate how structural and dynamical transitions hamper our ability to draw firm conclusions about the existence and observability of a scale-free regime in these empirical networks. These findings underscore the need for refined tools and models to study transient growth regimes, to extend current frameworks toward minimal causal growth models, and to enable robust comparisons between empirical growing networks and minimal models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10145v5</guid>
      <category>physics.data-an</category>
      <category>physics.comp-ph</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 29 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guillaume Rousseau</dc:creator>
    </item>
  </channel>
</rss>
