<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 12 Aug 2025 04:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Asymptotic Statistical Theory for the Samples Problems using the Functional Empirical Process, revisited I</title>
      <link>https://arxiv.org/abs/2508.06581</link>
      <description>arXiv:2508.06581v1 Announce Type: new 
Abstract: In this paper we study the asymptotic theory for samples problem based on the functional empirical process (fep), this new method is called general samples problem. We suggest this method to develop the full theory of estimation of means, variances, ratios of variances and difference of means for independent samples. We compare the results of our new method to the Gaussian method using simulated and real data. The obtained results are almost equivalent to those in the Gaussian case for samples's size equal to $10$. It has been prove that the estimation of the means difference is very precise regardless of the equality or inequality of variances for greater sizes of sample. This method is recommended when the sizes of samples is around or greater that $15$ and it requires the finiteness of the fourth order moment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06581v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Abdoulaye Camara, Adja Mbarka Fall, Moumouni Diallo, Gane Samb Lo</dc:creator>
    </item>
    <item>
      <title>A test statistic, $h^*$, for outlier analysis</title>
      <link>https://arxiv.org/abs/2508.06792</link>
      <description>arXiv:2508.06792v1 Announce Type: new 
Abstract: Outlier analysis is a critical tool across diverse domains, from clinical decision-making to cybersecurity and talent identification. Traditional statistical outlier detection methods, such as Grubb's test and Dixon's Q, are predicated on the assumption of normality and often fail to reckon the meaningfulness of exceptional values within non-normal datasets. In this paper, we introduce the h* statistic, a novel parametric, frequentist approach for evaluating global outliers without the normality assumption. Unlike conventional techniques that primarily remove outliers to preserve statistical `integrity,' h* assesses the distinctiveness as phenomena worthy of investigation by quantifying a data point's extremity relative to its group as a measure of statistical significance analogous to the role of Student's t in comparing means. We detail the mathematical formulation of h* with tabulated confidence intervals of significance levels and extensions to Bayesian inference and paired analysis. The capacity of h* to discern between stable extraordinary deviations and values that merely appear extreme under conventional criteria is demonstrated using empirical data from a mood intervention study. A generalisation of h* is subsequently proposed, with individual weights assigned to differences for nuanced contextual description, and a variable sensitivity exponent for objective inference optimisation and subjective inference specification. The physical significance of an h*-recognised outlier is linked to the signature of unique occurrences. Our findings suggest that h* offers a robust alternative for outlier evaluation, enriching the analytical repertoire for researchers and practitioners by foregrounding the interpretative value of outliers within complex, real-world datasets. This paper is also a statement against the dominance of normality in celebration of the luminary and the lunatic alike.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06792v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Johan F. Hoorn, Johnny K. W. Ho</dc:creator>
    </item>
    <item>
      <title>Causal Inference Under Network Interference</title>
      <link>https://arxiv.org/abs/2508.06808</link>
      <description>arXiv:2508.06808v1 Announce Type: new 
Abstract: We review and conceptualize recent advances in causal inference under network interference, drawing on a complex and diverse body of work that ranges from causal inference, statistical network analysis, economics, the health sciences, and the social sciences. Network interference arises in connected populations when the treatment assignments of units affect the outcomes of other units. Examples include economic, financial, and public health interventions with spillover in connected populations, reinforcement learning in connected populations, and advertising on social media. We discuss the design of experiments, targets of causal inference, interpretations and characterizations of causal effects, interference tests, and design- and model-based estimators of causal effects under network interference. We then contrast inferential frameworks based on fixed networks (finite population inference) and random networks (super population inference) and the generalizability afforded by them. We demonstrate that expected outcomes can depend on the network structure (e.g., the absence or presence of superstars and communities) and could be different if another network were observed, highlighting the need to understand how network structure affects causal conclusions. We conclude with a selection of open problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06808v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhankar Bhadra, Michael Schweinberger</dc:creator>
    </item>
    <item>
      <title>Probabilistic combination forecasts based on particle filtering: predictive prior</title>
      <link>https://arxiv.org/abs/2508.07136</link>
      <description>arXiv:2508.07136v1 Announce Type: new 
Abstract: We develop a Bayesian combination forecast framework that incorporates forward-looking signals as a predictive prior into the estimation of time-varying combination weights, enabling the weights to reflect both historical data and forward-looking information from individual models. Model diversity is employed as a feature to represent forward-looking feedback, giving rise to the proposed method termed diversity time-varying weights (DTVW). The weights are estimated via particle filtering within a nonlinear state space. This method extends the time-varying weights (TVW) by integrating diversity-driven predictive priors, which penalize redundancy and encourage informative contributions across individual models. Simulation studies demonstrate improved forecast accuracy across both simple complete model set and complex misspecified one, with these gains stemming from the framework's ability to dynamically assess the relative performance of individual models and allocate weights accordingly. Empirically, we apply the method to multi-step ahead oil price forecast and bi-variate forecast of U.S. inflation and GDP growth. In both cases, the proposed method DTVW outperforms traditional combination forecasts, such as the Equal Weighting, Bayesian Model Averaging, TVW, etc. Additionally, using model diversity as a predictive prior provides diagnostic insights into model incompleteness and forecast uncertainty in evolving complex economic environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07136v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaorui Luo, Yanfei Kang, Xue Luo</dc:creator>
    </item>
    <item>
      <title>Is Repeated Bayesian Interim Analysis Consequence-Free?</title>
      <link>https://arxiv.org/abs/2508.07403</link>
      <description>arXiv:2508.07403v1 Announce Type: new 
Abstract: Interim analyses are commonly used in clinical trials to enable early stopping for efficacy, futility, or safety. While their impact on frequentist operating characteristics is well studied and broadly understood, the effect of repeated Bayesian interim analyses - when conducted without appropriate multiplicity adjustment - remains an area of active debate. In this article, we provide both theoretical justification and numerical evidence illustrating how such analyses affect key inferential properties, including bias, mean squared error, the coverage probability of posterior credible intervals, false discovery rate, familywise error rate, and power. Our findings demonstrate that Bayesian interim analyses can significantly alter a trial's operating characteristics, even when the prior used for Bayesian inference is correctly specified and aligned with the data-generating process. Extensive simulation studies, covering a variety of endpoints, trial designs (single-arm and two-arm randomized controlled trials), and scenarios with both correctly specified and misspecified priors, support theoretical insights. Collectively, these results underscore the necessity of appropriate adjustment, thoughtful prior specification, and comprehensive evaluation to ensure valid and reliable inference in Bayesian adaptive trial designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07403v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Suyu Liu, Beibei Guo, Laura Thompson, Lei Nie, Ying Yuan</dc:creator>
    </item>
    <item>
      <title>Statistical Theory of Multi-stage Newton Iteration Algorithm for Online Continual Learning</title>
      <link>https://arxiv.org/abs/2508.07419</link>
      <description>arXiv:2508.07419v1 Announce Type: new 
Abstract: We focus on the critical challenge of handling non-stationary data streams in online continual learning environments, where constrained storage capacity prevents complete retention of historical data, leading to catastrophic forgetting during sequential task training. To more effectively analyze and address the problem of catastrophic forgetting in continual learning, we propose a novel continual learning framework from a statistical perspective. Our approach incorporates random effects across all model parameters and allows the dimension of parameters to diverge to infinity, offering a general formulation for continual learning problems. To efficiently process streaming data, we develop a Multi-step Newton Iteration algorithm that significantly reduces computational costs in certain scenarios by alleviating the burden of matrix inversion. Theoretically, we derive the asymptotic normality of the estimator, enabling subsequent statistical inference. Comprehensive validation through synthetic data experiments and two real datasets analyses demonstrates the effectiveness of our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07419v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinjia Lu, Chuhan Wang, Qian Zhao, Lixing Zhu, Xuehu Zhu</dc:creator>
    </item>
    <item>
      <title>High-dimensional Longitudinal Inference via a De-sparsified Dantzig-Selector</title>
      <link>https://arxiv.org/abs/2508.07498</link>
      <description>arXiv:2508.07498v1 Announce Type: new 
Abstract: In this paper, we consider statistical inference with generalized linear models in high dimensions under a longitudinal clustered data framework. Specifically, we propose a de-sparsified version of an initial Dantzig-type regularized estimator in regression settings and provide theoretical justification for both linear and generalized linear models. We present extensive numerical simulations demonstrating the effectiveness of our method for continuous and binary data. For continuous outcomes under linear models, we show that our estimator asymptotically attains an appropriate efficiency bound when the correlation structure is correctly specified. We conclude with an application of our method to a well-established genetics dataset, with bacterial riboflavin production as the outcome of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07498v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Huey, Rajarshi Mukherjee</dc:creator>
    </item>
    <item>
      <title>Soil Texture Prediction with Bayesian Generalized Additive Models for Spatial Compositional Data</title>
      <link>https://arxiv.org/abs/2508.07708</link>
      <description>arXiv:2508.07708v1 Announce Type: new 
Abstract: Compositional data (CoDa) plays an important role in many fields such as ecology, geology, or biology. The most widely used modeling approaches are based on the Dirichlet and the logistic-normal formulation under Aitchison geometry. Recent developments in the mathematical field on the simplex geometry allow to express the regression model in terms of coordinates and estimate its coefficients. Once the model is projected in the real space, we can employ a multivariate Gaussian regression to deal with it. However, most existing methods focus on linear models, and there is a lack of flexible alternatives such as additive or spatial models, especially within a Bayesian framework and with practical implementation details.
  In this work, we present a geoadditive regression model for CoDa from a Bayesian perspective using the brms package in R. The model applies the isometric log-ratio (ilr) transformation and penalized splines to incorporate nonlinear effects. We also propose two new Bayesian goodness-of-fit measures for CoDa regression: BR-CoDa-$R^2$ and BM-CoDa-$R^2$, extending the Bayesian $R^2$ to the compositional setting. These measures, alongside WAIC, support model selection and evaluation. The methodology is validated through simulation studies and applied to predict soil texture composition in the Basque Country. Results demonstrate good performance, interpretable spatial patterns, and reliable quantification of explained variability in compositional outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07708v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joaqu\'in Mart\'inez-Minaya, Lore Zumeta-Olaskoaga, Dae-Jin Lee</dc:creator>
    </item>
    <item>
      <title>Modelling phenology using ordered categorical generalized additive models</title>
      <link>https://arxiv.org/abs/2508.07789</link>
      <description>arXiv:2508.07789v1 Announce Type: new 
Abstract: One form of data collected in ecology is phenological, describing the timing of life stages. It can be tempting to analyze such data using a continuous distribution or to model individual transitions via probit/logit models. Such simplifications can lead to incorrect inference in various ways, all of which stem from ignoring the natural structure of the data. This paper presents a flexible approach to modelling ordered categorical data using the popular R package `mgcv`. An example analysis of saxifrage phenology in Greenland including useful plots, model checking and derived quantities is included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07789v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>David L Miller</dc:creator>
    </item>
    <item>
      <title>Adding structure to generalized additive models, with applications in ecology</title>
      <link>https://arxiv.org/abs/2508.07915</link>
      <description>arXiv:2508.07915v1 Announce Type: new 
Abstract: Generalized additive models (GAMs) that connect a set of scalar covariates that map 1-1 to a response variable are commonly employed in ecological and other scientific disciplines. However, covariates are often inherently non-scalar, taking on multiple values for each observation of the response. They can sometimes have a temporal structure, e.g., a time series of temperature and precipitation measurements, or a spatial structure, e.g., multiple soil pH measurements made at nearby locations. While aggregating or selectively summarizing such covariates to yield a scalar covariate allows the use of standard GAM fitting procedures, exactly how to do so can be problematic, e.g., using a mean or median value for some subsequence of a time series, and information is necessarily lost as well. On the other hand naively including all $p$ components of a vector-valued covariate as $p$ separate covariates, say, without recognizing the structure, can lead to problems of multicollinearity, data sets that are excessively wide given the sample size, and difficulty extracting the primary signal provided by the covariate. In this paper we introduce three useful extensions to GAMs that provide means of efficiently and effectively handling vector-valued covariates without requiring one to choose problematic aggregations or selective summarizations. These extensions are varying-coefficient, scalar-on-function and distributed lag models. While these models have existed for some time they remain relatively underused in ecology. This article aims to show when these models can be useful and how to fit them with the popular R package \texttt{mgcv}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07915v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>David L Miller, Ken Newman, Thomas Cornulier</dc:creator>
    </item>
    <item>
      <title>Straightforward Bayesian A/B testing with Dirichlet posteriors</title>
      <link>https://arxiv.org/abs/2508.08077</link>
      <description>arXiv:2508.08077v1 Announce Type: new 
Abstract: Bayesian A/B testing investigates metric changes using the joint posterior distribution of two (or more) experimentally-derived datasets. The construction of said joint posterior is often a time-consuming process requiring specialized knowledge and domain expertise. In businesses that perform tens to hundreds of A/B tests per month it is important to have a robust analysis pipeline that can handle the variety of experiments performed on a modern web platform; requiring a domain expert to select appropriate prior and likelihood distributions for each experiment simply does not scale. In this work, we highlight a solution to this problem using a generalized approximation of the true joint posterior using a Dirichlet-Categorical model. While a manually-constructed, expert-tuned model for every dataset is preferable, the Dirichlet-Categorical approximation performs sufficiently well in both simulations and real-world scenarios to be internally used as the standard analysis method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08077v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dustin Hayden, Thomas Armitage</dc:creator>
    </item>
    <item>
      <title>Variance estimation for weighted average treatment effects</title>
      <link>https://arxiv.org/abs/2508.08167</link>
      <description>arXiv:2508.08167v1 Announce Type: new 
Abstract: Common variance estimation methods for weighted average treatment effects (WATEs) in observational studies include nonparametric bootstrap and model-based, closed-form sandwich variance estimation. However, the computational cost of bootstrap increases with the size of the data at hand. Besides, some replicates may exhibit random violations of the positivity assumption even when the original data do not. Sandwich variance estimation relies on regularity conditions that may be structurally violated. Moreover, the sandwich variance estimation is model-dependent on the propensity score model, the outcome model, or both; thus it does not have a unified closed-form expression. Recent studies have explored the use of wild bootstrap to estimate the variance of the average treatment effect on the treated (ATT). This technique adopts a one-dimensional, nonparametric, and computationally efficient resampling strategy. In this article, we propose a "post-weighting" bootstrap approach as an alternative to the conventional bootstrap, which helps avoid random positivity violations in replicates and improves computational efficiency. We also generalize the wild bootstrap algorithm from ATT to the broader class of WATEs by providing new justification for correctly accounting for sampling variability from multiple sources under different weighting functions. We evaluate the performance of all four methods through extensive simulation studies and demonstrate their application using data from the National Health and Nutrition Examination Survey (NHANES). Our findings offer several practical recommendations for the variance estimation of WATE estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08167v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huiyue Li, Yi Liu, Yunji Zhou, Jiajun Liu, Dezhao Fu, Roland A. Matsouaka</dc:creator>
    </item>
    <item>
      <title>LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference</title>
      <link>https://arxiv.org/abs/2508.07221</link>
      <description>arXiv:2508.07221v1 Announce Type: cross 
Abstract: Estimating individualized treatment effects from observational data presents a persistent challenge due to unmeasured confounding and structural bias. Causal Machine Learning (causal ML) methods, such as causal trees and doubly robust estimators, provide tools for estimating conditional average treatment effects. These methods have limited effectiveness in complex real-world environments due to the presence of latent confounders or those described in unstructured formats. Moreover, reliance on domain experts for confounder identification and rule interpretation introduces high annotation cost and scalability concerns. In this work, we proposed Large Language Model-based agents for automated confounder discovery and subgroup analysis that integrate agents into the causal ML pipeline to simulate domain expertise. Our framework systematically performs subgroup identification and confounding structure discovery by leveraging the reasoning capabilities of LLM-based agents, which reduces human dependency while preserving interpretability. Experiments on real-world medical datasets show that our proposed approach enhances treatment effect estimation robustness by narrowing confidence intervals and uncovering unrecognized confounding biases. Our findings suggest that LLM-based agents offer a promising path toward scalable, trustworthy, and semantically aware causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07221v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Po-Han Lee, Yu-Cheng Lin, Chan-Tung Ku, Chan Hsu, Pei-Cing Huang, Ping-Hsun Wu, Yihuang Kang</dc:creator>
    </item>
    <item>
      <title>Asymptotic Consistency and Generalization in Hybrid Models of Regularized Selection and Nonlinear Learning</title>
      <link>https://arxiv.org/abs/2508.07754</link>
      <description>arXiv:2508.07754v1 Announce Type: cross 
Abstract: This study explores how different types of supervised models perform in the task of predicting and selecting relevant variables in high-dimensional contexts, especially when the data is very noisy. We analyzed three approaches: regularized models (such as Lasso, Ridge, and Elastic Net), black-box models (such as Random Forest, XGBoost, LightGBM, CatBoost, and H2O GBM), and hybrid models that combine both approaches: regularization with nonlinear algorithms. Based on simulations inspired by the Friedman equation, we evaluated 23 models using three complementary metrics: RMSE, Jaccard index, and recall rate. The results reveal that, although black-box models excel in predictive accuracy, they lack interpretability and simplicity, essential factors in many real-world contexts. Regularized models, on the other hand, proved to be more sensitive to an excess of irrelevant variables. In this scenario, hybrid models stood out for their balance: they maintain good predictive performance, identify relevant variables more consistently, and offer greater robustness, especially as the sample size increases. Therefore, we recommend using this hybrid framework in market applications, where it is essential that the results make sense in a practical context and support decisions with confidence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07754v1</guid>
      <category>stat.OT</category>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luciano Ribeiro Galv\~ao, Rafael de Andrade Mora</dc:creator>
    </item>
    <item>
      <title>Meta Off-Policy Estimation</title>
      <link>https://arxiv.org/abs/2508.07914</link>
      <description>arXiv:2508.07914v1 Announce Type: cross 
Abstract: Off-policy estimation (OPE) methods enable unbiased offline evaluation of recommender systems, directly estimating the online reward some target policy would have obtained, from offline data and with statistical guarantees. The theoretical elegance of the framework combined with practical successes have led to a surge of interest, with many competing estimators now available to practitioners and researchers. Among these, Doubly Robust methods provide a prominent strategy to combine value- and policy-based estimators.
  In this work, we take an alternative perspective to combine a set of OPE estimators and their associated confidence intervals into a single, more accurate estimate. Our approach leverages a correlated fixed-effects meta-analysis framework, explicitly accounting for dependencies among estimators that arise due to shared data. This yields a best linear unbiased estimate (BLUE) of the target policy's value, along with an appropriately conservative confidence interval that reflects inter-estimator correlation. We validate our method on both simulated and real-world data, demonstrating improved statistical efficiency over existing individual estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07914v1</guid>
      <category>stat.ML</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3705328.3759308</arxiv:DOI>
      <dc:creator>Olivier Jeunen</dc:creator>
    </item>
    <item>
      <title>Likelihood Ratio Tests by Kernel Gaussian Embedding</title>
      <link>https://arxiv.org/abs/2508.07982</link>
      <description>arXiv:2508.07982v1 Announce Type: cross 
Abstract: We propose a novel kernel-based nonparametric two-sample test, employing the combined use of kernel mean and kernel covariance embedding. Our test builds on recent results showing how such combined embeddings map distinct probability measures to mutually singular Gaussian measures on the kernel's RKHS. Leveraging this result, we construct a test statistic based on the relative entropy between the Gaussian embeddings, i.e.\ the likelihood ratio. The likelihood ratio is specifically tailored to detect equality versus singularity of two Gaussians, and satisfies a ``$0/\infty$" law, in that it vanishes under the null and diverges under the alternative. To implement the test in finite samples, we introduce a regularised version, calibrated by way of permutation. We prove consistency, establish uniform power guarantees under mild conditions, and discuss how our framework unifies and extends prior approaches based on spectrally regularized MMD. Empirical results on synthetic and real data demonstrate remarkable gains in power compared to state-of-the-art methods, particularly in high-dimensional and weak-signal regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07982v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leonardo V. Santoro, Victor M. Panaretos</dc:creator>
    </item>
    <item>
      <title>Statistical inference of random graphs with a surrogate likelihood function</title>
      <link>https://arxiv.org/abs/2207.01702</link>
      <description>arXiv:2207.01702v3 Announce Type: replace 
Abstract: Spectral estimators have been broadly applied to statistical network analysis, but they do not incorporate the likelihood information of the network sampling model. This paper proposes a novel surrogate likelihood function for statistical inference of a class of popular network models referred to as random dot product graphs. In contrast to the structurally complicated exact likelihood function, the surrogate likelihood function has a separable structure and is log-concave yet approximates the exact likelihood function well. From the frequentist perspective, we study the maximum surrogate likelihood estimator and establish the accompanying theory. We show its existence, uniqueness, large sample properties, and that it improves upon the baseline spectral estimator with a smaller sum of squared errors. Furthermore, we derive the second-order bias of the proposed estimator and gain insight into why it outperforms some of the existing estimators. A computationally convenient stochastic gradient descent algorithm is designed to find the maximum surrogate likelihood estimator in practice. From the Bayesian perspective, we establish the Bernstein--von Mises theorem of the posterior distribution with the surrogate likelihood function and show that the resulting credible sets have the correct frequentist coverage. The empirical performance of the proposed surrogate-likelihood-based methods is validated through the analyses of simulation examples and a real-world Wikipedia graph dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.01702v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dingbo Wu, Fangzheng Xie</dc:creator>
    </item>
    <item>
      <title>A Meta-Learning Method for Estimation of Causal Excursion Effects to Assess Time-Varying Moderation</title>
      <link>https://arxiv.org/abs/2306.16297</link>
      <description>arXiv:2306.16297v4 Announce Type: replace 
Abstract: Advances in wearable technologies and health interventions delivered by smartphones have greatly increased the accessibility of mobile health (mHealth) interventions. Micro-randomized trials (MRTs) are designed to assess the effectiveness of the mHealth intervention and introduce a novel class of causal estimands called "causal excursion effects." These estimands enable the evaluation of how intervention effects change over time and are influenced by individual characteristics or context. Existing methods for analyzing causal excursion effects assume known randomization probabilities, complete observations, and a linear nuisance function with prespecified features of the high dimensional observed history. However, in complex mobile systems, these assumptions often fall short: randomization probabilities can be uncertain, observations may be incomplete, and the granularity of mHealth data makes linear modeling difficult. To address this issue, we propose a flexible and doubly robust inferential procedure, called "DR-WCLS," for estimating causal excursion effects from a meta-learner perspective. We present the bidirectional asymptotic properties of the proposed estimators and compare them with existing methods both theoretically and through extensive simulations. The results show a consistent and more efficient estimate, even with missing observations or uncertain treatment randomization probabilities. Finally, the practical utility of the proposed methods is demonstrated by analyzing data from a multiinstitution cohort of first-year medical residents in the United States (NeCamp et al., 2020).</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16297v4</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jieru Shi, Walter Dempsey</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis for Unmeasured Confounding in Medical Product Development and Evaluation Using Real World Evidence</title>
      <link>https://arxiv.org/abs/2307.07442</link>
      <description>arXiv:2307.07442v2 Announce Type: replace 
Abstract: The American Statistical Association Biopharmaceutical Section (ASA BIOP) scientific working group on real-world evidence (RWE) has been making continuous, extended efforts towards a goal of supporting and advancing regulatory science with respect to clinical studies intended to use real-world data for evidence generation for the purpose of medical product development and evaluation (i.e., RWE studies). In 2023, the working group published a manuscript delineating challenges and opportunities in constructing estimands for RWE studies following the framework in ICH E9(R1) guidance on estimand and sensitivity analysis. As a follow-up task, we describe the other issue, sensitivity analysis. Although the FDA's definition of RWE studies includes randomized trials using RWD as a primary source of evidence generation such as pragmatic trials, here we focus on non-randomized RWE studies and the issue of unmeasured confounding which is a major source of bias for most RWE studies. We review the availability and applicability of sensitivity analysis methods for different types of unmeasured confounding. We also provide some practical and regulatory considerations on using sensitivity analysis for such RWE studies. Then, we use a plasmode case study to demonstrate realistic ways to interpret sensitivity analysis results in supporting regulatory decision-making. We conclude with a brief discussion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.07442v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/19466315.2025.2546360</arxiv:DOI>
      <dc:creator>Yixin Fang, Pallavi Mishra-Kalyani, Xiang Zhang, Susan Gruber, Shu Yang, Peng Ding, Mingyang Shan, Joo-Yeon Lee, Mark van der Laan, Douglas Faries, Hana Lee</dc:creator>
    </item>
    <item>
      <title>Generalized Universal Inference on Risk Minimizers</title>
      <link>https://arxiv.org/abs/2402.00202</link>
      <description>arXiv:2402.00202v3 Announce Type: replace 
Abstract: A common goal in statistics and machine learning is estimation of unknowns. Point estimates alone are of little value without an accompanying measure of uncertainty, but traditional uncertainty quantification methods, such as confidence sets and p-values, often require distributional or structural assumptions that may not be justified in modern applications. The present paper considers a very common case in machine learning, where the quantity of interest is the minimizer of a given risk (expected loss) function. We propose a generalization of universal inference specifically designed for inference on risk minimizers. Notably, our generalized universal inference attains finite-sample frequentist validity guarantees under a condition common in the statistical learning literature. One version of our procedure is also anytime-valid, i.e., it maintains the finite-sample validity properties regardless of the stopping rule used for the data collection process. Practical use of our proposal requires tuning, and we offer a data-driven procedure with strong empirical performance across a broad range of challenging statistical and machine learning examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00202v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neil Dey, Ryan Martin, Jonathan P. Williams</dc:creator>
    </item>
    <item>
      <title>Efficient nonparametric estimators of discrimination measures with censored survival data</title>
      <link>https://arxiv.org/abs/2409.05632</link>
      <description>arXiv:2409.05632v3 Announce Type: replace 
Abstract: Discrimination measures such as the concordance index and the cumulative-dynamic time-dependent area under the ROC-curve (AUC) are widely used in the medical literature for evaluating the predictive accuracy of a scoring rule which relates a set of prognostic markers to the risk of experiencing a particular event. Often the scoring rule being evaluated in terms of discriminatory ability is the linear predictor of a survival regression model such as the Cox proportional hazards model. This has the undesirable feature that the scoring rule depends on the censoring distribution when the model is misspecified. In this work we focus on linear scoring rules where the coefficient vector is a nonparametric estimand defined in the setting where there is no censoring. We propose so-called debiased estimators of the aforementioned discrimination measures for this class of scoring rules. The proposed estimators make efficient use of the data and minimize bias by allowing for the use of data-adaptive methods for model fitting. Moreover, the estimators do not rely on correct specification of the censoring model to produce consistent estimation. We compare the estimators to existing methods in a simulation study, and we illustrate the method by an application to a brain cancer study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05632v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssb/qkaf044</arxiv:DOI>
      <arxiv:journal_reference>Journal of the Royal Statistical Society Series B: Statistical Methodology, 2025, 00, 1-23</arxiv:journal_reference>
      <dc:creator>Marie Skov Breum, Torben Martinussen</dc:creator>
    </item>
    <item>
      <title>Randomization Inference for Before-and-After Studies with Multiple Units: An Application to a Criminal Procedure Reform in Uruguay</title>
      <link>https://arxiv.org/abs/2410.15477</link>
      <description>arXiv:2410.15477v2 Announce Type: replace 
Abstract: Learning about the immediate causal effects of large-scale policy interventions poses a significant challenge for quasi-experimental methods that rely on long-term trends or parametric modeling assumptions. As an alternative, we develop a randomization inference framework for before-and-after studies with multiple units, designed specifically for short-term causal inference and allowing for general assignment mechanisms. The method provides finite-sample-valid statistical inferences without relying on parametric time series models or extrapolation. We demonstrate its utility by analyzing a major criminal justice reform in Uruguay that switched from an inquisitorial to an adversarial system in November 2017. Our method relies on the key assumption of no local time trends near the policy adoption time, which is supported by several falsification tests in our empirical study. We find a statistically significant short-term causal effect: an increase of approximately 25 daily police reports (an 8% rise) in the first week of the new justice system. Our randomization inference framework provides a robust and flexible methodology for evaluating policy adoptions in before-and-after studies with multiple units.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15477v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Carlos Diaz, Rocio Titiunik</dc:creator>
    </item>
    <item>
      <title>Comparing multilevel and fixed effect approaches in the generalized linear model setting</title>
      <link>https://arxiv.org/abs/2411.01723</link>
      <description>arXiv:2411.01723v2 Announce Type: replace 
Abstract: We extend prior work comparing linear multilevel models (MLM) and fixed effect (FE) models to the generalized linear model (GLM) setting, where the coefficient on a treatment variable is of primary interest. This leads to three insights. (i) First, as in the linear setting, MLM can be thought of as a regularized form of FE (RegFE). This explains why group-level confounding can greatly bias MLM's treatment coefficient estimates. However, unlike the linear setting, there is not an exact equivalence between MLM and RegFE in GLMs. (ii) Second, we study a generalization of "bias-corrected MLM" (bcMLM) to the GLM setting, and a corresponding "bias-corrected RegFE" (bcRegFE). None of FE, bcMLM, or bcRegFE entirely solve MLM's bias problem in GLMs, but bcMLM and bcRegFE tend to show less bias than does FE. (iii) Third, as in the linear setting, MLM's default standard errors can misspecify the true intragroup dependence structure in the GLM setting, which can yield downwardly biased standard errors. A cluster bootstrap is a more agnostic alternative. We also consider a cluster-robust standard error for (bc)RegFE. Ultimately, for non-linear GLMs, we recommend bcMLM for estimating the treatment coefficient, and a cluster bootstrap for standard errors and confidence intervals. If a bootstrap is not computationally feasible, then we recommend bcRegFE with cluster-robust standard errors, or FE with cluster-robust standard errors when group sizes are larger.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01723v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>He Bai, Asa Ferguson, Leonard Wainstein, Jonathan Wells</dc:creator>
    </item>
    <item>
      <title>Stability and performance guarantees for misspecified multivariate score-driven filters</title>
      <link>https://arxiv.org/abs/2502.05021</link>
      <description>arXiv:2502.05021v3 Announce Type: replace 
Abstract: Can stochastic gradient methods track a moving target? We address the problem of tracking multivariate time-varying parameters under noisy observations and potential model misspecification. Specifically, we examine implicit and explicit score-driven (ISD and ESD) filters, which update parameter predictions using the gradient of the logarithmic postulated observation density (commonly referred to as the score). For both filter types, we derive novel sufficient conditions that ensure the exponential stability of the filtered parameter path and the existence of a finite mean squared error (MSE) bound relative to the pseudo-true parameter path. Our (non-)asymptotic MSE bounds rely on mild moment conditions on the data-generating process, while our stability results are agnostic about the true process. For the ISD filter, concavity of the postulated log density combined with simple parameter restrictions is sufficient to guarantee stability. In contrast, the ESD filter additionally requires the score to be Lipschitz continuous and the learning rate to be sufficiently small. We validate our theoretical findings through simulation studies, showing that ISD filters outperform ESD filters in terms of accuracy and stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05021v3</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Donker van Heel, Rutger-Jan Lange, Bram van Os, Dick van Dijk</dc:creator>
    </item>
    <item>
      <title>A Bayesian Multivariate Spatial Point Pattern Model: Application to Oral Microbiome FISH Image Data</title>
      <link>https://arxiv.org/abs/2502.10513</link>
      <description>arXiv:2502.10513v2 Announce Type: replace 
Abstract: Advances in cellular imaging technologies, especially those based on fluorescence in situ hybridization (FISH) now allow detailed visualization of the spatial organization of human or bacterial cells. Quantifying this spatial organization is crucial for understanding the function of multicellular tissues or biofilms, with implications for human health and disease. To address the need for better methods to achieve such quantification, we propose a flexible multivariate point process model that characterizes and estimates complex spatial interactions among multiple cell types. The proposed Bayesian framework is appealing due to its unified estimation process and the ability to directly quantify uncertainty in key estimates of interest, such as those of inter-type correlation and the proportion of variance due to inter-type relationships. To ensure stable and interpretable estimation, we consider shrinkage priors for coefficients associated with latent processes. Model selection and comparison are conducted by using a deviance information criterion designed for models with latent variables, effectively balancing the risk of overfitting with that of oversimplifying key quantities. Furthermore, we develop a hierarchical modeling approach to integrate multiple image-specific estimates from a given subject, allowing inference at both the global and subject-specific levels. We apply the proposed method to microbial biofilm image data from the human tongue dorsum and find that specific taxon pairs, such as Streptococcus mitis-Streptococcus salivarius and Streptococcus mitis-Veillonella, exhibit strong positive spatial correlations, while others, such as Actinomyces-Rothia, show slight negative correlations. For most of the taxa, a substantial portion of spatial variance can be attributed to inter-taxon relationships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10513v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyu Ha Lee, Brent A. Coull, Suman Majumder, Patrick J. La Riviere, Jessica L. Mark Welch, Jacqueline R. Starr</dc:creator>
    </item>
    <item>
      <title>Likelihood-based Modeling of Covariate-Specific Time-Dependent ROC Curves</title>
      <link>https://arxiv.org/abs/2502.20892</link>
      <description>arXiv:2502.20892v2 Announce Type: replace 
Abstract: Identifying reliable biomarkers for predicting clinical events in longitudinal studies is important for accurate disease prognosis and for guiding development of new treatments. However, prognostic studies are often observational, making it difficult to account for patient heterogeneity. In amyotrophic lateral sclerosis (ALS), factors such as age, site of onset and genetic status influence both survival and biomarker levels, yet their impact on the prognostic accuracy of biomarkers over time remains unclear. While time-dependent receiver operating characteristic methods have been developed to handle censored time-to-event outcomes, most do not adjust for covariates. To address this, we propose the nonparanormal prognostic biomarker framework, which models the joint distribution of the biomarker and event time while accounting for covariates. This allows estimation of covariate-specific time-dependent ROC curves and related summary measures. We apply the NPB framework to evaluate serum neurofilament light as a prognostic biomarker in ALS, showing that its accuracy varies over time and with patient characteristics. By capturing these covariate-specific effects, the NPB framework supports more targeted risk stratification and can potentially improve the design of clinical trials for new ALS treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20892v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ainesh Sewak, Vanda Inacio, Joanne Wuu, Michael Benatar, Torsten Hothorn</dc:creator>
    </item>
    <item>
      <title>Nonparametric Estimation of Local Treatment Effects with Continuous Instruments</title>
      <link>https://arxiv.org/abs/2504.03063</link>
      <description>arXiv:2504.03063v2 Announce Type: replace 
Abstract: Instrumental variable methods are widely used to address unmeasured confounding, yet much of the existing literature has focused on the binary instrument setting. Extensions to continuous instruments often impose strong parametric assumptions for identification and estimation, which can be difficult to justify and may limit their applicability in complex real-world settings. In this work, we develop theory and methods for nonparametric estimation of treatment effects with a continuous instrumental variable. We introduce an estimand that, under a monotonicity assumption, quantifies the treatment effect among the maximal complier class, generalizing the local average treatment effect framework to continuous instruments. Considering this estimand and the local instrumental variable curve, we draw connections to the dose-response function and its derivative, and propose doubly robust estimation methods. We establish convergence rates and conditions for asymptotic normality, providing valuable insights into the role of nuisance function estimation when the instrument is continuous. Additionally, we present practical procedures for bandwidth selection and variance estimation. Through extensive simulations, we demonstrate the advantages of the proposed nonparametric estimators. Finally, we apply our methods to data where excess travel time is an instrument for patients' likelihood of receiving care at specialized health care facilities. We use this instrument to estimate the effect of delivering at low-quality neonatal intensive care units (NICUs) on infant mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03063v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenghao Zeng, Alexander W. Levis, JungHo Lee, Edward H. Kennedy, Luke Keele</dc:creator>
    </item>
    <item>
      <title>Bayesian Smoothed Quantile Regression</title>
      <link>https://arxiv.org/abs/2508.01738</link>
      <description>arXiv:2508.01738v2 Announce Type: replace 
Abstract: Bayesian quantile regression based on the asymmetric Laplace distribution (ALD) likelihood suffers from two fundamental limitations: the non-differentiability of the check loss precludes gradient-based Markov chain Monte Carlo (MCMC) methods, and the posterior mean provides biased quantile estimates. We propose Bayesian smoothed quantile regression (BSQR), which replaces the check loss with a kernel-smoothed version, creating a continuously differentiable likelihood. This smoothing has two crucial consequences: it enables efficient Hamiltonian Monte Carlo sampling, and it yields a consistent posterior distribution, thereby resolving the inferential bias of the standard approach. We further establish conditions for posterior propriety under various priors (including improper and hierarchical) and characterize how kernel choice affects posterior concentration and computational efficiency. Extensive simulations validate our theoretical findings, demonstrating that BSQR achieves up to a 50% reduction in predictive check loss at extreme quantiles compared to ALD-based methods, while improving MCMC efficiency by 20-40% in effective sample size. An empirical application to financial risk measurement during the COVID-19 era illustrates BSQR's practical advantages in capturing dynamic systemic risk. The BSQR framework provides a theoretically-grounded and computationally-efficient solution to longstanding challenges in Bayesian quantile regression, with compact-support kernels like the uniform and triangular emerging as particularly effective choices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01738v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingqi Liu, Kangqiang Li, Tianxiao Pang</dc:creator>
    </item>
    <item>
      <title>Decision Theory For Large Scale Outlier Detection Using Aleatoric Uncertainty: With a Note on Bayesian FDR</title>
      <link>https://arxiv.org/abs/2508.01988</link>
      <description>arXiv:2508.01988v2 Announce Type: replace 
Abstract: Aleatoric and Epistemic uncertainty have achieved recent attention in the literature as different sources from which uncertainty can emerge in stochastic modeling. Epistemic being intrinsic or model based notions of uncertainty, and aleatoric being the uncertainty inherent in the data. We propose a novel decision theoretic framework for outlier detection in the context of aleatoric uncertainty; in the context of Bayesian modeling. The model incorporates bayesian false discovery rate control for multiplicty adjustment, and a new generalization of Bayesian FDR is introduced. The model is applied to simulations based on temporally fluctuating outlier detection where fixing thresholds often results in poor performance due to nonstationarity, and a case study is outlined on on a novel cybersecurity detection. Cyberthreat signals are highly nonstationary; giving a credible stress test of the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01988v2</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ryan Warnick</dc:creator>
    </item>
    <item>
      <title>Estimation for multistate models subject to reporting delays and incomplete event adjudication with application to disability insurance</title>
      <link>https://arxiv.org/abs/2311.04318</link>
      <description>arXiv:2311.04318v4 Announce Type: replace-cross 
Abstract: Accurate forecasting of an insurer's outstanding liabilities is vital for the solvency of insurance companies and the financial stability of the insurance sector. For health and disability insurance, the liabilities are intimately linked with the biometric event history of the insured. Complete observation of event histories is often impossible due to sampling effects such as right-censoring and left-truncation, but also due to reporting delays and incomplete event adjudication. In this paper, we develop a parametric two-step M-estimation method that takes the aforementioned effects into account, treating the latter two as partially exogenous. The approach is valid under weak assumptions and allows for complicated dependencies between the event history, reporting delays, and adjudication while remaining relatively simple to implement. The estimation approach has desirable properties which are demonstrated by theoretical results and numerical experiments.
  In the application, we introduce and consider a large portfolio of disability insurance policies. We find that properly accounting for the sampling effects has a large impact on the number of disabilities and reactivations that an insurer would forecast, allowing for a more accurate assessment of the insurer's liabilities and improved risk management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04318v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>K. Buchardt, C. Furrer, O. L. Sandqvist</dc:creator>
    </item>
    <item>
      <title>Quasi-Bayes in Latent Variable Models</title>
      <link>https://arxiv.org/abs/2311.06831</link>
      <description>arXiv:2311.06831v3 Announce Type: replace-cross 
Abstract: Latent variable models are widely used to account for unobserved determinants of economic behavior. This paper introduces a quasi-Bayes approach to nonparametrically estimate a large class of latent variable models. As an application, we model U.S. individual log earnings from the Panel Study of Income Dynamics (PSID) as the sum of latent permanent and transitory components. Simulations illustrate the favorable performance of quasi-Bayes estimators relative to common alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.06831v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sid Kankanala</dc:creator>
    </item>
    <item>
      <title>Tensor Decomposition with Unaligned Observations</title>
      <link>https://arxiv.org/abs/2410.14046</link>
      <description>arXiv:2410.14046v2 Announce Type: replace-cross 
Abstract: This paper presents a canonical polyadic (CP) tensor decomposition that addresses unaligned observations. The mode with unaligned observations is represented using functions in a reproducing kernel Hilbert space (RKHS). We introduce a versatile loss function that effectively accounts for various types of data, including binary, integer-valued, and positive-valued types. Additionally, we propose an optimization algorithm for computing tensor decompositions with unaligned observations, along with a stochastic gradient method to enhance computational efficiency. A sketching algorithm is also introduced to further improve efficiency when using the $\ell_2$ loss function. To demonstrate the efficacy of our methods, we provide illustrative examples using both synthetic data and an early childhood human microbiome dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14046v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Runshi Tang, Tamara Kolda, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Priors for second-order unbiased Bayes estimators</title>
      <link>https://arxiv.org/abs/2412.19187</link>
      <description>arXiv:2412.19187v2 Announce Type: replace-cross 
Abstract: Asymptotically unbiased priors, introduced by Hartigan (1965), are designed to achieve second-order unbiasedness of Bayes estimators. This paper extends Hartigan's framework to non-i.i.d. models by deriving a system of partial differential equations that characterizes asymptotically unbiased priors. Furthermore, we establish a necessary and sufficient condition for the existence of such priors and propose a simple procedure for constructing them. The proposed method is applied to the linear regression model and the nested error regression model (also known as the random effects model). Simulation studies evaluate the frequentist properties of the Bayes estimator under the asymptotically unbiased prior for the nested error regression model, highlighting its effectiveness in small-sample settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19187v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mana Sakai, Takeru Matsuda, Tatsuya Kubokawa</dc:creator>
    </item>
    <item>
      <title>Optimal differentially private kernel learning with random projection</title>
      <link>https://arxiv.org/abs/2507.17544</link>
      <description>arXiv:2507.17544v2 Announce Type: replace-cross 
Abstract: Differential privacy has become a cornerstone in the development of privacy-preserving learning algorithms. This work addresses optimizing differentially private kernel learning within the empirical risk minimization (ERM) framework. We propose a novel differentially private kernel ERM algorithm based on random projection in the reproducing kernel Hilbert space using Gaussian processes. Our method achieves minimax-optimal excess risk for both the squared loss and Lipschitz-smooth convex loss functions under a local strong convexity condition. We further show that existing approaches based on alternative dimension reduction techniques, such as random Fourier feature mappings or $\ell_2$ regularization, yield suboptimal generalization performance. Our key theoretical contribution also includes the derivation of dimension-free generalization bounds for objective perturbation-based private linear ERM -- marking the first such result that does not rely on noisy gradient-based mechanisms. Additionally, we obtain sharper generalization bounds for existing differentially private kernel ERM algorithms. Empirical evaluations support our theoretical claims, demonstrating that random projection enables statistically efficient and optimally private kernel learning. These findings provide new insights into the design of differentially private algorithms and highlight the central role of dimension reduction in balancing privacy and utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17544v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bonwoo Lee, Cheolwoo Park, Jeongyoun Ahn</dc:creator>
    </item>
    <item>
      <title>Diagrams-to-Dynamics (D2D): Exploring Causal Loop Diagram Leverage Points under Uncertainty</title>
      <link>https://arxiv.org/abs/2508.05659</link>
      <description>arXiv:2508.05659v2 Announce Type: replace-cross 
Abstract: Causal loop diagrams (CLDs) are widely used in health and environmental research to represent hypothesized causal structures underlying complex problems. However, as qualitative and static representations, CLDs are limited in their ability to support dynamic analysis and inform intervention strategies. Additionally, quantitative CLD analysis methods like network centrality analysis often lead to false inference. We propose Diagrams-to-Dynamics (D2D), a method for converting CLDs into exploratory system dynamics models (SDMs) in the absence of empirical data. With minimal user input - following a protocol to label variables as stocks, flows or auxiliaries, and constants - D2D leverages the structural information already encoded in CLDs, namely, link existence and polarity, to simulate hypothetical interventions and explore potential leverage points under uncertainty. Results suggest that D2D helps distinguish between high- and low-ranked leverage points. We compare D2D to a data-driven SDM constructed from the same CLD and variable labels. D2D showed greater consistency with the data-driven model than network centrality analysis, while providing uncertainty estimates and guidance for future data collection. The method is implemented in an open-source Python package and a web-based application to support further testing and lower the barrier to dynamic modeling for researchers working with CLDs. We expect additional validation will further establish the approach's utility across a broad range of cases and domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05659v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jeroen F. Uleman, Loes Crielaard, Leonie K. Elsenburg, Guido A. Veldhuis, Karien Stronks, Naja Hulvej Rod, Rick Quax, V\'itor V. Vasconcelos</dc:creator>
    </item>
  </channel>
</rss>
