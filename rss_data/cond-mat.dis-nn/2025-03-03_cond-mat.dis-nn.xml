<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Mar 2025 05:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Noise induced extreme events in single Fitzhugh-Nagumo oscillator</title>
      <link>https://arxiv.org/abs/2502.20404</link>
      <description>arXiv:2502.20404v1 Announce Type: new 
Abstract: The FitzHugh-Nagumo (FHN) model serves as a fundamental neuronal model which is extensively studied across various dynamical scenarios, we explore the dynamics of a scalar FHN oscillator under the influence of white noise. Unlike previous studies, in which extreme events (EE) were observed solely in coupled FHN oscillators, we demonstrate that a single system can exhibit EE induced by noise. Perturbation of the deterministic model in its steady state by random fluctuations reveals the emergence of subthreshold/small-amplitude oscillations (SAO), eventually leading to rare and extreme large-amplitude oscillations (LAO), which become particularly evident at minimal noise intensities. We elucidate the route by which these EE emerge, confirming their occurrence through probability calculations of trajectories in phase space. Additionally, our investigation reveals bursting phenomena in the system, which are characterized by specific levels of noise amplitude and elucidated using inter-spike interval statistics. At higher noise amplitudes, frequent LAO production is observed and attributed to self-induced stochastic resonance. The emergence of EE is explained through the theory of large fluctuations, with the escape rates of trajectories estimated via both analytical and numerical approaches. This study is significant because it reveals EE and bursting phenomena in a single FHN oscillator, offering potential new insights into the dynamics of neuronal populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20404v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>nlin.CD</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>S. Hariharan, R. Suresh, V. K. Chandrasekar</dc:creator>
    </item>
    <item>
      <title>Lattice Protein Folding with Variational Annealing</title>
      <link>https://arxiv.org/abs/2502.20632</link>
      <description>arXiv:2502.20632v1 Announce Type: new 
Abstract: Understanding the principles of protein folding is a cornerstone of computational biology, with implications for drug design, bioengineering, and the understanding of fundamental biological processes. Lattice protein folding models offer a simplified yet powerful framework for studying the complexities of protein folding, enabling the exploration of energetically optimal folds under constrained conditions. However, finding these optimal folds is a computationally challenging combinatorial optimization problem. In this work, we introduce a novel upper-bound training scheme that employs masking to identify the lowest-energy folds in two-dimensional Hydrophobic-Polar (HP) lattice protein folding. By leveraging Dilated Recurrent Neural Networks (RNNs) integrated with an annealing process driven by temperature-like fluctuations, our method accurately predicts optimal folds for benchmark systems of up to 60 beads. Our approach also effectively masks invalid folds from being sampled without compromising the autoregressive sampling properties of RNNs. This scheme is generalizable to three spatial dimensions and can be extended to lattice protein models with larger alphabets. Our findings emphasize the potential of advanced machine learning techniques in tackling complex protein folding problems and a broader class of constrained combinatorial optimization challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20632v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.BM</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shoummo Ahsan Khandoker, Estelle M. Inack, Mohamed Hibat-Allah</dc:creator>
    </item>
    <item>
      <title>Critical exponents of the spin glass transition in a field at zero temperature</title>
      <link>https://arxiv.org/abs/2502.21089</link>
      <description>arXiv:2502.21089v1 Announce Type: new 
Abstract: We analyze the spin glass transition in a field in finite dimension $D$ below the upper critical dimension directly at zero temperature using a recently introduced perturbative loop expansion around the Bethe lattice solution. The expansion is generated by the so-called $M$-layer construction, and it has $1/M$ as the associated small parameter. Computing analytically and numerically these non-standard diagrams at first order in the $1/M$ expansion, we construct an $\epsilon$-expansion around the upper critical dimension $D_\text{uc}=8$, with $\epsilon=D_\text{uc}-D$. Following standard field theoretical methods, we can write a $\beta$ function, finding a new zero-temperature fixed-point associated with the spin glass transition in a field in dimensions $D&lt;8$. We are also able to compute, at first order in the $\epsilon$-expansion, the three independent critical exponents characterizing the transition, plus the correction-to-scaling exponent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21089v1</guid>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Maria Chiara Angelini, Saverio Palazzi, Giorgio Parisi, Tommaso Rizzo</dc:creator>
    </item>
    <item>
      <title>Variational Transformer Ansatz for the Density Operator of Steady States in Dissipative Quantum Many-Body Systems</title>
      <link>https://arxiv.org/abs/2502.20723</link>
      <description>arXiv:2502.20723v1 Announce Type: cross 
Abstract: The transformer architecture, known for capturing long-range dependencies and intricate patterns, has extended beyond natural language processing. Recently, it has attracted significant attention in quantum information and condensed matter physics. In this work, we propose the transformer density operator ansatz for determining the steady states of dissipative quantum many-body systems. By vectorizing the density operator as a many-body state in a doubled Hilbert space, the transformer encodes the amplitude and phase of the state's coefficients, with its parameters serving as variational variables. Our design preserves translation invariance while leveraging attention mechanisms to capture diverse long-range correlations. We demonstrate the effectiveness of our approach by numerically calculating the steady states of dissipative Ising and Heisenberg spin chain models, showing that our method achieves excellent accuracy in predicting steady states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20723v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.str-el</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lu Wei, Zhian Jia, Yufeng Wang, Dagomir Kaszlikowski, Haibin Ling</dc:creator>
    </item>
    <item>
      <title>Analysis of Evolving Cortical Neuronal Networks Using Visual Informatics</title>
      <link>https://arxiv.org/abs/2502.20862</link>
      <description>arXiv:2502.20862v1 Announce Type: cross 
Abstract: Understanding the nature of the changes exhibited by evolving neuronal dynamics from high-dimensional activity data is essential for advancing neuroscience, particularly in the study of neuronal network development and the pathophysiology of neurological disorders. This work examines how advanced dimensionality reduction techniques can efficiently summarize and enhance our understanding of the development of neuronal networks over time and in response to stimulation. We develop a framework based on the Minimum-Distortion Embedding (MDE) methods and demonstrate how MDE outperforms better known benchmarks based on Principal Component Analysis (PCA) and t-distributed Stochastic Neighbor Embedding (t-SNE) by effectively preserving both global structures and local relationships within complex neuronal datasets. Our \emph{in silico} experiments reveal MDE's capability to capture the evolving connectivity patterns of simulated neuronal networks, illustrating a clear trajectory tracking the simulated network development. Complementary \emph{in vitro} experiments further validate MDE's advantages, highlighting its ability to identify behavioral differences and connectivity changes in neuronal cultures over a 35-day observation period. Additionally, we explore the effects of stimulation on neuronal activity, providing valuable insights into the plasticity and learning mechanisms of neuronal networks. Our findings underscore the importance of metric selection in dimensionality reduction, showing that correlation metrics yield more meaningful embeddings compared to Euclidean distance. The implications of this research extend to various areas, including the potential development of therapeutic intervention strategies for neurological disorders, and the identification of distinct phases of neuronal activity for advancing cortical-based computing devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20862v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.data-an</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ho Fai Po, Akke Mats Houben, Anna-Christina Haeb, Yordan P. Raykov, Daniel Tornero, Jordi Soriano, David Saad</dc:creator>
    </item>
    <item>
      <title>Bulk-edge correspondence at the spin-to-integer quantum Hall effect crossover in topological superconductors</title>
      <link>https://arxiv.org/abs/2502.21230</link>
      <description>arXiv:2502.21230v1 Announce Type: cross 
Abstract: The spin and integer quantum Hall effects are two cousins of topological phase transitions in two-dimensional electronic systems. Their close relationship makes it possible to transform spin to integer quantum Hall effect in two-dimensional topological superconductors by continuous increase in a symmetry breaking Zeeman magnetic field. We study peculiarities of bulk-edge correspondence and a fate of massless edge and bulk topological (instantons) excitations at such the crossover.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21230v1</guid>
      <category>cond-mat.mes-hall</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.supr-con</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maksim Parfenov, Igor Burmistrov</dc:creator>
    </item>
    <item>
      <title>Dynamical Decoupling of Generalization and Overfitting in Large Two-Layer Networks</title>
      <link>https://arxiv.org/abs/2502.21269</link>
      <description>arXiv:2502.21269v1 Announce Type: cross 
Abstract: The inductive bias and generalization properties of large machine learning models are -- to a substantial extent -- a byproduct of the optimization algorithm used for training. Among others, the scale of the random initialization, the learning rate, and early stopping all have crucial impact on the quality of the model learnt by stochastic gradient descent or related algorithms. In order to understand these phenomena, we study the training dynamics of large two-layer neural networks. We use a well-established technique from non-equilibrium statistical physics (dynamical mean field theory) to obtain an asymptotic high-dimensional characterization of this dynamics. This characterization applies to a Gaussian approximation of the hidden neurons non-linearity, and empirically captures well the behavior of actual neural network models.
  Our analysis uncovers several interesting new phenomena in the training dynamics: $(i)$ The emergence of a slow time scale associated with the growth in Gaussian/Rademacher complexity; $(ii)$ As a consequence, algorithmic inductive bias towards small complexity, but only if the initialization has small enough complexity; $(iii)$ A separation of time scales between feature learning and overfitting; $(iv)$ A non-monotone behavior of the test error and, correspondingly, a `feature unlearning' phase at large times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21269v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Montanari, Pierfrancesco Urbani</dc:creator>
    </item>
    <item>
      <title>Evidence of Replica Symmetry Breaking under the Nishimori conditions in epidemic inference on graphs</title>
      <link>https://arxiv.org/abs/2502.13249</link>
      <description>arXiv:2502.13249v2 Announce Type: replace 
Abstract: In Bayesian inference, computing the posterior distribution from the data is typically a non-trivial problem, which usually requires approximations such as mean-field approaches or numerical methods, like the Monte Carlo Markov Chain. Being a high-dimensional distribution over a set of correlated variables, the posterior distribution can undergo the notorious replica symmetry breaking transition. When it happens, several mean-field methods and virtually every Monte Carlo scheme can not provide a reasonable approximation to the posterior and its marginals. Replica symmetry is believed to be guaranteed whenever the data is generated with known prior and likelihood distributions, namely under the so-called Nishimori conditions. In this paper, we break this belief, by providing a counter-example showing that, under the Nishimori conditions, replica symmetry breaking arises. Introducing a simple, geometrical model that can be thought of as a patient zero retrieval problem in a highly infectious regime of the epidemic Susceptible-Infectious model, we show that under the Nishimori conditions, there is evidence of replica symmetry breaking. We achieve this result by computing the instability of the replica symmetric cavity method toward the one step replica symmetry broken phase. The origin of this phenomenon -- replica symmetry breaking under the Nishimori conditions -- is likely due to the correlated disorder appearing in the epidemic models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13249v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alfredo Braunstein, Louise Budzynski, Matteo Mariani, Federico Ricci-Tersenghi</dc:creator>
    </item>
    <item>
      <title>Quantum reservoir computing on random regular graphs</title>
      <link>https://arxiv.org/abs/2409.03665</link>
      <description>arXiv:2409.03665v2 Announce Type: replace-cross 
Abstract: Quantum reservoir computing (QRC) is a low-complexity learning paradigm that combines the inherent dynamics of input-driven many-body quantum systems with classical learning techniques for nonlinear temporal data processing. Optimizing the QRC process and computing device is a complex task due to the dependence of many-body quantum systems to various factors. To explore this, we introduce a strongly interacting spin model on random regular graphs as the quantum component and investigate the interplay between static disorder, interactions, and graph connectivity, revealing their critical impact on quantum memory capacity and learnability accuracy. We tackle linear quantum and nonlinear classical tasks, and identify optimal learning and memory regimes through studying information localization, dynamical quantum correlations, and the many-body structure of the disordered Hamiltonian. In particular, we uncover the role of previously overlooked network connectivity and demonstrate how the presence of quantum correlations can significantly enhance the learning performance. Our findings thus provide guidelines for the optimal design of disordered analog quantum learning platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03665v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.str-el</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moein N. Ivaki, Achilleas Lazarides, Tapio Ala-Nissila</dc:creator>
    </item>
    <item>
      <title>Applications of Statistical Field Theory in Deep Learning</title>
      <link>https://arxiv.org/abs/2502.18553</link>
      <description>arXiv:2502.18553v2 Announce Type: replace-cross 
Abstract: Deep learning algorithms have made incredible strides in the past decade yet due to the complexity of these algorithms, the science of deep learning remains in its early stages. Being an experimentally driven field, it is natural to seek a theory of deep learning within the physics paradigm. As deep learning is largely about learning functions and distributions over functions, statistical field theory, a rich and versatile toolbox for tackling complex distributions over functions (fields) is an obvious choice of formalism. Research efforts carried out in the past few years have demonstrated the ability of field theory to provide useful insights on generalization, implicit bias, and feature learning effects. Here we provide a pedagogical review of this emerging line of research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18553v2</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zohar Ringel, Noa Rubin, Edo Mor, Moritz Helias, Inbar Seroussi</dc:creator>
    </item>
  </channel>
</rss>
