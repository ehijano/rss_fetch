<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Sep 2025 01:23:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Dissecting Multifractal detrended cross-correlation analysis</title>
      <link>https://arxiv.org/abs/2406.19406</link>
      <description>arXiv:2406.19406v2 Announce Type: cross 
Abstract: In this work we address the question of the Multifractal detrended cross-correlation analysis method that has been subject to some controversies since its inception almost two decades ago. To this end we propose several new options to deal with negative cross-covariance among two time series, that may serve to construct a more robust view of the multifractal spectrum among the series. We compare these novel options with the proposals already existing in the literature, and we provide fast code in C, R and Python for both new and the already existing proposals. We test different algorithms on synthetic series with an exact analytical solution, as well as on daily price series of ethanol and sugar in Brazil from 2010 to 2023.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19406v2</guid>
      <category>q-fin.ST</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.physa.2025.130971</arxiv:DOI>
      <dc:creator>Borko Stosic, Tatijana Stosic</dc:creator>
    </item>
    <item>
      <title>Analog-based ensembles to characterize turbulent dynamics from observed data</title>
      <link>https://arxiv.org/abs/2509.07992</link>
      <description>arXiv:2509.07992v1 Announce Type: cross 
Abstract: We present a methodology for the study of the dispersion of trajectories of stochastic processes in reconstructed phase spaces from observed data. The methodology allows to find ensembles of analog states, i.e. states that are infinitesimally close in the phase space. Once these states are found, we focus on the characterization of their dispersion in function of 1) the time and 2) their initial separation. We study a experimental turbulent velocity measurement and two scale-invariant stochastic processes: a regularized fractional Brownian motion and a regularized multifractal random walk. Both stochastic processes are synthesized to have the same covariance structure as the experimental turbulent velocity, but only the regularized multifractal random walk mimics the intermittency of turbulent velocity. We illustrate that while the covariance structure of the processes governs the time dependence of the dispersion of the analog states, the intermittency phenomenon is responsible of the impact of the initial separation of the analogs on their dispersion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07992v1</guid>
      <category>nlin.CD</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Granero-Belinchon (ODYSSEY, IMT Atlantique - MEE, Lab-STICC\_OSE)</dc:creator>
    </item>
    <item>
      <title>Contributions to Robust and Efficient Methods for Analysis of High Dimensional Data</title>
      <link>https://arxiv.org/abs/2509.08155</link>
      <description>arXiv:2509.08155v1 Announce Type: cross 
Abstract: A ubiquitous feature of data of our era is their extra-large sizes and dimensions. Analyzing such high-dimensional data poses significant challenges, since the feature dimension is often much larger than the sample size. This thesis introduces robust and computationally efficient methods to address several common challenges associated with high-dimensional data. In my first manuscript, I propose a coherent approach to variable screening that accommodates nonlinear associations. I develop a novel variable screening method that transcends traditional linear assumptions by leveraging mutual information, with an intended application in neuroimaging data. This approach allows for accurate identification of important variables by capturing nonlinear as well as linear relationships between the outcome and covariates. Building on this foundation, I develop new optimization methods for sparse estimation using nonconvex penalties in my second manuscript. These methods address notable challenges in current statistical computing practices, facilitating computationally efficient and robust analyses of complex datasets. The proposed method can be applied to a general class of optimization problems. In my third manuscript, I contribute to robust modeling of high-dimensional correlated observations by developing a mixed-effects model based on Tsallis power-law entropy maximization and discussed the theoretical properties of such distribution. This model surpasses the constraints of conventional Gaussian models by accommodating a broader class of distributions with enhanced robustness to outliers. Additionally, I develop a proximal nonlinear conjugate gradient algorithm that accelerates convergence while maintaining numerical stability, along with rigorous statistical properties for the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08155v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kai Yang</dc:creator>
    </item>
    <item>
      <title>Agents of Discovery</title>
      <link>https://arxiv.org/abs/2509.08535</link>
      <description>arXiv:2509.08535v1 Announce Type: cross 
Abstract: The substantial data volumes encountered in modern particle physics and other domains of fundamental physics research allow (and require) the use of increasingly complex data analysis tools and workflows. While the use of machine learning (ML) tools for data analysis has recently proliferated, these tools are typically special-purpose algorithms that rely, for example, on encoded physics knowledge to reach optimal performance. In this work, we investigate a new and orthogonal direction: Using recent progress in large language models (LLMs) to create a team of agents -- instances of LLMs with specific subtasks -- that jointly solve data analysis-based research problems in a way similar to how a human researcher might: by creating code to operate standard tools and libraries (including ML systems) and by building on results of previous iterations. If successful, such agent-based systems could be deployed to automate routine analysis components to counteract the increasing complexity of modern tool chains. To investigate the capabilities of current-generation commercial LLMs, we consider the task of anomaly detection via the publicly available and highly-studied LHC Olympics dataset. Several current models by OpenAI (GPT-4o, o4-mini, GPT-4.1, and GPT-5) are investigated and their stability tested. Overall, we observe the capacity of the agent-based system to solve this data analysis problem. The best agent-created solutions mirror the performance of human state-of-the-art results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08535v1</guid>
      <category>hep-ph</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sascha Diefenbacher, Anna Hallin, Gregor Kasieczka, Michael Kr\"amer, Anne Lauscher, Tim Lukas</dc:creator>
    </item>
    <item>
      <title>Quantifying model prediction sensitivity to model-form uncertainty</title>
      <link>https://arxiv.org/abs/2509.08708</link>
      <description>arXiv:2509.08708v1 Announce Type: cross 
Abstract: Model-form uncertainty (MFU) in assumptions made during physics-based model development is widely considered a significant source of uncertainty; however, there are limited approaches that can quantify MFU in predictions extrapolating beyond available data. As a result, it is challenging to know how important MFU is in practice, especially relative to other sources of uncertainty in a model, making it difficult to prioritize resources and efforts to drive down error in model predictions. To address these challenges, we present a novel method to quantify the importance of uncertainties associated with model assumptions. We combine parameterized modifications to assumptions (called MFU representations) with grouped variance-based sensitivity analysis to measure the importance of assumptions. We demonstrate how, in contrast to existing methods addressing MFU, our approach can be applied without access to calibration data. However, if calibration data is available, we demonstrate how it can be used to inform the MFU representation, and how variance-based sensitivity analysis can be meaningfully applied even in the presence of dependence between parameters (a common byproduct of calibration).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08708v1</guid>
      <category>cs.CE</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teresa Portone, Rebekah D. White, Joseph L. Hart</dc:creator>
    </item>
    <item>
      <title>Accelerating Hamiltonian Monte Carlo for Bayesian Inference in Neural Networks and Neural Operators</title>
      <link>https://arxiv.org/abs/2507.14652</link>
      <description>arXiv:2507.14652v2 Announce Type: replace-cross 
Abstract: Hamiltonian Monte Carlo (HMC) is a powerful and accurate method to sample from the posterior distribution in Bayesian inference. However, HMC techniques are computationally demanding for Bayesian neural networks due to the high dimensionality of the network's parameter space and the non-convexity of their posterior distributions. Therefore, various approximation techniques, such as variational inference (VI) or stochastic gradient MCMC, are often employed to infer the posterior distribution of the network parameters. Such approximations introduce inaccuracies in the inferred distributions, resulting in unreliable uncertainty estimates. In this work, we propose a hybrid approach that combines inexpensive VI and accurate HMC methods to efficiently and accurately quantify uncertainties in neural networks and neural operators. The proposed approach leverages an initial VI training on the full network. We examine the influence of individual parameters on the prediction uncertainty, which shows that a large proportion of the parameters do not contribute substantially to uncertainty in the network predictions. This information is then used to significantly reduce the dimension of the parameter space, and HMC is performed only for the subset of network parameters that strongly influence prediction uncertainties. This yields a framework for accelerating the full batch HMC for posterior inference in neural networks. We demonstrate the efficiency and accuracy of the proposed framework on deep neural networks and operator networks, showing that inference can be performed for large networks with tens to hundreds of thousands of parameters. We show that this method can effectively learn surrogates for complex physical systems by modeling the operator that maps from upstream conditions to wall-pressure data on a cone in hypersonic flow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14652v2</guid>
      <category>stat.ML</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ponkrshnan Thiagarajan, Tamer A. Zaki, Michael D. Shields</dc:creator>
    </item>
  </channel>
</rss>
