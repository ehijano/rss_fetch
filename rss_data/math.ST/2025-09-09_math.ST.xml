<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Sep 2025 04:03:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Parametric convergence rate of a non-parametric estimator in multivariate mixtures of power series distributions under conditional independence</title>
      <link>https://arxiv.org/abs/2509.05452</link>
      <description>arXiv:2509.05452v1 Announce Type: new 
Abstract: The conditional independence assumption has recently appeared in a growing body of literature on the estimation of multivariate mixtures. We consider here conditionally independent multivariate mixtures of power series distributions with infinite support, to which belong Poisson, Geometric or Negative Binomial mixtures. We show that for all these mixtures, the non-parametric maximum likelihood estimator converges to the truth at the rate $(\log (nd))^{1+d/2} n^{-1/2}$ in the Hellinger distance, where $n$ denotes the size of the observed sample and $d$ represents the dimension of the mixture. Using this result, we then construct a new non-parametric estimator based on the maximum likelihood estimator that converges with the parametric rate $n^{-1/2}$ in all $\ell_p$-distances, for $p \ge 1$. These convergences rates are supported by simulations and the theory is illustrated using the famous V\'{e}lib dataset of the bike sharing system of Paris. We also introduce a testing procedure for whether the conditional independence assumption is satisfied for a given sample. This testing procedure is applied for several multivariate mixtures, with varying levels of dependence, and is thereby shown to distinguish well between conditionally independent and dependent mixtures. Finally, we use this testing procedure to investigate whether conditional independence holds for V\'{e}lib dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05452v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fadoua Balabdaoui, Harald Besdziek, Yong Wang</dc:creator>
    </item>
    <item>
      <title>Broadly discrete stable distributions</title>
      <link>https://arxiv.org/abs/2509.05497</link>
      <description>arXiv:2509.05497v1 Announce Type: new 
Abstract: Stable distributions are of fundamental importance in probability theory, yet their absolute continuity makes them unsuitable for modeling count data. A discrete analog of strict stability has been previously proposed by replacing scaling with binomial thinning, but it only holds for a subset of the tail index parameters. Here, we generalize the discrete stable class to the full range of tail indices and show that it is equivalent to the mixed Poisson-stable family. This broadly discrete stable family is discretely infinitely divisible, with a compound Poisson representation involving a novel generalization of the Sibuya distribution. Under additional parameter constraints, they are also discretely self-decomposable and unimodal. The discrete stable distributions provide a new frontier in probabilistic modeling of both light and heavy tailed count data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05497v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>F. William Townes</dc:creator>
    </item>
    <item>
      <title>Robust Confidence Intervals for a Binomial Proportion: Local Optimality and Adaptivity</title>
      <link>https://arxiv.org/abs/2509.05568</link>
      <description>arXiv:2509.05568v1 Announce Type: new 
Abstract: This paper revisits the classical problem of interval estimation of a binomial proportion under Huber contamination. Our main result derives the rate of optimal interval length when the contamination proportion is unknown under a local minimax framework, where the performance of an interval is evaluated at each point in the parameter space. By comparing the rate with the optimal length of a confidence interval that is allowed to use the knowledge of contamination proportion, we characterize the exact adaptation cost due to the ignorance of data quality. Our construction of the confidence interval to achieve local length optimality builds on robust hypothesis testing with a new monotonization step, which guarantees valid coverage, boundary-respecting intervals, and an efficient algorithm for computing the endpoints. The general strategy of interval construction can be applied beyond the binomial setting, and leads to optimal interval estimation for Poisson data with contamination as well. We also investigate a closely related Erd\H{o}s--R\'{e}nyi model with node contamination. Though its optimal rate of parameter estimation agrees with that of the binomial setting, we show that adaptation to unknown contamination proportion is provably impossible for interval estimation in that setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05568v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minjun Cho, Yuetian Luo, Chao Gao</dc:creator>
    </item>
    <item>
      <title>On a surprising behavior of the likelihood ratio test in non-parametric mixture models</title>
      <link>https://arxiv.org/abs/2509.05610</link>
      <description>arXiv:2509.05610v1 Announce Type: new 
Abstract: We study the likelihood ratio test in general mixture models where the base density is parametric, the null is a known fixed mixing distribution, and the alternative is a general mixing distribution supported on a bounded parameter space. For Gaussian location mixtures and Poisson mixtures, we show a surprising result: the non-parametric likelihood ratio test statistic converges to a tight limit if and only if the null distribution is a finite mixture, and diverges to infinity otherwise. We further demonstrate that the likelihood ratio test diverges for a fairly general class of distributions when the null mixing distribution is not finitely discrete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05610v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Zhang, Stanislav Volgushev</dc:creator>
    </item>
    <item>
      <title>Network signflip parallel analysis for selecting the embedding dimension</title>
      <link>https://arxiv.org/abs/2509.05722</link>
      <description>arXiv:2509.05722v1 Announce Type: new 
Abstract: This paper investigates the problem of selecting the embedding dimension for large heterogeneous networks that have weakly distinguishable community structure. For a broad family of embeddings based on normalized adjacency matrices, we introduce a novel spectral method that compares the eigenvalues of the normalized adjacency matrix to those obtained after randomly signflipping its entries. The proposed method, called network signflip parallel analysis (NetFlipPA), is interpretable, simple to implement, data driven, and does not require users to carefully tune parameters. For large random graphs arising from degree-corrected stochastic blockmodels with weakly distinguishable community structure (and consequently, non-diverging eigenvalues), NetFlipPA provably recovers the spectral noise floor (i.e., the upper-edge of the eigenvalues corresponding to the noise component of the normalized adjacency matrix). NetFlipPA thus provides a statistically rigorous randomization-based method for selecting the embedding dimension by keeping the eigenvalues that rise above the recovered spectral noise floor. Compared to traditional cutoff-based methods, the data-driven threshold used in NetFlipPA is provably effective under milder assumptions on the node degree heterogeneity and the number of node communities. Our main results rely on careful non-asymptotic perturbation analysis and leverage recent progress on local laws for nonhomogeneous Wigner-type random matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05722v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Hong, Joshua Cape</dc:creator>
    </item>
    <item>
      <title>Polynomial Log-Marginals and Tweedie's Formula : When Is Bayes Possible?</title>
      <link>https://arxiv.org/abs/2509.05823</link>
      <description>arXiv:2509.05823v1 Announce Type: new 
Abstract: Motivated by Tweedie's formula for the Compound Decision problem, we examine the theoretical foundations of empirical Bayes estimators that directly model the marginal density $m(y)$. Our main result shows that polynomial log-marginals of degree $k \ge 3 $ cannot arise from any valid prior distribution in exponential family models, while quadratic forms correspond exactly to Gaussian priors. This provides theoretical justification for why certain empirical Bayes decision rules, while practically useful, do not correspond to any formal Bayes procedures. We also strengthen the diagnostic by showing that a marginal is a Gaussian convolution only if it extends to a bounded solution of the heat equation in a neighborhood of the smoothing parameter, beyond the convexity of $c(y)=\tfrac12 y^2+\log m(y)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05823v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jyotishka Datta, Nicholas G. Polson</dc:creator>
    </item>
    <item>
      <title>Brownian sheet and uniformity tests on the hypercube</title>
      <link>https://arxiv.org/abs/2509.06134</link>
      <description>arXiv:2509.06134v1 Announce Type: new 
Abstract: A construction of $p$-parameter Brownian sheet on the hypercube $C=[0,1]^p$ as a sum of $2^p$ independent Gaussian processes is obtained. The terms are closely related to Brownian pillows, and the probability laws of their $L^2(C)$ squared norms are computed.
  This allows us to propose consistent tests of uniformity for samples of i.i.d. random vectors on $C$. A comparison of powers of the new tests with those of several uniformity tests found in the statistical literature completes the article. The proposed tests show a good performance in detecting copula alternatives.
  Keywords: Brownian sheet, multivariate uniformity tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06134v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> Caba\~na,  A.,  Caba\~na, E. M</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Misspecified Contextual Bandits</title>
      <link>https://arxiv.org/abs/2509.06287</link>
      <description>arXiv:2509.06287v1 Announce Type: new 
Abstract: Contextual bandit algorithms have transformed modern experimentation by enabling real-time adaptation for personalized treatment and efficient use of data. Yet these advantages create challenges for statistical inference due to adaptivity. A fundamental property that supports valid inference is policy convergence, meaning that action-selection probabilities converge in probability given the context. Convergence ensures replicability of adaptive experiments and stability of online algorithms. In this paper, we highlight a previously overlooked issue: widely used algorithms such as LinUCB may fail to converge when the reward model is misspecified, and such non-convergence creates fundamental obstacles for statistical inference. This issue is practically important, as misspecified models -- such as linear approximations of complex dynamic system -- are often employed in real-world adaptive experiments to balance bias and variance.
  Motivated by this insight, we propose and analyze a broad class of algorithms that are guaranteed to converge even under model misspecification. Building on this guarantee, we develop a general inference framework based on an inverse-probability-weighted Z-estimator (IPW-Z) and establish its asymptotic normality with a consistent variance estimator. Simulation studies confirm that the proposed method provides robust and data-efficient confidence intervals, and can outperform existing approaches that exist only in the special case of offline policy evaluation. Taken together, our results underscore the importance of designing adaptive algorithms with built-in convergence guarantees to enable stable experimentation and valid statistical inference in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06287v1</guid>
      <category>math.ST</category>
      <category>cs.AI</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yongyi Guo, Ziping Xu</dc:creator>
    </item>
    <item>
      <title>Interpretable dimension reduction for compositional data</title>
      <link>https://arxiv.org/abs/2509.05563</link>
      <description>arXiv:2509.05563v1 Announce Type: cross 
Abstract: High-dimensional compositional data, such as those from human microbiome studies, pose unique statistical challenges due to the simplex constraint and excess zeros. While dimension reduction is indispensable for analyzing such data, conventional approaches often rely on log-ratio transformations that compromise interpretability and distort the data through ad hoc zero replacements. We introduce a novel framework for interpretable dimension reduction of compositional data that avoids extra transformations and zero imputations. Our approach generalizes the concept of amalgamation by softening its operation, mapping high-dimensional compositions directly to a lower-dimensional simplex, which can be visualized in ternary plots. The framework further provides joint visualization of the reduction matrix, enabling intuitive, at-a-glance interpretation. To achieve optimal reduction within our framework, we incorporate sufficient dimension reduction, which defines a new identifiable objective: the central compositional subspace. For estimation, we propose a compositional kernel dimension reduction (CKDR) method. The estimator is provably consistent, exhibits sparsity that reveals underlying amalgamation structures, and comes with an intrinsic predictive model for downstream analyses. Applications to real microbiome datasets demonstrate that our approach provides a powerful graphical exploration tool for uncovering meaningful biological patterns, opening a new pathway for analyzing high-dimensional compositional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05563v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyoung Park, Cheolwoo Park, Jeongyoun Ahn</dc:creator>
    </item>
    <item>
      <title>Fisher Random Walk: Automatic Debiasing Contextual Preference Inference for Large Language Model Evaluation</title>
      <link>https://arxiv.org/abs/2509.05852</link>
      <description>arXiv:2509.05852v1 Announce Type: cross 
Abstract: Motivated by the need for rigorous and scalable evaluation of large language models, we study contextual preference inference for pairwise comparison functionals of context-dependent preference score functions across domains. Focusing on the contextual Bradley-Terry-Luce model, we develop a semiparametric efficient estimator that automates the debiased estimation through aggregating weighted residual balancing terms across the comparison graph. We show that the efficiency is achieved when the weights are derived from a novel strategy called Fisher random walk. We also propose a computationally feasible method to compute the weights by a potential representation of nuisance weight functions. We show our inference procedure is valid for general score function estimators accommodating the practitioners' need to implement flexible deep learning methods. We extend the procedure to multiple hypothesis testing using a Gaussian multiplier bootstrap that controls familywise error and to distributional shift via a cross-fitted importance-sampling adjustment for target-domain inference. Numerical studies, including language model evaluations under diverse contexts, corroborate the accuracy, efficiency, and practical utility of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05852v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yichi Zhang, Alexander Belloni, Ethan X. Fang, Junwei Lu, Xiaoan Xu</dc:creator>
    </item>
    <item>
      <title>Maximum-likelihood estimation of the Mat\'ern covariance structure of isotropic spatial random fields on finite, sampled grids</title>
      <link>https://arxiv.org/abs/2509.06223</link>
      <description>arXiv:2509.06223v1 Announce Type: cross 
Abstract: We present a statistically and computationally efficient spectral-domain maximum-likelihood procedure to solve for the structure of Gaussian spatial random fields within the Matern covariance hyperclass. For univariate, stationary, and isotropic fields, the three controlling parameters are the process variance, smoothness, and range. The debiased Whittle likelihood maximization explicitly treats discretization and edge effects for finite sampled regions in parameter estimation and uncertainty quantification. As even the best parameter estimate may not be good enough, we provide a test for whether the model specification itself warrants rejection. Our results are practical and relevant for the study of a variety of geophysical fields, and for spatial interpolation, out-of-sample extension, kriging, machine learning, and feature detection of geological data. We present procedural details and high-level results on real-world examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06223v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frederik J. Simons, Olivia L. Walbert, Arthur P. Guillaumin, Gabriel L. Eggers, Kevin W. Lewis, Sofia C. Olhede</dc:creator>
    </item>
    <item>
      <title>Minimax optimal transfer learning for high-dimensional additive regression</title>
      <link>https://arxiv.org/abs/2509.06308</link>
      <description>arXiv:2509.06308v1 Announce Type: cross 
Abstract: This paper studies high-dimensional additive regression under the transfer learning framework, where one observes samples from a target population together with auxiliary samples from different but potentially related regression models. We first introduce a target-only estimation procedure based on the smooth backfitting estimator with local linear smoothing. In contrast to previous work, we establish general error bounds under sub-Weibull($\alpha$) noise, thereby accommodating heavy-tailed error distributions. In the sub-exponential case ($\alpha=1$), we show that the estimator attains the minimax lower bound under regularity conditions, which requires a substantial departure from existing proof strategies. We then develop a novel two-stage estimation method within a transfer learning framework, and provide theoretical guarantees at both the population and empirical levels. Error bounds are derived for each stage under general tail conditions, and we further demonstrate that the minimax optimal rate is achieved when the auxiliary and target distributions are sufficiently close. All theoretical results are supported by simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06308v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Seung Hyun Moon</dc:creator>
    </item>
    <item>
      <title>Information-Theoretic Bounds and Task-Centric Learning Complexity for Real-World Dynamic Nonlinear Systems</title>
      <link>https://arxiv.org/abs/2509.06599</link>
      <description>arXiv:2509.06599v1 Announce Type: cross 
Abstract: Dynamic nonlinear systems exhibit distortions arising from coupled static and dynamic effects. Their intertwined nature poses major challenges for data-driven modeling. This paper presents a theoretical framework grounded in structured decomposition, variance analysis, and task-centric complexity bounds.
  The framework employs a directional lower bound on interactions between measurable system components, extending orthogonality in inner product spaces to structurally asymmetric settings. This bound supports variance inequalities for decomposed systems. Key behavioral indicators are introduced along with a memory finiteness index. A rigorous power-based condition establishes a measurable link between finite memory in realizable systems and the First Law of Thermodynamics. This offers a more foundational perspective than classical bounds based on the Second Law.
  Building on this foundation, we formulate a `Behavioral Uncertainty Principle,' demonstrating that static and dynamic distortions cannot be minimized simultaneously. We identify that real-world systems seem to resist complete deterministic decomposition due to entangled static and dynamic effects. We also present two general-purpose theorems linking function variance to mean-squared Lipschitz continuity and learning complexity. This yields a model-agnostic, task-aware complexity metric, showing that lower-variance components are inherently easier to learn.
  These insights explain the empirical benefits of structured residual learning, including improved generalization, reduced parameter count, and lower training cost, as previously observed in power amplifier linearization experiments. The framework is broadly applicable and offers a scalable, theoretically grounded approach to modeling complex dynamic nonlinear systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06599v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sri Satish Krishna Chaitanya Bulusu, Mikko Sillanp\"a\"a</dc:creator>
    </item>
    <item>
      <title>Specification Tests for the Error--Law in Vector Multiplicative Errors Models</title>
      <link>https://arxiv.org/abs/2509.06732</link>
      <description>arXiv:2509.06732v1 Announce Type: cross 
Abstract: We suggest specification tests for the error distribution in vector multiplicative error models (vMEM). The test statistic is formulated as a weighted integrated distance between the parametric estimator of the Laplace transform of the null distribution and its empirical counterpart computed from the residuals. Asymptotic results are obtained under both the null and alternative hypotheses. If the Laplace transform of the null distribution is not available in closed form, we propose a test statistic that uses independent artificial samples generated from the distribution under test, possibly with estimated parameters. The test statistic compares the empirical Laplace transforms of the residuals and the artificial errors using a similar weighted integrated distance. Bootstrap resampling is used to approximate the critical values of the test. The finite-sample performance of the two testing procedures is compared in a Monte Carlo simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06732v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>\v{S}\'arka Hudecov\'a, Simos G. Meintanis</dc:creator>
    </item>
    <item>
      <title>Learning from one graph: transductive learning guarantees via the geometry of small random worlds</title>
      <link>https://arxiv.org/abs/2509.06894</link>
      <description>arXiv:2509.06894v1 Announce Type: cross 
Abstract: Since their introduction by Kipf and Welling in $2017$, a primary use of graph convolutional networks is transductive node classification, where missing labels are inferred within a single observed graph and its feature matrix. Despite the widespread use of the network model, the statistical foundations of transductive learning remain limited, as standard inference frameworks typically rely on multiple independent samples rather than a single graph. In this work, we address these gaps by developing new concentration-of-measure tools that leverage the geometric regularities of large graphs via low-dimensional metric embeddings. The emergent regularities are captured using a random graph model; however, the methods remain applicable to deterministic graphs once observed. We establish two principal learning results. The first concerns arbitrary deterministic $k$-vertex graphs, and the second addresses random graphs that share key geometric properties with an Erd\H{o}s-R\'{e}nyi graph $\mathbf{G}=\mathbf{G}(k,p)$ in the regime $p \in \mathcal{O}((\log (k)/k)^{1/2})$. The first result serves as the basis for and illuminates the second. We then extend these results to the graph convolutional network setting, where additional challenges arise. Lastly, our learning guarantees remain informative even with a few labelled nodes $N$ and achieve the optimal nonparametric rate $\mathcal{O}(N^{-1/2})$ as $N$ grows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06894v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.MG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nils Detering, Luca Galimberti, Anastasis Kratsios, Giulia Livieri, A. Martina Neuman</dc:creator>
    </item>
    <item>
      <title>A nonparametric test of spherical symmetry applicable to high dimensional data</title>
      <link>https://arxiv.org/abs/2403.12491</link>
      <description>arXiv:2403.12491v2 Announce Type: replace 
Abstract: We develop a test for spherical symmetry of a multivariate distribution $\Pr$ that works well even when the dimension of the data $d$ is larger than the sample size $n$. We propose a non-negative measure of spherical asymmetry $\zeta(\Pr)$ such that $\zeta(\Pr)=0$ if and only if $\Pr$ is spherically symmetric. We construct a consistent estimator of $\zeta(\Pr)$ using the data augmentation method and investigate its large sample properties. The proposed test based on this estimator is calibrated using a novel resampling algorithm. Our test controls the type I error, and it is consistent against general alternatives. We also study its behavior for a sequence of alternatives $(1-\delta_n) F+\delta_n G$, where $\zeta(G)=0$ but $\zeta(F)&gt;0$, and $\delta_n \in [0,1]$. When $\lim\sup\delta_n&lt;1$, for any $G$, the power of our test converges to unity as $n$ increases. However, if $\lim\sup\delta_n=1$, the asymptotic power of our test depends on $\lim n(1-\delta_n)^2$. We establish this by proving the minimax rate optimality of our test over a suitable class of alternatives and showing that it is Pitman efficient when $\lim n(1-\delta_n)^2&gt;0$. Moreover, our test is provably consistent for high-dimensional data even when $d$ grows with $n$. When the center of symmetry is not specified by the null hypothesis, most of the existing tests often fail to satisfy the level property. To take care of this problem, we propose a general recipe for constructing modified tests based on pairwise differences of the observations. Our numerical results amply demonstrate the superiority of the proposed test over some state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12491v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bilol Banerjee, Anil K. Ghosh</dc:creator>
    </item>
    <item>
      <title>Optimality of Approximate Message Passing Algorithms for Spiked Matrix Models with Rotationally Invariant Noise</title>
      <link>https://arxiv.org/abs/2405.18081</link>
      <description>arXiv:2405.18081v2 Announce Type: replace 
Abstract: We study the problem of estimating a rank one signal matrix from an observed matrix generated by corrupting the signal with additive rotationally invariant noise. We develop a new class of approximate message-passing algorithms for this problem and provide a simple and concise characterization of their dynamics in the high-dimensional limit. At each iteration, these algorithms exploit prior knowledge about the noise structure by applying a non-linear matrix denoiser to the eigenvalues of the observed matrix and prior information regarding the signal structure by applying a non-linear iterate denoiser to the previous iterates generated by the algorithm. We exploit our result on the dynamics of these algorithms to derive the optimal choices for the matrix and iterate denoisers. We show that the resulting algorithm achieves the smallest possible asymptotic estimation error among a broad class of iterative algorithms under a fixed iteration budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18081v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rishabh Dudeja, Songbin Liu, Junjie Ma</dc:creator>
    </item>
    <item>
      <title>The feasibility of multi-graph alignment: a Bayesian approach</title>
      <link>https://arxiv.org/abs/2502.17142</link>
      <description>arXiv:2502.17142v2 Announce Type: replace 
Abstract: We establish thresholds for the feasibility of random multi-graph alignment in two models. In the Gaussian model, we demonstrate an "all-or-nothing" phenomenon: above a critical threshold, exact alignment is achievable with high probability, while below it, even partial alignment is statistically impossible. In the sparse Erd\H{o}s-R\'enyi model, we rigorously identify a threshold below which no meaningful partial alignment is possible and conjecture that above this threshold, partial alignment can be achieved. To prove these results, we develop a general Bayesian estimation framework over metric spaces, which provides insight into a broader class of high-dimensional statistical problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17142v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Louis Vassaux, Laurent Massouli\'e</dc:creator>
    </item>
    <item>
      <title>Directional Gaussian hypergeometric beta distributions and their uses in contaminated binary sampling</title>
      <link>https://arxiv.org/abs/2503.11128</link>
      <description>arXiv:2503.11128v2 Announce Type: replace 
Abstract: We examine the Gaussian hypergeometric beta distribution and look at the effect of having an additional term in the density kernel relative to the standard beta distribution. We reparameterise and classify this distribution into left and right directional variants using parameters that give a simple and symmetrical representation of the directional push/pull from this additional term in the density kernel. We examine the properties of the directional variants and their uses in contaminated binary sampling using Bayesian inference. We find that the Gaussian hypergeometric beta distribution arises as the appropriate posterior distribution for inference in certain kinds of contaminated binary models and that the directional parameterisation aids in representation of the resulting Bayesian models. We derive a broad range of properties and computational methods for the directional variants of the distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11128v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ben O'Neill</dc:creator>
    </item>
    <item>
      <title>Approximation rates for finite mixtures of location-scale models</title>
      <link>https://arxiv.org/abs/2508.10612</link>
      <description>arXiv:2508.10612v3 Announce Type: replace 
Abstract: Finite mixture models provide useful methods for approximation and estimation of probability density functions. In the context of location-scale mixture models, we improve upon previous results to show that it is possible to quantitatively bound the ${\cal L}_{p}$ approximation error in terms of the number of mixture components $m$ for any $p\in\left(1,\infty\right)$. In particular, for approximation on $\mathbb{R}^{d}$ of targets $f_{0}\in{\cal W}^{s,p}$, the (fractional) Sobolev spaces with smoothness order $s\in\left(0,1\right]$, if $p&lt;2$ then the error has size $O\left(m^{-\frac{s}{sq+d}}\right)$, while if $p\ge2$, the error has size $O\left(m^{-\frac{2s}{2\left(sq+d\right)}}\right)$, where $q=1/\left(p-1\right)$. We demonstrate that these results can be used to derive estimation rates for a location-scale mixture-based adaptive least-squares estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10612v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hien Duy Nguyen, TrungTin Nguyen, Jacob Westerhout, Xin Guo</dc:creator>
    </item>
    <item>
      <title>Bayesimax Theory: Selecting Priors by Minimizing Total Information</title>
      <link>https://arxiv.org/abs/2508.15976</link>
      <description>arXiv:2508.15976v2 Announce Type: replace 
Abstract: We introduce Bayesimax theory, a paradigm for objective Bayesian analysis which selects priors by applying minimax theory to prior disclosure games. In these games, the uniquely optimal strategy for a Bayesian agent upon observing the data is to reveal their prior. As such, the prior chosen by minimax theory is, in effect, the implicit prior of minimax agents. Since minimax analysis disregards prior information, this prior is arguably noninformative. We refer to minimax solutions of certain prior disclosure games as Bayesimax priors, and we classify a statistical procedure as Bayesimax if it is a Bayes rule with respect to a Bayesimax prior. Under regular conditions, if a decision rule is minimax, then it is a Bayes rule under priors which maximize the minimum Bayes risk. We study games leveraging strictly proper scoring rules to induce posterior (and thereby prior) revelation. In such games, the minimum Bayes risk equals the conditional (generalized) entropy of the parameter given the data. Bayesimax theory therefore prescribes conditional entropy maximization. As conditional entropy equals marginal entropy (prior uninformativeness) minus mutual information (data informativeness), Bayesimax priors effectively minimize total information. We provide a rigorous formulation of these ideas, characterize sufficient conditions for regularity and identifiability, and investigate asymptotics and conjugate family examples. We next describe a generic Monte Carlo algorithm for estimating conditional entropy under a given prior. Finally, we compare and contrast Bayesimax theory with various related proposals from the objective Bayes and robust Bayes literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15976v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sitaram Vangala</dc:creator>
    </item>
    <item>
      <title>Comment on "Worst-case Nonparametric Bounds for the Student T-statistic", arXiv:2508.13226</title>
      <link>https://arxiv.org/abs/2509.02410</link>
      <description>arXiv:2509.02410v3 Announce Type: replace 
Abstract: Concerning Version 1 of ``Worst-case Nonparametric Bounds for the Student T-statistic'', arXiv:2508.13226: The main result there is incorrect. Concerning Version 2 of arXiv:2508.13226: At least the proof of the main result there is incorrect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02410v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iosif Pinelis</dc:creator>
    </item>
    <item>
      <title>The Curious Price of Distributional Robustness in Reinforcement Learning with a Generative Model</title>
      <link>https://arxiv.org/abs/2305.16589</link>
      <description>arXiv:2305.16589v3 Announce Type: replace-cross 
Abstract: This paper investigates model robustness in reinforcement learning (RL) to reduce the sim-to-real gap in practice. We adopt the framework of distributionally robust Markov decision processes (RMDPs), aimed at learning a policy that optimizes the worst-case performance when the deployed environment falls within a prescribed uncertainty set around the nominal MDP. Despite recent efforts, the sample complexity of RMDPs remained mostly unsettled regardless of the uncertainty set in use. It was unclear if distributional robustness bears any statistical consequences when benchmarked against standard RL. Assuming access to a generative model that draws samples based on the nominal MDP, we provide a near-optimal characterization of the sample complexity of RMDPs when the uncertainty set is specified via either the total variation (TV) distance or chi-squared divergence. The algorithm studied here is a model-based method called distributionally robust value iteration, which is shown to be near-optimal for the full range of uncertainty levels. Somewhat surprisingly, our results uncover that RMDPs are not necessarily easier or harder to learn than standard MDPs. The statistical consequence incurred by the robustness requirement depends heavily on the size and shape of the uncertainty set: in the case w.r.t.~the TV distance, the minimax sample complexity of RMDPs is always smaller than that of standard MDPs; in the case w.r.t.~the chi-squared divergence, the sample complexity of RMDPs far exceeds the standard MDP counterpart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.16589v3</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laixi Shi, Gen Li, Yuting Wei, Yuxin Chen, Matthieu Geist, Yuejie Chi</dc:creator>
    </item>
    <item>
      <title>On Rate-Optimal Partitioning Classification from Observable and from Privatised Data</title>
      <link>https://arxiv.org/abs/2312.14889</link>
      <description>arXiv:2312.14889v3 Announce Type: replace-cross 
Abstract: In this paper we revisit the classical method of partitioning classification and study its convergence rate under relaxed conditions, both for observable (non-privatised) and for privatised data. We consider the problem of classification in a $d$ dimensional Euclidean space. Previous results on the partitioning classifier worked with the strong density assumption, which is restrictive, as we demonstrate through simple examples. Here, we study the problem under much milder assumptions. We presuppose that the distribution of the inputs is a mixture of an absolutely continuous and a discrete distribution, such that the absolutely continuous component is concentrated to a $d_a$ dimensional subspace. In addition to the standard Lipschitz and margin conditions, a novel characteristic of the absolutely continuous component is introduced, by which the exact convergence rate of the classification error probability is computed, both for the binary and for the multi-label cases. Interestingly, this rate of convergence depends only on the intrinsic dimension of the inputs, $d_a$. The privacy constraints mean that the independent identically distributed data cannot be directly observed, and the classifiers are functions of the randomised outcome of a suitable local differential privacy mechanism. In this paper we add Laplace distributed noises to the discontinuations of all possible locations of the feature vector and to its label. Again, tight upper bounds on the rate of convergence of the classification error probability are derived, without the strong density assumption, such that this rate depends on $2d_a$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14889v3</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bal\'azs Csan\'ad Cs\'aji, L\'aszl\'o Gy\"orfi, Ambrus Tam\'as, Harro Walk</dc:creator>
    </item>
    <item>
      <title>Estimation of Integrated Volatility Functionals with Kernel Spot Volatility Estimators</title>
      <link>https://arxiv.org/abs/2407.09759</link>
      <description>arXiv:2407.09759v3 Announce Type: replace-cross 
Abstract: For a multidimensional It\^o semimartingale, we consider the problem of estimating integrated volatility functionals. Jacod and Rosenbaum (2013) studied a plug-in type of estimator based on a Riemann sum approximation of the integrated functional and a spot volatility estimator with a forward uniform kernel. Motivated by recent results that show that spot volatility estimators with general two-side kernels of unbounded support are more accurate, in this paper, an estimator using a general kernel spot volatility estimator as the plug-in is considered. A biased central limit theorem for estimating the integrated functional is established with an optimal convergence rate. Unbiased central limit theorems for estimators with proper de-biasing terms are also obtained both at the optimal convergence regime for the bandwidth and when applying undersmoothing. Our results show that one can significantly reduce the estimator's bias by adopting a general kernel instead of the standard uniform kernel. Our proposed bias-corrected estimators are found to maintain remarkable robustness against bandwidth selection in a variety of sampling frequencies and functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09759v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jos\'e E. Figueroa-L\'opez, Jincheng Pang, Bei Wu</dc:creator>
    </item>
    <item>
      <title>Flow-based generative models as iterative algorithms in probability space</title>
      <link>https://arxiv.org/abs/2502.13394</link>
      <description>arXiv:2502.13394v2 Announce Type: replace-cross 
Abstract: Generative AI (GenAI) has revolutionized data-driven modeling by enabling the synthesis of high-dimensional data across various applications, including image generation, language modeling, biomedical signal processing, and anomaly detection. Flow-based generative models provide a powerful framework for capturing complex probability distributions, offering exact likelihood estimation, efficient sampling, and deterministic transformations between distributions. These models leverage invertible mappings governed by Ordinary Differential Equations (ODEs), enabling precise density estimation and likelihood evaluation. This tutorial presents an intuitive mathematical framework for flow-based generative models, formulating them as neural network-based representations of continuous probability densities. We explore key theoretical principles, including the Wasserstein metric, gradient flows, and density evolution governed by ODEs, to establish convergence guarantees and bridge empirical advancements with theoretical insights. By providing a rigorous yet accessible treatment, we aim to equip researchers and practitioners with the necessary tools to effectively apply flow-based generative models in signal processing and machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13394v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yao Xie, Xiuyuan Cheng</dc:creator>
    </item>
    <item>
      <title>On the rate of convergence of estimating the Hurst parameter of rough stochastic volatility models</title>
      <link>https://arxiv.org/abs/2504.09276</link>
      <description>arXiv:2504.09276v2 Announce Type: replace-cross 
Abstract: In [Han \&amp; Schied, 2023, \textit{arXiv 2307.02582}], an easily computable scale-invariant estimator $\widehat{\mathscr{R}}^s_n$ was constructed to estimate the Hurst parameter of the drifted fractional Brownian motion $X$ from its antiderivative. This paper extends this convergence result by proving that $\widehat{\mathscr{R}}^s_n$ also consistently estimates the Hurst parameter when applied to the antiderivative of $g \circ X$ for a general nonlinear function $g$. We also establish an almost sure rate of convergence in this general setting. Our result applies, in particular, to the estimation of the Hurst parameter of a wide class of rough stochastic volatility models from discrete observations of the integrated variance, including the rough fractional stochastic volatility model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09276v2</guid>
      <category>q-fin.ST</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>q-fin.MF</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiyue Han, Alexander Schied</dc:creator>
    </item>
    <item>
      <title>Potential Contrast: Properties, Equivalences, and Generalization to Multiple Classes</title>
      <link>https://arxiv.org/abs/2505.01388</link>
      <description>arXiv:2505.01388v2 Announce Type: replace-cross 
Abstract: Potential contrast is typically used as an image quality measure and quantifies the maximal possible contrast between samples from two classes of pixels in an image after an arbitrary grayscale transformation. It has been applied in cultural heritage to evaluate multispectral images using a small number of labeled pixels. In this work, we introduce a normalized version of potential contrast that removes dependence on image format and also prove equalities that enable generalization to more than two classes and to continuous settings. Finally, we exemplify the utility of multi-class normalized potential contrast through an application to a medieval music manuscript with visible bleedthrough from the back of the page. We share our implementations, based on both original algorithms and our new equalities, including generalization to multiple classes, at https://github.com/wallacepeaslee/Multiple-Class-Normalized-Potential-Contrast.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01388v2</guid>
      <category>eess.IV</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wallace Peaslee, Anna Breger, Carola-Bibiane Sch\"onlieb</dc:creator>
    </item>
    <item>
      <title>One-dimensional quantile-stratified sampling and its application in statistical simulations</title>
      <link>https://arxiv.org/abs/2506.07437</link>
      <description>arXiv:2506.07437v2 Announce Type: replace-cross 
Abstract: In this paper we examine quantile-stratified samples from a known univariate probability distribution, with stratification occurring over a partition of the quantile regions in the distribution. We examine some general properties of this sampling method and we contrast it with standard IID sampling to highlight its similarities and differences. We examine the applications of this sampling method to various statistical simulations including importance sampling. We conduct simulation analysis to compare the performance of standard importance sampling against the quantile-stratified importance sampling to see how they each perform on a range of functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07437v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ben O'Neill</dc:creator>
    </item>
    <item>
      <title>The exact distribution of the conditional likelihood-ratio test in instrumental variables regression</title>
      <link>https://arxiv.org/abs/2509.04144</link>
      <description>arXiv:2509.04144v2 Announce Type: replace-cross 
Abstract: We derive the exact asymptotic distribution of the conditional likelihood-ratio test in instrumental variables regression under weak instrument asymptotics and for multiple endogenous variables. The distribution is conditional on all eigenvalues of the concentration matrix, rather than only the smallest eigenvalue as in an existing asymptotic upper bound. This exact characterization leads to a substantially more powerful test if there are differently identified endogenous variables. We provide computational methods implementing the test and demonstrate the power gains through numerical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04144v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Malte Londschien</dc:creator>
    </item>
  </channel>
</rss>
