<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 31 Dec 2024 05:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Fitting Dynamically Misspecified Models: An Optimal Transportation Approach</title>
      <link>https://arxiv.org/abs/2412.20204</link>
      <description>arXiv:2412.20204v1 Announce Type: new 
Abstract: This paper considers filtering, parameter estimation, and testing for potentially dynamically misspecified state-space models. When dynamics are misspecified, filtered values of state variables often do not satisfy model restrictions, making them hard to interpret, and parameter estimates may fail to characterize the dynamics of filtered variables. To address this, a sequential optimal transportation approach is used to generate a model-consistent sample by mapping observations from a flexible reduced-form to the structural conditional distribution iteratively. Filtered series from the generated sample are model-consistent. Specializing to linear processes, a closed-form Optimal Transport Filtering algorithm is derived. Minimizing the discrepancy between generated and actual observations defines an Optimal Transport Estimator. Its large sample properties are derived. A specification test determines if the model can reproduce the sample path, or if the discrepancy is statistically significant. Empirical applications to trend-cycle decomposition, DSGE models, and affine term structure models illustrate the methodology and the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20204v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean-Jacques Forneron, Zhongjun Qu</dc:creator>
    </item>
    <item>
      <title>Automated Demand Forecasting in small to medium-sized enterprises</title>
      <link>https://arxiv.org/abs/2412.20420</link>
      <description>arXiv:2412.20420v1 Announce Type: new 
Abstract: In response to the growing demand for accurate demand forecasts, this research proposes a generalized automated sales forecasting pipeline tailored for small- to medium-sized enterprises (SMEs). Unlike large corporations with dedicated data scientists for sales forecasting, SMEs often lack such resources. To address this, we developed a comprehensive forecasting pipeline that automates time series sales forecasting, encompassing data preparation, model training, and selection based on validation results.
  The development included two main components: model preselection and the forecasting pipeline. In the first phase, state-of-the-art methods were evaluated on a showcase dataset, leading to the selection of ARIMA, SARIMAX, Holt-Winters Exponential Smoothing, Regression Tree, Dilated Convolutional Neural Networks, and Generalized Additive Models. An ensemble prediction of these models was also included. Long-Short-Term Memory (LSTM) networks were excluded due to suboptimal prediction accuracy, and Facebook Prophet was omitted for compatibility reasons.
  In the second phase, the proposed forecasting pipeline was tested with SMEs in the food and electric industries, revealing variable model performance across different companies. While one project-based company derived no benefit, others achieved superior forecasts compared to naive estimators.
  Our findings suggest that no single model is universally superior. Instead, a diverse set of models, when integrated within an automated validation framework, can significantly enhance forecasting accuracy for SMEs. These results emphasize the importance of model diversity and automated validation in addressing the unique needs of each business. This research contributes to the field by providing SMEs access to state-of-the-art sales forecasting tools, enabling data-driven decision-making and improving operational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20420v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Gaertner, Christoph Lippert, Stefan Konigorski</dc:creator>
    </item>
    <item>
      <title>Econometric Analysis of Pandemic Disruption and Recovery Trajectory in the U.S. Rail Freight Industry</title>
      <link>https://arxiv.org/abs/2412.20669</link>
      <description>arXiv:2412.20669v1 Announce Type: new 
Abstract: To measure the impacts on U.S. rail and intermodal freight by economic disruptions of the 2007-09 Great Recession and the COVID-19 pandemic, this paper uses time series analysis with the AutoRegressive Integrated Moving Average (ARIMA) family of models and covariates to model intermodal and commodity-specific rail freight volumes based on pre-disruption data. A framework to construct scenarios and select parameters and variables is demonstrated. By comparing actual freight volumes during the disruptions against three counterfactual scenarios, Trend Continuation, Covariate-adapted Trend Continuation, and Full Covariate-adapted Prediction, the characteristics and differences in magnitude and timing between the two disruptions and their effects across nine freight components are examined.
  Results show the disruption impacts differ from measurement by simple comparison with pre-disruption levels or year-on-year comparison depending on the structural trend and seasonal pattern. Recovery Pace Plots are introduced to support comparison in recovery speeds across freight components. Accounting for economic variables helps improve model fitness. It also enables evaluation of the change in association between freight volumes and covariates, where intermodal freight was found to respond more slowly during the pandemic, potentially due to supply constraint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20669v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Max T. M. Ng, Hani S. Mahmassani, Joseph L. Schofer</dc:creator>
    </item>
    <item>
      <title>Causal Hangover Effects</title>
      <link>https://arxiv.org/abs/2412.21181</link>
      <description>arXiv:2412.21181v1 Announce Type: new 
Abstract: It's not unreasonable to think that in-game sporting performance can be affected partly by what takes place off the court. We can't observe what happens between games directly. Instead, we proxy for the possibility of athletes partying by looking at play following games in party cities. We are interested to see if teams exhibit a decline in performance the day following a game in a city with active nightlife; we call this a "hangover effect". Part of the question is determining a reasonable way to measure levels of nightlife, and correspondingly which cities are notorious for it; we colloquially refer to such cities as "party cities". To carry out this study, we exploit data on bookmaker spreads: the expected score differential between two teams after conditioning on observable performance in past games and expectations about the upcoming game. We expect a team to meet the spread half the time, since this is one of the easiest ways for bookmakers to guarantee a profit. We construct a model which attempts to estimate the causal effect of visiting a "party city" on subsequent day performance as measured by the odds of beating the spread. In particular, we only consider the hangover effect on games played back-to-back within 24 hours of each other. To the extent that odds of beating the spread against next day opponent is uncorrelated with playing in a party city the day before, which should be the case under an efficient betting market, we have identification in our variable of interest. We find that visiting a city with active nightlife the day prior to a game does have a statistically significant negative effect on a team's likelihood of meeting bookmakers' expectations for both NBA and MLB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21181v1</guid>
      <category>econ.EM</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andreas Santucci, Eric Lax</dc:creator>
    </item>
    <item>
      <title>Debiased Nonparametric Regression for Statistical Inference and Distributionally Robustness</title>
      <link>https://arxiv.org/abs/2412.20173</link>
      <description>arXiv:2412.20173v1 Announce Type: cross 
Abstract: This study proposes a debiasing method for smooth nonparametric estimators. While machine learning techniques such as random forests and neural networks have demonstrated strong predictive performance, their theoretical properties remain relatively underexplored. Specifically, many modern algorithms lack assurances of pointwise asymptotic normality and uniform convergence, which are critical for statistical inference and robustness under covariate shift and have been well-established for classical methods like Nadaraya-Watson regression. To address this, we introduce a model-free debiasing method that guarantees these properties for smooth estimators derived from any nonparametric regression approach. By adding a correction term that estimates the conditional expected residual of the original estimator, or equivalently, its estimation error, we obtain a debiased estimator with proven pointwise asymptotic normality, uniform convergence, and Gaussian process approximation. These properties enable statistical inference and enhance robustness to covariate shift, making the method broadly applicable to a wide range of nonparametric regression problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20173v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>The Effect of Omitted Variables on the Sign of Regression Coefficients</title>
      <link>https://arxiv.org/abs/2208.00552</link>
      <description>arXiv:2208.00552v3 Announce Type: replace 
Abstract: We show that, depending on how the impact of omitted variables is measured, it can be substantially easier for omitted variables to flip coefficient signs than to drive them to zero. This behavior occurs with "Oster's delta" (Oster 2019), a widely reported robustness measure. Consequently, any time this measure is large -- suggesting that omitted variables may be unimportant -- a much smaller value reverses the sign of the parameter of interest. We propose a modified measure of robustness to address this concern. We illustrate our results in four empirical applications and two meta-analyses. We implement our methods in the companion Stata module regsensitivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.00552v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew A. Masten, Alexandre Poirier</dc:creator>
    </item>
    <item>
      <title>Clustered Covariate Regression</title>
      <link>https://arxiv.org/abs/2302.09255</link>
      <description>arXiv:2302.09255v3 Announce Type: replace 
Abstract: High covariate dimensionality is increasingly occurrent in model estimation, and existing techniques to address this issue typically require sparsity or discrete heterogeneity of the \emph{unobservable} parameter vector. However, neither restriction may be supported by economic theory in some empirical contexts, leading to severe bias and misleading inference. The clustering-based grouped parameter estimator (GPE) introduced in this paper drops both restrictions and maintains the natural one that the parameter support be bounded. GPE exhibits robust large sample properties under standard conditions and accommodates both sparse and non-sparse parameters whose support can be bounded away from zero. Extensive Monte Carlo simulations demonstrate the excellent performance of GPE in terms of bias reduction and size control compared to competing estimators. An empirical application of GPE to estimating price and income elasticities of demand for gasoline highlights its practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09255v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.2139/ssrn.3394012</arxiv:DOI>
      <dc:creator>Abdul-Nasah Soale, Emmanuel Selorm Tsyawo</dc:creator>
    </item>
    <item>
      <title>The Fragility of Sparsity</title>
      <link>https://arxiv.org/abs/2311.02299</link>
      <description>arXiv:2311.02299v3 Announce Type: replace 
Abstract: We show, using three empirical applications, that linear regression estimates which rely on the assumption of sparsity are fragile in two ways. First, we document that different choices of the regressor matrix that do not impact ordinary least squares (OLS) estimates, such as the choice of baseline category with categorical controls, can move sparsity-based estimates two standard errors or more. Second, we develop two tests of the sparsity assumption based on comparing sparsity-based estimators with (OLS). The tests tend to reject the sparsity assumption in all three applications. Unless the number of regressors is comparable to or exceeds the sample size, (OLS) yields more robust results at little efficiency cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02299v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michal Koles\'ar, Ulrich K. M\"uller, Sebastian T. Roelsgaard</dc:creator>
    </item>
    <item>
      <title>Nonparametric Regression under Cluster Sampling</title>
      <link>https://arxiv.org/abs/2403.04766</link>
      <description>arXiv:2403.04766v2 Announce Type: replace 
Abstract: This paper develops a general asymptotic theory for nonparametric kernel regression in the presence of cluster dependence. We examine nonparametric density estimation, Nadaraya-Watson kernel regression, and local linear estimation. Our theory accommodates growing and heterogeneous cluster sizes. We derive asymptotic conditional bias and variance, establish uniform consistency, and prove asymptotic normality. Our findings reveal that under heterogeneous cluster sizes, the asymptotic variance includes a new term reflecting within-cluster dependence, which is overlooked when cluster sizes are presumed to be bounded. We propose valid approaches for bandwidth selection and inference, introduce estimators of the asymptotic variance, and demonstrate their consistency. In simulations, we verify the effectiveness of the cluster-robust bandwidth selection and show that the derived cluster-robust confidence interval improves the coverage ratio. We illustrate the application of these methods using a policy-targeting dataset in development economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04766v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuya Shimizu</dc:creator>
    </item>
    <item>
      <title>Conditional Rank-Rank Regression</title>
      <link>https://arxiv.org/abs/2407.06387</link>
      <description>arXiv:2407.06387v2 Announce Type: replace 
Abstract: Rank-rank regression is commonly employed in economic research as a way of capturing the relationship between two economic variables. It frequently features in studies of intergenerational mobility as the resulting coefficient, capturing the rank correlation between the variables, is easy to interpret and measures overall persistence. However, in many applications it is common practice to include other covariates to account for differences in persistence levels between groups defined by the values of these covariates. In these instances the resulting coefficients can be difficult to interpret. We propose the conditional rank-rank regression, which uses conditional ranks instead of unconditional ranks, to measure average within-group income persistence. The difference between conditional and unconditional rank-rank regression coefficients can then be interpreted as a measure of between-group persistence. We develop a flexible estimation approach using distribution regression and establish a theoretical framework for large sample inference. An empirical study on intergenerational income mobility in Switzerland demonstrates the advantages of this approach. The study reveals stronger intergenerational persistence between fathers and sons compared to fathers and daughters, with the within-group persistence explaining 62% of the overall income persistence for sons and 52% for daughters. Smaller families and those with highly educated fathers exhibit greater persistence in economic status.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06387v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Iv\'an Fern\'andez-Val, Jonas Meier, Aico van Vuuren, Francis Vella</dc:creator>
    </item>
    <item>
      <title>Difference-in-differences with as few as two cross-sectional units -- A new perspective to the democracy-growth debate</title>
      <link>https://arxiv.org/abs/2408.13047</link>
      <description>arXiv:2408.13047v3 Announce Type: replace 
Abstract: Pooled panel analyses often mask heterogeneity in unit-specific treatment effects. This challenge, for example, crops up in studies of the impact of democracy on economic growth, where findings vary substantially due to differences in country composition. To address this challenge, this paper introduces a Difference-in-Differences (DiD) estimator that leverages the temporal dimension of the data to estimate unit-specific average treatment effects on the treated (ATT) with as few as two cross-sectional units. Under weak identification and temporal dependence conditions, the proposed DiD estimator is shown to be asymptotically normal. The method is further complemented with an identification test that, unlike pre-trends tests, is more powerful and can detect violations of parallel trends in post-treatment periods. Empirical results using the DiD estimator suggest Benin's economy would have been 6.3% smaller on average over the 1993-2018 period had she not democratised.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13047v3</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gilles Koumou, Emmanuel Selorm Tsyawo</dc:creator>
    </item>
    <item>
      <title>An Adversarial Approach to Identification</title>
      <link>https://arxiv.org/abs/2411.04239</link>
      <description>arXiv:2411.04239v2 Announce Type: replace 
Abstract: We introduce a new framework for characterizing identified sets of structural and counterfactual parameters in econometric models. By reformulating the identification problem as a set membership question, we leverage the separating hyperplane theorem in the space of observed probability measures to characterize the identified set through the zeros of a discrepancy function with an adversarial game interpretation. The set can be a singleton, resulting in point identification. A feature of many econometric models, with or without distributional assumptions on the error terms, is that the probability measure of observed variables can be expressed as a linear transformation of the probability measure of latent variables. This structure provides a unifying framework and facilitates computation and inference via linear programming. We demonstrate the versatility of our approach by applying it to nonlinear panel models with fixed effects, with parametric and nonparametric error distributions, and across various exogeneity restrictions, including strict and sequential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04239v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Irene Botosaru, Isaac Loh, Chris Muris</dc:creator>
    </item>
    <item>
      <title>From Replications to Revelations: Heteroskedasticity-Robust Inference</title>
      <link>https://arxiv.org/abs/2411.14763</link>
      <description>arXiv:2411.14763v2 Announce Type: replace 
Abstract: Analysing the Stata regression commands from 4,420 reproduction packages of leading economic journals, we find that, among the 40,571 regressions specifying heteroskedasticity-robust standard errors, 98.1% adhere to Stata's default HC1 specification. We then compare several heteroskedasticity-robust inference methods with a large-scale Monte Carlo study based on regressions from 155 reproduction packages. Our results show that t-tests based on HC1 or HC2 with default degrees of freedom exhibit substantial over-rejection. Inference methods with customized degrees of freedom, as proposed by Bell and McCaffrey (2002), Hansen (2024), and a novel approach based on partial leverages, perform best. Additionally, we provide deeper insights into the role of leverages and partial leverages across different inference methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14763v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Kranz</dc:creator>
    </item>
  </channel>
</rss>
