<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Dec 2025 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A proof-of-principle experiment on the spontaneous symmetry breaking machine and numerical estimation of its performance on the $K_{2000}$ benchmark problem</title>
      <link>https://arxiv.org/abs/2512.17922</link>
      <description>arXiv:2512.17922v1 Announce Type: new 
Abstract: In a previous paper, we proposed a unique physically implemented type simulator for combinatorial optimization problems, called the spontaneous symmetry breaking machine (SSBM). In this paper, we first report the results of experimental verification of SSBM using a small-scale benchmark system, and then describe numerical simulations using the benchmark problems (K2000) conducted to confirm its usefulness for large-scale problems. From 1000 samples with different initial fluctuations, it became clear that SSBM can explore a single extremely stable state. This is based on the principle of a phenomenon used in SSBM, and could be a notable advantage over other simulators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17922v1</guid>
      <category>math.OC</category>
      <category>nlin.AO</category>
      <category>physics.optics</category>
      <category>quant-ph</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toshiya Sato, Takashi Goh</dc:creator>
    </item>
    <item>
      <title>Learning Generalized Nash Equilibria in Non-Monotone Games with Quadratic Costs</title>
      <link>https://arxiv.org/abs/2512.18330</link>
      <description>arXiv:2512.18330v1 Announce Type: new 
Abstract: We study generalized Nash equilibrium (GNE) problems in games with quadratic costs and individual linear equality constraints. Departing from approaches that require strong monotonicity and/or shared constraints, we reformulate the KKT conditions of the (generally non-monotone) games into a tractable convex program whose objective satisfies the Polyak-Lojasiewicz (PL) condition. This PL geometry enables a distributed gradient method over a fixed communication graph with global geometric (linear) convergence to a GNE. When gradient information is unavailable or costly, we further develop a zero-order fully distributed scheme in which each player uses only local cost evaluations and their own constraint residuals. With an appropriate step size policy, the proposed zero-order method converges to a GNE, provided one exists, at rate O(1/t).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18330v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tatiana Tatarenko, Lucas Wey Hacker</dc:creator>
    </item>
    <item>
      <title>Modified Quasi-Newton Method for Nonconvex Multiobjective Optimization Problems with Barzilai-Borwein diagonal matrix</title>
      <link>https://arxiv.org/abs/2512.18348</link>
      <description>arXiv:2512.18348v1 Announce Type: new 
Abstract: This paper addresses the challenge of developing efficient algorithms for large-scale nonconvex multiobjective optimization problems (MOPs). While quasi-Newton methods are effective, their traditional application to MOPs is computationally expensive as they require maintaining and inverting separate Hessian approximations for each objective function. To overcome this limitation, we propose a novel Barzilai-Borwein diagonal-type Quasi-Newton method (BB-DQN). Our key innovation is the use of a single, shared, and modified BB-type matrix, updated iteratively using function and gradient information, to approximate the Hessians of all objectives simultaneously. We theoretically demonstrate that this approximation matrix remains positive definite throughout the iterative process. Furthermore, we establish the global convergence of the BB-DQN method without convexity assumptions and prove its R-linear convergence under mild conditions. Numerical experiments on a diverse set of test problems confirm that BB-DQN outperforms existing methods like M-BFGSMO, achieving superior performance in terms of computational time, iteration count, and reliability, especially for large-scale instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18348v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hua Liu</dc:creator>
    </item>
    <item>
      <title>Studies on the Rao-Nakra Sandwich Beam: Well-Posedness, Dynamics, and Controllability</title>
      <link>https://arxiv.org/abs/2512.18381</link>
      <description>arXiv:2512.18381v1 Announce Type: new 
Abstract: In this work, we investigate the well-posedness, stabilization, and boundary controllability of a linear Rao-Nakra type sandwich beam. The system consists of three coupled equations that represent the longitudinal displacements of the outer layers and the transverse displacement of the composite beam, all of which are coupled with dynamical boundary conditions. In the first problem, time-dependent weights and delays are considered. Then, we establish the existence and uniqueness of solutions for the Cauchy problem associated with the damped system using semigroup theory and a classical result by Kato. Furthermore, employing a Lyapunov-based approach, we prove that the system's energy decays exponentially, despite the presence of time-varying weights and delays. In the second problem, we consider a boundary linear control system and prove its well-posedness. By deriving an observability inequality for the adjoint system and applying the Hilbert Uniqueness Method (HUM), we show that the system is null controllable. A key contribution of this work lies in handling the full three-equation coupled system, which involves significant difficulty due to the dynamic boundary conditions, resolved via appropriately constructed Lyapunov functionals and intermediate observability inequalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18381v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>George J. Bautista, Roberto de A. Capistrano-Filho, Boumediene Chentouf, Oscar Sierra Fonseca, Juan L\'imaco</dc:creator>
    </item>
    <item>
      <title>Prioritized Constraints in Optimization-Based Control</title>
      <link>https://arxiv.org/abs/2512.18458</link>
      <description>arXiv:2512.18458v1 Announce Type: new 
Abstract: We provide theoretical foundations and computational tools for the systematic design of optimization-based control laws with constraints that have different priorities. By introducing the concept of prioritized intersections, we extend and unify previous work on the topic. Moreover, to enable the use of prioritized intersection in real-time applications, we propose an efficient solver for forming such intersections for polyhedral constraints. The solver in question is a tailored implementation of a dual active-set quadratic programming solver that leverages the particular problem structure of the optimization problems arising for prioritized intersections. The method is validated in a real-time MPC application for autonomous driving, where it successfully resolves six different levels of conflicting constraints, confirming its efficiency and practicality for control. Furthermore, we show that the proposed solver outperforms existing solvers for hierarchical quadratic programming, making it relevant beyond control applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18458v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Arnstr\"om, Gianluca Garofalo</dc:creator>
    </item>
    <item>
      <title>Preference-based optimization from noisy pairwise comparisons</title>
      <link>https://arxiv.org/abs/2512.18511</link>
      <description>arXiv:2512.18511v1 Announce Type: new 
Abstract: In interactive systems, feedback is often provided in the form of preference between queried options rather than precise scores, which motivates optimization methods to learn from such comparisons. In this work, we propose a preference-based optimization algorithm that relies on noisy two-point comparisons. At each iteration, the algorithm employs a uniform-sphere perturbation to generate a perturbed action and queries the resulting loss comparison to estimate a descent direction. We demonstrate that, under standard smoothness and bounded variance assumptions, the algorithm converges to a stationary point when the smoothing and step size parameters are properly chosen. Numerical experiments on an LQG system demonstrate the effectiveness of the preference-based optimization algorithm with comparison feedback.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18511v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Siyi Wang, Zifan Wang, Karl Henrik Johanssson</dc:creator>
    </item>
    <item>
      <title>An adaptive adjoint-oriented neural network for solving parametric optimal control problems with singularities</title>
      <link>https://arxiv.org/abs/2512.18548</link>
      <description>arXiv:2512.18548v1 Announce Type: new 
Abstract: In this work, we present an adaptive adjoint-oriented neural network (adaptive AONN) for solving parametric optimal control problems governed by partial differential equations. The proposed method integrates deep adaptive sampling techniques with the adjoint-oriented neural network (AONN) framework. It alleviates the limitations of AONN in handling low-regularity solutions and enhances the generalizability of deep adaptive sampling for surrogate modeling without labeled data ($\text{DAS}^2$). The effectiveness of the adaptive AONN is demonstrated through numerical examples involving singularities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18548v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zikang Yuan, Guanjie Wang, Qifeng Liao</dc:creator>
    </item>
    <item>
      <title>Tight Lower Bounds and Optimal Algorithms for Stochastic Nonconvex Optimization with Heavy-Tailed Noise</title>
      <link>https://arxiv.org/abs/2512.18713</link>
      <description>arXiv:2512.18713v1 Announce Type: new 
Abstract: We study stochastic nonconvex optimization under heavy-tailed noise. In this setting, the stochastic gradients only have bounded $p$--th central moment ($p$--BCM) for some $p \in (1,2]$. Building on the foundational work of Arjevani et al. (2022) in stochastic optimization, we establish tight sample complexity lower bounds for all first-order methods under \emph{relaxed} mean-squared smoothness ($q$-WAS) and $\delta$-similarity ($(q, \delta)$-S) assumptions, allowing any exponent $q \in [1,2]$ instead of the standard $q = 2$. These results substantially broaden the scope of existing lower bounds. To complement them, we show that Normalized Stochastic Gradient Descent with Momentum Variance Reduction (NSGD-MVR), a known algorithm, matches these bounds in expectation. Beyond expectation guarantees, we introduce a new algorithm, Double-Clipped NSGD-MVR, which allows the derivation of high-probability convergence rates under weaker assumptions than previous works. Finally, for second-order methods with stochastic Hessians satisfying bounded $q$-th central moment assumptions for some exponent $q \in [1, 2]$ (allowing $q \neq p$), we establish sharper lower bounds than previous works while improving over Sadiev et al. (2025) (where only $p = q$ is considered) and yielding stronger convergence exponents. Together, these results provide a nearly complete complexity characterization of stochastic nonconvex optimization in heavy-tailed regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18713v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Fradin, Abdurakhmon Sadiev, Laurent Condat, Peter Richt\'arik</dc:creator>
    </item>
    <item>
      <title>Locational Marginal Emissions for Carbon-Aware Data Center Operations in Large-Scale Power Grids</title>
      <link>https://arxiv.org/abs/2512.18819</link>
      <description>arXiv:2512.18819v1 Announce Type: new 
Abstract: Carbon accounting methods for electricity consumption face challenges regarding physical deliverability, double counting, additionality, and impact magnitude. Locational Marginal Emissions (LMEs) show potential to address many of these key issues. However, their use in a large-scale power grids remains understudied. We analyze the properties of LMEs from a data center's perspective in a 1493-bus Western Interconnection over one year of hourly operation. We find that LME characteristics create three distinct regions: the hydropower-dominated Pacific Northwest, with low and stable LMEs; the coal-heavy Intermountain West, containing often high LMEs; and the Sunbelt, where mixed generation leads to variable LMEs correlated with solar output. This characterization provides analytical guidance for data center emission reduction. In particular, LME-guided emission reduction interventions through data center temporal-spatial load shifting, siting, and renewable procurement display over 85% accuracy with respect to actual emission reduction. Moreover, large-scale, nodal grid simulation is shown to be critical to accurate evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18819v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luc Cote, Andy Sun</dc:creator>
    </item>
    <item>
      <title>Bi-Level Optimal Control Framework For Missed-Thrust-Design With First-Order Bounds On Maximum Missed-Thrust-Duration</title>
      <link>https://arxiv.org/abs/2512.18984</link>
      <description>arXiv:2512.18984v1 Announce Type: new 
Abstract: In this paper, we present a bi-level optimal control framework for designing low-thrust spacecraft trajectories with robustness against missed-thrust-events. The upper-level (UL) problem generates a nominal trajectory assuming full control authority, while each lower-level (LL) problem computes the optimal recovery maneuver following a missed-thrust-event along the nominal solution. Under suitable regularity conditions ensuring uniqueness and smoothness of the LL response, the hierarchy admits a single-level reformulation by embedding the LL first-order optimality conditions within the UL constraints. We further establish a robustness certificate, which provides an upper bound on the maximum admissible missed-thrust-duration for which the structural assumptions remain valid for the LL problem. The bound depends explicitly on precomputable dynamical quantities along the nominal solution, enabling rapid evaluation over large ensembles without iterative solves. Numerical experiments show that while the certificate identifies when modeling assumptions are valid, it does not fully characterize recoverability after missed-thrust-events. A finite-horizon controllability-energy analysis is therefore used to interpret recovery beyond the theoretical bounds. Collectively, these results provide a deterministic, certifiable approach for incorporating robustness directly into trajectory design, replacing post-hoc margin allocation techniques with formal guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18984v1</guid>
      <category>math.OC</category>
      <category>math.DS</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amlan Sinha, Ryne Beeson</dc:creator>
    </item>
    <item>
      <title>On-Orbit Servicing-Integrated Maintenance Strategy for Satellite Constellation</title>
      <link>https://arxiv.org/abs/2512.18985</link>
      <description>arXiv:2512.18985v1 Announce Type: new 
Abstract: This paper proposes a maintenance strategy for a satellite constellation that utilizes on-orbit servicing (OOS). Under this strategy, the constellation operator addresses satellite failures in two ways: by deploying new satellites and by recovering failed satellites through OOS. We develop an inventory management model with a parametric replenishment policy for the maintenance process, which can evaluate the performance of the satellite constellation system. Based on this model, we formulate two single-objective optimization problems representing the decision-making contexts of two main stakeholders -- the constellation operator and the OOS provider -- and a bi-objective optimization problem that can reflect the tension between the two. A case study of the OOS-supported maintenance for a real-world-scale constellation provides valuable insights that help explain the behaviors of the stakeholders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18985v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jaewoo Kim, Taehyun Sung, Woonam Hwang, Jaemyung Ahn</dc:creator>
    </item>
    <item>
      <title>Explicit and Non-asymptotic Query Complexities of Rank-Based Zeroth-order Algorithm on Stochastic Smooth Functions</title>
      <link>https://arxiv.org/abs/2512.19104</link>
      <description>arXiv:2512.19104v1 Announce Type: new 
Abstract: Zeroth-order (ZO) optimization with ordinal feedback has emerged as a fundamental problem in modern machine learning systems, particularly in human-in-the-loop settings such as reinforcement learning from human feedback, preference learning, and evolutionary strategies. While rank-based ZO algorithms enjoy strong empirical success and robustness properties, their theoretical understanding, especially under stochastic objectives and standard smoothness assumptions, remains limited. In this paper, we study rank-based zeroth-order optimization for stochastic functions where only ordinal feedback of the stochastic function is available. We propose a simple and computationally efficient rank-based ZO algorithm. Under standard assumptions including smoothness, strong convexity, and bounded second moments of stochastic gradients, we establish explicit non-asymptotic query complexity bounds for both convex and nonconvex objectives. Notably, our results match the best-known query complexities of value-based ZO algorithms, demonstrating that ordinal information alone is sufficient for optimal query efficiency in stochastic settings. Our analysis departs from existing drift-based and information-geometric techniques, offering new tools for the study of rank-based optimization under noise. These findings narrow the gap between theory and practice and provide a principled foundation for optimization driven by human preferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19104v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haishan Ye</dc:creator>
    </item>
    <item>
      <title>Solving Stengle's Example in Rational Arithmetic: Exact Values of the Moment-SOS Relaxations</title>
      <link>https://arxiv.org/abs/2512.19141</link>
      <description>arXiv:2512.19141v1 Announce Type: new 
Abstract: We revisit Stengle's classical univariate polynomial optimization example $min 1 - x^2 s.t. (1 - x^2)^3 \geq 0$ whose constraint description is degenerate at the minimizers. We prove that the moment-SOS hierarchy of relaxation order $r \geq 3$ has the exact value $-1/r(r - 2)$. For this we construct in rational arithmetic a dual polynomial sum-of-squares (SOS) certificate and a primal moment sequence representing a finitely atomic measure. The key ingredients are elementary trigonometric properties of Chebyshev and Gegenbauer polynomial, and a Christoffel-Darboux kernel argument.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19141v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Didier Henrion (LAAS-POP)</dc:creator>
    </item>
    <item>
      <title>A Characterization of Law-Invariant and Coherent Risk Measures through Optimal Transport</title>
      <link>https://arxiv.org/abs/2512.19157</link>
      <description>arXiv:2512.19157v1 Announce Type: new 
Abstract: In this article, we propose a novel characterization of law-invariant and coherent risk measures, based on a generalized optimal transport problem in which the second marginal of the admissible plans is not fixed, but required to lie within a target set of probability measures. One of the main contributions of this work is a general representation formula for such risk measures, which is closely related to Kusuoka's theorem. When the aforementioned target set is convex, our representation result allows for the systematic derivation of general duality formulas. To illustrate our findings, we explicitly compute the target sets associated with several classical law-invariant coherent risk measures, including the prototypical conditional value at risk and higher moment measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19157v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Riccardo Bonalli (L2S, CNRS), Beno\^it Bonnet-Weill (CNRS, L2S), Laurent Pfeiffer (DISCO, L2S)</dc:creator>
    </item>
    <item>
      <title>Finite-sample guarantees for data-driven forward-backward operator methods</title>
      <link>https://arxiv.org/abs/2512.19172</link>
      <description>arXiv:2512.19172v1 Announce Type: new 
Abstract: We establish finite sample certificates on the quality of solutions produced by data-based forward-backward (FB) operator splitting schemes. As frequently happens in stochastic regimes, we consider the problem of finding a zero of the sum of two operators, where one is either unavailable in closed form or computationally expensive to evaluate, and shall therefore be approximated using a finite number of noisy oracle samples. Under the lens of algorithmic stability, we then derive probabilistic bounds on the distance between a true zero and the FB output without making specific assumptions about the underlying data distribution. We show that under weaker conditions ensuring the convergence of FB schemes, stability bounds grow proportionally to the number of iterations. Conversely, stronger assumptions yield stability guarantees that are independent of the iteration count. We then specialize our results to a popular FB stochastic Nash equilibrium seeking algorithm and validate our theoretical bounds on a control problem for smart grids, where the energy price uncertainty is approximated by means of historical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19172v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filippo Fabiani, Barbara Franci</dc:creator>
    </item>
    <item>
      <title>Mean-field optimal control with stochastic leaders</title>
      <link>https://arxiv.org/abs/2512.19201</link>
      <description>arXiv:2512.19201v1 Announce Type: new 
Abstract: We consider interacting agent systems with a large number of stochastic agents (or particles) influenced by a fixed number of external stochastic lead agents. Such examples arise, for example in models of opinion dynamics, where a small number of leaders (influencers) can steer the behaviour of a large population of followers. In this context, we study a partial mean-field limit where the number of followers tends to infinity, while the number of leaders stays constant. The partial mean-field limit dynamics is then given by a McKean-Vlasov stochastic differential equation (SDE) for the followers, coupled to a controlled It\^o-SDE governing the dynamics of the lead agents. For a given cost functional that the lead agents seek to minimise, we show that the unique optimal control of the finite agent system convergences to the optimal control of the limiting system. This establishes that the low-dimensional control of the partial (mean-field) system provides an effective approximation for controlling the high-dimensional finite agent system. In addition, we propose a stochastic gradient descent algorithm that can efficiently approximate the mean-field control. Our theoretical results are illustrated on opinion dynamics model with lead agents, where the control objective is to drive the followers to reach consensus in finite time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19201v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Zimper, Ana Djurdjevac, Carsten Hartmann, Christof Sch\"utte, Nata\v{s}a Djurdjevac Conrad</dc:creator>
    </item>
    <item>
      <title>Make an optimization problem multidisciplinary</title>
      <link>https://arxiv.org/abs/2512.19217</link>
      <description>arXiv:2512.19217v1 Announce Type: new 
Abstract: Despite the abundance of benchmark problems for optimization algorithms, there is a notable scarcity of such problems in multidisciplinary design optimization (MDO). To address this gap, we introduce a novel methodology that enables the transformation of any optimization problem with a known solution into an equivalent MDO problem. This equivalence holds for a large class of coupling functions, including non-linear ones. The proposed methodology exploits a ''link function'' that effectively eliminates the coupling variables from the MDO problem, without influencing the solution. This approach allows for the creation of benchmark problems with reference solutions, facilitating the comparison and evaluation of various MDO algorithms. Moreover, it is adaptable to scalable optimization problems, where the dimensions of the search and constraint spaces can be configured. We also present a variant tailored to linear coupling functions with constant coefficients sampled independently at random, for which we derive a closed-form solution to the coupling equations. For the sake of illustration, we put our approach into action on a multidimensional Rosenbrock problem, varying the number of disciplines and design variable sizes. This example showcases the versatility and applicability of our methodology in generating benchmark problems for MDO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19217v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias De Lozzo (INSA, EPE UT, IMT), Olivier Roustant (INSA, EPE UT, IMT), Amine Aziz-Alaoui</dc:creator>
    </item>
    <item>
      <title>An alternative approach to well-posedness of McKean-Vlasov equations arising in Consensus-Based Optimization</title>
      <link>https://arxiv.org/abs/2512.19446</link>
      <description>arXiv:2512.19446v1 Announce Type: new 
Abstract: In this work we study the mean-field description of Consensus-Based Optimization (CBO), a derivative-free particle optimization method. Such a description is provided by a non-local SDE of McKean-Vlasov type, whose fields lack of global Lipschitz continuity. We propose a novel approach to prove the well-posedness of the mean-field CBO equation based on a truncation argument. The latter is performed through the introduction of a cut-off function, defined on the space of probability measures, acting on the fields. This procedure allows us to study the well-posedness problem in the classical framework of Sznitman. Through this argument, we recover the established result on the existence of strong solutions, and we extend the class of solutions for which pathwise uniqueness holds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19446v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <category>math.PR</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessandro Baldi</dc:creator>
    </item>
    <item>
      <title>Spectral Shinkage of Gaussian Entropic Optimal Transport</title>
      <link>https://arxiv.org/abs/2512.19457</link>
      <description>arXiv:2512.19457v1 Announce Type: new 
Abstract: We present a functional calculus treatment of Entropic Optimal Transport (EOT) between Gaussian measures on separable Hilbert spaces, providing a unified framework that handles infinite-dimensional degeneracy. By leveraging the notion of proper alignment and the Schur complement, we reveal that the Gaussian EOT solution operates as a precise \textit{spectral shrinkage}: the optimal coupling is uniquely determined by contracting the spectrum of the correlation operator via a universal scalar function. This geometric insight facilitates an algorithmic shift from iterative fixed-point schemes (e.g., Sinkhorn) to direct algebraic computation, enabling efficient multi-scale analysis, where a single spectral decomposition allows for the exact evaluation of entropic costs across arbitrary regularization parameters $\varepsilon &gt; 0$ at negligible additional cost. Furthermore, we investigate the asymptotic behavior as $\varepsilon \downarrow 0$ in settings where the unregularized Optimal Transport problem admits non-unique solutions. We establish a selection principle that the regularized limit converges to the most diffusive optimal coupling --characterized as the centroid of the convex set of optimal Kantorovich plans. This demonstrates that in degenerate regimes, the entropic limit systematically rejects deterministic Monge solutions (extremal points) in favor of the optimal solution with minimal Hilbert-Schmidt correlation, effectively filtering out spurious correlations in the null space. Finally, we derive stability bounds and convergence rates, recovering established parametric rates ($\varepsilon \log(1/\varepsilon)$) in finite dimensions while identifying distinct non-parametric rates dependent on spectral decay in infinite-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19457v1</guid>
      <category>math.OC</category>
      <category>math.FA</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ho Yun</dc:creator>
    </item>
    <item>
      <title>A perturbed preconditioned gradient descent method for the unconstrained minimization of composite objectives</title>
      <link>https://arxiv.org/abs/2512.19532</link>
      <description>arXiv:2512.19532v1 Announce Type: new 
Abstract: We introduce a perturbed preconditioned gradient descent (PPGD) method for the unconstrained minimization of a strongly convex objective $G$ with a locally Lipschitz continuous gradient. We assume that $G(v)=E(v)+F(v)$ and that the gradient of $F$ is only known approximately. Our analysis is conducted in infinite dimensions with a preconditioner built into the framework. We prove a linear rate of convergence, up to an error term dependent on the gradient approximation. We apply the PPGD to the stationary Cahn-Hilliard equations with variable mobility under periodic boundary conditions. Numerical experiments are presented to validate the theoretical convergence rates and explore how the mobility affects the computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19532v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jea-Hyun Park, Abner J. Salgado, Steven M. Wise</dc:creator>
    </item>
    <item>
      <title>Scenario Reduction for the Two-Stage Stochastic Unit Commitment Problem</title>
      <link>https://arxiv.org/abs/2512.19614</link>
      <description>arXiv:2512.19614v1 Announce Type: new 
Abstract: The two-stage stochastic unit commitment problem has become an important tool to support decision-making under uncertainty in power systems. Representing the uncertainty by a large number of scenarios guarantees accurate results but challenges the solution process. One way to overcome this is by using scenario reduction methods, which aim at finding a distribution supported on fewer scenarios, but leading to similar optimal first-stage decisions. In this paper, we recap the classical scenario reduction theory based on the distance of probability distributions and the optimal mass transportation problem. We then review and compare various formulations of the underlying cost function of the latter used in the literature. Using the Forward Selection Algorithm, we show that a specific formulation of the cost function can be proven to select the best possible scenario from a given sample on the first draw with respect to the Relative Approximation Error. We demonstrate this result and compare the quality of the approximation as well as the computational performance of the different cost functions using a modified version of the IEEE RTS 24-Bus System. In many cases, we find that the optimal solution of the two-stage stochastic unit commitment problem with 200 scenarios can be approximated with around 2% scenarios when using this cost function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19614v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yannick Werner, Juan Miguel Morales, Salvador Pineda, Line Roald, Sonja Wogrin</dc:creator>
    </item>
    <item>
      <title>Risk-Aware Financial Forecasting Enhanced by Machine Learning and Intuitionistic Fuzzy Multi-Criteria Decision-Making</title>
      <link>https://arxiv.org/abs/2512.17936</link>
      <description>arXiv:2512.17936v1 Announce Type: cross 
Abstract: In the face of increasing financial uncertainty and market complexity, this study presents a novel risk-aware financial forecasting framework that integrates advanced machine learning techniques with intuitionistic fuzzy multi-criteria decision-making (MCDM). Tailored to the BIST 100 index and validated through a case study of a major defense company in T\"urkiye, the framework fuses structured financial data, unstructured text data, and macroeconomic indicators to enhance predictive accuracy and robustness. It incorporates a hybrid suite of models, including extreme gradient boosting (XGBoost), long short-term memory (LSTM) network, graph neural network (GNN), to deliver probabilistic forecasts with quantified uncertainty. The empirical results demonstrate high forecasting accuracy, with a net profit mean absolute percentage error (MAPE) of 3.03% and narrow 95% confidence intervals for key financial indicators. The risk-aware analysis indicates a favorable risk-return profile, with a Sharpe ratio of 1.25 and a higher Sortino ratio of 1.80, suggesting relatively low downside volatility and robust performance under market fluctuations. Sensitivity analysis shows that the key financial indicator predictions are highly sensitive to variations of inflation, interest rates, sentiment, and exchange rates. Additionally, using an intuitionistic fuzzy MCDM approach, combining entropy weighting, evaluation based on distance from the average solution (EDAS), and the measurement of alternatives and ranking according to compromise solution (MARCOS) methods, the tabular data learning network (TabNet) outperforms the other models and is identified as the most suitable candidate for deployment. Overall, the findings of this work highlight the importance of integrating advanced machine learning, risk quantification, and fuzzy MCDM methodologies in financial forecasting, particularly in emerging markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17936v1</guid>
      <category>q-fin.ST</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Safiye Turgay, Serkan Erdo\u{g}an, \v{Z}eljko Stevi\'c, Orhan Emre Elma, Tevfik Eren, Zhiyuan Wang, Mahmut Bayda\c{s}</dc:creator>
    </item>
    <item>
      <title>Robustness of Delayed Higher Order Sliding Mode Control</title>
      <link>https://arxiv.org/abs/2512.18018</link>
      <description>arXiv:2512.18018v1 Announce Type: cross 
Abstract: In this paper, the feasibility of recently developed higher order delayed sliding mode controllers is addressed. With this aim the robustness against the measurement noise and mismatched perturbations for the systems governed by such controllers is established using ISS implicit Lyapunov-Razumikhin function approach. To illustrate proposed results, a simulation example validating the efficiency of the method is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18018v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moussa Labbadi, Denis Efimov, Leonid Fridman</dc:creator>
    </item>
    <item>
      <title>Optimization of Si/SiGe Heterostructures for Large and Robust Valley Splitting in Silicon Qubits</title>
      <link>https://arxiv.org/abs/2512.18064</link>
      <description>arXiv:2512.18064v1 Announce Type: cross 
Abstract: The notoriously low and fluctuating valley splitting is one of the key challenges for electron spin qubits in silicon (Si), limiting the scalability of Si-based quantum processors. In silicon-germanium (SiGe) heterostructures, the problem can be addressed by the design of the epitaxial layer stack. Several heuristic strategies have been proposed to enhance the energy gap between the two nearly degenerate valley states in strained Si/SiGe quantum wells (QWs), e.g., sharp Si/SiGe interfaces, Ge spikes or oscillating Ge concentrations within the QW. In this work, we develop a systematic variational optimization approach to compute optimal Ge concentration profiles that boost selected properties of the intervalley coupling matrix element. Our free-shape optimization approach is augmented by a number of technological constraints to ensure feasibility of the resulting epitaxial profiles. The method is based on an effective-mass-type envelope-function theory accounting for the effects of strain and compositional alloy disorder. Various previously proposed heterostructure designs are recovered as special cases of the constrained optimization problem. Our main result is a novel heterostructure design we refer to as the "modulated wiggle well," which provides a large deterministic enhancement of the valley splitting along with a reliable suppression of the disorder-induced volatility. In addition, our new design offers a wide-range tunability of the valley splitting ranging from about 200 $\mu$eV to above 1 meV controlled by the vertical electric field, which offers new perspectives to engineer switchable qubits with on-demand adjustable valley splitting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18064v1</guid>
      <category>cond-mat.mes-hall</category>
      <category>math.OC</category>
      <category>physics.app-ph</category>
      <category>quant-ph</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abel Thayil, Lasse Ermoneit, Lars R. Schreiber, Thomas Koprucki, Markus Kantner</dc:creator>
    </item>
    <item>
      <title>Alternating Minimization for Time-Shifted Synergy Extraction in Human Hand Coordination</title>
      <link>https://arxiv.org/abs/2512.18206</link>
      <description>arXiv:2512.18206v1 Announce Type: cross 
Abstract: Identifying motor synergies -- coordinated hand joint patterns activated at task-dependent time shifts -- from kinematic data is central to motor control and robotics. Existing two-stage methods first extract candidate waveforms (via SVD) and then select shifted templates using sparse optimization, requiring at least two datasets and complicating data collection. We introduce an optimization-based framework that jointly learns a small set of synergies and their sparse activation coefficients. The formulation enforces group sparsity for synergy selection and element-wise sparsity for activation timing. We develop an alternating minimization method in which coefficient updates decouple across tasks and synergy updates reduce to regularized least-squares problems. Our approach requires only a single data set, and simulations show accurate velocity reconstruction with compact, interpretable synergies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18206v1</guid>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Trevor Stepp, Parthan Olikkal, Ramana Vinjamuri, Rajasekhar Anguluri</dc:creator>
    </item>
    <item>
      <title>FedSUM Family: Efficient Federated Learning Methods under Arbitrary Client Participation</title>
      <link>https://arxiv.org/abs/2512.18275</link>
      <description>arXiv:2512.18275v1 Announce Type: cross 
Abstract: Federated Learning (FL) methods are often designed for specific client participation patterns, limiting their applicability in practical deployments. We introduce the FedSUM family of algorithms, which supports arbitrary client participation without additional assumptions on data heterogeneity. Our framework models participation variability with two delay metrics, the maximum delay $\tau_{\max}$ and the average delay $\tau_{\text{avg}}$. The FedSUM family comprises three variants: FedSUM-B (basic version), FedSUM (standard version), and FedSUM-CR (communication-reduced version). We provide unified convergence guarantees demonstrating the effectiveness of our approach across diverse participation patterns, thereby broadening the applicability of FL in real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18275v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Runze You, Shi Pu</dc:creator>
    </item>
    <item>
      <title>Robust H-infinity control under stochastic requirements: minimizing conditional value-at-risk instead of worst-case performance</title>
      <link>https://arxiv.org/abs/2512.18356</link>
      <description>arXiv:2512.18356v1 Announce Type: cross 
Abstract: Conventional robust $\mathcal H_2/\mathcal H_\infty$ control minimizes the worst-case performance, often leading to a conservative design driven by very rare and somewhat arbitrary parametric configurations. To reduce this conservatism while taking advantage of the stochastic properties of Monte-Carlo sampling and its compatibility with parallel computing, we introduce an alternative paradigm that optimizes the controller with respect to a stochastic criterion, namely the conditional value at risk. We illustrate the potential of this approach on a realistic satellite benchmark, showing that it can significantly improve overall performance by tolerating some degradation in very rare worst-case scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18356v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ervan Kassarian, Francesco Sanfedino, Daniel Alazard, Andrea Marrazza</dc:creator>
    </item>
    <item>
      <title>Towards Guided Descent: Optimization Algorithms for Training Neural Networks At Scale</title>
      <link>https://arxiv.org/abs/2512.18373</link>
      <description>arXiv:2512.18373v1 Announce Type: cross 
Abstract: Neural network optimization remains one of the most consequential yet poorly understood challenges in modern AI research, where improvements in training algorithms can lead to enhanced feature learning in foundation models, order-of-magnitude reductions in training time, and improved interpretability into how networks learn. While stochastic gradient descent (SGD) and its variants have become the de facto standard for training deep networks, their success in these over-parameterized regimes often appears more empirical than principled. This thesis investigates this apparent paradox by tracing the evolution of optimization algorithms from classical first-order methods to modern higher-order techniques, revealing how principled algorithmic design can demystify the training process. Starting from first principles with SGD and adaptive gradient methods, the analysis progressively uncovers the limitations of these conventional approaches when confronted with anisotropy that is representative of real-world data. These breakdowns motivate the exploration of sophisticated alternatives rooted in curvature information: second-order approximation techniques, layer-wise preconditioning, adaptive learning rates, and more. Next, the interplay between these optimization algorithms and the broader neural network training toolkit, which includes prior and recent developments such as maximal update parametrization, learning rate schedules, and exponential moving averages, emerges as equally essential to empirical success. To bridge the gap between theoretical understanding and practical deployment, this paper offers practical prescriptions and implementation strategies for integrating these methods into modern deep learning workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18373v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ansh Nagwekar</dc:creator>
    </item>
    <item>
      <title>Sink Proximity: A Novel Approach for Online Vehicle Dispatch in Ride-hailing</title>
      <link>https://arxiv.org/abs/2512.18501</link>
      <description>arXiv:2512.18501v1 Announce Type: cross 
Abstract: Ride-hailing platforms have a profound impact on urban transportation systems, and their performance largely depends on how intelligently they dispatch vehicles in real time. In this work, we develop a new approach to online vehicle dispatch that strengthens a platform's ability to serve more requests under demand uncertainty. We introduce a novel measure called sink proximity, a network-science-inspired measure that captures how demand and vehicle flows are likely to evolve across the city. By integrating this measure into a shareability-network framework, we design an online dispatch algorithm that naturally considers future network states, without depending on fragile spatiotemporal forecasts. Numerical studies demonstrate that our proposed solution significantly improves the request service rate under peak hours within a receding horizon framework with limited future information available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18501v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ruiting Wang, Jiaman Wu, Fabio Paparella, Scott J. Moura, Marta C. Gonzalez</dc:creator>
    </item>
    <item>
      <title>The Narrow Corridor of Stable Solutions in an Extended Osipov--Lanchester Model with Constant Total Population</title>
      <link>https://arxiv.org/abs/2512.18515</link>
      <description>arXiv:2512.18515v1 Announce Type: cross 
Abstract: This paper considers a modification of the classical Osipov--Lanchester model in which the total population of the two forces $N=R+B$ is preserved over time. It is shown that the dynamics of the ratio $y=R/B$ reduce to the Riccati equation $\dot y=\alpha y^2-\beta$, which admits a complete analytical study. The main result is that asymptotically stable invariant sets in the positive quadrant $R,B\ge 0$ exist exactly in three sign cases of $(\alpha,\beta)$: (i) $\alpha&lt;0,\beta&lt;0$ (stable interior equilibrium), (ii) $\alpha=0,\beta&lt;0$ (the face $B=0$ is stable), (iii) $\alpha&lt;0,\beta=0$ (the face $R=0$ is stable). For $\alpha&gt;0$ or $\beta&gt;0$ the solutions reach the boundaries of applicability of the model in finite time. Moreover, $\alpha&lt;0,\beta&lt;0$ corresponds to exponential growth of solutions in the original system. Passing to a model perturbed in $\alpha(t),\beta(t)$ requires buffer dynamics repelling from the axes to preserve stability of the solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18515v1</guid>
      <category>math.DS</category>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sergey Salishev</dc:creator>
    </item>
    <item>
      <title>Scaling up Stability: Reinforcement Learning for Distributed Control of Networked Systems in the Space of Stabilizing Policies</title>
      <link>https://arxiv.org/abs/2512.18540</link>
      <description>arXiv:2512.18540v1 Announce Type: cross 
Abstract: We study distributed control of networked systems through reinforcement learning, where neural policies must be simultaneously scalable, expressive and stabilizing. We introduce a policy parameterization that embeds Graph Neural Networks (GNNs) into a Youla-like magnitude-direction parameterization, yielding distributed stochastic controllers that guarantee network-level closed-loop stability by design. The magnitude is implemented as a stable operator consisting of a GNN acting on disturbance feedback, while the direction is a GNN acting on local observations. We prove robustness of the closed loop to perturbations in both the graph topology and model parameters, and show how to integrate our parameterization with Proximal Policy Optimization. Experiments on a multi-agent navigation task show that policies trained on small networks transfer directly to larger ones and unseen network topologies, achieve higher returns and lower variance than a state-of-the-art MARL baseline while preserving stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18540v1</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John Cao, Luca Furieri</dc:creator>
    </item>
    <item>
      <title>Boundary regularity of a fourth order Alt-Caffarelli problem and applications to the minimization of the critical buckling load</title>
      <link>https://arxiv.org/abs/2512.18626</link>
      <description>arXiv:2512.18626v1 Announce Type: cross 
Abstract: We study a higher order analogue to the Alt-Caffarelli functional that arises in several shape optimization problems, among which the minimization of the critical buckling load of a clamped plate of fixed area. We obtain several regularity results up to the boundary in two dimensions, in particular we prove the full regularity of the boundary (analytic outside angles of opening $\approx 1.43\pi$) near any point of density less than 1 of the optimal shape. These results are based on the monotonicity formula discovered by Dipierro, Karakhanyan, and Valdinoci, which we improve with a new epiperimetric inequality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18626v1</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jimmy Lamboley, Micka\"el Nahon</dc:creator>
    </item>
    <item>
      <title>Smart nudging for efficient routing through networks</title>
      <link>https://arxiv.org/abs/2512.18630</link>
      <description>arXiv:2512.18630v1 Announce Type: cross 
Abstract: In this paper, we formulate the design of efficient digitalised deposit return schemes as a control problem. We focus on the recycling of paper cups, though the proposed methodology applies more broadly to reverse logistics systems arising in circular economy R-strategies. Each item is assumed to carry a digital wallet through which monetary rewards are allocated to actors transferring the item across successive stages, incentivising completion of the recycling process. System efficiency is ensured by: (i) decentralised algorithms that avoid congestion at individual nodes; (ii) a decentralised AIMD-based algorithm that optimally splits the deposit across layers; and (iii) a feedback control loop that dynamically adjusts the deposit to achieve a desired throughput. The effectiveness of the framework is demonstrated through extensive simulations using realistic paper cup recycling data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18630v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pouria M. Oqaz, Emanuele Crisostomi, Elena Dieckmann, Robert Shorten</dc:creator>
    </item>
    <item>
      <title>Multiscale homogenization of non-local energies of convolution-type</title>
      <link>https://arxiv.org/abs/2512.18697</link>
      <description>arXiv:2512.18697v1 Announce Type: cross 
Abstract: We analyze a family of non-local integral functionals of convolution-type depending on two small positive parameters $\varepsilon,\delta$: the first rules the length-scale of the non-local interactions and produces a `localization' effect as it tends to $0$, the second is the scale of oscillation of a finely inhomogeneous periodic structure in the domain. We prove that a separation of the two scales occurs and that the interplay between the localization and homogenization effects in the asymptotic analysis is determined by the parameter $\lambda$ defined as the limit of the ratio $\varepsilon/\delta$. We compute the $\Gamma$-limit of the functionals with respect to the strong $L^p$-topology for each possible value of $\lambda$ and detect three different regimes, the critical scale being obtained when $\lambda\in(0,+\infty)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18697v1</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Cosma Brusca</dc:creator>
    </item>
    <item>
      <title>Structure-Preserving Optimal Control of Open Quantum Systems via a Discrete Contact PMP</title>
      <link>https://arxiv.org/abs/2512.18879</link>
      <description>arXiv:2512.18879v1 Announce Type: cross 
Abstract: We develop a discrete Pontryagin Maximum Principle (PMP) for controlled open quantum systems governed by Lindblad dynamics, and introduce a second--order \emph{contact Lie--group variational integrator} (contact LGVI) that preserves both the CPTP (completely positive and trace--preserving) structure of the Lindblad flow and the contact geometry underlying the discrete PMP. A type--II discrete contact generating function produces a strict discrete contactomorphism under which the state, costate, and cost propagate in exact agreement with the variational structure of the discrete contact PMP.
  We apply this framework to the optimal control of a dissipative qubit and compare it with a non--geometric explicit RK2 discretization of the Lindblad equation. Although both schemes have the same formal order, the RK2 method accumulates geometric drift (loss of trace, positivity violations, and breakdown of the discrete contact form) that destabilizes PMP shooting iterations, especially under strong dissipation or long horizons. In contrast, the contact LGVI maintains exact CPTP structure and discrete contact geometry step by step, yielding stable, physically consistent, and geometrically faithful optimal control trajectories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18879v1</guid>
      <category>quant-ph</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonardo Colombo</dc:creator>
    </item>
    <item>
      <title>A Convex Loss Function for Set Prediction with Optimal Trade-offs Between Size and Conditional Coverage</title>
      <link>https://arxiv.org/abs/2512.19142</link>
      <description>arXiv:2512.19142v1 Announce Type: cross 
Abstract: We consider supervised learning problems in which set predictions provide explicit uncertainty estimates. Using Choquet integrals (a.k.a. Lov{\'a}sz extensions), we propose a convex loss function for nondecreasing subset-valued functions obtained as level sets of a real-valued function. This loss function allows optimal trade-offs between conditional probabilistic coverage and the ''size'' of the set, measured by a non-decreasing submodular function. We also propose several extensions that mimic loss functions and criteria for binary classification with asymmetric losses, and show how to naturally obtain sets with optimized conditional coverage. We derive efficient optimization algorithms, either based on stochastic gradient descent or reweighted least-squares formulations, and illustrate our findings with a series of experiments on synthetic datasets for classification and regression tasks, showing improvements over approaches that aim for marginal coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19142v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francis Bach (SIERRA)</dc:creator>
    </item>
    <item>
      <title>Rapid stabilization of the heat equation with localized disturbance</title>
      <link>https://arxiv.org/abs/2512.19160</link>
      <description>arXiv:2512.19160v1 Announce Type: cross 
Abstract: This paper studies the rapid stabilization of a multidimensional heat equation in the presence of an unknown spatially localized disturbance. A novel multivalued feedback control strategy is proposed, which synthesizes the frequency Lyapunov method (introduced by Xiang [41]) with the sign multivalued operator. This methodology connects Lyapunov-based stability analysis with spectral inequalities, while the inclusion of the sign operator ensures robustness against the disturbance. The closed-loop system is governed by a differential inclusion, for which well-posedness is proved via the theory of maximal monotone operators. This approach not only guarantees exponential stabilization but also circumvents the need for explicit disturbance modeling or estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19160v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patricio Guzm\'an (SPHINX, IECL), Hugo Parada (SPHINX, IECL), Christian Calle-C\'ardenas</dc:creator>
    </item>
    <item>
      <title>Optimal stabilization rate for the wave equation with hyperbolic boundary condition</title>
      <link>https://arxiv.org/abs/2512.19167</link>
      <description>arXiv:2512.19167v1 Announce Type: cross 
Abstract: We show that the energy of classical solutions to the wave equation with hyperbolic boundary condition (i.e., dynamic Wentzell boundary condition) and damping on the boundary decays like 1/t. In fact we allow mixed boundary conditions: a possibly empty, disjoint part of the boundary may be kept at rest provided that the dynamic part satisfies the geometric control condition. We also prove that this decay rate is sharp. Our results follow from resolvent estimates, which we establish by studying high-frequency quasimodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19167v1</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Parada (IECL), Nicolas Vanspranghe (L2S)</dc:creator>
    </item>
    <item>
      <title>Binary Kernel Logistic Regression: a sparsity-inducing formulation and a convergent decomposition training algorithm</title>
      <link>https://arxiv.org/abs/2512.19440</link>
      <description>arXiv:2512.19440v1 Announce Type: cross 
Abstract: Kernel logistic regression (KLR) is a widely used supervised learning method for binary and multi-class classification, which provides estimates of the conditional probabilities of class membership for the data points. Unlike other kernel methods such as Support Vector Machines (SVMs), KLRs are generally not sparse. Previous attempts to deal with sparsity in KLR include a heuristic method referred to as the Import Vector Machine (IVM) and ad hoc regularizations such as the $\ell_{1/2}$-based one. Achieving a good trade-off between prediction accuracy and sparsity is still a challenging issue with a potential significant impact from the application point of view. In this work, we revisit binary KLR and propose an extension of the training formulation proposed by Keerthi et al., which is able to induce sparsity in the trained model, while maintaining good testing accuracy. To efficiently solve the dual of this formulation, we devise a decomposition algorithm of Sequential Minimal Optimization type which exploits second-order information, and for which we establish global convergence. Numerical experiments conducted on 12 datasets from the literature show that the proposed binary KLR approach achieves a competitive trade-off between accuracy and sparsity with respect to IVM, $\ell_{1/2}$-based regularization for KLR, and SVM while retaining the advantages of providing informative estimates of the class membership probabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19440v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Antonio Consolo, Andrea Manno, Edoardo Amaldi</dc:creator>
    </item>
    <item>
      <title>Fare Zone Assignment</title>
      <link>https://arxiv.org/abs/2512.19493</link>
      <description>arXiv:2512.19493v1 Announce Type: cross 
Abstract: Tariff setting in public transportation networks is an important challenge. A popular approach is to partition the network into fare zones ("zoning") and fix journey prices depending on the number of traversed zones ("pricing"). In this paper, we focus on finding revenue-optimal solutions to the zoning problem for a given concave pricing function. We consider tree networks with $n$ vertices, since trees already pose non-trivial algorithmic challenges. Our main results are efficient algorithms that yield a simple $\mathcal{O}(\log n)$-approximation as well as a more involved $\mathcal{O}(\log n/\log \log n)$-approximation. We show how to solve the problem exactly on rooted instances, in which all demand arises at the same source. For paths, we prove strong NP-hardness and outline a PTAS. Moreover, we show that computing an optimal solution is in FPT or XP for several natural problem parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19493v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Hoefer, Lennart Kauther, Philipp Pabst, Britta Peis, Khai Van Tran</dc:creator>
    </item>
    <item>
      <title>Deep Legendre Transform</title>
      <link>https://arxiv.org/abs/2512.19649</link>
      <description>arXiv:2512.19649v1 Announce Type: cross 
Abstract: We introduce a novel deep learning algorithm for computing convex conjugates of differentiable convex functions, a fundamental operation in convex analysis with various applications in different fields such as optimization, control theory, physics and economics. While traditional numerical methods suffer from the curse of dimensionality and become computationally intractable in high dimensions, more recent neural network-based approaches scale better, but have mostly been studied with the aim of solving optimal transport problems and require the solution of complicated optimization or max-min problems. Using an implicit Fenchel formulation of convex conjugation, our approach facilitates an efficient gradient-based framework for the minimization of approximation errors and, as a byproduct, also provides a posteriori error estimates for the approximation quality. Numerical experiments demonstrate our method's ability to deliver accurate results across different high-dimensional examples. Moreover, by employing symbolic regression with Kolmogorov--Arnold networks, it is able to obtain the exact convex conjugates of specific convex functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.19649v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aleksey Minabutdinov, Patrick Cheridito</dc:creator>
    </item>
    <item>
      <title>Variance Reduction and Low Sample Complexity in Stochastic Optimization via Proximal Point Method</title>
      <link>https://arxiv.org/abs/2402.08992</link>
      <description>arXiv:2402.08992v3 Announce Type: replace 
Abstract: High-probability guarantees in stochastic optimization are often obtained only under strong noise assumptions such as sub-Gaussian tails. We show that such guarantees can also be achieved under the weaker assumption of bounded variance by developing a stochastic proximal point method. This method combines a proximal subproblem solver, which inherently reduces variance, with a probability booster that amplifies per-iteration reliability into high-confidence results. The analysis demonstrates convergence with low sample complexity, without restrictive noise assumptions or reliance on mini-batching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08992v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaming Liang</dc:creator>
    </item>
    <item>
      <title>Optimal control for coupled sweeping processes under minimal assumptions</title>
      <link>https://arxiv.org/abs/2409.07722</link>
      <description>arXiv:2409.07722v2 Announce Type: replace 
Abstract: In this paper, the study of nonsmooth optimal control problems (P) involving a controlled sweeping process with three main characteristics is launched. First, the sweeping sets are nonsmooth, time-dependent, and uniformly prox-regular. Second, the sweeping process is coupled with a controlled differential equation. Third, a joint-state endpoints constraint set S is present. This general model incorporates different important controlled submodels, such as a class of second order sweeping processes, and coupled evolution variational inequalities. A full form of the nonsmooth Pontryagin maximum principle for strong local minimizers in (P) is derived for bounded or unbounded moving sweeping sets satisfying local constraint qualifications (CQ) without any additional restriction. The existence and uniqueness of a Lipschitz solution for the Cauchy problem of our dynamic is established and the existence of an optimal solution for (P) is obtained. Two of the novelties in achieving the first goal are (i) the construction of a problem over truncated sweeping sets and truncated joint endpoints constraint set that has the same strong local minimizer as (P) and its (CQ) automatically holds, and (ii) the complete redesign of the exponential-penalty approximation technique for problems with moving sweeping sets that do not require any special assumption on the sets, their corners, or on the gradients of their generators. The utility of the optimality conditions is illustrated with an example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07722v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samara Chamoun, Vera Zeidan</dc:creator>
    </item>
    <item>
      <title>Accessible Complexity Bounds for Restarted PDHG on Linear Programs with a Unique Optimizer</title>
      <link>https://arxiv.org/abs/2410.04043</link>
      <description>arXiv:2410.04043v3 Announce Type: replace 
Abstract: The restarted primal-dual hybrid gradient method (rPDHG) has recently emerged as an important tool for solving large-scale linear programs (LPs). For LPs with unique optima, we present an iteration bound of $O\left(\kappa\Phi\cdot\ln\left(\frac{\kappa\Phi\|w^*\|}{\varepsilon}\right)\right)$, where $\varepsilon$ is the target tolerance, $\kappa$ is the standard matrix condition number, $\|w^*\|$ is the norm of the optimal solution, and $\Phi$ is a geometric condition number of the LP sublevel sets. This iteration bound is "accessible" in the sense that computing it is typically no more difficult than computing the optimal solution itself. Indeed, we present a closed-form and tractably computable expression for $\Phi$. This enables an analysis of the "two-stage performance" of rPDHG: we show that the first stage identifies the optimal basis in ${O}\left(\kappa\Phi\cdot\ln(\kappa\Phi)\right)$ iterations, and the second stage computes an $\varepsilon$-optimal solution in $O\left(\|B^{-1}\|\|A\|\cdot\ln\left(\frac{\xi}{\varepsilon}\right)\right)$ additional iterations, where $A$ is the constraint matrix, $B$ is the optimal basis and $\xi$ is the smallest nonzero in the optimal solution. Furthermore, computational tests mostly confirm the tightness of our iteration bounds. We also show a reciprocal relation between the iteration bound and stability under data perturbation, which is also equivalent to (i) proximity to multiple optima, and (ii) the LP sharpness of the instance. Finally, we analyze an "optimized" primal-dual reweighting which offers some intuition concerning the step-size heuristics used in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04043v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zikai Xiong</dc:creator>
    </item>
    <item>
      <title>Improving the Accuracy of DC Optimal Power Flow Formulations via Parameter Optimization</title>
      <link>https://arxiv.org/abs/2410.11725</link>
      <description>arXiv:2410.11725v3 Announce Type: replace 
Abstract: DC Optimal Power Flow (DC-OPF) problems optimize the generators' active power setpoints while satisfying constraints based on the DC power flow linearization. The computational tractability advantages of DC-OPF problems come at the expense of inaccuracies relative to AC Optimal Power Flow (AC-OPF) problems that accurately model the nonlinear steady-state behavior of power grids. This paper proposes an algorithm that significantly improves the accuracy of the generators' active power setpoints from DC-OPF problems with respect to the corresponding AC-OPF problems over a specified range of operating conditions. Using sensitivity information in a machine learning-inspired methodology, this algorithm tunes coefficient and bias parameters in the DC power flow approximation to improve the accuracy of the resulting DC-OPF solutions. Employing the Truncated Newton Conjugate-Gradient (TNC) method, a Quasi-Newton optimization technique, this parameter tuning occurs during an offline training phase, with the resulting parameters then used in online computations. Numerical results underscore the algorithm's efficacy with accuracy improvements in squared two-norm and $\infty$-norm losses of up to $90\%$ and $79\%$, respectively, relative to traditional DC-OPF formulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11725v3</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Babak Taheri, Daniel K. Molzahn</dc:creator>
    </item>
    <item>
      <title>Optimal control under unknown intensity with Bayesian learning</title>
      <link>https://arxiv.org/abs/2411.04917</link>
      <description>arXiv:2411.04917v3 Announce Type: replace 
Abstract: We investigate an optimal control problem motivated by neuroscience, where the dynamics is driven by a Poisson process with a controlled stochastic intensity and an unknown parameter. Given a prior distribution for the unknown parameter, we describe its evolution using Bayes' rule. We reformulate the optimization problem by applying Girsanov's theorem and establish a dynamic programming principle. Finally, we characterize the value function as the unique viscosity solution to a finite-dimensional Hamilton-Jacobi-Bellman equation, which can be solved numerically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04917v3</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Baradel, Quentin Cormier</dc:creator>
    </item>
    <item>
      <title>A Riemannian Optimization Perspective of the Gauss-Newton Method for Feedforward Neural Networks</title>
      <link>https://arxiv.org/abs/2412.14031</link>
      <description>arXiv:2412.14031v5 Announce Type: replace 
Abstract: In this work, we establish non-asymptotic convergence bounds for the Gauss-Newton method in training neural networks with smooth activations. In the underparameterized regime, the Gauss-Newton gradient flow in parameter space induces a Riemannian gradient flow on a low-dimensional embedded submanifold of the function space. Using tools from Riemannian optimization, we establish geodesic Polyak-Lojasiewicz and Lipschitz-smoothness conditions for the loss under appropriately chosen output scaling, yielding geometric convergence to the optimal in-class predictor at an explicit rate independent of the conditioning of the Gram matrix. In the overparameterized regime, we propose adaptive, curvature-aware regularization schedules that ensure fast geometric convergence to a global optimum at a rate independent of the minimum eigenvalue of the neural tangent kernel and, locally, of the modulus of strong convexity of the loss. These results demonstrate that Gauss-Newton achieves accelerated convergence rates in settings where first-order methods exhibit slow convergence due to ill-conditioned kernel matrices and loss landscapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.14031v5</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.ML</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Semih Cayci</dc:creator>
    </item>
    <item>
      <title>Efficient Implementation of Third-Order Tensor Methods with Adaptive Regularization for Unconstrained Optimization</title>
      <link>https://arxiv.org/abs/2501.00404</link>
      <description>arXiv:2501.00404v3 Announce Type: replace 
Abstract: High-order tensor methods that employ local Taylor models of degree $p$ within adaptive regularization frameworks (AR$p$) have recently received significant attention, due to their optimal/improved global and local rates of convergence, for both convex and nonconvex optimization problems. In this paper, we showcase the numerical performance of standard second- and third-order variants ($p=2,3$) and propose novel techniques for key algorithmic aspects when $p\geq 3$. In particular, we extend the interpolation-based updating strategy for the regularization parameter introduced in [Gould, Porcelli and Toint, Comput Optim Appl (2012) 53:1--22] for $p=2$, to the case when $p \geq 3$. We identify fundamental differences between the different local minima of the regularised subproblems for $p=2$ and $p \geq 3$ and their effect on algorithm performance. For $p\geq 3$, we introduce a novel pre-rejection technique that rejects poor/unsuccessful subproblem minimizers prior to any function evaluation. Numerical studies showcase the efficiency improvements generated by our proposed modifications of the AR$3$ algorithm. We also assess numerically, the effect of different subproblem termination conditions and the choice of the initial regularization parameter on the overall algorithm performance. Finally, we benchmark our best-performing AR$3$ variants, as well as those in [Birgin et al., Optim Lett (2020) 14:815--838], against second-order ones (AR$2$). Encouraging results on standard test problems are obtained, confirming that AR$3$ variants can be made to outperform second-order variants in terms of objective evaluations, derivative evaluations, and number of subproblem solves. We provide an efficient, extensive and modular software package in MATLAB that includes many AR$2$ and AR$3$ variants, including Hessian- and tensor-free ones, allowing ease of use and experimentation for interested users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00404v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Coralia Cartis, Raphael Hauser, Yang Liu, Karl Welzel, Wenqi Zhu</dc:creator>
    </item>
    <item>
      <title>The Golden Ratio Primal-Dual Algorithm with Two New Stepsize Rules for Convex-Concave Saddle Point Problems</title>
      <link>https://arxiv.org/abs/2502.17918</link>
      <description>arXiv:2502.17918v3 Announce Type: replace 
Abstract: In this paper, we present two stepsize strategies for the extended Golden Ratio primal-dual algorithm (E-GRPDA) designed to address structured convex optimization problems in finite-dimensional real Hilbert spaces. The first rule features a non-increasing primal stepsize that remains bounded below by a positive constant and is updated adaptively at each iteration, eliminating the need to compute the Lipschitz constant of the gradient of the function and the norm of the operator, without using backtracking. The second stepsize rule is adaptive, adjusting based on the local smoothness of the smooth component function and the norm of the operator involved. In other words, we present an adaptive version of the E-GRPDA algorithm. We prove that E-GRPDA achieves an ergodic sublinear convergence rate with both stepsize rules, based on the function-value residual and constraint violation rather than on the so-called primal-dual gap function. Additionally, we establish an R-linear convergence rate for E-GRPDA with the first stepsize rule, under standard assumptions and with appropriately chosen parameters. Through numerical experiments on various convex optimization problems, we demonstrate the effectiveness of our approaches and compare their performance to existing ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17918v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Santanu Soe, Matthew K. Tam, V. Vetrivel</dc:creator>
    </item>
    <item>
      <title>Stochastic Optimization with Optimal Importance Sampling</title>
      <link>https://arxiv.org/abs/2504.03560</link>
      <description>arXiv:2504.03560v2 Announce Type: replace 
Abstract: Importance Sampling (IS) is a widely used variance reduction technique for enhancing the efficiency of Monte Carlo methods, particularly in rare-event simulation and related applications. Despite its effectiveness, the performance of IS is highly sensitive to the choice of the proposal distribution and often requires stochastic calibration. While the design and analysis of IS have been extensively studied in estimation settings, applying IS within stochastic optimization introduces a fundamental challenge: the decision variable and the importance sampling distribution are mutually dependent, creating a circular optimization structure. This interdependence complicates both convergence analysis and variance control. We consider convex stochastic optimization problems with linear constraints and propose a single-loop stochastic approximation algorithm, based on a joint variant of Nesterov's dual averaging, that jointly updates the decision variable and the importance sampling distribution, without time-scale separation or nested optimization. The method is globally convergent and achieves minimal asymptotic variance among stochastic gradient schemes, matching the performance of an oracle sampler adapted to the optimal solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03560v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liviu Aolaritei, Bart P. G. Van Parys, Henry Lam, Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>Feature Selection for Data-driven Explainable Optimization</title>
      <link>https://arxiv.org/abs/2504.12184</link>
      <description>arXiv:2504.12184v2 Announce Type: replace 
Abstract: Mathematical optimization, although often leading to NP-hard models, is now capable of solving even large-scale instances within reasonable time. However, the primary focus is often placed solely on optimality. This implies that while obtained solutions are globally optimal, they are frequently not comprehensible to humans, in particular when obtained by black-box routines. In contrast, explainability is a standard requirement for results in Artificial Intelligence, but it is rarely considered in optimization yet. There are only a few studies that aim to find solutions that are both of high quality and explainable. In recent work, explainability for optimization was defined in a data-driven manner: A solution is considered explainable if it closely resembles solutions that have been used in the past under similar circumstances. To this end, it is crucial to identify a preferably small subset of features from a presumably large set that can be used to measure instance similarity. In this work, we formally define the feature selection problem for explainable optimization and prove that its decision version is NP-complete. We introduce mathematical models for optimized feature selection. As their global solution requires significant computation time with modern mixed-integer linear solvers, we employ local heuristics. Our computational study using data that reflect real-world scenarios demonstrates that the problem can be solved practically efficiently for instances of reasonable size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.12184v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin-Martin Aigner, Marc Goerigk, Michael Hartisch, Frauke Liers, Arthur Miehlich, Florian R\"osel</dc:creator>
    </item>
    <item>
      <title>Differentiable Nonlinear Model Predictive Control</title>
      <link>https://arxiv.org/abs/2505.01353</link>
      <description>arXiv:2505.01353v2 Announce Type: replace 
Abstract: The efficient computation of parametric solution sensitivities is a key challenge in the integration of learning-enhanced methods with nonlinear model predictive control (MPC), as their availability is crucial for many learning algorithms. This paper discusses the computation of solution sensitivities of general nonlinear programs (NLPs) using the implicit function theorem (IFT) and smoothed optimality conditions treated in interior-point methods (IPM). We detail sensitivity computation within a sequential quadratic programming (SQP) method which employs an IPM for the quadratic subproblems. Previous works presented in the machine learning community are limited to convex or unconstrained formulations, or lack an implementation for efficient sensitivity evaluation. The publication is accompanied by an efficient open-source implementation within the acados framework, providing both forward and adjoint sensitivities for general optimal control problems, achieving speedups exceeding 3x over the state-of-the-art solvers mpc.pytorch and cvxpygen.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01353v2</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Frey, Katrin Baumg\"artner, Gianluca Frison, Dirk Reinhardt, Jasper Hoffmann, Leonard Fichtner, Sebastien Gros, Moritz Diehl</dc:creator>
    </item>
    <item>
      <title>Lower Bounds on the Haraux Function</title>
      <link>https://arxiv.org/abs/2508.15735</link>
      <description>arXiv:2508.15735v3 Announce Type: replace 
Abstract: The Haraux function is an important tool in monotone operator theory and its applications. One of its salient properties for maximally monotone operators is to be valued in $[0,+\infty]$ and to vanish only on the graph of the operator. Sharper lower bounds for this function were recently proposed in specific cases. We derive lower bounds in the general context of set-valued operators in reflexive Banach spaces. These bounds are new, even for maximally monotone operators acting on Euclidean spaces, a scenario in which we show that they can be better than existing ones. As a by-product, we obtain lower bounds for the Fenchel--Young function in variational analysis. Several examples are given and applications to composite monotone inclusions are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15735v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick L. Combettes, Julien N. Mayrand</dc:creator>
    </item>
    <item>
      <title>Trust-region Filter Algorithms utilising Hessian Information for Grey-Box Optimisation</title>
      <link>https://arxiv.org/abs/2509.01651</link>
      <description>arXiv:2509.01651v3 Announce Type: replace 
Abstract: Optimising industrial processes often involves grey-box models that couple algebraic glass-box equations with black-box components lacking analytic derivatives. Such hybrid systems challenge derivative-based solvers. The classical trust-region filter (TRF) algorithm provides a robust framework but requires extensive parameter tuning and numerous black-box evaluations. This work introduces four Hessian-informed TRF variants (A1-A4) that use projected positive definite Hessians for automatic step scaling and minimal tuning, combined with both low-fidelity (linear, quadratic) and high-fidelity (Taylor series, Gaussian process) surrogates for local black-box approximation. Tested on 25 grey-box benchmarks and five engineering case studies (Himmelblau, liquid-liquid extraction, pressure vessel design, alkylation, and spring design), the new variants achieved up to an order-of-magnitude reduction in iterations and black-box evaluations, with reduced sensitivity to tuning parameters relative to the classical TRF algorithm. High-fidelity surrogates solved 92-100 % problems, compared to 72-84 % for the low-fidelity surrogates. Developed TRF methods also outperformed classical derivative-free optimisation solvers. The results show that new variants offer robust and scalable alternatives for grey-box process systems optimisation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01651v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gul Hameed, Tao Chen, Antonio del Rio Chanona, Lorenz T. Biegler, Michael Short</dc:creator>
    </item>
    <item>
      <title>Safe Navigation in the Presence of Range-Limited Pursuers</title>
      <link>https://arxiv.org/abs/2509.04258</link>
      <description>arXiv:2509.04258v3 Announce Type: replace 
Abstract: This paper examines the degree to which an evader seeking a safe and efficient path to a target location can benefit from increasing levels of knowledge regarding one or more range-limited pursuers seeking to intercept it. Unlike previous work, this research considers the time of flight of the pursuers actively attempting interception. It is shown that additional knowledge allows the evader to safely steer closer to the threats, shortening paths without accepting additional risk of capture. A control heuristic is presented, suitable for real-time implementation, which capitalizes on all knowledge available to the evader.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04258v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/LCSYS.2025.3645221</arxiv:DOI>
      <dc:creator>Thomas Chapman, Alexander Von Moll, Isaac E. Weintraub</dc:creator>
    </item>
    <item>
      <title>What is the Best Way to Do Something? A Discreet Tour of Discrete Optimization</title>
      <link>https://arxiv.org/abs/2509.05932</link>
      <description>arXiv:2509.05932v3 Announce Type: replace 
Abstract: In mathematical optimization, we want to find the best possible solution for a decision-making problem. Curiously, these problems are harder to solve if they have discrete decisions. Imagine that you would like to buy chocolate: you can buy no chocolate or one chocolate bar, but typically you cannot buy just half of a bar. Now imagine that you could also buy many other items, and that you need to meet nutritional needs while minimizing the grocery bill. With more options and more demands, finding the best solution becomes trickier. But since many real-world settings benefit from mathematical optimization, such as scheduling trains and flights, planning truck deliveries, and making better investment decisions, these problems are widely studied in a branch of mathematics called Operations Research (OR). Sometimes we can simply write the mathematical model and find an optimal solution with OR software, but for larger problems we may need to develop new mathematical models and even write our own algorithms. We explore both cases with a simple and well-known problem (the traveling salesperson problem), some computer programming (in Python), and software that is free for academic use (Gurobi). All the code and data used is available at: https://github.com/thserra/discreet</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05932v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thiago Serra</dc:creator>
    </item>
    <item>
      <title>A Single-Loop First-Order Algorithm for Linearly Constrained Bilevel Optimization</title>
      <link>https://arxiv.org/abs/2510.24710</link>
      <description>arXiv:2510.24710v2 Announce Type: replace 
Abstract: We study bilevel optimization problems where the lower-level problems are strongly convex and have coupled linear constraints. To overcome the potential non-smoothness of the hyper-objective and the computational challenges associated with the Hessian matrix, we utilize penalty and augmented Lagrangian methods to reformulate the original problem as a single-level one. Especially, we establish a strong theoretical connection between the reformulated function and the original hyper-objective by characterizing the closeness of their values and derivatives. Based on this reformulation, we propose a single-loop, first-order algorithm for linearly constrained bilevel optimization (SFLCB). We provide rigorous analyses of its non-asymptotic convergence rates, showing an improvement over prior double-loop algorithms -- form $O(\epsilon^{-3}\log(\epsilon^{-1}))$ to $O(\epsilon^{-3})$. The experiments corroborate our theoretical findings and demonstrate the practical efficiency of the proposed SFLCB algorithm. Simulation code is provided at https://github.com/ShenGroup/SFLCB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24710v2</guid>
      <category>math.OC</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Shen, Jiawei Zhang, Minhui Huang, Cong Shen</dc:creator>
    </item>
    <item>
      <title>Nonasymptotic Convergence Rates for Plug-and-Play Methods With MMSE Denoisers</title>
      <link>https://arxiv.org/abs/2510.27211</link>
      <description>arXiv:2510.27211v4 Announce Type: replace 
Abstract: It is known that the minimum-mean-squared-error (MMSE) denoiser under Gaussian noise can be written as a proximal operator, which suffices for asymptotic convergence of plug-and-play (PnP) methods but does not reveal the structure of the induced regularizer or give convergence rates. We show that the MMSE denoiser corresponds to a regularizer that can be written explicitly as an upper Moreau envelope of the negative log-marginal density, which in turn implies that the regularizer is 1-weakly convex. Using this property, we derive (to the best of our knowledge) the first sublinear convergence guarantee for PnP proximal gradient descent with an MMSE denoiser. We validate the theory with a one-dimensional synthetic study that recovers the implicit regularizer. We also validate the theory with imaging experiments (deblurring and computed tomography), which exhibit the predicted sublinear behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27211v4</guid>
      <category>math.OC</category>
      <category>eess.SP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henry Pritchard, Rahul Parhi</dc:creator>
    </item>
    <item>
      <title>Quadratic Mean-Field BSDEs and Exponential Utility Maximization</title>
      <link>https://arxiv.org/abs/2511.17214</link>
      <description>arXiv:2511.17214v2 Announce Type: replace 
Abstract: In this paper, we study a class of real-valued mean-field backward stochastic differential equations (BSDEs) with generator of quadratic growth in the control variable and the mean-field term. Under this assumption, together with a bounded terminal condition, we establish existence and uniqueness of solutions. Our approach departs from classical fixed-point arguments and instead combines Malliavin calculus with refined BMO and stability estimates. The result bridges the gap between the quadratic BSDE results of Cheridito and Nam (2017) and Hao et al. (2025). Moreover, motivated by the structure of the mean-field exponential utility maximization problem introduced in our paper, we extend our framework to generators satisfying a weaker quadratic condition on the generator. This relaxation is designed to accommodate the additional mean-field terms that arise in our utility maximization setting and that fall outside the scope of previous quadratic assumptions. Within this more general regime, we establish existence and uniqueness of solutions under a smallness condition on the terminal random variable. We then apply this extended theory to solve a mean-field exponential utility maximization problem, thereby generalizing the classical framework of Hu et al. (2005)to a fully coupled quadratic mean-field setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17214v2</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yining Ding, Kihun Nam, Jiaqiang Wen</dc:creator>
    </item>
    <item>
      <title>Continuous-time reinforcement learning for optimal switching over multiple regimes</title>
      <link>https://arxiv.org/abs/2512.04697</link>
      <description>arXiv:2512.04697v2 Announce Type: replace 
Abstract: This paper studies the continuous-time reinforcement learning (RL) for optimal switching problems across multiple regimes. We consider a type of exploratory formulation under entropy regularization where the agent randomizes both the timing of switches and the selection of regimes through the generator matrix of an associated continuous-time finite-state Markov chain. We establish the well-posedness of the associated system of Hamilton-Jacobi-Bellman (HJB) equations and provide a characterization of the optimal policy. The policy improvement and the convergence of the policy iterations are rigorously established by analyzing the system of equations. We also show the convergence of the value function in the exploratory formulation towards the value function in the classical formulation as the temperature parameter vanishes. Finally, a reinforcement learning algorithm is devised and implemented by invoking the policy evaluation based on the martingale characterization. Our numerical examples with the aid of neural networks illustrate the effectiveness of the proposed RL algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04697v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>q-fin.CP</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijie Huang, Mengge Li, Xiang Yu, Zhou Zhou</dc:creator>
    </item>
    <item>
      <title>Stopping Rules for Stochastic Gradient Descent via Anytime-Valid Confidence Sequences</title>
      <link>https://arxiv.org/abs/2512.13123</link>
      <description>arXiv:2512.13123v3 Announce Type: replace 
Abstract: We study stopping rules for stochastic gradient descent (SGD) for convex optimization from the perspective of anytime-valid confidence sequences. Classical analyses of SGD provide convergence guarantees in expectation or at a fixed horizon, but offer no statistically valid way to assess, at an arbitrary time, how close the current iterate is to the optimum. We develop an anytime-valid, data-dependent upper confidence sequence for the weighted average suboptimality of projected SGD, constructed via nonnegative supermartingales and requiring no smoothness or strong convexity. This confidence sequence yields a simple stopping rule that is provably $\varepsilon$-optimal with probability at least $1-\alpha$, with explicit bounds on the stopping time under standard stochastic approximation stepsizes. To the best of our knowledge, these are the first rigorous, time-uniform performance guarantees and finite-time $\varepsilon$-optimality certificates for projected SGD with general convex objectives, based solely on observable trajectory quantities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13123v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liviu Aolaritei, Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>Deep Reinforcement Learning Optimization for Uncertain Nonlinear Systems via Event-Triggered Robust Adaptive Dynamic Programming</title>
      <link>https://arxiv.org/abs/2512.15735</link>
      <description>arXiv:2512.15735v2 Announce Type: replace 
Abstract: This work proposes a unified control architecture that couples a Reinforcement Learning (RL)-driven controller with a disturbance-rejection Extended State Observer (ESO), complemented by an Event-Triggered Mechanism (ETM) to limit unnecessary computations. The ESO is utilized to estimate the system states and the lumped disturbance in real time, forming the foundation for effective disturbance compensation. To obtain near-optimal behavior without an accurate system description, a value-iteration-based Adaptive Dynamic Programming (ADP) method is adopted for policy approximation. The inclusion of the ETM ensures that parameter updates of the learning module are executed only when the state deviation surpasses a predefined bound, thereby preventing excessive learning activity and substantially reducing computational load. A Lyapunov-oriented analysis is used to characterize the stability properties of the resulting closed-loop system. Numerical experiments further confirm that the developed approach maintains strong control performance and disturbance tolerance, while achieving a significant reduction in sampling and processing effort compared with standard time-triggered ADP schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15735v2</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ningwei Bai, Chi Pui Chan, Qichen Yin, Tengyang Gong, Yunda Yan, Zezhi Tang</dc:creator>
    </item>
    <item>
      <title>Training robust and generalizable quantum models</title>
      <link>https://arxiv.org/abs/2311.11871</link>
      <description>arXiv:2311.11871v4 Announce Type: replace-cross 
Abstract: Adversarial robustness and generalization are both crucial properties of reliable machine learning models. In this paper, we study these properties in the context of quantum machine learning based on Lipschitz bounds. We derive parameter-dependent Lipschitz bounds for quantum models with trainable encoding, showing that the norm of the data encoding has a crucial impact on the robustness against data perturbations. Further, we derive a bound on the generalization error which explicitly involves the parameters of the data encoding. Our theoretical findings give rise to a practical strategy for training robust and generalizable quantum models by regularizing the Lipschitz bound in the cost. Further, we show that, for fixed and non-trainable encodings, as those frequently employed in quantum machine learning, the Lipschitz bound cannot be influenced by tuning the parameters. Thus, trainable encodings are crucial for systematically adapting robustness and generalization during training. The practical implications of our theoretical findings are illustrated with numerical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11871v4</guid>
      <category>quant-ph</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevResearch.6.043326</arxiv:DOI>
      <arxiv:journal_reference>Physical Review Research 6, 043326, 2024</arxiv:journal_reference>
      <dc:creator>Julian Berberich, Daniel Fink, Daniel Pranji\'c, Christian Tutschku, Christian Holm</dc:creator>
    </item>
    <item>
      <title>Majority voting is not good for heaven or hell, with mirrored performance</title>
      <link>https://arxiv.org/abs/2401.00592</link>
      <description>arXiv:2401.00592v5 Announce Type: replace-cross 
Abstract: Within the ViSE (Voting in Stochastic Environment) model, we study the effectiveness of majority voting in various environments. As shown by the pit-of-losses paradox identified in previous work, majority decisions in apparently hostile environments tend to reduce the capital of society. In such cases, the simple social decision rule of ``rejecting all proposals without voting'' outperforms majority voting. In this paper, we identify another pit of losses appearing in favorable environments; here, the simple social decision rule of ``accepting all proposals without voting'' is superior to majority voting. We prove that, under a version of simple majority called symmetrized majority and under the antisymmetry of the voting body, this second pit of losses is a mirror image of the one arising in hostile environments, and we explain this phenomenon. Technically, we consider a voting society consisting of individualists who support all proposals that increase their personal capital and a group (or groups) whose members vote to increase their group's wealth. According to the key lemma, the expected capital gain of each agent under the social decision rule when the random gain generator is $X$ with mean $\mu&gt;0$ exceeds their expected gain under the reflected generator $-X$ by exactly $\mu$. This extends to location-scale families of generators with distributions symmetric about their mean. This result reveals a mirror symmetry in the performance of the symmetrized majority rule relative to a baseline rule. The baseline rule accepts all proposals in favorable environments and rejects them in unfavorable (hostile) ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00592v5</guid>
      <category>physics.soc-ph</category>
      <category>cs.GT</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pavel Chebotarev, Vadim Afonkin</dc:creator>
    </item>
    <item>
      <title>An overview of systems-theoretic guarantees in data-driven model predictive control</title>
      <link>https://arxiv.org/abs/2406.04130</link>
      <description>arXiv:2406.04130v2 Announce Type: replace-cross 
Abstract: The development of control methods based on data has seen a surge of interest in recent years. When applying data-driven controllers in real-world applications, providing theoretical guarantees for the closed-loop system is of crucial importance to ensure reliable operation. In this review, we provide an overview of data-driven model predictive control (MPC) methods for controlling unknown systems with guarantees on systems-theoretic properties such as stability, robustness, and constraint satisfaction. The considered approaches rely on the Fundamental Lemma from behavioral theory in order to predict input-output trajectories directly from data. We cover various setups, ranging from linear systems and noise-free data to more realistic formulations with noise and nonlinearities, and we provide an overview of different techniques to ensure guarantees for the closed-loop system. Moreover, we discuss avenues for future research that may further improve the theoretical understanding and practical applicability of data-driven MPC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04130v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1146/annurev-control-030323-024328</arxiv:DOI>
      <arxiv:journal_reference>Annual Review of Control, Robotics, and Autonomous Systems 8 (1), pp. 77-100, 2025</arxiv:journal_reference>
      <dc:creator>Julian Berberich, Frank Allg\"ower</dc:creator>
    </item>
    <item>
      <title>Single-cell 3D genome reconstruction in the haploid setting using rigidity theory</title>
      <link>https://arxiv.org/abs/2407.10700</link>
      <description>arXiv:2407.10700v2 Announce Type: replace-cross 
Abstract: This article considers the problem of 3-dimensional genome reconstruction for single-cell data, and the uniqueness of such reconstructions in the setting of haploid organisms. We consider multiple graph models as representations of this problem, and use techniques from graph rigidity theory to determine identifiability. Biologically, our models come from Hi-C data, microscopy data, and combinations thereof. Mathematically, we use unit ball and sphere packing models, as well as models consisting of distance and inequality constraints. In each setting, we describe and/or derive new results on realisability and uniqueness. We then propose a 3D reconstruction method based on semidefinite programming and apply it to synthetic and real data sets using our models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10700v2</guid>
      <category>q-bio.GN</category>
      <category>math.CO</category>
      <category>math.MG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s00285-025-02203-2</arxiv:DOI>
      <arxiv:journal_reference>Journal of Mathematical Biology, Volume 90, article number 45, (2025)</arxiv:journal_reference>
      <dc:creator>Sean Dewar, Georg Grasegger, Kaie Kubjas, Fatemeh Mohammadi, Anthony Nixon</dc:creator>
    </item>
    <item>
      <title>Planning and Learning in Average Risk-aware MDPs</title>
      <link>https://arxiv.org/abs/2503.17629</link>
      <description>arXiv:2503.17629v3 Announce Type: replace-cross 
Abstract: For continuing tasks, average cost Markov decision processes have well-documented value and can be solved using efficient algorithms. However, it explicitly assumes that the agent is risk-neutral. In this work, we extend risk-neutral algorithms to accommodate the more general class of dynamic risk measures. Specifically, we propose a relative value iteration (RVI) algorithm for planning and design two model-free Q-learning algorithms, namely a generic algorithm based on the multi-level Monte Carlo (MLMC) method, and an off-policy algorithm dedicated to utility-based shortfall risk measures. Both the RVI and MLMC-based Q-learning algorithms are proven to converge to optimality. Numerical experiments validate our analysis, confirm empirically the convergence of the off-policy algorithm, and demonstrate that our approach enables the identification of policies that are finely tuned to the intricate risk-awareness of the agent that they serve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17629v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Weikai Wang, Erick Delage</dc:creator>
    </item>
    <item>
      <title>Personalized and Resilient Distributed Learning Through Opinion Dynamics</title>
      <link>https://arxiv.org/abs/2505.14081</link>
      <description>arXiv:2505.14081v2 Announce Type: replace-cross 
Abstract: In this paper, we address two practical challenges of distributed learning in multi-agent network systems, namely personalization and resilience. Personalization is the need of heterogeneous agents to learn local models tailored to their own data and tasks, while still generalizing well; on the other hand, the learning process must be resilient to cyberattacks or anomalous training data to avoid disruption. Motivated by a conceptual affinity between these two requirements, we devise a distributed learning algorithm that combines distributed gradient descent and the Friedkin-Johnsen model of opinion dynamics to fulfill both of them. We quantify its convergence speed and the neighborhood that contains the final learned models, which can be easily controlled by tuning the algorithm parameters to enforce a more personalized/resilient behavior. We numerically showcase the effectiveness of our algorithm on synthetic and real-world distributed learning tasks, where it achieves high global accuracy both for personalized models and with malicious agents compared to standard strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14081v2</guid>
      <category>cs.MA</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Ballotta, Nicola Bastianello, Riccardo M. G. Ferrari, Karl H. Johansson</dc:creator>
    </item>
    <item>
      <title>Strong Lyapunov functions for rough systems</title>
      <link>https://arxiv.org/abs/2508.14559</link>
      <description>arXiv:2508.14559v4 Announce Type: replace-cross 
Abstract: We extend the Lyapunov function technique, a fundamental tool for investigating asymptotic stability and existence of attractors for ordinary differential equations, by introducing the notion of a {\it strong Lyapunov function} for an autonomous drift under stochastic perturbation driven by general H\"older-continuous multiplicative noise, not necessarily Brownian.
  The mathematical setting within which our method proceeds consists of rough path calculus and the framework of random dynamical systems. We conclude that if such a function exists for the drift then the perturbed system admits a global random pullback attractor that is upper semi-continuous w.r.t. the noise intensity coefficient and the dyadic approximation of the noise. Moreover, in case the drift is globally Lipschitz continuous, then there exists a numerical attractor for the discretization which is upper semi-continuous w.r.t. the noise intensity and converges to the continuous attractor as the step size tends to zero. Several applications, including dissipative systems, the pendulum, the FitzHugh-Nagumo neuro-system and the Lorenz system, demonstrate the power of our approach. We also prove that strong Lyapunov functions can be approximated in practice by Lyapunov neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14559v4</guid>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luu Hoang Duc, J\"urgen Jost</dc:creator>
    </item>
    <item>
      <title>Network-Optimised Spiking Neural Network for Event-Driven Networking</title>
      <link>https://arxiv.org/abs/2509.23516</link>
      <description>arXiv:2509.23516v3 Announce Type: replace-cross 
Abstract: Time-critical networking requires low-latency decisions from sparse and bursty telemetry, where fixed-step neural inference waste computation. We introduce Network-Optimised Spiking (NOS), a two-state neuron whose variables correspond to normalised queue occupancy and a recovery resource. NOS combines a saturating excitability nonlinearity for finite buffers, service and damping leaks, graph-local inputs with per-link gates and delays, and differentiable resets compatible with surrogate gradients and neuromorphic deployment. We establish existence and uniqueness of subthreshold equilibria, derive Jacobian-based local stability tests, and obtain a scalar network stability threshold that separates topology from node physics through a Perron-mode spectral condition. A stochastic arrival model aligned with telemetry smoothing links NOS responses to classical queueing behaviour while explaining increased variability near stability margins. Across chain, star, and scale-free graphs, NOS improves early-warning F1 and detection latency over MLP, RNN, GRU, and temporal-GNN baselines under a common residual-based protocol, while providing practical calibration and stability rules suited to resource-constrained networking deployments. Code and Demos: https://mbilal84.github.io/nos-snn-networking/</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23516v3</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Bilal</dc:creator>
    </item>
    <item>
      <title>Asymptotics of motion planning complexity for control-affine systems</title>
      <link>https://arxiv.org/abs/2511.17130</link>
      <description>arXiv:2511.17130v2 Announce Type: replace-cross 
Abstract: In this paper, we study the complexity of the approximation of nonadmissible curves for nonlinear control-affine systems satisfying the strong H{\"o}rmander condition. Focusing on tubular approximation complexities, we provide asymptotic equivalences, with explicit constants, for all generic situations where the distribution, i.e., the linear part of the control system, is of co-rank one. Namely, we consider curves in step 2 distributions and any dimension. In the 3 dimensional case, we also consider the case of distributions with Martinet-type singularities that are crossed by the curve at isolated points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17130v2</guid>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michele Motta (SISSA / ISAS), Dario Prandi (L2S)</dc:creator>
    </item>
    <item>
      <title>Configuration-Constrained Tube MPC for Periodic Operation</title>
      <link>https://arxiv.org/abs/2512.04239</link>
      <description>arXiv:2512.04239v2 Announce Type: replace-cross 
Abstract: Periodic operation often emerges as the economically optimal mode in industrial processes, particularly under varying economic or environmental conditions. This paper proposes a robust model predictive control (MPC) framework for uncertain systems modeled as polytopic linear differential inclusions (LDIs), where the dynamics evolve as convex combinations of finitely many affine control systems with additive disturbances. The robust control problem is reformulated as a convex optimization program by optimizing over configuration-constrained polytopic tubes and tracks a periodic trajectory that is optimal for a given economic criterion. Artificial variables embedded in the formulation ensure recursive feasibility and robust constraint satisfaction when the economic criterion is updated online, while guaranteeing convergence to the corresponding optimal periodic tube when the criterion remains constant. To improve computational efficiency, we introduce a quadratic over-approximation of the periodic cost under a Lipschitz continuity assumption, yielding a Quadratic Program (QP) formulation that preserves the above theoretical guarantees. The effectiveness and scalability of the approach are demonstrated on a benchmark example and a ball-plate system with eight states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04239v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filippo Badalamenti, Jose A. Borja-Conde, Sampath Kumar Mulagaleti, Boris Houska, Alberto Bemporad, Mario Eduardo Villanueva</dc:creator>
    </item>
    <item>
      <title>The Moroccan Public Procurement Game</title>
      <link>https://arxiv.org/abs/2512.10109</link>
      <description>arXiv:2512.10109v2 Announce Type: replace-cross 
Abstract: In this paper, we study the public procurement market through the lens of game theory by modeling it as a strategic game with discontinuous and non-quasiconcave payoffs. We first show that the game admits no Nash equilibrium in pure strategies. We then analyze the two-player case and derive two explicit mixed-strategy equilibria for the symmetric game and for the weighted $(p,1-p)$ formulation. Finally, we study the existence of a symmetric mixed strategies Nash equilibrium in the general $N$-player case by applying the diagonal disjoint payoff matching condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10109v2</guid>
      <category>econ.TH</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nizar Riane</dc:creator>
    </item>
    <item>
      <title>Generative modeling of conditional probability distributions on the level-sets of collective variables</title>
      <link>https://arxiv.org/abs/2512.17374</link>
      <description>arXiv:2512.17374v2 Announce Type: replace-cross 
Abstract: Given a probability distribution $\mu$ in $\mathbb{R}^d$ represented by data, we study in this paper the generative modeling of its conditional probability distributions on the level-sets of a collective variable $\xi: \mathbb{R}^d \rightarrow \mathbb{R}^k$, where $1 \le k&lt;d$. We propose a general and efficient learning approach that is able to learn generative models on different level-sets of $\xi$ simultaneously. To improve the learning quality on level-sets in low-probability regions, we also propose a strategy for data enrichment by utilizing data from enhanced sampling techniques. We demonstrate the effectiveness of our proposed learning approach through concrete numerical examples. The proposed approach is potentially useful for the generative modeling of molecular systems in biophysics, for instance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17374v2</guid>
      <category>stat.ML</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fatima-Zahrae Akhyar, Wei Zhang, Gabriel Stoltz, Christof Sch\"utte</dc:creator>
    </item>
    <item>
      <title>Alternating Direction Method of Multipliers for Nonlinear Matrix Decompositions</title>
      <link>https://arxiv.org/abs/2512.17473</link>
      <description>arXiv:2512.17473v2 Announce Type: replace-cross 
Abstract: We present an algorithm based on the alternating direction method of multipliers (ADMM) for solving nonlinear matrix decompositions (NMD). Given an input matrix $X \in \mathbb{R}^{m \times n}$ and a factorization rank $r \ll \min(m, n)$, NMD seeks matrices $W \in \mathbb{R}^{m \times r}$ and $H \in \mathbb{R}^{r \times n}$ such that $X \approx f(WH)$, where $f$ is an element-wise nonlinear function. We evaluate our method on several representative nonlinear models: the rectified linear unit activation $f(x) = \max(0, x)$, suitable for nonnegative sparse data approximation, the component-wise square $f(x) = x^2$, applicable to probabilistic circuit representation, and the MinMax transform $f(x) = \min(b, \max(a, x))$, relevant for recommender systems. The proposed framework flexibly supports diverse loss functions, including least squares, $\ell_1$ norm, and the Kullback-Leibler divergence, and can be readily extended to other nonlinearities and metrics. We illustrate the applicability, efficiency, and adaptability of the approach on real-world datasets, highlighting its potential for a broad range of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17473v2</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Atharva Awari, Nicolas Gillis, Arnaud Vandaele</dc:creator>
    </item>
    <item>
      <title>A Unified Representation of Neural Networks Architectures</title>
      <link>https://arxiv.org/abs/2512.17593</link>
      <description>arXiv:2512.17593v2 Announce Type: replace-cross 
Abstract: In this paper we consider the limiting case of neural networks (NNs) architectures when the number of neurons in each hidden layer and the number of hidden layers tend to infinity thus forming a continuum, and we derive approximation errors as a function of the number of neurons and/or hidden layers. Firstly, we consider the case of neural networks with a single hidden layer and we derive an integral infinite width neural representation that generalizes existing continuous neural networks (CNNs) representations. Then we extend this to deep residual CNNs that have a finite number of integral hidden layers and residual connections. Secondly, we revisit the relation between neural ODEs and deep residual NNs and we formalize approximation errors via discretization techniques. Then, we merge these two approaches into a unified homogeneous representation of NNs as a Distributed Parameter neural Network (DiPaNet) and we show that most of the existing finite and infinite-dimensional NNs architectures are related via homogenization/discretization with the DiPaNet representation. Our approach is purely deterministic and applies to general, uniformly continuous matrix weight functions. Relations with neural fields and other neural integro-differential equations are discussed along with further possible generalizations and applications of the DiPaNet framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17593v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christophe Prieur, Mircea Lazar, Bogdan Robu</dc:creator>
    </item>
  </channel>
</rss>
