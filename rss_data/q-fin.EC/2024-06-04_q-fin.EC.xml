<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.EC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.EC</link>
    <description>q-fin.EC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.EC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Jun 2024 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>How public funding affects complexity in R&amp;D projects. An analysis of team project perceptions</title>
      <link>https://arxiv.org/abs/2406.00076</link>
      <description>arXiv:2406.00076v1 Announce Type: new 
Abstract: In this paper, we apply a case study approach to advance current understanding of what effects public co-funding of R&amp;D projects have on project team members' perceived complexity. We chose an R&amp;D project carried out by an industrial SME in northern Spain. The chosen research strategy was a qualitative approach, and sixteen employees participated in the project. We held in-depth semi-structured interviews at the beginning and end of the co-funded part of the project. NVivo data analysis software was used for qualitative data analysis. Results showed a substantial increase in perceived complexity. We observed that this was due to unresolved tension between the requirements of the project's co-financing entity and normal SME working procedures. New working procedures needed to be developed in order to comply with the co-financing entity's requirements. However, overall perceived complexity significantly decreased once the co-financed part of the project was completed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00076v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jbusres.2023.113672</arxiv:DOI>
      <arxiv:journal_reference>Journal of Business Research 158, 113672 2023</arxiv:journal_reference>
      <dc:creator>Jose M. Gonzalez-Varona, Natalia Martin-Cruz, Fernando Acebes, Javier Pajares</dc:creator>
    </item>
    <item>
      <title>On the project risk baseline: integrating aleatory uncertainty into project scheduling</title>
      <link>https://arxiv.org/abs/2406.00077</link>
      <description>arXiv:2406.00077v1 Announce Type: new 
Abstract: Obtaining a viable schedule baseline that meets all project constraints is one of the main issues for project managers. The literature on this topic focuses mainly on methods to obtain schedules that meet resource restrictions and, more recently, financial limitations. The methods provide different viable schedules for the same project, and the solutions with the shortest duration are considered the best-known schedule for that project. However, no tools currently select which schedule best performs in project risk terms. To bridge this gap, this paper aims to propose a method for selecting the project schedule with the highest probability of meeting the deadline of several alternative schedules with the same duration. To do so, we propose integrating aleatory uncertainty into project scheduling by quantifying the risk of several execution alternatives for the same project. The proposed method, tested with a well-known repository for schedule benchmarking, can be applied to any project type to help managers to select the project schedules from several alternatives with the same duration, but the lowest risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00077v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>q-fin.RM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cie.2021.107537</arxiv:DOI>
      <arxiv:journal_reference>Computers &amp; Industrial Engineering, 160(2021), 107537. 2021</arxiv:journal_reference>
      <dc:creator>Fernando Acebes, David Poza, Jose M Gonzalez-Varona, Javier Pajares, Adolfo Lopez-Paredes</dc:creator>
    </item>
    <item>
      <title>Integrating solid direct air capture systems with green hydrogen production: Economic synergy of sector coupling</title>
      <link>https://arxiv.org/abs/2406.00665</link>
      <description>arXiv:2406.00665v1 Announce Type: new 
Abstract: In the global pursuit of sustainable energy solutions, mitigating carbon dioxide (CO2) emissions stands as a pivotal challenge. With escalating atmospheric CO2 levels, the imperative of direct air capture (DAC) systems becomes evident. Simultaneously, green hydrogen (GH) emerges as a pivotal medium for renewable energy. Nevertheless, the substantial expenses associated with these technologies impede widespread adoption, primarily due to significant installation costs and underutilized operational advantages when deployed independently. Integration through sector coupling enhances system efficiency and sustainability, while shared power sources and energy storage devices offer additional economic benefits. In this study, we assess the economic viability of polymer electrolyte membrane electrolyzers versus alkaline electrolyzers within the context of sector coupling. Our findings indicate that combining GH production with solid DAC systems yields significant economic advantages, with approximately a 10% improvement for PEM electrolyzers and a 20% enhancement for alkaline electrolyzers. These results highlight a substantial opportunity to improve the efficiency and economic viability of renewable energy and green hydrogen initiatives, thereby facilitating the broader adoption of cleaner technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00665v1</guid>
      <category>econ.GN</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sunwoo Kim, Joungho Park, Jay H. Lee</dc:creator>
    </item>
    <item>
      <title>How Ethical Should AI Be? How AI Alignment Shapes the Risk Preferences of LLMs</title>
      <link>https://arxiv.org/abs/2406.01168</link>
      <description>arXiv:2406.01168v1 Announce Type: new 
Abstract: This study explores the risk preferences of Large Language Models (LLMs) and how the process of aligning them with human ethical standards influences their economic decision-making. By analyzing 30 LLMs, we uncover a broad range of inherent risk profiles ranging from risk-averse to risk-seeking. We then explore how different types of AI alignment, a process that ensures models act according to human values and that focuses on harmlessness, helpfulness, and honesty, alter these base risk preferences. Alignment significantly shifts LLMs towards risk aversion, with models that incorporate all three ethical dimensions exhibiting the most conservative investment behavior. Replicating a prior study that used LLMs to predict corporate investments from company earnings call transcripts, we demonstrate that although some alignment can improve the accuracy of investment forecasts, excessive alignment results in overly cautious predictions. These findings suggest that deploying excessively aligned LLMs in financial decision-making could lead to severe underinvestment. We underline the need for a nuanced approach that carefully balances the degree of ethical alignment with the specific requirements of economic domains when leveraging LLMs within finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01168v1</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shumiao Ouyang, Hayong Yun, Xingjian Zheng</dc:creator>
    </item>
    <item>
      <title>Utilizing Large Language Models for Automating Technical Customer Support</title>
      <link>https://arxiv.org/abs/2406.01407</link>
      <description>arXiv:2406.01407v1 Announce Type: new 
Abstract: The use of large language models (LLMs) such as OpenAI's GPT-4 in technical customer support (TCS) has the potential to revolutionize this area. This study examines automated text correction, summarization of customer inquiries and question answering using LLMs. Through prototypes and data analyses, the potential and challenges of integrating LLMs into the TCS will be demonstrated. Our results show promising approaches for improving the efficiency and quality of customer service through LLMs, but also emphasize the need for quality-assured implementation and organizational adjustments in the data ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01407v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jochen Wulf, J\"urg Meierhofer</dc:creator>
    </item>
    <item>
      <title>Multi-technology co-optimization approach for sustainable hydrogen and electricity supply chains considering variability and demand scale</title>
      <link>https://arxiv.org/abs/2406.00669</link>
      <description>arXiv:2406.00669v1 Announce Type: cross 
Abstract: In the pursuit of a carbon-neutral future, hydrogen emerges as a pivotal element, serving as a carbon-free energy carrier and feedstock. As efforts to decarbonize sectors such as heating and transportation intensify, understanding and navigating through the dynamics of hydrogen demand expansion becomes critical. Transitioning to hydrogen economy is complicated by varying regional scales and types of hydrogen demand, with forecasts indicating a rise in variable demand that calls for diverse production technologies. Currently, steam methane reforming is prevalent, but its significant carbon emissions make a shift to cleaner alternatives like blue and green hydrogen imperative. Each production method possesses distinct characteristics, necessitating a thorough exploration and co-optimization with electricity supply chains as well as carbon capture, utilization, and storage systems. Our study fills existing research gaps by introducing a superstructure optimization framework that accommodates various demand scenarios and technologies. Through case studies in California, we underscore the critical role of demand profiles in shaping the optimal configurations and economics of supply chains and emphasize the need for diversified portfolios and co-optimization to facilitate sustainable energy transitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00669v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sunwoo Kim, Joungho Park, Jay H. Lee</dc:creator>
    </item>
    <item>
      <title>Unpacking the Black Box: Regulating Algorithmic Decisions</title>
      <link>https://arxiv.org/abs/2110.03443</link>
      <description>arXiv:2110.03443v3 Announce Type: replace 
Abstract: What should regulators of complex algorithms regulate? We propose a model of oversight over 'black-box' algorithms used in high-stakes applications such as lending, medical testing, or hiring. In our model, a regulator is limited in how much she can learn about a black-box model deployed by an agent with misaligned preferences. The regulator faces two choices: first, whether to allow for the use of complex algorithms; and second, which key properties of algorithms to regulate. We show that limiting agents to algorithms that are simple enough to be fully transparent is inefficient as long as the misalignment is limited and complex algorithms have sufficiently better performance than simple ones. Allowing for complex algorithms can improve welfare, but the gains depend on how the regulator regulates them. Regulation that focuses on the overall average behavior of algorithms, for example based on standard explainer tools, will generally be inefficient. Targeted regulation that focuses on the source of incentive misalignment, e.g., excess false positives or racial disparities, can provide second-best solutions. We provide empirical support for our theoretical findings using an application in consumer lending, where we document that complex models regulated based on context-specific explanation tools outperform simple, fully transparent models. This gain from complex models represents a Pareto improvement across our empirical applications that is preferred both by the lender and from the perspective of the financial regulator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.03443v3</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-fin.EC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laura Blattner, Scott Nelson, Jann Spiess</dc:creator>
    </item>
    <item>
      <title>The Heterogeneous Productivity Effects of Generative AI</title>
      <link>https://arxiv.org/abs/2403.01964</link>
      <description>arXiv:2403.01964v2 Announce Type: replace 
Abstract: We analyse the individual productivity effects of Italy's ban on ChatGPT, a generative pretrained transformer chatbot. We compile data on the daily coding output quantity and quality of over 36,000 GitHub users in Italy and other European countries and combine these data with the sudden announcement of the ban in a difference-in-differences framework. Among the affected users in Italy, we find a short-term increase in output quantity and quality for less experienced users and a decrease in productivity on more routine tasks for experienced users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01964v2</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Kreitmeir, Paul A. Raschky</dc:creator>
    </item>
    <item>
      <title>Revisiting Granular Models of Firm Growth</title>
      <link>https://arxiv.org/abs/2404.15226</link>
      <description>arXiv:2404.15226v3 Announce Type: replace 
Abstract: We revisit granular models that represent the size of a firm as the sum of the sizes of multiple constituents or sub-units. Originally developed to address the unexpectedly slow reduction in volatility as firm size increases, these models also explain the shape of the distribution of firm growth rates.
  We introduce new theoretical insights regarding the relationship between firm size and growth rate statistics within this framework, directly linking the growth statistics of a firm to how diversified it is. The non-intuitive nature of our results arises from the fat-tailed distributions of the size and the number of sub-units, which suggest the categorization of firms into three distinct diversification types: well-diversified firms with sizes evenly distributed across many sub-units, firms with many sub-units but concentrated size in just a few, and poorly diversified firms consisting of only a small number of sub-units.
  Inspired by our theoretical findings, we identify new empirical patterns in firm growth. Our findings show that growth volatility, when adjusted by average size-conditioned volatility, has a size-independent distribution, but with a tail that is much too thin to be in agreement with the predictions of granular models. Furthermore, the predicted Gaussian distribution of growth rates, even when rescaled for firm-specific volatility, remains fat-tailed across all sizes. Such discrepancies not only challenge the granularity hypothesis but also underscore the need for deeper exploration into the mechanisms driving firm growth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15226v3</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>q-fin.GN</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jos\'e Moran, Angelo Secchi, Jean-Philippe Bouchaud</dc:creator>
    </item>
    <item>
      <title>Exploring the Potential of Large Language Models for Automation in Technical Customer Service</title>
      <link>https://arxiv.org/abs/2405.09161</link>
      <description>arXiv:2405.09161v2 Announce Type: replace 
Abstract: Purpose: The purpose of this study is to investigate the potential of Large Language Models (LLMs) in transforming technical customer service (TCS) through the automation of cognitive tasks. Design/Methodology/Approach: Using a prototyping approach, the research assesses the feasibility of automating cognitive tasks in TCS with LLMs, employing real-world technical incident data from a Swiss telecommunications operator. Findings: Lower-level cognitive tasks such as translation, summarization, and content generation can be effectively automated with LLMs like GPT-4, while higher-level tasks such as reasoning require more advanced technological approaches such as Retrieval-Augmented Generation (RAG) or finetuning ; furthermore, the study underscores the significance of data ecosystems in enabling more complex cognitive tasks by fostering data sharing among various actors involved. Originality/Value: This study contributes to the emerging theory on LLM potential and technical feasibility in service management, providing concrete insights for operators of TCS units and highlighting the need for further research to address limitations and validate the applicability of LLMs across different domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09161v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the Spring Servitization Conference (SSC2024)</arxiv:journal_reference>
      <dc:creator>Jochen Wulf, Juerg Meierhofer</dc:creator>
    </item>
    <item>
      <title>Product Design Using Generative Adversarial Network: Incorporating Consumer Preference and External Data</title>
      <link>https://arxiv.org/abs/2405.15929</link>
      <description>arXiv:2405.15929v2 Announce Type: replace 
Abstract: The development of generative artificial intelligence (AI) enables large-scale product design automation. However, this automated process usually does not incorporate consumer preference information from the internal dataset of a company. Furthermore, external sources such as social media and user-generated content (UGC) websites often contain rich product design and consumer preference information, but such information is not utilized by companies when generating designs. We propose a semi-supervised deep generative framework that integrates consumer preferences and external data into the product design process, allowing companies to generate consumer-preferred designs in a cost-effective and scalable way. We train a predictor model to learn consumer preferences and use predicted popularity levels as additional input labels to guide the training procedure of a continuous conditional generative adversarial network (CcGAN). The CcGAN can be instructed to generate new designs with a certain popularity level, enabling companies to efficiently create consumer-preferred designs and save resources by avoiding the development and testing of unpopular designs. The framework also incorporates existing product designs and consumer preference information from external sources, which is particularly helpful for small or start-up companies that have limited internal data and face the "cold-start" problem. We apply the proposed framework to a real business setting by helping a large self-aided photography chain in China design new photo templates. We show that our proposed model performs well in terms of generating appealing template designs for the company.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15929v2</guid>
      <category>econ.GN</category>
      <category>cs.HC</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hui Li, Jian Ni, Fangzhu Yang</dc:creator>
    </item>
    <item>
      <title>AI Diffusion to Low-Middle Income Countries; A Blessing or a Curse?</title>
      <link>https://arxiv.org/abs/2405.20399</link>
      <description>arXiv:2405.20399v2 Announce Type: replace 
Abstract: Rapid advancements in AI have sparked significant research into its impacts on productivity and labor, which can be profoundly positive or negative. Often overlooked in this debate is understanding of how AI technologies spread across and within economies. Equally ignored are developing economies facing substantial labor market impacts from rapid, and a loss in competitiveness, from slow AI diffusion. This paper reviews literature on technology diffusion and proposes a three-way framework for understanding AI diffusion: global value chains, research collaboration, and inter-firm knowledge transfers. This is used to measure AI diffusion in sixteen low-middle-income, and four developed economies, as well as to evaluate dependence on China and the USA for access to AI technologies. The study finds a significant gap in diffusion rates between the two groups, but current trends indicate it is narrowing. China is identified as a crucial future source of AI diffusion through value chains, while the USA is more influential in research and knowledge transfers. The paper's limitations include the omission of additional data sources and countries, and the lack of investigation into the relationship between diffusion and technology intensity. Nonetheless, it raises salient macro-level questions about AI diffusion and suggests emphasis on redistribution mechanisms of AI induced economic gains, and bilateral agreements as a complement to international accords, to address diverse needs and corresponding risks faced by economies transitioning into an AI-dominated era. Additionally, it highlights the need for research into the links between AI diffusion, technology intensity, and productivity; case studies combined with targeted policy recommendations; more accurate methods for measuring AI diffusion; and a deeper investigation into its labor market impacts particular to LMICs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20399v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rafael Andersson Lipcsey</dc:creator>
    </item>
    <item>
      <title>Tracking the Credibility Revolution across Fields</title>
      <link>https://arxiv.org/abs/2405.20604</link>
      <description>arXiv:2405.20604v2 Announce Type: replace 
Abstract: This paper updates Currie, Kleven, and Zwiers (2020) by examining the credibility revolution across fields, including finance and macroeconomics, using NBER working papers up to May 2024. While the growth in terms related to identification and research designs have continued, finance and macroeconomics have lagged behind applied micro. Difference-in-differences and regression discontinuity designs have risen since 2002, but the growth in difference-in-difference has been larger, more persistent, and more ubiquitous. In contrast, instrumental variables have stayed flat over this period. Finance and macro, particularly corporate finance, has experienced significant growth in mentions of experimental and quasi-experimental methods and identification over this time period, but a large component of the credibility revolution in finance is due to difference-in-differences. Bartik and shift-share instruments have grown across all fields, with the most pronounced growth in international trade and investment, economic history, and labor studies. Synthetic control has not seen continued growth, and has fallen since 2020.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20604v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Goldsmith-Pinkham</dc:creator>
    </item>
    <item>
      <title>Time Instability of the Fama-French Multifactor Models: An International Evidence</title>
      <link>https://arxiv.org/abs/2208.01270</link>
      <description>arXiv:2208.01270v3 Announce Type: replace-cross 
Abstract: This paper investigates the time-varying structure of Fama and French's (1993; 2015) multi-factor models using Fama and MacBeth's (1973) two-step estimation based on the rolling window method. In particular, we employ the generalized GRS statistics proposed by Kamstra and Shi (2024) to examine whether the validity of the risk factors (or factor redundancy) in the FF3 and FF5 models remains stable over time, and investigate whether the manner of portfolio sorting affects the time stability of the validity of the risk factors. In addition, we examine whether the similar results are obtained even when we use different datasets by country and region. First, we find that the effectiveness of factors in the FF3 and FF5 models is not stable over time in all countries. Second, the effectiveness of factors is also affected by the manner of portfolio sorting. Third, the validity of the FF3, FF5, and their nested models do not remain stable over time except for Japan. This suggests that the efficient market hypothesis is supported in the Japanese stock market. Finally, the factor redundancy varies over time and is affected by the manner of portfolio sorting mainly in the U.S. and Europe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.01270v3</guid>
      <category>q-fin.ST</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>q-fin.PR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Koichiro Moriya, Akihiko Noda</dc:creator>
    </item>
  </channel>
</rss>
