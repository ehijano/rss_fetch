<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PL</link>
    <description>cs.PL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Aug 2025 04:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Homomorphism Calculus for User-Defined Aggregations</title>
      <link>https://arxiv.org/abs/2508.15109</link>
      <description>arXiv:2508.15109v1 Announce Type: new 
Abstract: Data processing frameworks like Apache Spark and Flink provide built-in support for user-defined aggregation functions (UDAFs), enabling the integration of domain-specific logic. However, for these frameworks to support \emph{efficient} UDAF execution, the function needs to satisfy a \emph{homomorphism property}, which ensures that partial results from independent computations can be merged correctly. Motivated by this problem, this paper introduces a novel \emph{homomorphism calculus} that can both verify and refute whether a UDAF is a dataframe homomorphism. If so, our calculus also enables the construction of a corresponding merge operator which can be used for incremental computation and parallel execution. We have implemented an algorithm based on our proposed calculus and evaluate it on real-world UDAFs, demonstrating that our approach significantly outperforms two leading synthesizers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15109v1</guid>
      <category>cs.PL</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3763072</arxiv:DOI>
      <arxiv:journal_reference>Proc. ACM Program. Lang. 9, OOPSLA2, Article 294 (October 2025), 27 pages</arxiv:journal_reference>
      <dc:creator>Ziteng Wang, Ruijie Fang, Linus Zheng, Dixin Tang, Isil Dillig</dc:creator>
    </item>
    <item>
      <title>Software Model Checking via Summary-Guided Search (Extended Version)</title>
      <link>https://arxiv.org/abs/2508.15137</link>
      <description>arXiv:2508.15137v1 Announce Type: new 
Abstract: In this work, we describe a new software model-checking algorithm called GPS. GPS treats the task of model checking a program as a directed search of the program states, guided by a compositional, summary-based static analysis. The summaries produced by static analysis are used both to prune away infeasible paths and to drive test generation to reach new, unexplored program states. GPS can find both proofs of safety and counter-examples to safety (i.e., inputs that trigger bugs), and features a novel two-layered search strategy that renders it particularly efficient at finding bugs in programs featuring long, input-dependent error paths. To make GPS refutationally complete (in the sense that it will find an error if one exists, if it is allotted enough time), we introduce an instrumentation technique and show that it helps GPS achieve refutation-completeness without sacrificing overall performance. We benchmarked GPS on a suite of benchmarks including both programs from the Software Verification Competition (SV-COMP) and from prior literature, and found that our implementation of GPS outperforms state-of-the-art software model checkers (including the top performers in SV-COMP ReachSafety-Loops category), both in terms of the number of benchmarks solved and in terms of running time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15137v1</guid>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruijie Fang, Zachary Kincaid, Thomas Reps</dc:creator>
    </item>
    <item>
      <title>Big-Stop Semantics: A Simple Way to Get the Benefits of Small-Step Semantics in a Big-Step Judgment</title>
      <link>https://arxiv.org/abs/2508.15157</link>
      <description>arXiv:2508.15157v1 Announce Type: new 
Abstract: As evident in the programming language literature, many practitioners favor specifying dynamic program behavior using big-step over small-step semantics. Unlike small-step semantics, which must dwell on every intermediate program state, big-step semantics conveniently jump directly to the ever-important result of the computation. Big-step semantics also typically involve fewer inference rules than their small-step counterparts. However, in exchange for ergonomics, big-step semantics give up power: Small-step semantics describes program behaviors that are outside the grasp of big-step semantics, notably divergence. This work presents a little-known extension of big-step semantics with inductive definitions that captures diverging computations without introducing error states. This big-stop semantics is illustrated for typed, untyped, and effectful variants of PCF, as well as a while-loop-based imperative language. Big-stop semantics extends the standard big-step inference rules with a few additional rules to define an evaluation judgment that is equivalent to the reflexive-transitive closure of small-step transitions. This simple extension contrasts with other solutions in the literature which sacrifice ergonomics by introducing many additional inference rules, global state, and/or less-commonly-understood reasoning principles like coinduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15157v1</guid>
      <category>cs.PL</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David M Kahn, Jan Hoffmann, Runming Li</dc:creator>
    </item>
    <item>
      <title>Probabilistic Inference for Datalog with Correlated Inputs</title>
      <link>https://arxiv.org/abs/2508.15166</link>
      <description>arXiv:2508.15166v1 Announce Type: new 
Abstract: Probabilistic extensions of logic programming languages, such as ProbLog, integrate logical reasoning with probabilistic inference to evaluate probabilities of output relations; however, prior work does not account for potential statistical correlations among input facts. This paper introduces Praline, a new extension to Datalog designed for precise probabilistic inference in the presence of (partially known) input correlations. We formulate the inference task as a constrained optimization problem, where the solution yields sound and precise probability bounds for output facts. However, due to the complexity of the resulting optimization problem, this approach alone often does not scale to large programs. To address scalability, we propose a more efficient $\delta$-exact inference algorithm that leverages constraint solving, static analysis, and iterative refinement. Our empirical evaluation on challenging real-world benchmarks, including side-channel analysis, demonstrates that our method not only scales effectively but also delivers tight probability bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15166v1</guid>
      <category>cs.PL</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingbo Wang, Shashin Halalingaiah, Weiyi Chen, Chao Wang, Isil Dillig</dc:creator>
    </item>
    <item>
      <title>Exploring the Theory and Practice of Concurrency in the Entity-Component-System Pattern</title>
      <link>https://arxiv.org/abs/2508.15264</link>
      <description>arXiv:2508.15264v1 Announce Type: new 
Abstract: The Entity-Component-System (ECS) software design pattern, long used in game development, encourages a clean separation of identity (entities), data properties (components), and computational behaviors (systems). Programs written using the ECS pattern are naturally concurrent, and the pattern offers modularity, flexibility, and performance benefits that have led to a proliferation of ECS frameworks. Nevertheless, the ECS pattern is little-known and not well understood outside of a few domains. Existing explanations of the ECS pattern tend to be mired in the concrete details of particular ECS frameworks, or they explain the pattern in terms of imperfect metaphors or in terms of what it is not. We seek a rigorous understanding of the ECS pattern via the design of a formal model, Core ECS, that abstracts away the details of specific implementations to reveal the essence of software using the ECS pattern. We identify a class of Core ECS programs that behave deterministically regardless of scheduling, enabling use of the ECS pattern as a deterministic-by-construction concurrent programming model. With Core ECS as a point of comparison, we then survey several real-world ECS frameworks and find that they all leave opportunities for deterministic concurrency unexploited. Our findings point out a space for new ECS implementation techniques that better leverage such opportunities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15264v1</guid>
      <category>cs.PL</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Redmond, Jonathan Castello, Jos\'e Manuel Calder\'on Trilla, Lindsey Kuper</dc:creator>
    </item>
    <item>
      <title>Fair Termination for Resource-Aware Active Objects</title>
      <link>https://arxiv.org/abs/2508.15333</link>
      <description>arXiv:2508.15333v1 Announce Type: new 
Abstract: Active object systems are a model of distributed computation that has been adopted for modelling distributed systems and business process workflows. This field of modelling is, in essence, concurrent and resource-aware, motivating the development of resource-aware formalisations on the active object model. The contributions of this work are the development of a core calculus for resource-aware active objects together with a type system ensuring that well-typed programs are fairly terminating, i.e., they can always eventually terminate. To achieve this, we combine techniques from graded semantics and type systems, which are quite well understood for sequential programs, with those for fair termination, which have been developed for synchronous~sessions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15333v1</guid>
      <category>cs.PL</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Dagnino, Paola Giannini, Violet Ka I Pun, Ulises Torrella</dc:creator>
    </item>
    <item>
      <title>Compositional Symbolic Execution for the Next 700 Memory Models (Extended Version)</title>
      <link>https://arxiv.org/abs/2508.15576</link>
      <description>arXiv:2508.15576v1 Announce Type: new 
Abstract: Multiple successful compositional symbolic execution (CSE) tools and platforms exploit separation logic (SL) for compositional verification and/or incorrectness separation logic (ISL) for compositional bug-finding, including VeriFast, Viper, Gillian, CN, and Infer-Pulse. Previous work on the Gillian platform, the only CSE platform that is parametric on the memory model, meaning that it can be instantiated to different memory models, suggests that the ability to use custom memory models allows for more flexibility in supporting analysis of a wide range of programming languages, for implementing custom automation, and for improving performance. However, the literature lacks a satisfactory formal foundation for memory-model-parametric CSE platforms.
  In this paper, inspired by Gillian, we provide a new formal foundation for memory-model-parametric CSE platforms. Our foundation advances the state of the art in four ways. First, we mechanise our foundation (in the interactive theorem prover Rocq). Second, we validate our foundation by instantiating it to a broad range of memory models, including models for C and CHERI. Third, whereas previous memory-model-parametric work has only covered SL analyses, we cover both SL and ISL analyses. Fourth, our foundation is based on standard definitions of SL and ISL (including definitions of function specification validity, to ensure sound interoperation with other tools and platforms also based on standard definitions).</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15576v1</guid>
      <category>cs.PL</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3763151</arxiv:DOI>
      <arxiv:journal_reference>OOPSLA 2025</arxiv:journal_reference>
      <dc:creator>Andreas L\"o\"ow, Seung Hoon Park, Daniele Nantes-Sobrinho, Sacha-\'Elie Ayoun, Opale Sj\"ostedt, Philippa Gardner</dc:creator>
    </item>
    <item>
      <title>Active Learning for Neurosymbolic Program Synthesis</title>
      <link>https://arxiv.org/abs/2508.15750</link>
      <description>arXiv:2508.15750v1 Announce Type: new 
Abstract: The goal of active learning for program synthesis is to synthesize the desired program by asking targeted questions that minimize user interaction. While prior work has explored active learning in the purely symbolic setting, such techniques are inadequate for the increasingly popular paradigm of neurosymbolic program synthesis, where the synthesized program incorporates neural components. When applied to the neurosymbolic setting, such techniques can -- and, in practice, do -- return an unintended program due to mispredictions of neural components. This paper proposes a new active learning technique that can handle the unique challenges posed by neural network mispredictions. Our approach is based upon a new evaluation strategy called constrained conformal evaluation (CCE), which accounts for neural mispredictions while taking into account user-provided feedback. Our proposed method iteratively makes CCE more precise until all remaining programs are guaranteed to be observationally equivalent. We have implemented this method in a tool called SmartLabel and experimentally evaluated it on three neurosymbolic domains. Our results demonstrate that SmartLabel identifies the ground truth program for 98% of the benchmarks, requiring under 5 rounds of user interaction on average. In contrast, prior techniques for active learning are only able to converge to the ground truth program for at most 65% of the benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15750v1</guid>
      <category>cs.PL</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3763102</arxiv:DOI>
      <dc:creator>Celeste Barnaby, Qiaochu Chen, Ramya Ramalingam, Osbert Bastani, Isil Dillig</dc:creator>
    </item>
    <item>
      <title>Mini-Batch Robustness Verification of Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2508.15454</link>
      <description>arXiv:2508.15454v1 Announce Type: cross 
Abstract: Neural network image classifiers are ubiquitous in many safety-critical applications. However, they are susceptible to adversarial attacks. To understand their robustness to attacks, many local robustness verifiers have been proposed to analyze $\epsilon$-balls of inputs. Yet, existing verifiers introduce a long analysis time or lose too much precision, making them less effective for a large set of inputs. In this work, we propose a new approach to local robustness: group local robustness verification. The key idea is to leverage the similarity of the network computations of certain $\epsilon$-balls to reduce the overall analysis time. We propose BaVerLy, a sound and complete verifier that boosts the local robustness verification of a set of $\epsilon$-balls by dynamically constructing and verifying mini-batches. BaVerLy adaptively identifies successful mini-batch sizes, accordingly constructs mini-batches of $\epsilon$-balls that have similar network computations, and verifies them jointly. If a mini-batch is verified, all $\epsilon$-balls are proven robust. Otherwise, one $\epsilon$-ball is suspected as not being robust, guiding the refinement. In the latter case, BaVerLy leverages the analysis results to expedite the analysis of that $\epsilon$-ball as well as the other $\epsilon$-balls in the batch. We evaluate BaVerLy on fully connected and convolutional networks for MNIST and CIFAR-10. Results show that BaVerLy scales the common one by one verification by 2.3x on average and up to 4.1x, in which case it reduces the total analysis time from 24 hours to 6 hours.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15454v1</guid>
      <category>cs.LG</category>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3763150</arxiv:DOI>
      <dc:creator>Saar Tzour-Shaday, Dana Drachsler Cohen</dc:creator>
    </item>
    <item>
      <title>Reasoning about Weak Isolation Levels in Separation Logic</title>
      <link>https://arxiv.org/abs/2501.14421</link>
      <description>arXiv:2501.14421v2 Announce Type: replace 
Abstract: Consistency guarantees among concurrently executing transactions in local- and distributed systems, commonly referred to as isolation levels, have been formalized in a number of models. Thus far, no model can reason about executable implementations of databases or local transaction libraries providing weak isolation levels. Weak isolation levels are characterized by being highly concurrent and, unlike their stronger counterpart serializability, they are not equivalent to the consistency guarantees provided by a transaction library implemented using a global lock. Industrial-strength databases almost exclusively implement weak isolation levels as their default level. This calls for formalism as numerous bugs violating isolation have been detected in these databases. In this paper, we formalize three weak isolation levels in separation logic, namely read uncommitted, read committed, and snapshot isolation. We define modular separation logic specifications that are independent of the underlying transaction library implementation. Historically, isolation levels have been specified using examples of executions between concurrent transactions that are not allowed to occur, and we demonstrate that our specifications correctly prohibit such examples. To show that our specifications are realizable, we formally verify that an executable implementation of a key-value database running the multi-version concurrency control algorithm from the original snapshot isolation paper satisfies our specification of snapshot isolation. Moreover, we prove implications between the specifications -- snapshot isolation implies read committed and read committed implies read uncommitted -- and thus the verification effort of the database serves as proof that all of our specifications are realizable. All results are mechanized in the Rocq proof assistant on top of the Iris separation logic framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14421v2</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3747515</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the ACM on Programming Languages, Volume 9, Issue ICFP, August 2025, Article No. 246, Pages 306 - 340</arxiv:journal_reference>
      <dc:creator>Anders Alnor Mathiasen, L\'eon Gondelman, L\'eon Ducruet, Amin Timany, Lars Birkedal</dc:creator>
    </item>
    <item>
      <title>Encode the $\forall\exists$ Relational Hoare Logic into Standard Hoare Logic</title>
      <link>https://arxiv.org/abs/2504.17444</link>
      <description>arXiv:2504.17444v2 Announce Type: replace 
Abstract: Verifying a real-world program's functional correctness can be decomposed into (1) a refinement proof showing that the program implements a more abstract high-level program and (2) an algorithm correctness proof at the high level. Relational Hoare logic serves as a powerful tool to establish refinement but often necessitates formalization beyond standard Hoare logic. Particularly in the nondeterministic setting, the $\forall\exists$ relational Hoare logic is required. Existing approaches encode this logic into a Hoare logic with ghost states and invariants, yet these extensions significantly increase formalization complexity and soundness proof overhead. This paper proposes a generic encoding theory that reduces the $\forall\exists$ relational Hoare logic to standard (unary) Hoare logic. Precisely, we propose to redefine the validity of relational Hoare triples while preserving the original proof rules and then encapsulate the $\forall\exists$ pattern within assertions. We have proved that the validity of encoded standard Hoare triples is equivalent to the validity of the desired relational Hoare triples. Moreover, the encoding theory demonstrates how common relational Hoare logic proof rules are indeed special cases of standard Hoare logic proof rules, and relational proof steps correspond to standard proof steps. Our theory enables standard Hoare logic to prove $\forall\exists$ relational properties by defining a predicate Exec, without requiring modifications to the logic framework or re-verification of soundness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17444v2</guid>
      <category>cs.PL</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3763138</arxiv:DOI>
      <dc:creator>Shushu Wu, Xiwei Wu, Qinxiang Cao</dc:creator>
    </item>
    <item>
      <title>Choreographic Quick Changes: First-Class Location (Set) Polymorphism</title>
      <link>https://arxiv.org/abs/2506.10913</link>
      <description>arXiv:2506.10913v2 Announce Type: replace 
Abstract: Choreographic programming is a promising new paradigm for programming concurrent systems where a developer writes a single centralized program that compiles to individual programs for each node. Existing choreographic languages, however, lack critical features integral to modern systems, like the ability of one node to dynamically compute who should perform a computation and send that decision to others. This work addresses this gap with $\lambda_{QC}$, the first typed choreographic language with \emph{first class process names} and polymorphism over both types and (sets of) locations. $\lambda_{QC}$ also improves expressive power over previous work by supporting algebraic and recursive data types as well as multiply-located values. We formalize and mechanically verify our results in Rocq, including the standard choreographic guarantee of deadlock freedom.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10913v2</guid>
      <category>cs.PL</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashley Samuelson, Andrew K. Hirsch, Ethan Cecchetti</dc:creator>
    </item>
    <item>
      <title>React-tRace: A Semantics for Understanding React Hooks</title>
      <link>https://arxiv.org/abs/2507.05234</link>
      <description>arXiv:2507.05234v2 Announce Type: replace 
Abstract: React has become the most widely used web front-end framework, enabling the creation of user interfaces in a declarative and compositional manner. Hooks are a set of APIs that manage side effects in function components in React. However, their semantics are often seen as opaque to developers, leading to UI bugs. We introduce React-tRace, a formalization of the semantics of the essence of React Hooks, providing a semantics that clarifies their behavior. We demonstrate that our model captures the behavior of React, by theoretically showing that it embodies essential properties of Hooks and empirically comparing our React-tRace-definitional interpreter against a test suite. Furthermore, we showcase a practical visualization tool based on the formalization to demonstrate how developers can better understand the semantics of Hooks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05234v2</guid>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3763067</arxiv:DOI>
      <dc:creator>Jay Lee, Joongwon Ahn, Kwangkeun Yi</dc:creator>
    </item>
    <item>
      <title>Correct Black-Box Monitors for Distributed Deadlock Detection: Formalisation and Implementation (Technical Report)</title>
      <link>https://arxiv.org/abs/2508.14851</link>
      <description>arXiv:2508.14851v2 Announce Type: replace-cross 
Abstract: Many software applications rely on concurrent and distributed (micro)services that interact via message-passing and various forms of remote procedure calls (RPC). As these systems organically evolve and grow in scale and complexity, the risk of introducing deadlocks increases and their impact may worsen: even if only a few services deadlock, many other services may block while awaiting responses from the deadlocked ones. As a result, the "core" of the deadlock can be obfuscated by its consequences on the rest of the system, and diagnosing and fixing the problem can be challenging.
  In this work we tackle the challenge by proposing distributed black-box monitors that are deployed alongside each service and detect deadlocks by only observing the incoming and outgoing messages, and exchanging probes with other monitors. We present a formal model that captures popular RPC-based application styles (e.g., gen_servers in Erlang/OTP), and a distributed black-box monitoring algorithm that we prove sound and complete (i.e., identifies deadlocked services with neither false positives nor false negatives). We implement our results in a tool called DDMon for the monitoring of Erlang/OTP applications, and we evaluate its performance.
  This is the first work that formalises, proves the correctness, and implements distributed black-box monitors for deadlock detection. Our results are mechanised in Coq. DDMon is the companion artifact of this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14851v2</guid>
      <category>cs.LO</category>
      <category>cs.PL</category>
      <pubDate>Fri, 22 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3763069</arxiv:DOI>
      <arxiv:journal_reference>OOPSLA 2025</arxiv:journal_reference>
      <dc:creator>Rados{\l}aw Jan Rowicki, Adrian Francalanza, Alceste Scalas</dc:creator>
    </item>
  </channel>
</rss>
