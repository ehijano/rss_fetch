<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Jan 2025 05:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Rethinking Hard Thresholding Pursuit: Full Adaptation and Sharp Estimation</title>
      <link>https://arxiv.org/abs/2501.02554</link>
      <description>arXiv:2501.02554v1 Announce Type: new 
Abstract: Hard Thresholding Pursuit (HTP) has aroused increasing attention for its robust theoretical guarantees and impressive numerical performance in non-convex optimization. In this paper, we introduce a novel tuning-free procedure, named Full-Adaptive HTP (FAHTP), that simultaneously adapts to both the unknown sparsity and signal strength of the underlying model. We provide an in-depth analysis of the iterative thresholding dynamics of FAHTP, offering refined theoretical insights. In specific, under the beta-min condition $\min_{i \in S^*}|{\boldsymbol{\beta}}^*_i| \ge C\sigma (\log p/n)^{1/2}$, we show that the FAHTP achieves oracle estimation rate $\sigma (s^*/n)^{1/2}$, highlighting its theoretical superiority over convex competitors such as LASSO and SLOPE, and recovers the true support set exactly. More importantly, even without the beta-min condition, our method achieves a tighter error bound than the classical minimax rate with high probability. The comprehensive numerical experiments substantiate our theoretical findings, underscoring the effectiveness and robustness of the proposed FAHTP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02554v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanhang Zhang, Zhifan Li, Shixiang Liu, Xueqin Wang, Jianxin Yin</dc:creator>
    </item>
    <item>
      <title>The Lasso error is bounded iff its active set size is bounded away from n in the proportional regime</title>
      <link>https://arxiv.org/abs/2501.02601</link>
      <description>arXiv:2501.02601v1 Announce Type: new 
Abstract: This note develops an analysis of the Lasso \( \hat b\) in linear models without any sparsity or L1 assumption on the true regression vector, in the proportional regime where dimension \( p \) and sample \( n \) are of the same order. Under Gaussian design and covariance matrix with spectrum bounded away from 0 and $+\infty$, it is shown that the L2 risk is stochastically bounded if and only if the number of selected variables is bounded away from \( n \), in the sense that $$
  (1-\|\hat b\|_0/n)^{-1} = O_P(1)
  \Longleftrightarrow
  \|\hat b- b^*\|_2 = O_P(1) $$ as \( n,p\to+\infty \). The right-to-left implication rules out constant risk for dense Lasso estimates (estimates with close to $n$ active variables), which can be used to discard tuning parameters leading to dense estimates.
  We then bring back sparsity in the picture, and revisit the precise phase transition characterizing the sparsity patterns of the true regression vector leading to unbounded Lasso risk -- or by the above equivalence to dense Lasso estimates. This precise phase transition was established by \citet{miolane2018distribution,celentano2020lasso} using fixed-point equations in an equivalent sequence model. An alternative proof of this phase transition is provided here using simple arguments without relying on the fixed-point equations or the equivalent sequence model. A modification of the well-known Restricted Eigenvalue argument allows to extend the analysis to any small tuning parameter of constant order, leading to a bounded risk on one side of the phase transition. On the other side of the phase transition, it is established the Lasso risk can be unbounded for a given sign pattern as soon as Basis Pursuit fails to recover that sign pattern in noiseless problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02601v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre C. Bellec</dc:creator>
    </item>
    <item>
      <title>Simultaneous analysis of approximate leave-one-out cross-validation and mean-field inference</title>
      <link>https://arxiv.org/abs/2501.02624</link>
      <description>arXiv:2501.02624v1 Announce Type: new 
Abstract: Approximate Leave-One-Out Cross-Validation (ALO-CV) is a method that has been proposed to estimate the generalization error of a regularized estimator in the high-dimensional regime where dimension and sample size are of the same order, the so called ``proportional regime''. A new analysis is developed to derive the consistency of ALO-CV for non-differentiable regularizer under Gaussian covariates and strong-convexity of the regularizer. Using a conditioning argument, the difference between the ALO-CV weights and their counterparts in mean-field inference is shown to be small. Combined with upper bounds between the mean-field inference estimate and the leave-one-out quantity, this provides a proof that ALO-CV approximates the leave-one-out quantity as well up to negligible error terms. Linear models with square loss, robust linear regression and single-index models are explicitly treated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02624v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre C Bellec</dc:creator>
    </item>
    <item>
      <title>On statistical and causal models associated with acyclic directed mixed graphs</title>
      <link>https://arxiv.org/abs/2501.03048</link>
      <description>arXiv:2501.03048v1 Announce Type: new 
Abstract: Causal models in statistics are often described using acyclic directed mixed graphs (ADMGs), which contain directed and bidirected edges and no directed cycles. This article surveys various interpretations of ADMGs, discusses their relations in different sub-classes of ADMGs, and argues that one of them -- nonparametric equation system (the E model below) -- should be used as the default interpretation. The E model is closely related to but different from the interpretation of ADMGs as directed acyclic graphs (DAGs) with latent variables that is commonly found in the literature. Our endorsement of the E model is based on two observations. First, in a subclass of ADMGs called unconfounded graphs (which retain most of the good properties of directed acyclic graphs and bidirected graphs), the E model is equivalent to many other interpretations including the global Markov and nested Markov models. Second, the E model for an arbitrary ADMG is exactly the union of that for all unconfounded expansions of that graph. This property is referred to as completeness, as it shows that the model does not commit to any specific latent variable explanation. In proving that the E model is nested Markov, we also develop an ADMG-based theory for causality. Finally, we criticize the common interpretation of ADMG as a convenient shortcut to represent some unspecified large causal DAG that generate the data. We argue that the "latent DAG" interpretation is mathematically unnecessary, makes obscure ontological assumptions, and discourages practitioners from deliberating over important structural assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03048v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qingyuan Zhao</dc:creator>
    </item>
    <item>
      <title>Phase transition for unregularized M-estimation in single index model</title>
      <link>https://arxiv.org/abs/2501.03163</link>
      <description>arXiv:2501.03163v1 Announce Type: new 
Abstract: This note studies phase transitions for the existence of unregularized M-estimators under proportional asymptotics where the sample size $n$ and feature dimension $p$ grow proportionally with $n/p \to \delta \in (0, \infty)$. We study the existence of M-estimators in single-index models where the response $y_i$ depends on covariates $x_i \sim N(0, I_p)$ through an unknown index $w \in R^p$ and an unknown link function. An explicit expression is derived for the critical threshold $\delta_\infty$ that determines the phase transition for the existence of the M-estimator, generalizing the results of Cand`es and Sur (2020) for binary logistic regression to other single-index linear models.
  Furthermore, we investigate the existence of a solution to the nonlinear system of equations governing the asymptotic behavior of the M-estimator when it exists. The existence of solution to this system for $\delta &gt; \delta_\infty$ remains largely unproven outside the global null in binary logistic regression. We address this gap with a proof that the system admits a solution if and only if $\delta &gt; \delta_\infty$, providing a comprehensive theoretical foundation for proportional asymptotic results that require as a prerequisite the existence of a solution to the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03163v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre C. Bellec, Takuya Koriyama</dc:creator>
    </item>
    <item>
      <title>Statistical learning does not always entail knowledge</title>
      <link>https://arxiv.org/abs/2501.01963</link>
      <description>arXiv:2501.01963v1 Announce Type: cross 
Abstract: In this paper, we study learning and knowledge acquisition (LKA) of an agent about a proposition that is either true or false. We use a Bayesian approach, where the agent receives data to update his beliefs about the proposition according to a posterior distribution. The LKA is formulated in terms of active information, with data representing external or exogenous information that modifies the agent's beliefs. It is assumed that data provide details about a number of features that are relevant to the proposition. We show that this leads to a Gibbs distribution posterior, which is in maximum entropy relative to the prior, conditioned on the side constraints that the data provide in terms of the features. We demonstrate that full learning is sometimes not possible and full knowledge acquisition is never possible when the number of extracted features is too small. We also distinguish between primary learning (receiving data about features of relevance for the proposition) and secondary learning (receiving data about the learning of another agent). We argue that this type of secondary learning does not represent true knowledge acquisition. Our results have implications for statistical learning algorithms, and we claim that such algorithms do not always generate true knowledge. The theory is illustrated with several examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01963v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel Andr\'es D\'iaz-Pach\'on, H. Renata Gallegos, Ola H\"ossjer, J. Sunil Rao</dc:creator>
    </item>
    <item>
      <title>Laws of thermodynamics for exponential families</title>
      <link>https://arxiv.org/abs/2501.02071</link>
      <description>arXiv:2501.02071v1 Announce Type: cross 
Abstract: We develop the laws of thermodynamics in terms of general exponential families. By casting learning (log-loss minimization) problems in max-entropy and statistical mechanics terms, we translate thermodynamics results to learning scenarios. We extend the well-known way in which exponential families characterize thermodynamic and learning equilibria. Basic ideas of work and heat, and advanced concepts of thermodynamic cycles and equipartition of energy, find exact and useful counterparts in AI / statistics terms. These ideas have broad implications for quantifying and addressing distribution shift.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02071v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akshay Balsubramani</dc:creator>
    </item>
    <item>
      <title>A Statistical Hypothesis Testing Framework for Data Misappropriation Detection in Large Language Models</title>
      <link>https://arxiv.org/abs/2501.02441</link>
      <description>arXiv:2501.02441v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are rapidly gaining enormous popularity in recent years. However, the training of LLMs has raised significant privacy and legal concerns, particularly regarding the inclusion of copyrighted materials in their training data without proper attribution or licensing, which falls under the broader issue of data misappropriation. In this article, we focus on a specific problem of data misappropriation detection, namely, to determine whether a given LLM has incorporated data generated by another LLM. To address this issue, we propose embedding watermarks into the copyrighted training data and formulating the detection of data misappropriation as a hypothesis testing problem. We develop a general statistical testing framework, construct a pivotal statistic, determine the optimal rejection threshold, and explicitly control the type I and type II errors. Furthermore, we establish the asymptotic optimality properties of the proposed tests, and demonstrate its empirical effectiveness through intensive numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02441v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinpeng Cai, Lexin Li, Linjun Zhang</dc:creator>
    </item>
    <item>
      <title>High-dimensional inference for single-index model with latent factors</title>
      <link>https://arxiv.org/abs/2501.02489</link>
      <description>arXiv:2501.02489v1 Announce Type: cross 
Abstract: Models with latent factors recently attract a lot of attention. However, most investigations focus on linear regression models and thus cannot capture nonlinearity. To address this issue, we propose a novel Factor Augmented Single-Index Model. We first address the concern whether it is necessary to consider the augmented part by introducing a score-type test statistic. Compared with previous test statistics, our proposed test statistic does not need to estimate the high-dimensional regression coefficients, nor high-dimensional precision matrix, making it simpler in implementation. We also propose a Gaussian multiplier bootstrap to determine the critical value. The validity of our procedure is theoretically established under suitable conditions. We further investigate the penalized estimation of the regression model. With estimated latent factors, we establish the error bounds of the estimators. Lastly, we introduce debiased estimator and construct confidence interval for individual coefficient based on the asymptotic normality. No moment condition for the error term is imposed for our proposal. Thus our procedures work well when random error follows heavy-tailed distributions or when outliers are present. We demonstrate the finite sample performance of the proposed method through comprehensive numerical studies and its application to an FRED-MD macroeconomics dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02489v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanmei Shi, Meiling Hao, Yanlin Tang, Heng Lian, Xu Guo</dc:creator>
    </item>
    <item>
      <title>Upper and lower bounds on TVD and KLD between centered elliptical distributions in high-dimensional setting</title>
      <link>https://arxiv.org/abs/2501.02553</link>
      <description>arXiv:2501.02553v1 Announce Type: cross 
Abstract: In this paper, we derive some upper and lower bounds and inequalities for the total variation distance (TVD) and the Kullback-Leibler divergence (KLD), also known as the relative entropy, between two probability measures $\mu$ and $\nu$ defined by $$
  D_{\mathrm{TV}} ( \mu, \nu ) = \sup_{B \in \mathcal{B} (\mathbb{R}^n)}
  \left| \mu(B) - \nu(B) \right|
  \quad \text{and} \quad
  D_{\mathrm{KL}} ( \mu \, \| \, \nu ) = \int_{\mathbb{R}^n}
  \ln \left( \frac{d\mu(x)}{d\nu(x)} \right) \, \mu(dx) $$ correspondingly when the dimension $n$ is high. We begin with some elementary bounds for centered elliptical distributions admitting densities and showcase how these bounds may be used by estimating the TVD and KLD between multivariate Student and multivariate normal distribution in the high-dimensional setting. Next, we show how the same approach simplifies when we apply it to multivariate Gamma distributions with independent components (in the latter case, we only study the TVD, because KLD may be calculated explicitly, see [1]). Our approach is motivated by the recent contribution by Barabesi and Pratelli [2].</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02553v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ievlev Pavel, Timofei Shashkov</dc:creator>
    </item>
    <item>
      <title>A Large-dimensional Analysis of ESPRIT DoA Estimation: Inconsistency and a Correction via RMT</title>
      <link>https://arxiv.org/abs/2501.02746</link>
      <description>arXiv:2501.02746v1 Announce Type: cross 
Abstract: In this paper, we perform asymptotic analyses of the widely used ESPRIT direction-of-arrival (DoA) estimator for large arrays, where the array size $N$ and the number of snapshots $T$ grow to infinity at the same pace. In this large-dimensional regime, the sample covariance matrix (SCM) is known to be a poor eigenspectral estimator of the population covariance. We show that the classical ESPRIT algorithm, that relies on the SCM, and as a consequence of the large-dimensional inconsistency of the SCM, produces inconsistent DoA estimates as $N,T \to \infty$ with $N/T \to c \in (0,\infty)$, for both widely- and closely-spaced DoAs. Leveraging tools from random matrix theory (RMT), we propose an improved G-ESPRIT method and prove its consistency in the same large-dimensional setting. From a technical perspective, we derive a novel bound on the eigenvalue differences between two potentially non-Hermitian random matrices, which may be of independent interest. Numerical simulations are provided to corroborate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02746v1</guid>
      <category>eess.SP</category>
      <category>math.PR</category>
      <category>math.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengyu Wang, Wei Yang, Xiaoyi Mai, Zenan Ling, Zhenyu Liao, Robert C. Qiu</dc:creator>
    </item>
    <item>
      <title>A Stable Measure for Conditional Periodicity of Time Series using Persistent Homology</title>
      <link>https://arxiv.org/abs/2501.02817</link>
      <description>arXiv:2501.02817v1 Announce Type: cross 
Abstract: Given a pair of time series, we study how the periodicity of one influences the periodicity of the other. There are several known methods to measure the similarity between a pair of time series, such as cross-correlation, coherence, cross-recurrence, and dynamic time warping. But we have yet to find any measures with theoretical stability results.
  Persistence homology has been utilized to construct a scoring function with theoretical guarantees of stability that quantifies the periodicity of a single univariate time series f1, denoted score(f1). Building on this concept, we propose a conditional periodicity score that quantifies the periodicity of one univariate time series f1 given another f2, denoted score(f1|f2), and derive theoretical stability results for the same. With the use of dimension reduction in mind, we prove a new stability result for score(f1|f2) under principal component analysis (PCA) when we use the projections of the time series embeddings onto their respective first K principal components. We show that the change in our score is bounded by a function of the eigenvalues corresponding to the remaining (unused) N-K principal components and hence is small when the first K principal components capture most of the variation in the time series embeddings. Finally we derive a lower bound on the minimum embedding dimension to use in our pipeline which guarantees that any two such embeddings give scores that are within a given epsilon of each other.
  We present a procedure for computing conditional periodicity scores and implement it on several pairs of synthetic signals. We experimentally compare our similarity measure to the most-similar statistical measure of cross-recurrence, and show the increased accuracy and stability of our score when predicting and measuring whether or not the periodicities of two time series are similar.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02817v1</guid>
      <category>math.AT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bala Krishnamoorthy, Elizabeth P. Thompson</dc:creator>
    </item>
    <item>
      <title>Coarsened confounding for causal effects: a large-sample framework</title>
      <link>https://arxiv.org/abs/2501.03129</link>
      <description>arXiv:2501.03129v1 Announce Type: cross 
Abstract: There has been widespread use of causal inference methods for the rigorous analysis of observational studies and to identify policy evaluations. In this article, we consider coarsened exact matching, developed in Iacus et al. (2011). While they developed some statistical properties, in this article, we study the approach using asymptotics based on a superpopulation inferential framework. This methodology is generalized to what we termed as coarsened confounding, for which we propose two new algorithms. We develop asymptotic results for the average causal effect estimator as well as providing conditions for consistency. In addition, we provide an asymptotic justification for the variance formulae in Iacus et al. (2011). A bias correction technique is proposed, and we apply the proposed methodology to data from two well-known observational studi</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03129v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Debashis Ghosh, Lei Wang</dc:creator>
    </item>
    <item>
      <title>Is completeness necessary? Estimation in nonidentified linear models</title>
      <link>https://arxiv.org/abs/1709.03473</link>
      <description>arXiv:1709.03473v5 Announce Type: replace 
Abstract: Modern data analysis depends increasingly on estimating models via flexible high-dimensional or nonparametric machine learning methods, where the identification of structural parameters is often challenging and untestable. In linear settings, this identification hinges on the completeness condition, which requires the nonsingularity of a high-dimensional matrix or operator and may fail for finite samples or even at the population level. Regularized estimators provide a solution by enabling consistent estimation of structural or average structural functions, sometimes even under identification failure. We show that the asymptotic distribution in these cases can be nonstandard. We develop a comprehensive theory of regularized estimators, which include methods such as high-dimensional ridge regularization, gradient descent, and principal component analysis (PCA). The results are illustrated for high-dimensional and nonparametric instrumental variable regressions and are supported through simulation experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:1709.03473v5</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrii Babii, Jean-Pierre Florens</dc:creator>
    </item>
    <item>
      <title>The noise barrier and the large signal bias of the Lasso and other convex estimators</title>
      <link>https://arxiv.org/abs/1804.01230</link>
      <description>arXiv:1804.01230v5 Announce Type: replace 
Abstract: Convex estimators such as the Lasso, the matrix Lasso and the group Lasso have been studied extensively in the last two decades, demonstrating great success in both theory and practice. Two quantities are introduced, the noise barrier and the large scale bias, that provides insights on the performance of these convex regularized estimators. It is now well understood that the Lasso achieves fast prediction rates, provided that the correlations of the design satisfy some Restricted Eigenvalue or Compatibility condition, and provided that the tuning parameter is large enough. Using the two quantities introduced in the paper, we show that the compatibility condition on the design matrix is actually unavoidable to achieve fast prediction rates with the Lasso. The Lasso must incur a loss due to the correlations of the design matrix, measured in terms of the compatibility constant. This results holds for any design matrix, any active subset of covariates, and any tuning parameter. It is now well known that the Lasso enjoys a dimension reduction property: the prediction error is of order $\lambda\sqrt k$ where $k$ is the sparsity; even if the ambient dimension $p$ is much larger than $k$. Such results require that the tuning parameters is greater than some universal threshold. We characterize sharp phase transitions for the tuning parameter of the Lasso around a critical threshold dependent on $k$. If $\lambda$ is equal or larger than this critical threshold, the Lasso is minimax over $k$-sparse target vectors. If $\lambda$ is equal or smaller than critical threshold, the Lasso incurs a loss of order $\sigma\sqrt k$ -- which corresponds to a model of size $k$ -- even if the target vector has fewer than $k$ nonzero coefficients. Remarkably, the lower bounds obtained in the paper also apply to random, data-driven tuning parameters. The results extend to convex penalties beyond the Lasso.</description>
      <guid isPermaLink="false">oai:arXiv.org:1804.01230v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre C Bellec</dc:creator>
    </item>
    <item>
      <title>Clusterization in D-optimal designs: the case against linearization</title>
      <link>https://arxiv.org/abs/2007.12032</link>
      <description>arXiv:2007.12032v3 Announce Type: replace 
Abstract: Estimation of parameters in physical processes often demands costly measurements, prompting the pursuit of an optimal measurement strategy. Finding such strategy is termed the problem of optimal experimental design, abbreviated as optimal design. Remarkably, optimal designs can yield tightly clustered measurement locations, leading researchers to fundamentally revise the design problem just to circumvent this issue. Some authors introduce error correlation among error terms that are initially independent, while others restrict measurement locations to a finite set of locations. While both approaches may prevent clusterization, they also fundamentally alter the optimal design problem.
  In this study, we consider Bayesian D-optimal designs, i.e.~designs that maximize the expected Kullback-Leibler divergence between posterior and prior. We propose an analytically tractable model for D-optimal designs over Hilbert spaces. In this framework, we make several key contributions: (a) We establish that measurement clusterization is a generic trait of D-optimal designs for linear inverse problems with independent Gaussian measurement errors and a Gaussian prior. (b) We prove that introducing correlations among measurement error terms mitigates clusterization. (c) We characterize D-optimal designs as reducing uncertainty across a subset of prior covariance eigenvectors. (d) We leverage this characterization to argue that measurement clusterization arises as a consequence of the pigeonhole principle: when more measurements are taken than there are locations where the select eigenvectors are large and others are small -- clusterization occurs. Finally, we use our analysis to argue against the use of Gaussian priors with linearized physical models when seeking a D-optimal design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.12032v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yair Daon</dc:creator>
    </item>
    <item>
      <title>From Poisson Observations to Fitted Negative Binomial Distribution</title>
      <link>https://arxiv.org/abs/2404.07457</link>
      <description>arXiv:2404.07457v2 Announce Type: replace 
Abstract: Negative binomial distribution has been widely used as a more flexible model than Poisson distribution for count data. When the observations come from a Poisson distribution, it is often challenging to rule out the possibility that the data come from a negative binomial distribution with extreme parameter values. To address this phenomenon, we develop a more efficient and accurate algorithm for finding the maximum likelihood estimate of negative binomial parameters, which outperforms the state-of-the-art programs for the same purpose. We also theoretically justify that the negative binomial distribution with parameters estimated from Poisson data converges to the true Poisson distribution with probability one. As a solution to this phenomenon, we extend the negative binomial distributions with a new parameterization, which include Poisson distributions as a special class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07457v2</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingying Yang, Niloufar Dousti Mousavi, Zhou Yu, Jie Yang</dc:creator>
    </item>
    <item>
      <title>Trimmed Mean for Partially Observed Functional Data</title>
      <link>https://arxiv.org/abs/2408.13062</link>
      <description>arXiv:2408.13062v2 Announce Type: replace 
Abstract: In recent years, partially observable functional data has gained significant attention in practical applications and has become the focus of increasing interest in the literature. In this thesis, we build upon the concept of data integration depth for partially observable functions, as proposed by Elias et al. (2023), and the trimmed-mean estimator method along with its consistency proof introduced by Fraiman and Muniz (2001) for completely observable functions. We introduce the concept of trimmed mean specifically for partially observable functional data. Additionally, we address several theoretical and practical issues, including a proof of the strong consistency of the proposed trimmed mean, and we provide a simulation study. The results demonstrate that our estimator outperforms the ordinary mean in terms of accuracy and robustness when applied to partially observable functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13062v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixiao Wang</dc:creator>
    </item>
    <item>
      <title>Optimality of the Right-Invariant Prior</title>
      <link>https://arxiv.org/abs/2412.12054</link>
      <description>arXiv:2412.12054v4 Announce Type: replace 
Abstract: In this paper, we discuss optimal next-sample prediction for families of probability distributions with a locally compact topological group structure. The right-invariant prior was previously shown to yield a posterior predictive distribution minimizing the worst-case Kullback-Leibler risk among all predictive procedures. However, the assumptions for the proof are so strong that they rarely hold in practice and it is unclear when the density functions used in the proof exist. Therefore, we provide a measure-theoretic proof using a more appropriate set of assumptions. As an application, we show a strong optimality result for next-sample prediction for multivariate normal distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12054v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jannis Bolik, Thomas Hofmann</dc:creator>
    </item>
    <item>
      <title>Least squares estimation in nonstationary nonlinear cohort panels with learning from experience</title>
      <link>https://arxiv.org/abs/2309.08982</link>
      <description>arXiv:2309.08982v4 Announce Type: replace-cross 
Abstract: We discuss techniques of estimation and inference for nonstationary nonlinear cohort panels with learning from experience, showing, inter alia, the consistency and asymptotic normality of the nonlinear least squares estimator used in empirical practice. Potential pitfalls for hypothesis testing are identified and solutions proposed. Monte Carlo simulations verify the properties of the estimator and corresponding test statistics in finite samples, while an application to a panel of survey expectations demonstrates the usefulness of the theory developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08982v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alexander Mayer, Michael Massmann</dc:creator>
    </item>
    <item>
      <title>A central limit theorem for random tangent fields on stratified spaces</title>
      <link>https://arxiv.org/abs/2311.09454</link>
      <description>arXiv:2311.09454v3 Announce Type: replace-cross 
Abstract: Variation of empirical Fr\'echet means on a metric space with curvature bounded above is encoded via random fields indexed by unit tangent vectors. A central limit theorem shows these random tangent fields converge to a Gaussian such field and lays the foundation for more traditionally formulated central limit theorems in subsequent work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09454v3</guid>
      <category>math.PR</category>
      <category>math.MG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan C. Mattingly, Ezra Miller, Do Tran</dc:creator>
    </item>
    <item>
      <title>Finely Stratified Rerandomization Designs</title>
      <link>https://arxiv.org/abs/2407.03279</link>
      <description>arXiv:2407.03279v3 Announce Type: replace-cross 
Abstract: We study estimation and inference on causal parameters under finely stratified rerandomization designs, which use baseline covariates to match units into groups (e.g. matched pairs), then rerandomize within-group treatment assignments until a balance criterion is satisfied. We show that finely stratified rerandomization does partially linear regression adjustment by design, providing nonparametric control over the stratified covariates and linear control over the rerandomized covariates. We introduce several new forms of rerandomization, allowing for imbalance metrics based on nonlinear estimators, and proposing a minimax scheme that minimizes the computational cost of rerandomization subject to a bound on estimation error. While the asymptotic distribution of GMM estimators under stratified rerandomization is generically non-normal, we show how to restore asymptotic normality using ex-post linear adjustment tailored to the stratification. We derive new variance bounds that enable conservative inference on finite population causal parameters, and provide asymptotically exact inference on their superpopulation counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03279v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Max Cytrynbaum</dc:creator>
    </item>
    <item>
      <title>Learning Spectral Methods by Transformers</title>
      <link>https://arxiv.org/abs/2501.01312</link>
      <description>arXiv:2501.01312v2 Announce Type: replace-cross 
Abstract: Transformers demonstrate significant advantages as the building block of modern LLMs. In this work, we study the capacities of Transformers in performing unsupervised learning. We show that multi-layered Transformers, given a sufficiently large set of pre-training instances, are able to learn the algorithms themselves and perform statistical estimation tasks given new instances. This learning paradigm is distinct from the in-context learning setup and is similar to the learning procedure of human brains where skills are learned through past experience. Theoretically, we prove that pre-trained Transformers can learn the spectral methods and use the classification of bi-class Gaussian mixture model as an example. Our proof is constructive using algorithmic design techniques. Our results are built upon the similarities of multi-layered Transformer architecture with the iterative recovery algorithms used in practice. Empirically, we verify the strong capacity of the multi-layered (pre-trained) Transformer on unsupervised learning through the lens of both the PCA and the Clustering tasks performed on the synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01312v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 07 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yihan He, Yuan Cao, Hong-Yu Chen, Dennis Wu, Jianqing Fan, Han Liu</dc:creator>
    </item>
  </channel>
</rss>
