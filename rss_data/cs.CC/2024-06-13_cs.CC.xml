<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 14 Jun 2024 01:42:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Relations between monotone complexity measures based on decision tree complexity</title>
      <link>https://arxiv.org/abs/2406.07859</link>
      <description>arXiv:2406.07859v2 Announce Type: new 
Abstract: In a recent result, Knop, Lovett, McGuire and Yuan (STOC 2021) proved the log-rank conjecture for communication complexity, up to log n factor, for any Boolean function composed with AND function as the inner gadget. One of the main tools in this result was the relationship between monotone analogues of well-studied Boolean complexity measures like block sensitivity and certificate complexity. The relationship between the standard measures has been a long line of research, with a landmark result by Huang (Annals of Mathematics 2019), finally showing that sensitivity is polynomially related to all other standard measures. In this article, we study the monotone analogues of standard measures like block sensitivity (mbs(f)), certificate complexity (MCC(f)) and fractional block sensitivity (fmbs(f)); and study the relationship between these measures given their connection with AND-decision tree and sparsity of a Boolean function. We show the following results: 1) Given a Boolean function $f : \{0, 1\}^{n} \rightarrow \{0, 1\}$, the ratio $fmbs(f^l )/mbs(f^l )$ is bounded by a function of n (and not l). A similar result was known for the corresponding standard measures (Tal, ITCS 2013). This result allows us to extend any upper bound by a well behaved measure on monotone block sensitivity to monotone fractional block sensitivity. 2) The question of the best possible upper bound on monotone block sensitivity by the logarithm of sparsity is equivalent to the natural question of best upper bound by degree on sensitivity. One side of this relationship was used in the proof by Knop, Lovett, McGuire and Yuan (STOC 2021). 3) For two natural classes of functions, symmetric and monotone, hitting set complexity (MCC) is equal to monotone sensitivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07859v2</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Farzan Byramji, Vatsal Jha, Chandrima Kayal, Rajat Mittal</dc:creator>
    </item>
    <item>
      <title>Resource Leveling: Complexity of a UET two-processor scheduling variant and related problems</title>
      <link>https://arxiv.org/abs/2406.08104</link>
      <description>arXiv:2406.08104v1 Announce Type: new 
Abstract: This paper mainly focuses on a resource leveling variant of a two-processor scheduling problem. The latter problem is to schedule a set of dependent UET jobs on two identical processors with minimum makespan. It is known to be polynomial-time solvable.
  In the variant we consider, the resource constraint on processors is relaxed and the objective is no longer to minimize makespan. Instead, a deadline is imposed on the makespan and the objective is to minimize the total resource use exceeding a threshold resource level of two. This resource leveling criterion is known as the total overload cost. Sophisticated matching arguments allow us to provide a polynomial algorithm computing the optimal solution as a function of the makespan deadline. It extends a solving method from the literature for the two-processor scheduling problem.
  Moreover, the complexity of related resource leveling problems sharing the same objective is studied. These results lead to polynomial or pseudo-polynomial algorithms or NP-hardness proofs, allowing for an interesting comparison with classical machine scheduling problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08104v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pascale Bendotti, Luca Brunod Indrigo, Philippe Chr\'etienne, Bruno Escoffier</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Learning and Planning in Separated Latent MDPs</title>
      <link>https://arxiv.org/abs/2406.07920</link>
      <description>arXiv:2406.07920v1 Announce Type: cross 
Abstract: We study computational and statistical aspects of learning Latent Markov Decision Processes (LMDPs). In this model, the learner interacts with an MDP drawn at the beginning of each epoch from an unknown mixture of MDPs. To sidestep known impossibility results, we consider several notions of separation of the constituent MDPs. The main thrust of this paper is in establishing a nearly-sharp *statistical threshold* for the horizon length necessary for efficient learning. On the computational side, we show that under a weaker assumption of separability under the optimal policy, there is a quasi-polynomial algorithm with time complexity scaling in terms of the statistical threshold. We further show a near-matching time complexity lower bound under the exponential time hypothesis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07920v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Chen, Constantinos Daskalakis, Noah Golowich, Alexander Rakhlin</dc:creator>
    </item>
    <item>
      <title>On $[1,2]$-Domination in Interval and Circle Graphs</title>
      <link>https://arxiv.org/abs/2403.04694</link>
      <description>arXiv:2403.04694v2 Announce Type: replace 
Abstract: A subset $S$ of vertices in a graph $G=(V, E)$ is Dominating Set if each vertex in $V(G)\setminus S$ is adjacent to at least one vertex in $S$. Chellali et al. in 2013, by restricting the number of neighbors in $S$ of a vertex outside $S$, introduced the concept of $[1,j]$-dominating set. A set $D \subseteq V$ of a graph $G = (V, E)$ is called $[1,j]$-Dominating Set of $G$ if every vertex not in $D$ has at least one neighbor and at most $j$ neighbors in $D$. The Minimum $[1,j]$-Domination problem is the problem of finding the minimum set $D$. Given a positive integer $k$ and a graph $G = (V, E)$, the $[1,j]$-Domination Decision problem is to decide whether $G$ has $[1,j]$-dominating set of cardinality at most $k$. A polynomial-time algorithm was obtained in split graphs for a constant $j$ in contrast to the classic Dominating Set problem which is NP-hard in split graphs. This result motivates us to investigate the effect of restriction $j$ on the complexity of $[1,j]$-domination problem on various classes of graphs. Although for $j\geq 3$, it has been proved that the minimum of classical domination is equal to minimum $[1,j]$-domination in interval graphs, the complexity of finding the minimum $[1,2]$-domination in interval graphs is still outstanding. In this paper, we propose a polynomial-time algorithm for computing a minimum $[1,2]$ on non-proper interval graphs by a dynamic programming technique. Next, on the negative side, we show that the minimum $[1,2]$-dominating set problem on circle graphs is $NP$-complete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04694v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Alambardar Meybodi, Abolfazl Poureidi</dc:creator>
    </item>
    <item>
      <title>Almost Optimal Time Lower Bound for Approximating Parameterized Clique, CSP, and More, under ETH</title>
      <link>https://arxiv.org/abs/2404.08870</link>
      <description>arXiv:2404.08870v2 Announce Type: replace 
Abstract: The Parameterized Inapproximability Hypothesis (PIH), which is an analog of the PCP theorem in parameterized complexity, asserts that, there is a constant $\varepsilon&gt; 0$ such that for any computable function $f:\mathbb{N}\to\mathbb{N}$, no $f(k)\cdot n^{O(1)}$-time algorithm can, on input a $k$-variable CSP instance with domain size $n$, find an assignment satisfying $1-\varepsilon$ fraction of the constraints. A recent work by Guruswami, Lin, Ren, Sun, and Wu (STOC'24) established PIH under the Exponential Time Hypothesis (ETH).
  In this work, we improve the quantitative aspects of PIH and prove (under ETH) that approximating sparse parameterized CSPs within a constant factor requires $n^{k^{1-o(1)}}$ time. This immediately implies that, assuming ETH, finding a $(k/2)$-clique in an $n$-vertex graph with a $k$-clique requires $n^{k^{1-o(1)}}$ time. We also prove almost optimal time lower bounds for approximating $k$-ExactCover and Max $k$-Coverage.
  Our proof follows the blueprint of the previous work to identify a "vector-structured" ETH-hard CSP whose satisfiability can be checked via an appropriate form of "parallel" PCP. Using further ideas in the reduction, we guarantee additional structures for constraints in the CSP. We then leverage this to design a parallel PCP of almost linear size based on Reed-Muller codes and derandomized low degree testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08870v2</guid>
      <category>cs.CC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Venkatesan Guruswami, Bingkai Lin, Xuandi Ren, Yican Sun, Kewen Wu</dc:creator>
    </item>
    <item>
      <title>Filter Pruning based on Information Capacity and Independence</title>
      <link>https://arxiv.org/abs/2303.03645</link>
      <description>arXiv:2303.03645v2 Announce Type: replace-cross 
Abstract: Filter pruning has gained widespread adoption for the purpose of compressing and speeding up convolutional neural networks (CNNs). However, existing approaches are still far from practical applications due to biased filter selection and heavy computation cost. This paper introduces a new filter pruning method that selects filters in an interpretable, multi-perspective, and lightweight manner. Specifically, we evaluate the contributions of filters from both individual and overall perspectives. For the amount of information contained in each filter, a new metric called information capacity is proposed. Inspired by the information theory, we utilize the interpretable entropy to measure the information capacity, and develop a feature-guided approximation process. For correlations among filters, another metric called information independence is designed. Since the aforementioned metrics are evaluated in a simple but effective way, we can identify and prune the least important filters with less computation cost. We conduct comprehensive experiments on benchmark datasets employing various widely-used CNN architectures to evaluate the performance of our method. For instance, on ILSVRC-2012, our method outperforms state-of-the-art methods by reducing FLOPs by 77.4% and parameters by 69.3% for ResNet-50 with only a minor decrease in accuracy of 2.64%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.03645v2</guid>
      <category>cs.CV</category>
      <category>cs.CC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaolong Tang, Shuo Ye, Yufeng Shi, Tianheng Hu, Qinmu Peng, Xinge You</dc:creator>
    </item>
    <item>
      <title>Additive approximation algorithm for geodesic centers in $\delta$-hyperbolic graphs</title>
      <link>https://arxiv.org/abs/2404.03812</link>
      <description>arXiv:2404.03812v2 Announce Type: replace-cross 
Abstract: For an integer $k\geq 1$, the objective of \textsc{$k$-Geodesic Center} is to find a set $\mathcal{C}$ of $k$ isometric paths such that the maximum distance between any vertex $v$ and $\mathcal{C}$ is minimised. Introduced by Gromov, \emph{$\delta$-hyperbolicity} measures how treelike a graph is from a metric point of view. Our main contribution in this paper is to provide an additive $O(\delta)$-approximation algorithm for \textsc{$k$-Geodesic Center} on $\delta$-hyperbolic graphs. On the way, we define a coarse version of the pairing property introduced by Gerstel \&amp; Zaks (Networks, 1994) and show it holds for $\delta$-hyperbolic graphs. This result allows to reduce the \textsc{$k$-Geodesic Center} problem to its rooted counterpart, a main idea behind our algorithm. We also adapt a technique of Dragan \&amp; Leitert, (TCS, 2017) to show that for every $k\geq 1$, $k$-\textsc{Geodesic Center} is NP-hard even on partial grids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03812v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dibyayan Chakraborty, Yann Vax\`es</dc:creator>
    </item>
    <item>
      <title>Maximal Line Digraphs</title>
      <link>https://arxiv.org/abs/2406.05141</link>
      <description>arXiv:2406.05141v2 Announce Type: replace-cross 
Abstract: A line digraph $L(G) = (A, E)$ is the digraph constructed from the digraph $G = (V, A)$ such that there is an arc $(a,b)$ in $L(G)$ if the terminal node of $a$ in $G$ is the initial node of $b$. The maximum number of arcs in a line digraph with $m$ nodes is $(m/2)^2 + (m/2)$ if $m$ is even, and $((m - 1)/2)^2 + m - 1$ otherwise. For $m \geq 7$, there is only one line digraph with as many arcs if $m$ is even, and if $m$ is odd, there are two line digraphs, each being the transpose of the other.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05141v2</guid>
      <category>cs.DM</category>
      <category>cs.CC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quentin Japhet (DAVID), Dimitri Watel (IP Paris, SAMOVAR, SOP - SAMOVAR, ENSIIE), Dominique Barth (DAVID), Marc-Antoine Weisser (GALaC)</dc:creator>
    </item>
  </channel>
</rss>
