<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 31 Jul 2024 01:46:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Versatility Measure for Parametric Risk Models</title>
      <link>https://arxiv.org/abs/2407.19218</link>
      <description>arXiv:2407.19218v1 Announce Type: new 
Abstract: Parametric statistical methods play a central role in analyzing risk through its underlying frequency and severity components. Given the wide availability of numerical algorithms and high-speed computers, researchers and practitioners often model these separate (although possibly statistically dependent) random variables by fitting a large number of parametric probability distributions to historical data and then comparing goodness-of-fit statistics. However, this approach is highly susceptible to problems of overfitting because it gives insufficient weight to fundamental considerations of functional simplicity and adaptability. To address this shortcoming, we propose a formal mathematical measure for assessing the versatility of frequency and severity distributions prior to their application. We then illustrate this approach by computing and comparing values of the versatility measure for a variety of probability distributions commonly used in risk analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19218v1</guid>
      <category>stat.AP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael R. Powers, Jiaxin Xu</dc:creator>
    </item>
    <item>
      <title>Analysis of the Impact of Unforced Errors in Tennis</title>
      <link>https://arxiv.org/abs/2407.19321</link>
      <description>arXiv:2407.19321v1 Announce Type: new 
Abstract: This paper considers the impact of unforced errors in sport. Although the proposed methods are applicable to various sports, we demonstrate the approach in the context of professional tennis. The value of the approach is that we can provide estimates of the points lost, games lost, sets lost and matches lost due to unforced errors. The methods are based on a bootstrapping procedure which also yields standard errors for the estimates. The approach is valuable in terms of player evaluation, and can also be used for training purposes where it is possible to assess the quantification of improvement based on fewer unforced errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19321v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hashan Peiris, Nirodha Epasinghege Dona, Tim Swartz</dc:creator>
    </item>
    <item>
      <title>Shotgun DNA sequencing for human identification: Dynamic SNP selection and likelihood ratio calculations accounting for errors</title>
      <link>https://arxiv.org/abs/2407.19761</link>
      <description>arXiv:2407.19761v1 Announce Type: new 
Abstract: In forensic genetics, short tandem repeats (STRs) are used for human identification (HID). Degraded biological trace samples with low amounts of short DNA fragments (low-quality DNA samples) pose a challenge for STR typing. Predefined single nucleotide polymorphisms (SNPs) can be amplified on short PCR fragments and used to generate SNP profiles from low-quality DNA samples. However, the stochastic results from low-quality DNA samples may result in frequent locus drop-outs and insufficient numbers of SNP genotypes for convincing identification of individuals. Shotgun DNA sequencing potentially analyses all DNA fragments in a sample in contrast to the targeted PCR-based sequencing methods and may be applied to DNA samples of very low quality, like heavily compromised crime-scene samples and ancient DNA samples. Here, we developed a statistical model for shotgun sequencing, sequence alignment, and genotype calling. Results from replicated shotgun sequencing of buccal swab (high-quality samples) and hair samples (low-quality samples) were arranged in a genotype-call confusion matrix to estimate the calling error probability by maximum likelihood and Bayesian inference. We developed formulas for calculating the evidential weight as a likelihood ratio (LR) based on data from dynamically selected SNPs from shotgun DNA sequencing. The method accounts for potential genotyping errors. Different genotype quality filters may be applied to account for genotyping errors. An error probability of zero resulted in the forensically commonly used LR formula. When considering a single SNP marker's contribution to the LR, error probabilities larger than zero reduced the LR contribution of matching genotypes and increased the LR in the case of a mismatch. We developed an open-source R package, wgsLR, which implements the method, including estimating the calling error probability and calculating LR values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19761v1</guid>
      <category>stat.AP</category>
      <category>q-bio.GN</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mikkel Meyer Andersen, Marie-Louise Kampmann, Alberte Honor\'e Jepsen, Niels Morling, Poul Svante Eriksen, Claus B{\o}rsting, Jeppe Dyrberg Andersen</dc:creator>
    </item>
    <item>
      <title>An Adaptive Image-denoising Method Based on Jump Regression and Local Clustering</title>
      <link>https://arxiv.org/abs/2407.20210</link>
      <description>arXiv:2407.20210v1 Announce Type: new 
Abstract: Image denoising is crucial for reliable image analysis. Researchers from diverse fields have long worked on this, but we still need better solutions. This article focuses on efficiently preserving key image features like edges and structures during denoising. Jump regression analysis is commonly used to estimate true image intensity amid noise. One approach is adaptive smoothing, which uses various local neighborhood shapes and sizes based on empirical data, while another is local pixel clustering to reduce noise while maintaining important details. This manuscript combines both methods to propose an integrated denoising technique.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20210v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhasish Basak, Partha Sarathi Mukherjee</dc:creator>
    </item>
    <item>
      <title>Enhancing Psychometric Analysis with Interactive ShinyItemAnalysis Modules</title>
      <link>https://arxiv.org/abs/2407.18943</link>
      <description>arXiv:2407.18943v1 Announce Type: cross 
Abstract: ShinyItemAnalysis (SIA) is an R package and shiny application for an interactive presentation of psychometric methods and analysis of multi-item measurements in psychology, education, and social sciences in general. In this article, we present a new feature introduced in the recent version of the package, called "SIA modules", which allows researchers and practitioners to offer new analytical methods for broader use via add-on extensions. We describe how to build the add-on modules with the support of the new SIAtools package and demonstrate the concepts using sample modules from the newly introduced SIAmodules package. SIA modules are designed to integrate with and build upon the SIA interactive application, enabling them to leverage the existing infrastructure for tasks such as data uploading and processing. They can access a range of outputs from various analyses, including item response theory models, exploratory factor analysis, or differential item functioning models. Because SIA modules come in R packages (or extend the existing ones), they may come bundled with their datasets, use object-oriented systems, or even compiled code. We discuss the possibility of broader use of the concept of SIA modules in other areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18943v1</guid>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patr\'icia Martinkov\'a, Jan Net\'ik, Ad\'ela Hladk\'a</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Confidence Regions in Sparse MRI</title>
      <link>https://arxiv.org/abs/2407.18964</link>
      <description>arXiv:2407.18964v1 Announce Type: cross 
Abstract: One of the most promising solutions for uncertainty quantification in high-dimensional statistics is the debiased LASSO that relies on unconstrained $\ell_1$-minimization. The initial works focused on real Gaussian designs as a toy model for this problem. However, in medical imaging applications, such as compressive sensing for MRI, the measurement system is represented by a (subsampled) complex Fourier matrix. The purpose of this work is to extend the method to the MRI case in order to construct confidence intervals for each pixel of an MR image. We show that a sufficient amount of data is $n \gtrsim \max\{ s_0\log^2 s_0\log p, s_0 \log^2 p \}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18964v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICASSP49357.2023.10096320</arxiv:DOI>
      <dc:creator>Frederik Hoppe, Felix Krahmer, Claudio Mayrink Verdun, Marion Menzel, Holger Rauhut</dc:creator>
    </item>
    <item>
      <title>Flusion: Integrating multiple data sources for accurate influenza predictions</title>
      <link>https://arxiv.org/abs/2407.19054</link>
      <description>arXiv:2407.19054v1 Announce Type: cross 
Abstract: Over the last ten years, the US Centers for Disease Control and Prevention (CDC) has organized an annual influenza forecasting challenge with the motivation that accurate probabilistic forecasts could improve situational awareness and yield more effective public health actions. Starting with the 2021/22 influenza season, the forecasting targets for this challenge have been based on hospital admissions reported in the CDC's National Healthcare Safety Network (NHSN) surveillance system. Reporting of influenza hospital admissions through NHSN began within the last few years, and as such only a limited amount of historical data are available for this signal. To produce forecasts in the presence of limited data for the target surveillance system, we augmented these data with two signals that have a longer historical record: 1) ILI+, which estimates the proportion of outpatient doctor visits where the patient has influenza; and 2) rates of laboratory-confirmed influenza hospitalizations at a selected set of healthcare facilities. Our model, Flusion, is an ensemble that combines gradient boosting quantile regression models with a Bayesian autoregressive model. The gradient boosting models were trained on all three data signals, while the autoregressive model was trained on only the target signal; all models were trained jointly on data for multiple locations. Flusion was the top-performing model in the CDC's influenza prediction challenge for the 2023/24 season. In this article we investigate the factors contributing to Flusion's success, and we find that its strong performance was primarily driven by the use of a gradient boosting model that was trained jointly on data from multiple surveillance signals and locations. These results indicate the value of sharing information across locations and surveillance signals, especially when doing so adds to the pool of available training data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19054v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evan L. Ray, Yijin Wang, Russell D. Wolfinger, Nicholas G. Reich</dc:creator>
    </item>
    <item>
      <title>Bayesian Mapping of Mortality Clusters</title>
      <link>https://arxiv.org/abs/2407.19135</link>
      <description>arXiv:2407.19135v1 Announce Type: cross 
Abstract: Disease mapping analyses the distribution of several diseases within a territory. Primary goals include identifying areas with unexpected changes in mortality rates, studying the relation among multiple diseases, and dividing the analysed territory into clusters based on the observed levels of disease incidence or mortality. In this work, we focus on detecting spatial mortality clusters, that occur when neighbouring areas within a territory exhibit similar mortality levels due to one or more diseases. When multiple death causes are examined together, it is relevant to identify both the spatial boundaries of the clusters and the diseases that lead to their formation. However, existing methods in literature struggle to address this dual problem effectively and simultaneously. To overcome these limitations, we introduce Perla, a multivariate Bayesian model that clusters areas in a territory according to the observed mortality rates of multiple death causes, also exploiting the information of external covariates. Our model incorporates the spatial data structure directly into the clustering probabilities by leveraging the stick-breaking formulation of the multinomial distribution. Additionally, it exploits suitable global-local shrinkage priors to ensure that the detection of clusters is driven by concrete differences across mortality levels while excluding spurious differences. We propose an MCMC algorithm for posterior inference that consists of closed-form Gibbs sampling moves for nearly every model parameter, without requiring complex tuning operations. This work is primarily motivated by a case study on the territory of the local unit ULSS6 Euganea within the Italian public healthcare system. To demonstrate the flexibility and effectiveness of our methodology, we also validate Perla with a series of simulation experiments and an extensive case study on mortality levels in U.S. counties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19135v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrea Sottosanti, Pietro Belloni, Enrico Bovo, Giovanna Boccuzzo</dc:creator>
    </item>
    <item>
      <title>Independent fact-checking organizations exhibit a departure from political neutrality</title>
      <link>https://arxiv.org/abs/2407.19498</link>
      <description>arXiv:2407.19498v1 Announce Type: cross 
Abstract: Independent fact-checking organizations have emerged as the crusaders to debunk fake news. However, they may not always remain neutral, as they can be selective in the false news they choose to expose and in how they present the information. They can deviate from neutrality by being selective in what false news they debunk and how the information is presented. Prompting the now popular large language model, GPT-3.5, with journalistic frameworks, we establish a longitudinal measure (2018-2023) for political neutrality that looks beyond the left-right spectrum. Specified on a range of -1 to 1 (with zero being absolute neutrality), we establish the extent of negative portrayal of political entities that makes a difference in the readers' perception in the USA and India. Here, we observe an average score of -0.17 and -0.24 in the USA and India, respectively. The findings indicate how seemingly objective fact-checking can still carry distorted political views, indirectly and subtly impacting the perception of consumers of the news.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19498v1</guid>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sahajpreet Singh, Sarah Masud, Tanmoy Chakraborty</dc:creator>
    </item>
    <item>
      <title>Identification and Inference with Invalid Instruments</title>
      <link>https://arxiv.org/abs/2407.19558</link>
      <description>arXiv:2407.19558v1 Announce Type: cross 
Abstract: Instrumental variables (IVs) are widely used to study the causal effect of an exposure on an outcome in the presence of unmeasured confounding. IVs require an instrument, a variable that is (A1) associated with the exposure, (A2) has no direct effect on the outcome except through the exposure, and (A3) is not related to unmeasured confounders. Unfortunately, finding variables that satisfy conditions (A2) or (A3) can be challenging in practice. This paper reviews works where instruments may not satisfy conditions (A2) or (A3), which we refer to as invalid instruments. We review identification and inference under different violations of (A2) or (A3), specifically under linear models, non-linear models, and heteroskedatic models. We conclude with an empirical comparison of various methods by re-analyzing the effect of body mass index on systolic blood pressure from the UK Biobank.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19558v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyunseung Kang, Zijian Guo, Zhonghua Liu, Dylan Small</dc:creator>
    </item>
    <item>
      <title>Experimenting on Markov Decision Processes with Local Treatments</title>
      <link>https://arxiv.org/abs/2407.19618</link>
      <description>arXiv:2407.19618v1 Announce Type: cross 
Abstract: As service systems grow increasingly complex and dynamic, many interventions become localized, available and taking effect only in specific states. This paper investigates experiments with local treatments on a widely-used class of dynamic models, Markov Decision Processes (MDPs). Particularly, we focus on utilizing the local structure to improve the inference efficiency of the average treatment effect. We begin by demonstrating the efficiency of classical inference methods, including model-based estimation and temporal difference learning under a fixed policy, as well as classical A/B testing with general treatments. We then introduce a variance reduction technique that exploits the local treatment structure by sharing information for states unaffected by the treatment policy. Our new estimator effectively overcomes the variance lower bound for general treatments while matching the more stringent lower bound incorporating the local treatment structure. Furthermore, our estimator can optimally achieve a linear reduction with the number of test arms for a major part of the variance. Finally, we explore scenarios with perfect knowledge of the control arm and design estimators that further improve inference efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19618v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuze Chen, David Simchi-Levi, Chonghuan Wang</dc:creator>
    </item>
    <item>
      <title>AI-Powered Energy algorithmic Trading: Integrating Hidden Markov Models with Neural Networks</title>
      <link>https://arxiv.org/abs/2407.19858</link>
      <description>arXiv:2407.19858v1 Announce Type: cross 
Abstract: In the field of quantitative finance, machine learning methods have become essential for alpha generation. This paper presents a pioneering method that uniquely combines Hidden Markov Models (HMM) and neural networks, creating a dual-model alpha generation system integrated with Black-Litterman portfolio optimization. The methodology, implemented on the QuantConnect platform, aims to predict future price movements and optimize trading strategies. Specifically, it filters for highly liquid, top-cap energy stocks to ensure stable and predictable performance while also accounting for broker payments. QuantConnect was selected because of its robust framework and to guarantee experimental reproducibility. The algorithm achieved a 31% return between June 1, 2023, and January 1, 2024, with a Sharpe ratio of 1.669, demonstrating its potential. The findings suggest significant improvements in trading strategy performance through the combined use of the HMM and neural networks. This study explores the architecture of the algorithm, data pre-processing techniques, model training procedures, and performance evaluation, highlighting its practical applicability and effectiveness in real-world trading environments. The full code and backtesting data are available under the MIT license.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19858v1</guid>
      <category>q-fin.PM</category>
      <category>cs.LG</category>
      <category>q-fin.GN</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tiago Monteiro</dc:creator>
    </item>
    <item>
      <title>Integrated Scenario-based Analysis: A data-driven approach to support automated driving systems development and safety evaluation</title>
      <link>https://arxiv.org/abs/2407.19975</link>
      <description>arXiv:2407.19975v1 Announce Type: cross 
Abstract: Several scenario-based frameworks exist to aid in vehicle system development and safety assurance. However, there is a need for approaches that combine different types of datasets that offer varying levels of case severity, data richness, and representativeness. This study presents an integrated scenario-based analysis approach that encompasses scenario definition, fusion, parametrization, and test case generation. For this process, ten years of fatal and non-fatal national crash data from the United States are combined with over 34 million miles of naturalistic driving data. An illustrative example scenario, "turns at intersection", is chosen to demonstrate this approach. First, scenario definitions are established from both record-based and continuous time series data. Second, a frequency analysis is performed to understand how often events from the same scenario occur at different severities across datasets. Third, an analysis is performed to show the key factors relevant to the scenario and the distribution of various parameters. Finally, a method to combine both types of data into representative test case scenarios is presented. These techniques improve scenario representativeness in two major ways: first, they populate an entire spectrum of cases ranging from routine events to fatal crashes; and second, they provide context-rich, multi-year data by combining large-scale national and naturalistic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19975v1</guid>
      <category>cs.RO</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gibran Ali, Kaye Sullivan, Eileen Herbers, Vicki Williams, Dustin Holley, Jacobo Antona-Makoshi, Kevin Kefauver</dc:creator>
    </item>
    <item>
      <title>Statistical Analysis of Data Repeatability Measures</title>
      <link>https://arxiv.org/abs/2005.11911</link>
      <description>arXiv:2005.11911v4 Announce Type: replace 
Abstract: The advent of modern data collection and processing techniques has seen the size, scale, and complexity of data grow exponentially. A seminal step in leveraging these rich datasets for downstream inference is understanding the characteristics of the data which are repeatable -- the aspects of the data that are able to be identified under a duplicated analysis. Conflictingly, the utility of traditional repeatability measures, such as the intraclass correlation coefficient, under these settings is limited. In recent work, novel data repeatability measures have been introduced in the context where a set of subjects are measured twice or more, including: fingerprinting, rank sums, and generalizations of the intraclass correlation coefficient. However, the relationships between, and the best practices among these measures remains largely unknown. In this manuscript, we formalize a novel repeatability measure, discriminability. We show that it is deterministically linked with the correlation coefficient under univariate random effect models, and has desired property of optimal accuracy for inferential tasks using multivariate measurements. Additionally, we overview and systematically compare repeatability statistics using both theoretical results and simulations. We show that the rank sum statistic is deterministically linked to a consistent estimator of discriminability. The power of permutation tests derived from these measures are compared numerically under Gaussian and non-Gaussian settings, with and without simulated batch effects. Motivated by both theoretical and empirical results, we provide methodological recommendations for each benchmark setting to serve as a resource for future analyses. We believe these recommendations will play an important role towards improving repeatability in fields such as functional magnetic resonance imaging, genomics, pharmacology, and more.</description>
      <guid isPermaLink="false">oai:arXiv.org:2005.11911v4</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeyi Wang, Eric Bridgeford, Shangsi Wang, Joshua T. Vogelstein, Brian Caffo</dc:creator>
    </item>
    <item>
      <title>Quantile Regression Tree</title>
      <link>https://arxiv.org/abs/2402.14955</link>
      <description>arXiv:2402.14955v2 Announce Type: replace 
Abstract: This study introduces and evaluates the Quantile Regressor Tree (QRT), a novel methodology merging the robust characteristics of quantile regression with the versatility of decision trees. The quantile regressor tree introduces non-linearity to the quantile regression due to the splitting by features in the decision tree, enhancing flexibility while maintaining interpretability. The quantile regression tree gives a parametric and non-parametric mixture of estimating conditional quantiles for high-dimensional predictor variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14955v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jaachinma Okafor, Lateefah Isegen, Ark Ifeanyi</dc:creator>
    </item>
    <item>
      <title>On Consistency of Signature Using Lasso</title>
      <link>https://arxiv.org/abs/2305.10413</link>
      <description>arXiv:2305.10413v3 Announce Type: replace-cross 
Abstract: Signatures are iterated path integrals of continuous and discrete-time processes, and their universal nonlinearity linearizes the problem of feature selection in time series data analysis. This paper studies the consistency of signature using Lasso regression, both theoretically and numerically. We establish conditions under which the Lasso regression is consistent both asymptotically and in finite sample. Furthermore, we show that the Lasso regression is more consistent with the It\^o signature for time series and processes that are closer to the Brownian motion and with weaker inter-dimensional correlations, while it is more consistent with the Stratonovich signature for mean-reverting time series and processes. We demonstrate that signature can be applied to learn nonlinear functions and option prices with high accuracy, and the performance depends on properties of the underlying process and the choice of the signature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.10413v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xin Guo, Binnan Wang, Ruixun Zhang, Chaoyi Zhao</dc:creator>
    </item>
    <item>
      <title>Private and Collaborative Kaplan-Meier Estimators</title>
      <link>https://arxiv.org/abs/2305.15359</link>
      <description>arXiv:2305.15359v2 Announce Type: replace-cross 
Abstract: Kaplan-Meier estimators are essential tools in survival analysis, capturing the survival behavior of a cohort. Their accuracy improves with large, diverse datasets, encouraging data holders to collaborate for more precise estimations. However, these datasets often contain sensitive individual information, necessitating stringent data protection measures that preclude naive data sharing.
  In this work, we introduce two novel differentially private methods that offer flexibility in applying differential privacy to various functions of the data. Additionally, we propose a synthetic dataset generation technique that enables easy and rapid conversion between different data representations. Utilizing these methods, we propose various paths that allow a joint estimation of the Kaplan-Meier curves with strict privacy guarantees. Our contribution includes a taxonomy of methods for this task and an extensive experimental exploration and evaluation based on this structure. We demonstrate that our approach can construct a joint, global Kaplan-Meier estimator that adheres to strict privacy standards ($\varepsilon = 1$) while exhibiting no statistically significant deviation from the nonprivate centralized estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.15359v2</guid>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shadi Rahimian, Raouf Kerkouche, Ina Kurth, Mario Fritz</dc:creator>
    </item>
    <item>
      <title>Design-based inference for generalized network experiments with stochastic interventions</title>
      <link>https://arxiv.org/abs/2312.03268</link>
      <description>arXiv:2312.03268v2 Announce Type: replace-cross 
Abstract: A growing number of researchers are conducting randomized experiments to analyze causal relationships in network settings where units influence one another. A dominant methodology for analyzing these experiments is design-based, leveraging random treatment assignments as the basis for inference. In this paper, we generalize this design-based approach to accommodate complex experiments with a variety of causal estimands and different target populations. An important special case of such generalized network experiments is a bipartite network experiment, in which treatment is randomized among one set of units, and outcomes are measured on a separate set of units. We propose a broad class of causal estimands based on stochastic interventions for generalized network experiments. Using a design-based approach, we show how to estimate these causal quantities without bias and develop conservative variance estimators. We apply our methodology to a randomized experiment in education where participation in an anti-conflict promotion program is randomized among selected students. Our analysis estimates the causal effects of treating each student or their friends among different target populations in the network. We find that the program improves the overall conflict awareness among students but does not significantly reduce the total number of such conflicts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.03268v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ambarish Chattopadhyay, Kosuke Imai, Jose R. Zubizarreta</dc:creator>
    </item>
    <item>
      <title>SportsNGEN: Sustained Generation of Realistic Multi-player Sports Gameplay</title>
      <link>https://arxiv.org/abs/2403.12977</link>
      <description>arXiv:2403.12977v2 Announce Type: replace-cross 
Abstract: We present a transformer decoder based sports simulation engine, SportsNGEN, trained on sports player and ball tracking sequences, that is capable of generating sustained gameplay and accurately mimicking the decision making of real players. By training on a large database of professional tennis tracking data, we demonstrate that simulations produced by SportsNGEN can be used to predict the outcomes of rallies, determine the best shot choices at any point, and evaluate counterfactual or what if scenarios to inform coaching decisions and elevate broadcast coverage. By combining the generated simulations with a shot classifier and logic to start and end rallies, the system is capable of simulating an entire tennis match. We evaluate SportsNGEN by comparing statistics of the simulations with those of real matches between the same players. We show that the model output sampling parameters are crucial to simulation realism and that SportsNGEN is probabilistically well-calibrated to real data. In addition, a generic version of SportsNGEN can be customized to a specific player by fine-tuning on the subset of match data that includes that player. Finally, we show qualitative results indicating the same approach works for football.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12977v2</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lachlan Thorpe, Lewis Bawden, Karanjot Vendal, John Bronskill, Richard E. Turner</dc:creator>
    </item>
    <item>
      <title>Extracting Emotion Phrases from Tweets using BART</title>
      <link>https://arxiv.org/abs/2403.14050</link>
      <description>arXiv:2403.14050v3 Announce Type: replace-cross 
Abstract: Sentiment analysis is a natural language processing task that aims to identify and extract the emotional aspects of a text. However, many existing sentiment analysis methods primarily classify the overall polarity of a text, overlooking the specific phrases that convey sentiment. In this paper, we applied an approach to sentiment analysis based on a question-answering framework. Our approach leverages the power of Bidirectional Autoregressive Transformer (BART), a pre-trained sequence-to-sequence model, to extract a phrase from a given text that amplifies a given sentiment polarity. We create a natural language question that identifies the specific emotion to extract and then guide BART to pay attention to the relevant emotional cues in the text. We use a classifier within BART to predict the start and end positions of the answer span within the text, which helps to identify the precise boundaries of the extracted emotion phrase. Our approach offers several advantages over most sentiment analysis studies, including capturing the complete context and meaning of the text and extracting precise token spans that highlight the intended sentiment. We achieved an end loss of 87% and Jaccard score of 0.61.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14050v3</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Mahdi Rezapour</dc:creator>
    </item>
    <item>
      <title>Emotion Detection with Transformers: A Comparative Study</title>
      <link>https://arxiv.org/abs/2403.15454</link>
      <description>arXiv:2403.15454v4 Announce Type: replace-cross 
Abstract: In this study, we explore the application of transformer-based models for emotion classification on text data. We train and evaluate several pre-trained transformer models, on the Emotion dataset using different variants of transformers. The paper also analyzes some factors that in-fluence the performance of the model, such as the fine-tuning of the transformer layer, the trainability of the layer, and the preprocessing of the text data. Our analysis reveals that commonly applied techniques like removing punctuation and stop words can hinder model performance. This might be because transformers strength lies in understanding contextual relationships within text. Elements like punctuation and stop words can still convey sentiment or emphasis and removing them might disrupt this context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15454v4</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Mahdi Rezapour</dc:creator>
    </item>
    <item>
      <title>Estimating Metocean Environments Associated with Extreme Structural Response to Demonstrate the Dangers of Environmental Contour Methods</title>
      <link>https://arxiv.org/abs/2404.16775</link>
      <description>arXiv:2404.16775v4 Announce Type: replace-cross 
Abstract: Extreme value analysis (EVA) uses data to estimate long-term extreme environmental conditions for variables such as significant wave height and period, for the design of marine structures. Together with models for the short-term evolution of the ocean environment and for wave-structure interaction, EVA provides a basis for full probabilistic design analysis. Alternatively, environmental contours provide an approximate approach to estimating structural integrity, without requiring structural knowledge. These contour methods also exploit statistical models, including EVA, but avoid the need for structural modelling by making what are believed to be conservative assumptions about the shape of the structural failure boundary in the environment space. These assumptions, however, may not always be appropriate, or may lead to unnecessary wasted resources from over design. We demonstrate a methodology for efficient fully probabilistic analysis of structural failure. From this, we estimate the joint conditional probability density of the environment (CDE), given the occurrence of an extreme structural response. We use CDE as a diagnostic to highlight the deficiencies of environmental contour methods for design; none of the IFORM environmental contours considered characterise CDE well for three example structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16775v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Speers, David Randell, Jonathan Angus Tawn, Philip Jonathan</dc:creator>
    </item>
    <item>
      <title>Multicalibration for Modeling Censored Survival Data with Universal Adaptability</title>
      <link>https://arxiv.org/abs/2405.15948</link>
      <description>arXiv:2405.15948v2 Announce Type: replace-cross 
Abstract: Traditional statistical and machine learning methods assume identical distribution for the training and test data sets. This assumption, however, is often violated in real applications, particularly in health care research, where the training data~(source) may underrepresent specific subpopulations in the testing or target domain. Such disparities, coupled with censored observations, present significant challenges for investigators aiming to make predictions for those minority groups. This paper focuses on target-independent learning under covariate shift, where we study multicalibration for survival probability and restricted mean survival time, and propose a black-box post-processing boosting algorithm designed for censored survival data. Our algorithm, leveraging the pseudo observations, yields a multicalibrated predictor competitive with propensity scoring regarding predictions on the unlabeled target domain, not just overall but across diverse subpopulations. Our theoretical analysis for pseudo observations relies on functional delta method and $p$-variational norm. We further investigate the algorithm's sample complexity and convergence properties, as well as the multicalibration guarantee for post-processed predictors. Our theoretical insights reveal the link between multicalibration and universal adaptability, suggesting that our calibrated function performs comparably to, if not better than, the inverse propensity score weighting estimator. The performance of our proposed methods is corroborated through extensive numerical simulations and a real-world case study focusing on prediction of cardiovascular disease risk in two large prospective cohort studies. These empirical results confirm its potential as a powerful tool for predictive analysis with censored outcomes in diverse and shifting populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15948v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hanxuan Ye, Hongzhe Li</dc:creator>
    </item>
    <item>
      <title>CASTRO -- Efficient constrained sampling method for material and chemical experimental design</title>
      <link>https://arxiv.org/abs/2407.16567</link>
      <description>arXiv:2407.16567v2 Announce Type: replace-cross 
Abstract: The exploration of multicomponent material composition space requires significant time and financial investments, necessitating efficient use of resources for statistically relevant compositions. This article introduces a novel methodology, implemented in the open-source CASTRO (ConstrAined Sequential laTin hypeRcube sampling methOd) software package, to overcome equality-mixture constraints and ensure comprehensive design space coverage. Our approach leverages Latin hypercube sampling (LHS) and LHS with multidimensional uniformity (LHSMDU) using a divide-and-conquer strategy to manage high-dimensional problems effectively. By incorporating previous experimental knowledge within a limited budget, our method strategically recommends a feasible number of experiments to explore the design space. We validate our methodology with two examples: a four-dimensional problem with near-uniform distributions and a nine-dimensional problem with additional mixture constraints, yielding specific component distributions. Our constrained sequential LHS or LHSMDU approach enables thorough design space exploration, proving robustness for experimental design. This research not only advances material science but also offers promising solutions for efficiency challenges in pharmaceuticals and chemicals. CASTRO and the case studies are available for free download on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16567v2</guid>
      <category>stat.CO</category>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christina Schenk, Maciej Haranczyk</dc:creator>
    </item>
  </channel>
</rss>
