<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Aug 2025 01:23:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Tight Bounds for Sparsifying Random CSPs</title>
      <link>https://arxiv.org/abs/2508.13345</link>
      <description>arXiv:2508.13345v1 Announce Type: new 
Abstract: The problem of CSP sparsification asks: for a given CSP instance, what is the sparsest possible reweighting such that for every possible assignment to the instance, the number of satisfied constraints is preserved up to a factor of $1 \pm \epsilon$? We initiate the study of the sparsification of random CSPs. In particular, we consider two natural random models: the $r$-partite model and the uniform model. In the $r$-partite model, CSPs are formed by partitioning the variables into $r$ parts, with constraints selected by randomly picking one vertex out of each part. In the uniform model, $r$ distinct vertices are chosen at random from the pool of variables to form each constraint.
  In the $r$-partite model, we exhibit a sharp threshold phenomenon. For every predicate $P$, there is an integer $k$ such that a random instance on $n$ vertices and $m$ edges cannot (essentially) be sparsified if $m \le n^k$ and can be sparsified to size $\approx n^k$ if $m \ge n^k$. Here, $k$ corresponds to the largest copy of the AND which can be found within $P$. Furthermore, these sparsifiers are simple, as they can be constructed by i.i.d. sampling of the edges.
  In the uniform model, the situation is a bit more complex. For every predicate $P$, there is an integer $k$ such that a random instance on $n$ vertices and $m$ edges cannot (essentially) be sparsified if $m \le n^k$ and can sparsified to size $\approx n^k$ if $m \ge n^{k+1}$. However, for some predicates $P$, if $m \in [n^k, n^{k+1}]$, there may or may not be a nontrivial sparsifier. In fact, we show that there are predicates where the sparsifiability of random instances is non-monotone, i.e., as we add more random constraints, the instances become more sparsifiable. We give a precise (efficiently computable) procedure for determining which situation a specific predicate $P$ falls into.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13345v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Joshua Brakensiek, Venkatesan Guruswami, Aaron Putterman</dc:creator>
    </item>
    <item>
      <title>On the 2D Demand Bin Packing Problem: Hardness and Approximation Algorithms</title>
      <link>https://arxiv.org/abs/2508.13347</link>
      <description>arXiv:2508.13347v1 Announce Type: new 
Abstract: We study a two-dimensional generalization of the classical Bin Packing problem, denoted as 2D Demand Bin Packing. In this context, each bin is a horizontal timeline, and rectangular tasks (representing electric appliances or computational requirements) must be allocated into the minimum number of bins so that the sum of the heights of tasks at any point in time is at most a given constant capacity. We prove that simple variants of the problem are NP-hard to approximate within a factor better than $2$, namely when tasks have short height and when they are squares, and provide best-possible approximation algorithms for them; we also present a simple $3$-approximation for the general case. All our algorithms are based on a general framework that computes structured solutions for relatively large tasks, while including relatively small tasks on top via a generalization of the well-known First-Fit algorithm for Bin Packing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13347v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Susanne Albers, Waldo G\'alvez, \"Omer Behic \"Ozdemir</dc:creator>
    </item>
    <item>
      <title>Concurrent Double-Ended Priority Queues</title>
      <link>https://arxiv.org/abs/2508.13399</link>
      <description>arXiv:2508.13399v1 Announce Type: new 
Abstract: This work provides the first concurrent implementation specifically designed for a double-ended priority queue (DEPQ). We do this by describing a general way to add an ExtractMax operation to any concurrent priority queue that already supports Insert and ExtractMin operations. The construction uses two linearizable single-consumer priority queues to build a linearizable dual-consumer DEPQ (only one process can perform Extract operations at each end). This construction preserves lock-freedom. We then describe how to use a lock-based combining scheme to allow multiple consumers at each end of the DEPQ. To illustrate the technique, we apply it to a list-based priority queue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13399v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Panagiota Fatourou (FORTH ICS, Greece, University of Crete, Greece), Eric Ruppert (York University, Canada), Ioannis Xiradakis (FORTH ICS, Greece, University of Crete, Greece)</dc:creator>
    </item>
    <item>
      <title>Generating the Spanning Trees of Series-Parallel Graphs up to Graph Automorphism</title>
      <link>https://arxiv.org/abs/2508.13480</link>
      <description>arXiv:2508.13480v1 Announce Type: new 
Abstract: In this paper, we investigate the problem of generating the spanning trees of a graph $G$ up to the automorphisms or "symmetries" of $G$. After introducing and surveying this problem for general input graphs, we present algorithms that fully solve the case of series-parallel graphs, under two standard definitions. We first show how to generate the nonequivalent spanning trees of a oriented series-parallel graph $G$ in output-linear time, where both terminals of $G$ have been individually distinguished (i.e. applying an automorphism that exchanges the terminals produces a different series-parallel graph). Subsequently, we show how to adapt these oriented algorithms to the case of semioriented series-parallel graphs, where we still have a set of two distinguished terminals but neither has been designated as a source or sink. Finally, we discuss the case of unoriented series-parallel graphs, where no terminals have been distinguished and present a few observations and open questions relating to them. The algorithms we present generate the nonequivalent spanning trees of $G$ but never explicitly compute the automorphism group of $G$, revealing how the recursive structure of $G$'s automorphism group mirrors that of its spanning trees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13480v1</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mithra Karamchedu, Lucas Bang</dc:creator>
    </item>
    <item>
      <title>Finding subdigraphs in digraphs of bounded directed treewidth</title>
      <link>https://arxiv.org/abs/2508.13830</link>
      <description>arXiv:2508.13830v1 Announce Type: new 
Abstract: It is well known that directed treewidth does not enjoy the nice algorithmic properties of its undirected counterpart. There exist, however, some positive results that, essentially, present XP algorithms for the problem of finding, in a given digraph $D$, a subdigraph isomorphic to a digraph $H$ that can be formed by the union of $k$ directed paths (with some extra properties), parameterized by $k$ and the directed treewidth of $D$. Our motivation is to tackle the following question: Are there subdigraphs, other than the directed paths, that can be found efficiently in digraphs of bounded directed treewidth? In a nutshell, the main message of this article is that, other than the directed paths, the only digraphs that seem to behave well with respect to directed treewidth are the stars. For this, we present a number of positive and negative results, generalizing several results in the literature, as well as some directions for further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13830v1</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raul Lopes, Ignasi Sau</dc:creator>
    </item>
    <item>
      <title>Multi-Metric Algorithmic Complexity: Beyond Asymptotic Analysis</title>
      <link>https://arxiv.org/abs/2508.13249</link>
      <description>arXiv:2508.13249v1 Announce Type: cross 
Abstract: Traditional algorithm analysis treats all basic operations as equally costly, which hides significant differences in time, energy consumption, and cost between different types of computations on modern processors. We propose a weighted-operation complexity model that assigns realistic cost values to different instruction types across multiple dimensions: computational effort, energy usage, carbon footprint, and monetary cost. The model computes overall efficiency scores based on user-defined priorities and can be applied through automated code analysis or integrated with performance measurement tools. This approach complements existing theoretical models by enabling practical, architecture-aware algorithm comparisons that account for performance, sustainability, and economic factors. We demonstrate an open-source implementation that analyzes code, estimates multi-dimensional costs, and provides efficiency recommendations across various algorithms. We address two research questions: (RQ1) Can a multi-metric model predict time/energy with high accuracy across architectures? (RQ2) How does it compare to baselines like Big-O, ICE, and EVM gas? Validation shows strong correlations (\r{ho}&gt;0.9) with measured data, outperforming baselines in multi-objective scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13249v1</guid>
      <category>cs.PF</category>
      <category>cs.AR</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sergii Kavun</dc:creator>
    </item>
    <item>
      <title>Online Stochastic Packing with General Correlations</title>
      <link>https://arxiv.org/abs/2508.13458</link>
      <description>arXiv:2508.13458v1 Announce Type: cross 
Abstract: There has been a growing interest in studying online stochastic packing under more general correlation structures, motivated by the complex data sets and models driving modern applications. Several past works either assume correlations are weak or have a particular structure, have a complexity scaling with the number of Markovian "states of the world" (which may be exponentially large e.g. in the case of full history dependence), scale poorly with the horizon $T$, or make additional continuity assumptions. Surprisingly, we show that for all $\epsilon$, the online stochastic packing linear programming problem with general correlations (suitably normalized and with sparse columns) has an approximately optimal policy (with optimality gap $\epsilon T$) whose per-decision runtime scales as the time to simulate a single sample path of the underlying stochastic process (assuming access to a Monte Carlo simulator), multiplied by a constant independent of the horizon or number of Markovian states. We derive analogous results for network revenue management, and online bipartite matching and independent set in bounded-degree graphs, by rounding. Our algorithms implement stochastic gradient methods in a novel on-the-fly/recursive manner for the associated massive deterministic-equivalent linear program on the corresponding probability space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13458v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sabri Cetin, Yilun Chen, David A. Goldberg</dc:creator>
    </item>
    <item>
      <title>Finite matrix multiplication algorithms from infinite groups</title>
      <link>https://arxiv.org/abs/2410.14905</link>
      <description>arXiv:2410.14905v2 Announce Type: replace-cross 
Abstract: The Cohn-Umans (FOCS '03) group-theoretic framework for matrix multiplication produces fast matrix multiplication algorithms from three subsets of a finite group $G$ satisfying a simple combinatorial condition (the Triple Product Property). The complexity of such an algorithm then depends on the representation theory of $G$. In this paper we extend the group-theoretic framework to the setting of infinite groups. In particular, this allows us to obtain constructions in Lie groups, with favorable parameters, that are provably impossible in finite groups of Lie type (Blasiak, Cohn, Grochow, Pratt, and Umans, ITCS '23). Previously the Lie group setting was investigated purely as an analogue of the finite group case; a key contribution in this paper is a fully developed framework for obtaining bona fide matrix multiplication algorithms directly from Lie group constructions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14905v2</guid>
      <category>math.GR</category>
      <category>cs.DS</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonah Blasiak, Henry Cohn, Joshua A. Grochow, Kevin Pratt, Chris Umans</dc:creator>
    </item>
    <item>
      <title>Simulating quantum collision models with Hamiltonian simulations using early fault-tolerant quantum computers</title>
      <link>https://arxiv.org/abs/2504.21564</link>
      <description>arXiv:2504.21564v2 Announce Type: replace-cross 
Abstract: We develop randomized quantum algorithms to simulate quantum collision models, also known as repeated interaction schemes, which provide a rich framework to model various open-system dynamics. The underlying technique involves composing time evolutions of the total (system, bath, and interaction) Hamiltonian and intermittent tracing out of the environment degrees of freedom. This results in a unified framework where any near-term Hamiltonian simulation algorithm can be incorporated to implement an arbitrary number of such collisions on early fault-tolerant quantum computers: we do not assume access to specialized oracles such as block encodings and minimize the number of ancilla qubits needed. In particular, using the correspondence between Lindbladian evolution and completely positive trace-preserving maps arising out of memoryless collisions, we provide an end-to-end quantum algorithm for simulating Lindbladian dynamics. For a system of $n$-qubits, we exhaustively compare the circuit depth needed to estimate the expectation value of an observable with respect to the reduced state of the system after time $t$ while employing different near-term Hamiltonian simulation techniques, requiring at most $n+2$ qubits in all. We compare the CNOT gate counts of the various approaches for estimating the Transverse Field Magnetization of a $10$-qubit XX-Heisenberg spin chain under amplitude damping. Finally, we also develop a framework to efficiently simulate an arbitrary number of memory-retaining collisions, i.e., where environments interact, leading to non-Markovian dynamics. Overall, our methods can leverage quantum collision models for both Markovian and non-Markovian dynamics on early fault-tolerant quantum computers, shedding light on the advantages and limitations of simulating open systems dynamics using this framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21564v2</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/3trk-smbh</arxiv:DOI>
      <arxiv:journal_reference>Physical Review A 112, 022425 (2025)</arxiv:journal_reference>
      <dc:creator>Kushagra Garg, Zeeshan Ahmed, Subhadip Mitra, Shantanav Chakraborty</dc:creator>
    </item>
    <item>
      <title>Certificate-Sensitive Subset Sum: Realizing Instance Complexity</title>
      <link>https://arxiv.org/abs/2507.15511</link>
      <description>arXiv:2507.15511v5 Announce Type: replace-cross 
Abstract: We present, to our knowledge, the first deterministic, certificate-sensitive algorithm for a canonical NP-complete problem whose runtime provably adapts to the structure of each input. For a Subset-Sum instance $(S, t)$, let $\Sigma(S)$ denote the set of distinct subset sums and define $U = |\Sigma(S)|$. This set serves as an information-theoretically minimal witness, the instance-complexity (IC) certificate.
  Our solver, IC-SubsetSum, enumerates every element of $\Sigma(S)$ in deterministic time $O(U \cdot n^2)$ and space $O(U \cdot n)$. A randomized variant achieves expected runtime $O(U \cdot n)$. The algorithm's complexity is thus directly governed by the certificate size, and this structure-sensitive performance is paired with a guaranteed worst-case runtime of $O^*(2^{n/2 - \varepsilon})$ for some constant $\varepsilon &gt; 0$, the first such result to strictly outperform classical methods on every instance.
  We revisit fine-grained reductions that rely on the classical $2^{n/2}$ hardness of SubsetSum and show that these arguments hold only for collision-free instances where $U$ is maximal. IC-SubsetSum reframes this barrier structurally and introduces a new paradigm for certificate-sensitive algorithms across NP-complete problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15511v5</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesus Salas</dc:creator>
    </item>
    <item>
      <title>NP-Hardness and ETH-Based Inapproximability of Communication Complexity via Relaxed Interlacing</title>
      <link>https://arxiv.org/abs/2508.05597</link>
      <description>arXiv:2508.05597v3 Announce Type: replace-cross 
Abstract: We prove that computing the deterministic communication complexity D(f) of a Boolean function is NP-hard in the standard protocol-tree model, answering, independently and concurrently with Hirahara-Llango-Loff (arXiv:2507.10426), a question first posed by Yao (1979). Our reduction builds and expands on a suite of structural "interlacing" lemmas introduced by Mackenzie and Saffidine (arXiv:2411.19003); these lemmas can be reused as black boxes in future lower-bound constructions.
  The instances produced by our reduction admit optimal protocols for self-similar constructions with strong structural properties, giving a flexible framework for the design of reductions showing NP-hardness of deciding the communication complexity of a Boolean matrix. This complements the work by Hirahara, Ilango, and Loff, which establishes NP-hardness in the same model via a different route; our analysis additionally yields reusable structural guarantees and underpins further consequences concerning inapproximability.
  Because the gadgets in our construction are self-similar, they can be recursively embedded. We sketch how this yields, under the Exponential-Time Hypothesis, an additive inapproximability gap that grows without bound. Furthermore we outline a route toward NP-hardness of approximating D(f) within a fixed constant additive error. Full details of the ETH-based inapproximability results will appear in a future version.
  Beyond settling the complexity of deterministic communication complexity itself, the modular framework we develop opens the door to a wider class of reductions and, we believe, will prove useful in tackling other long-standing questions in communication complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05597v3</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Serge Gaspers, Tao Zixu He, Simon Mackenzie</dc:creator>
    </item>
    <item>
      <title>On the complexity of constrained reconfiguration and motion planning</title>
      <link>https://arxiv.org/abs/2508.13032</link>
      <description>arXiv:2508.13032v2 Announce Type: replace-cross 
Abstract: Coordinating the motion of multiple agents in constrained environments is a fundamental challenge in robotics, motion planning, and scheduling. A motivating example involves $n$ robotic arms, each represented as a line segment. The objective is to rotate each arm to its vertical orientation, one at a time (clockwise or counterclockwise), without collisions nor rotating any arm more than once. This scenario is an example of the more general $k$-Compatible Ordering problem, where $n$ agents, each capable of $k$ state-changing actions, must transition to specific target states under constraints encoded as a set $\mathcal{G}$ of $k$ pairs of directed graphs.
  We show that $k$-Compatible Ordering is $\mathsf{NP}$-complete, even when $\mathcal{G}$ is planar, degenerate, or acyclic. On the positive side, we provide polynomial-time algorithms for cases such as when $k = 1$ or $\mathcal{G}$ has bounded treewidth. We also introduce generalized variants supporting multiple state-changing actions per agent, broadening the applicability of our framework. These results extend to a wide range of scheduling, reconfiguration, and motion planning applications in constrained environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13032v2</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.RO</category>
      <category>math.CO</category>
      <pubDate>Wed, 20 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Bousquet, Remy El Sabeh, Amer E. Mouawad, Naomi Nishimura</dc:creator>
    </item>
  </channel>
</rss>
