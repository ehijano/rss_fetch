<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Sep 2024 01:46:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Query complexity lower bounds for local list-decoding and hard-core predicates (even for small rate and huge lists)</title>
      <link>https://arxiv.org/abs/2409.01708</link>
      <description>arXiv:2409.01708v1 Announce Type: new 
Abstract: A binary code Enc$:\{0,1\}^k \to \{0,1\}^n$ is $(0.5-\epsilon,L)$-list decodable if for all $w \in \{0,1\}^n$, the set List$(w)$ of all messages $m \in \{0,1\}^k$ such that the relative Hamming distance between Enc$(m)$ and $w$ is at most $0.5 -\epsilon$, has size at most $L$. Informally, a $q$-query local list-decoder for Enc is a randomized procedure Dec$:[k]\times [L] \to \{0,1\}$ that when given oracle access to a string $w$, makes at most $q$ oracle calls, and for every message $m \in \text{List}(w)$, with high probability, there exists $j \in [L]$ such that for every $i \in [k]$, with high probability, Dec$^w(i,j)=m_i$.
  We prove lower bounds on $q$, that apply even if $L$ is huge (say $L=2^{k^{0.9}}$) and the rate of Enc is small (meaning that $n \ge 2^{k}$):
  1. For $\epsilon \geq 1/k^{\nu}$ for some universal constant $0&lt; \nu &lt; 1$, we prove a lower bound of $q=\Omega(\frac{\log(1/\delta)}{\epsilon^2})$, where $\delta$ is the error probability of the local list-decoder. This bound is tight as there is a matching upper bound by Goldreich and Levin (STOC 1989) of $q=O(\frac{\log(1/\delta)}{\epsilon^2})$ for the Hadamard code (which has $n=2^k$). This bound extends an earlier work of Grinberg, Shaltiel and Viola (FOCS 2018) which only works if $n \le 2^{k^{\gamma}}$ for some universal constant $0&lt;\gamma &lt;1$, and the number of coins tossed by Dec is small (and therefore does not apply to the Hadamard code, or other codes with low rate).
  2. For smaller $\epsilon$, we prove a lower bound of roughly $q = \Omega(\frac{1}{\sqrt{\epsilon}})$. To the best of our knowledge, this is the first lower bound on the number of queries of local list-decoders that gives $q \ge k$ for small $\epsilon$.
  We also prove black-box limitations for improving some of the parameters of the Goldreich-Levin hard-core predicate construction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01708v1</guid>
      <category>cs.CC</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noga Ron-Zewi, Ronen Shaltiel, Nithin Varma</dc:creator>
    </item>
    <item>
      <title>Scalable Neighborhood Local Search for Single-Machine Scheduling with Family Setup Times</title>
      <link>https://arxiv.org/abs/2409.00771</link>
      <description>arXiv:2409.00771v1 Announce Type: cross 
Abstract: In this work, we study the task of scheduling jobs on a single machine with sequence dependent family setup times under the goal of minimizing the makespan, that is, the completion time of the last job in the schedule. This notoriously NP-hard problem is highly relevant in practical productions and requires heuristics that provide good solutions quickly in order to deal with large instances. In this paper, we present a heuristic based on the approach of parameterized local search. That is, we aim to replace a given solution by a better solution having distance at most $k$ in a pre-defined distance measure. This is done multiple times in a hill-climbing manner, until a locally optimal solution is reached. We analyze the trade-off between the allowed distance $k$ and the algorithm's running time for four natural distance measures. Example of allowed operations for our considered distance measures are: swapping $k$ pairs of jobs in the sequence, or rearranging $k$ consecutive jobs. For two distance measures, we show that finding an improvement for given $k$ can be done in $f(k) \cdot n^{\mathcal{O}(1)}$ time, while such a running time for the other two distance measures is unlikely. We provide a preliminary experimental evaluation of our local search approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00771v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaja Balzereit, Niels Gr\"uttemeier, Nils Morawietz, Dennis Reinhardt, Stefan Windmann, Petra Wolf</dc:creator>
    </item>
    <item>
      <title>Undecidability of Translational Tiling of the 4-dimensional Space with a Set of 4 Polyhypercubes</title>
      <link>https://arxiv.org/abs/2409.00846</link>
      <description>arXiv:2409.00846v1 Announce Type: cross 
Abstract: Recently, Greenfeld and Tao disproof the conjecture that translational tilings of a single tile can always be periodic [Ann. Math. 200(2024), 301-363]. In another paper [to appear in J. Eur. Math. Soc.], they also show that if the dimension $n$ is part of the input, the translational tiling for subsets of $\mathbb{Z}^n$ with one tile is undecidable. These two results are very strong pieces of evidence for the conjecture that translational tiling of $\mathbb{Z}^n$ with a monotile is undecidable, for some fixed $n$. This paper shows that translational tiling of the $3$-dimensional space with a set of $5$ polycubes is undecidable. By introducing a technique that lifts a set of polycubes and its tiling from $3$-dimensional space to $4$-dimensional space, we manage to show that translational tiling of the $4$-dimensional space with a set of $4$ tiles is undecidable. This is a step towards the attempt to settle the conjecture of the undecidability of translational tiling of the $n$-dimensional space with a monotile, for some fixed $n$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00846v1</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>math.MG</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chao Yang, Zhujun Zhang</dc:creator>
    </item>
    <item>
      <title>Classically estimating observables of noiseless quantum circuits</title>
      <link>https://arxiv.org/abs/2409.01706</link>
      <description>arXiv:2409.01706v1 Announce Type: cross 
Abstract: We present a classical algorithm for estimating expectation values of arbitrary observables on most quantum circuits across all circuit architectures and depths, including those with all-to-all connectivity. We prove that for any architecture where each circuit layer is equipped with a measure invariant under single-qubit rotations, our algorithm achieves a small error $\varepsilon$ on all circuits except for a small fraction $\delta$. The computational time is polynomial in qubit count and circuit depth for any small constant $\varepsilon, \delta$, and quasi-polynomial for inverse-polynomially small $\varepsilon, \delta$. For non-classically-simulable input states or observables, the expectation values can be estimated by augmenting our algorithm with classical shadows of the relevant state or observable. Our approach leverages a Pauli-path method under Heisenberg evolution. While prior works are limited to noisy quantum circuits, we establish classical simulability in noiseless regimes. Given that most quantum circuits in an architecture exhibit chaotic and locally scrambling behavior, our work demonstrates that estimating observables of such quantum dynamics is classically tractable across all geometries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01706v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Armando Angrisani, Alexander Schmidhuber, Manuel S. Rudolph, M. Cerezo, Zo\"e Holmes, Hsin-Yuan Huang</dc:creator>
    </item>
    <item>
      <title>From Amortized to Worst Case Delay in Enumeration Algorithms</title>
      <link>https://arxiv.org/abs/2108.10208</link>
      <description>arXiv:2108.10208v2 Announce Type: replace 
Abstract: The quality of enumeration algorithms is often measured by their delay, that is, the maximal time spent between the output of two distinct solutions. If the goal is to enumerate $t$ distinct solutions for any given $t$, then another relevant measure is the maximal time needed to output $t$ solutions divided by $t$, a notion we call the amortized delay of the algorithm, since it can be seen as the amortized complexity of the problem of enumerating $t$ elements in the set. In this paper, we study the relation between these two notions of delay, showing different schemes allowing one to transform an algorithm with polynomial amortized delay for which one has a blackbox access into an algorithm with polynomial delay. We complement our results by providing several lower bounds and impossibility theorems in the blackbox model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.10208v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florent Capelli, Yann Strozecki</dc:creator>
    </item>
    <item>
      <title>A Framework for Universality in Physics, Computer Science, and Beyond</title>
      <link>https://arxiv.org/abs/2307.06851</link>
      <description>arXiv:2307.06851v3 Announce Type: replace 
Abstract: Turing machines and spin models share a notion of universality according to which some simulate all others. Is there a theory of universality that captures this notion? We set up a categorical framework for universality which includes as instances universal Turing machines, universal spin models, NP completeness, top of a preorder, denseness of a subset, and more. By identifying necessary conditions for universality, we show that universal spin models cannot be finite. We also characterize when universality can be distinguished from a trivial one and use it to show that universal Turing machines are non-trivial in this sense. Our framework allows not only to compare universalities within each instance, but also instances themselves. We leverage a Fixed Point Theorem inspired by a result of Lawvere to establish that universality and negation give rise to unreachability (such as uncomputability). As such, this work sets the basis for a unified approach to universality and invites the study of further examples within the framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.06851v3</guid>
      <category>cs.CC</category>
      <category>cs.FL</category>
      <category>cs.LO</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.46298/compositionality-6-3</arxiv:DOI>
      <arxiv:journal_reference>Compositionality, Volume 6 (2024) (August 29, 2024) compositionality:14134</arxiv:journal_reference>
      <dc:creator>Tom\'a\v{s} Gonda, Tobias Reinhart, Sebastian Stengele, Gemma De les Coves</dc:creator>
    </item>
    <item>
      <title>Bounded-Depth Frege Lower Bounds for Random 3-CNFs via Deterministic Restrictions</title>
      <link>https://arxiv.org/abs/2403.02275</link>
      <description>arXiv:2403.02275v3 Announce Type: replace 
Abstract: A major open problem in proof complexity is to demonstrate that random 3-CNFs with a linear number of clauses require super-polynomial size refutations in bounded-depth Frege systems. We take the first step towards addressing this question by establishing a super-linear lower bound: for every $k$, there exists $\epsilon_k &gt; 0$ such that any depth-$k$ Frege refutation of a random $n$-variable 3-CNF with $\Theta(n)$ clauses has $\Omega(n^{1 + \epsilon_k})$ steps w.h.p. Our proof involves a novel adaptation of the deterministic restriction technique introduced by Chaudhuri and Radhakrishnan (STOC'96).
  For a given formula, this technique provides a method to fix a small number of variables in a bottom-up manner, ensuring that every surviving gate has small fan-in. Consequently, the resulting formula depends on a limited number of variables and can be simplified to a constant by a small variable assignment. Adapting this approach to proof complexity requires addressing the usual challenges associated with maintaining the hardness of a given instance. To this end, we introduce the following generalizations of standard proof complexity tools:
  - Weak expanders: These bipartite graphs relax the classical notion of expansion by only requiring that small sets have a non-empty boundary, while intermediate-sized sets have a large boundary. This property is sufficient to preserve hardness (e.g., for resolution width) and is easier to maintain as we remove vertices from the graph.
  - Formula assignments: To simplify a Frege proof, we consider a generalization of partial restrictions that assign values to formulas instead of just variables. We treat these assignments as new axioms added to our formula, as they generally cannot be expressed as variable substitutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02275v3</guid>
      <category>cs.CC</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Svyatoslav Gryaznov, Navid Talebanfard</dc:creator>
    </item>
    <item>
      <title>A Refined Laser Method and Faster Matrix Multiplication</title>
      <link>https://arxiv.org/abs/2010.05846</link>
      <description>arXiv:2010.05846v2 Announce Type: replace-cross 
Abstract: The complexity of matrix multiplication is measured in terms of $\omega$, the smallest real number such that two $n\times n$ matrices can be multiplied using $O(n^{\omega+\epsilon})$ field operations for all $\epsilon&gt;0$; the best bound until now is $\omega&lt;2.37287$ [Le Gall'14]. All bounds on $\omega$ since 1986 have been obtained using the so-called laser method, a way to lower-bound the `value' of a tensor in designing matrix multiplication algorithms. The main result of this paper is a refinement of the laser method that improves the resulting value bound for most sufficiently large tensors. Thus, even before computing any specific values, it is clear that we achieve an improved bound on $\omega$, and we indeed obtain the best bound on $\omega$ to date: $$\omega &lt; 2.37286.$$ The improvement is of the same magnitude as the improvement that [Le Gall'14] obtained over the previous bound [Vassilevska W.'12]. Our improvement to the laser method is quite general, and we believe it will have further applications in arithmetic complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.05846v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.46298/theoretics.24.21</arxiv:DOI>
      <arxiv:journal_reference>TheoretiCS, Volume 3 (2024), Article 21, 1-32</arxiv:journal_reference>
      <dc:creator>Josh Alman, Virginia Vassilevska Williams</dc:creator>
    </item>
    <item>
      <title>The Power of Lorentz Quantum Computer</title>
      <link>https://arxiv.org/abs/2403.04170</link>
      <description>arXiv:2403.04170v2 Announce Type: replace-cross 
Abstract: We demonstrate the superior capabilities of the recently proposed Lorentz quantum computer (LQC) compared to conventional quantum computers. We introduce an associated computational complexity class termed bounded-error Lorentz quantum polynomial-time (BLQP), demonstrating its equivalence to the complexity class ${\text P}^{\sharp \text{P}}$. We present LQC algorithms that efficiently solve the problem of maximum independent set, PP (probabilistic polynomial-time), and consequently ${\text P}^{\sharp \text{P}}$, all within polynomial time. Additionally, we show that the quantum computing with postselection proposed by Aaronson can be efficiently simulated by LQC, but not vice versa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04170v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi Zhang, Biao Wu</dc:creator>
    </item>
    <item>
      <title>Assembly Theory Reduced to Shannon Entropy and Rendered Redundant by Naive Statistical Algorithms</title>
      <link>https://arxiv.org/abs/2408.15108</link>
      <description>arXiv:2408.15108v2 Announce Type: replace-cross 
Abstract: Previously, we formally proved that any implementation of the concept of `copy number' underlying Assembly Theory (AT) and its assembly index (Ai) was equivalent to Shannon Entropy and not fundamentally or methodologically different from algorithms like ZIP and PNG via an LZ grammar. We show that the weak empirical correlation between Ai and LZW, which the authors offered as a defence against the previously proven result that the assembly index calculation method is an LZ scheme, is based on a misleading experiment. When the experiment is conducted properly the asymptotic convergence to LZ compression and Shannon Entropy is evident, and aligns with the mathematical proof previously provided. This completes both the theoretical and empirical demonstrations that any variation of the copy-number concept underlying AT, which resorts to counting the number of repetitions to arrive at a measure for life, is equivalent to statistical compression and Shannon Entropy. We demonstrate that the authors' `we-are-better-because-we-are-worse argument' does not withstand basic scrutiny, and that their primary empirical results separating organic from inorganic compounds have not only been previously reported -- sans claims to unify physics and biology -- but are also driven solely by molecular length, not by any special feature of life captured by their assembly index. Finally, we show that Ai is a special subcase of our Block Decomposition Method introduced almost a decade earlier.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15108v2</guid>
      <category>cs.IT</category>
      <category>cs.CC</category>
      <category>math.IT</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luan Ozelim, Abicumaran Uthamacumaran, Felipe S. Abrah\~ao, Santiago Hern\'andez-Orozco, Narsis A. Kiani, Jesper Tegn\'er, Hector Zenil</dc:creator>
    </item>
  </channel>
</rss>
