<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Jan 2025 03:26:52 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Automating Care by Self-maintainability for Full Laboratory Automation</title>
      <link>https://arxiv.org/abs/2501.05789</link>
      <description>arXiv:2501.05789v1 Announce Type: new 
Abstract: The automation of experiments in life sciences and chemistry has significantly advanced with the development of various instruments and AI technologies. However, achieving full laboratory automation, where experiments conceived by scientists are seamlessly executed in automated laboratories, remains a challenge. We identify the lack of automation in planning and operational tasks--critical human-managed processes collectively termed "care"--as a major barrier. Automating care is the key enabler for full laboratory automation. To address this, we propose the concept of self-maintainability (SeM): the ability of a laboratory system to autonomously adapt to internal and external disturbances, maintaining operational readiness akin to living cells. A SeM-enabled laboratory features autonomous recognition of its state, dynamic resource and information management, and adaptive responses to unexpected conditions. This shifts the planning and execution of experimental workflows, including scheduling and reagent allocation, from humans to the system. We present a conceptual framework for implementing SeM-enabled laboratories, comprising three modules--Requirement manager, Labware manager, and Device manager--and a Central manager. SeM not only enables scientists to execute envisioned experiments seamlessly but also provides developers with a design concept that drives the technological innovations needed for full automation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05789v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Koji Ochiai, Yuya Tahara-Arai, Akari Kato, Kazunari Kaizu, Hirokazu Kariyazaki, Makoto Umeno, Koichi Takahashi, Genki N. Kanda, Haruka Ozaki</dc:creator>
    </item>
    <item>
      <title>AI-powered virtual tissues from spatial proteomics for clinical diagnostics and biomedical discovery</title>
      <link>https://arxiv.org/abs/2501.06039</link>
      <description>arXiv:2501.06039v1 Announce Type: new 
Abstract: Spatial proteomics technologies have transformed our understanding of complex tissue architectures by enabling simultaneous analysis of multiple molecular markers and their spatial organization. The high dimensionality of these data, varying marker combinations across experiments and heterogeneous study designs pose unique challenges for computational analysis. Here, we present Virtual Tissues (VirTues), a foundation model framework for biological tissues that operates across the molecular, cellular and tissue scale. VirTues introduces innovations in transformer architecture design, including a novel tokenization scheme that captures both spatial and marker dimensions, and attention mechanisms that scale to high-dimensional multiplex data while maintaining interpretability. Trained on diverse cancer and non-cancer tissue datasets, VirTues demonstrates strong generalization capabilities without task-specific fine-tuning, enabling cross-study analysis and novel marker integration. As a generalist model, VirTues outperforms existing approaches across clinical diagnostics, biological discovery and patient case retrieval tasks, while providing insights into tissue function and disease mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06039v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Johann Wenckstern, Eeshaan Jain, Kiril Vasilev, Matteo Pariset, Andreas Wicki, Gabriele Gut, Charlotte Bunne</dc:creator>
    </item>
    <item>
      <title>Zero-shot Shark Tracking and Biometrics from Aerial Imagery</title>
      <link>https://arxiv.org/abs/2501.05717</link>
      <description>arXiv:2501.05717v1 Announce Type: cross 
Abstract: The recent widespread adoption of drones for studying marine animals provides opportunities for deriving biological information from aerial imagery. The large scale of imagery data acquired from drones is well suited for machine learning (ML) analysis. Development of ML models for analyzing marine animal aerial imagery has followed the classical paradigm of training, testing, and deploying a new model for each dataset, requiring significant time, human effort, and ML expertise. We introduce Frame Level ALIgment and tRacking (FLAIR), which leverages the video understanding of Segment Anything Model 2 (SAM2) and the vision-language capabilities of Contrastive Language-Image Pre-training (CLIP). FLAIR takes a drone video as input and outputs segmentation masks of the species of interest across the video. Notably, FLAIR leverages a zero-shot approach, eliminating the need for labeled data, training a new model, or fine-tuning an existing model to generalize to other species. With a dataset of 18,000 drone images of Pacific nurse sharks, we trained state-of-the-art object detection models to compare against FLAIR. We show that FLAIR massively outperforms these object detectors and performs competitively against two human-in-the-loop methods for prompting SAM2, achieving a Dice score of 0.81. FLAIR readily generalizes to other shark species without additional human effort and can be combined with novel heuristics to automatically extract relevant information including length and tailbeat frequency. FLAIR has significant potential to accelerate aerial imagery analysis workflows, requiring markedly less human effort and expertise than traditional machine learning workflows, while achieving superior accuracy. By reducing the effort required for aerial imagery analysis, FLAIR allows scientists to spend more time interpreting results and deriving insights about marine ecosystems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05717v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chinmay K Lalgudi, Mark E Leone, Jaden V Clark, Sergio Madrigal-Mora, Mario Espinoza</dc:creator>
    </item>
    <item>
      <title>Uncovering the Genetic Basis of Glioblastoma Heterogeneity through Multimodal Analysis of Whole Slide Images and RNA Sequencing Data</title>
      <link>https://arxiv.org/abs/2410.18710</link>
      <description>arXiv:2410.18710v2 Announce Type: replace 
Abstract: Glioblastoma is a highly aggressive form of brain cancer characterized by rapid progression and poor prognosis. Despite advances in treatment, the underlying genetic mechanisms driving this aggressiveness remain poorly understood. In this study, we employed multimodal deep learning approaches to investigate glioblastoma heterogeneity using joint image/RNA-seq analysis. Our results reveal novel genes associated with glioblastoma. By leveraging a combination of whole-slide images and RNA-seq, as well as introducing novel methods to encode RNA-seq data, we identified specific genetic profiles that may explain different patterns of glioblastoma progression. These findings provide new insights into the genetic mechanisms underlying glioblastoma heterogeneity and highlight potential targets for therapeutic intervention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18710v2</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmad Berjaoui (CRCT,IUCT Oncopole - UMR 1037), Louis Roussel (CRCT,IUCT Oncopole - UMR 1037), Eduardo Hugo Sanchez (CRCT,IUCT Oncopole - UMR 1037), Elizabeth Cohen-Jonathan Moyal (CRCT,IUCT Oncopole - UMR 1037)</dc:creator>
    </item>
  </channel>
</rss>
