<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Oct 2024 04:00:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Generalized Compare and Swap</title>
      <link>https://arxiv.org/abs/2410.19102</link>
      <description>arXiv:2410.19102v1 Announce Type: new 
Abstract: In this paper, we first propose a natural generalization of the well-known compare-and-swap object, one that replaces the equality comparison with an arbitrary comparator. We then present a simple wait-free universal construction using this object and prove its correctness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19102v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vassos Hadzilacos, Myles Thiessen, Sam Toueg</dc:creator>
    </item>
    <item>
      <title>Prediction-driven resource provisioning for serverless container runtimes</title>
      <link>https://arxiv.org/abs/2410.19215</link>
      <description>arXiv:2410.19215v1 Announce Type: new 
Abstract: In recent years Serverless Computing has emerged as a compelling cloud based model for the development of a wide range of data-intensive applications. However, rapid container provisioning introduces non-trivial challenges for FaaS cloud providers, as (i) real-world FaaS workloads may exhibit highly dynamic request patterns, (ii) applications have service-level objectives (SLOs) that must be met, and (iii) container provisioning can be a costly process. In this paper, we present SLOPE, a prediction framework for serverless FaaS platforms to address the aforementioned challenges. Specifically, it trains a neural network model that utilizes knowledge from past runs in order to estimate the number of instances required to satisfy the invocation rate requirements of the serverless applications. In cases that a priori knowledge is not available, SLOPE makes predictions using a graph edit distance approach to capture the similarities among serverless applications. Our experimental results illustrate the efficiency and benefits of our approach, which can reduce the operating costs by 66.25% on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19215v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dimitrios Tomaras, Michail Tsenos, Vana Kalogeraki</dc:creator>
    </item>
    <item>
      <title>Task queue implementation for edge computing platform</title>
      <link>https://arxiv.org/abs/2410.19344</link>
      <description>arXiv:2410.19344v1 Announce Type: new 
Abstract: With the rising number of distributed computer systems, from microservice web applications to IoT platforms, the question of reliable communication between different parts of the aforementioned systems is becoming increasingly important. As part of this paper, a task queue, which facilitates reliable asynchronous communication between different services, will be implemented. In order to control the flow of tasks through the queue and limit the load on downstream components, we are going to explore ways of efficiently restricting throughput, and defining priority queues within the task queue service. The research will also take a look at how different aspects of the task queue, such as the underlying persistence layer, affect their performance, reliability, and resource usage. This task queue will be implemented as a component in the already existing platform for managing clusters, constellations</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19344v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Veljko Maksimovic, Milos Simic, Milan Stojkov, Miroslav Zaric</dc:creator>
    </item>
    <item>
      <title>Efficient D-2-D with a Strong Group: Arbitrary Initial Configuration and No Global Knowledge</title>
      <link>https://arxiv.org/abs/2410.19468</link>
      <description>arXiv:2410.19468v1 Announce Type: new 
Abstract: Distance-2-Dispersion (D-2-D) problem aims to disperse $k$ mobile agents starting from an arbitrary initial configuration on an anonymous port-labeled graph $G$ with $n$ nodes such that no two agents occupy adjacent nodes in the final configuration, though multiple agents may occupy a single node if there is no other empty node whose all adjacent nodes are also empty. In the existing literature, this problem is solved starting from a rooted configuration for $k(\geq 1)$ agents using $O(m\Delta)$ synchronous rounds with a total of $O(\log n)$ memory per agent, where $m$ is the number of edges and $\Delta$ is the maximum degree of the graph. The existing work that solves the problem of D-2-D from scattered initial configuration considers $n+1$ agents to begin with. In this work, we start with $n$ mobile agents that start from an arbitrary initial configuration. They achieve D-2-D configuration that is also a maximal independent set of the graph and terminate in $O(max\{n\log^2 n, m\})$ rounds using $O(\log n)$ memory per agent. The agents do not have any prior knowledge of any graph parameters. This is a significant improvement over the existing works that solve D-2-D on arbitrary graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19468v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tanvir Kaur, Barun Gorain, Kaushik Mondal</dc:creator>
    </item>
    <item>
      <title>Validity in Network-Agnostic Byzantine Agreement</title>
      <link>https://arxiv.org/abs/2410.19721</link>
      <description>arXiv:2410.19721v1 Announce Type: new 
Abstract: In Byzantine Agreement (BA), there is a set of $n$ parties, from which up to $t$ can act byzantine. All honest parties must eventually decide on a common value (agreement), which must belong to a set determined by the inputs (validity). Depending on the use case, this set can grow or shrink, leading to various possible desiderata collectively known as validity conditions. Varying the validity property requirement can affect the regime under which BA is solvable. We study how the selected validity property impacts BA solvability in the network-agnostic model, where the network can either be synchronous with up to $t_s$ byzantine parties or asynchronous with up to $t_a \leq t_s$ byzantine parties.
  We show that for any non-trivial validity property the condition $2t_s + t_a &lt; n$ is necessary for BA to be solvable, even with cryptographic setup. Noteworthy, specializing this claim to $t_a = 0$ gives that $t &lt; n / 2$ is required when one expects a purely synchronous protocol to also work in asynchrony when there are no corruptions. This is especially surprising given that for some validity properties $t &lt; n$ are known to be achievable without the last stipulation. Thereafter, we give necessary and sufficient conditions for a validity property to render BA solvable, both for the case with cryptographic setup and for the one without. This traces the precise boundary of solvability in the network-agnostic model for every validity property. Our proof of sufficiency provides a universal protocol, that achieves BA for a given validity property whenever the provided conditions are satisfied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19721v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrei Constantinescu, Marc Dufay, Diana Ghinea, Roger Wattenhofer</dc:creator>
    </item>
    <item>
      <title>Parallelization of Network Dynamics Computations in Heterogeneous Distributed Environment</title>
      <link>https://arxiv.org/abs/2410.19075</link>
      <description>arXiv:2410.19075v1 Announce Type: cross 
Abstract: This paper addresses the problem of parallelizing computations to study non-linear dynamics in large networks of non-locally coupled oscillators using heterogeneous computing resources. The proposed approach can be applied to a variety of non-linear dynamics models with runtime specification of parameters and network topologies. Parallelizing the solution of equations for different network elements is performed transparently and, in contrast to available tools, does not require parallel programming from end-users. The runtime scheduler takes into account the performance of computing and communication resources to reduce downtime and to achieve a quasi-optimal parallelizing speed-up. The proposed approach was implemented, and its efficiency is proven by numerous applications for simulating large dynamical networks with 10^3-10^8 elements described by Hodgkin-Huxley, FitzHugh-Nagumo, and Kuramoto models, for investigating pathological synchronization during Parkinson's disease, analyzing multi-stability, for studying chimera and solitary states in 3D networks, etc. All the above computations may be performed using symmetrical multiprocessors, graphic processing units, and a network of workstations within the same run and it was demonstrated that near-linear speed-up can be achieved for large networks. The proposed approach is promising for extension to new hardware like edge-computing devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19075v1</guid>
      <category>nlin.CD</category>
      <category>cs.DC</category>
      <category>nlin.PS</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oleksandr Sudakov, Volodymyr Maistrenko</dc:creator>
    </item>
    <item>
      <title>BitPipe: Bidirectional Interleaved Pipeline Parallelism for Accelerating Large Models Training</title>
      <link>https://arxiv.org/abs/2410.19367</link>
      <description>arXiv:2410.19367v1 Announce Type: cross 
Abstract: With the increasing scale of models, the need for efficient distributed training has become increasingly urgent. Recently, many synchronous pipeline parallelism approaches have been proposed to improve training throughput. However, these approaches still suffer from two major issues, i.e., pipeline bubbles caused by periodic flushing and extra communication due to the increasing number of pipeline stages. To this end, we propose BitPipe, a bidirectional interleaved pipeline parallelism for accelerating large models training. Specifically, a hybrid scheme of fusing interleaved pipelines with bidirectional pipelines is proposed to reduce the computational time of each single micro-batch and multiply the number of devices executing simultaneously. A V-shaped schedule with eager gradient synchronization is introduced to reduce and overlap the communication between devices. Experiments conducted on up to 32 GPUs show that BitPipe improves the training throughput of GPT-style and BERT-style models by 1.05x-1.28x compared to the state-of-the-art synchronous approaches. The code of our implementation is available at https://github.com/wuhouming/BitPipe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19367v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Houming Wu, Ling Chen, Wenjie Yu</dc:creator>
    </item>
    <item>
      <title>COMSPLIT: A Communication-Aware Split Learning Design for Heterogeneous IoT Platforms</title>
      <link>https://arxiv.org/abs/2410.19375</link>
      <description>arXiv:2410.19375v1 Announce Type: cross 
Abstract: The significance of distributed learning and inference algorithms in Internet of Things (IoT) network is growing since they flexibly distribute computation load between IoT devices and the infrastructure, enhance data privacy, and minimize latency. However, a notable challenge stems from the influence of communication channel conditions on their performance. In this work, we introduce COMSPLIT: a novel communication-aware design for split learning (SL) and inference paradigm tailored to processing time series data in IoT networks. COMSPLIT provides a versatile framework for deploying adaptable SL in IoT networks affected by diverse channel conditions. In conjunction with the integration of an early-exit strategy, and addressing IoT scenarios containing devices with heterogeneous computational capabilities, COMSPLIT represents a comprehensive design solution for communication-aware SL in IoT networks. Numerical results show superior performance of COMSPLIT compared to vanilla SL approaches (that assume ideal communication channel), demonstrating its ability to offer both design simplicity and adaptability to different channel conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19375v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Vukan Ninkovic, Dejan Vukobratovic, Dragisa Miskovic, Marco Zennaro</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Deterministic Network Decomposition and Ruling Set, and Improved MIS</title>
      <link>https://arxiv.org/abs/2410.19516</link>
      <description>arXiv:2410.19516v1 Announce Type: cross 
Abstract: This paper improves and in two cases nearly settles, up to logarithmically lower-order factors, the deterministic complexity of some of the most central problems in distributed graph algorithms, which have been studied for over three decades:
  Near-Optimal Network Decomposition: We present a deterministic distributed algorithm that computes a network decomposition in approximately O(log^2 n) rounds, with O(log n) diameter and O(log n) colors. This round complexity is near-optimal in the following sense: even given an ideal network decomposition, using it (in the standard way) requires round complexity equal to the product of diameter and number of colors, which is known to be approximately Omega(log^2 n). This near-optimality is remarkable, considering the rarity of optimal deterministic distributed algorithms and that for network decomposition, the first polylogarithmic-round algorithm was achieved only recently, by Rozhon and Ghaffari [STOC 2020], after three decades.
  Near-Optimal Ruling Set: We present a deterministic distributed algorithm that computes an O(log log n) ruling set in approximately O(log n) rounds. This is an exponential improvement over the O(log n) ruling set of Awerbuch, Goldberg, Luby, and Plotkin [FOCS 1989], while almost matching their O(log n) round complexity. Our result's round complexity nearly matches the approximately Omega(log n) lower bound established by Balliu, Brandt, Kuhn, and Olivetti [STOC 2022], which applies to any poly(log log n) ruling set.
  Improved Maximal Independent Set (MIS): We present a deterministic distributed algorithm for computing an MIS in approximately O(log^(5/3) n) rounds. This improves upon the approximately O(log^2 n) complexity achieved by Ghaffari and Grunau [STOC 2023] and breaks the log-squared barrier necessary for any method based on network decomposition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19516v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Ghaffari, Christoph Grunau</dc:creator>
    </item>
    <item>
      <title>Compress then Serve: Serving Thousands of LoRA Adapters with Little Overhead</title>
      <link>https://arxiv.org/abs/2407.00066</link>
      <description>arXiv:2407.00066v2 Announce Type: replace 
Abstract: Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs) has become common practice, often yielding numerous copies of the same LLM differing only in their LoRA updates. This paradigm presents challenges for systems that serve real-time responses to queries that each involve a different LoRA. Prior works optimize the design of such systems but still require continuous loading and offloading of LoRAs, as it is infeasible to store thousands of LoRAs in GPU memory. To mitigate this issue, we investigate the efficacy of model compression when serving LoRAs. We propose a method for joint compression of LoRAs into a shared basis paired with LoRA-specific scaling matrices. We extend our algorithm to learn clusters of LoRAs that are more amenable to joint compression, allowing it to scale gracefully to large LoRA collections. Our experiments with up to 500 LoRAs demonstrate that compressed LoRAs preserve performance while offering major throughput gains in realistic serving scenarios with over a thousand LoRAs, maintaining 80% of the throughput of serving a single LoRA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00066v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rickard Br\"uel-Gabrielsson, Jiacheng Zhu, Onkar Bhardwaj, Leshem Choshen, Kristjan Greenewald, Mikhail Yurochkin, Justin Solomon</dc:creator>
    </item>
    <item>
      <title>Optimal Fault-Tolerant Dispersion on Oriented Grids</title>
      <link>https://arxiv.org/abs/2410.17813</link>
      <description>arXiv:2410.17813v2 Announce Type: replace 
Abstract: Dispersion of mobile robots over the nodes of an anonymous graph is an important problem and turns out to be a crucial subroutine for designing efficient algorithms for many fundamental graph problems via mobile robots. In this problem, starting from an arbitrary initial distribution of $n$ robots across the $n$ nodes, the goal is to achieve a final configuration where each node holds at most one robot. This paper investigates the dispersion problem on an oriented grid, considering the possibility of robot failures (crashes) at any time during the algorithm's execution. We present a crash-tolerant dispersion algorithm that solves the dispersion problem on an anonymous oriented grid in $O(\sqrt{n})$ time and using $O(\log n)$ bits of memory per robot. The algorithm is optimal in terms of both time and memory per robot. We further extend this algorithm to deal with weak Byzantine robots. The weak Byzantine fault dispersion algorithm takes optimal $O(\sqrt{n})$ rounds but requires $O(n\log n)$ bits of memory per robot.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17813v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rik Banerjee, Manish Kumar, Anisur Rahaman Molla</dc:creator>
    </item>
    <item>
      <title>DistRL: An Asynchronous Distributed Reinforcement Learning Framework for On-Device Control Agents</title>
      <link>https://arxiv.org/abs/2410.14803</link>
      <description>arXiv:2410.14803v2 Announce Type: replace-cross 
Abstract: On-device control agents, especially on mobile devices, are responsible for operating mobile devices to fulfill users' requests, enabling seamless and intuitive interactions. Integrating Multimodal Large Language Models (MLLMs) into these agents enhances their ability to understand and execute complex commands, thereby improving user experience. However, fine-tuning MLLMs for on-device control presents significant challenges due to limited data availability and inefficient online training processes. This paper introduces DistRL, a novel framework designed to enhance the efficiency of online RL fine-tuning for mobile device control agents. DistRL employs centralized training and decentralized data acquisition to ensure efficient fine-tuning in the context of dynamic online interactions. Additionally, the framework is backed by our tailor-made RL algorithm, which effectively balances exploration with the prioritized utilization of collected data to ensure stable and robust training. Our experiments show that, on average, DistRL delivers a 3X improvement in training efficiency and enables training data collection 2.4X faster than the leading synchronous multi-machine methods. Notably, after training, DistRL achieves a 20% relative improvement in success rate compared to state-of-the-art methods on general Android tasks from an open benchmark, significantly outperforming existing approaches while maintaining the same training time. These results validate DistRL as a scalable and efficient solution, offering substantial improvements in both training efficiency and agent performance for real-world, in-the-wild device control tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14803v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Taiyi Wang, Zhihao Wu, Jianheng Liu, Jianye Hao, Jun Wang, Kun Shao</dc:creator>
    </item>
  </channel>
</rss>
