<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 10 Oct 2024 04:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A New Method for Multinomial Inference using Dempster-Shafer Theory</title>
      <link>https://arxiv.org/abs/2410.05512</link>
      <description>arXiv:2410.05512v1 Announce Type: new 
Abstract: A new method for multinomial inference is proposed by representing the cell probabilities as unordered segments on the unit interval and following Dempster-Shafer (DS) theory. The resulting DS posterior is then strengthened to improve symmetry and learning properties with the final posterior model being characterized by a Dirichlet distribution. In addition to computational simplicity, the new model has desirable invariance properties related to category permutations, refinements, and coarsenings. Furthermore, posterior inference on relative probabilities amongst certain cells depends only on data for the cells in question. Finally, the model is quite flexible with regard to parameterization and the range of testable assertions. Comparisons are made to existing methods and illustrated with two examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05512v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Earl C. Lawrence, Alexander C. Murph, Scott A. Vander Wiel, Chaunhai Liu</dc:creator>
    </item>
    <item>
      <title>Comparing HIV Vaccine Immunogenicity across Trials with Different Populations and Study Designs</title>
      <link>https://arxiv.org/abs/2410.05594</link>
      <description>arXiv:2410.05594v1 Announce Type: new 
Abstract: Safe and effective preventive vaccines have the potential to help stem the HIV epidemic. The efficacy of such vaccines is typically measured in randomized, double-blind phase IIb/III trials and described as a reduction in newly acquired HIV infections. However, such trials are often expensive, time-consuming, and/or logistically challenging. These challenges lead to a great interest in immune responses induced by vaccination, and in identifying which immune responses predict vaccine efficacy. These responses are termed vaccine correlates of protection. Studies of vaccine-induced immunogenicity vary in size and design, ranging from small, early phase trials, to case-control studies nested in a broader late-phase randomized trial. Moreover, trials can be conducted in geographically diverse study populations across the world. Such diversity presents a challenge for objectively comparing vaccine-induced immunogenicity. To address these practical challenges, we propose a framework that is capable of identifying appropriate causal estimands and estimators, which can be used to provide standardized comparisons of vaccine-induced immunogenicity across trials. We evaluate the performance of the proposed estimands via extensive simulation studies. Our estimators are well-behaved and enjoy robustness properties. The proposed technique is applied to compare vaccine immunogenicity using data from three recent HIV vaccine trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05594v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yutong Jin, Alex Luedtke, Zoe Moodie, Holly Janes, David Benkeser</dc:creator>
    </item>
    <item>
      <title>Identification and estimation for matrix time series CP-factor models</title>
      <link>https://arxiv.org/abs/2410.05634</link>
      <description>arXiv:2410.05634v1 Announce Type: new 
Abstract: We investigate the identification and the estimation for matrix time series CP-factor models. Unlike the generalized eigenanalysis-based method of Chang et al. (2023) which requires the two factor loading matrices to be full-ranked, the newly proposed estimation can handle rank-deficient factor loading matrices. The estimation procedure consists of the spectral decomposition of several matrices and a matrix joint diagonalization algorithm, resulting in low computational cost. The theoretical guarantee established without the stationarity assumption shows that the proposed estimation exhibits a faster convergence rate than that of Chang et al. (2023). In fact the new estimator is free from the adverse impact of any eigen-gaps, unlike most eigenanalysis-based methods such as that of Chang et al. (2023). Furthermore, in terms of the error rates of the estimation, the proposed procedure is equivalent to handling a vector time series of dimension $\max(p,q)$ instead of $p \times q$, where $(p, q)$ are the dimensions of the matrix time series concerned. We have achieved this without assuming the "near orthogonality" of the loadings under various incoherence conditions often imposed in the CP-decomposition literature, see Han and Zhang (2022), Han et al. (2024) and the references within. Illustration with both simulated and real matrix time series data shows the usefulness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05634v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chang, Yue Du, Guanglin Huang, Qiwei Yao</dc:creator>
    </item>
    <item>
      <title>Detecting dependence structure: visualization and inference</title>
      <link>https://arxiv.org/abs/2410.05858</link>
      <description>arXiv:2410.05858v1 Announce Type: new 
Abstract: Identifying dependency between two random variables is a fundamental problem. Clear interpretability and ability of a procedure to provide information on the form of the possible dependence is particularly important in evaluating dependencies. We introduce a new estimator of the quantile dependence function and pertinent local acceptance regions. This leads to insightful visualization and evaluation of underlying dependence structure. We also propose a test of independence of two random variables, pertinent to this new estimator. Our procedures are based on ranks and we derive a finite-sample theory that guarantees the inferential validity of our solutions at any given sample size. The procedures are simple to implement and computationally efficient. Large-sample consistency of the proposed test is also proved. We show that, in terms of power, new test is one of the best statistics for independence testing when considering a wide range of alternative models. Finally, we demonstrate use of our approach to visualize dependence structure and to detect local departures from independence through analyzing some datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05858v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bogdan \'Cmiel, Teresa Ledwina</dc:creator>
    </item>
    <item>
      <title>Persistence-Robust Break Detection in Predictive Quantile and CoVaR Regressions</title>
      <link>https://arxiv.org/abs/2410.05861</link>
      <description>arXiv:2410.05861v1 Announce Type: new 
Abstract: Forecasting risk (as measured by quantiles) and systemic risk (as measured by Adrian and Brunnermeiers's (2016) CoVaR) is important in economics and finance. However, past research has shown that predictive relationships may be unstable over time. Therefore, this paper develops structural break tests in predictive quantile and CoVaR regressions. These tests can detect changes in the forecasting power of covariates, and are based on the principle of self-normalization. We show that our tests are valid irrespective of whether the predictors are stationary or near-stationary, rendering the tests suitable for a range of practical applications. Simulations illustrate the good finite-sample properties of our tests. Two empirical applications concerning equity premium and systemic risk forecasting models show the usefulness of the tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05861v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yannick Hoga</dc:creator>
    </item>
    <item>
      <title>Model Uncertainty and Missing Data: An Objective Bayesian Perspective</title>
      <link>https://arxiv.org/abs/2410.05893</link>
      <description>arXiv:2410.05893v1 Announce Type: new 
Abstract: The interplay between missing data and model uncertainty -- two classic statistical problems -- leads to primary questions that we formally address from an objective Bayesian perspective. For the general regression problem, we discuss the probabilistic justification of Rubin's rules applied to the usual components of Bayesian variable selection, arguing that prior predictive marginals should be central to the pursued methodology. In the regression settings, we explore the conditions of prior distributions that make the missing data mechanism ignorable. Moreover, when comparing multiple linear models, we provide a complete methodology for dealing with special cases, such as variable selection or uncertainty regarding model errors. In numerous simulation experiments, we demonstrate that our method outperforms or equals others, in consistently producing results close to those obtained using the full dataset. In general, the difference increases with the percentage of missing data and the correlation between the variables used for imputation. Finally, we summarize possible directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05893v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gonzalo Garc\'ia-Donato, Mar\'ia Eugenia Castellanos, Stefano Cabras, Alicia Quir\'os, Anabel Forte</dc:creator>
    </item>
    <item>
      <title>Dynamic graphical models: Theory, structure and counterfactual forecasting</title>
      <link>https://arxiv.org/abs/2410.06125</link>
      <description>arXiv:2410.06125v1 Announce Type: new 
Abstract: Simultaneous graphical dynamic linear models (SGDLMs) provide advances in flexibility, parsimony and scalability of multivariate time series analysis, with proven utility in forecasting. Core theoretical aspects of such models are developed, including new results linking dynamic graphical and latent factor models. Methodological developments extend existing Bayesian sequential analyses for model marginal likelihood evaluation and counterfactual forecasting. The latter, involving new Bayesian computational developments for missing data in SGDLMs, is motivated by causal applications. A detailed example illustrating the models and new methodology concerns global macroeconomic time series with complex, time-varying cross-series relationships and primary interests in potential causal effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06125v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mike West, Luke Vrotsos</dc:creator>
    </item>
    <item>
      <title>Sequential Design with Derived Win Statistics</title>
      <link>https://arxiv.org/abs/2410.06281</link>
      <description>arXiv:2410.06281v1 Announce Type: new 
Abstract: The Win Ratio has gained significant traction in cardiovascular trials as a novel method for analyzing composite endpoints (Pocock and others, 2012). Compared with conventional approaches based on time to the first event, the Win Ratio accommodates the varying priorities and types of outcomes among components, potentially offering greater statistical power by fully utilizing the information contained within each outcome. However, studies using Win Ratio have largely been confined to fixed design, limiting flexibility for early decisions, such as stopping for futility or efficacy. Our study proposes a sequential design framework incorporating multiple interim analyses based on Win Ratio or Net Benefit statistics. Moreover, we provide rigorous proof of the canonical joint distribution for sequential Win Ratio and Net Benefit statistics, and an algorithm for sample size determination is developed. We also provide results from a finite sample simulation study, which show that our proposed method controls Type I error maintains power level, and has a smaller average sample size than the fixed design. A real study of cardiovascular study is applied to illustrate the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06281v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Baoshan Zhang, Yuan Wu</dc:creator>
    </item>
    <item>
      <title>A convex formulation of covariate-adjusted Gaussian graphical models via natural parametrization</title>
      <link>https://arxiv.org/abs/2410.06326</link>
      <description>arXiv:2410.06326v1 Announce Type: new 
Abstract: Gaussian graphical models (GGMs) are widely used for recovering the conditional independence structure among random variables. Recently, several key advances have been made to exploit an additional set of variables for better estimating the GGMs of the variables of interest. For example, in co-expression quantitative trait locus (eQTL) studies, both the mean expression level of genes as well as their pairwise conditional independence structure may be adjusted by genetic variants local to those genes. Existing methods to estimate covariate-adjusted GGMs either allow only the mean to depend on covariates or suffer from poor scaling assumptions due to the inherent non-convexity of simultaneously estimating the mean and precision matrix. In this paper, we propose a convex formulation that jointly estimates the covariate-adjusted mean and precision matrix by utilizing the natural parametrization of the multivariate Gaussian likelihood. This convexity yields theoretically better performance as the sparsity and dimension of the covariates grow large relative to the number of samples. We verify our theoretical results with numerical simulations and perform a reanalysis of an eQTL study of glioblastoma multiforme (GBM), an aggressive form of brain cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06326v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruobin Liu, Guo Yu</dc:creator>
    </item>
    <item>
      <title>A Non-parametric Direct Learning Approach to Heterogeneous Treatment Effect Estimation under Unmeasured Confounding</title>
      <link>https://arxiv.org/abs/2410.06377</link>
      <description>arXiv:2410.06377v1 Announce Type: new 
Abstract: In many social, behavioral, and biomedical sciences, treatment effect estimation is a crucial step in understanding the impact of an intervention, policy, or treatment. In recent years, an increasing emphasis has been placed on heterogeneity in treatment effects, leading to the development of various methods for estimating Conditional Average Treatment Effects (CATE). These approaches hinge on a crucial identifying condition of no unmeasured confounding, an assumption that is not always guaranteed in observational studies or randomized control trials with non-compliance. In this paper, we proposed a general framework for estimating CATE with a possible unmeasured confounder using Instrumental Variables. We also construct estimators that exhibit greater efficiency and robustness against various scenarios of model misspecification. The efficacy of the proposed framework is demonstrated through simulation studies and a real data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06377v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinhai Zhang, Xingye Qiao</dc:creator>
    </item>
    <item>
      <title>Nested Compound Random Measures</title>
      <link>https://arxiv.org/abs/2410.06394</link>
      <description>arXiv:2410.06394v1 Announce Type: new 
Abstract: Nested nonparametric processes are vectors of random probability measures widely used in the Bayesian literature to model the dependence across distinct, though related, groups of observations. These processes allow a two-level clustering, both at the observational and group levels. Several alternatives have been proposed starting from the nested Dirichlet process by Rodr\'iguez et al. (2008). However, most of the available models are neither computationally efficient or mathematically tractable. In the present paper, we aim to introduce a range of nested processes that are mathematically tractable, flexible, and computationally efficient. Our proposal builds upon Compound Random Measures, which are vectors of dependent random measures early introduced by Griffin and Leisen (2017). We provide a complete investigation of theoretical properties of our model. In particular, we prove a general posterior characterization for vectors of Compound Random Measures, which is interesting per se and still not available in the current literature. Based on our theoretical results and the available posterior representation, we develop the first Ferguson &amp; Klass algorithm for nested nonparametric processes. We specialize our general theorems and algorithms in noteworthy examples. We finally test the model's performance on different simulated scenarios, and we exploit the construction to study air pollution in different provinces of an Italian region (Lombardy). We empirically show how nested processes based on Compound Random Measures outperform other Bayesian competitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06394v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federico Camerlenghi, Riccardo Corradin, Andrea Ongaro</dc:creator>
    </item>
    <item>
      <title>False Discovery Rate Control via Data Splitting for Testing-after-Clustering</title>
      <link>https://arxiv.org/abs/2410.06451</link>
      <description>arXiv:2410.06451v1 Announce Type: new 
Abstract: Testing for differences in features between clusters in various applications often leads to inflated false positives when practitioners use the same dataset to identify clusters and then test features, an issue commonly known as ``double dipping''. To address this challenge, inspired by data-splitting strategies for controlling the false discovery rate (FDR) in regressions \parencite{daiFalseDiscoveryRate2023}, we present a novel method that applies data-splitting to control FDR while maintaining high power in unsupervised clustering. We first divide the dataset into two halves, then apply the conventional testing-after-clustering procedure to each half separately and combine the resulting test statistics to form a new statistic for each feature. The new statistic can help control the FDR due to its property of having a sampling distribution that is symmetric around zero for any null feature. To further enhance stability and power, we suggest multiple data splitting, which involves repeatedly splitting the data and combining results. Our proposed data-splitting methods are mathematically proven to asymptotically control FDR in Gaussian settings. Through extensive simulations and analyses of single-cell RNA sequencing (scRNA-seq) datasets, we demonstrate that the data-splitting methods are easy to implement, adaptable to existing single-cell data analysis pipelines, and often outperform other approaches when dealing with weak signals and high correlations among features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06451v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lijun Wang, Yingxin Lin, Hongyu Zhao</dc:creator>
    </item>
    <item>
      <title>Model-assisted and Knowledge-guided Transfer Regression for the Underrepresented Population</title>
      <link>https://arxiv.org/abs/2410.06484</link>
      <description>arXiv:2410.06484v1 Announce Type: new 
Abstract: Covariate shift and outcome model heterogeneity are two prominent challenges in leveraging external sources to improve risk modeling for underrepresented cohorts in paucity of accurate labels. We consider the transfer learning problem targeting some unlabeled minority sample encountering (i) covariate shift to the labeled source sample collected on a different cohort; and (ii) outcome model heterogeneity with some majority sample informative to the targeted minority model. In this scenario, we develop a novel model-assisted and knowledge-guided transfer learning targeting underrepresented population (MAKEUP) approach for high-dimensional regression models. Our MAKEUP approach includes a model-assisted debiasing step in response to the covariate shift, accompanied by a knowledge-guided sparsifying procedure leveraging the majority data to enhance learning on the minority group. We also develop a model selection method to avoid negative knowledge transfer that can work in the absence of gold standard labels on the target sample. Theoretical analyses show that MAKEUP provides efficient estimation for the target model on the minority group. It maintains robustness to the high complexity and misspecification of the nuisance models used for covariate shift correction, as well as adaptivity to the model heterogeneity and potential negative transfer between the majority and minority groups. Numerical studies demonstrate similar advantages in finite sample settings over existing approaches. We also illustrate our approach through a real-world application about the transfer learning of Type II diabetes genetic risk models on some underrepresented ancestry group.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06484v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Doudou Zhou, Mengyan Li, Tianxi Cai, Molei Liu</dc:creator>
    </item>
    <item>
      <title>When Does Interference Matter? Decision-Making in Platform Experiments</title>
      <link>https://arxiv.org/abs/2410.06580</link>
      <description>arXiv:2410.06580v1 Announce Type: new 
Abstract: This paper investigates decision-making in A/B experiments for online platforms and marketplaces. In such settings, due to constraints on inventory, A/B experiments typically lead to biased estimators because of interference; this phenomenon has been well studied in recent literature. By contrast, there has been relatively little discussion of the impact of interference on decision-making. In this paper, we analyze a benchmark Markovian model of an inventory-constrained platform, where arriving customers book listings that are limited in supply; our analysis builds on a self-contained analysis of general A/B experiments for Markov chains. We focus on the commonly used frequentist hypothesis testing approach for making launch decisions based on data from customer-randomized experiments, and we study the impact of interference on (1) false positive probability and (2) statistical power.
  We obtain three main findings. First, we show that for {\em monotone} treatments -- i.e., those where the treatment changes booking probabilities in the same direction relative to control in all states -- the false positive probability of the na\"ive difference-in-means estimator with classical variance estimation is correctly controlled. This result stems from a novel analysis of A/A experiments with arbitrary dependence structures, which may be of independent interest. Second, we demonstrate that for monotone treatments, the statistical power of this na\"ive approach is higher than that of any similar pipeline using a debiased estimator. Taken together, these two findings suggest that platforms may be better off *not* debiasing when treatments are monotone. Finally, using simulations, we investigate false positive probability and statistical power when treatments are non-monotone, and we show that the performance of the na\"ive approach can be arbitrarily worse than a debiased approach in such cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06580v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ramesh Johari, Hannah Li, Anushka Murthy, Gabriel Y. Weintraub</dc:creator>
    </item>
    <item>
      <title>Sharp Bounds of the Causal Effect Under MNAR Confounding</title>
      <link>https://arxiv.org/abs/2410.06726</link>
      <description>arXiv:2410.06726v1 Announce Type: new 
Abstract: We report bounds for any contrast between the probabilities of the counterfactual outcome under exposure and non-exposure when the confounders are missing not at random. We assume that the missingness mechanism is outcome-independent, and prove that our bounds are arbitrarily sharp, i.e., practically attainable or logically possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06726v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose M. Pe\~na</dc:creator>
    </item>
    <item>
      <title>Direct Estimation for Commonly Used Pattern-Mixture Models in Clinical Trials</title>
      <link>https://arxiv.org/abs/2410.06939</link>
      <description>arXiv:2410.06939v1 Announce Type: new 
Abstract: Pattern-mixture models have received increasing attention as they are commonly used to assess treatment effects in primary or sensitivity analyses for clinical trials with nonignorable missing data. Pattern-mixture models have traditionally been implemented using multiple imputation, where the variance estimation may be a challenge because the Rubin's approach of combining between- and within-imputation variance may not provide consistent variance estimation while bootstrap methods may be time-consuming. Direct likelihood-based approaches have been proposed in the literature and implemented for some pattern-mixture models, but the assumptions are sometimes restrictive, and the theoretical framework is fragile. In this article, we propose an analytical framework for an efficient direct likelihood estimation method for commonly used pattern-mixture models corresponding to return-to-baseline, jump-to-reference, placebo washout, and retrieved dropout imputations. A parsimonious tipping point analysis is also discussed and implemented. Results from simulation studies demonstrate that the proposed methods provide consistent estimators. We further illustrate the utility of the proposed methods using data from a clinical trial evaluating a treatment for type 2 diabetes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06939v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jitong Lou, Mallikarjuna Rettiganti, Yongming Qu</dc:creator>
    </item>
    <item>
      <title>Spatiotemporal Modeling and Forecasting at Scale with Dynamic Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2410.07161</link>
      <description>arXiv:2410.07161v1 Announce Type: new 
Abstract: Spatiotemporal data consisting of timestamps, GPS coordinates, and IDs occurs in many settings. Modeling approaches for this type of data must address challenges in terms of sensor noise, uneven sampling rates, and non-persistent IDs. In this work, we characterize and forecast human mobility at scale with dynamic generalized linear models (DGLMs). We represent mobility data as occupancy counts of spatial cells over time and use DGLMs to model the occupancy counts for each spatial cell in an area of interest. DGLMs are flexible to varying numbers of occupancy counts across spatial cells, are dynamic, and easily incorporate daily and weekly seasonality in the aggregate-level behavior. Our overall approach is robust to various types of noise and scales linearly in the number of spatial cells, time bins, and agents. Our results show that DGLMs provide accurate occupancy count forecasts over a variety of spatial resolutions and forecast horizons. We also present scaling results for spatiotemporal data consisting of hundreds of millions of observations. Our approach is flexible to support several downstream applications, including characterizing human mobility, forecasting occupancy counts, and anomaly detection for aggregate-level behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07161v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3681765.3698449</arxiv:DOI>
      <arxiv:journal_reference>In 1st ACM SIGSPATIAL International Workshop on Geospatial Anomaly Detection (GeoAnomalies'24), October 29, 2024, Atlanta, GA, USA</arxiv:journal_reference>
      <dc:creator>Pranay Pherwani, Nicholas Hass, Anna K. Yanchenko</dc:creator>
    </item>
    <item>
      <title>Refining Counterfactual Explanations With Joint-Distribution-Informed Shapley Towards Actionable Minimality</title>
      <link>https://arxiv.org/abs/2410.05419</link>
      <description>arXiv:2410.05419v1 Announce Type: cross 
Abstract: Counterfactual explanations (CE) identify data points that closely resemble the observed data but produce different machine learning (ML) model outputs, offering critical insights into model decisions. Despite the diverse scenarios, goals and tasks to which they are tailored, existing CE methods often lack actionable efficiency because of unnecessary feature changes included within the explanations that are presented to users and stakeholders. We address this problem by proposing a method that minimizes the required feature changes while maintaining the validity of CE, without imposing restrictions on models or CE algorithms, whether instance- or group-based. The key innovation lies in computing a joint distribution between observed and counterfactual data and leveraging it to inform Shapley values for feature attributions (FA). We demonstrate that optimal transport (OT) effectively derives this distribution, especially when the alignment between observed and counterfactual data is unclear in used CE methods. Additionally, a counterintuitive finding is uncovered: it may be misleading to rely on an exact alignment defined by the CE generation mechanism in conducting FA. Our proposed method is validated on extensive experiments across multiple datasets, showcasing its effectiveness in refining CE towards greater actionable efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05419v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lei You, Yijun Bian, Lele Cao</dc:creator>
    </item>
    <item>
      <title>Online scalable Gaussian processes with conformal prediction for guaranteed coverage</title>
      <link>https://arxiv.org/abs/2410.05444</link>
      <description>arXiv:2410.05444v1 Announce Type: cross 
Abstract: The Gaussian process (GP) is a Bayesian nonparametric paradigm that is widely adopted for uncertainty quantification (UQ) in a number of safety-critical applications, including robotics, healthcare, as well as surveillance. The consistency of the resulting uncertainty values however, hinges on the premise that the learning function conforms to the properties specified by the GP model, such as smoothness, periodicity and more, which may not be satisfied in practice, especially with data arriving on the fly. To combat against such model mis-specification, we propose to wed the GP with the prevailing conformal prediction (CP), a distribution-free post-processing framework that produces it prediction sets with a provably valid coverage under the sole assumption of data exchangeability. However, this assumption is usually violated in the online setting, where a prediction set is sought before revealing the true label. To ensure long-term coverage guarantee, we will adaptively set the key threshold parameter based on the feedback whether the true label falls inside the prediction set. Numerical results demonstrate the merits of the online GP-CP approach relative to existing alternatives in the long-term coverage performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05444v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinwen Xu, Qin Lu, Georgios B. Giannakis</dc:creator>
    </item>
    <item>
      <title>Testing Credibility of Public and Private Surveys through the Lens of Regression</title>
      <link>https://arxiv.org/abs/2410.05458</link>
      <description>arXiv:2410.05458v1 Announce Type: cross 
Abstract: Testing whether a sample survey is a credible representation of the population is an important question to ensure the validity of any downstream research. While this problem, in general, does not have an efficient solution, one might take a task-based approach and aim to understand whether a certain data analysis tool, like linear regression, would yield similar answers both on the population and the sample survey. In this paper, we design an algorithm to test the credibility of a sample survey in terms of linear regression. In other words, we design an algorithm that can certify if a sample survey is good enough to guarantee the correctness of data analysis done using linear regression tools. Nowadays, one is naturally concerned about data privacy in surveys. Thus, we further test the credibility of surveys published in a differentially private manner. Specifically, we focus on Local Differential Privacy (LDP), which is a standard technique to ensure privacy in surveys where the survey participants might not trust the aggregator. We extend our algorithm to work even when the data analysis has been done using surveys with LDP. In the process, we also propose an algorithm that learns with high probability the guarantees a linear regression model on a survey published with LDP. Our algorithm also serves as a mechanism to learn linear regression models from data corrupted with noise coming from any subexponential distribution. We prove that it achieves the optimal estimation error bound for $\ell_1$ linear regression, which might be of broader interest. We prove the theoretical correctness of our algorithms while trying to reduce the sample complexity for both public and private surveys. We also numerically demonstrate the performance of our algorithms on real and synthetic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05458v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Debabrota Basu, Sourav Chakraborty, Debarshi Chanda, Buddha Dev Das, Arijit Ghosh, Arnab Ray</dc:creator>
    </item>
    <item>
      <title>Neural Networks Decoded: Targeted and Robust Analysis of Neural Network Decisions via Causal Explanations and Reasoning</title>
      <link>https://arxiv.org/abs/2410.05484</link>
      <description>arXiv:2410.05484v1 Announce Type: cross 
Abstract: Despite their success and widespread adoption, the opaque nature of deep neural networks (DNNs) continues to hinder trust, especially in critical applications. Current interpretability solutions often yield inconsistent or oversimplified explanations, or require model changes that compromise performance. In this work, we introduce TRACER, a novel method grounded in causal inference theory designed to estimate the causal dynamics underpinning DNN decisions without altering their architecture or compromising their performance. Our approach systematically intervenes on input features to observe how specific changes propagate through the network, affecting internal activations and final outputs. Based on this analysis, we determine the importance of individual features, and construct a high-level causal map by grouping functionally similar layers into cohesive causal nodes, providing a structured and interpretable view of how different parts of the network influence the decisions. TRACER further enhances explainability by generating counterfactuals that reveal possible model biases and offer contrastive explanations for misclassifications. Through comprehensive evaluations across diverse datasets, we demonstrate TRACER's effectiveness over existing methods and show its potential for creating highly compressed yet accurate models, illustrating its dual versatility in both understanding and optimizing DNNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05484v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec F. Diallo, Vaishak Belle, Paul Patras</dc:creator>
    </item>
    <item>
      <title>Scalable Inference for Bayesian Multinomial Logistic-Normal Dynamic Linear Models</title>
      <link>https://arxiv.org/abs/2410.05548</link>
      <description>arXiv:2410.05548v1 Announce Type: cross 
Abstract: Many scientific fields collect longitudinal count compositional data. Each observation is a multivariate count vector, where the total counts are arbitrary, and the information lies in the relative frequency of the counts. Multiple authors have proposed Bayesian Multinomial Logistic-Normal Dynamic Linear Models (MLN-DLMs) as a flexible approach to modeling these data. However, adoption of these methods has been limited by computational challenges. This article develops an efficient and accurate approach to posterior state estimation, called $\textit{Fenrir}$. Our approach relies on a novel algorithm for MAP estimation and an accurate approximation to a key posterior marginal of the model. As there are no equivalent methods against which we can compare, we also develop an optimized Stan implementation of MLN-DLMs. Our experiments suggest that Fenrir can be three orders of magnitude more efficient than Stan and can even be incorporated into larger sampling schemes for joint inference of model hyperparameters. Our methods are made available to the community as a user-friendly software library written in C++ with an R interface.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05548v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manan Saxena, Tinghua Chen, Justin D. Silverman</dc:creator>
    </item>
    <item>
      <title>With random regressors, least squares inference is robust to correlated errors with unknown correlation structure</title>
      <link>https://arxiv.org/abs/2410.05567</link>
      <description>arXiv:2410.05567v1 Announce Type: cross 
Abstract: Linear regression is arguably the most widely used statistical method. With fixed regressors and correlated errors, the conventional wisdom is to modify the variance-covariance estimator to accommodate the known correlation structure of the errors. We depart from the literature by showing that with random regressors, linear regression inference is robust to correlated errors with unknown correlation structure. The existing theoretical analyses for linear regression are no longer valid because even the asymptotic normality of the least-squares coefficients breaks down in this regime. We first prove the asymptotic normality of the t statistics by establishing their Berry-Esseen bounds based on a novel probabilistic analysis of self-normalized statistics. We then study the local power of the corresponding t tests and show that, perhaps surprisingly, error correlation can even enhance power in the regime of weak signals. Overall, our results show that linear regression is applicable more broadly than the conventional theory suggests, and further demonstrate the value of randomization to ensure robustness of inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05567v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zifeng Zhang, Peng Ding, Wen Zhou, Haonan Wang</dc:creator>
    </item>
    <item>
      <title>Diversity and Inclusion Index with Networks and Similarity: Analysis and its Application</title>
      <link>https://arxiv.org/abs/2410.05668</link>
      <description>arXiv:2410.05668v1 Announce Type: cross 
Abstract: In recent years, the concepts of ``diversity'' and ``inclusion'' have attracted considerable attention across a range of fields, encompassing both social and biological disciplines. To fully understand these concepts, it is critical to not only examine the number of categories but also the similarities and relationships among them. In this study, I introduce a novel index for diversity and inclusion that considers similarities and network connections. I analyzed the properties of these indices and investigated their mathematical relationships using established measures of diversity and networks. Moreover, I developed a methodology for estimating similarities based on the utility of diversity. I also created a method for visualizing proportions, similarities, and network connections. Finally, I evaluated the correlation with external metrics using real-world data, confirming that both the proposed indices and our index can be effectively utilized. This study contributes to a more nuanced understanding of diversity and inclusion analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05668v1</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keita Kinjo</dc:creator>
    </item>
    <item>
      <title>Pathwise Gradient Variance Reduction with Control Variates in Variational Inference</title>
      <link>https://arxiv.org/abs/2410.05753</link>
      <description>arXiv:2410.05753v1 Announce Type: cross 
Abstract: Variational inference in Bayesian deep learning often involves computing the gradient of an expectation that lacks a closed-form solution. In these cases, pathwise and score-function gradient estimators are the most common approaches. The pathwise estimator is often favoured for its substantially lower variance compared to the score-function estimator, which typically requires variance reduction techniques. However, recent research suggests that even pathwise gradient estimators could benefit from variance reduction. In this work, we review existing control-variates-based variance reduction methods for pathwise gradient estimators to assess their effectiveness. Notably, these methods often rely on integrand approximations and are applicable only to simple variational families. To address this limitation, we propose applying zero-variance control variates to pathwise gradient estimators. This approach offers the advantage of requiring minimal assumptions about the variational distribution, other than being able to sample from it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05753v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenyon Ng, Susan Wei</dc:creator>
    </item>
    <item>
      <title>Temperature Optimization for Bayesian Deep Learning</title>
      <link>https://arxiv.org/abs/2410.05757</link>
      <description>arXiv:2410.05757v1 Announce Type: cross 
Abstract: The Cold Posterior Effect (CPE) is a phenomenon in Bayesian Deep Learning (BDL), where tempering the posterior to a cold temperature often improves the predictive performance of the posterior predictive distribution (PPD). Although the term `CPE' suggests colder temperatures are inherently better, the BDL community increasingly recognizes that this is not always the case. Despite this, there remains no systematic method for finding the optimal temperature beyond grid search. In this work, we propose a data-driven approach to select the temperature that maximizes test log-predictive density, treating the temperature as a model parameter and estimating it directly from the data. We empirically demonstrate that our method performs comparably to grid search, at a fraction of the cost, across both regression and classification tasks. Finally, we highlight the differing perspectives on CPE between the BDL and Generalized Bayes communities: while the former primarily focuses on predictive performance of the PPD, the latter emphasizes calibrated uncertainty and robustness to model misspecification; these distinct objectives lead to different temperature preferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05757v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenyon Ng, Chris van der Heide, Liam Hodgkinson, Susan Wei</dc:creator>
    </item>
    <item>
      <title>A time warping model for seasonal data with application to age estimation from narwhal tusks</title>
      <link>https://arxiv.org/abs/2410.05843</link>
      <description>arXiv:2410.05843v1 Announce Type: cross 
Abstract: Signals with varying periodicity frequently appear in real-world phenomena, necessitating the development of efficient modelling techniques to map the measured nonlinear timeline to linear time. Here we propose a regression model that allows for a representation of periodic and dynamic patterns observed in time series data. The model incorporates a hidden strictly increasing stochastic process that represents the instantaneous frequency, allowing the model to adapt and accurately capture varying time scales. A case study focusing on age estimation of narwhal tusks is presented, where cyclic element signals associated with annual growth layer groups are analyzed. We apply the methodology to data from one such tusk collected in West Greenland and use the fitted model to estimate the age of the narwhal. The proposed method is validated using simulated signals with known cycle counts and practical considerations and modelling challenges are discussed in detail. This research contributes to the field of time series analysis, providing a tool and valuable insights for understanding and modeling complex cyclic patterns in diverse domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05843v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars Reiter Nielsen, Mads Peter Heide-J{\o}rgensen, Eva Garde, Adeline Samson, Susanne Ditlevsen</dc:creator>
    </item>
    <item>
      <title>Likelihood-based Differentiable Structure Learning</title>
      <link>https://arxiv.org/abs/2410.06163</link>
      <description>arXiv:2410.06163v1 Announce Type: cross 
Abstract: Existing approaches to differentiable structure learning of directed acyclic graphs (DAGs) rely on strong identifiability assumptions in order to guarantee that global minimizers of the acyclicity-constrained optimization problem identifies the true DAG. Moreover, it has been observed empirically that the optimizer may exploit undesirable artifacts in the loss function. We explain and remedy these issues by studying the behavior of differentiable acyclicity-constrained programs under general likelihoods with multiple global minimizers. By carefully regularizing the likelihood, it is possible to identify the sparsest model in the Markov equivalence class, even in the absence of an identifiable parametrization. We first study the Gaussian case in detail, showing how proper regularization of the likelihood defines a score that identifies the sparsest model. Assuming faithfulness, it also recovers the Markov equivalence class. These results are then generalized to general models and likelihoods, where the same claims hold. These theoretical results are validated empirically, showing how this can be done using standard gradient-based optimizers, thus paving the way for differentiable structure learning under general models and losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06163v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chang Deng, Kevin Bello, Pradeep Ravikumar, Bryon Aragam</dc:creator>
    </item>
    <item>
      <title>Robust Domain Generalisation with Causal Invariant Bayesian Neural Networks</title>
      <link>https://arxiv.org/abs/2410.06349</link>
      <description>arXiv:2410.06349v1 Announce Type: cross 
Abstract: Deep neural networks can obtain impressive performance on various tasks under the assumption that their training domain is identical to their target domain. Performance can drop dramatically when this assumption does not hold. One explanation for this discrepancy is the presence of spurious domain-specific correlations in the training data that the network exploits. Causal mechanisms, in the other hand, can be made invariant under distribution changes as they allow disentangling the factors of distribution underlying the data generation. Yet, learning causal mechanisms to improve out-of-distribution generalisation remains an under-explored area. We propose a Bayesian neural architecture that disentangles the learning of the the data distribution from the inference process mechanisms. We show theoretically and experimentally that our model approximates reasoning under causal interventions. We demonstrate the performance of our method, outperforming point estimate-counterparts, on out-of-distribution image recognition tasks where the data distribution acts as strong adversarial confounders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06349v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ga\"el Gendron, Michael Witbrock, Gillian Dobbie</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Low-Rank Tensors: Heteroskedasticity, Subgaussianity, and Applications</title>
      <link>https://arxiv.org/abs/2410.06381</link>
      <description>arXiv:2410.06381v1 Announce Type: cross 
Abstract: In this paper, we consider inference and uncertainty quantification for low Tucker rank tensors with additive noise in the high-dimensional regime. Focusing on the output of the higher-order orthogonal iteration (HOOI) algorithm, a commonly used algorithm for tensor singular value decomposition, we establish non-asymptotic distributional theory and study how to construct confidence regions and intervals for both the estimated singular vectors and the tensor entries in the presence of heteroskedastic subgaussian noise, which are further shown to be optimal for homoskedastic Gaussian noise. Furthermore, as a byproduct of our theoretical results, we establish the entrywise convergence of HOOI when initialized via diagonal deletion. To further illustrate the utility of our theoretical results, we then consider several concrete statistical inference tasks. First, in the tensor mixed-membership blockmodel, we consider a two-sample test for equality of membership profiles, and we propose a test statistic with consistency under local alternatives that exhibits a power improvement relative to the corresponding matrix test considered in several previous works. Next, we consider simultaneous inference for small collections of entries of the tensor, and we obtain consistent confidence regions. Finally, focusing on the particular case of testing whether entries of the tensor are equal, we propose a consistent test statistic that shows how index overlap results in different asymptotic standard deviations. All of our proposed procedures are fully data-driven, adaptive to noise distribution and signal strength, and do not rely on sample-splitting, and our main results highlight the effect of higher-order structures on estimation relative to the matrix setting. Our theoretical results are demonstrated through numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06381v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Agterberg, Anru Zhang</dc:creator>
    </item>
    <item>
      <title>A Skewness-Based Criterion for Addressing Heteroscedastic Noise in Causal Discovery</title>
      <link>https://arxiv.org/abs/2410.06407</link>
      <description>arXiv:2410.06407v1 Announce Type: cross 
Abstract: Real-world data often violates the equal-variance assumption (homoscedasticity), making it essential to account for heteroscedastic noise in causal discovery. In this work, we explore heteroscedastic symmetric noise models (HSNMs), where the effect $Y$ is modeled as $Y = f(X) + \sigma(X)N$, with $X$ as the cause and $N$ as independent noise following a symmetric distribution. We introduce a novel criterion for identifying HSNMs based on the skewness of the score (i.e., the gradient of the log density) of the data distribution. This criterion establishes a computationally tractable measurement that is zero in the causal direction but nonzero in the anticausal direction, enabling the causal direction discovery. We extend this skewness-based criterion to the multivariate setting and propose SkewScore, an algorithm that handles heteroscedastic noise without requiring the extraction of exogenous noise. We also conduct a case study on the robustness of SkewScore in a bivariate model with a latent confounder, providing theoretical insights into its performance. Empirical studies further validate the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06407v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingyu Lin, Yuxing Huang, Wenqin Liu, Haoran Deng, Ignavier Ng, Kun Zhang, Mingming Gong, Yi-An Ma, Biwei Huang</dc:creator>
    </item>
    <item>
      <title>Retrieved dropout imputation considering administrative study withdrawal</title>
      <link>https://arxiv.org/abs/2410.06774</link>
      <description>arXiv:2410.06774v1 Announce Type: cross 
Abstract: The International Council for Harmonisation of Technical Requirements for Pharmaceuticals for Human Use (ICH) E9 (R1) Addendum provides a framework for defining estimands in clinical trials. Treatment policy strategy is the mostly used approach to handle intercurrent events in defining estimands. Imputing missing values for potential outcomes under the treatment policy strategy has been discussed in the literature. Missing values as a result of administrative study withdrawals (such as site closures due to business reasons, COVID-19 control measures, and geopolitical conflicts, etc.) are often imputed in the same way as other missing values occurring after intercurrent events related to safety or efficacy. Some research suggests using a hypothetical strategy to handle the treatment discontinuations due to administrative study withdrawal in defining the estimands and imputing the missing values based on completer data assuming missing at random, but this approach ignores the fact that subjects might experience other intercurrent events had they not had the administrative study withdrawal. In this article, we consider the administrative study withdrawal censors the normal real-world like intercurrent events and propose two methods for handling the corresponding missing values under the retrieved dropout imputation framework. Simulation shows the two methods perform well. We also applied the methods to actual clinical trial data evaluating an anti-diabetes treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06774v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rong Liu, Yongming Qu</dc:creator>
    </item>
    <item>
      <title>Group Shapley Value and Counterfactual Simulations in a Structural Model</title>
      <link>https://arxiv.org/abs/2410.06875</link>
      <description>arXiv:2410.06875v1 Announce Type: cross 
Abstract: We propose a variant of the Shapley value, the group Shapley value, to interpret counterfactual simulations in structural economic models by quantifying the importance of different components. Our framework compares two sets of parameters, partitioned into multiple groups, and applying group Shapley value decomposition yields unique additive contributions to the changes between these sets. The relative contributions sum to one, enabling us to generate an importance table that is as easily interpretable as a regression table. The group Shapley value can be characterized as the solution to a constrained weighted least squares problem. Using this property, we develop robust decomposition methods to address scenarios where inputs for the group Shapley value are missing. We first apply our methodology to a simple Roy model and then illustrate its usefulness by revisiting two published papers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06875v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongchan Kwon, Sokbae Lee, Guillaume A. Pouliot</dc:creator>
    </item>
    <item>
      <title>Resistant convex clustering: How does the fusion penalty enhance resistantance?</title>
      <link>https://arxiv.org/abs/1906.09581</link>
      <description>arXiv:1906.09581v3 Announce Type: replace 
Abstract: Convex clustering is a convex relaxation of the $k$-means and hierarchical clustering. It involves solving a convex optimization problem with the objective function being a squared error loss plus a fusion penalty that encourages the estimated centroids for observations in the same cluster to be identical. However, when data are contaminated, convex clustering with a squared error loss fails even when there is only one arbitrary outlier. To address this challenge, we propose a resistant convex clustering method. Theoretically, we show that the new estimator is resistant to arbitrary outliers: it does not break down until more than half of the observations are arbitrary outliers. Perhaps surprisingly, the fusion penalty can help enhance resistance by fusing the estimators to the cluster centers of uncontaminated samples, but not the other way around. Numerical studies demonstrate the competitive performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:1906.09581v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiang Sun, Archer Gong Zhang, Chenyu Liu, Kean Ming Tan</dc:creator>
    </item>
    <item>
      <title>Testing for the Network Small-World Property</title>
      <link>https://arxiv.org/abs/2103.08035</link>
      <description>arXiv:2103.08035v2 Announce Type: replace 
Abstract: Researchers have long observed that the ``small-world" property, which combines the concepts of high transitivity or clustering with a low average path length, is ubiquitous for networks obtained from a variety of disciplines, including social sciences, biology, neuroscience, and ecology. However, we find several shortcomings of the currently prevalent definition and detection methods rendering the concept less powerful. First, the widely used \textit{small world coefficient} metric combines high transitivity with a low average path length in a single measure that confounds the two separate aspects. We find that the value of the metric is dominated by transitivity, and in several cases, networks get flagged as ``small world" solely because of their high transitivity. Second, the detection methods lack a formal statistical inference. Third, the comparison is typically performed against simplistic random graph models as the baseline, ignoring well-known network characteristics and risks confounding the small world property with other network properties. We decouple the properties of high transitivity and low average path length as separate events to test for. Then we define the property as a statistical test between a suitable null hypothesis and a superimposed alternative hypothesis. We propose a parametric bootstrap test with several null hypothesis models to allow a wide range of background structures in the network. In addition to the bootstrap tests, we also propose an asymptotic test under the Erd\"{o}s-Ren\'{y}i null model for which we provide theoretical guarantees on the asymptotic level and power. Our theoretical results include asymptotic distributions of clustering coefficient for various asymptotic growth rates on the probability of an edge. Applying the proposed methods to a large number of network datasets, we uncover new insights about their small-world property.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.08035v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kartik Lovekar, Srijan Sengupta, Subhadeep Paul</dc:creator>
    </item>
    <item>
      <title>Learning from a Biased Sample</title>
      <link>https://arxiv.org/abs/2209.01754</link>
      <description>arXiv:2209.01754v3 Announce Type: replace 
Abstract: The empirical risk minimization approach to data-driven decision making requires access to training data drawn under the same conditions as those that will be faced when the decision rule is deployed. However, in a number of settings, we may be concerned that our training sample is biased in the sense that some groups (characterized by either observable or unobservable attributes) may be under- or over-represented relative to the general population; and in this setting empirical risk minimization over the training set may fail to yield rules that perform well at deployment. We propose a model of sampling bias called conditional $\Gamma$-biased sampling, where observed covariates can affect the probability of sample selection arbitrarily much but the amount of unexplained variation in the probability of sample selection is bounded by a constant factor. Applying the distributionally robust optimization framework, we propose a method for learning a decision rule that minimizes the worst-case risk incurred under a family of test distributions that can generate the training distribution under $\Gamma$-biased sampling. We apply a result of Rockafellar and Uryasev to show that this problem is equivalent to an augmented convex risk minimization problem. We give statistical guarantees for learning a model that is robust to sampling bias via the method of sieves, and propose a deep learning algorithm whose loss function captures our robust learning target. We empirically validate our proposed method in a case study on prediction of mental health scores from health survey data and a case study on ICU length of stay prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.01754v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roshni Sahoo, Lihua Lei, Stefan Wager</dc:creator>
    </item>
    <item>
      <title>Inferring unknown unknowns: Regularized bias-aware ensemble Kalman filter</title>
      <link>https://arxiv.org/abs/2306.04315</link>
      <description>arXiv:2306.04315v3 Announce Type: replace 
Abstract: Because of physical assumptions and numerical approximations, low-order models are affected by uncertainties in the state and parameters, and by model biases. Model biases, also known as model errors or systematic errors, are difficult to infer because they are `unknown unknowns', i.e., we do not necessarily know their functional form a priori. With biased models, data assimilation methods may be ill-posed because either (i) they are 'bias-unaware' because the estimators are assumed unbiased, (ii) they rely on an a priori parametric model for the bias, or (iii) they can infer model biases that are not unique for the same model and data. First, we design a data assimilation framework to perform combined state, parameter, and bias estimation. Second, we propose a mathematical solution with a sequential method, i.e., the regularized bias-aware ensemble Kalman Filter (r-EnKF), which requires a model of the bias and its gradient (i.e., the Jacobian). Third, we propose an echo state network as the model bias estimator. We derive the Jacobian of the network, and design a robust training strategy with data augmentation to accurately infer the bias in different scenarios. Fourth, we apply the r-EnKF to nonlinearly coupled oscillators (with and without time-delay) affected by different forms of bias. The r-EnKF infers in real-time parameters and states, and a unique bias. The applications that we showcase are relevant to acoustics, thermoacoustics, and vibrations; however, the r-EnKF opens new opportunities for combined state, parameter and bias estimation for real-time and on-the-fly prediction in nonlinear systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.04315v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cma.2023.116502</arxiv:DOI>
      <dc:creator>Andrea N\'ovoa, Alberto Racca, Luca Magri</dc:creator>
    </item>
    <item>
      <title>Separable pathway effects of semi-competing risks using multi-state models</title>
      <link>https://arxiv.org/abs/2306.15947</link>
      <description>arXiv:2306.15947v3 Announce Type: replace 
Abstract: Semi-competing risks refer to the phenomenon where a primary event (such as mortality) can ``censor'' an intermediate event (such as relapse of a disease), but not vice versa. Under the multi-state model, the primary event consists of two specific types: the direct outcome event and an indirect outcome event developed from intermediate events. Within this framework, we show that the total treatment effect on the cumulative incidence of the primary event can be decomposed into three separable pathway effects, capturing treatment effects on population-level transition rates between states. We next propose two estimators for the counterfactual cumulative incidences of the primary event under hypothetical treatment components. One estimator is given by the generalized Nelson--Aalen estimator with inverse probability weighting under covariates isolation, and the other is given based on the efficient influence function. The asymptotic normality of these estimators is established. The first estimator only involves a propensity score model and avoid modeling the cause-specific hazards. The second estimator has robustness against the misspecification of submodels. As an illustration of its potential usefulness, the proposed method is applied to compare effects of different allogeneic stem cell transplantation types on overall survival after transplantation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15947v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhao Deng, Yi Wang, Xiang Zhan, Xiao-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>Utilizing a Capture-Recapture Strategy to Accelerate Infectious Disease Surveillance</title>
      <link>https://arxiv.org/abs/2307.00214</link>
      <description>arXiv:2307.00214v2 Announce Type: replace 
Abstract: Monitoring key elements of disease dynamics (e.g., prevalence, case counts) is of great importance in infectious disease prevention and control, as emphasized during the COVID-19 pandemic. To facilitate this effort, we propose a new capture-recapture (CRC) analysis strategy that takes misclassification into account from easily-administered, imperfect diagnostic test kits, such as the Rapid Antigen Test-kits or saliva tests. Our method is based on a recently proposed "anchor stream" design, whereby an existing voluntary surveillance data stream is augmented by a smaller and judiciously drawn random sample. It incorporates manufacturer-specified sensitivity and specificity parameters to account for imperfect diagnostic results in one or both data streams. For inference to accompany case count estimation, we improve upon traditional Wald-type confidence intervals by developing an adapted Bayesian credible interval for the CRC estimator that yields favorable frequentist coverage properties. When feasible, the proposed design and analytic strategy provides a more efficient solution than traditional CRC methods or random sampling-based biased-corrected estimation to monitor disease prevalence while accounting for misclassification. We demonstrate the benefits of this approach through simulation studies that underscore its potential utility in practice for economical disease monitoring among a registered closed population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.00214v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Ge, Yuzi Zhang, Lance A. Waller, Robert H. Lyles</dc:creator>
    </item>
    <item>
      <title>Outlier-Robust Neural Network Training: Efficient Optimization of Transformed Trimmed Loss with Variation Regularization</title>
      <link>https://arxiv.org/abs/2308.02293</link>
      <description>arXiv:2308.02293v3 Announce Type: replace 
Abstract: In this study, we consider outlier-robust predictive modeling using highly-expressive neural networks. To this end, we employ (1) a transformed trimmed loss (TTL), which is a computationally feasible variant of the classical trimmed loss, and (2) a higher-order variation regularization (HOVR) of the prediction model. Note that using only TTL to train the neural network may possess outlier vulnerability, as its high expressive power causes it to overfit even the outliers perfectly. However, simultaneously introducing HOVR constrains the effective degrees of freedom, thereby avoiding fitting outliers. We newly provide an efficient stochastic algorithm for optimization and its theoretical convergence guarantee. (*Two authors contributed equally to this work.)</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.02293v3</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akifumi Okuno, Shotaro Yagishita</dc:creator>
    </item>
    <item>
      <title>Harmonized Estimation of Subgroup-Specific Treatment Effects in Randomized Trials: The Use of External Control Data</title>
      <link>https://arxiv.org/abs/2308.05073</link>
      <description>arXiv:2308.05073v2 Announce Type: replace 
Abstract: Subgroup analyses of randomized controlled trials (RCTs) constitute an important component of the drug development process in precision medicine. In particular, subgroup analyses of early-stage trials often influence the design and eligibility criteria of subsequent confirmatory trials and ultimately influence which subpopulations will receive the treatment after regulatory approval. However, subgroup analyses are often complicated by small sample sizes, which leads to substantial uncertainty about subgroup-specific treatment effects. We explore the use of external control (EC) data to augment RCT subgroup analyses. We define and discuss it harmonized estimators of subpopulation-specific treatment effects that leverage EC data. Our approach can be used to modify any subgroup-specific treatment effect estimates that are obtained by combining RCT and EC data, such as linear regression. We alter these subgroup-specific estimates to make them coherent with a robust estimate of the average effect in the randomized population based only on RCT data. The weighted average of the resulting subgroup-specific harmonized estimates matches the RCT-only estimate of the overall effect in the randomized population. We discuss the proposed harmonized estimators through analytic results and simulations, and investigate standard performance metrics. The method is illustrated with a case study in oncology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05073v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Schwartz, Riddhiman Saha, Steffen Ventz, Lorenzo Trippa</dc:creator>
    </item>
    <item>
      <title>Causal progress with imperfect placebo treatments and outcomes</title>
      <link>https://arxiv.org/abs/2310.15266</link>
      <description>arXiv:2310.15266v2 Announce Type: replace 
Abstract: In the quest to make defensible causal claims from observational data, it is sometimes possible to leverage information from "placebo treatments" and "placebo outcomes". Existing approaches employing such information focus largely on point identification and assume (i) "perfect placebos", meaning placebo treatments have precisely zero effect on the outcome and the real treatment has precisely zero effect on a placebo outcome; and (ii) "equiconfounding", meaning that the treatment-outcome relationship where one is a placebo suffers the same amount of confounding as does the real treatment-outcome relationship, on some scale. We instead consider an omitted variable bias framework, in which users can postulate ranges of values for the degree of unequal confounding and the degree of placebo imperfection. Once postulated, these assumptions identify or bound the linear estimates of treatment effects. Our approach also does not require using both a placebo treatment and placebo outcome, as some others do. While applicable in many settings, one ubiquitous use-case for this approach is to employ pre-treatment outcomes as (perfect) placebo outcomes, as in difference-in-difference. The parallel trends assumption in this setting is identical to the equiconfounding assumption, on a particular scale, which our framework allows the user to relax. Finally, we demonstrate the use of our framework with two applications and a simulation, employing an R package that implements these approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.15266v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Rohde, Chad Hazlett</dc:creator>
    </item>
    <item>
      <title>FedECA: A Federated External Control Arm Method for Causal Inference with Time-To-Event Data in Distributed Settings</title>
      <link>https://arxiv.org/abs/2311.16984</link>
      <description>arXiv:2311.16984v3 Announce Type: replace 
Abstract: External control arms (ECA) can inform the early clinical development of experimental drugs and provide efficacy evidence for regulatory approval. However, the main challenge in implementing ECA lies in accessing real-world or historical clinical trials data. Indeed, regulations protecting patients' rights by strictly controlling data processing make pooling data from multiple sources in a central server often difficult. To address these limitations, we develop a new method, 'FedECA' that leverages federated learning (FL) to enable inverse probability of treatment weighting (IPTW) for time-to-event outcomes on separate cohorts without needing to pool data. To showcase the potential of FedECA, we apply it in different settings of increasing complexity culminating with a real-world use-case in which FedECA provides evidence for a differential effect between two drugs that would have otherwise go unnoticed. By sharing our code, we hope FedECA will foster the creation of federated research networks and thus accelerate drug development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16984v3</guid>
      <category>stat.ME</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean Ogier du Terrail, Quentin Klopfenstein, Honghao Li, Imke Mayer, Nicolas Loiseau, Mohammad Hallal, Michael Debouver, Thibault Camalon, Thibault Fouqueray, Jorge Arellano Castro, Zahia Yanes, Laetitia Dahan, Julien Ta\"ieb, Pierre Laurent-Puig, Jean-Baptiste Bachet, Shulin Zhao, Remy Nicolle, J\'erome Cros, Daniel Gonzalez, Robert Carreras-Torres, Adelaida Garcia Velasco, Kawther Abdilleh, Sudheer Doss, F\'elix Balazard, Mathieu Andreux</dc:creator>
    </item>
    <item>
      <title>A Multivariate Polya Tree Model for Meta-Analysis with Event Time Distributions</title>
      <link>https://arxiv.org/abs/2312.06018</link>
      <description>arXiv:2312.06018v2 Announce Type: replace 
Abstract: We develop a non-parametric Bayesian prior for a family of random probability measures by extending the Polya tree ($PT$) prior to a joint prior for a set of probability measures $G_1,\dots,G_n$, suitable for meta-analysis with event time outcomes. In the application to meta-analysis $G_i$ is the event time distribution specific to study $i$. The proposed model defines a regression on study-specific covariates by introducing increased correlation for any pair of studies with similar characteristics. The desired multivariate $PT$ model is constructed by introducing a hierarchical prior on the conditional splitting probabilities in the $PT$ construction for each of the $G_i$. The hierarchical prior replaces the independent beta priors for the splitting probability in the $PT$ construction with a Gaussian process prior for corresponding (logit) splitting probabilities across all studies. The Gaussian process is indexed by study-specific covariates, introducing the desired dependence with increased correlation for similar studies. The main feature of the proposed construction is (conditionally) conjugate posterior updating with commonly reported inference summaries for event time data. The construction is motivated by a meta-analysis over cancer immunotherapy studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.06018v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giovanni Poli, Elena Fountzilas, Apostolia-Maria Tsimeridou, Peter M\"uller</dc:creator>
    </item>
    <item>
      <title>On the error control of invariant causal prediction</title>
      <link>https://arxiv.org/abs/2401.03834</link>
      <description>arXiv:2401.03834v3 Announce Type: replace 
Abstract: Invariant causal prediction (ICP, Peters et al. (2016)) provides a novel way for identifying causal predictors of a response by utilizing heterogeneous data from different environments. One notable advantage of ICP is that it guarantees to make no false causal discoveries with high probability. Such a guarantee, however, can be overly conservative in some applications, resulting in few or no causal discoveries. This raises a natural question: Can we use less conservative error control guarantees for ICP so that more causal information can be extracted from data? We address this question in the paper. We focus on two commonly used and more liberal guarantees: false discovery rate control and simultaneous true discovery bound. Unexpectedly, we find that false discovery rate does not seem to be a suitable error criterion for ICP. The simultaneous true discovery bound, on the other hand, proves to be an ideal choice, enabling users to explore potential causal predictors and extract more causal information. Importantly, the additional information comes for free, in the sense that no extra assumptions are required and the discoveries from the original ICP approach are fully retained. We demonstrate the practical utility of our method through simulations and a real dataset about the educational attainment of teenagers in the US.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03834v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinzhou Li, Jelle J Goeman</dc:creator>
    </item>
    <item>
      <title>Generalized Universal Inference on Risk Minimizers</title>
      <link>https://arxiv.org/abs/2402.00202</link>
      <description>arXiv:2402.00202v2 Announce Type: replace 
Abstract: A common goal in statistics and machine learning is estimation of unknowns. Point estimates alone are of little value without an accompanying measure of uncertainty, but traditional uncertainty quantification methods, such as confidence sets and p-values, often require strong distributional or structural assumptions that may not be justified in modern problems. The present paper considers a very common case in machine learning, where the quantity of interest is the minimizer of a given risk (expected loss) function. For such cases, we propose a generalization of the recently developed universal inference procedure that is designed for inference on risk minimizers. Notably, our generalized universal inference attains finite-sample frequentist validity guarantees under a condition common in the statistical learning literature. One version of our procedure is also anytime-valid in the sense that it maintains the finite-sample validity properties regardless of the stopping rule used for the data collection process, thereby providing a link between safe inference and fast convergence rates in statistical learning. Practical use of our proposal requires tuning, and we offer a data-driven procedure with strong empirical performance across a broad range of challenging statistical and machine learning examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00202v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neil Dey, Ryan Martin, Jonathan P. Williams</dc:creator>
    </item>
    <item>
      <title>Effects of model misspecification on small area estimators</title>
      <link>https://arxiv.org/abs/2403.11276</link>
      <description>arXiv:2403.11276v2 Announce Type: replace 
Abstract: Nested error regression models are commonly used to incorporate observational unit specific auxiliary variables to improve small area estimates. When the mean structure of this model is misspecified, there is generally an increase in the mean square prediction error (MSPE) of Empirical Best Linear Unbiased Predictors (EBLUP). Observed Best Prediction (OBP) method has been proposed with the intent to improve on the MSPE over EBLUP. We conduct a Monte Carlo simulation experiment to understand the effect of mispsecification of mean structures on different small area estimators. Our simulation results lead to an unexpected result that OBP may perform very poorly when observational unit level auxiliary variables are used and that OBP can be improved significantly when population means of those auxiliary variables (area level auxiliary variables) are used in the nested error regression model or when a corresponding area level model is used. Our simulation also indicates that the MSPE of OBP in an increasing function of the difference between the sample and population means of the auxiliary variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11276v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuting Chen, Partha Lahiri, Nicola Salvati</dc:creator>
    </item>
    <item>
      <title>A Marginal Maximum Likelihood Approach for Hierarchical Simultaneous Autoregressive Models with Missing Data</title>
      <link>https://arxiv.org/abs/2403.17257</link>
      <description>arXiv:2403.17257v2 Announce Type: replace 
Abstract: Efficient estimation methods for simultaneous autoregressive (SAR) models with missing data in the response variable have been well-explored in the literature. A common practice is to introduce measurement error into SAR models to separate the noise component from the spatial process. However, prior research has not considered incorporating measurement error into SAR models with missing data. Maximum likelihood estimation for such models, especially with large datasets, poses significant computational challenges. This paper proposes an efficient likelihood-based estimation method, the marginal maximum likelihood (ML), for estimating SAR models on large datasets with measurement errors and a high percentage of missing data in the response variable. The spatial error model (SEM) and the spatial autoregressive model (SAM), two popular SAR model types, are considered. The missing data mechanism is assumed to follow a missing at random (MAR) pattern. We propose a fast method for marginal ML estimation with a computational complexity of $O(n^{3/2})$, where $n$ is the total number of observations. This complexity applies when the spatial weight matrix is constructed based on a local neighbourhood structure. The effectiveness of the proposed methods is demonstrated through simulations and real-world data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.17257v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anjana Wijayawardhana, Thomas Suesse, David Gunawan</dc:creator>
    </item>
    <item>
      <title>Fast Bayesian Basis Selection for Functional Data Representation with Correlated Errors</title>
      <link>https://arxiv.org/abs/2405.20758</link>
      <description>arXiv:2405.20758v2 Announce Type: replace 
Abstract: Functional data analysis finds widespread application across various fields. While functional data are intrinsically infinite-dimensional, in practice, they are observed only at a finite set of points, typically over a dense grid. As a result, smoothing techniques are often used to approximate the observed data as functions. In this work, we propose a novel Bayesian approach for selecting basis functions for smoothing one or multiple curves simultaneously. Our method differentiates from other Bayesian approaches in two key ways: (i) by accounting for correlated errors and (ii) by developing a variational EM algorithm, which is faster than MCMC methods such as Gibbs sampling. Simulation studies demonstrate that our method effectively identifies the true underlying structure of the data across various scenarios and it is applicable to different types of functional data. Our variational EM algorithm not only recovers the basis coefficients and the correct set of basis functions but also estimates the existing within-curve correlation. When applied to the motorcycle and temperature datasets, our method demonstrates comparable, and in some cases superior, performance in terms of adjusted $R^2$ compared to regression splines, smoothing splines, Bayesian LASSO and LASSO. Our proposed method is implemented in R and codes are available at https://github.com/acarolcruz/VB-Bases-Selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20758v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ana Carolina da Cruz, Camila P. E. de Souza, Pedro H. T. O. Sousa</dc:creator>
    </item>
    <item>
      <title>Statistical inference of convex order by Wasserstein projection</title>
      <link>https://arxiv.org/abs/2406.02840</link>
      <description>arXiv:2406.02840v2 Announce Type: replace 
Abstract: Ranking distributions according to a stochastic order has wide applications in diverse areas. Although stochastic dominance has received much attention, convex order, particularly in general dimensions, has yet to be investigated from a statistical point of view. This article addresses this gap by introducing a simple statistical test for convex order based on the Wasserstein projection distance. This projection distance not only encodes whether two distributions are indeed in convex order, but also quantifies the deviation from the desired convex order and produces an optimal convex order approximation. Lipschitz stability of the backward and forward Wasserstein projection distance is proved, which leads to elegant consistency and concentration results of the estimator we employ as our test statistic. Combining these with state of the art results regarding the convergence rate of empirical distributions, we also derive upper bounds for the $p$-value and type I error of our test statistic, as well as upper bounds on the type II error for an appropriate class of strict alternatives. With proper choices of families of distributions, we further attain that the power of the proposed test increases to one as the number of samples grows to infinity. Lastly, we provide an efficient numerical scheme for our test statistic, by way of an entropic Frank-Wolfe algorithm. Experiments based on synthetic data sets illuminate the success of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02840v2</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakwang Kim, Young-Heon Kim, Yuanlong Ruan, Andrew Warren</dc:creator>
    </item>
    <item>
      <title>Selecting the Number of Communities for Weighted Degree-Corrected Stochastic Block Models</title>
      <link>https://arxiv.org/abs/2406.05340</link>
      <description>arXiv:2406.05340v2 Announce Type: replace 
Abstract: We investigate how to select the number of communities for weighted networks without a full likelihood modeling. First, we propose a novel weighted degree-corrected stochastic block model (DCSBM), in which the mean adjacency matrix is modeled as the same as in standard DCSBM, while the variance profile matrix is assumed to be related to the mean adjacency matrix through a given variance function. Our method of selecting the number of communities is based on a sequential testing framework, and in each step the weighted DCSBM is fitted via some spectral clustering method. A key step is to carry out matrix scaling on the estimated variance profile matrix. The resulting scaling factors can be used to normalize the adjacency matrix, from which the testing statistic is obtained. Under mild conditions on the weighted DCSBM, our proposed procedure is shown to be consistent in estimating the true number of communities. Numerical experiments on both simulated and real-world network data also demonstrate the desirable empirical properties of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05340v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yucheng Liu, Xiaodong Li</dc:creator>
    </item>
    <item>
      <title>Local Effects of Continuous Instruments without Positivity</title>
      <link>https://arxiv.org/abs/2409.07350</link>
      <description>arXiv:2409.07350v2 Announce Type: replace 
Abstract: Instrumental variables are a popular study design for the estimation of treatment effects in the presence of unobserved confounders. In the canonical instrumental variables design, the instrument is a binary variable. In many settings, however, the instrument is continuous. Standard estimation methods can be applied with continuous instruments, but they require strong assumptions. While recent work has introduced more flexible estimation approaches, these methods require a positivity assumption that is implausible in many applications. We derive a novel family of causal estimands using stochastic dynamic interventions that allows a range of intervention distributions that are continuous with respect to the observed distribution of the instrument. These estimands focus on a specific local effect but do not require a positivity assumption. Next, we develop doubly robust estimators for these estimands that allow for estimation of the nuisance functions via nonparametric estimators. We use empirical process theory and sample splitting to derive asymptotic properties of the proposed estimators under weak conditions. In addition, we derive methods for profiling the principal strata as well as a method of sensitivity analysis. We evaluate our methods via simulation and demonstrate their feasibility using an application on the effectiveness of surgery for specific emergency conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07350v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Prabrisha Rakshit, Alexander Levis, Luke Keele</dc:creator>
    </item>
    <item>
      <title>Adaptive radar detection of subspace-based distributed target in power heterogeneous clutter</title>
      <link>https://arxiv.org/abs/2409.14049</link>
      <description>arXiv:2409.14049v2 Announce Type: replace 
Abstract: This paper investigates the problem of adaptive detection of distributed targets in power heterogeneous clutter. In the considered scenario, all the data share the identical structure of clutter covariance matrix, but with varying and unknown power mismatches. To address this problem, we iteratively estimate all the unknowns, including the coordinate matrix of the target, the clutter covariance matrix, and the corresponding power mismatches, and propose three detectors based on the generalized likelihood ratio test (GLRT), Rao and the Wald tests. The results from simulated and real data both illustrate that the detectors based on GLRT and Rao test have higher probabilities of detection (PDs) than the existing competitors. Among them, the Rao test-based detector exhibits the best overall detection performance. We also analyze the impact of the target extended dimensions, the signal subspace dimensions, and the number of training samples on the detection performance. Furthermore, simulation experiments also demonstrate that the proposed detectors have a constant false alarm rate (CFAR) property for the structure of clutter covariance matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14049v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/JSEN.2024.3472040</arxiv:DOI>
      <dc:creator>Daipeng Xiao, Weijian Liu, Jun Liu, Lingyan Dai, Xueli Fang, Jianjun Ge</dc:creator>
    </item>
    <item>
      <title>A simple emulator that enables interpretation of parameter-output relationships, applied to two climate model PPEs</title>
      <link>https://arxiv.org/abs/2410.00931</link>
      <description>arXiv:2410.00931v2 Announce Type: replace 
Abstract: We present a new additive method, nicknamed sage for Simplified Additive Gaussian processes Emulator, to emulate climate model Perturbed Parameter Ensembles (PPEs). It estimates the value of a climate model output as the sum of additive terms. Each additive term is the mean of a Gaussian Process, and corresponds to the impact of a parameter or parameter group on the variable of interest. This design caters to the sparsity of PPEs which are characterized by limited ensemble members and high dimensionality of the parameter space. sage quantifies the variability explained by different parameters and parameter groups, providing additional insights on the parameter-climate model output relationship. We apply the method to two climate model PPEs and compare it to a fully connected Neural Network. The two methods have comparable performance with both PPEs, but sage provides insights on parameter and parameter group importance as well as diagnostics useful for optimizing PPE design. Insights gained are valid regardless of the emulator method used, and have not been previously addressed. Our work highlights that analyzing the PPE used to train an emulator is different from analyzing data generated from an emulator trained on the PPE, as the former provides more insights on the data structure in the PPE which could help inform the emulator design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00931v2</guid>
      <category>stat.ME</category>
      <category>physics.ao-ph</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Qingyuan Yang, Gregory S Elsaesser, Marcus Van Lier-Walqui, Trude Eidhammer</dc:creator>
    </item>
    <item>
      <title>Spatial Hyperspheric Models for Compositional Data</title>
      <link>https://arxiv.org/abs/2410.03648</link>
      <description>arXiv:2410.03648v2 Announce Type: replace 
Abstract: Compositional data are an increasingly prevalent data source in spatial statistics. Analysis of such data is typically done on log-ratio transformations or via Dirichlet regression. However, these approaches often make unnecessarily strong assumptions (e.g., strictly positive components, exclusively negative correlations). An alternative approach uses square-root transformed compositions and directional distributions. Such distributions naturally allow for zero-valued components and positive correlations, yet they may include support outside the non-negative orthant and are not generative for compositional data. To overcome this challenge, we truncate the elliptically symmetric angular Gaussian (ESAG) distribution to the non-negative orthant. Additionally, we propose a spatial hyperspheric regression that contains fixed and random multivariate spatial effects. The proposed method also contains a term that can be used to propagate uncertainty that may arise from precursory stochastic models (i.e., machine learning classification). We demonstrate our method on a simulation study and on classified bioacoustic signals of the Dryobates pubescens (downy woodpecker).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03648v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael R. Schwob, Mevin B. Hooten, Nicholas M. Calzada</dc:creator>
    </item>
    <item>
      <title>Double Robust Bayesian Inference on Average Treatment Effects</title>
      <link>https://arxiv.org/abs/2211.16298</link>
      <description>arXiv:2211.16298v5 Announce Type: replace-cross 
Abstract: We propose a double robust Bayesian inference procedure on the average treatment effect (ATE) under unconfoundedness. For our new Bayesian approach, we first adjust the prior distributions of the conditional mean functions, and then correct the posterior distribution of the resulting ATE. Both adjustments make use of pilot estimators motivated by the semiparametric influence function for ATE estimation. We prove asymptotic equivalence of our Bayesian procedure and efficient frequentist ATE estimators by establishing a new semiparametric Bernstein-von Mises theorem under double robustness; i.e., the lack of smoothness of conditional mean functions can be compensated by high regularity of the propensity score and vice versa. Consequently, the resulting Bayesian credible sets form confidence intervals with asymptotically exact coverage probability. In simulations, our method provides precise point estimates of the ATE through the posterior mean and credible intervals that closely align with the nominal coverage probability. Furthermore, our approach achieves a shorter interval length in comparison to existing methods. We illustrate our method in an application to the National Supported Work Demonstration following LaLonde [1986] and Dehejia and Wahba [1999].</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.16298v5</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Breunig, Ruixuan Liu, Zhengfei Yu</dc:creator>
    </item>
    <item>
      <title>The extended Ville's inequality for nonintegrable nonnegative supermartingales</title>
      <link>https://arxiv.org/abs/2304.01163</link>
      <description>arXiv:2304.01163v3 Announce Type: replace-cross 
Abstract: Following the initial work by Robbins, we rigorously present an extended theory of nonnegative supermartingales, requiring neither integrability nor finiteness. In particular, we derive a key maximal inequality foreshadowed by Robbins, which we call the extended Ville's inequality, that strengthens the classical Ville's inequality (for integrable nonnegative supermartingales), and also applies to our nonintegrable setting. We derive an extension of the method of mixtures, which applies to $\sigma$-finite mixtures of our extended nonnegative supermartingales. We present some implications of our theory for sequential statistics, such as the use of improper mixtures (priors) in deriving nonparametric confidence sequences and (extended) e-processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.01163v3</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Wang, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Probabilistic Conformal Prediction with Approximate Conditional Validity</title>
      <link>https://arxiv.org/abs/2407.01794</link>
      <description>arXiv:2407.01794v2 Announce Type: replace-cross 
Abstract: We develop a new method for generating prediction sets that combines the flexibility of conformal methods with an estimate of the conditional distribution $P_{Y \mid X}$. Existing methods, such as conformalized quantile regression and probabilistic conformal prediction, usually provide only a marginal coverage guarantee. In contrast, our approach extends these frameworks to achieve approximately conditional coverage, which is crucial for many practical applications. Our prediction sets adapt to the behavior of the predictive distribution, making them effective even under high heteroscedasticity. While exact conditional guarantees are infeasible without assumptions on the underlying data distribution, we derive non-asymptotic bounds that depend on the total variation distance of the conditional distribution and its estimate. Using extensive simulations, we show that our method consistently outperforms existing approaches in terms of conditional coverage, leading to more reliable statistical inference in a variety of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01794v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincent Plassier, Alexander Fishkov, Mohsen Guizani, Maxim Panov, Eric Moulines</dc:creator>
    </item>
    <item>
      <title>Graph Fourier Neural Kernels (G-FuNK): Learning Solutions of Nonlinear Diffusive Parametric PDEs on Multiple Domains</title>
      <link>https://arxiv.org/abs/2410.04655</link>
      <description>arXiv:2410.04655v2 Announce Type: replace-cross 
Abstract: Predicting time-dependent dynamics of complex systems governed by non-linear partial differential equations (PDEs) with varying parameters and domains is a challenging task motivated by applications across various fields. We introduce a novel family of neural operators based on our Graph Fourier Neural Kernels, designed to learn solution generators for nonlinear PDEs in which the highest-order term is diffusive, across multiple domains and parameters. G-FuNK combines components that are parameter- and domain-adapted with others that are not. The domain-adapted components are constructed using a weighted graph on the discretized domain, where the graph Laplacian approximates the highest-order diffusive term, ensuring boundary condition compliance and capturing the parameter and domain-specific behavior. Meanwhile, the learned components transfer across domains and parameters using our variant Fourier Neural Operators. This approach naturally embeds geometric and directional information, improving generalization to new test domains without need for retraining the network. To handle temporal dynamics, our method incorporates an integrated ODE solver to predict the evolution of the system. Experiments show G-FuNK's capability to accurately approximate heat, reaction diffusion, and cardiac electrophysiology equations across various geometries and anisotropic diffusivity fields. G-FuNK achieves low relative errors on unseen domains and fiber fields, significantly accelerating predictions compared to traditional finite-element solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04655v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.SP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 10 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shane E. Loeffler, Zan Ahmad, Syed Yusuf Ali, Carolyna Yamamoto, Dan M. Popescu, Alana Yee, Yash Lal, Natalia Trayanova, Mauro Maggioni</dc:creator>
    </item>
  </channel>
</rss>
