<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Jun 2025 01:35:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Introducing RobustiPy: An efficient next generation multiversal library with model selection, averaging, resampling, and explainable artificial intelligence</title>
      <link>https://arxiv.org/abs/2506.19958</link>
      <description>arXiv:2506.19958v1 Announce Type: new 
Abstract: We present RobustiPy, a next generation Python-based framework for model uncertainty quantification and multiverse analysis, released under the GNU GPL v3.0. Through the integration of efficient bootstrap-based confidence intervals, combinatorial exploration of dependent-variable specifications, model selection and averaging, and two complementary joint-inference routines, RobustiPy transcends existing uncertainty-quantification tools. Its design further supports rigorous out-of-sample evaluation and apportions the predictive contribution of each covariate. We deploy the library across five carefully constructed simulations and ten empirically grounded case studies drawn from high-impact literature and teaching examples, including a novel re-analysis of "unexplained discrepancies" in famous prior work. To illustrate its performance, we time-profile RobustiPy over roughly 672 million simulated linear regressions. These applications showcase how RobustiPy not only accelerates robust inference but also deepens our interpretive insight into model sensitivity across the vast analytical multiverse within which scientists operate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19958v1</guid>
      <category>stat.ME</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel Valdenegro Ibarra, Jiani Yan, Duiyi Dai, Charles Rahal</dc:creator>
    </item>
    <item>
      <title>Tipping Point Sensitivity Analysis for Missing Data in Time-to-Event Endpoints: Model-Based and Model-Free Approaches</title>
      <link>https://arxiv.org/abs/2506.19988</link>
      <description>arXiv:2506.19988v1 Announce Type: new 
Abstract: Missing data frequently occurs in clinical trials with time-to-event endpoints, often due to administrative censoring. Other reasons, such as loss-to-follow up and patient withdrawal of consent, can violate the censoring-at-random assumption hence lead to biased estimates of the treatment effect under treatment policy estimand. Numerous methods have been proposed to conduct sensitivity analyses in these situations, one of which is the tipping point analysis. It aims to evaluate the robustness of trial conclusions by varying certain data and/or model aspects while imputing missing data. We provide an overview of the missing data considerations. The main contribution of this paper lies in categorizing and contrasting tipping point methods as two groups, namely model-based and model-free approaches, where the latter is under-emphasized in the literature. We highlight their important differences in terms of assumptions, behaviors and interpretations. Through two case studies and simulations under various scenarios, we provide insight on how different tipping point methods impact the interpretation of trial outcomes. Through these comparisons, we aim to provide a practical guide to conduct tipping point analyses from the choice of methods to ways of conducting clinical plausibility assessment, and ultimately contributing to more robust and reliable interpretation of clinical trial results in the presence of missing data for time-to-event endpoints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19988v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ajmal Oodally, Craig Wang, Zheng Li, Arunava Chakravartty</dc:creator>
    </item>
    <item>
      <title>Speeding up the ordered allocation sampler</title>
      <link>https://arxiv.org/abs/2506.20021</link>
      <description>arXiv:2506.20021v1 Announce Type: new 
Abstract: The ordered allocation sampler is a Gibbs sampler designed to explore the posterior distribution in nonparametric mixture models. It encompasses both infinite mixtures and finite mixtures with random number of components, and it has be shown to possess mixing properties that pair well with collapsed, or marginal, samplers that integrate out the mixing distribution. The main advantage is that it adapts to mixing priors that do not enjoy tractable predictive structures needed for the implementation of marginal sampling methods. Thus it is as widely applicable as other conditional samplers while enjoying better algorithmic performances. In this paper we provide a modification of the ordered allocation sampler that enhances its performances in a substantial way while easing its implementation. In addition, exploiting the similarity with marginal samplers, we are able to adapt to the new version of the sampler the split-merge moves of Jain and Neal. Simulation studies confirm these findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20021v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria F. Gil-Leyva, Fidel Selva, Pierpaolo De Blasi</dc:creator>
    </item>
    <item>
      <title>Dynamic Causal Mediation Analysis for Intensive Longitudinal Data</title>
      <link>https://arxiv.org/abs/2506.20027</link>
      <description>arXiv:2506.20027v1 Announce Type: new 
Abstract: Intensive longitudinal data, characterized by frequent measurements across numerous time points, are increasingly common due to advances in wearable devices and mobile health technologies. We consider evaluating causal mediation pathways between time-varying exposures, time-varying mediators, and a final, distal outcome using such data. Addressing mediation questions in these settings is challenging due to numerous potential exposures, complex mediation pathways, and intermediate confounding. Existing methods, such as interventional and path-specific effects, become impractical in intensive longitudinal data. We propose novel mediation effects termed natural direct and indirect excursion effects, which quantify mediation through the most immediate mediator following each treatment time. These effects are identifiable under plausible assumptions and decompose the total excursion effect. We derive efficient influence functions and propose multiply-robust estimators for these mediation effects. The estimators are multiply-robust and accommodate flexible machine learning algorithms and optional cross-fitting. In settings where the treatment assignment mechanism is known, such as the micro-randomized trial, the estimators are doubly-robust. We establish the consistency and asymptotic normality of the proposed estimators. Our methodology is illustrated using real-world data from the HeartSteps micro-randomized trial and the SleepHealth observational study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20027v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianchen Qian</dc:creator>
    </item>
    <item>
      <title>Causal mediation analysis for longitudinal and survival data in continuous time using Bayesian non-parametric joint models</title>
      <link>https://arxiv.org/abs/2506.20058</link>
      <description>arXiv:2506.20058v1 Announce Type: new 
Abstract: Observational cohort data is an important source of information for understanding the causal effects of treatments on survival and the degree to which these effects are mediated through changes in disease-related risk factors. However, these analyses are often complicated by irregular data collection intervals and the presence of longitudinal confounders and mediators. We propose a causal mediation framework that jointly models longitudinal exposures, confounders, mediators, and time-to-event outcomes as continuous functions of age. This framework for longitudinal covariate trajectories enables statistical inference even at ages where the subject's covariate measurements are unavailable. The observed data distribution in our framework is modeled using an enriched Dirichlet process mixture (EDPM) model. Using data from the Atherosclerosis Risk in Communities cohort study, we apply our methods to assess how medication -- prescribed to target cardiovascular disease (CVD) risk factors -- affects the time-to-CVD death.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20058v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saurabh Bhandari, Michael J. Daniels, Juned Siddique</dc:creator>
    </item>
    <item>
      <title>hdbayes: An R Package for Bayesian Analysis of Generalized Linear Models Using Historical Data</title>
      <link>https://arxiv.org/abs/2506.20060</link>
      <description>arXiv:2506.20060v1 Announce Type: new 
Abstract: There has been increased interest in the use of historical data to formulate informative priors in regression models. While many such priors for incorporating historical data have been proposed, adoption is limited due to access to software. Where software does exist, the implementations between different methods could be vastly different, making comparisons between methods difficult. In this paper, we introduce the R package hdbayes, an implementation of the power prior, normalized power prior, Bayesian hierarchical model, robust meta-analytic prior, commensurate prior, and latent exchangeability prior for generalized linear models. The bulk of the package is written in the Stan programming language, with user-friendly R wrapper functions to call samplers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20060v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ethan M. Alt, Xinxin Chen, Luiz M. Carvalho, Joseph G. Ibrahim</dc:creator>
    </item>
    <item>
      <title>Wordkrill: Extending Wordfish into the multidimensional political space</title>
      <link>https://arxiv.org/abs/2506.20275</link>
      <description>arXiv:2506.20275v1 Announce Type: new 
Abstract: Spatial models are central to the study of political conflict, yet their empirical application often depends on text-based methods. A prominent example is the Wordfish model, which estimates actor positions from political texts. However, a key limitation of Wordfish is its unidimensionality, despite the well-established multidimensional nature of political competition. This contribution introduces Wordkrill, a multidimensional extension of Wordfish that retains the original model's interpretability while allowing for efficient estimation of political positions along multiple latent dimensions. After presenting the mathematical framework of Wordkrill, its utility through brief applications to party manifestos and parliamentary speeches is demonstrated. These examples illustrate both the practical advantages and current limitations of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20275v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Benjamin Riesch</dc:creator>
    </item>
    <item>
      <title>Path-Based Approach for Detecting and Assessing Inconsistency in Network Meta-Analysis: A Novel Method</title>
      <link>https://arxiv.org/abs/2506.20364</link>
      <description>arXiv:2506.20364v1 Announce Type: new 
Abstract: Network Meta-Analysis (NMA) plays a pivotal role in synthesizing evidence from various sources and comparing multiple interventions. At its core, NMA relies on integrating both direct evidence from head-to-head comparisons and indirect evidence from different paths that link treatments through common comparators. A key aspect is evaluating consistency between direct and indirect sources. Existing methods to detect inconsistency, although widely used, have limitations. For example, they do not account for differences within indirect sources or cannot estimate inconsistency when direct evidence is absent.
  In this paper, we introduce a path-based approach that explores all sources of evidence without separating direct and indirect. We introduce a measure based on the square of differences to quantitatively capture inconsistency, and propose a Netpath plot to visualize inconsistencies between various paths. We provide an implementation of our path-based method within the netmeta R package. Via application to fictional and real-world examples, we show that our method is able to detect and visualize inconsistency between multiple paths of evidence that would otherwise be masked by considering all indirect sources together. The path-based approach therefore provides a more comprehensive evaluation of inconsistency within a network of treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20364v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noosheen R. Tahmasebi, Annabel L. Davies, Theodoros Papakonstantinou, Gerta R\"ucker, Adriani Nikolakopoulou</dc:creator>
    </item>
    <item>
      <title>Fast Penalized Generalized Estimating Equations for Large Longitudinal Functional Datasets</title>
      <link>https://arxiv.org/abs/2506.20437</link>
      <description>arXiv:2506.20437v1 Announce Type: new 
Abstract: Longitudinal binary or count functional data are common in neuroscience, but are often too large to analyze with existing functional regression methods. We propose one-step penalized generalized estimating equations that supports continuous, count, or binary functional outcomes and is fast even when datasets have a large number of clusters and large cluster sizes. The method applies to both functional and scalar covariates, and the one-step estimation framework enables efficient smoothing parameter selection, bootstrapping, and joint confidence interval construction. Importantly, this semi-parametric approach yields coefficient confidence intervals that are provably valid asymptotically even under working correlation misspecification. By developing a general theory for adaptive one-step M-estimation, we prove that the coefficient estimates are asymptotically normal and as efficient as the fully-iterated estimator; we verify these theoretical properties in extensive simulations. Finally, we apply our method to a calcium imaging dataset published in Nature, and show that it reveals important timing effects obscured in previous non-functional analyses. In doing so, we demonstrate scaling to common neuroscience dataset sizes: the one-step estimator fits to a dataset with 150,000 (binary) functional outcomes, each observed at 120 functional domain points, in only 13.5 minutes on a laptop without parallelization. We release our implementation in the 'fastFGEE' package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20437v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Gabriel Loewinger, Alex W. Levis, Erjia Cui, Francisco Pereira</dc:creator>
    </item>
    <item>
      <title>Anytime-Valid Inference in Adaptive Experiments: Covariate Adjustment and Balanced Power</title>
      <link>https://arxiv.org/abs/2506.20523</link>
      <description>arXiv:2506.20523v1 Announce Type: new 
Abstract: Adaptive experiments have become central to modern experimental design, enabling researchers to efficiently identify optimal treatments, improve statistical power, and maximize respondent welfare. However, adaptive experiments compromise valid inference and leave sub-optimal treatments underpowered. We introduce two methodological advances for adaptive experimentation. First, covariate-adjusted Mixture Adaptive Design (MADCovar) achieves substantial improvements in average treatment effect (ATE) precision by incorporating covariate adjustment within an anytime-valid inference framework. Second, power-modified MAD (MADMod) reallocates sample to underpowered treatment arms, improving statistical power across all treatments while maintaining error control. Both methods provide anytime-valid guarantees, enabling continuous monitoring without inflating Type 1 error rates. Simulation studies and empirical analyses demonstrate that MADCovar delivers significant precision gains and that MADMod ensures robust inference even for suboptimal treatments. Together, these methods address key limitations of adaptive experiments and equip researchers with practical tools for precise and reliable causal inference. Our proposed methods are implemented through an open-source software package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20523v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Molitor, Samantha Gold</dc:creator>
    </item>
    <item>
      <title>Causal Inference for Latent Outcomes Learned with Factor Models</title>
      <link>https://arxiv.org/abs/2506.20549</link>
      <description>arXiv:2506.20549v1 Announce Type: new 
Abstract: In many fields$\unicode{x2013}$including genomics, epidemiology, natural language processing, social and behavioral sciences, and economics$\unicode{x2013}$it is increasingly important to address causal questions in the context of factor models or representation learning. In this work, we investigate causal effects on $\textit{latent outcomes}$ derived from high-dimensional observed data using nonnegative matrix factorization. To the best of our knowledge, this is the first study to formally address causal inference in this setting. A central challenge is that estimating a latent factor model can cause an individual's learned latent outcome to depend on other individuals' treatments, thereby violating the standard causal inference assumption of no interference. We formalize this issue as $\textit{learning-induced interference}$ and distinguish it from interference present in a data-generating process. To address this, we propose a novel, intuitive, and theoretically grounded algorithm to estimate causal effects on latent outcomes while mitigating learning-induced interference and improving estimation efficiency. We establish theoretical guarantees for the consistency of our estimator and demonstrate its practical utility through simulation studies and an application to cancer mutational signature analysis. All baseline and proposed methods are available in our open-source R package, ${\tt causalLFO}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20549v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jenna M. Landy, Dafne Zorzetto, Roberta De Vito, Giovanni Parmigiani</dc:creator>
    </item>
    <item>
      <title>Inference for Error-Prone Count Data: Estimation under a Binomial Convolution Framework</title>
      <link>https://arxiv.org/abs/2506.20596</link>
      <description>arXiv:2506.20596v1 Announce Type: new 
Abstract: Measurement error in count data is common but underexplored in the literature, particularly in contexts where observed scores are bounded and arise from discrete scoring processes. Motivated by applications in oral reading fluency assessment, we propose a binomial convolution framework that extends binary misclassification models to settings where only the aggregate number of correct responses is observed, and errors may involve both overcounting and undercounting the number of events. The model accommodates distinct true positive and true negative accuracy rates and preserves the bounded nature of the data.
  Assuming the availability of both contaminated and error-free scores on a subset of items, we develop and compare three estimation strategies: maximum likelihood estimation (MLE), linear regression, and generalized method of moments (GMM). Extensive simulations show that MLE is most accurate when the model is correctly specified but is computationally intensive and less robust to misspecification. Regression is simple and stable but less precise, while GMM offers a compromise in model dependence, though it is sensitive to outliers.
  In practice, this framework supports improved inference in unsupervised settings where contaminated scores serve as inputs to downstream analyses. By quantifying accuracy rates, the model enables score corrections even when no specific outcome is yet defined. We demonstrate its utility using real oral reading fluency data, comparing human and AI-generated scores. Findings highlight the practical implications of estimator choice and underscore the importance of explicitly modeling asymmetric measurement error in count data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20596v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuqiu Yang, Christina Vu, Cornelis J. Potgieter, Xinlei Wang, Akihito Kamata</dc:creator>
    </item>
    <item>
      <title>A New Regression Model for Analyzing Non-Stationary Extremes in Response and Covariate Variables with an Application in Meteorology</title>
      <link>https://arxiv.org/abs/2506.20615</link>
      <description>arXiv:2506.20615v1 Announce Type: new 
Abstract: The paper introduces a new regression model designed for situations where both the response and covariates are non-stationary extremes. This method is specifically designed for situations where both the response variable and covariates are represented as block maxima, as the limiting distribution of suitably standardized componentwise maxima follows an extreme value copula. The framework focuses on the regression manifold, which consists of a collection of regression lines aligned with the asymptotic result. A Logistic-normal prior is applied to the space of spectral densities to gain insights into the model based on the data, resulting in an induced prior on the regression manifolds. Numerical studies demonstrate the effectiveness of the proposed method, and an analysis of real meteorological data provides intriguing insights into the relationships between extreme losses in precipitation and temperature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20615v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Amina El Bernoussi, Mohamed El Arrouchi</dc:creator>
    </item>
    <item>
      <title>On unbiased estimators for functions of the rate parameter of the exponential distribution</title>
      <link>https://arxiv.org/abs/2506.20005</link>
      <description>arXiv:2506.20005v1 Announce Type: cross 
Abstract: In this paper, we explicitly derive unbiased estimators for various functions of the rate parameter of the exponential distribution, including powers of the rate parameter, the $q$th quantile, the $p$th moment, the survival function, the maximum, minimum, probability density function, mean past lifetime, moment generating function, and others. It is also noteworthy that this work corrects a general formula originally proposed by Tate, R. F. (Ann. Math. Statist., 30(2): 341-366, 1959) for constructing unbiased estimators of functions of the exponential distribution's rate parameter in the absence of a location parameter. Additionally, we establish a result demonstrating the asymptotic normality of the proposed unbiased estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20005v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Eduardo Yoshio Nakano</dc:creator>
    </item>
    <item>
      <title>Causal discovery in deterministic discrete LTI-DAE systems</title>
      <link>https://arxiv.org/abs/2506.20169</link>
      <description>arXiv:2506.20169v1 Announce Type: cross 
Abstract: Discovering pure causes or driver variables in deterministic LTI systems is of vital importance in the data-driven reconstruction of causal networks. A recent work by Kathari and Tangirala, proposed in 2022, formulated the causal discovery method as a constraint identification problem. The constraints are identified using a dynamic iterative PCA (DIPCA)-based approach for dynamical systems corrupted with Gaussian measurement errors. The DIPCA-based method works efficiently for dynamical systems devoid of any algebraic relations. However, several dynamical systems operate under feedback control and/or are coupled with conservation laws, leading to differential-algebraic (DAE) or mixed causal systems. In this work, a method, namely the partition of variables (PoV), for causal discovery in LTI-DAE systems is proposed. This method is superior to the method that was presented by Kathari and Tangirala (2022), as PoV also works for pure dynamical systems, which are devoid of algebraic equations. The proposed method identifies the causal drivers up to a minimal subset. PoV deploys DIPCA to first determine the number of algebraic relations ($n_a$), the number of dynamical relations ($n_d$) and the constraint matrix. Subsequently, the subsets are identified through an admissible partitioning of the constraint matrix by finding the condition number of it. Case studies are presented to demonstrate the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20169v1</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bala Rajesh Konkathi, Arun K. Tangirala</dc:creator>
    </item>
    <item>
      <title>Valid Selection among Conformal Sets</title>
      <link>https://arxiv.org/abs/2506.20173</link>
      <description>arXiv:2506.20173v1 Announce Type: cross 
Abstract: Conformal prediction offers a distribution-free framework for constructing prediction sets with coverage guarantees. In practice, multiple valid conformal prediction sets may be available, arising from different models or methodologies. However, selecting the most desirable set, such as the smallest, can invalidate the coverage guarantees. To address this challenge, we propose a stability-based approach that ensures coverage for the selected prediction set. We extend our results to the online conformal setting, propose several refinements in settings where additional structure is available, and demonstrate its effectiveness through experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20173v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahmoud Hegazy, Liviu Aolaritei, Michael I. Jordan, Aymeric Dieuleveut</dc:creator>
    </item>
    <item>
      <title>Multivariate Long-term Profile Monitoring with Application to the KW51 Railway Bridge</title>
      <link>https://arxiv.org/abs/2506.20295</link>
      <description>arXiv:2506.20295v1 Announce Type: cross 
Abstract: Structural Health Monitoring (SHM) plays a pivotal role in modern civil engineering, providing critical insights into the health and integrity of infrastructure systems. This work presents a novel multivariate long-term profile monitoring approach to eliminate fluctuations in the measured response quantities, e.g., caused by environmental influences or measurement error. Our methodology addresses critical challenges in SHM and combines supervised methods with unsupervised, principal component analysis-based approaches in a single overarching framework, offering both flexibility and robustness in handling real-world large and/or sparse sensor data streams. We propose a function-on-function regression framework, which leverages functional data analysis for multivariate sensor data and integrates nonlinear modeling techniques, mitigating covariate-induced variations that can obscure structural changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20295v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Wittenberg, Alexander Mendler, Sven Knoth, Jan Gertheiss</dc:creator>
    </item>
    <item>
      <title>POLAR: A Pessimistic Model-based Policy Learning Algorithm for Dynamic Treatment Regimes</title>
      <link>https://arxiv.org/abs/2506.20406</link>
      <description>arXiv:2506.20406v1 Announce Type: cross 
Abstract: Dynamic treatment regimes (DTRs) provide a principled framework for optimizing sequential decision-making in domains where decisions must adapt over time in response to individual trajectories, such as healthcare, education, and digital interventions. However, existing statistical methods often rely on strong positivity assumptions and lack robustness under partial data coverage, while offline reinforcement learning approaches typically focus on average training performance, lack statistical guarantees, and require solving complex optimization problems. To address these challenges, we propose POLAR, a novel pessimistic model-based policy learning algorithm for offline DTR optimization. POLAR estimates the transition dynamics from offline data and quantifies uncertainty for each history-action pair. A pessimistic penalty is then incorporated into the reward function to discourage actions with high uncertainty. Unlike many existing methods that focus on average training performance, POLAR directly targets the suboptimality of the final learned policy and offers theoretical guarantees, without relying on computationally intensive minimax or constrained optimization procedures. To the best of our knowledge, POLAR is the first model-based DTR method to provide both statistical and computational guarantees, including finite-sample bounds on policy suboptimality. Empirical results on both synthetic data and the MIMIC-III dataset demonstrate that POLAR outperforms state-of-the-art methods and yields near-optimal, history-aware treatment strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20406v1</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruijia Zhang, Zhengling Qi, Yue Wu, Xiangyu Zhang, Yanxun Xu</dc:creator>
    </item>
    <item>
      <title>Scalable Subset Selection in Linear Mixed Models</title>
      <link>https://arxiv.org/abs/2506.20425</link>
      <description>arXiv:2506.20425v1 Announce Type: cross 
Abstract: Linear mixed models (LMMs), which incorporate fixed and random effects, are key tools for analyzing heterogeneous data, such as in personalized medicine or adaptive marketing. Nowadays, this type of data is increasingly wide, sometimes containing thousands of candidate predictors, necessitating sparsity for prediction and interpretation. However, existing sparse learning methods for LMMs do not scale well beyond tens or hundreds of predictors, leaving a large gap compared with sparse methods for linear models, which ignore random effects. This paper closes the gap with a new $\ell_0$ regularized method for LMM subset selection that can run on datasets containing thousands of predictors in seconds to minutes. On the computational front, we develop a coordinate descent algorithm as our main workhorse and provide a guarantee of its convergence. We also develop a local search algorithm to help traverse the nonconvex optimization surface. Both algorithms readily extend to subset selection in generalized LMMs via a penalized quasi-likelihood approximation. On the statistical front, we provide a finite-sample bound on the Kullback-Leibler divergence of the new method. We then demonstrate its excellent performance in synthetic experiments and illustrate its utility on two datasets from biology and journalism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20425v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Thompson, Matt P. Wand, Joanna J. J. Wang</dc:creator>
    </item>
    <item>
      <title>Benchmarking Unsupervised Strategies for Anomaly Detection in Multivariate Time Series</title>
      <link>https://arxiv.org/abs/2506.20574</link>
      <description>arXiv:2506.20574v1 Announce Type: cross 
Abstract: Anomaly detection in multivariate time series is an important problem across various fields such as healthcare, financial services, manufacturing or physics detector monitoring. Accurately identifying when unexpected errors or faults occur is essential, yet challenging, due to the unknown nature of anomalies and the complex interdependencies between time series dimensions. In this paper, we investigate transformer-based approaches for time series anomaly detection, focusing on the recently proposed iTransformer architecture. Our contributions are fourfold: (i) we explore the application of the iTransformer to time series anomaly detection, and analyse the influence of key parameters such as window size, step size, and model dimensions on performance; (ii) we examine methods for extracting anomaly labels from multidimensional anomaly scores and discuss appropriate evaluation metrics for such labels; (iii) we study the impact of anomalous data present during training and assess the effectiveness of alternative loss functions in mitigating their influence; and (iv) we present a comprehensive comparison of several transformer-based models across a diverse set of datasets for time series anomaly detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20574v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Boggia, Rafael Teixeira de Lima, Bogdan Malaescu</dc:creator>
    </item>
    <item>
      <title>A Unified Multiple Testing Framework based on rho-values</title>
      <link>https://arxiv.org/abs/2310.17845</link>
      <description>arXiv:2310.17845v2 Announce Type: replace 
Abstract: Multiple testing is an important research area with widespread scientific applications, including in biology and neuroscience. Among popularly adopted multiple testing procedures, many are based on p-values or Local false discovery rate (Lfdr) statistics. However, p-values--often obtained via the probability integral transform of standard test statistics--typically lack information from the alternatives, resulting in suboptimal performance. In contrast, Lfdr-based methods can achieve asymptotic optimality, but their ability to control the false discovery rate (FDR) hinges on accurate estimation of the Lfdr, which can be challenging, especially when incorporating side information. In this article, we introduce a novel and flexible class of statistics, termed rho-values, and develop a corresponding multiple testing framework that integrates the strengths of both p-values and Lfdr, while addressing their respective limitations. Specifically, the rho-value framework unifies these two paradigms through a two-step process: ranking and thresholding. The ranking induced by rho-values closely resembles that of Lfdr-based methods, while the thresholding step aligns with conventional p-value procedures. Therefore, the proposed framework guarantees FDR control under mild assumptions; it maintains the integrity of the structural information encoded by the summary statistics and the auxiliary covariates, and hence can be asymptotically optimal. We demonstrate the advantages of the rho-value framework through comprehensive simulations and analyses of two real datasets: one from microbiome research and another related to attention deficit hyperactivity disorder (ADHD).</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17845v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shenghao Qin, Bowen Gang, Yin Xia</dc:creator>
    </item>
    <item>
      <title>A Stability Framework for Parameter Selection in the Minimum Covariance Determinant Problem</title>
      <link>https://arxiv.org/abs/2401.14359</link>
      <description>arXiv:2401.14359v5 Announce Type: replace 
Abstract: The Minimum Covariance Determinant (MCD) method is a widely adopted tool for robust estimation and outlier detection. In this paper, we introduce MCD model selection based on the notion of stability. Our best subset method leverages prior best practices such as statistical depths for initialization and concentration steps for subset refinement. Our contribution lies in constructing a bootstrap procedure to estimate the instability of the best subset algorithm. The instability path offers insights into a dataset's inlier/outlier structure and facilitates suitable choice of the subset size. We rigorously benchmark the proposed framework against existing MCD variants and illustrate its practical utility on several real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14359v5</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiang Heng, Hui Shen, Kenneth Lange</dc:creator>
    </item>
    <item>
      <title>Modeling asymmetry in multi-way contingency tables with ordinal categories: A maximum-entropy approach with f-divergence</title>
      <link>https://arxiv.org/abs/2405.12157</link>
      <description>arXiv:2405.12157v2 Announce Type: replace 
Abstract: This study introduces a novel model that effectively captures asymmetric structures in multivariate contingency tables with ordinal categories. Leveraging the principle of maximum entropy, our approach employs f-divergence to provide a rational model under the presence of a ''prior guess.'' Inspired by the constraints used in the derivation of multivariate normal distributions, we demonstrate that the proposed model minimizes f-divergence from complete symmetry under specific constraints. The proposed model encompasses existing asymmetry models as special cases while offering remarkably high interpretability. By modifying divergence measures included in f-divergence, the model provides the flexibility to adapt to specific probabilistic structures of interest. Furthermore, we established theorems that show that a complete symmetry model can be decomposed into two or more models, each imposing less restrictive parameter constraints. We also investigated the properties of the goodness-of-fit statistics with an emphasis on the likelihood ratio and Wald test statistics. Extensive Monte Carlo simulations confirmed the nominal size, high power, and robustness of the choice of f-divergence. Finally, an application to real-world data highlights the practical utility of the proposed model for analyzing asymmetric structures in ordinal contingency tables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12157v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hisaya Okahara, Kouji Tahata</dc:creator>
    </item>
    <item>
      <title>Building Population-Informed Priors for Bayesian Inference Using Data-Consistent Stochastic Inversion</title>
      <link>https://arxiv.org/abs/2407.13814</link>
      <description>arXiv:2407.13814v3 Announce Type: replace 
Abstract: Bayesian inference provides a powerful tool for leveraging observational data to inform model predictions and uncertainties. However, when such data is limited, Bayesian inference may not adequately constrain uncertainty without the use of highly informative priors. Common approaches for constructing informative priors typically rely on either assumptions or knowledge of the underlying physics, which may not be available in all scenarios. In this work, we consider the scenario where data are available on a population of assets/individuals, which occurs in many problem domains such as biomedical or digital twin applications, and leverage this population-level data to systematically constrain the Bayesian prior and subsequently improve individualized inferences. The approach proposed in this paper is based upon a recently developed technique known as data-consistent inversion (DCI) for constructing a pullback probability measure. Succinctly, we utilize DCI to build population-informed priors for subsequent Bayesian inference on individuals. While the approach is general and applies to nonlinear maps and arbitrary priors, we prove that for linear inverse problems with Gaussian priors, the population-informed prior produces an increase in the information gain as measured by the determinant and trace of the inverse posterior covariance. We also demonstrate that the Kullback-Leibler divergence often improves with high probability. Numerical results, including linear-Gaussian examples and one inspired by digital twins for additively manufactured assets, indicate that there is significant value in using these population-informed priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13814v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rebekah D. White, John D. Jakeman, Tim Wildey, Troy Butler</dc:creator>
    </item>
    <item>
      <title>gcor: A Python Implementation of Categorical Gini Correlation and Its Inference</title>
      <link>https://arxiv.org/abs/2506.19230</link>
      <description>arXiv:2506.19230v2 Announce Type: replace 
Abstract: Categorical Gini Correlation (CGC), introduced by Dang et al. (2020), is a novel dependence measure designed to quantify the association between a numerical variable and a categorical variable. It has appealing properties compared to existing dependence measures, such as zero correlation mutually implying independence between the variables. It has also shown superior performance over existing methods when applied to feature screening for classification. This article presents a Python implementation for computing CGC, constructing confidence intervals, and performing independence tests based on it. Efficient algorithms have been implemented for all procedures, and they have been optimized using vectorization and parallelization to enhance computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19230v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sameera Hewage</dc:creator>
    </item>
    <item>
      <title>E-STGCN: Extreme Spatiotemporal Graph Convolutional Networks for Air Quality Forecasting</title>
      <link>https://arxiv.org/abs/2411.12258</link>
      <description>arXiv:2411.12258v2 Announce Type: replace-cross 
Abstract: Modeling and forecasting air quality is crucial for effective air pollution management and protecting public health. Air quality data, characterized by nonlinearity, nonstationarity, and spatiotemporal correlations, often include extreme pollutant levels in severely polluted cities (e.g., Delhi, the capital of India). This is ignored by various geometric deep learning models, such as Spatiotemporal Graph Convolutional Networks (STGCN), which are otherwise effective for spatiotemporal forecasting. This study develops an extreme value theory (EVT) guided modified STGCN model (E-STGCN) for air pollution data to incorporate extreme behavior across pollutant concentrations. E-STGCN combines graph convolutional networks for spatial modeling and EVT-guided long short-term memory units for temporal sequence learning. Along with spatial and temporal components, it incorporates a generalized Pareto distribution to capture the extreme behavior of different air pollutants and embed this information into the learning process. The proposal is then applied to analyze air pollution data of 37 monitoring stations across Delhi, India. The forecasting performance for different test horizons is compared to benchmark forecasters (both temporal and spatiotemporal). It is found that E-STGCN has consistent performance across all seasons. The robustness of our results has also been evaluated empirically. Moreover, combined with conformal prediction, E-STGCN can produce probabilistic prediction intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12258v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Madhurima Panja, Tanujit Chakraborty, Anubhab Biswas, Soudeep Deb</dc:creator>
    </item>
    <item>
      <title>GMM with Many Weak Moment Conditions and Nuisance Parameters: General Theory and Applications to Causal Inference</title>
      <link>https://arxiv.org/abs/2505.07295</link>
      <description>arXiv:2505.07295v2 Announce Type: replace-cross 
Abstract: Weak identification is a common issue for many statistical problems -- for example, when instrumental variables are weakly correlated with treatment, or when proxy variables are weakly correlated with unmeasured confounders. Under weak identification, standard estimation methods, such as the generalized method of moments (GMM), can have sizeable bias in finite samples or even asymptotically. In addition, many practical settings involve a growing number of nuisance parameters, adding further complexity to the problem. In this paper, we study estimation and inference under a general nonlinear moment model with many weak moment conditions and many nuisance parameters. To obtain debiased inference for finite-dimensional target parameters, we demonstrate that Neyman orthogonality plays a stronger role than in conventional settings with strong identification. We study a general two-step debiasing estimator that allows for possibly nonparametric first-step estimation of nuisance parameters, and we establish its consistency and asymptotic normality under a many weak moment asymptotic regime. Our theory accommodates both high-dimensional moment conditions and function-valued nuisance parameters. We provide high-level assumptions for a general setting and discuss specific applications to the problems of estimation and inference with weak instruments and weak proxies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07295v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 26 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rui Wang, Kwun Chuen Gary Chan, Ting Ye</dc:creator>
    </item>
  </channel>
</rss>
