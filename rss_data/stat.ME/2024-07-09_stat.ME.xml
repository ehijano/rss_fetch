<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Jul 2024 01:38:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Active-Controlled Trial Design for HIV Prevention Trials with a Counterfactual Placebo</title>
      <link>https://arxiv.org/abs/2407.04812</link>
      <description>arXiv:2407.04812v1 Announce Type: new 
Abstract: In the quest for enhanced HIV prevention methods, the advent of antiretroviral drugs as pre-exposure prophylaxis (PrEP) has marked a significant stride forward. However, the ethical challenges in conducting placebo-controlled trials for new PrEP agents against a backdrop of highly effective existing PrEP options necessitates innovative approaches. This manuscript delves into the design and implementation of active-controlled trials that incorporate a counterfactual placebo estimate - a theoretical estimate of what HIV incidence would have been without effective prevention. We introduce a novel statistical framework for regulatory approval of new PrEP agents, predicated on the assumption of an available and consistent counterfactual placebo estimate. Our approach aims to assess the absolute efficacy (i.e., against placebo) of the new PrEP agent relative to the absolute efficacy of the active control. We propose a two-step procedure for hypothesis testing and further develop an approach that addresses potential biases inherent in non-randomized comparison to counterfactual placebos. By exploring different scenarios with moderately and highly effective active controls and counterfactual placebo estimates from various sources, we demonstrate how our design can significantly reduce sample sizes compared to traditional non-inferiority trials and offer a robust framework for evaluating new PrEP agents. This work contributes to the methodological repertoire for HIV prevention trials and underscores the importance of adaptability in the face of ethical and practical challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04812v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fei Gao, Holly Janes, Susan Buchbinder, Deborah Donnell</dc:creator>
    </item>
    <item>
      <title>A Triginometric Seasonal Component Model and its Application to Time Series with Two Types of Seasonality</title>
      <link>https://arxiv.org/abs/2407.04933</link>
      <description>arXiv:2407.04933v1 Announce Type: new 
Abstract: A finite trigonometric series model for seasonal time series is considered in this paper. This component model is shown to be useful, in particular, for the modeling of time series with two types of seasonality, a long-period and a short period. This component model is also shown to be effective in the case of ordinary seasonal time series with only one seasonal component, if the seasonal pattern is simple and can be well represented by a small number of trigonometric components. As examples, electricity demand data, bi-hourly temperature data, CO2 data, and two economic time series are considered. The last section summarizes the findings from the emperical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04933v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>G. Kitagawa (The Institute of Statistical Mathmatics,The Graduate University for Advanced Study)</dc:creator>
    </item>
    <item>
      <title>Treatment effect estimation under covariate-adaptive randomization with heavy-tailed outcomes</title>
      <link>https://arxiv.org/abs/2407.05001</link>
      <description>arXiv:2407.05001v1 Announce Type: new 
Abstract: Randomized experiments are the gold standard for investigating causal relationships, with comparisons of potential outcomes under different treatment groups used to estimate treatment effects. However, outcomes with heavy-tailed distributions pose significant challenges to traditional statistical approaches. While recent studies have explored these issues under simple randomization, their application in more complex randomization designs, such as stratified randomization or covariate-adaptive randomization, has not been adequately addressed. To fill the gap, this paper examines the properties of the estimated influence function-based M-estimator under covariate-adaptive randomization with heavy-tailed outcomes, demonstrating its consistency and asymptotic normality. Yet, the existing variance estimator tends to overestimate the asymptotic variance, especially under more balanced designs, and lacks universal applicability across randomization methods. To remedy this, we introduce a novel stratified transformed difference-in-means estimator to enhance efficiency and propose a universally applicable variance estimator to facilitate valid inferences. Additionally, we establish the consistency of kernel-based density estimation in the context of covariate-adaptive randomization. Numerical results demonstrate the effectiveness of the proposed methods in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05001v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongzi Li, Wei Ma, Yingying Ma, Hanzhong Liu</dc:creator>
    </item>
    <item>
      <title>Bayesian network-guided sparse regression with flexible varying effects</title>
      <link>https://arxiv.org/abs/2407.05089</link>
      <description>arXiv:2407.05089v1 Announce Type: new 
Abstract: In this paper, we propose Varying Effects Regression with Graph Estimation (VERGE), a novel Bayesian method for feature selection in regression. Our model has key aspects that allow it to leverage the complex structure of data sets arising from genomics or imaging studies. We distinguish between the predictors, which are the features utilized in the outcome prediction model, and the subject-level covariates, which modulate the effects of the predictors on the outcome. We construct a varying coefficients modeling framework where we infer a network among the predictor variables and utilize this network information to encourage the selection of related predictors. We employ variable selection spike-and-slab priors that enable the selection of both network-linked predictor variables and covariates that modify the predictor effects. We demonstrate through simulation studies that our method outperforms existing alternative methods in terms of both feature selection and predictive accuracy. We illustrate VERGE with an application to characterizing the influence of gut microbiome features on obesity, where we identify a set of microbial taxa and their ecological dependence relations. We allow subject-level covariates including sex and dietary intake variables to modify the coefficients of the microbiome predictors, providing additional insight into the interplay between these factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05089v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yangfan Ren, Christine B. Peterson, Marina Vannucci</dc:creator>
    </item>
    <item>
      <title>Roughness regularization for functional data analysis with free knots spline estimation</title>
      <link>https://arxiv.org/abs/2407.05159</link>
      <description>arXiv:2407.05159v1 Announce Type: new 
Abstract: In the era of big data, an ever-growing volume of information is recorded, either continuously over time or sporadically, at distinct time intervals. Functional Data Analysis (FDA) stands at the cutting edge of this data revolution, offering a powerful framework for handling and extracting meaningful insights from such complex datasets. The currently proposed FDA me\-thods can often encounter challenges, especially when dealing with curves of varying shapes. This can largely be attributed to the method's strong dependence on data approximation as a key aspect of the analysis process. In this work, we propose a free knots spline estimation method for functional data with two penalty terms and demonstrate its performance by comparing the results of several clustering methods on simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05159v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna De Magistris (University of Campania "Luigi Vanvitelli"), Valentina De Simone (University of Campania "Luigi Vanvitelli"), Elvira Romano (University of Campania "Luigi Vanvitelli"), Gerardo Toraldo (University of Campania "Luigi Vanvitelli")</dc:creator>
    </item>
    <item>
      <title>Joint identification of spatially variable genes via a network-assisted Bayesian regularization approach</title>
      <link>https://arxiv.org/abs/2407.05241</link>
      <description>arXiv:2407.05241v1 Announce Type: new 
Abstract: Identifying genes that display spatial patterns is critical to investigating expression interactions within a spatial context and further dissecting biological understanding of complex mechanistic functionality. Despite the increase in statistical methods designed to identify spatially variable genes, they are mostly based on marginal analysis and share the limitation that the dependence (network) structures among genes are not well accommodated, where a biological process usually involves changes in multiple genes that interact in a complex network. Moreover, the latent cellular composition within spots may introduce confounding variations, negatively affecting identification accuracy. In this study, we develop a novel Bayesian regularization approach for spatial transcriptomic data, with the confounding variations induced by varying cellular distributions effectively corrected. Significantly advancing from the existing studies, a thresholded graph Laplacian regularization is proposed to simultaneously identify spatially variable genes and accommodate the network structure among genes. The proposed method is based on a zero-inflated negative binomial distribution, effectively accommodating the count nature, zero inflation, and overdispersion of spatial transcriptomic data. Extensive simulations and the application to real data demonstrate the competitive performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05241v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingcong Wu, Yang Li, Shuangge Ma, Mengyun Wu</dc:creator>
    </item>
    <item>
      <title>Efficient Bayesian dynamic closed skew-normal model preserving mean and covariance for spatio-temporal data</title>
      <link>https://arxiv.org/abs/2407.05288</link>
      <description>arXiv:2407.05288v1 Announce Type: new 
Abstract: Although Bayesian skew-normal models are useful for flexibly modeling spatio-temporal processes, they still have difficulty in computation cost and interpretability in their mean and variance parameters, including regression coefficients. To address these problems, this study proposes a spatio-temporal model that incorporates skewness while maintaining mean and variance, by applying the flexible subclass of the closed skew-normal distribution. An efficient sampling method is introduced, leveraging the autoregressive representation of the model. Additionally, the model's symmetry concerning spatial order is demonstrated, and Mardia's skewness and kurtosis are derived, showing independence from the mean and variance. Simulation studies compare the estimation performance of the proposed model with that of the Gaussian model. The result confirms its superiority in high skewness and low observation noise scenarios. The identification of Cobb-Douglas production functions across US states is examined as an application to real data, revealing that the proposed model excels in both goodness-of-fit and predictive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05288v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hajime Kuno, Daisuke Murakami</dc:creator>
    </item>
    <item>
      <title>Collaborative Analysis for Paired A/B Testing Experiments</title>
      <link>https://arxiv.org/abs/2407.05400</link>
      <description>arXiv:2407.05400v1 Announce Type: new 
Abstract: With the extensive use of digital devices, online experimental platforms are commonly used to conduct experiments to collect data for evaluating different variations of products, algorithms, and interface designs, a.k.a., A/B tests. In practice, multiple A/B testing experiments are often carried out based on a common user population on the same platform. The same user's responses to different experiments can be correlated to some extent due to the individual effect of the user. In this paper, we propose a novel framework that collaboratively analyzes the data from paired A/B tests, namely, a pair of A/B testing experiments conducted on the same set of experimental subjects. The proposed analysis approach for paired A/B tests can lead to more accurate estimates than the traditional separate analysis of each experiment. We obtain the asymptotic distribution of the proposed estimators and demonstrate that the proposed estimators are asymptotically the best linear unbiased estimators under certain assumptions. Moreover, the proposed analysis approach is computationally efficient, easy to implement, and robust to different types of responses. Both numerical simulations and numerical studies based on a real case are used to examine the performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05400v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiong Zhang, Lulu Kang, Xinwei Deng</dc:creator>
    </item>
    <item>
      <title>Without Pain -- Clustering Categorical Data Using a Bayesian Mixture of Finite Mixtures of Latent Class Analysis Models</title>
      <link>https://arxiv.org/abs/2407.05431</link>
      <description>arXiv:2407.05431v1 Announce Type: new 
Abstract: We propose a Bayesian approach for model-based clustering of multivariate categorical data where variables are allowed to be associated within clusters and the number of clusters is unknown. The approach uses a two-layer mixture of finite mixtures model where the cluster distributions are approximated using latent class analysis models. A careful specification of priors with suitable hyperparameter values is crucial to identify the two-layer structure and obtain a parsimonious cluster solution. We outline the Bayesian estimation based on Markov chain Monte Carlo sampling with the telescoping sampler and describe how to obtain an identified clustering model by resolving the label switching issue. Empirical demonstrations in a simulation study using artificial data as well as a data set on low back pain indicate the good clustering performance of the proposed approach, provided hyperparameters are selected which induce sufficient shrinkage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05431v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gertraud Malsiner-Walli, Bettina Gr\"un, Sylvia Fr\"uhwirth-Schnatter</dc:creator>
    </item>
    <item>
      <title>Bayesian Finite Mixture Models</title>
      <link>https://arxiv.org/abs/2407.05470</link>
      <description>arXiv:2407.05470v1 Announce Type: new 
Abstract: Finite mixture models are a useful statistical model class for clustering and density approximation. In the Bayesian framework finite mixture models require the specification of suitable priors in addition to the data model. These priors allow to avoid spurious results and provide a principled way to define cluster shapes and a preference for specific cluster solutions. A generic model estimation scheme for finite mixtures with a fixed number of components is available using Markov chain Monte Carlo (MCMC) sampling with data augmentation. The posterior allows to assess uncertainty in a comprehensive way, but component-specific posterior inference requires resolving the label switching issue.
  In this paper we focus on the application of Bayesian finite mixture models for clustering. We start with discussing suitable specification, estimation and inference of the model if the number of components is assumed to be known. We then continue to explain suitable strategies for fitting Bayesian finite mixture models when the number of components is not known. In addition, all steps required to perform Bayesian finite mixture modeling are illustrated on a data example where a finite mixture model of multivariate Gaussian distributions is fitted. Suitable prior specification, estimation using MCMC and posterior inference are discussed for this example assuming the number of components to be known as well as unknown.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05470v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/9781118445112.stat08373</arxiv:DOI>
      <dc:creator>Bettina Gr\"un, Gertraud Malsiner-Walli</dc:creator>
    </item>
    <item>
      <title>Optimal treatment strategies for prioritized outcomes</title>
      <link>https://arxiv.org/abs/2407.05537</link>
      <description>arXiv:2407.05537v1 Announce Type: new 
Abstract: Dynamic treatment regimes formalize precision medicine as a sequence of decision rules, one for each stage of clinical intervention, that map current patient information to a recommended intervention. Optimal regimes are typically defined as maximizing some functional of a scalar outcome's distribution, e.g., the distribution's mean or median. However, in many clinical applications, there are multiple outcomes of interest. We consider the problem of estimating an optimal regime when there are multiple outcomes that are ordered by priority but which cannot be readily combined by domain experts into a meaningful single scalar outcome. We propose a definition of optimality in this setting and show that an optimal regime with respect to this definition leads to maximal mean utility under a large class of utility functions. Furthermore, we use inverse reinforcement learning to identify a composite outcome that most closely aligns with our definition within a pre-specified class. Simulation experiments and an application to data from a sequential multiple assignment randomized trial (SMART) on HIV/STI prevention illustrate the usefulness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05537v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyle Duke, Eric B. Laber, Marie Davidian, Michael Newcomb, Brian Mustanksi</dc:creator>
    </item>
    <item>
      <title>Functional Principal Component Analysis for Truncated Data</title>
      <link>https://arxiv.org/abs/2407.05543</link>
      <description>arXiv:2407.05543v1 Announce Type: new 
Abstract: Functional principal component analysis (FPCA) is a key tool in the study of functional data, driving both exploratory analyses and feature construction for use in formal modeling and testing procedures. However, existing methods for FPCA do not apply when functional observations are truncated, e.g., the measurement instrument only supports recordings within a pre-specified interval, thereby truncating values outside of the range to the nearest boundary. A naive application of existing methods without correction for truncation induces bias. We extend the FPCA framework to accommodate truncated noisy functional data by first recovering smooth mean and covariance surface estimates that are representative of the latent process's mean and covariance functions. Unlike traditional sample covariance smoothing techniques, our procedure yields a positive semi-definite covariance surface, computed without the need to retroactively remove negative eigenvalues in the covariance operator decomposition. Additionally, we construct a FPC score predictor and demonstrate its use in the generalized functional linear model. Convergence rates for the proposed estimators are provided. In simulation experiments, the proposed method yields better predictive performance and lower bias than existing alternatives. We illustrate its practical value through an application to a study with truncated blood glucose measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05543v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caitrin Murphy, Eric Laber, Rhonda Merwin, Brian Reich</dc:creator>
    </item>
    <item>
      <title>Unmasking Bias: A Framework for Evaluating Treatment Benefit Predictors Using Observational Studies</title>
      <link>https://arxiv.org/abs/2407.05585</link>
      <description>arXiv:2407.05585v1 Announce Type: new 
Abstract: Treatment benefit predictors (TBPs) map patient characteristics into an estimate of the treatment benefit tailored to individual patients, which can support optimizing treatment decisions. However, the assessment of their performance might be challenging with the non-random treatment assignment. This study conducts a conceptual analysis, which can be applied to finite-sample studies. We present a framework for evaluating TBPs using observational data from a target population of interest. We then explore the impact of confounding bias on TBP evaluation using measures of discrimination and calibration, which are the moderate calibration and the concentration of the benefit index ($C_b$), respectively. We illustrate that failure to control for confounding can lead to misleading values of performance metrics and establish how the confounding bias propagates to an evaluation bias to quantify the explicit bias for the performance metrics. These findings underscore the necessity of accounting for confounding factors when evaluating TBPs, ensuring more reliable and contextually appropriate treatment decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05585v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Xia, Mohsen Sadatsafavi, Paul Gustafson</dc:creator>
    </item>
    <item>
      <title>Dynamic Matrix Factor Models for High Dimensional Time Series</title>
      <link>https://arxiv.org/abs/2407.05624</link>
      <description>arXiv:2407.05624v1 Announce Type: new 
Abstract: Matrix time series, which consist of matrix-valued data observed over time, are prevalent in various fields such as economics, finance, and engineering. Such matrix time series data are often observed in high dimensions. Matrix factor models are employed to reduce the dimensionality of such data, but they lack the capability to make predictions without specified dynamics in the latent factor process. To address this issue, we propose a two-component dynamic matrix factor model that extends the standard matrix factor model by incorporating a matrix autoregressive structure for the low-dimensional latent factor process. This two-component model injects prediction capability to the matrix factor model and provides deeper insights into the dynamics of high-dimensional matrix time series. We present the estimation procedures of the model and their theoretical properties, as well as empirical analysis of the estimation procedures via simulations, and a case study of New York city taxi data, demonstrating the performance and usefulness of the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05624v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruofan Yu, Rong Chen, Han Xiao, Yuefeng Han</dc:creator>
    </item>
    <item>
      <title>New User Event Prediction Through the Lens of Causal Inference</title>
      <link>https://arxiv.org/abs/2407.05625</link>
      <description>arXiv:2407.05625v1 Announce Type: new 
Abstract: Modeling and analysis for event series generated by heterogeneous users of various behavioral patterns are closely involved in our daily lives, including credit card fraud detection, online platform user recommendation, and social network analysis. The most commonly adopted approach to this task is to classify users into behavior-based categories and analyze each of them separately. However, this approach requires extensive data to fully understand user behavior, presenting challenges in modeling newcomers without historical knowledge. In this paper, we propose a novel discrete event prediction framework for new users through the lens of causal inference. Our method offers an unbiased prediction for new users without needing to know their categories. We treat the user event history as the ''treatment'' for future events and the user category as the key confounder. Thus, the prediction problem can be framed as counterfactual outcome estimation, with the new user model trained on an adjusted dataset where each event is re-weighted by its inverse propensity score. We demonstrate the superior performance of the proposed framework with a numerical simulation study and two real-world applications, including Netflix rating prediction and seller contact prediction for customer support at Amazon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05625v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henry Shaowu Yuchi, Shixiang Zhu</dc:creator>
    </item>
    <item>
      <title>Multi-resolution subsampling for large-scale linear classification</title>
      <link>https://arxiv.org/abs/2407.05691</link>
      <description>arXiv:2407.05691v1 Announce Type: new 
Abstract: Subsampling is one of the popular methods to balance statistical efficiency and computational efficiency in the big data era. Most approaches aim at selecting informative or representative sample points to achieve good overall information of the full data. The present work takes the view that sampling techniques are recommended for the region we focus on and summary measures are enough to collect the information for the rest according to a well-designed data partitioning. We propose a multi-resolution subsampling strategy that combines global information described by summary measures and local information obtained from selected subsample points. We show that the proposed method will lead to a more efficient subsample-based estimator for general large-scale classification problems. Some asymptotic properties of the proposed method are established and connections to existing subsampling procedures are explored. Finally, we illustrate the proposed subsampling strategy via simulated and real-world examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05691v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Haolin Chen, Holger Dette, Jun Yu</dc:creator>
    </item>
    <item>
      <title>Counting on count regression: overlooked aspects of the Negative Binomial specification</title>
      <link>https://arxiv.org/abs/2407.05824</link>
      <description>arXiv:2407.05824v1 Announce Type: new 
Abstract: Negative Binomial regression is a staple in Operations Management empirical research. Most of its analytical aspects are considered either self-evident, or minutiae that are better left to specialised textbooks. But what if the evidence provided by trusted sources disagrees? In this note I set out to verify results about the Negative Binomial regression specification presented in widely-cited academic sources. I identify problems in how these sources approach the gamma function and its derivatives, with repercussions on the Fisher Information Matrix that may ultimately affect statistical testing. By elevating computations that are rarely specified in full, I provide recommendations to improve methodological evidence that is typically presented without proof.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05824v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ettore Settanni</dc:creator>
    </item>
    <item>
      <title>Small area prediction of counts under machine learning-type mixed models</title>
      <link>https://arxiv.org/abs/2407.05849</link>
      <description>arXiv:2407.05849v1 Announce Type: new 
Abstract: This paper proposes small area estimation methods that utilize generalized tree-based machine learning techniques to improve the estimation of disaggregated means in small areas using discrete survey data. Specifically, we present two approaches based on random forests: the Generalized Mixed Effects Random Forest (GMERF) and a Mixed Effects Random Forest (MERF), both tailored to address challenges associated with count outcomes, particularly overdispersion. Our analysis reveals that the MERF, which does not assume a Poisson distribution to model the mean behavior of count data, excels in scenarios of severe overdispersion. Conversely, the GMERF performs best under conditions where Poisson distribution assumptions are moderately met. Additionally, we introduce and evaluate three bootstrap methodologies - one parametric and two non-parametric - designed to assess the reliability of point estimators for area-level means. The effectiveness of these methodologies is tested through model-based (and design-based) simulations and applied to a real-world dataset from the state of Guerrero in Mexico, demonstrating their robustness and potential for practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05849v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Frink, Timo Schmid</dc:creator>
    </item>
    <item>
      <title>A Low-Rank Bayesian Approach for Geoadditive Modeling</title>
      <link>https://arxiv.org/abs/2407.05854</link>
      <description>arXiv:2407.05854v1 Announce Type: new 
Abstract: Kriging is an established methodology for predicting spatial data in geostatistics. Current kriging techniques can handle linear dependencies on spatially referenced covariates. Although splines have shown promise in capturing nonlinear dependencies of covariates, their combination with kriging, especially in handling count data, remains underexplored. This paper proposes a novel Bayesian approach to the low-rank representation of geoadditive models, which integrates splines and kriging to account for both spatial correlations and nonlinear dependencies of covariates. The proposed method accommodates Gaussian and count data inherent in many geospatial datasets. Additionally, Laplace approximations to selected posterior distributions enhances computational efficiency, resulting in faster computation times compared to Markov chain Monte Carlo techniques commonly used for Bayesian inference. Method performance is assessed through a simulation study, demonstrating the effectiveness of the proposed approach. The methodology is applied to the analysis of heavy metal concentrations in the Meuse river and vulnerability to the coronavirus disease 2019 (COVID-19) in Belgium. Through this work, we provide a new flexible and computationally efficient framework for analyzing spatial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05854v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bryan Sumalinab, Oswaldo Gressani, Niel Hens, Christel Faes</dc:creator>
    </item>
    <item>
      <title>A new multivariate Poisson model</title>
      <link>https://arxiv.org/abs/2407.05896</link>
      <description>arXiv:2407.05896v1 Announce Type: new 
Abstract: Multi-dimensional data frequently occur in many different fields, including risk management, insurance, biology, environmental sciences, and many more. In analyzing multivariate data, it is imperative that the underlying modelling assumptions adequately reflect both the marginal behavior as well as the associations between components. This work focuses specifically on developing a new multivariate Poisson model appropriate for multi-dimensional count data. The proposed formulation is based on convolutions of comonotonic shock vectors with Poisson distributed components and allows for flexibility in capturing different degrees of positive dependence. In this paper, the general model framework will be presented along with various distributional properties. Several estimation techniques will be explored and assessed both through simulations and in a real data application involving extreme rainfall events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05896v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Orla A. Murphy, Juliana Schulz</dc:creator>
    </item>
    <item>
      <title>Constructing Level Sets Using Smoothed Approximate Bayesian Computation</title>
      <link>https://arxiv.org/abs/2407.05914</link>
      <description>arXiv:2407.05914v1 Announce Type: new 
Abstract: This paper presents a novel approach to level set estimation for any function/simulation with an arbitrary number of continuous inputs and arbitrary numbers of continuous responses. We present a method that uses existing data from computer model simulations to fit a Gaussian process surrogate and use a newly proposed Markov Chain Monte Carlo technique, which we refer to as Smoothed Approximate Bayesian Computation to sample sets of parameters that yield a desired response, which improves on ``hard-clipped" versions of ABC. We prove that our method converges to the correct distribution (i.e. the posterior distribution of level sets, or probability contours) and give results of our method on known functions and a dam breach simulation where the relationship between input parameters and responses of interest is unknown. Two versions of S-ABC are offered based on: 1) surrogating an accurately known target model and 2) surrogating an approximate model, which leads to uncertainty in estimating the level sets. In addition, we show how our method can be extended to multiple responses with an accompanying example. As demonstrated, S-ABC is able to estimate a level set accurately without the use of a predefined grid or signed distance function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05914v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Edwards, Julie Bessac, Franck Cappello, Scotland Leman</dc:creator>
    </item>
    <item>
      <title>A likelihood ratio test for circular multimodality</title>
      <link>https://arxiv.org/abs/2407.05957</link>
      <description>arXiv:2407.05957v1 Announce Type: new 
Abstract: The modes of a statistical population are high frequency points around which most of the probability mass is accumulated. For the particular case of circular densities, we address the problem of testing if, given an observed sample of a random angle, the underlying circular distribution model is multimodal. Our work is motivated by the analysis of migration patterns of birds and the methodological proposal follows a novel approach based on likelihood ratio ideas, combined with critical bandwidths. Theoretical results support the behaviour of the test, whereas simulation examples show its finite sample performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05957v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diego Bol\'on, Rosa M. Crujeiras, Alberto Rodr\'iguez-Casal</dc:creator>
    </item>
    <item>
      <title>Comparing Causal Inference Methods for Point Exposures with Missing Confounders: A Simulation Study</title>
      <link>https://arxiv.org/abs/2407.06038</link>
      <description>arXiv:2407.06038v1 Announce Type: new 
Abstract: Causal inference methods based on electronic health record (EHR) databases must simultaneously handle confounding and missing data. Vast scholarship exists aimed at addressing these two issues separately, but surprisingly few papers attempt to address them simultaneously. In practice, when faced with simultaneous missing data and confounding, analysts may proceed by first imputing missing data and subsequently using outcome regression or inverse-probability weighting (IPW) to address confounding. However, little is known about the theoretical performance of such $\textit{ad hoc}$ methods. In a recent paper Levis $\textit{et al.}$ outline a robust framework for tackling these problems together under certain identifying conditions, and introduce a pair of estimators for the average treatment effect (ATE), one of which is non-parametric efficient. In this work we present a series of simulations, motivated by a published EHR based study of the long-term effects of bariatric surgery on weight outcomes, to investigate these new estimators and compare them to existing $\textit{ad hoc}$ methods. While the latter perform well in certain scenarios, no single estimator is uniformly best. As such, the work of Levis $\textit{et al.}$ may serve as a reasonable default for causal inference when handling confounding and missing data together.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06038v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Benz, Alexander Levis, Sebastien Haneuse</dc:creator>
    </item>
    <item>
      <title>How to Add Baskets to an Ongoing Basket Trial with Information Borrowing</title>
      <link>https://arxiv.org/abs/2407.06069</link>
      <description>arXiv:2407.06069v1 Announce Type: new 
Abstract: Basket trials test a single therapeutic treatment on several patient populations under one master protocol. A desirable adaptive design feature in these studies may be the incorporation of new baskets to an ongoing study. Limited basket sample sizes can cause issues in power and precision of treatment effect estimates which could be amplified in added baskets due to the shortened recruitment time. While various Bayesian information borrowing techniques have been introduced to tackle the issue of small sample sizes, the impact of including new baskets in the trial and into the borrowing model has yet to be investigated. We explore approaches for adding baskets to an ongoing trial under information borrowing and highlight when it is beneficial to add a basket compared to running a separate investigation for new baskets. We also propose a novel calibration approach for the decision criteria that is more robust to false decision making. Simulation studies are conducted to assess the performance of approaches which is monitored primarily through type I error control and precision of estimates. Results display a substantial improvement in power for a new basket when information borrowing is utilized, however, this comes with potential inflation of error rates which can be shown to be reduced under the proposed calibration procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06069v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Libby Daniells, Pavel Mozgunov, Helen Barnett, Alun Bedding, Thomas Jaki</dc:creator>
    </item>
    <item>
      <title>Large Row-Constrained Supersaturated Designs for High-throughput Screening</title>
      <link>https://arxiv.org/abs/2407.06173</link>
      <description>arXiv:2407.06173v1 Announce Type: new 
Abstract: High-throughput screening, in which multiwell plates are used to test large numbers of compounds against specific targets, is widely used across many areas of the biological sciences and most prominently in drug discovery. We propose a statistically principled approach to these screening experiments, using the machinery of supersaturated designs and the Lasso. To accommodate limitations on the number of biological entities that can be applied to a single microplate well, we present a new class of row-constrained supersaturated designs. We develop a computational procedure to construct these designs, provide some initial lower bounds on the average squared off-diagonal values of their main-effects information matrix, and study the impact of the constraint on design quality. We also show via simulation that the proposed constrained row screening method is statistically superior to existing methods and demonstrate the use of the new methodology on a real drug-discovery system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06173v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Byran J. Smucker, Stephen E. Wright, Isaac Williams, Richard C. Page, Andor J. Kiss, Surendra Bikram Silwal, Maria Weese, David J. Edwards</dc:creator>
    </item>
    <item>
      <title>Enabling Causal Discovery in Post-Nonlinear Models with Normalizing Flows</title>
      <link>https://arxiv.org/abs/2407.04980</link>
      <description>arXiv:2407.04980v1 Announce Type: cross 
Abstract: Post-nonlinear (PNL) causal models stand out as a versatile and adaptable framework for modeling intricate causal relationships. However, accurately capturing the invertibility constraint required in PNL models remains challenging in existing studies. To address this problem, we introduce CAF-PoNo (Causal discovery via Normalizing Flows for Post-Nonlinear models), harnessing the power of the normalizing flows architecture to enforce the crucial invertibility constraint in PNL models. Through normalizing flows, our method precisely reconstructs the hidden noise, which plays a vital role in cause-effect identification through statistical independence testing. Furthermore, the proposed approach exhibits remarkable extensibility, as it can be seamlessly expanded to facilitate multivariate causal discovery via causal order identification, empowering us to efficiently unravel complex causal relationships. Extensive experimental evaluations on both simulated and real datasets consistently demonstrate that the proposed method outperforms several state-of-the-art approaches in both bivariate and multivariate causal discovery tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04980v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nu Hoang, Bao Duong, Thin Nguyen</dc:creator>
    </item>
    <item>
      <title>Scalable Variational Causal Discovery Unconstrained by Acyclicity</title>
      <link>https://arxiv.org/abs/2407.04992</link>
      <description>arXiv:2407.04992v1 Announce Type: cross 
Abstract: Bayesian causal discovery offers the power to quantify epistemic uncertainties among a broad range of structurally diverse causal theories potentially explaining the data, represented in forms of directed acyclic graphs (DAGs). However, existing methods struggle with efficient DAG sampling due to the complex acyclicity constraint. In this study, we propose a scalable Bayesian approach to effectively learn the posterior distribution over causal graphs given observational data thanks to the ability to generate DAGs without explicitly enforcing acyclicity. Specifically, we introduce a novel differentiable DAG sampling method that can generate a valid acyclic causal graph by mapping an unconstrained distribution of implicit topological orders to a distribution over DAGs. Given this efficient DAG sampling scheme, we are able to model the posterior distribution over causal graphs using a simple variational distribution over a continuous domain, which can be learned via the variational inference framework. Extensive empirical experiments on both simulated and real datasets demonstrate the superior performance of the proposed model compared to several state-of-the-art baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04992v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nu Hoang, Bao Duong, Thin Nguyen</dc:creator>
    </item>
    <item>
      <title>Fast Proxy Experiment Design for Causal Effect Identification</title>
      <link>https://arxiv.org/abs/2407.05330</link>
      <description>arXiv:2407.05330v1 Announce Type: cross 
Abstract: Identifying causal effects is a key problem of interest across many disciplines. The two long-standing approaches to estimate causal effects are observational and experimental (randomized) studies. Observational studies can suffer from unmeasured confounding, which may render the causal effects unidentifiable. On the other hand, direct experiments on the target variable may be too costly or even infeasible to conduct. A middle ground between these two approaches is to estimate the causal effect of interest through proxy experiments, which are conducted on variables with a lower cost to intervene on compared to the main target. Akbari et al. [2022] studied this setting and demonstrated that the problem of designing the optimal (minimum-cost) experiment for causal effect identification is NP-complete and provided a naive algorithm that may require solving exponentially many NP-hard problems as a sub-routine in the worst case. In this work, we provide a few reformulations of the problem that allow for designing significantly more efficient algorithms to solve it as witnessed by our extensive simulations. Additionally, we study the closely-related problem of designing experiments that enable us to identify a given effect through valid adjustments sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05330v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sepehr Elahi, Sina Akbari, Jalal Etesami, Negar Kiyavash, Patrick Thiran</dc:creator>
    </item>
    <item>
      <title>A Convexified Matching Approach to Imputation and Individualized Inference</title>
      <link>https://arxiv.org/abs/2407.05372</link>
      <description>arXiv:2407.05372v1 Announce Type: cross 
Abstract: We introduce a new convexified matching method for missing value imputation and individualized inference inspired by computational optimal transport. Our method integrates favorable features from mainstream imputation approaches: optimal matching, regression imputation, and synthetic control. We impute counterfactual outcomes based on convex combinations of observed outcomes, defined based on an optimal coupling between the treated and control data sets. The optimal coupling problem is considered a convex relaxation to the combinatorial optimal matching problem. We estimate granular-level individual treatment effects while maintaining a desirable aggregate-level summary by properly constraining the coupling. We construct transparent, individual confidence intervals for the estimated counterfactual outcomes. We devise fast iterative entropic-regularized algorithms to solve the optimal coupling problem that scales favorably when the number of units to match is large. Entropic regularization plays a crucial role in both inference and computation; it helps control the width of the individual confidence intervals and design fast optimization algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05372v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>YoonHaeng Hur, Tengyuan Liang</dc:creator>
    </item>
    <item>
      <title>Gaussian Approximation and Output Analysis for High-Dimensional MCMC</title>
      <link>https://arxiv.org/abs/2407.05492</link>
      <description>arXiv:2407.05492v1 Announce Type: cross 
Abstract: The widespread use of Markov Chain Monte Carlo (MCMC) methods for high-dimensional applications has motivated research into the scalability of these algorithms with respect to the dimension of the problem. Despite this, numerous problems concerning output analysis in high-dimensional settings have remained unaddressed. We present novel quantitative Gaussian approximation results for a broad range of MCMC algorithms. Notably, we analyse the dependency of the obtained approximation errors on the dimension of both the target distribution and the feature space. We demonstrate how these Gaussian approximations can be applied in output analysis. This includes determining the simulation effort required to guarantee Markov chain central limit theorems and consistent variance estimation in high-dimensional settings. We give quantitative convergence bounds for termination criteria and show that the termination time of a wide class of MCMC algorithms scales polynomially in dimension while ensuring a desired level of precision. Our results offer guidance to practitioners for obtaining appropriate standard errors and deciding the minimum simulation effort of MCMC algorithms in both multivariate and high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05492v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ardjen Pengel, Jun Yang, Zhou Zhou</dc:creator>
    </item>
    <item>
      <title>Causality-driven Sequence Segmentation for Enhancing Multiphase Industrial Process Data Analysis and Soft Sensing</title>
      <link>https://arxiv.org/abs/2407.05954</link>
      <description>arXiv:2407.05954v1 Announce Type: cross 
Abstract: The dynamic characteristics of multiphase industrial processes present significant challenges in the field of industrial big data modeling. Traditional soft sensing models frequently neglect the process dynamics and have difficulty in capturing transient phenomena like phase transitions. To address this issue, this article introduces a causality-driven sequence segmentation (CDSS) model. This model first identifies the local dynamic properties of the causal relationships between variables, which are also referred to as causal mechanisms. It then segments the sequence into different phases based on the sudden shifts in causal mechanisms that occur during phase transitions. Additionally, a novel metric, similarity distance, is designed to evaluate the temporal consistency of causal mechanisms, which includes both causal similarity distance and stable similarity distance. The discovered causal relationships in each phase are represented as a temporal causal graph (TCG). Furthermore, a soft sensing model called temporal-causal graph convolutional network (TC-GCN) is trained for each phase, by using the time-extended data and the adjacency matrix of TCG. The numerical examples are utilized to validate the proposed CDSS model, and the segmentation results demonstrate that CDSS has excellent performance on segmenting both stable and unstable multiphase series. Especially, it has higher accuracy in separating non-stationary time series compared to other methods. The effectiveness of the proposed CDSS model and the TC-GCN model is also verified through a penicillin fermentation process. Experimental results indicate that the breakpoints discovered by CDSS align well with the reaction mechanisms and TC-GCN significantly has excellent predictive accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05954v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yimeng He, Le Yao, Xinmin Zhang, Xiangyin Kong, Zhihuan Song</dc:creator>
    </item>
    <item>
      <title>Community Correlations and Testing Independence Between Binary Graphs</title>
      <link>https://arxiv.org/abs/1906.03661</link>
      <description>arXiv:1906.03661v3 Announce Type: replace 
Abstract: Graph data has a unique structure that deviates from standard data assumptions, often necessitating modifications to existing methods or the development of new ones to ensure valid statistical analysis. In this paper, we explore the notion of correlation and dependence between two binary graphs. Given vertex communities, we propose community correlations to measure the edge association, which equals zero if and only if the two graphs are conditionally independent within a specific pair of communities. The set of community correlations naturally leads to the maximum community correlation, indicating conditional independence on all possible pairs of communities, and to the overall graph correlation, which equals zero if and only if the two binary graphs are unconditionally independent. We then compute the sample community correlations via graph encoder embedding, proving they converge to their respective population versions, and derive the asymptotic null distribution to enable a fast, valid, and consistent test for conditional or unconditional independence between two binary graphs. The theoretical results are validated through comprehensive simulations, and we provide two real-data examples: one using Enron email networks and another using mouse connectome graphs, to demonstrate the utility of the proposed correlation measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:1906.03661v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cencheng Shen, Jes\"us Arroyo, Junhao Xiong, Joshua T. Vogelstein</dc:creator>
    </item>
    <item>
      <title>Estimating spatially varying health effects of wildland fire smoke using mobile health data</title>
      <link>https://arxiv.org/abs/2005.12017</link>
      <description>arXiv:2005.12017v3 Announce Type: replace 
Abstract: Wildland fire smoke exposures are an increasing threat to public health, and thus there is a growing need for studying the effects of protective behaviors on reducing health outcomes. Emerging smartphone applications provide unprecedented opportunities to deliver health risk communication messages to a large number of individuals when and where they experience the exposure and subsequently study the effectiveness, but also pose novel methodological challenges. Smoke Sense, a citizen science project, provides an interactive smartphone app platform for participants to engage with information about air quality and ways to protect their health and record their own health symptoms and actions taken to reduce smoke exposure. We propose a new, doubly robust estimator of the structural nested mean model parameter that accounts for spatially- and time-varying effects via a local estimating equation approach with geographical kernel weighting. Moreover, our analytical framework is flexible enough to handle informative missingness by inverse probability weighting of estimating functions. We evaluate the new method using extensive simulation studies and apply it to Smoke Sense data reported by the citizen scientists to increase the knowledge base about the relationship between health preventive measures and improved health outcomes. Our results estimate how the protective behaviors effects vary over space and time and find that protective behaviors have more significant effects on reducing health symptoms in the Southwest than the Northwest region of the USA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2005.12017v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lili Wu, Chenyin Gao, Shu Yang, Brian J. Reich, Ana G. Rappold</dc:creator>
    </item>
    <item>
      <title>Bayesian and Frequentist Inference for Synthetic Controls</title>
      <link>https://arxiv.org/abs/2206.01779</link>
      <description>arXiv:2206.01779v3 Announce Type: replace 
Abstract: The synthetic control method has become a widely popular tool to estimate causal effects with observational data. Despite this, inference for synthetic control methods remains challenging. Often, inferential results rely on linear factor model data generating processes. In this paper, we characterize the conditions on the factor model primitives (the factor loadings) for which the statistical risk minimizers are synthetic controls (in the simplex). Then, we propose a Bayesian alternative to the synthetic control method that preserves the main features of the standard method and provides a new way of doing valid inference. We explore a Bernstein-von Mises style result to link our Bayesian inference to the frequentist inference. For linear factor model frameworks we show that a maximum likelihood estimator (MLE) of the synthetic control weights can consistently estimate the predictive function of the potential outcomes for the treated unit and that our Bayes estimator is asymptotically close to the MLE in the total variation sense. Through simulations, we show that there is convergence between the Bayes and frequentist approach even in sparse settings. Finally, we apply the method to re-visit the study of the economic costs of the German re-unification and the Catalan secession movement. The Bayesian synthetic control method is available in the bsynth R-package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.01779v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ignacio Martinez, Jaume Vives-i-Bastida</dc:creator>
    </item>
    <item>
      <title>Visibility graph-based covariance functions for scalable spatial analysis in non-convex domains</title>
      <link>https://arxiv.org/abs/2307.11941</link>
      <description>arXiv:2307.11941v4 Announce Type: replace 
Abstract: We present a new method for constructing valid covariance functions of Gaussian processes for spatial analysis in irregular, non-convex domains such as bodies of water. Standard covariance functions based on geodesic distances are not guaranteed to be positive definite on such domains, while existing non-Euclidean approaches fail to respect the partially Euclidean nature of these domains where the geodesic distance agrees with the Euclidean distances for some pairs of points. Using a visibility graph on the domain, we propose a class of covariance functions that preserve Euclidean-based covariances between points that are connected in the domain while incorporating the non-convex geometry of the domain via conditional independence relationships. We show that the proposed method preserves the partially Euclidean nature of the intrinsic geometry on the domain while maintaining validity (positive definiteness) and marginal stationarity of the covariance function over the entire parameter space, properties which are not always fulfilled by existing approaches to construct covariance functions on non-convex domains. We provide useful approximations to improve computational efficiency, resulting in a scalable algorithm. We compare the performance of our method with those of competing state-of-the-art methods using simulation studies on synthetic non-convex domains. The method is applied to data regarding acidity levels in the Chesapeake Bay, showing its potential for ecological monitoring in real-world spatial applications on irregular domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11941v4</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Gilbert, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>Reconciling Functional Data Regression with Excess Bases</title>
      <link>https://arxiv.org/abs/2308.01724</link>
      <description>arXiv:2308.01724v2 Announce Type: replace 
Abstract: As the development of measuring instruments and computers has accelerated the collection of massive amounts of data, functional data analysis (FDA) has experienced a surge of attention. The FDA methodology treats longitudinal data as a set of functions on which inference, including regression, is performed. Functionalizing data typically involves fitting the data with basis functions. In general, the number of basis functions smaller than the sample size is selected. This paper casts doubt on this convention. Recent statistical theory has revealed the so-called double-descent phenomenon in which excess parameters overcome overfitting and lead to precise interpolation. Applying this idea to choosing the number of bases to be used for functional data, we show that choosing an excess number of bases can lead to more accurate predictions. Specifically, we explored this phenomenon in a functional regression context and examined its validity through numerical experiments. In addition, we introduce two real-world datasets to demonstrate that the double-descent phenomenon goes beyond theoretical and numerical experiments, confirming its importance in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01724v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoya Wakayama, Hidetoshi Matsui</dc:creator>
    </item>
    <item>
      <title>Consistency of common spatial estimators under spatial confounding</title>
      <link>https://arxiv.org/abs/2308.12181</link>
      <description>arXiv:2308.12181v3 Announce Type: replace 
Abstract: This paper addresses the asymptotic performance of popular spatial regression estimators on the task of estimating the linear effect of an exposure on an outcome under "spatial confounding" -- the presence of an unmeasured spatially-structured variable influencing both the exposure and the outcome. The existing literature on spatial confounding is informal and inconsistent; this paper is an attempt to bring clarity through rigorous results on the asymptotic bias and consistency of estimators from popular spatial regression models. We consider two data generation processes: one where the confounder is a fixed function of space and one where it is a random function (i.e., a stochastic process on the spatial domain). We first show that the estimators from ordinary least squares (OLS) and restricted spatial regression are asymptotically biased under spatial confounding. We then prove a novel main result on the consistency of the generalized least squares (GLS) estimator using a Gaussian process (GP) covariance matrix in the presence of spatial confounding under in-fill (fixed domain) asymptotics. The result holds under very general conditions -- for any exposure with some non-spatial variation (noise), for any spatially continuous confounder, using any choice of Mat\'ern or square exponential Gaussian process covariance used to construct the GLS estimator, and without requiring Gaussianity of errors. Finally, we prove that spatial estimators from GLS, GP regression, and spline models that are consistent under confounding by a fixed function will also be consistent under confounding by a random function. We conclude that, contrary to much of the literature on spatial confounding, traditional spatial estimators are capable of estimating linear exposure effects under spatial confounding in the presence of some noise in the exposure. We support our theoretical arguments with simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12181v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Gilbert, Elizabeth L. Ogburn, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>Model-based causal feature selection for general response types</title>
      <link>https://arxiv.org/abs/2309.12833</link>
      <description>arXiv:2309.12833v4 Announce Type: replace 
Abstract: Discovering causal relationships from observational data is a fundamental yet challenging task. Invariant causal prediction (ICP, Peters et al., 2016) is a method for causal feature selection which requires data from heterogeneous settings and exploits that causal models are invariant. ICP has been extended to general additive noise models and to nonparametric settings using conditional independence tests. However, the latter often suffer from low power (or poor type I error control) and additive noise models are not suitable for applications in which the response is not measured on a continuous scale, but reflects categories or counts. Here, we develop transformation-model (TRAM) based ICP, allowing for continuous, categorical, count-type, and uninformatively censored responses (these model classes, generally, do not allow for identifiability when there is no exogenous heterogeneity). As an invariance test, we propose TRAM-GCM based on the expected conditional covariance between environments and score residuals with uniform asymptotic level guarantees. For the special case of linear shift TRAMs, we also consider TRAM-Wald, which tests invariance based on the Wald statistic. We provide an open-source R package 'tramicp' and evaluate our approach on simulated data and in a case study investigating causal features of survival in critically ill patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.12833v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Kook, Sorawit Saengkyongam, Anton Rask Lundborg, Torsten Hothorn, Jonas Peters</dc:creator>
    </item>
    <item>
      <title>Estimating a function of the scale parameter in a gamma distribution with bounded variance</title>
      <link>https://arxiv.org/abs/2310.18858</link>
      <description>arXiv:2310.18858v2 Announce Type: replace 
Abstract: Given a gamma population with known shape parameter $\alpha$, we develop a general theory for estimating a function $g(\cdot)$ of the scale parameter $\beta$ with bounded variance. We begin by defining a sequential sampling procedure with $g(\cdot)$ satisfying some desired condition in proposing the stopping rule, and show the procedure enjoys appealing asymptotic properties. After these general conditions, we substitute $g(\cdot)$ with specific functions including the gamma mean, the gamma variance, the gamma rate parameter, and a gamma survival probability as four possible illustrations. For each illustration, Monte Carlo simulations are carried out to justify the remarkable performance of our proposed sequential procedure. This is further substantiated with a real data study on weights of newly born babies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18858v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jun Hu, Ibtihal Alanazi, Zhe Wang</dc:creator>
    </item>
    <item>
      <title>Constrained least squares simplicial-simplicial regression</title>
      <link>https://arxiv.org/abs/2403.19835</link>
      <description>arXiv:2403.19835v2 Announce Type: replace 
Abstract: Simplicial-simplicial regression refers to the regression setting where both the responses and predictor variables lie within the simplex space, i.e. they are compositional. For this setting, constrained least squares, where the regression coefficients themselves lie within the simplex, is proposed. The model is transformation-free but the adoption of a power transformation is straightforward, it can treat more than one compositional datasets as predictors and offers the possibility of weights among the simplicial predictors. Among the model's advantages are its ability to treat zeros in a natural way and a highly computationally efficient algorithm to estimate its coefficients. Resampling based hypothesis testing procedures are employed regarding inference, such as linear independence, and equality of the regression coefficients to some pre-specified values. The performance of the proposed technique and its comparison to an existing methodology that is of the same spirit takes place using simulation studies and real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19835v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Tsagris</dc:creator>
    </item>
    <item>
      <title>Analysis of Linked Files: A Missing Data Perspective</title>
      <link>https://arxiv.org/abs/2406.14717</link>
      <description>arXiv:2406.14717v3 Announce Type: replace 
Abstract: In many applications, researchers seek to identify overlapping entities across multiple data files. Record linkage algorithms facilitate this task, in the absence of unique identifiers. As these algorithms rely on semi-identifying information, they may miss records that represent the same entity, or incorrectly link records that do not represent the same entity. Analysis of linked files commonly ignores such linkage errors, resulting in biased, or overly precise estimates of the associations of interest. We view record linkage as a missing data problem, and delineate the linkage mechanisms that underpin analysis methods with linked files. Following the missing data literature, we group these methods under three categories: likelihood and Bayesian methods, imputation methods, and weighting methods. We summarize the assumptions and limitations of the methods, and evaluate their performance in a wide range of simulation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14717v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gauri Kamat, Roee Gutman</dc:creator>
    </item>
    <item>
      <title>Statistical ranking with dynamic covariates</title>
      <link>https://arxiv.org/abs/2406.16507</link>
      <description>arXiv:2406.16507v2 Announce Type: replace 
Abstract: We consider a covariate-assisted ranking model grounded in the Plackett--Luce framework. Unlike existing works focusing on pure covariates or individual effects with fixed covariates, our approach integrates individual effects with dynamic covariates. This added flexibility enhances realistic ranking yet poses significant challenges for analyzing the associated estimation procedures. This paper makes an initial attempt to address these challenges. We begin by discussing the sufficient and necessary condition for the model's identifiability. We then introduce an efficient alternating maximization algorithm to compute the maximum likelihood estimator (MLE). Under suitable assumptions on the topology of comparison graphs and dynamic covariates, we establish a quantitative uniform consistency result for the MLE with convergence rates characterized by the asymptotic graph connectivity. The proposed graph topology assumption holds for several popular random graph models under optimal leading-order sparsity conditions. A comprehensive numerical study is conducted to corroborate our theoretical findings and demonstrate the application of the proposed model to real-world datasets, including horse racing and tennis competitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16507v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pinjun Dong, Ruijian Han, Binyan Jiang, Yiming Xu</dc:creator>
    </item>
    <item>
      <title>Robust CATE Estimation Using Novel Ensemble Methods</title>
      <link>https://arxiv.org/abs/2407.03690</link>
      <description>arXiv:2407.03690v2 Announce Type: replace 
Abstract: The estimation of Conditional Average Treatment Effects (CATE) is crucial for understanding the heterogeneity of treatment effects in clinical trials. We evaluate the performance of common methods, including causal forests and various meta-learners, across a diverse set of scenarios revealing that each of the methods fails in one or more of the tested scenarios. Given the inherent uncertainty of the data-generating process in real-life scenarios, the robustness of a CATE estimator to various scenarios is critical for its reliability.
  To address this limitation of existing methods, we propose two new ensemble methods that integrate multiple estimators to enhance prediction stability and performance - Stacked X-Learner which uses the X-Learner with model stacking for estimating the nuisance functions, and Consensus Based Averaging (CBA), which averages only the models with highest internal agreement. We show that these models achieve good performance across a wide range of scenarios varying in complexity, sample size and structure of the underlying-mechanism, including a biologically driven model for PD-L1 inhibition pathway for cancer treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03690v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Oshri Machluf, Tzviel Frostig, Gal Shoham, Tomer Milo, Elad Berkman, Raviv Pryluk</dc:creator>
    </item>
    <item>
      <title>Optimal Sparse Singular Value Decomposition for High-dimensional High-order Data</title>
      <link>https://arxiv.org/abs/1809.01796</link>
      <description>arXiv:1809.01796v2 Announce Type: replace-cross 
Abstract: In this article, we consider the sparse tensor singular value decomposition, which aims for dimension reduction on high-dimensional high-order data with certain sparsity structure. A method named Sparse Tensor Alternating Thresholding for Singular Value Decomposition (STAT-SVD) is proposed. The proposed procedure features a novel double projection \&amp; thresholding scheme, which provides a sharp criterion for thresholding in each iteration. Compared with regular tensor SVD model, STAT-SVD permits more robust estimation under weaker assumptions. Both the upper and lower bounds for estimation accuracy are developed. The proposed procedure is shown to be minimax rate-optimal in a general class of situations. Simulation studies show that STAT-SVD performs well under a variety of configurations. We also illustrate the merits of the proposed procedure on a longitudinal tensor dataset on European country mortality rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:1809.01796v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anru Zhang, Rungang Han</dc:creator>
    </item>
    <item>
      <title>Nonparametric Density Estimation via Variance-Reduced Sketching</title>
      <link>https://arxiv.org/abs/2401.11646</link>
      <description>arXiv:2401.11646v2 Announce Type: replace-cross 
Abstract: Nonparametric density models are of great interest in various scientific and engineering disciplines. Classical density kernel methods, while numerically robust and statistically sound in low-dimensional settings, become inadequate even in moderate higher-dimensional settings due to the curse of dimensionality. In this paper, we introduce a new framework called Variance-Reduced Sketching (VRS), specifically designed to estimate multivariable density functions with a reduced curse of dimensionality. Our framework conceptualizes multivariable functions as infinite-size matrices, and facilitates a new sketching technique motivated by numerical linear algebra literature to reduce the variance in density estimation problems. We demonstrate the robust numerical performance of VRS through a series of simulated experiments and real-world data applications. Notably, VRS shows remarkable improvement over existing neural network estimators and classical kernel methods in numerous density models. Additionally, we offer theoretical justifications for VRS to support its ability to deliver nonparametric density estimation with a reduced curse of dimensionality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11646v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Peng, Yuehaw Khoo, Daren Wang</dc:creator>
    </item>
    <item>
      <title>A Tutorial on Doubly Robust Learning for Causal Inference</title>
      <link>https://arxiv.org/abs/2406.00853</link>
      <description>arXiv:2406.00853v2 Announce Type: replace-cross 
Abstract: Doubly robust learning offers a robust framework for causal inference from observational data by integrating propensity score and outcome modeling. Despite its theoretical appeal, practical adoption remains limited due to perceived complexity and inaccessible software. This tutorial aims to demystify doubly robust methods and demonstrate their application using the EconML package. We provide an introduction to causal inference, discuss the principles of outcome modeling and propensity scores, and illustrate the doubly robust approach through simulated case studies. By simplifying the methodology and offering practical coding examples, we intend to make doubly robust learning accessible to researchers and practitioners in data science and statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00853v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hlynur Dav\'i{\dh} Hlynsson</dc:creator>
    </item>
  </channel>
</rss>
