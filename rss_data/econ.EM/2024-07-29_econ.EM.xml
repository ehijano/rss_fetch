<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Jul 2024 04:00:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Accounting for Nonresponse in Election Polls: Total Margin of Error</title>
      <link>https://arxiv.org/abs/2407.19339</link>
      <description>arXiv:2407.19339v1 Announce Type: new 
Abstract: The potential impact of nonresponse on election polls is well known and frequently acknowledged. Yet measurement and reporting of polling error has focused solely on sampling error, represented by the margin of error of a poll. Survey statisticians have long recommended measurement of the total survey error of a sample estimate by its mean square error (MSE), which jointly measures sampling and non-sampling errors. Extending the conventional language of polling, we think it reasonable to use the square root of maximum MSE to measure the total margin of error. This paper demonstrates how to measure the potential impact of nonresponse using the concept of the total margin of error, which we argue should be a standard feature in the reporting of election poll results. We first show how to jointly measure statistical imprecision and response bias when a pollster lacks any knowledge of the candidate preferences of non-responders. We then extend the analysis to settings where the pollster has partial knowledge that bounds the preferences of non-responders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19339v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeff Dominitz, Charles F. Manski</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Grouping Structures in Panel Data</title>
      <link>https://arxiv.org/abs/2407.19509</link>
      <description>arXiv:2407.19509v1 Announce Type: new 
Abstract: In this paper we examine the existence of heterogeneity within a group, in panels with latent grouping structure. The assumption of within group homogeneity is prevalent in this literature, implying that the formation of groups alleviates cross-sectional heterogeneity, regardless of the prior knowledge of groups. While the latter hypothesis makes inference powerful, it can be often restrictive. We allow for models with richer heterogeneity that can be found both in the cross-section and within a group, without imposing the simple assumption that all groups must be heterogeneous. We further contribute to the method proposed by \cite{su2016identifying}, by showing that the model parameters can be consistently estimated and the groups, while unknown, can be identifiable in the presence of different types of heterogeneity. Within the same framework we consider the validity of assuming both cross-sectional and within group homogeneity, using testing procedures. Simulations demonstrate good finite-sample performance of the approach in both classification and estimation, while empirical applications across several datasets provide evidence of multiple clusters, as well as reject the hypothesis of within group homogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19509v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Katerina Chrysikou, George Kapetanios</dc:creator>
    </item>
    <item>
      <title>Experimenting on Markov Decision Processes with Local Treatments</title>
      <link>https://arxiv.org/abs/2407.19618</link>
      <description>arXiv:2407.19618v1 Announce Type: cross 
Abstract: As service systems grow increasingly complex and dynamic, many interventions become localized, available and taking effect only in specific states. This paper investigates experiments with local treatments on a widely-used class of dynamic models, Markov Decision Processes (MDPs). Particularly, we focus on utilizing the local structure to improve the inference efficiency of the average treatment effect. We begin by demonstrating the efficiency of classical inference methods, including model-based estimation and temporal difference learning under a fixed policy, as well as classical A/B testing with general treatments. We then introduce a variance reduction technique that exploits the local treatment structure by sharing information for states unaffected by the treatment policy. Our new estimator effectively overcomes the variance lower bound for general treatments while matching the more stringent lower bound incorporating the local treatment structure. Furthermore, our estimator can optimally achieve a linear reduction with the number of test arms for a major part of the variance. Finally, we explore scenarios with perfect knowledge of the control arm and design estimators that further improve inference efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19618v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuze Chen, David Simchi-Levi, Chonghuan Wang</dc:creator>
    </item>
    <item>
      <title>Testing for the Asymmetric Optimal Hedge Ratios: With an Application to Bitcoin</title>
      <link>https://arxiv.org/abs/2407.19932</link>
      <description>arXiv:2407.19932v1 Announce Type: cross 
Abstract: Reducing financial risk is of paramount importance to investors, financial institutions, and corporations. Since the pioneering contribution of Johnson (1960), the optimal hedge ratio based on futures is regularly utilized. The current paper suggests an explicit and efficient method for testing the null hypothesis of a symmetric optimal hedge ratio against an asymmetric alternative one within a multivariate setting. If the null is rejected, the position dependent optimal hedge ratios can be estimated via the suggested model. This approach is expected to enhance the accuracy of the implemented hedging strategies compared to the standard methods since it accounts for the fact that the source of risk depends on whether the investor is a buyer or a seller of the risky asset. An application is provided using spot and futures prices of Bitcoin. The results strongly support the view that the optimal hedge ratio for this cryptocurrency is position dependent. The investor that is long in Bitcoin has a much higher conditional optimal hedge ratio compared to the one that is short in the asset. The difference between the two conditional optimal hedge ratios is statistically significant, which has important repercussions for implementing risk management strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19932v1</guid>
      <category>q-fin.RM</category>
      <category>econ.EM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Abdulnasser Hatemi-J</dc:creator>
    </item>
    <item>
      <title>Optimal Pre-Analysis Plans: Statistical Decisions Subject to Implementability</title>
      <link>https://arxiv.org/abs/2208.09638</link>
      <description>arXiv:2208.09638v3 Announce Type: replace 
Abstract: What is the purpose of pre-analysis plans, and how should they be designed? We model the interaction between an agent who analyzes data and a principal who makes a decision based on agent reports. The agent could be the manufacturer of a new drug, and the principal a regulator deciding whether the drug is approved. Or the agent could be a researcher submitting a research paper, and the principal an editor deciding whether it is published. The agent decides which statistics to report to the principal. The principal cannot verify whether the analyst reported selectively. Absent a pre-analysis message, if there are conflicts of interest, then many desirable decision rules cannot be implemented. Allowing the agent to send a message before seeing the data increases the set of decision rules that can be implemented, and allows the principal to leverage agent expertise. The optimal mechanisms that we characterize require pre-analysis plans. Applying these results to hypothesis testing, we show that optimal rejection rules pre-register a valid test, and make worst-case assumptions about unreported statistics. Optimal tests can be found as a solution to a linear-programming problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.09638v3</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Maximilian Kasy, Jann Spiess</dc:creator>
    </item>
    <item>
      <title>Adaptive maximization of social welfare</title>
      <link>https://arxiv.org/abs/2310.09597</link>
      <description>arXiv:2310.09597v2 Announce Type: replace 
Abstract: We consider the problem of repeatedly choosing policies to maximize social welfare. Welfare is a weighted sum of private utility and public revenue. Earlier outcomes inform later policies. Utility is not observed, but indirectly inferred. Response functions are learned through experimentation. We derive a lower bound on regret, and a matching adversarial upper bound for a variant of the Exp3 algorithm. Cumulative regret grows at a rate of $T^{2/3}$. This implies that (i) welfare maximization is harder than the multi-armed bandit problem (with a rate of $T^{1/2}$ for finite policy sets), and (ii) our algorithm achieves the optimal rate. For the stochastic setting, if social welfare is concave, we can achieve a rate of $T^{1/2}$ (for continuous policy sets), using a dyadic search algorithm. We analyze an extension to nonlinear income taxation, and sketch an extension to commodity taxation. We compare our setting to monopoly pricing (which is easier), and price setting for bilateral trade (which is harder).</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09597v2</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nicolo Cesa-Bianchi, Roberto Colomboni, Maximilian Kasy</dc:creator>
    </item>
    <item>
      <title>A Note on Identification of Match Fixed Effects as Interpretable Unobserved Match Affinity</title>
      <link>https://arxiv.org/abs/2406.18913</link>
      <description>arXiv:2406.18913v2 Announce Type: replace 
Abstract: We highlight that match fixed effects, represented by the coefficients of interaction terms involving dummy variables for two elements, lack identification without specific restrictions on parameters. Consequently, the coefficients typically reported as relative match fixed effects by statistical software are not interpretable. To address this, we establish normalization conditions that enable identification of match fixed effect parameters as interpretable indicators of unobserved match affinity, facilitating comparisons among observed matches. Using data from middle school students in the 2007 Trends in International Mathematics and Science Study (TIMSS), we highlight the distribution of comparable match fixed effects within a specific school.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18913v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suguru Otani, Tohya Sugano</dc:creator>
    </item>
    <item>
      <title>Finely Stratified Rerandomization Designs</title>
      <link>https://arxiv.org/abs/2407.03279</link>
      <description>arXiv:2407.03279v2 Announce Type: replace 
Abstract: We study estimation and inference on causal parameters under finely stratified rerandomization designs, which use baseline covariates to match units into groups (e.g. matched pairs), then rerandomize within-group treatment assignments until a balance criterion is satisfied. We show that finely stratified rerandomization does partially linear regression adjustment "by design," providing nonparametric control over the stratified covariates and linear control over the rerandomized covariates. We introduce several new rerandomization schemes, allowing for imbalance metrics based on nonlinear estimators. We also propose a novel minimax scheme that uses pilot data or prior information to minimize the computational cost of rerandomization, subject to a strict bound on statistical efficiency. While the asymptotic distribution of generalized method of moments (GMM) estimators under stratified rerandomization is generically non-normal, we show how to restore asymptotic normality using ex-post linear adjustment tailored to the stratification. This enables simple asymptotically exact inference on superpopulation parameters, as well as efficient conservative inference on finite population parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03279v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Max Cytrynbaum</dc:creator>
    </item>
    <item>
      <title>The Transfer Performance of Economic Models</title>
      <link>https://arxiv.org/abs/2202.04796</link>
      <description>arXiv:2202.04796v4 Announce Type: replace-cross 
Abstract: Economists often estimate models using data from a particular domain, e.g. estimating risk preferences in a particular subject pool or for a specific class of lotteries. Whether a model's predictions extrapolate well across domains depends on whether the estimated model has captured generalizable structure. We provide a tractable formulation for this "out-of-domain" prediction problem and define the transfer error of a model based on how well it performs on data from a new domain. We derive finite-sample forecast intervals that are guaranteed to cover realized transfer errors with a user-selected probability when domains are iid, and use these intervals to compare the transferability of economic models and black box algorithms for predicting certainty equivalents. We find that in this application, the black box algorithms we consider outperform standard economic models when estimated and tested on data from the same domain, but the economic models generalize across domains better than the black-box algorithms do.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.04796v4</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaiah Andrews, Drew Fudenberg, Lihua Lei, Annie Liang, Chaofeng Wu</dc:creator>
    </item>
    <item>
      <title>Nonparametric Causal Decomposition of Group Disparities</title>
      <link>https://arxiv.org/abs/2306.16591</link>
      <description>arXiv:2306.16591v3 Announce Type: replace-cross 
Abstract: We introduce a new nonparametric causal decomposition approach that identifies the mechanisms by which a treatment variable contributes to a group-based outcome disparity. Our approach distinguishes three mechanisms: group differences in 1) treatment prevalence, 2) average treatment effects, and 3) selection into treatment based on individual-level treatment effects. Our approach reformulates classic Kitagawa-Blinder-Oaxaca decompositions in causal and nonparametric terms, complements causal mediation analysis by explaining group disparities instead of group effects, and isolates conceptually distinct mechanisms conflated in recent random equalization decompositions. In contrast to all prior approaches, our framework uniquely identifies differential selection into treatment as a novel disparity-generating mechanism. Our approach can be used for both the retrospective causal explanation of disparities and the prospective planning of interventions to change disparities. We present both an unconditional and a conditional decomposition, where the latter quantifies the contributions of the treatment within levels of certain covariates. We develop nonparametric estimators that are $\sqrt{n}$-consistent, asymptotically normal, semiparametrically efficient, and multiply robust. We apply our approach to analyze the mechanisms by which college graduation causally contributes to intergenerational income persistence (the disparity in income attainment between parental income groups).</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16591v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ang Yu, Felix Elwert</dc:creator>
    </item>
    <item>
      <title>Improving Robust Decisions with Data</title>
      <link>https://arxiv.org/abs/2310.16281</link>
      <description>arXiv:2310.16281v4 Announce Type: replace-cross 
Abstract: A decision-maker faces uncertainty governed by a data-generating process (DGP), which is only known to belong to a set of sequences of independent but possibly non-identical distributions. A robust decision maximizes the expected payoff against the worst possible DGP in this set. This paper characterizes when and how such robust decisions can be improved with data, measured by the expected payoff under the true DGP, no matter which possible DGP is the truth. It further develops novel and simple inference methods to achieve it, as common methods (e.g., maximum likelihood) may fail to deliver such an improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16281v4</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyu Cheng</dc:creator>
    </item>
    <item>
      <title>Privacy-Protected Spatial Autoregressive Model</title>
      <link>https://arxiv.org/abs/2403.16773</link>
      <description>arXiv:2403.16773v2 Announce Type: replace-cross 
Abstract: Spatial autoregressive (SAR) models are important tools for studying network effects. However, with an increasing emphasis on data privacy, data providers often implement privacy protection measures that make classical SAR models inapplicable. In this study, we introduce a privacy-protected SAR model with noise-added response and covariates to meet privacy-protection requirements. However, in this scenario, the traditional quasi-maximum likelihood estimator becomes infeasible because the likelihood function cannot be directly formulated. To address this issue, we first consider an explicit expression for the likelihood function with only noise-added responses. Then, we develop techniques to correct the biases for derivatives introduced by noise. Correspondingly, a Newton-Raphson-type algorithm is proposed to obtain the estimator, leading to a corrected likelihood estimator. To further enhance computational efficiency, we introduce a corrected least squares estimator based on the idea of bias correction. These two estimation methods ensure both data security and the attainment of statistically valid estimators. Theoretical analysis of both estimators is carefully conducted, statistical inference methods and model extensions are discussed. The finite sample performances of different methods are demonstrated through extensive simulations and the analysis of a real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16773v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danyang Huang, Ziyi Kong, Shuyuan Wu, Hansheng Wang</dc:creator>
    </item>
  </channel>
</rss>
