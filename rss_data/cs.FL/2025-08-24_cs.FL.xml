<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.FL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.FL</link>
    <description>cs.FL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.FL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 25 Aug 2025 04:01:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Passive Model Learning of Visibly Deterministic Context-free Grammars</title>
      <link>https://arxiv.org/abs/2508.16305</link>
      <description>arXiv:2508.16305v1 Announce Type: new 
Abstract: We present PAPNI, a passive automata learning algorithm capable of learning deterministic context-free grammars, which are modeled with visibly deterministic pushdown automata. PAPNI is a generalization of RPNI, a passive automata learning algorithm capable of learning regular languages from positive and negative samples. PAPNI uses RPNI as its underlying learning algorithm while assuming a priori knowledge of the visibly deterministic input alphabet, that is, the alphabet decomposition into symbols that push to the stack, pop from the stack, or do not affect the stack.
  In this paper, we show how passive learning of deterministic pushdown automata can be viewed as a preprocessing step of standard RPNI implementations. We evaluate the proposed approach on various deterministic context-free grammars found in the literature and compare the predictive accuracy of learned models with RPNI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16305v1</guid>
      <category>cs.FL</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edi Mu\v{s}kardin, Tamim Burgstaller</dc:creator>
    </item>
    <item>
      <title>Automata Learning -- Expect Delays!</title>
      <link>https://arxiv.org/abs/2508.16384</link>
      <description>arXiv:2508.16384v1 Announce Type: new 
Abstract: This paper studies active automata learning (AAL) in the presence of stochastic delays. We consider Mealy machines that have stochastic delays associated with each transition and explore how the learner can efficiently arrive at faithful estimates of those machines, the precision of which crucially relies on repetitive sampling of transition delays. While it is possible to na\"ively integrate the delay sampling into AAL algorithms such as $L^*$, this leads to considerable oversampling near the root of the state space. We address this problem by separating conceptually the learning of behavior and delays such that the learner uses the information gained while learning the logical behavior to arrive at efficient input sequences for collecting the needed delay samples. We put emphasis on treating cases in which identical input/output behaviors might stem from distinct delay characteristics. Finally, we provide empirical evidence that our method outperforms the na\"ive baseline across a wide range of benchmarks and investigate its applicability in a realistic setting by studying the join order in a relational database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16384v1</guid>
      <category>cs.FL</category>
      <category>cs.SE</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Dengler, Sven Apel, Holger Hermanns</dc:creator>
    </item>
    <item>
      <title>Synthesizing DSLs for Few-Shot Learning</title>
      <link>https://arxiv.org/abs/2508.16063</link>
      <description>arXiv:2508.16063v1 Announce Type: cross 
Abstract: We study the problem of synthesizing domain-specific languages (DSLs) for few-shot learning in symbolic domains. Given a base language and instances of few-shot learning problems, where each instance is split into training and testing samples, the DSL synthesis problem asks for a grammar over the base language that guarantees that small expressions solving training samples also solve corresponding testing samples. We prove that the problem is decidable for a class of languages whose semantics over fixed structures can be evaluated by tree automata and when expression size corresponds to parse tree depth in the grammar, and, furthermore, the grammars solving the problem correspond to a regular set of trees. We also prove decidability results for variants of the problem where DSLs are only required to express solutions for input learning problems and where DSLs are defined using macro grammars.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.16063v1</guid>
      <category>cs.PL</category>
      <category>cs.FL</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Krogmeier, P. Madhusudan</dc:creator>
    </item>
    <item>
      <title>Unsupervised Automata Learning via Discrete Optimization</title>
      <link>https://arxiv.org/abs/2303.14111</link>
      <description>arXiv:2303.14111v3 Announce Type: replace-cross 
Abstract: Automata learning is a successful tool for many application domains such as robotics and automatic verification. Typically, automata learning techniques operate in a supervised learning setting (active or passive) where they learn a finite state machine in contexts where additional information, such as labeled system executions, is available. However, other settings, such as learning from unlabeled data - an important aspect in machine learning - remain unexplored. To overcome this limitation, we propose a framework for learning a deterministic finite automaton (DFA) from a given multi-set of unlabeled words. We show that this problem is computationally hard and develop three learning algorithms based on constraint optimization. Moreover, we introduce novel regularization schemes for our optimization problems that improve the overall interpretability of our DFAs. Using a prototype implementation, we demonstrate practical feasibility in the context of unsupervised anomaly detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.14111v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.FL</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Lutz, Daniil Kaminskyi, Florian Wittbold, Simon Dierl, Falk Howar, Barbara K\"onig, Emmanuel M\"uller, Daniel Neider</dc:creator>
    </item>
    <item>
      <title>Sparse regular subsets of the reals</title>
      <link>https://arxiv.org/abs/2311.11162</link>
      <description>arXiv:2311.11162v2 Announce Type: replace-cross 
Abstract: This paper concerns the expansion of the real ordered additive group by a predicate for a subset of $[0,1]$ whose base-$r$ representations are recognized by a B\"uchi automaton. In the case that this predicate is closed, a dichotomy is established for when this expansion is interdefinable with the structure $(\mathbb{R},&lt;,+,0,r^{-\mathbb{N}})$ for some $r \in \mathbb{N}_{&gt;1}$. In the case that the closure of the predicate has Hausdorff dimension less than $1$, the dichotomy further characterizes these expansions of $(\mathbb{R},&lt;,+,0,1)$ by when they have NIP and NTP$_2$, which is precisely when the closure of the predicate has Hausdorff dimension $0$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11162v2</guid>
      <category>math.LO</category>
      <category>cs.FL</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jason Bell, Alexi Block Gorman</dc:creator>
    </item>
    <item>
      <title>AutoVerus: Automated Proof Generation for Rust Code</title>
      <link>https://arxiv.org/abs/2409.13082</link>
      <description>arXiv:2409.13082v3 Announce Type: replace-cross 
Abstract: Generative AI has shown its values for many software engineering tasks. Still in its infancy, large language model (LLM)-based proof generation lags behind LLM-based code generation. In this paper, we present AutoVerus. AutoVerus uses LLMs to automatically generate correctness proof for Rust code. AutoVerus is designed to match the unique features of Verus, a verification tool that can prove the correctness of Rust code using proofs and specifications also written in Rust. AutoVerus consists of a network of LLM agents that are crafted and orchestrated to mimic human experts' three phases of proof construction: preliminary proof generation, proof refinement guided by generic tips, and proof debugging guided by verification errors. To thoroughly evaluate AutoVerus and help foster future research in this direction, we have built a benchmark suite of 150 non-trivial proof tasks, based on existing code-generation benchmarks and verification benchmarks. Our evaluation shows that AutoVerus can automatically generate correct proof for more than 90% of them, with more than half of them tackled in less than 30 seconds or 3 LLM calls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13082v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.FL</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3763174</arxiv:DOI>
      <dc:creator>Chenyuan Yang, Xuheng Li, Md Rakib Hossain Misu, Jianan Yao, Weidong Cui, Yeyun Gong, Chris Hawblitzel, Shuvendu Lahiri, Jacob R. Lorch, Shuai Lu, Fan Yang, Ziqiao Zhou, Shan Lu</dc:creator>
    </item>
    <item>
      <title>CMSO-transducing tree-like graph decompositions</title>
      <link>https://arxiv.org/abs/2412.04970</link>
      <description>arXiv:2412.04970v4 Announce Type: replace-cross 
Abstract: We give $\operatorname{CMSO}$-transductions that, given a graph $G$, output its modular decomposition, its split decomposition and its bi-join decomposition. This improves results by Courcelle [Logical Methods in Computer Science, 2006] who gave such transductions using order-invariant $\operatorname{MSO}$, a strictly more expressive logic than $\operatorname{CMSO}$. Our methods more generally yield $\operatorname{C}_2\!\operatorname{MSO}$-transductions that output the canonical decompositions of weakly-partitive set systems and weakly-bipartitive systems of bipartitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04970v4</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.FL</category>
      <pubDate>Mon, 25 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rutger Campbell, Bruno Guillon, Mamadou Moustapha Kant\'e, Eun Jung Kim, Noleen K\"ohler</dc:creator>
    </item>
  </channel>
</rss>
