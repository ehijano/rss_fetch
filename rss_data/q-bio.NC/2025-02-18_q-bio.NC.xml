<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Feb 2025 03:01:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Neuron Platonic Intrinsic Representation From Dynamics Using Contrastive Learning</title>
      <link>https://arxiv.org/abs/2502.10425</link>
      <description>arXiv:2502.10425v2 Announce Type: new 
Abstract: The Platonic Representation Hypothesis suggests a universal, modality-independent reality representation behind different data modalities. Inspired by this, we view each neuron as a system and detect its multi-segment activity data under various peripheral conditions. We assume there's a time-invariant representation for the same neuron, reflecting its intrinsic properties like molecular profiles, location, and morphology. The goal of obtaining these intrinsic neuronal representations has two criteria: (I) segments from the same neuron should have more similar representations than those from different neurons; (II) the representations must generalize well to out-of-domain data. To meet these, we propose the NeurPIR (Neuron Platonic Intrinsic Representation) framework. It uses contrastive learning, with segments from the same neuron as positive pairs and those from different neurons as negative pairs. In implementation, we use VICReg, which focuses on positive pairs and separates dissimilar samples via regularization. We tested our method on Izhikevich model-simulated neuronal population dynamics data. The results accurately identified neuron types based on preset hyperparameters. We also applied it to two real-world neuron dynamics datasets with neuron type annotations from spatial transcriptomics and neuron locations. Our model's learned representations accurately predicted neuron types and locations and were robust on out-of-domain data (from unseen animals). This shows the potential of our approach for understanding neuronal systems and future neuroscience research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10425v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Wu, Can Liao, Zizhen Deng, Zhengrui Guo, Jinzhuo Wang</dc:creator>
    </item>
    <item>
      <title>Hybrid Brain-Machine Interface: Integrating EEG and EMG for Reduced Physical Demand</title>
      <link>https://arxiv.org/abs/2502.10904</link>
      <description>arXiv:2502.10904v1 Announce Type: new 
Abstract: We present a hybrid brain-machine interface (BMI) that integrates steady-state visually evoked potential (SSVEP)-based EEG and facial EMG to improve multimodal control and mitigate fatigue in assistive applications. Traditional BMIs relying solely on EEG or EMG suffer from inherent limitations; EEG-based control requires sustained visual focus, leading to cognitive fatigue, while EMG-based control induces muscular fatigue over time. Our system dynamically alternates between EEG and EMG inputs, using EEG to detect SSVEP signals at 9.75 Hz and 14.25 Hz and EMG from cheek and neck muscles to optimize control based on task demands. In a virtual turtle navigation task, the hybrid system achieved task completion times comparable to an EMG-only approach, while 90% of users reported reduced or equal physical demand. These findings demonstrate that multimodal BMI systems can enhance usability, reduce strain, and improve long-term adherence in assistive technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10904v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Wang, Katie Hong, Zachary Sayyah, Malcolm Krolick, Emma Steinberg, Rohan Venkatdas, Sidharth Pavuluri, Yipeng Wang, Zihan Huang</dc:creator>
    </item>
    <item>
      <title>Emergent functions of noise-driven spontaneous activity: Homeostatic maintenance of criticality and memory consolidation</title>
      <link>https://arxiv.org/abs/2502.10946</link>
      <description>arXiv:2502.10946v1 Announce Type: new 
Abstract: Unlike digital computers, the brain exhibits spontaneous activity even during complete rest, despite the evolutionary pressure for energy efficiency. Inspired by the critical brain hypothesis, which proposes that the brain operates optimally near a critical point of phase transition in the dynamics of neural networks to improve computational efficiency, we postulate that spontaneous activity plays a homeostatic role in the development and maintenance of criticality. Criticality in the brain is associated with the balance between excitatory and inhibitory synaptic inputs (EI balance), which is essential for maintaining neural computation performance. Here, we hypothesize that both criticality and EI balance are stabilized by appropriate noise levels and spike-timing-dependent plasticity (STDP) windows. Using spiking neural network (SNN) simulations and in vitro experiments with dissociated neuronal cultures, we demonstrated that while repetitive stimuli transiently disrupt both criticality and EI balance, spontaneous activity can develop and maintain these properties and prolong the fading memory of past stimuli. Our findings suggest that the brain may achieve self-optimization and memory consolidation as emergent functions of noise-driven spontaneous activity. This noise-harnessing mechanism provides insights for designing energy-efficient neural networks, and may explain the critical function of sleep in maintaining homeostasis and consolidating memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10946v1</guid>
      <category>q-bio.NC</category>
      <category>nlin.AO</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marumitsu Ikeda, Dai Akita, Hirokazu Takahashi</dc:creator>
    </item>
    <item>
      <title>Graceful forgetting: Memory as a process</title>
      <link>https://arxiv.org/abs/2502.11105</link>
      <description>arXiv:2502.11105v1 Announce Type: new 
Abstract: A rational theory of memory is proposed to explain how we can accommodate unbounded sensory input within bounded storage space. Memory is stored as statistics, organized into complex structures that are constantly summarized and compressed to make room for new input. This process, driven by space constraints, is guided by heuristics that optimize the memory for future needs. Sensory input is rapidly encoded as simple statistics that are more slowly elaborated into more abstract constructs. This theory differs from previous accounts of memory by (a) its reliance on statistics, (b) its use of heuristics to guide the choice of statistics, and (c) the emphasis on memory as a process that is intensive, complex, and expensive. The theory is intended as an aid to make sense of our extensive knowledge of memory, and bring us closer to an understanding of memory in functional and mechanistic terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11105v1</guid>
      <category>q-bio.NC</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alain de Cheveign\'e</dc:creator>
    </item>
    <item>
      <title>Targeting C99 Mediated Metabolic Disruptions with Ketone Therapy in Alzheimer's Disease</title>
      <link>https://arxiv.org/abs/2502.11395</link>
      <description>arXiv:2502.11395v1 Announce Type: new 
Abstract: INTRODUCTION: Alzheimer's disease (AD) involves neurodegeneration, metabolic dysfunction, and proteostasis failure. While amyloid and tau pathology are well studied, the role of metabolic dysregulation as an upstream driver remains unclear.
  METHODS:We used Drosophila AD models expressing APP and BACE1 under the neuron-specific driver, applying quantitative mass spectrometry (MS) to analyze C99-induced proteomic changes and metabolic disruption. Additional biochemical and imaging analyses were performed to assess mitochondrial function and autophagy.
  RESULTS: C99 disrupted mitochondrial proteostasis, impairing TCA cycle enzymes, fatty acid oxidation, and lysosomal clearance. Immunoprecipitation confirmed C99's interaction with proteostasis regulators, leading to neurodegenerative stress.
  DISCUSSION: Our findings extend previous models of AD pathogenesis by demonstrating that C99 impairs lipid metabolism, disrupting ketone availability and neuronal energy balance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11395v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hao Huang, Kaijing Xu, Michael Lardelli</dc:creator>
    </item>
    <item>
      <title>Morphological Neuron Classification Using Machine Learning</title>
      <link>https://arxiv.org/abs/2502.11591</link>
      <description>arXiv:2502.11591v1 Announce Type: new 
Abstract: Classification and quantitative characterization of neuronal morphologies from histological neuronal reconstruction is challenging since it is still unclear how to delineate a neuronal cell class and which are the best features to define them by. The morphological neuron characterization represents a primary source to address anatomical comparisons, morphometric analysis of cells, or brain modeling. The objectives of this paper are (i) to develop and integrate a pipeline that goes from morphological feature extraction to classification and (ii) to assess and compare the accuracy of machine learning algorithms to classify neuron morphologies. The algorithms were trained on 430 digitally reconstructed neurons subjectively classified into layers and/or m-types using young and/or adult development state population of the somatosensory cortex in rats. For supervised algorithms, linear discriminant analysis provided better classification results in comparison with others. For unsupervised algorithms, the affinity propagation and the Ward algorithms provided slightly better results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11591v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3389/fnana.2016.00102</arxiv:DOI>
      <arxiv:journal_reference>Xavier Vasques, Laurent Vanel, Guillaume Villette, Laura Cif, Morphological Neuron Classification Using Machine Learning, Frontiers in Neuroanatomy, 10, 2016</arxiv:journal_reference>
      <dc:creator>Xavier Vasques, Laurent Vanel, Guillaume Villette, Laura Cif</dc:creator>
    </item>
    <item>
      <title>Automatic target validation based on neuroscientific literature mining for tractography</title>
      <link>https://arxiv.org/abs/2502.11597</link>
      <description>arXiv:2502.11597v1 Announce Type: new 
Abstract: Target identification for tractography studies requires solid anatomical knowledge validated by an extensive literature review across species for each seed structure to be studied. Manual literature review to identify targets for a given seed region is tedious and potentially subjective. Therefore, complementary approaches would be useful. We propose to use text-mining models to automatically suggest potential targets from the neuroscientific literature, full-text articles and abstracts, so that they can be used for anatomical connection studies and more specifically for tractography. We applied text-mining models to three structures: two well-studied structures, since validated deep brain stimulation targets, the internal globus pallidus and the subthalamic nucleus and, the nucleus accumbens, an exploratory target for treating psychiatric disorders. We performed a systematic review of the literature to document the projections of the three selected structures and compared it with the targets proposed by text-mining models, both in rat and primate (including human). We ran probabilistic tractography on the nucleus accumbens and compared the output with the results of the text-mining models and literature review. Overall, text-mining the literature could find three times as many targets as two man-weeks of curation could. The overall efficiency of the text-mining against literature review in our study was 98% recall (at 36% precision), meaning that over all the targets for the three selected seeds, only one target has been missed by text-mining. We demonstrate that connectivity for a structure of interest can be extracted from a very large amount of publications and abstracts. We believe this tool will be useful in helping the neuroscience community to facilitate connectivity studies of particular brain regions. The text mining tools used for the study are part of the HBP Neuroinformatics Platform, publicly available at http://connectivity-brainer.rhcloud.com</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11597v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3389/fnana.2015.00066</arxiv:DOI>
      <arxiv:journal_reference>Vasques X, Richardet R, Hill SL, Slater D, Chappelier JC, Pralong E, Bloch J, Draganski B, Cif L. Automatic target validation based on neuroscientific literature mining for tractography. Front Neuroanat. 2015 May 27;9:66</arxiv:journal_reference>
      <dc:creator>Xavier Vasques, Renaud Richardet, Sean L Hill, David Slater, Jean-Cedric Chappelier, Etienne Pralong, Jocelyne Bloch, Bogdan Draganski, Laura Cif</dc:creator>
    </item>
    <item>
      <title>Using economic value signals from primate prefrontal cortex in neuro-engineering applications</title>
      <link>https://arxiv.org/abs/2502.12092</link>
      <description>arXiv:2502.12092v1 Announce Type: new 
Abstract: Neural signals related to movement can be measured from intracranial recordings and used in brain-machine interface devices (BMI) to restore physical function in impaired patients. In this study, we explore the use of more abstract neural signals related to economic value in a BMI context. Using data collected from the orbitofrontal cortex in non-human primates, we develop deep learning-based neural decoders that can predict the monkey's choice in a value-based decision-making task. Out-of-sample performance was improved by augmenting the training set with synthesized data, showing the feasibility of using limited training data. We further demonstrate that we can predict the monkey's choice sooner using a neural forecasting module that is equipped with task-related information. These findings support the feasibility of user preference-informed neuroengineering devices that leverage abstract cognitive signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12092v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tevin C. Rouse, Shira M. Lupkin, Vincent B. McGinty</dc:creator>
    </item>
    <item>
      <title>Personal Danger Signals Reprocessing: New Online Group Intervention for Chronic Pain</title>
      <link>https://arxiv.org/abs/2502.12106</link>
      <description>arXiv:2502.12106v1 Announce Type: new 
Abstract: Chronic pain is a significant global health issue, with many patients experiencing persistent pain despite no identifiable organic cause, classified as nociplastic pain. Increasing evidence highlights the role of danger signal processing in the maintenance of chronic pain. In response, we developed Personal Danger Signals Reprocessing (PDSR), an online, group-based intervention designed to modify these mechanisms using coaching techniques to enhance accessibility and affordability.
  This study evaluated the efficacy of PDSR in reducing pain and mental health comorbidities. A cohort of women (N=19, mean age 43) participated in an 8-week online program, receiving weekly sessions on chronic pain mechanisms within a systemic framework. Outcomes were assessed at three time points: pre-intervention, mid-intervention, and post-intervention. A waiting list group (N=20, mean age 43.5) completed assessments at the same intervals.
  Participants in the PDSR group showed significant pain reduction (p &lt; .001), with moderate to large effects observed at mid-intervention (Cohen's D = 0.7) and post-intervention (Cohen's D = 1.5) compared to controls. Pain interference significantly decreased (p &lt; .01), with large reductions in the PDSR group (Cohen's D = -1.7, p &lt; .0001). Well-being also improved substantially (p &lt; .001, Cohen's D = 1.7-1.8). Secondary outcomes, including pain catastrophizing, sleep interference, anxiety, and depressive symptoms, consistently improved (all p-values &lt; .01).
  Findings suggest PDSR is an effective, scalable intervention for reducing pain, improving function, and enhancing well-being in individuals with chronic pain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12106v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carmit Himmelblau Gat, Natalia Polyviannaya, Pavel Goldstein</dc:creator>
    </item>
    <item>
      <title>A recurrent vision transformer shows signatures of primate visual attention</title>
      <link>https://arxiv.org/abs/2502.10955</link>
      <description>arXiv:2502.10955v1 Announce Type: cross 
Abstract: Attention is fundamental to both biological and artificial intelligence, yet research on animal attention and AI self attention remains largely disconnected. We propose a Recurrent Vision Transformer (Recurrent ViT) that integrates self-attention with recurrent memory, allowing both current inputs and stored information to guide attention allocation. Trained solely via sparse reward feedback on a spatially cued orientation change detection task, a paradigm used in primate studies, our model exhibits primate like signatures of attention, including improved accuracy and faster responses for cued stimuli that scale with cue validity. Analysis of self-attention maps reveals dynamic spatial prioritization with reactivation prior to expected changes, and targeted perturbations produce performance shifts similar to those observed in primate frontal eye fields and superior colliculus. These findings demonstrate that incorporating recurrent feedback into self attention can capture key aspects of primate visual attention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10955v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Morgan, Badr Albanna, James P. Herman</dc:creator>
    </item>
    <item>
      <title>Explaining Necessary Truths</title>
      <link>https://arxiv.org/abs/2502.11251</link>
      <description>arXiv:2502.11251v1 Announce Type: cross 
Abstract: Knowing the truth is rarely enough -- we also seek out reasons why the fact is true. While much is known about how we explain contingent truths, we understand less about how we explain facts, such as those in mathematics, that are true as a matter of logical necessity. We present a framework, based in computational complexity, where explanations for deductive truths co-emerge with discoveries of simplifying steps during the search process. When such structures are missing, we revert, in turn, to error-based reasons, where a (corrected) mistake can serve as fictitious, but explanatory, contingency-cause: not making the mistake serves as a reason why the truth takes the form it does. We simulate human subjects, using GPT-4o, presented with SAT puzzles of varying complexity and reasonableness, validating our theory and showing how its predictions can be tested in future human studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11251v1</guid>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>math.HO</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>G\"ulce Karde\c{s}, Simon DeDeo</dc:creator>
    </item>
    <item>
      <title>Neural Simulation via Bootstrapping</title>
      <link>https://arxiv.org/abs/2402.14186</link>
      <description>arXiv:2402.14186v2 Announce Type: replace 
Abstract: Learning is easy; representation is hard. Representation and simulation are two sides of the same coin. How are the external environment, internal states, and the mind of a person represented by the brain based on a simple Hebbian learning principle? Conceptually similar to the cloning of DNA for genetics, we suggest a bootstrapping-based constructive framework for representation learning via neural simulation. The key insight is to understand how the evolution of the brain drives the repurposing of old structures for new applications via alternating between temporal exploitation and spatial exploration. For temporal exploitation, a simple dopamine-based reinforcement learning algorithm offers a principled solution to the credit assignment problem and its variations in space and time. For spatial exploration, new structures are invented to generate more powerful representations of the agent's external environment and internal state by neural simulation. Unlike Monte Carlo simulation, the neural simulation uses delta measures as the seed and implements four levels of predictive processing by the neocortex via bootstrapping, including integral transform (fast thinking), conditional sampling (slow thinking), latent quantization (abstract thinking), and flow maximization (social thinking). From reptiles and mammals to primates and humans, our simulation-based framework offers a unified perspective for understanding the evolution of natural intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14186v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Xin Li</dc:creator>
    </item>
    <item>
      <title>The geometry of efficient codes: how rate-distortion trade-offs distort the latent representations of generative models</title>
      <link>https://arxiv.org/abs/2406.07269</link>
      <description>arXiv:2406.07269v2 Announce Type: replace 
Abstract: Living organisms rely on internal models of the world to act adaptively. These models, because of resource limitations, cannot encode every detail and hence need to compress information. From a cognitive standpoint, information compression can manifest as a distortion of latent representations, resulting in the emergence of representations that may not accurately reflect the external world or its geometry. Rate-distortion theory formalizes the optimal way to compress information while minimizing such distortions, by considering factors such as capacity limitations, the frequency and the utility of stimuli. However, while this theory explains why the above factors distort latent representations, it does not specify which specific distortions they produce. To address this question, here we investigate how rate-distortion trade-offs shape the latent representations of images in generative models, specifically Beta Variational Autoencoders ($\beta$-VAEs), under varying constraints of model capacity, data distributions, and task objectives. By systematically exploring these factors, we identify three primary distortions in latent representations: prototypization, specialization, and orthogonalization. These distortions emerge as signatures of information compression, reflecting the model's adaptation to capacity limitations, data imbalances, and task demands. Additionally, our findings demonstrate that these distortions can coexist, giving rise to a rich landscape of latent spaces, whose geometry could differ significantly across generative models subject to different constraints. Our findings contribute to explain how the normative constraints of rate-distortion theory shape the geometry of latent representations of generative models of artificial systems and living organisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07269v2</guid>
      <category>q-bio.NC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Leo D'Amato, Gian Luca Lancia, Giovanni Pezzulo</dc:creator>
    </item>
    <item>
      <title>Modeling flexible behavior with remapping-based hippocampal sequence learning</title>
      <link>https://arxiv.org/abs/2407.14708</link>
      <description>arXiv:2407.14708v2 Announce Type: replace 
Abstract: Animals flexibly change their behavior depending on context. It is reported that the hippocampus is one of the most prominent regions for contextual behaviors, and its sequential activity shows context dependency. However, how such context-dependent sequential activity is established through reorganization of neuronal activity (remapping) is unclear. To better understand the formation of hippocampal activity and its contribution to context-dependent flexible behavior, we present a novel biologically plausible reinforcement learning model. In this model, a context-selection module promotes the formation of context-dependent sequential activity and allows for flexible switching of behavior in multiple contexts. This model reproduces a variety of findings from neural activity, optogenetic inactivation, human fMRI, and clinical research. Furthermore, our model predicts that imbalances in the ratio between sensory and contextual inputs in the context-selection module account for schizophrenia (SZ) and autism spectrum disorder (ASD)-like behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14708v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshiki Ito, Taro Toyoizumi</dc:creator>
    </item>
    <item>
      <title>Vision CNNs trained to estimate spatial latents learned similar ventral-stream-aligned representations</title>
      <link>https://arxiv.org/abs/2412.09115</link>
      <description>arXiv:2412.09115v2 Announce Type: replace 
Abstract: Studies of the functional role of the primate ventral visual stream have traditionally focused on object categorization, often ignoring -- despite much prior evidence -- its role in estimating "spatial" latents such as object position and pose. Most leading ventral stream models are derived by optimizing networks for object categorization, which seems to imply that the ventral stream is also derived under such an objective. Here, we explore an alternative hypothesis: Might the ventral stream be optimized for estimating spatial latents? And a closely related question: How different -- if at all -- are representations learned from spatial latent estimation compared to categorization? To ask these questions, we leveraged synthetic image datasets generated by a 3D graphic engine and trained convolutional neural networks (CNNs) to estimate different combinations of spatial and category latents. We found that models trained to estimate just a few spatial latents achieve neural alignment scores comparable to those trained on hundreds of categories, and the spatial latent performance of models strongly correlates with their neural alignment. Spatial latent and category-trained models have very similar -- but not identical -- internal representations, especially in their early and middle layers. We provide evidence that this convergence is partly driven by non-target latent variability in the training data, which facilitates the implicit learning of representations of those non-target latents. Taken together, these results suggest that many training objectives, such as spatial latents, can lead to similar models aligned neurally with the ventral stream. Thus, one should not assume that the ventral stream is optimized for object categorization only. As a field, we need to continue to sharpen our measures of comparing models to brains to better understand the functional roles of the ventral stream.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09115v2</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yudi Xie, Weichen Huang, Esther Alter, Jeremy Schwartz, Joshua B. Tenenbaum, James J. DiCarlo</dc:creator>
    </item>
    <item>
      <title>Human alignment of neural network representations</title>
      <link>https://arxiv.org/abs/2211.01201</link>
      <description>arXiv:2211.01201v5 Announce Type: replace-cross 
Abstract: Today's computer vision models achieve human or near-human level performance across a wide variety of vision tasks. However, their architectures, data, and learning algorithms differ in numerous ways from those that give rise to human vision. In this paper, we investigate the factors that affect the alignment between the representations learned by neural networks and human mental representations inferred from behavioral responses. We find that model scale and architecture have essentially no effect on the alignment with human behavioral responses, whereas the training dataset and objective function both have a much larger impact. These findings are consistent across three datasets of human similarity judgments collected using two different tasks. Linear transformations of neural network representations learned from behavioral responses from one dataset substantially improve alignment with human similarity judgments on the other two datasets. In addition, we find that some human concepts such as food and animals are well-represented by neural networks whereas others such as royal or sports-related objects are not. Overall, although models trained on larger, more diverse datasets achieve better alignment with humans than models trained on ImageNet alone, our results indicate that scaling alone is unlikely to be sufficient to train neural networks with conceptual representations that match those used by humans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.01201v5</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Muttenthaler, Jonas Dippel, Lorenz Linhardt, Robert A. Vandermeulen, Simon Kornblith</dc:creator>
    </item>
    <item>
      <title>Language Writ Large: LLMs, ChatGPT, Grounding, Meaning and Understanding</title>
      <link>https://arxiv.org/abs/2402.02243</link>
      <description>arXiv:2402.02243v2 Announce Type: replace-cross 
Abstract: Apart from what (little) OpenAI may be concealing from us, we all know (roughly) how ChatGPT works (its huge text database, its statistics, its vector representations, and their huge number of parameters, its next-word training, and so on). But none of us can say (hand on heart) that we are not surprised by what ChatGPT has proved to be able to do with these resources. This has even driven some of us to conclude that ChatGPT actually understands. It is not true that it understands. But it is also not true that we understand how it can do what it can do. I will suggest some hunches about benign biases: convergent constraints that emerge at LLM scale that may be helping ChatGPT do so much better than we would have expected. These biases are inherent in the nature of language itself, at LLM scale, and they are closely linked to what it is that ChatGPT lacks, which is direct sensorimotor grounding to connect its words to their referents and its propositions to their meanings. These convergent biases are related to (1) the parasitism of indirect verbal grounding on direct sensorimotor grounding, (2) the circularity of verbal definition, (3) the mirroring of language production and comprehension, (4) iconicity in propositions at LLM scale, (5) computational counterparts of human categorical perception in category learning by neural nets, and perhaps also (6) a conjecture by Chomsky about the laws of thought. The exposition will be in the form of a dialogue with ChatGPT-4.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02243v2</guid>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Frontiers in Artificial Intelligence 7: 1490698 (2025)</arxiv:journal_reference>
      <dc:creator>Stevan Harnad</dc:creator>
    </item>
    <item>
      <title>Vision Language Models Know Law of Conservation without Understanding More-or-Less</title>
      <link>https://arxiv.org/abs/2410.00332</link>
      <description>arXiv:2410.00332v4 Announce Type: replace-cross 
Abstract: Conservation is a critical milestone of cognitive development considered to be supported by both the understanding of quantitative concepts and the reversibility of operations. To assess whether this critical component of human intelligence has emerged in Vision Language Models, we have curated the ConserveBench, a battery of 365 cognitive experiments across four dimensions of physical quantities: volume, solid quantity, length, and number. The former two involve transformational tasks which require reversibility understanding. The latter two involve non-transformational tasks which assess quantity understanding. Surprisingly, we find that while Vision Language Models are generally good at transformational tasks, they tend to fail at non-transformational tasks. There is a dissociation between understanding the reversibility of operations and understanding of quantity, which both are believed to be the cornerstones of the understanding of law of conservation in humans. $\href{https://growing-ai-like-a-child.github.io/pages/Conservation/}{Website}$</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00332v4</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dezhi Luo, Haiyun Lyu, Qingying Gao, Haoran Sun, Yijiang Li, Hokin Deng</dc:creator>
    </item>
    <item>
      <title>Parametric PerceptNet: A bio-inspired deep-net trained for Image Quality Assessment</title>
      <link>https://arxiv.org/abs/2412.03210</link>
      <description>arXiv:2412.03210v2 Announce Type: replace-cross 
Abstract: Human vision models are at the core of image processing. For instance, classical approaches to the problem of image quality are based on models that include knowledge about human vision. However, nowadays, deep learning approaches have obtained competitive results by simply approaching this problem as regression of human decisions, and training an standard network on human-rated datasets. These approaches have the advantages of being easily adaptable to a particular problem and they fit very efficiently when data is available. However, mainly due to the excess of parameters, they have the problems of lack of interpretability, and over-fitting. Here we propose a vision model that combines the best of both worlds by using a parametric neural network architecture. We parameterize the layers to have bioplausible functionality, and provide a set of bioplausible parameters. We analyzed different versions of the model and compared it with the non-parametric version. The parametric models achieve a three orders of magnitude reduction in the number of parameters without suffering in regression performance. Furthermore, we show that the parametric models behave better during training and are easier to interpret as vision models. Interestingly, we find that, even initialized with bioplausible trained for regression using human rated datasets, which we call the feature-spreading problem. This suggests that the deep learning approach is inherently flawed, and emphasizes the need to evaluate and train models beyond regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03210v2</guid>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jorge Vila-Tom\'as, Pablo Hern\'andez-C\'amara, Valero Laparra, Jes\'us Malo</dc:creator>
    </item>
    <item>
      <title>CBraMod: A Criss-Cross Brain Foundation Model for EEG Decoding</title>
      <link>https://arxiv.org/abs/2412.07236</link>
      <description>arXiv:2412.07236v2 Announce Type: replace-cross 
Abstract: Electroencephalography (EEG) is a non-invasive technique to measure and record brain electrical activity, widely used in various BCI and healthcare applications. Early EEG decoding methods rely on supervised learning, limited by specific tasks and datasets, hindering model performance and generalizability. With the success of large language models, there is a growing body of studies focusing on EEG foundation models. However, these studies still leave challenges: Firstly, most of existing EEG foundation models employ full EEG modeling strategy. It models the spatial and temporal dependencies between all EEG patches together, but ignores that the spatial and temporal dependencies are heterogeneous due to the unique structural characteristics of EEG signals. Secondly, existing EEG foundation models have limited generalizability on a wide range of downstream BCI tasks due to varying formats of EEG data, making it challenging to adapt to. To address these challenges, we propose a novel foundation model called CBraMod. Specifically, we devise a criss-cross transformer as the backbone to thoroughly leverage the structural characteristics of EEG signals, which can model spatial and temporal dependencies separately through two parallel attention mechanisms. And we utilize an asymmetric conditional positional encoding scheme which can encode positional information of EEG patches and be easily adapted to the EEG with diverse formats. CBraMod is pre-trained on a very large corpus of EEG through patch-based masked EEG reconstruction. We evaluate CBraMod on up to 10 downstream BCI tasks (12 public datasets). CBraMod achieves the state-of-the-art performance across the wide range of tasks, proving its strong capability and generalizability. The source code is publicly available at https://github.com/wjq-learning/CBraMod.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07236v2</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiquan Wang, Sha Zhao, Zhiling Luo, Yangxuan Zhou, Haiteng Jiang, Shijian Li, Tao Li, Gang Pan</dc:creator>
    </item>
    <item>
      <title>ScholaWrite: A Dataset of End-to-End Scholarly Writing Process</title>
      <link>https://arxiv.org/abs/2502.02904</link>
      <description>arXiv:2502.02904v3 Announce Type: replace-cross 
Abstract: Writing is a cognitively demanding task involving continuous decision-making, heavy use of working memory, and frequent switching between multiple activities. Scholarly writing is particularly complex as it requires authors to coordinate many pieces of multiform knowledge. To fully understand writers' cognitive thought process, one should fully decode the end-to-end writing data (from individual ideas to final manuscript) and understand their complex cognitive mechanisms in scholarly writing. We introduce ScholaWrite dataset, a first-of-its-kind keystroke corpus of an end-to-end scholarly writing process for complete manuscripts, with thorough annotations of cognitive writing intentions behind each keystroke. Our dataset includes LaTeX-based keystroke data from five preprints with nearly 62K total text changes and annotations across 4 months of paper writing. ScholaWrite shows promising usability and applications (e.g., iterative self-writing), demonstrating the importance of collection of end-to-end writing data, rather than the final manuscript, for the development of future writing assistants to support the cognitive thinking process of scientists. Our de-identified data examples and code are available on our project page.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02904v3</guid>
      <category>cs.HC</category>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linghe Wang, Minhwa Lee, Ross Volkov, Luan Tuyen Chau, Dongyeop Kang</dc:creator>
    </item>
  </channel>
</rss>
