<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 14 Jan 2026 02:39:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Separation Results for Constant-Depth and Multilinear Ideal Proof Systems</title>
      <link>https://arxiv.org/abs/2601.06299</link>
      <description>arXiv:2601.06299v1 Announce Type: new 
Abstract: In this work, we establish separation theorems for several subsystems of the Ideal Proof System (IPS), an algebraic proof system introduced by Grochow and Pitassi (J. ACM, 2018). Separation theorems are well-studied in the context of classical complexity theory, Boolean circuit complexity, and algebraic complexity.
  In an important work of Forbes, Shpilka, Tzameret, and Wigderson (ToC, 2021), two proof techniques were introduced to prove lower bounds for subsystems of the IPS, namely the functional method and the multiples method. We use these techniques and obtain the following results.
  Hierarchy theorem for constant-depth IPS: Recently, Limaye, Srinivasan, and Tavenas (J. ACM 2025) proved a hierarchy theorem for constant-depth algebraic circuits. We adapt the result and prove a hierarchy theorem for constant-depth $\mathsf{IPS}$. We show that there is an unsatisfiable multilinear instance refutable by a depth-$\Delta$ $\mathsf{IPS}$ such that any depth-($\Delta/10)$ $\mathsf{IPS}$ refutation for it must have superpolynomial size. This result is proved by building on the multiples method.
  Separation theorems for multilinear IPS: In an influential work, Raz (ToC, 2006) unconditionally separated two algebraic complexity classes, namely multilinear $\mathsf{NC}^{1}$ from multilinear $\mathsf{NC}^{2}$. In this work, we prove a similar result for a well-studied fragment of multilinear-$\mathsf{IPS}$.
  Specifically, we present an unsatisfiable instance such that its functional refutation, i.e., the unique multilinear polynomial agreeing with the inverse of the polynomial over the Boolean cube, has a small multilinear-$\mathsf{NC}^{2}$ circuit. However, any multilinear-$\mathsf{NC}^{1}$ $\mathsf{IPS}$ refutation ($\mathsf{IPS}_{\mathsf{LIN}}$) for it must have superpolynomial size. This result is proved by building on the functional method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06299v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amik Raj Behera, Magnus Rahbek Dalgaard Hansen, Nutan Limaye, Srikanth Srinivasan</dc:creator>
    </item>
    <item>
      <title>Arithmetic Complexity of Solutions of the Dirichlet Problem</title>
      <link>https://arxiv.org/abs/2601.06954</link>
      <description>arXiv:2601.06954v1 Announce Type: new 
Abstract: The classical Dirichlet problem on the unit disk can be solved by different numerical approaches. The two most common and popular approaches are the integration of the associated Poisson integral and, by applying Dirichlet's principle, solving a particular minimization problem. For practical use, these procedures need to be implemented on concrete computing platforms. This paper studies the realization of these procedures on Turing machines, the fundamental model for any digital computer. We show that on this computing platform both approaches to solve Dirichlet's problem yield generally a solution that is not Turing computable, even if the boundary function is computable. Then the paper provides a precise characterization of this non-computability in terms of the Zheng--Weihrauch hierarchy. For both approaches, we derive a lower and an upper bound on the degree of non-computability in the Zheng--Weihrauch hierarchy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06954v1</guid>
      <category>cs.CC</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Holger Boche, Volker Pohl, H. Vincent Poor</dc:creator>
    </item>
    <item>
      <title>Recovering polynomials over finite fields from noisy character values</title>
      <link>https://arxiv.org/abs/2601.07137</link>
      <description>arXiv:2601.07137v1 Announce Type: new 
Abstract: Let $g(X)$ be a polynomial over a finite field ${\mathbb F}_q$ with degree $o(q^{1/2})$, and let $\chi$ be the quadratic residue character. We give a polynomial time algorithm to recover $g(X)$ (up to perfect square factors) given the values of $\chi \circ g$ on ${\mathbb F}_q$, with up to a constant fraction of the values having errors. This was previously unknown even for the case of no errors.
  We give a similar algorithm for additive characters of polynomials over fields of characteristic $2$. This gives the first polynomial time algorithm for decoding dual-BCH codes of polynomial dimension from a constant fraction of errors.
  Our algorithms use ideas from Stepanov's polynomial method proof of the classical Weil bounds on character sums, as well as from the Berlekamp-Welch decoding algorithm for Reed-Solomon codes. A crucial role is played by what we call *pseudopolynomials*: high degree polynomials, all of whose derivatives behave like low degree polynomials on ${\mathbb F}_q$.
  Both these results can be viewed as algorithmic versions of the Weil bounds for this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07137v1</guid>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.NT</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swastik Kopparty</dc:creator>
    </item>
    <item>
      <title>The Complexity of Finding Missing Answer Repairs</title>
      <link>https://arxiv.org/abs/2601.06764</link>
      <description>arXiv:2601.06764v1 Announce Type: cross 
Abstract: We investigate the problem of identifying database repairs for missing tuples in query answers. We show that when the query is part of the input - the combined complexity setting - determining whether or not a repair exists is polynomial-time is equivalent to the satisfiability problem for classes of queries admitting a weak form of projection and selection. We then identify the sub-classes of unions of conjunctive queries with negated atoms, defined by the relational algebra operations permitted to appear in the query, for which the minimal repair problem can be solved in polynomial time. In contrast, we show that the problem is NP-hard, as well as set cover-hard to approximate via strict reductions, whenever both projection and join are permitted in the input query. Additionally, we show that finding the size of a minimal repair for unions of conjunctive queries (with negated atoms permitted) is OptP[log(n)]-complete, while computing a minimal repair is possible with O($n^2$) queries to an NP oracle. With recursion permitted, the combined complexity of all of these variants increases significantly, with an EXP lower bound. However, from the data complexity perspective, we show that minimal repairs can be identified in polynomial time for all queries expressible as semi-positive datalog programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06764v1</guid>
      <category>cs.DB</category>
      <category>cs.CC</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jesse Comer, Val Tannen</dc:creator>
    </item>
    <item>
      <title>On the complexity of the Maker-Breaker happy vertex game</title>
      <link>https://arxiv.org/abs/2601.07673</link>
      <description>arXiv:2601.07673v1 Announce Type: cross 
Abstract: Given a c-colored graph G, a vertex of G is happy if it has the same color as all its neighbors. The notion of happy vertices was introduced by Zhang and Li to compute the homophily of a graph. Eto, et al. introduced the Maker-Maker version of the Happy vertex game, where two players compete to claim more happy vertices than their opponent. We introduce here the Maker-Breaker happy vertex game: two players, Maker and Breaker, alternately color the vertices of a graph with their respective colors. Maker aims to maximize the number of happy vertices at the end, while Breaker aims to prevent her. This game is also a scoring version of the Maker-Breaker Domination game introduced by Duchene, et al. as a happy vertex corresponds exactly to a vertex that is not dominated in the domination game. Therefore, this game is a very natural game on graphs and can be studied within the scope of scoring positional games. We initiate here the complexity study of this game, by proving that computing its score is PSPACE-complete on trees, NP-hard on caterpillars, and polynomial on subdivided stars. Finally, we provide the exact value of the score on graphs of maximum degree 2, and we provide an FPT-algorithm to compute the score on graphs of bounded neighborhood diversity. An important contribution of the paper is that, to achieve our hardness results, we introduce a new type of incidence graph called the literal-clause incidence graph for 2-SAT formulas. We prove that QMAX 2-SAT remains PSPACE-complete even if this graph is acyclic, and that MAX 2-SAT remains NP-complete, even if this graph is acyclic and has maximum degree 2, i.e. is a union of paths. We demonstrate the importance of this contribution by proving that Incidence, the scoring positional game played on a graph is also PSPACE-complete when restricted to forests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.07673v1</guid>
      <category>cs.DM</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mathieu Hilaire, Perig Montfort, Nacim Oijid</dc:creator>
    </item>
    <item>
      <title>Most Juntas Saturate the Hardcore Lemma</title>
      <link>https://arxiv.org/abs/2510.25165</link>
      <description>arXiv:2510.25165v2 Announce Type: replace 
Abstract: Consider a function that is mildly hard for size-$s$ circuits. For sufficiently large $s$, Impagliazzo's hardcore lemma guarantees a constant-density subset of inputs on which the same function is extremely hard for circuits of size $s'&lt;\!\!&lt;s$. Blanc, Hayderi, Koch, and Tan [FOCS 2024] recently showed that the degradation from $s$ to $s'$ in this lemma is quantitatively tight in certain parameter regimes. We give a simpler and more general proof of this result in almost all parameter regimes of interest by showing that a random junta witnesses the tightness of the hardcore lemma with high probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.25165v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1137/1.9781611978964.11</arxiv:DOI>
      <dc:creator>Vinayak M. Kumar</dc:creator>
    </item>
    <item>
      <title>Certificates in P and Subquadratic-Time Computation of Radius, Diameter, and all Eccentricities in Graphs</title>
      <link>https://arxiv.org/abs/1803.04660</link>
      <description>arXiv:1803.04660v5 Announce Type: replace-cross 
Abstract: In the context of fine-grained complexity, we investigate the notion of certificate enabling faster polynomial-time algorithms. We specifically target radius (minimum eccentricity), diameter (maximum eccentricity), and all-eccentricity computations for which quadratic-time lower bounds are known under plausible conjectures. In each case, we introduce a notion of certificate as a specific set of nodes from which appropriate bounds on all eccentricities can be derived in subquadratic time when this set has sublinear size. The existence of small certificates is a barrier against SETH-based lower bounds for these problems. We indeed prove that for graph classes with small certificates, there exist randomized subquadratic-time algorithms for computing the radius, the diameter, and all eccentricities respectively. Moreover, these notions of certificates are tightly related to algorithms probing the graph through one-to-all distance queries and allow to explain the efficiency of practical radius and diameter algorithms from the literature. Our formalization enables a novel primal-dual analysis of a classical approach for diameter computation that leads to algorithms for radius, diameter and all eccentricities with theoretical guarantees with respect to certain graph parameters. This is complemented by experimental results on various types of real-world graphs showing that these parameters appear to be low in practice. Finally, we obtain refined results for several graph classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:1803.04660v5</guid>
      <category>cs.DM</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.NI</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 2025 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), Jan 2025, New Orleans (LA), United States. pp.2157--2193</arxiv:journal_reference>
      <dc:creator>Feodor F. Dragan (UniBuc, ICI), Guillaume Ducoffe (UniBuc, ICI), Michel Habib (IRIF), Laurent Viennot (DI-ENS, ARGO)</dc:creator>
    </item>
    <item>
      <title>Faster Exponential-Time Approximation Algorithms Using Approximate Monotone Local Search</title>
      <link>https://arxiv.org/abs/2206.13481</link>
      <description>arXiv:2206.13481v2 Announce Type: replace-cross 
Abstract: We generalize the monotone local search approach of Fomin, Gaspers, Lokshtanov and Saurabh [J. ACM 2019], by establishing a connection between parameterized approximation and exponential-time approximation algorithms for monotone subset minimization problems. In a monotone subset minimization problem the input implicitly describes a non-empty set family over a universe of size $n$ which is closed under taking supersets. The task is to find a minimum cardinality set in this family. Broadly speaking, we use approximate monotone local search to show that a parameterized $\alpha$-approximation algorithm that runs in $c^k \cdot n^{O(1)}$ time, where $k$ is the solution size, can be used to derive an $\alpha$-approximation randomized algorithm that runs in $d^n \cdot n^{O(1)}$ time, where $d$ is the unique value in $d \in (1,1+\frac{c-1}{\alpha})$ such that $\mathcal{D}(\frac{1}{\alpha}\|\frac{d-1}{c-1})=\frac{\ln c}{\alpha}$ and $\mathcal{D}(a \|b)$ is the Kullback-Leibler divergence. This running time matches that of Fomin et al. for $\alpha=1$, and is strictly better when $\alpha &gt;1$, for any $c &gt; 1$. Furthermore, we also show that this result can be derandomized at the expense of a sub-exponential multiplicative factor in the running time.
  We demonstrate the potential of approximate monotone local search by deriving new and faster exponential approximation algorithms for Vertex Cover, $3$-Hitting Set, Directed Feedback Vertex Set, Directed Subset Feedback Vertex Set, Directed Odd Cycle Transversal and Undirected Multicut. For instance, we get a $1.1$-approximation algorithm for Vertex Cover with running time $1.114^n \cdot n^{O(1)}$, improving upon the previously best known $1.1$-approximation running in time $1.127^n \cdot n^{O(1)}$ by Bourgeois et al. [DAM 2011].</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.13481v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Tue, 13 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bar{\i}\c{s} Can Esmer, Ariel Kulik, D\'aniel Marx, Daniel Neuen, Roohani Sharma</dc:creator>
    </item>
  </channel>
</rss>
