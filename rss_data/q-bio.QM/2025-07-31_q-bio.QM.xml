<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 31 Jul 2025 04:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Sleep and Activity Patterns as Transdiagnostic Behavioral Biomarkers in Psychiatry: Initial Insights from the DeeP-DD study</title>
      <link>https://arxiv.org/abs/2507.22088</link>
      <description>arXiv:2507.22088v1 Announce Type: new 
Abstract: Background: Symptom rating scales in psychiatry are limited by reliance on self-report, and lack of predictive power. Actigraphy, a passive wearable-based method for measuring sleep and physical activity, offers objective, high-resolution behavioral data that may better reflect symptom fluctuations, but most studies have focused on narrow diagnostic groups or fixed time windows, limiting clinical translation. Objective: To examine whether actigraphy-derived sleep and activity features correlate with psychiatric symptom severity in a transdiagnostic psychiatric sample, and to identify which features are most clinically relevant across multiple temporal resolutions. Methods: We present a feasibility case series study with preliminary data from eight outpatients enrolled in the DeeP-DD study, a transdiagnostic study of digital phenotyping. Participants wore GENEActiv actigraphy devices and symptom severity was measured using a variety of validated scales. We performed intra-individual Spearman correlations and inter-individual repeated measures correlations across daily, weekly, monthly, and full-duration averages. Results: Intra-individual analyses revealed that later rise times were significantly associated with higher weekly PHQ-9 scores in participant #7 (\r{ho} = 0.74, P=.0003) and participant #4 (\r{ho} = 0.78, P=.022), as well as higher weekly GAD-7 scores in participant #7 (\r{ho} = 0.59, P=.026). Inter-individual analyses showed that weeks with later average rise time correlated with higher PHQ-9 (r = 0.48, P=.0003) and GAD-7 scores (r = 0.38, P=.032). Increased light physical activity was linked to lower PHQ-9 scores weekly (r = -0.44, P=.001) and monthly (r = -0.53, P=.014). Conclusion: Consistent associations between actigraphy features and symptoms across temporal scales and diagnostic groups underscore their potential utility for scalable, real-world clinical monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22088v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dylan Hamitouche, Tihare Zamorano, Youcef Barkat, Deven Parekh, Lena Palaniyappan, David Benrimoh</dc:creator>
    </item>
    <item>
      <title>Pathology Foundation Models are Scanner Sensitive: Benchmark and Mitigation with Contrastive ScanGen Loss</title>
      <link>https://arxiv.org/abs/2507.22092</link>
      <description>arXiv:2507.22092v1 Announce Type: new 
Abstract: Computational pathology (CPath) has shown great potential in mining actionable insights from Whole Slide Images (WSIs). Deep Learning (DL) has been at the center of modern CPath, and while it delivers unprecedented performance, it is also known that DL may be affected by irrelevant details, such as those introduced during scanning by different commercially available scanners. This may lead to scanner bias, where the model outputs for the same tissue acquired by different scanners may vary. In turn, it hinders the trust of clinicians in CPath-based tools and their deployment in real-world clinical practices. Recent pathology Foundation Models (FMs) promise to provide better domain generalization capabilities. In this paper, we benchmark FMs using a multi-scanner dataset and show that FMs still suffer from scanner bias. Following this observation, we propose ScanGen, a contrastive loss function applied during task-specific fine-tuning that mitigates scanner bias, thereby enhancing the models' robustness to scanner variations. Our approach is applied to the Multiple Instance Learning task of Epidermal Growth Factor Receptor (EGFR) mutation prediction from H\&amp;E-stained WSIs in lung cancer. We observe that ScanGen notably enhances the ability to generalize across scanners, while retaining or improving the performance of EGFR mutation prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22092v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <category>q-bio.TO</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gianluca Carloni, Biagio Brattoli, Seongho Keum, Jongchan Park, Taebum Lee, Chang Ho Ahn, Sergio Pereira</dc:creator>
    </item>
    <item>
      <title>Scaling and Data Saturation in Protein Language Models</title>
      <link>https://arxiv.org/abs/2507.22210</link>
      <description>arXiv:2507.22210v1 Announce Type: new 
Abstract: Data in biology is redundant, noisy, and sparse. How does the type and scale of available data impact model performance? In this work, we specifically investigate how protein language models (pLMs) scale with increasing pretraining data. We investigate this relationship by measuring the performance of protein function prediction on a suite of pLMs pretrained on yearly snapshots of UniRef100 from 2011 to 2024. We find no evidence of model saturation on this task: performance improves--but not monotonically--with added data, and this trend differs between unsupervised and supervised experiments. Using a well-characterized Beta-Lactamase protein from E. coli, we find that unsupervised model predictions get better year-over-year, though they do not yet consistently perform better than the supervised baseline. Our results underscore the need for targeted data acquisition and deeper study of data scaling in protein modeling. All training, inference, analysis, and visualization code is available at: https://github.com/Align-to-Innovate/data-saturation-and-scaling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22210v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aviv Spinner, Erika DeBenedictis, Corey M. Hudson</dc:creator>
    </item>
    <item>
      <title>Evaluating Integrative Strategies for Incorporating Phenotypic Features in Spatial Transcriptomics</title>
      <link>https://arxiv.org/abs/2507.22212</link>
      <description>arXiv:2507.22212v1 Announce Type: new 
Abstract: Spatial transcriptomics (ST) technologies not only offer an unprecedented opportunity to interrogate intact biological samples in a spatially informed manner, but also set the stage for integration with other imaging-based modalities. However, how to best exploit spatial context and integrate ST with imaging-based modalities remains an open question. To address this, particularly under real-world experimental constraints such as limited dataset size, class imbalance, and bounding-box-based segmentation, we used a publicly available murine ileum MERFISH dataset to evaluate whether a minimally tuned variational autoencoder (VAE) could extract informative low-dimensional representations from cell crops of spot counts, nuclear stain, membrane stain, or a combination thereof. We assessed the resulting embeddings through PERMANOVA, cross-validated classification, and unsupervised Leiden clustering, and compared them to classical image-based feature vectors extracted via CellProfiler. While transcript counts (TC) generally outperformed other feature spaces, the VAE-derived latent spaces (LSs) captured meaningful biological variation and enabled improved label recovery for specific cell types. LS2, in particular, trained solely on morphological input, also exhibited moderate predictive power for a handful of genes in a ridge regression model. Notably, combining TC with LSs through multiplex clustering led to consistent gains in cluster homogeneity, a trend that also held when augmenting only subsets of TC with the stain-derived LS2. In contrast, CellProfiler-derived features underperformed relative to LSs, highlighting the advantage of learned representations over hand-crafted features. Collectively, these findings demonstrate that even under constrained conditions, VAEs can extract biologically meaningful signals from imaging data and constitute a promising strategy for multi-modal integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22212v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Levin M Moser, Ahmad Kamal Hamid, Esteban Miglietta, Nodar Gogoberidze, Beth A Cimini</dc:creator>
    </item>
    <item>
      <title>Spatiodynamic inference using vision-based generative modelling</title>
      <link>https://arxiv.org/abs/2507.22256</link>
      <description>arXiv:2507.22256v1 Announce Type: new 
Abstract: Biological systems commonly exhibit complex spatiotemporal patterns whose underlying generative mechanisms pose a significant analytical challenge. Traditional approaches to spatiodynamic inference rely on dimensionality reduction through summary statistics, which sacrifice complexity and interdependent structure intrinsic to these data in favor of parameter identifiability. This imposes a fundamental constraint on reliably extracting mechanistic insights from spatiotemporal data, highlighting the need for analytical frameworks that preserve the full richness of these dynamical systems. To address this, we developed a simulation-based inference framework that employs vision transformer-driven variational encoding to generate compact representations of the data, exploiting the inherent contextual dependencies. These representations are subsequently integrated into a likelihood-free Bayesian approach for parameter inference. The central idea is to construct a fine-grained, structured mesh of latent representations from simulated dynamics through systematic exploration of the parameter space. This encoded mesh of latent embeddings then serves as a reference map for retrieving parameter values that correspond to observed data. By integrating generative modeling with Bayesian principles, our approach provides a unified inference framework to identify both spatial and temporal patterns that manifest in multivariate dynamical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22256v1</guid>
      <category>q-bio.QM</category>
      <category>q-bio.PE</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun Won Park, Kangyu Zhao, Sanket Rane</dc:creator>
    </item>
    <item>
      <title>Deep learning of geometrical cell division rules</title>
      <link>https://arxiv.org/abs/2507.22587</link>
      <description>arXiv:2507.22587v1 Announce Type: cross 
Abstract: The positioning of new cellular walls during cell division plays a key role in shaping plant tissue organization. The influence of cell geometry on the positioning of division planes has been previously captured into various geometrical rules. Accordingly, linking cell shape to division orientation has relied on the comparison between observed division patterns and predictions under specific rules. The need to define a priori the tested rules is a fundamental limitation of this hypothesis-driven approach. As an alternative, we introduce a data-based approach to investigate the relation between cell geometry and division plane positioning, exploiting the ability of deep neural network to learn complex relationships across multidimensional spaces. Adopting an image-based cell representation, we show how division patterns can be learned and predicted from mother cell geometry using a UNet architecture modified to operate on cell masks. Using synthetic data and A. thaliana embryo cells, we evaluate the model performances on a wide range of diverse cell shapes and division patterns. We find that the trained model accounted for embryo division patterns that were previously irreconcilable under existing geometrical rules. Our work shows the potential of deep networks to understand cell division patterns and to generate new hypotheses on the control of cell division positioning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22587v1</guid>
      <category>cs.LG</category>
      <category>q-bio.CB</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Alexandre Durrmeyer, Jean-Christophe Palauqui, Philippe Andrey</dc:creator>
    </item>
    <item>
      <title>Enhanced Prediction of CAR T-Cell Cytotoxicity with Quantum-Kernel Methods</title>
      <link>https://arxiv.org/abs/2507.22710</link>
      <description>arXiv:2507.22710v1 Announce Type: cross 
Abstract: Chimeric antigen receptor (CAR) T-cells are T-cells engineered to recognize and kill specific tumor cells. Through their extracellular domains, CAR T-cells bind tumor cell antigens which triggers CAR T activation and proliferation. These processes are regulated by co-stimulatory domains present in the intracellular region of the CAR T-cell. Through integrating novel signaling components into the co-stimulatory domains, it is possible to modify CAR T-cell phenotype. Identifying and experimentally testing new CAR constructs based on libraries of co-stimulatory domains is nontrivial given the vast combinatorial space defined by such libraries. This leads to a highly data constrained, poorly explored combinatorial problem, where the experiments undersample all possible combinations. We propose a quantum approach using a Projected Quantum Kernel (PQK) to address this challenge. PQK operates by embedding classical data into a high dimensional Hilbert space and employs a kernel method to measure sample similarity. Using 61 qubits on a gate-based quantum computer, we demonstrate the largest PQK application to date and an enhancement in the classification performance over purely classical machine learning methods for CAR T cytotoxicity prediction. Importantly, we show improved learning for specific signaling domains and domain positions, particularly where there was lower information highlighting the potential for quantum computing in data-constrained problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.22710v1</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>quant-ph</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Filippo Utro, Meltem Tolunay, Kahn Rhrissorrakrai, Tanvi P. Gujarati, Jie Shi, Sara Capponi, Mirko Amico, Nate Earnest-Noble, Laxmi Parida</dc:creator>
    </item>
    <item>
      <title>Inferring biological processes with intrinsic noise from cross-sectional data</title>
      <link>https://arxiv.org/abs/2410.07501</link>
      <description>arXiv:2410.07501v2 Announce Type: replace-cross 
Abstract: Inferring dynamical models from data continues to be a significant challenge in computational biology, especially given the stochastic nature of many biological processes. We explore a common scenario in omics, where statistically independent cross-sectional samples are available at a few time points, and the goal is to infer the underlying diffusion process that generated the data. Existing inference approaches often simplify or ignore noise intrinsic to the system, compromising accuracy for the sake of optimization ease. We circumvent this compromise by inferring the phase-space probability flow that shares the same time-dependent marginal distributions as the underlying stochastic process. Our approach, probability flow inference (PFI), disentangles force from intrinsic stochasticity while retaining the algorithmic ease of ODE inference. Analytically, we prove that for Ornstein-Uhlenbeck processes the regularized PFI formalism yields a unique solution in the limit of well-sampled distributions. In practical applications, we show that PFI enables accurate parameter and force estimation in high-dimensional stochastic reaction networks, and that it allows inference of cell differentiation dynamics with molecular noise, outperforming state-of-the-art approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07501v2</guid>
      <category>cs.LG</category>
      <category>physics.bio-ph</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Suryanarayana Maddu, Victor Chard\`es, Michael. J. Shelley</dc:creator>
    </item>
    <item>
      <title>TrajLens: Visual Analysis for Constructing Cell Developmental Trajectories in Cross-Sample Exploration</title>
      <link>https://arxiv.org/abs/2507.15620</link>
      <description>arXiv:2507.15620v2 Announce Type: replace-cross 
Abstract: Constructing cell developmental trajectories is a critical task in single-cell RNA sequencing (scRNA-seq) analysis, enabling the inference of potential cellular progression paths. However, current automated methods are limited to establishing cell developmental trajectories within individual samples, necessitating biologists to manually link cells across samples to construct complete cross-sample evolutionary trajectories that consider cellular spatial dynamics. This process demands substantial human effort due to the complex spatial correspondence between each pair of samples. To address this challenge, we first proposed a GNN-based model to predict cross-sample cell developmental trajectories. We then developed TrajLens, a visual analytics system that supports biologists in exploring and refining the cell developmental trajectories based on predicted links. Specifically, we designed the visualization that integrates features on cell distribution and developmental direction across multiple samples, providing an overview of the spatial evolutionary patterns of cell populations along trajectories. Additionally, we included contour maps superimposed on the original cell distribution data, enabling biologists to explore them intuitively. To demonstrate our system's performance, we conducted quantitative evaluations of our model with two case studies and expert interviews to validate its usefulness and effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15620v2</guid>
      <category>cs.CG</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 31 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qipeng Wang, Shaolun Ruan, Rui Sheng, Yong Wang, Min Zhu, Huamin Qu</dc:creator>
    </item>
  </channel>
</rss>
