<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 Aug 2024 04:00:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 06 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Can multivariate Granger causality detect directed connectivity of a multistable and dynamic biological decision network model?</title>
      <link>https://arxiv.org/abs/2408.01528</link>
      <description>arXiv:2408.01528v1 Announce Type: new 
Abstract: Extracting causal connections can advance interpretable AI and machine learning. Granger causality (GC) is a robust statistical method for estimating directed influences (DC) between signals. While GC has been widely applied to analysing neuronal signals in biological neural networks and other domains, its application to complex, nonlinear, and multistable neural networks is less explored. In this study, we applied time-domain multi-variate Granger causality (MVGC) to the time series neural activity of all nodes in a trained multistable biologically based decision neural network model with real-time decision uncertainty monitoring. Our analysis demonstrated that challenging two-choice decisions, where input signals could be closely matched, and the appropriate application of fine-grained sliding time windows, could readily reveal the original model's DC. Furthermore, the identified DC varied based on whether the network had correct or error decisions. Integrating the identified DC from different decision outcomes recovered most of the original model's architecture, despite some spurious and missing connectivity. This approach could be used as an initial exploration to enhance the interpretability and transparency of dynamic multistable and nonlinear biological or AI systems by revealing causal connections throughout different phases of neural network dynamics and outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01528v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>math.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abdoreza Asadpour, KongFatt Wong-Lin</dc:creator>
    </item>
    <item>
      <title>Probabilistic modeling reveals coordinated social interaction states and their multisensory bases</title>
      <link>https://arxiv.org/abs/2408.01683</link>
      <description>arXiv:2408.01683v1 Announce Type: new 
Abstract: Social behavior across animal species ranges from simple pairwise interactions to thousands of individuals coordinating goal-directed movements. Regardless of the scale, these interactions are governed by the interplay between multimodal sensory information and the internal state of each animal. Here, we investigate how animals use multiple sensory modalities to guide social behavior in the highly social zebrafish (Danio rerio) and uncover the complex features of pairwise interactions early in development. To identify distinct behaviors and understand how they vary over time, we developed a new hidden Markov model with constrained linear-model emissions to automatically classify states of coordinated interaction, using the movements of one animal to predict those of another. We discovered that social behaviors alternate between two interaction states within a single experimental session, distinguished by unique movements and timescales. Long-range interactions, akin to shoaling, rely on vision, while mechanosensation underlies rapid synchronized movements and parallel swimming, precursors of schooling. Altogether, we observe spontaneous interactions in pairs of fish, develop novel hidden Markov modeling to reveal two fundamental interaction modes, and identify the sensory systems involved in each. Our modeling approach to pairwise social interactions has broad applicability to a wide variety of naturalistic behaviors and species and solves the challenge of detecting transient couplings between quasi-periodic time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01683v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sarah Josephine Stednitz, Andrew Lesak, Adeline L Fecker, Peregrine Painter, Phil Washbourne, Luca Mazzucato, Ethan K Scott</dc:creator>
    </item>
    <item>
      <title>State-dependent Filtering of the Ring Model</title>
      <link>https://arxiv.org/abs/2408.01817</link>
      <description>arXiv:2408.01817v1 Announce Type: new 
Abstract: Robustness is a measure of functional reliability of a system against perturbations. To achieve a good and robust performance, a system must filter out external perturbations by its internal priors. These priors are usually distilled in the structure and the states of the system. Biophysical neural network are known to be robust but the exact mechanisms are still elusive. In this paper, we probe how orientation-selective neurons organized on a 1-D ring network respond to perturbations in the hope of gaining some insights on the robustness of visual system in brain. We analyze the steady-state of the rate-based network and prove that the activation state of neurons, rather than their firing rates, determines how the model respond to perturbations. We then identify specific perturbation patterns that induce the largest responses for different configurations of activation states, and find them to be sinusoidal or sinusoidal-like while other patterns are largely attenuated. Similar results are observed in a spiking ring model. Finally, we remap the perturbations in orientation back into the 2-D image space using Gabor functions. The resulted optimal perturbation patterns mirror adversarial attacks in deep learning that exploit the priors of the system. Our results suggest that based on different state configurations, these priors could underlie some of the illusionary experiences as the cost of visual robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01817v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jing Yan, Yunxuan Feng, Wei Dai, Yaoyu Zhang</dc:creator>
    </item>
    <item>
      <title>Efficient coding with chaotic neural networks: A journey from neuroscience to physics and back</title>
      <link>https://arxiv.org/abs/2408.01949</link>
      <description>arXiv:2408.01949v1 Announce Type: new 
Abstract: This essay, derived from a lecture at "The Physics Modeling of Thought" workshop in Berlin in winter 2023, explores the mutually beneficial relationship between theoretical neuroscience and statistical physics through the lens of efficient coding and computation in cortical circuits. It highlights how the study of neural networks has enhanced our understanding of complex, nonequilibrium, and disordered systems, while also demonstrating how neuroscientific challenges have spurred novel developments in physics. The paper traces the evolution of ideas from seminal work on chaos in random neural networks to recent developments in efficient coding and the partial suppression of chaotic fluctuations. It emphasizes how concepts from statistical physics, such as phase transitions and critical phenomena, have been instrumental in elucidating the computational capabilities of neural networks.
  By examining the interplay between order and disorder in neural computation, the essay illustrates the deep connection between theoretical neuroscience and the statistical physics of nonequilibrium systems. This synthesis underscores the ongoing importance of interdisciplinary approaches in advancing both fields, offering fresh perspectives on the fundamental principles governing information processing in biological and artificial systems. This multidisciplinary approach not only advances our understanding of neural computation and complex systems but also points toward future challenges at the intersection of neuroscience and physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01949v1</guid>
      <category>q-bio.NC</category>
      <category>nlin.CD</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Kadmon</dc:creator>
    </item>
    <item>
      <title>Selective pruning and neuronal death generate heavy-tail network connectivity</title>
      <link>https://arxiv.org/abs/2408.02625</link>
      <description>arXiv:2408.02625v1 Announce Type: new 
Abstract: From the proliferative mechanisms generating neurons from progenitor cells to neuron migration and synaptic connection formation, several vicissitudes culminate in the mature brain. Both component loss and gain remain ubiquitous during brain development. For example, rodent brains lose over half of their initial neurons and synapses during healthy development. The role of deleterious steps in network ontogeny remains unclear, yet it is unlikely these costly processes are random. Like neurogenesis and synaptogenesis, synaptic pruning and neuron death likely evolved to support complex, efficient computations. In order to incorporate both component loss and gain in describing neuronal networks, we propose an algorithm where a directed network evolves through the selective deletion of less-connected nodes (neurons) and edges (synapses). Resulting in networks that display scale-invariant degree distributions, provided the network is predominantly feed-forward. Scale-invariance offers several advantages in biological networks: scalability, resistance to random deletions, and strong connectivity with parsimonious wiring. Whilst our algorithm is not intended to be a realistic model of neuronal network formation, our results suggest selective deletion is an adaptive mechanism contributing to more stable and efficient networks. This process aligns with observed decreasing pruning rates in animal studies, resulting in higher synapse preservation. Our overall findings have broader implications for network science. Scale-invariance in degree distributions was demonstrated in growing preferential attachment networks and observed empirically. Our preferential detachment algorithm offers an alternative mechanism for generating such networks, suggesting that both mechanisms may be part of a broader class of algorithms resulting in scale-free networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02625v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.PE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rodrigo Siqueira Kazu, Kleber Neves, Bruno Mota</dc:creator>
    </item>
    <item>
      <title>Towards principles of brain network organization and function</title>
      <link>https://arxiv.org/abs/2408.02640</link>
      <description>arXiv:2408.02640v1 Announce Type: new 
Abstract: The brain is immensely complex, with diverse components and dynamic interactions building upon one another to orchestrate a wide range of functions and behaviors. Understanding patterns of these complex interactions and how they are coordinated to support collective neural activity and function is critical for parsing human and animal behavior, treating mental illness, and developing artificial intelligence. Rapid experimental advances in imaging, recording, and perturbing neural systems across various species now provide opportunities and challenges to distill underlying principles of brain organization and function. Here, we take stock of recent progresses and review methods used in the statistical analysis of brain networks, drawing from fields of statistical physics, network theory and information theory. Our discussion is organized by scale, starting with models of individual neurons and extending to large-scale networks mapped across brain regions. We then examine the organizing principles and constraints that shape the biological structure and function of neural circuits. Finally, we describe current opportunities aimed at improving models in light of recent developments and at bridging across scales to contribute to a better understanding of brain networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02640v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.bio-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suman Kulkarni, Dani S. Bassett</dc:creator>
    </item>
    <item>
      <title>Automatic rating of incomplete hippocampal inversions evaluated across multiple cohorts</title>
      <link>https://arxiv.org/abs/2408.02496</link>
      <description>arXiv:2408.02496v1 Announce Type: cross 
Abstract: Incomplete Hippocampal Inversion (IHI), sometimes called hippocampal malrotation, is an atypical anatomical pattern of the hippocampus found in about 20% of the general population. IHI can be visually assessed on coronal slices of T1 weighted MR images, using a composite score that combines four anatomical criteria. IHI has been associated with several brain disorders (epilepsy, schizophrenia). However, these studies were based on small samples. Furthermore, the factors (genetic or environmental) that contribute to the genesis of IHI are largely unknown. Large-scale studies are thus needed to further understand IHI and their potential relationships to neurological and psychiatric disorders. However, visual evaluation is long and tedious, justifying the need for an automatic method. In this paper, we propose, for the first time, to automatically rate IHI. We proceed by predicting four anatomical criteria, which are then summed up to form the IHI score, providing the advantage of an interpretable score. We provided an extensive experimental investigation of different machine learning methods and training strategies. We performed automatic rating using a variety of deep learning models (conv5-FC3, ResNet and SECNN) as well as a ridge regression. We studied the generalization of our models using different cohorts and performed multi-cohort learning. We relied on a large population of 2,008 participants from the IMAGEN study, 993 and 403 participants from the QTIM/QTAB studies as well as 985 subjects from the UKBiobank. We showed that deep learning models outperformed a ridge regression. We demonstrated that the performances of the conv5-FC3 network were at least as good as more complex networks while maintaining a low complexity and computation time. We showed that training on a single cohort may lack in variability while training on several cohorts improves generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02496v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.59275/j.melba.2024-3d4e</arxiv:DOI>
      <arxiv:journal_reference>Machine.Learning.for.Biomedical.Imaging. 2 (2024)</arxiv:journal_reference>
      <dc:creator>Lisa Hemforth, Baptiste Couvy-Duchesne, Kevin De Matos, Camille Brianceau, Matthieu Joulot, Tobias Banaschewski, Arun L. W. Bokde, Sylvane Desrivi\`eres, Herta Flor, Antoine Grigis, Hugh Garavan, Penny Gowland, Andreas Heinz, R\"udiger Br\"uhl, Jean-Luc Martinot, Marie-Laure Paill\`ere Martinot, Eric Artiges, Dimitri Papadopoulos, Herve Lemaitre, Tomas Paus, Luise Poustka, Sarah Hohmann, Nathalie Holz, Juliane H. Fr\"ohner, Michael N. Smolka, Nilakshi Vaidya, Henrik Walter, Robert Whelan, Gunter Schumann, Christian B\"uchel, JB Poline, Bernd Itterman, Vincent Frouin, Alexandre Martin, IMAGEN study group, Claire Cury, Olivier Colliot</dc:creator>
    </item>
    <item>
      <title>Nonlinear Model Predictive Control of a Conductance-Based Neuron Model via Data-Driven Forecasting</title>
      <link>https://arxiv.org/abs/2312.14274</link>
      <description>arXiv:2312.14274v2 Announce Type: replace 
Abstract: Objective. Precise control of neural systems is essential to experimental investigations of how the brain controls behavior and holds the potential for therapeutic manipulations to correct aberrant network states. Model predictive control, which employs a dynamical model of the system to find optimal control inputs, has promise for dealing with the nonlinear dynamics, high levels of exogenous noise, and limited information about unmeasured states and parameters that are common in a wide range of neural systems. However, the challenge still remains of selecting the right model, constraining its parameters, and synchronizing to the neural system. Approach. As a proof of principle, we used recent advances in data-driven forecasting to construct a nonlinear machine-learning model of a Hodgkin-Huxley type neuron when only the membrane voltage is observable and there are an unknown number of intrinsic currents. Main Results. We show that this approach is able to learn the dynamics of different neuron types and can be used with MPC to force the neuron to engage in arbitrary, researcher-defined spiking behaviors. Significance. To the best of our knowledge, this is the first application of nonlinear MPC of a conductance-based model where there is only realistically limited information about unobservable states and parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14274v2</guid>
      <category>q-bio.NC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christof Fehrman, C. Daniel Meliza</dc:creator>
    </item>
    <item>
      <title>Object Space is Embodied</title>
      <link>https://arxiv.org/abs/2406.19659</link>
      <description>arXiv:2406.19659v2 Announce Type: replace 
Abstract: The perceived similarity between objects has often been attributed to their physical and conceptual features, such as appearance and animacy, and the theoretical framework of object space is accordingly conceived. Here, we extend this framework by proposing that object space may also be defined by embodied features, specifically action possibilities that objects afford to an agent (i.e., affordance) and their spatial relation with the agent (i.e., situatedness). To test this proposal, we quantified the embodied features with a set of action atoms. We found that embodied features explained the subjective similarity among familiar objects along with the objects' visual features. This observation was further replicated with novel objects. Our study demonstrates that embodied features, which place objects within an ecological context, are essential in constructing object space in the human visual system, emphasizing the importance of incorporating embodiment as a fundamental dimension in our understanding of the visual world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19659v2</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shan Xu, Xinran Feng, Yuannan Li, Jia Liu</dc:creator>
    </item>
    <item>
      <title>Use-dependent Biases as Optimal Action under Information Bottleneck</title>
      <link>https://arxiv.org/abs/2407.17793</link>
      <description>arXiv:2407.17793v2 Announce Type: replace 
Abstract: Use-dependent bias is a phenomenon in human sensorimotor behavior whereby movements become biased towards previously repeated actions. Despite being well-documented, the reason why this phenomenon occurs is not year clearly understood. Here, we propose that use-dependent biases can be understood as a rational strategy for movement under limitations on the capacity to process sensory information to guide motor output. We adopt an information-theoretic approach to characterize sensorimotor information processing and determine how behavior should be optimized given limitations to this capacity. We show that this theory naturally predicts the existence of use-dependent biases. Our framework also generates two further predictions. The first prediction relates to handedness. The dominant hand is associated with enhanced dexterity and reduced movement variability compared to the non-dominant hand, which we propose relates to a greater capacity for information processing in regions that control movement of the dominant hand. Consequently, the dominant hand should exhibit smaller use-dependent biases compared to the non-dominant hand. The second prediction relates to how use-dependent biases are affected by movement speed. When moving faster, it is more challenging to correct for initial movement errors online during the movement. This should exacerbate costs associated with initial directional error and, according to our theory, reduce the extent of use-dependent biases compared to slower movements, and vice versa. We show that these two empirical predictions, the handedness effect and the speed-dependent effect, are confirmed by experimental data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17793v2</guid>
      <category>q-bio.NC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hokin Deng, Adrian Haith</dc:creator>
    </item>
    <item>
      <title>Asynchronous Bioplausible Neuron for SNN for Event Vision</title>
      <link>https://arxiv.org/abs/2311.11853</link>
      <description>arXiv:2311.11853v2 Announce Type: replace-cross 
Abstract: Spiking Neural Networks (SNNs) offer a biologically inspired approach to computer vision that can lead to more efficient processing of visual data with reduced energy consumption. However, maintaining homeostasis within these networks is challenging, as it requires continuous adjustment of neural responses to preserve equilibrium and optimal processing efficiency amidst diverse and often unpredictable input signals. In response to these challenges, we propose the Asynchronous Bioplausible Neuron (ABN), a dynamic spike firing mechanism to auto-adjust the variations in the input signal. Comprehensive evaluation across various datasets demonstrates ABN's enhanced performance in image classification and segmentation, maintenance of neural equilibrium, and energy efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11853v2</guid>
      <category>cs.NE</category>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanket Kachole, Hussain Sajwani, Fariborz Baghaei Naeini, Dimitrios Makris, Yahya Zweiri</dc:creator>
    </item>
    <item>
      <title>A brief tutorial on information theory</title>
      <link>https://arxiv.org/abs/2402.16556</link>
      <description>arXiv:2402.16556v3 Announce Type: replace-cross 
Abstract: At the 2023 Les Houches Summer School on Theoretical Biological Physics, several students asked for some background on information theory, and so we added a tutorial to the scheduled lectures. This is largely a transcript of that tutorial, lightly edited. It covers basic definitions and context rather than detailed calculations. We hope to have maintained the informality of the presentation, including exchanges with the students, while still being useful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16556v3</guid>
      <category>physics.bio-ph</category>
      <category>q-bio.MN</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tarek Tohme, William Bialek</dc:creator>
    </item>
  </channel>
</rss>
