<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Mar 2025 01:55:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Distributed Generalized Linear Models: A Privacy-Preserving Approach</title>
      <link>https://arxiv.org/abs/2503.15287</link>
      <description>arXiv:2503.15287v1 Announce Type: new 
Abstract: This paper presents a novel approach to classical linear regression, enabling model computation from data streams or in a distributed setting while preserving data privacy in federated environments. We extend this framework to generalized linear models (GLMs), ensuring scalability and adaptability to diverse data distributions while maintaining privacy-preserving properties. To assess the effectiveness of our approach, we conduct numerical studies on both simulated and real datasets, comparing our method with conventional maximum likelihood estimation for GLMs using iteratively reweighted least squares. Our results demonstrate the advantages of the proposed method in distributed and federated settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15287v1</guid>
      <category>stat.CO</category>
      <category>cs.DC</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Daniel Tinoco, Raquel Menezes, Carlos Baquero</dc:creator>
    </item>
    <item>
      <title>Bayesian Sociality Models: A Scalable and Flexible Alternative for Network Analysis</title>
      <link>https://arxiv.org/abs/2503.14697</link>
      <description>arXiv:2503.14697v1 Announce Type: cross 
Abstract: Bayesian sociality models provide a scalable and flexible alternative for network analysis, capturing degree heterogeneity through actor-specific parameters while mitigating the identifiability challenges of latent space models. This paper develops a comprehensive Bayesian inference framework, leveraging Markov chain Monte Carlo and variational inference to assess their efficiency-accuracy trade-offs. Through empirical and simulation studies, we demonstrate the model's robustness in goodness-of-fit, predictive performance, clustering, and other key network analysis tasks. The Bayesian paradigm further enhances uncertainty quantification and interpretability, positioning sociality models as a powerful and generalizable tool for modern network science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14697v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Sosa, Carlo Mart\'inez</dc:creator>
    </item>
    <item>
      <title>PSInference: A Package to Draw Inference for Released Plug-in Sampling Single Synthetic Dataset</title>
      <link>https://arxiv.org/abs/2503.14711</link>
      <description>arXiv:2503.14711v1 Announce Type: cross 
Abstract: The development and generation of synthetic data are becoming increasingly vital in the field of statistical disclosure control. The PSInference package provides tools to perform exact inferential analysis on singly imputed synthetic data generated through Plug-in Sampling assuming that the original dataset follows a multivariate normal distribution. Includes functions to test the synthetic data's covariance structure, covering aspects like generalized variance, sphericity, independence between subsets of variables, and regression of one set of variables on another. This package addresses the gap in the existing software by providing exact inferential methods suitable for cases where only a single synthetic dataset is released.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14711v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ricardo Moura, Mina Norouzirad, Vitor Augusto, Miguel Fonseca</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for case-control point pattern data in spatial epidemiology with the \texttt{inlabru} R package</title>
      <link>https://arxiv.org/abs/2503.14954</link>
      <description>arXiv:2503.14954v1 Announce Type: cross 
Abstract: The analysis of case-control point pattern data is an important problem in spatial epidemiology. The spatial variation of cases if often compared to that of a set of controls to assess spatial risk variation as well as the detection of risk factors and exposure to putative pollution sources using spatial regression models. The intensities of the point patterns of cases and controls are estimated using log-Gaussian Cox models, so that fixed and spatial random effects can be included. Bayesian inference is conducted via the integrated Nested Laplace approximation (INLA) method using the inlabru R package. In this way, potential risk factors can be assessed by including them as fixed effects while residual spatial variation is considered as a Gaussian process with Mat\'ern covariance. In addition, exposure to pollution sources is modeled using different smooth terms. The proposed methods have been applied to the Chorley-Ribble dataset, that records the locations of lung and larynx cancer cases as well as the location of an disused old incinerator in the area of Lancashire (England, United Kingdom). Taking the locations of lung cancer as controls, the spatial variation of both types of cases has been estimated and the increase of larynx cases in the vicinity of the incinerator has been assessed. The results are similar to those found in the literature. In a nutshell, a framework for Bayesian analysis of multivariate case-control point patterns within an epidemiological framework has been presented. Models to assess spatial variation and the effect of risk factors and pollution sources can be fit with ease with the inlabru R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14954v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francisco Palm\'i-Perales, Finn Lindgren, Virgilio G\'omez-Rubio</dc:creator>
    </item>
    <item>
      <title>Joint Hybrid Precoding and Multi-IRS Optimization for mmWave MU-MISO Communication Network</title>
      <link>https://arxiv.org/abs/2503.15261</link>
      <description>arXiv:2503.15261v1 Announce Type: cross 
Abstract: This paper attempts to jointly optimize the hybrid precoding (HP) and intelligent reflecting surfaces (IRS) beamforming matrices in a multi-IRS-aided mmWave communication network, utilizing the Alamouti scheme at the base station (BS). Considering the overall signal-to-noise ratio (SNR) as the objective function, the underlying problem is cast as an optimization problem, which is shown to be non-convex in general. To tackle the problem, noting that the unknown matrices contribute multiplicatively to the objective function, they are reformulated into two new matrices with rank constraints. Then, using the so-called inner approximation (IA) technique in conjunction with majorization-minimization (MM) approaches, these new matrices are solved iteratively. From one of these matrices, the IRS beamforming matrices can be effectively extracted. Meanwhile, HP precoding matrices can be solved separately through a new optimization problem aimed at minimizing the Euclidean distance between the fully digital (FD) precoder and HP analog/digital precoders. This is achieved through the use of a modified block coordinate descent (MBCD) algorithm. Simulation results demonstrate that the proposed algorithm outperforms various benchmark schemes in terms of achieving a higher achievable rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15261v1</guid>
      <category>eess.SP</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fardad Rahkheir, Soroush Akhlaghi</dc:creator>
    </item>
    <item>
      <title>fabisearch: A Package for Change Point Detection in and Visualization of the Network Structure of Multivariate High-Dimensional Time Series in R</title>
      <link>https://arxiv.org/abs/2207.02986</link>
      <description>arXiv:2207.02986v4 Announce Type: replace 
Abstract: Change point detection is a commonly used technique in time series analysis, capturing the dynamic nature in which many real-world processes function. With the ever increasing troves of multivariate high-dimensional time series data, especially in neuroimaging and finance, there is a clear need for scalable and data-driven change point detection methods. Currently, change point detection methods for multivariate high-dimensional data are scarce, with even less available in high-level, easily accessible software packages. To this end, we introduce the R package fabisearch, available on the Comprehensive R Archive Network (CRAN), which implements the factorized binary search (FaBiSearch) methodology. FaBiSearch is a novel statistical method for detecting change points in the network structure of multivariate high-dimensional time series which employs non-negative matrix factorization (NMF), an unsupervised dimension reduction and clustering technique. Given the high computational cost of NMF, we implement the method in C++ code and use parallelization to reduce computation time. Further, we also utilize a new binary search algorithm to efficiently identify multiple change points and provide a new method for network estimation for data between change points. We show the functionality of the package and the practicality of the method by applying it to a neuroimaging and a finance data set. Lastly, we provide an interactive, 3-dimensional, brain-specific network visualization capability in a flexible, stand-alone function. This function can be conveniently used with any node coordinate atlas, and nodes can be color coded according to community membership (if applicable). The output is an elegantly displayed network laid over a cortical surface, which can be rotated in the 3-dimensional space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.02986v4</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.neucom.2024.127321</arxiv:DOI>
      <dc:creator>Martin Ondrus, Ivor Cribben</dc:creator>
    </item>
    <item>
      <title>A Framework for Statistical Inference via Randomized Algorithms</title>
      <link>https://arxiv.org/abs/2307.11255</link>
      <description>arXiv:2307.11255v4 Announce Type: replace-cross 
Abstract: Randomized algorithms, such as randomized sketching or stochastic optimization, are a promising approach to ease the computational burden in analyzing large datasets. However, randomized algorithms also produce non-deterministic outputs, leading to the problem of evaluating their accuracy. In this paper, we develop a statistical inference framework for quantifying the uncertainty of the outputs of randomized algorithms.
  Our key conclusion is that one can perform statistical inference for the target of a sequence of randomized algorithms as long as in the limit, their outputs fluctuate around the target according to any (possibly unknown) probability distribution. In this setting, we develop appropriate statistical inference methods -- sub-randomization, multi-run plug-in and multi-run aggregation -- by estimating the unknown parameters of the limiting distribution either using multiple runs of the randomized algorithm, or by tailored estimates.
  As illustrations, we develop methods for statistical inference when using stochastic optimization (such as Polyak-Ruppert averaging in stochastic gradient descent and stochastic optimization with momentum). We also illustrate our methods in inference for least squares parameters via randomized sketching, by characterizing the limiting distributions of sketching estimates in a possibly growing dimensional case. We also characterize the computation and communication cost of our methods, showing that in certain cases, they add negligible overhead. The results are supported via a broad range of simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11255v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhixiang Zhang, Sokbae Lee, Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Bayesian Circular Regression with von Mises Quasi-Processes</title>
      <link>https://arxiv.org/abs/2406.13151</link>
      <description>arXiv:2406.13151v3 Announce Type: replace-cross 
Abstract: The need for regression models to predict circular values arises in many scientific fields. In this work we explore a family of expressive and interpretable distributions over circle-valued random functions related to Gaussian processes targeting two Euclidean dimensions conditioned on the unit circle. The probability model has connections with continuous spin models in statistical physics. Moreover, its density is very simple and has maximum-entropy, unlike previous Gaussian process-based approaches, which use wrapping or radial marginalization. For posterior inference, we introduce a new Stratonovich-like augmentation that lends itself to fast Gibbs sampling. We argue that transductive learning in these models favors a Bayesian approach to the parameters and apply our sampling scheme to the Double Metropolis-Hastings algorithm. We present experiments applying this model to the prediction of (i) wind directions and (ii) the percentage of the running gait cycle as a function of joint angles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13151v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 28th International Conference on Artificial Intelligence and Statistics (AISTATS) 2025</arxiv:journal_reference>
      <dc:creator>Yarden Cohen, Alexandre Khae Wu Navarro, Jes Frellsen, Richard E. Turner, Raziel Riemer, Ari Pakman</dc:creator>
    </item>
    <item>
      <title>A Principled Approach to Bayesian Transfer Learning</title>
      <link>https://arxiv.org/abs/2502.19796</link>
      <description>arXiv:2502.19796v2 Announce Type: replace-cross 
Abstract: Updating $\textit{a priori}$ information given some observed data is the core tenet of Bayesian inference. Bayesian transfer learning extends this idea by incorporating information from a related dataset to improve the inference on the observed data which may have been collected under slightly different settings. The use of related information can be useful when the observed data is scarce, for example. Current Bayesian transfer learning methods that are based on the so-called $\textit{power prior}$ can adaptively transfer information from related data. Unfortunately, it is not always clear under which scenario Bayesian transfer learning performs best or even if it will improve Bayesian inference. Additionally, current power prior methods rely on conjugacy to evaluate the posterior of interest. We propose using leave-one-out cross validation on the target dataset as a means of evaluating Bayesian transfer learning methods. Further, we introduce a new framework, $\textit{transfer sequential Monte Carlo}$, for power prior approaches that efficiently chooses the transfer parameter while avoiding the need for conjugate priors. We assess the performance of our proposed methods in two comprehensive simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19796v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Bretherton, Joshua J. Bon, David J. Warne, Kerrie Mengersen, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>Exploiting Inexact Computations in Multilevel Sampling Methods</title>
      <link>https://arxiv.org/abs/2503.05533</link>
      <description>arXiv:2503.05533v2 Announce Type: replace-cross 
Abstract: Multilevel sampling methods, such as multilevel and multifidelity Monte Carlo, multilevel stochastic collocation, or delayed acceptance Markov chain Monte Carlo, have become standard uncertainty quantification tools for a wide class of forward and inverse problems. The underlying idea is to achieve faster convergence by leveraging a hierarchy of models, such as partial differential equation (PDE) or stochastic differential equation (SDE) discretisations with increasing accuracy. By optimally redistributing work among the levels, multilevel methods can achieve significant performance improvement compared to single level methods working with one high-fidelity model. Intuitively, approximate solutions on coarser levels can tolerate large computational error without affecting the overall accuracy. We show how this can be used in high-performance computing applications to obtain a significant performance gain.
  As a use case, we analyse the computational error in the standard multilevel Monte Carlo method and formulate an adaptive algorithm which determines a minimum required computational accuracy on each level of discretisation. We show two examples of how the inexactness can be converted into actual gains using an elliptic PDE with lognormal random coefficients. Using a low precision sparse direct solver combined with iterative refinement results in a simulated gain in memory references of up to $3.5\times$ compared to the reference double precision solver; while using a MINRES iterative solver, a practical speedup of up to $1.5\times$ in terms of FLOPs is achieved. These results provide a step in the direction of energy-aware scientific computing, with significant potential for energy savings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05533v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Josef Mart\'inek, Erin Carson, Robert Scheichl</dc:creator>
    </item>
  </channel>
</rss>
