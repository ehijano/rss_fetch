<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Jan 2025 05:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A bang-bang optimal control for a nonlinear system modeling the Gate Control Theory of Pain</title>
      <link>https://arxiv.org/abs/2501.14821</link>
      <description>arXiv:2501.14821v1 Announce Type: new 
Abstract: We consider a nonlinear system of coupled ordinary differential equations (representing the excitatory, inhibitory, and T-cell potentials) based on the Gate Control Theory of Pain, initially proposed by R. Melzack and P.D. Wall in 1965, and later mathematically modeled by N.F. Britton and S.M. Skevington in 1988.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14821v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Gregorio Diaz, Jesus Ildefonso Diaz</dc:creator>
    </item>
    <item>
      <title>Iterative Proximal-Minimization for Computing Saddle Points with Fixed Index</title>
      <link>https://arxiv.org/abs/2501.14840</link>
      <description>arXiv:2501.14840v1 Announce Type: new 
Abstract: Computing saddle points with a prescribed Morse index on potential energy surfaces is crucial for characterizing transition states for nosie-induced rare transition events in physics and chemistry. Many numerical algorithms for this type of saddle points are based on the eigenvector-following idea and can be cast as an iterative minimization formulation (SINUM. Vol. 53, p.1786, 2015), but they may struggle with convergence issues and require good initial guesses. To address this challenge, we discuss the differential game interpretation of this iterative minimization formulation and investigate the relationship between this game's Nash equilibrium and saddle points on the potential energy surface. Our main contribution is that adding a proximal term, which grows faster than quadratic, to the game's cost function can enhance the stability and robustness. This approach produces a robust Iterative Proximal Minimization (IPM) algorithm for saddle point computing. We show that the IPM algorithm surpasses the preceding methods in robustness without compromising the convergence rate or increasing computational expense. The algorithm's efficacy and robustness are showcased through a two-dimensional test problem, and the Allen-Cahn, Cahn-Hilliard equation, underscoring its numerical robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14840v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuting Gu, Hao Zhang, Xiaoqun Zhang, Xiang Zhou</dc:creator>
    </item>
    <item>
      <title>Moving-Boundary Port-Hamiltonian Systems</title>
      <link>https://arxiv.org/abs/2501.14930</link>
      <description>arXiv:2501.14930v1 Announce Type: new 
Abstract: In this paper, we consider linear boundary port-Hamiltonian distributed parameter systems on a time-varying spatial domain. We derive the specific time-varying Dirac structure that these systems give rise to and use it to formally introduce a new class of moving-boundary port-Hamiltonian systems by showing that these distributed parameter systems on a time-varying spatial domain admit a port-Hamiltonian representation. We demonstrate that our results can be leveraged to develop a spatial discretization scheme with dynamic meshing for approximating the telegrapher's equations on a time-varying spatial domain, which we subsequently verify numerically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14930v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>T. J. Meijer, A. Das, S. Weiland</dc:creator>
    </item>
    <item>
      <title>Redefining Coherent Risk Measures: From Gauge Optimization to Regularization</title>
      <link>https://arxiv.org/abs/2501.14989</link>
      <description>arXiv:2501.14989v1 Announce Type: new 
Abstract: It is well understood that each coherent risk measure can be represented as the expectation with respect to the worst-case reweighted density function, chosen from an abstract risk envelope. This paper introduces an equivalent but more explicit definition of the risk envelope that uses gauge sets (i.e., a type of convex sets widely utilized in convex analysis and gauge optimization) to provide a generalized measure of distance between any reweighting function and the nominal one. Using the primal gauge set reweighting problem, we provide a unified framework for various existing methods in optimization under uncertainty, including risk-neutral/risk-averse stochastic programming, robust optimization, and distributionally robust optimization with moment-based and distance-based ambiguity sets. On the other hand, the associated dual problem offers an intuitive interpretation from the regularization perspective. This approach not only simplifies the derivation of classic results but also provides a versatile framework for robustness design via manipulations of the gauge sets (e.g., intersection, union, summation, convex combination, and function basis enforcement). To demonstrate this flexibility, we present approaches for customizing robustness to specific managerial needs, including methods for selecting flexible tail behaviors, addressing spatial distributional ambiguities, combining multiple robustness metrics, and achieving heterogeneous distributional robustness. We also discuss general reformulation techniques and computational approaches for this unified framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14989v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ningji Wei, Xian Yu, Peter Zhang</dc:creator>
    </item>
    <item>
      <title>Convergence Analysis of the Wasserstein Proximal Algorithm beyond Geodesic Convexity</title>
      <link>https://arxiv.org/abs/2501.14993</link>
      <description>arXiv:2501.14993v1 Announce Type: new 
Abstract: The proximal algorithm is a powerful tool to minimize nonlinear and nonsmooth functionals in a general metric space. Motivated by the recent progress in studying the training dynamics of the noisy gradient descent algorithm on two-layer neural networks in the mean-field regime, we provide in this paper a simple and self-contained analysis for the convergence of the general-purpose Wasserstein proximal algorithm without assuming geodesic convexity of the objective functional. Under a natural Wasserstein analog of the Euclidean Polyak-{\L}ojasiewicz inequality, we establish that the proximal algorithm achieves an unbiased and linear convergence rate. Our convergence rate improves upon existing rates of the proximal algorithm for solving Wasserstein gradient flows under strong geodesic convexity. We also extend our analysis to the inexact proximal algorithm for geodesically semiconvex objectives. In our numerical experiments, proximal training demonstrates a faster convergence rate than the noisy gradient descent algorithm on mean-field neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14993v1</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shuailong Zhu, Xiaohui Chen</dc:creator>
    </item>
    <item>
      <title>Difference vs. Quotient: A Novel Algorithm for Dominant Eigenvalue Problem</title>
      <link>https://arxiv.org/abs/2501.15131</link>
      <description>arXiv:2501.15131v1 Announce Type: new 
Abstract: The computation of the dominant eigenvector of symmetric positive semidefinite matrices is a cornerstone operation in numerous machine learning applications. Traditional approaches predominantly rely on the constrained Quotient formulation, which underpins most existing methods. However, these methods often suffer from challenges related to computational efficiency and dependence on spectral prior knowledge. This paper introduces a novel perspective by reformulating the eigenvalue problem using an unconstrained Difference formulation. This new approach sheds light on classical methods, revealing that the power method can be interpreted as a specific instance of Difference of Convex Algorithms. Building on this insight, we develop a generalized family of Difference-Type methods, which encompasses the power method as a special case. Within this family, we propose the Split-Merge algorithm, which achieves maximal acceleration without spectral prior knowledge and operates solely through matrix-vector products, making it both efficient and easy to implement. Extensive empirical evaluations on both synthetic and real-world datasets highlight that the Split-Merge algorithm achieves over a $\boldsymbol{10\times}$ speedup compared to the basic power method, offering significant advancements in efficiency and practicality for large-scale machine learning problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15131v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaozhi Liu, Yong Xia</dc:creator>
    </item>
    <item>
      <title>Partial regularity of semiconvex viscosity supersolutions to fully nonlinear elliptic HJB equations and applications to stochastic control</title>
      <link>https://arxiv.org/abs/2501.15285</link>
      <description>arXiv:2501.15285v1 Announce Type: new 
Abstract: In this note, we demonstrate that a locally semiconvex viscosity supersolution to a possibly degenerate fully nonlinear elliptic Hamilton-Jacobi-Bellman (HJB) equation is differentiable along the directions spanned by the range of the coefficient associated with the second-order term. The proof leverages techniques from convex analysis combined with a contradiction argument. This result has significant implications for various stationary stochastic control problems. In the context of drift-control problems, it provides a pathway to construct a candidate optimal feedback control in the classical sense and establish a verification theorem. Furthermore, in optimal stopping and impulse control problems, when the second-order term is nondegenerate, the value function of the problem is shown to be differentiable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15285v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Salvatore Federico, Giorgio Ferrari, Mauro Rosestolato</dc:creator>
    </item>
    <item>
      <title>Modified Dai-Liao Spectral Conjugate Gradient Method with Application to Signal Processing</title>
      <link>https://arxiv.org/abs/2501.15300</link>
      <description>arXiv:2501.15300v1 Announce Type: new 
Abstract: In this article, we present a modified variant of the Dai-Liao spectral conjugate gradient method, developed through an analysis of eigenvalues and inspired by a modified secant condition. We show that the proposed method is globally convergent for general nonlinear functions under standard assumptions. By incorporating the new secant condition and a quasi-Newton direction, we introduce updated spectral parameters. These changes ensure that the resulting search direction satisfies the sufficient descent property without relying on any line search. Numerical experiments show that the proposed algorithm performs better than several existing methods in terms of convergence speed and computational efficiency. Its effectiveness is further demonstrated through an application in signal processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15300v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>D. R. Sahu, Shikher Sharma, Pankaj Gautam</dc:creator>
    </item>
    <item>
      <title>Data-Driven Distributionally Robust Optimization for Long-Term Contract vs. Spot Allocation Decisions: Application to Electricity Markets</title>
      <link>https://arxiv.org/abs/2501.15340</link>
      <description>arXiv:2501.15340v1 Announce Type: new 
Abstract: There are numerous industrial settings in which a decision maker must decide whether to enter into long-term contracts to guarantee price (and hence cash flow) stability or to participate in more volatile spot markets. In this paper, we investigate a data-driven distributionally robust optimization (DRO) approach aimed at balancing this tradeoff. Unlike traditional risk-neutral stochastic optimization models that assume the underlying probability distribution generating the data is known, DRO models assume the distribution belongs to a family of possible distributions, thus providing a degree of immunization against unseen and potential worst-case outcomes. We compare and contrast the performance of a risk-neutral model, conditional value-at-risk formulation, and a Wasserstein distributionally robust model to demonstrate the potential benefits of a DRO approach for an ``elasticity-aware'' price-taking decision maker.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15340v1</guid>
      <category>math.OC</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.compchemeng.2023.108436</arxiv:DOI>
      <arxiv:journal_reference>Computers &amp; Chemical Engineering, 179, p.108436. 2023</arxiv:journal_reference>
      <dc:creator>Dimitri J. Papageorgiou</dc:creator>
    </item>
    <item>
      <title>Pseudo basic steps: Bound improvement guarantees from Lagrangian decomposition in convex disjunctive programming</title>
      <link>https://arxiv.org/abs/2501.15345</link>
      <description>arXiv:2501.15345v1 Announce Type: new 
Abstract: An elementary, but fundamental, operation in disjunctive programming is a basic step, which is the intersection of two disjunctions to form a new disjunction. Basic steps bring a disjunctive set in regular form closer to its disjunctive normal form and, in turn, produce relaxations that are at least as tight. An open question is: What are guaranteed bounds on the improvement from a basic step? In this paper, using properties of a convex disjunctive program's hull reformulation and multipliers from Lagrangian decomposition, we introduce an operation called a pseudo basic step and use it to provide provable bounds on this improvement along with techniques to exploit this information when solving a disjunctive program as a convex MINLP. Numerical examples illustrate the practical benefits of these bounds. In particular, on a set of K-means clustering instances, we make significant bound improvements relative to state-of-the-art commercial mixed-integer programming solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15345v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s13675-017-0088-0</arxiv:DOI>
      <arxiv:journal_reference>EURO Journal on Computational Optimization, 6, pp.55-83. 2018</arxiv:journal_reference>
      <dc:creator>Dimitri J. Papageorgiou, Francisco Trespalacios</dc:creator>
    </item>
    <item>
      <title>A new separable property of the joint numerical range of quadratic functions and its applications to the Smallest Enclosing Ball Problem</title>
      <link>https://arxiv.org/abs/2501.15399</link>
      <description>arXiv:2501.15399v1 Announce Type: new 
Abstract: We explore separable property of the joint numerical range $G(\Bbb R^n)$ of a special class of quadratic functions and apply it to solving the smallest enclosing ball (SEB) problem which asks to find a ball $B(a,r)$ in $\Bbb R^n$ with smallest radius $r$ such that $B(a,r)$ contains the intersection $\cap_{i=1}^mB(a_i,r_i)$ of $m$ given balls $B(a_i,r_i).$ We show that $G(\Bbb R^n)$ is convex if and only if ${\rm rank}\{a_1-a, a_2-a, \ldots, a_m-a\}\le n-1.$ Otherwise, ${\rm rank}\{a_1-a, a_2-a, \ldots, a_m-a\}=n$ and $G(\Bbb R^n)$ is not convex. In this case we propose a new set $G(\Bbb R^n)^\bullet$ which allows to show that if $m=n$ then $G(\Bbb R^n)^\bullet$ is convex even $G(\Bbb R^n)$ is not. Importantly, the separable property of $G(\Bbb R^n)^\bullet$ then implies the separable property for $G(\Bbb R^n).$ As a result, a new progress on solving the SEB problem is obtained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15399v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Van-Bong Nguyen, Huu-Quang Nguyen</dc:creator>
    </item>
    <item>
      <title>A primal-dual interior point trust region method for inequality-constrained optimization problems on Riemannian manifolds</title>
      <link>https://arxiv.org/abs/2501.15419</link>
      <description>arXiv:2501.15419v1 Announce Type: new 
Abstract: We consider Riemannian inequality-constrained optimization problems and propose a Riemannian primal-dual interior point trust region method (RIPTRM) for solving them. We prove its global convergence to an approximate Karush-Kuhn-Tucker point and a second-order stationary point. We also establish the local near-quadratic convergence. To the best of our knowledge, this is the first algorithm that incorporates the trust region strategy and has the second-order convergence property for optimization problems on Riemannian manifolds with nonlinear inequality constraints. It is also the first Riemannian interior point method that possesses both global and local convergence properties. We conduct numerical experiments in which we introduce a truncated conjugate gradient method and an eigenvalue-based subsolver for RIPTRM to approximately and exactly solve the trust region subproblems, respectively. Empirical results show that RIPTRMs find solutions with higher accuracy compared to an existing Riemannian interior point method and other algorithms. Additionally, we observe that RIPTRM with the exact search direction shows significantly promising performance in an instance where the Hessian of the Lagrangian has a large negative eigenvalue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15419v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mitsuaki Obara, Takayuki Okuno, Akiko Takeda</dc:creator>
    </item>
    <item>
      <title>Partial Smoothness, Subdifferentials and Set-valued Operators</title>
      <link>https://arxiv.org/abs/2501.15540</link>
      <description>arXiv:2501.15540v1 Announce Type: new 
Abstract: Over the past decades, the concept "partial smoothness" has been playing as a powerful tool in several fields involving nonsmooth analysis, such as nonsmooth optimization, inverse problems and operation research, etc. The essence of partial smoothness is that it builds an elegant connection between the optimization variable and the objective function value through the subdifferential. Identifiability is the most appealing property of partial smoothness, as locally it allows us to conduct much finer or even sharp analysis, such as linear convergence or sensitivity analysis. However, currently the identifiability relies on non-degeneracy condition and exact dual convergence, which limits the potential application of partial smoothness. In this paper, we provide an alternative characterization of partial smoothness through only subdifferentials. This new perspective enables us to establish stronger identification results, explain identification under degeneracy and non-vanishing error. Moreover, we can generalize this new characterization to set-valued operators, and provide a complement definition of partly smooth operator proposed in [14].</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15540v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziqi Qin, Jingwei Liang</dc:creator>
    </item>
    <item>
      <title>Boundary Stabilization with restricted observability</title>
      <link>https://arxiv.org/abs/2501.15906</link>
      <description>arXiv:2501.15906v1 Announce Type: new 
Abstract: Lyapunov functions are popularly used to investigate the stabilization problem of systems of hyperbolic conservation laws with boundary controls. In real life applications often not every boundary value can be observed. In this work, we show the stabilization under a restricted boundary observability. Thereby, we apply the boundary control directly on the observed (physical) variables. Using well-known stabilization results from the literature, we also discuss examples such as a density flow model or the Saint-Venant equations. This shows that a restricted observation can result in more restrictive control choices or can prevent the system from stabilizing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15906v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mapundi Kondwani Banda, Jan Friedrich, Michael Herty</dc:creator>
    </item>
    <item>
      <title>PyClustrPath: An efficient Python package for generating clustering paths with GPU acceleration</title>
      <link>https://arxiv.org/abs/2501.15964</link>
      <description>arXiv:2501.15964v1 Announce Type: new 
Abstract: Convex clustering is a popular clustering model without requiring the number of clusters as prior knowledge. It can generate a clustering path by continuously solving the model with a sequence of regularization parameter values. This paper introduces {\it PyClustrPath}, a highly efficient Python package for solving the convex clustering model with GPU acceleration. {\it PyClustrPath} implements popular first-order and second-order algorithms with a clean modular design. Such a design makes {\it PyClustrPath} more scalable to incorporate new algorithms for solving the convex clustering model in the future. We extensively test the numerical performance of {\it PyClustrPath} on popular clustering datasets, demonstrating its superior performance compared to the existing solvers for generating the clustering path based on the convex clustering model. The implementation of {\it PyClustrPath} can be found at: https://github.com/D3IntOpt/PyClustrPath.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15964v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hongfei Wu, Yancheng Yuan</dc:creator>
    </item>
    <item>
      <title>Modeling and stability analysis of live systems with time-varying dimension</title>
      <link>https://arxiv.org/abs/2501.15991</link>
      <description>arXiv:2501.15991v1 Announce Type: new 
Abstract: A major limitation of the classical control theory is the assumption that the state space and its dimension do not change with time. This prevents analyzing and even formalizing the stability and control problems for open multi-agent systems whose agents may enter or leave the network, industrial processes where the sensors or actuators may be exchanged frequently, smart grids, etc. In this work, we propose a framework of live systems that covers a rather general class of systems with a time-varying state space. We argue that input-to-state stability is a proper stability notion for this class of systems, and many of the classic tools and results, such as Lyapunov methods and superposition theorems, can be extended to this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15991v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.DS</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrii Mironchenko</dc:creator>
    </item>
    <item>
      <title>Control of a non-stationary dynamic system with estimating a strategy of human resources management by the integral indicators method</title>
      <link>https://arxiv.org/abs/2501.16108</link>
      <description>arXiv:2501.16108v1 Announce Type: new 
Abstract: A general idea research is a lack of articles to estimate system indicator of the effectiveness of the strategy of human resources management (HR) at an economic object (enterprise). We are use the method of integral indicators for a comprehensive assessment of the activities of an economic object (enterprise). The economic object is formalized as a nonstationary dynamic system. The system has a dimension of 1.2 million parameters. The parameters of the object under research (business processes) are compared with staff responsibilities. The sanctions mode is set by blocking staff responsibilities in the interval in each time period t. The integral indicator is calculated according to the standard mode (Strategy 1) of the economic object (enterprise) and without blocking the staff responsibilities. Also, the integral indicator is calculated according to the non-standard operating mode of the enterprise (Strategy 2) with the blocking of staff responsibilities. The difference between the integral indicator of Strategy 2 and Strategy 1 is an estimation of the impact of the imposed sanctions. The resource consumption for the restoration of the normal operation of an economic object after the imposed sanctions is give (61.63 million rubles). Example 2 introduces equipment sanctions from America for a new project. An analysis of the indicators of a new project is carried out with the search and use of analog equipment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16108v1</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/1742-6596/1679/5/052038</arxiv:DOI>
      <arxiv:journal_reference>J. Phys.: Conf. Ser. 1679 052038 (2020)</arxiv:journal_reference>
      <dc:creator>Sergey Masaev, Valentina Vingert, Elena Musiyachenko, Yass Salal</dc:creator>
    </item>
    <item>
      <title>Evaluating the effectiveness, reliability and efficiency of a multi-objective sequential optimization approach for building performance design</title>
      <link>https://arxiv.org/abs/2501.14742</link>
      <description>arXiv:2501.14742v1 Announce Type: cross 
Abstract: The complexity of performance-based building design stems from the evaluation of numerous candidate design options, driven by the plethora of variables, objectives, and constraints inherent in multi-disciplinary projects. This necessitates optimization approaches to support the identification of well performing designs while reducing the computational time of performance evaluation. In response, this paper proposes and evaluates a sequential approach for multi-objective design optimization of building geometry, fabric, HVAC system and controls for building performance. This approach involves sequential optimizations with optimal solutions from previous stages passed to the next. The performance of the sequential approach is benchmarked against a full factorial search, assessing its effectiveness in finding global optima, solution quality, reliability to scale and variations of problem formulations, and computational efficiency compared to the NSGA-II algorithm. 24 configurations of the sequential approach are tested on a multi-scale case study, simulating 874 to 4,147,200 design options for an office building, aiming to minimize energy demand while maintaining thermal comfort. A two-stage sequential process-(building geometry + fabric) and (HVAC system + controls) identified the same Pareto-optimal solutions as the full factorial search across all four scales and variations of problem formulations, demonstrating 100% effectiveness and reliability. This approach required 100,700 function evaluations, representing a 91.2% reduction in computational effort compared to the full factorial search. In contrast, NSGA-II achieved only 73.5% of the global optima with the same number of function evaluations. This research indicates that a sequential optimization approach is a highly efficient and robust alternative to the standard NSGA-II algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14742v1</guid>
      <category>cs.NE</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riccardo Talami, Jonathan Wright, Bianca Howard</dc:creator>
    </item>
    <item>
      <title>LPBSA: Enhancing Optimization Efficiency through Learner Performance-based Behavior and Simulated Annealing</title>
      <link>https://arxiv.org/abs/2501.14759</link>
      <description>arXiv:2501.14759v1 Announce Type: cross 
Abstract: This study introduces the LPBSA, an advanced optimization algorithm that combines Learner Performance-based Behavior (LPB) and Simulated Annealing (SA) in a hybrid approach. Emphasizing metaheuristics, the LPBSA addresses and mitigates the challenges associated with traditional LPB methodologies, enhancing convergence, robustness, and adaptability in solving complex optimization problems. Through extensive evaluations using benchmark test functions, the LPBSA demonstrates superior performance compared to LPB and competes favorably with established algorithms such as PSO, FDO, LEO, and GA. Real-world applications underscore the algorithm's promise, with LPBSA outperforming the LEO algorithm in two tested scenarios. Based on the study results many test function results such as TF5 by recording (4.76762333) and some other test functions provided in the result section prove that LPBSA outperforms popular algorithms. This research highlights the efficacy of a hybrid approach in the ongoing evolution of optimization algorithms, showcasing the LPBSA's capacity to navigate diverse optimization landscapes and contribute significantly to addressing intricate optimization challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14759v1</guid>
      <category>cs.NE</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dana R. Hamad, A. R. Tarik</dc:creator>
    </item>
    <item>
      <title>A unified approach for domination and packing problems in graphs</title>
      <link>https://arxiv.org/abs/2501.14789</link>
      <description>arXiv:2501.14789v1 Announce Type: cross 
Abstract: In this paper, we introduce new concepts of domination and packing functions in graphs, which generalize, respectively, the labelled dominating and packing functions defined by Lee and Chang in 2008, and Hinrichsen et al. in 2019. These generalized functions offer a unified and simpler framework for addressing many of the variations of domination and packing concepts in graphs explored in the literature. Interestingly, their associated optimization problems turn out to be equivalent, providing insight to explain the observed coincidences in computational complexity results for graph classes where both problems, the domination one and its corresponding packing variation, have been analyzed. This equivalence also allows us to solve some computational complexity open questions, for some graph classes.
  Furthermore, we prove that the generalized problems remain solvable in polynomial time for graphs with bounded clique-width and strongly chordal graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14789v1</guid>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>E. Hinrichsen, G. Nasini, N. Vansteenkiste</dc:creator>
    </item>
    <item>
      <title>A cluster mean approach for topology optimization of natural frequencies and bandgaps with simple/multiple eigenfrequencies</title>
      <link>https://arxiv.org/abs/2501.14910</link>
      <description>arXiv:2501.14910v1 Announce Type: cross 
Abstract: This study presents a novel approach utilizing cluster means to address the non-differentiability issue arising from multiple eigenvalues in eigenfrequency and bandgap optimization. By constructing symmetric functions of repeated eigenvalues -- including cluster mean, p-norm and KS functions -- the study confirms their differentiability when all repeated eigenvalues are included, i.e., clusters are complete. Numerical sensitivity analyses indicate that, under some symmetry conditions, multiple eigenvalues may also be differentiable w.r.t the symmetric design variables. Notably, regardless of enforced symmetry, the cluster mean approach guarantees differentiability of multiple eigenvalues, offering a reliable solution strategy in eigenfrequency topology optimization. Optimization schemes are proposed to maximize eigenfrequencies and bandgaps by integrating cluster means with the bound formulations. The efficacy of the proposed method is demonstrated through numerical examples on 2D and 3D solids and plate structures. All optimization results demonstrate smooth convergence under simple/multiple eigenvalues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14910v1</guid>
      <category>cs.CE</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiyao Sun, Kapil Khandelwal</dc:creator>
    </item>
    <item>
      <title>In-Context Operator Learning for Linear Propagator Models</title>
      <link>https://arxiv.org/abs/2501.15106</link>
      <description>arXiv:2501.15106v1 Announce Type: cross 
Abstract: We study operator learning in the context of linear propagator models for optimal order execution problems with transient price impact \`a la Bouchaud et al. (2004) and Gatheral (2010). Transient price impact persists and decays over time according to some propagator kernel. Specifically, we propose to use In-Context Operator Networks (ICON), a novel transformer-based neural network architecture introduced by Yang et al. (2023), which facilitates data-driven learning of operators by merging offline pre-training with an online few-shot prompting inference. First, we train ICON to learn the operator from various propagator models that maps the trading rate to the induced transient price impact. The inference step is then based on in-context prediction, where ICON is presented only with a few examples. We illustrate that ICON is capable of accurately inferring the underlying price impact model from the data prompts, even with propagator kernels not seen in the training data. In a second step, we employ the pre-trained ICON model provided with context as a surrogate operator in solving an optimal order execution problem via a neural network control policy, and demonstrate that the exact optimal execution strategies from Abi Jaber and Neuman (2022) for the models generating the context are correctly retrieved. Our introduced methodology is very general, offering a new approach to solving optimal stochastic control problems with unknown state dynamics, inferred data-efficiently from a limited number of examples by leveraging the few-shot and transfer learning capabilities of transformer networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15106v1</guid>
      <category>q-fin.TR</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>q-fin.CP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tingwei Meng, Moritz Vo{\ss}, Nils Detering, Giulio Farolfi, Stanley Osher, Georg Menz</dc:creator>
    </item>
    <item>
      <title>On Spectral Approach to the Synthesis of Shaping Filters</title>
      <link>https://arxiv.org/abs/2501.15174</link>
      <description>arXiv:2501.15174v1 Announce Type: cross 
Abstract: This paper describes various approaches to modeling a random process with a given rational power spectral density. The main attention is paid to the spectral form of mathematical description, which allows one to obtain a relation for the shaping filter using a transfer function without any additional calculations. The paper provides all necessary relations for the implementation of the shaping filter based on the spectral form of mathematical description.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15174v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantin A. Rybakov</dc:creator>
    </item>
    <item>
      <title>Scalable Decentralized Learning with Teleportation</title>
      <link>https://arxiv.org/abs/2501.15259</link>
      <description>arXiv:2501.15259v1 Announce Type: cross 
Abstract: Decentralized SGD can run with low communication costs, but its sparse communication characteristics deteriorate the convergence rate, especially when the number of nodes is large. In decentralized learning settings, communication is assumed to occur on only a given topology, while in many practical cases, the topology merely represents a preferred communication pattern, and connecting to arbitrary nodes is still possible. Previous studies have tried to alleviate the convergence rate degradation in these cases by designing topologies with large spectral gaps. However, the degradation is still significant when the number of nodes is substantial. In this work, we propose TELEPORTATION. TELEPORTATION activates only a subset of nodes, and the active nodes fetch the parameters from previous active nodes. Then, the active nodes update their parameters by SGD and perform gossip averaging on a relatively small topology comprising only the active nodes. We show that by activating only a proper number of nodes, TELEPORTATION can completely alleviate the convergence rate degradation. Furthermore, we propose an efficient hyperparameter-tuning method to search for the appropriate number of nodes to be activated. Experimentally, we showed that TELEPORTATION can train neural networks more stably and achieve higher accuracy than Decentralized SGD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15259v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuki Takezawa, Sebastian U. Stich</dc:creator>
    </item>
    <item>
      <title>Turing in the shadows of Nobel and Abel: an algorithmic story behind two recent prizes</title>
      <link>https://arxiv.org/abs/2501.15312</link>
      <description>arXiv:2501.15312v1 Announce Type: cross 
Abstract: The 2021 Nobel Prize in physics was awarded to Giorgio Parisi ``for the discovery of the interplay of disorder and fluctuations in physical systems from atomic to planetary scales,'' and the 2024 Abel Prize in mathematics was awarded to Michel Talagrand ``for his groundbreaking contributions to probability theory and functional analysis, with outstanding applications in mathematical physics and statistics.'' What remains largely absent in the popular descriptions of these prizes, however, is the profound contributions the works of both individuals have had to the field of \emph{algorithms and computation}. The ideas first developed by Parisi and his collaborators relying on remarkably precise physics intuition, and later confirmed by Talagrand and others by no less remarkable mathematical techniques, have revolutionized the way we think algorithmically about optimization problems involving randomness. This is true both in terms of the existence of fast algorithms for some optimization problems, but also in terms of our persistent failures of finding such algorithms for some other optimization problems.
  The goal of this article is to highlight these developments and explain how the ideas pioneered by Parisi and Talagrand have led to a remarkably precise characterization of which optimization problems admit fast algorithms, versus those which do not, and furthermore to explain why this characterization holds true.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15312v1</guid>
      <category>math.PR</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.CC</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Notices of AMS, May 2025</arxiv:journal_reference>
      <dc:creator>David Gamarnik</dc:creator>
    </item>
    <item>
      <title>PSO and the Traveling Salesman Problem: An Intelligent Optimization Approach</title>
      <link>https://arxiv.org/abs/2501.15319</link>
      <description>arXiv:2501.15319v1 Announce Type: cross 
Abstract: The Traveling Salesman Problem (TSP) is a well-known combinatorial optimization problem that aims to find the shortest possible route that visits each city exactly once and returns to the starting point. This paper explores the application of Particle Swarm Optimization (PSO), a population-based optimization algorithm, to solve TSP. Although PSO was originally designed for continuous optimization problems, this work adapts PSO for the discrete nature of TSP by treating the order of cities as a permutation. A local search strategy, including 2-opt and 3-opt techniques, is applied to improve the solution after updating the particle positions. The performance of the proposed PSO algorithm is evaluated using benchmark TSP instances and compared to other popular optimization algorithms, such as Genetic Algorithms (GA) and Simulated Annealing (SA). Results show that PSO performs well for small to medium-sized problems, though its performance diminishes for larger instances due to difficulties in escaping local optima. This paper concludes that PSO is a promising approach for solving TSP, with potential for further improvement through hybridization with other optimization techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15319v1</guid>
      <category>cs.NE</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kael Silva Ara\'ujo, Francisco M\'arcio Barboza</dc:creator>
    </item>
    <item>
      <title>Finite Strain Robust Topology Optimization Considering Multiple Uncertainties</title>
      <link>https://arxiv.org/abs/2501.15337</link>
      <description>arXiv:2501.15337v1 Announce Type: cross 
Abstract: This paper presents a computational framework for the robust stiffness design of hyperelastic structures at finite deformations subject to various uncertain sources. In particular, the loading, material properties, and geometry uncertainties are incorporated within the topology optimization framework and are modeled by random vectors or random fields. A stochastic perturbation method is adopted to quantify uncertainties, and analytical adjoint sensitivities are derived for efficient gradient-based optimization. Moreover, the mesh distortion of low-density elements under finite deformations is handled by an adaptive linear energy interpolation scheme. The proposed robust topology optimization framework is applied to several examples, and the effects of different uncertain sources on the optimized topologies are systematically investigated. As demonstrated, robust designs are less sensitive to the variation of target uncertain sources than deterministic designs. Finally, it is shown that incorporating symmetry-breaking uncertainties in the topology optimization framework promotes stable designs compared to the deterministic counterpart, where -- when no stability constraint is included -- can lead to unstable designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15337v1</guid>
      <category>cs.CE</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nan Feng, Guodong Zhang, Kapil Khandelwal</dc:creator>
    </item>
    <item>
      <title>Structural Symmetry, Multiplicity, and Differentiability of Eigenfrequencies</title>
      <link>https://arxiv.org/abs/2501.15357</link>
      <description>arXiv:2501.15357v1 Announce Type: cross 
Abstract: This work investigates the multiplicity and differentiability of eigenfrequencies in structures with various symmetries. In particular, the study explores how the geometric and design variable symmetries affect the distribution of eigenvalues, distinguishing between simple and multiple eigenvalues in 3-D trusses. Moreover, this article also examines the differentiability of multiple eigenvalues under various symmetry conditions, which is crucial for gradient-based optimization. The results presented in this study show that while full symmetry ensures the differentiability of all eigenvalues, increased symmetry in optimized design, such as accidental symmetry, may lead to non-differentiable eigenvalues. Additionally, the study presents solutions using symmetric functions, demonstrating their effectiveness in ensuring differentiability in scenarios where multiple eigenvalues are non-differentiable. The study also highlights a critical insight into the differentiability criterion of symmetric functions, i.e., the completeness of eigen-clusters, which is necessary to ensure the differentiability of such functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15357v1</guid>
      <category>cs.CE</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiyao Sun, Kapil Khandelwal</dc:creator>
    </item>
    <item>
      <title>Learning-Enhanced Safeguard Control for High-Relative-Degree Systems: Robust Optimization under Disturbances and Faults</title>
      <link>https://arxiv.org/abs/2501.15373</link>
      <description>arXiv:2501.15373v1 Announce Type: cross 
Abstract: Merely pursuing performance may adversely affect the safety, while a conservative policy for safe exploration will degrade the performance. How to balance the safety and performance in learning-based control problems is an interesting yet challenging issue. This paper aims to enhance system performance with safety guarantee in solving the reinforcement learning (RL)-based optimal control problems of nonlinear systems subject to high-relative-degree state constraints and unknown time-varying disturbance/actuator faults. First, to combine control barrier functions (CBFs) with RL, a new type of CBFs, termed high-order reciprocal control barrier function (HO-RCBF) is proposed to deal with high-relative-degree constraints during the learning process. Then, the concept of gradient similarity is proposed to quantify the relationship between the gradient of safety and the gradient of performance. Finally, gradient manipulation and adaptive mechanisms are introduced in the safe RL framework to enhance the performance with a safety guarantee. Two simulation examples illustrate that the proposed safe RL framework can address high-relative-degree constraint, enhance safety robustness and improve system performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15373v1</guid>
      <category>eess.SY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <category>nlin.AO</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyang Wang, Hongwei Zhang, Shimin Wang, Wei Xiao, Martin Guay</dc:creator>
    </item>
    <item>
      <title>BoTier: Multi-Objective Bayesian Optimization with Tiered Composite Objectives</title>
      <link>https://arxiv.org/abs/2501.15554</link>
      <description>arXiv:2501.15554v1 Announce Type: cross 
Abstract: Scientific optimization problems are usually concerned with balancing multiple competing objectives, which come as preferences over both the outcomes of an experiment (e.g. maximize the reaction yield) and the corresponding input parameters (e.g. minimize the use of an expensive reagent). Typically, practical and economic considerations define a hierarchy over these objectives, which must be reflected in algorithms for sample-efficient experiment planning. Herein, we introduce BoTier, a composite objective that can flexibly represent a hierarchy of preferences over both experiment outcomes and input parameters. We provide systematic benchmarks on synthetic and real-life surfaces, demonstrating the robust applicability of BoTier across a number of use cases. Importantly, BoTier is implemented in an auto-differentiable fashion, enabling seamless integration with the BoTorch library, thereby facilitating adoption by the scientific community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15554v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Haddadnia, Leonie Grashoff, Felix Strieth-Kalthoff</dc:creator>
    </item>
    <item>
      <title>Risk-Aware Distributional Intervention Policies for Language Models</title>
      <link>https://arxiv.org/abs/2501.15758</link>
      <description>arXiv:2501.15758v1 Announce Type: cross 
Abstract: Language models are prone to occasionally undesirable generations, such as harmful or toxic content, despite their impressive capability to produce texts that appear accurate and coherent. This paper presents a new two-stage approach to detect and mitigate undesirable content generations by rectifying activations. First, we train an ensemble of layerwise classifiers to detect undesirable content using activations by minimizing a smooth surrogate of the risk-aware score. Then, for contents that are detected as undesirable, we propose layerwise distributional intervention policies that perturb the attention heads minimally while guaranteeing probabilistically the effectiveness of the intervention. Benchmarks on several language models and datasets show that our method outperforms baselines in reducing the generation of undesirable output.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15758v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bao Nguyen, Binh Nguyen, Duy Nguyen, Viet Anh Nguyen</dc:creator>
    </item>
    <item>
      <title>Formal Verification of Markov Processes with Learned Parameters</title>
      <link>https://arxiv.org/abs/2501.15767</link>
      <description>arXiv:2501.15767v1 Announce Type: cross 
Abstract: We introduce the problem of formally verifying properties of Markov processes where the parameters are the output of machine learning models. Our formulation is general and solves a wide range of problems, including verifying properties of probabilistic programs that use machine learning, and subgroup analysis in healthcare modeling. We show that for a broad class of machine learning models, including linear models, tree-based models, and neural networks, verifying properties of Markov chains like reachability, hitting time, and total reward can be formulated as a bilinear program. We develop a decomposition and bound propagation scheme for solving the bilinear program and show through computational experiments that our method solves the problem to global optimality up to 100x faster than state-of-the-art solvers. We also release $\texttt{markovml}$, an open-source tool for building Markov processes, integrating pretrained machine learning models, and verifying their properties, available at https://github.com/mmaaz-git/markovml.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15767v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Maaz, Timothy C. Y. Chan</dc:creator>
    </item>
    <item>
      <title>Memorization and Regularization in Generative Diffusion Models</title>
      <link>https://arxiv.org/abs/2501.15785</link>
      <description>arXiv:2501.15785v1 Announce Type: cross 
Abstract: Diffusion models have emerged as a powerful framework for generative modeling. At the heart of the methodology is score matching: learning gradients of families of log-densities for noisy versions of the data distribution at different scales. When the loss function adopted in score matching is evaluated using empirical data, rather than the population loss, the minimizer corresponds to the score of a time-dependent Gaussian mixture. However, use of this analytically tractable minimizer leads to data memorization: in both unconditioned and conditioned settings, the generative model returns the training samples. This paper contains an analysis of the dynamical mechanism underlying memorization. The analysis highlights the need for regularization to avoid reproducing the analytically tractable minimizer; and, in so doing, lays the foundations for a principled understanding of how to regularize. Numerical experiments investigate the properties of: (i) Tikhonov regularization; (ii) regularization designed to promote asymptotic consistency; and (iii) regularizations induced by under-parameterization of a neural network or by early stopping when training a neural network. These experiments are evaluated in the context of memorization, and directions for future development of regularization are highlighted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15785v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ricardo Baptista, Agnimitra Dasgupta, Nikola B. Kovachki, Assad Oberai, Andrew M. Stuart</dc:creator>
    </item>
    <item>
      <title>Autonomous Horizon-based Asteroid Navigation With Observability-constrained Maneuvers</title>
      <link>https://arxiv.org/abs/2501.15806</link>
      <description>arXiv:2501.15806v1 Announce Type: cross 
Abstract: Asteroid exploration is a pertinent challenge due to the varying complexity of their dynamical environments, shape and communication delays due to distance. Thus, autonomous navigation methods are continually being developed and improved in current research to enable their safe exploration. These methods often involve using horizon-based Optical Navigation (OpNav) to determine the spacecraft's location, which is reliant on the visibility of the horizon. It is critical to ensure the reliability of this measurement such that the spacecraft may maintain an accurate state estimate throughout its mission. This paper presents an algorithm that generates control maneuvers for spacecraft to follow trajectories that allow continuously usable optical measurements to maintain system observability for safe navigation. This algorithm improves upon existing asteroid navigation capabilities by allowing the safe and robust autonomous targeting of various trajectories and orbits at a wide range of distances within optical measurement range. It is adaptable to different asteroid scenarios. Overall, the approach develops an all-encompassing system that simulates the asteroid dynamics, synthetic image generation, edge detection, horizon-based OpNav, filtering and observability-enhancing control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15806v1</guid>
      <category>cs.RO</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aditya Arjun Anibha, Kenshiro Oguri</dc:creator>
    </item>
    <item>
      <title>The Sample Complexity of Online Reinforcement Learning: A Multi-model Perspective</title>
      <link>https://arxiv.org/abs/2501.15910</link>
      <description>arXiv:2501.15910v1 Announce Type: cross 
Abstract: We study the sample complexity of online reinforcement learning for nonlinear dynamical systems with continuous state and action spaces. Our analysis accommodates a large class of dynamical systems ranging from a finite set of nonlinear candidate models to models with bounded and Lipschitz continuous dynamics, to systems that are parametrized by a compact and real-valued set of parameters. In the most general setting, our algorithm achieves a policy regret of $\mathcal{O}(N \epsilon^2 + \mathrm{ln}(m(\epsilon))/\epsilon^2)$, where $N$ is the time horizon, $\epsilon$ is a user-specified discretization width, and $m(\epsilon)$ measures the complexity of the function class under consideration via its packing number. In the special case where the dynamics are parametrized by a compact and real-valued set of parameters (such as neural networks, transformers, etc.), we prove a policy regret of $\mathcal{O}(\sqrt{N p})$, where $p$ denotes the number of parameters, recovering earlier sample-complexity results that were derived for linear time-invariant dynamical systems. While this article focuses on characterizing sample complexity, the proposed algorithms are likely to be useful in practice, due to their simplicity, the ability to incorporate prior knowledge, and their benign transient behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15910v1</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Muehlebach, Zhiyu He, Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>Inverse Reinforcement Learning via Convex Optimization</title>
      <link>https://arxiv.org/abs/2501.15957</link>
      <description>arXiv:2501.15957v1 Announce Type: cross 
Abstract: We consider the inverse reinforcement learning (IRL) problem, where an unknown reward function of some Markov decision process is estimated based on observed expert demonstrations. In most existing approaches, IRL is formulated and solved as a nonconvex optimization problem, posing challenges in scenarios where robustness and reproducibility are critical. We discuss a convex formulation of the IRL problem (CIRL) initially proposed by Ng and Russel, and reformulate the problem such that the domain-specific language CVXPY can be applied directly to specify and solve the convex problem. We also extend the CIRL problem to scenarios where the expert policy is not given analytically but by trajectory as state-action pairs, which can be strongly inconsistent with optimality, by augmenting some of the constraints. Theoretical analysis and practical implementation for hyperparameter auto-selection are introduced. This note helps the users to easily apply CIRL for their problems, without background knowledge on convex optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15957v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>math.OC</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Zhu, Yuan Zhang, Joschka Boedecker</dc:creator>
    </item>
    <item>
      <title>Ringmaster ASGD: The First Asynchronous SGD with Optimal Time Complexity</title>
      <link>https://arxiv.org/abs/2501.16168</link>
      <description>arXiv:2501.16168v1 Announce Type: cross 
Abstract: Asynchronous Stochastic Gradient Descent (Asynchronous SGD) is a cornerstone method for parallelizing learning in distributed machine learning. However, its performance suffers under arbitrarily heterogeneous computation times across workers, leading to suboptimal time complexity and inefficiency as the number of workers scales. While several Asynchronous SGD variants have been proposed, recent findings by Tyurin &amp; Richt\'arik (NeurIPS 2023) reveal that none achieve optimal time complexity, leaving a significant gap in the literature. In this paper, we propose Ringmaster ASGD, a novel Asynchronous SGD method designed to address these limitations and tame the inherent challenges of Asynchronous SGD. We establish, through rigorous theoretical analysis, that Ringmaster ASGD achieves optimal time complexity under arbitrarily heterogeneous and dynamically fluctuating worker computation times. This makes it the first Asynchronous SGD method to meet the theoretical lower bounds for time complexity in such scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16168v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Artavazd Maranjyan, Alexander Tyurin, Peter Richt\'arik</dc:creator>
    </item>
    <item>
      <title>The Fundamental Theorem of Weak Optimal Transport</title>
      <link>https://arxiv.org/abs/2501.16316</link>
      <description>arXiv:2501.16316v1 Announce Type: cross 
Abstract: The fundamental theorem of classical optimal transport establishes strong duality and characterizes optimizers through a complementary slackness condition. Milestones such as Brenier's theorem and the Kantorovich-Rubinstein formula are direct consequences.
  In this paper, we generalize this result to non-linear cost functions, thereby establishing a fundamental theorem for the weak optimal transport problem introduced by Gozlan, Roberto, Samson, and Tetali. As applications we provide concise derivations of the Brenier--Strassen theorem, the convex Kantorovich--Rubinstein formula and the structure theorem of entropic optimal transport. We also extend Strassen's theorem in the direction of Gangbo--McCann's transport problem for convex costs. Moreover, we determine the optimizers for a new family of transport problems which contains the Brenier--Strassen, the martingale Benamou--Brenier and the entropic martingale transport problem as extreme cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16316v1</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathias Beiglb\"ock, Gudmund Pammer, Lorenz Riess, Stefan Schrott</dc:creator>
    </item>
    <item>
      <title>Implicit Bias in Matrix Factorization and its Explicit Realization in a New Architecture</title>
      <link>https://arxiv.org/abs/2501.16322</link>
      <description>arXiv:2501.16322v1 Announce Type: cross 
Abstract: Gradient descent for matrix factorization is known to exhibit an implicit bias toward approximately low-rank solutions. While existing theories often assume the boundedness of iterates, empirically the bias persists even with unbounded sequences. We thus hypothesize that implicit bias is driven by divergent dynamics markedly different from the convergent dynamics for data fitting. Using this perspective, we introduce a new factorization model: $X\approx UDV^\top$, where $U$ and $V$ are constrained within norm balls, while $D$ is a diagonal factor allowing the model to span the entire search space. Our experiments reveal that this model exhibits a strong implicit bias regardless of initialization and step size, yielding truly (rather than approximately) low-rank solutions. Furthermore, drawing parallels between matrix factorization and neural networks, we propose a novel neural network model featuring constrained layers and diagonal components. This model achieves strong performance across various regression and classification tasks while finding low-rank solutions, resulting in efficient and lightweight networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16322v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yikun Hou, Suvrit Sra, Alp Yurtsever</dc:creator>
    </item>
    <item>
      <title>Tube-based Robust Model Predictive Control for a Distributed Parameter System Modeled as a Polytopic LPV (extended version)</title>
      <link>https://arxiv.org/abs/2003.05962</link>
      <description>arXiv:2003.05962v4 Announce Type: replace 
Abstract: Distributed parameter systems (DPS) are formulated as partial differential equations (PDE). Especially, under time-varying boundary conditions, PDE introduce force coupling. In the case of the flexible stacker crane (STC), nonlinear coupling is introduced. Accordingly, online trajectory planning and tracking can be addressed using a nonlinear model predictive control (NMPC). However, due to the high computational demands of a NMPC, this paper discusses a possibility of embedding nonlinearities inside a linear parameter varying (LPV) system and thus make a use of a numerically low-demanding linear MPC. The resulting mismatches are treated as parametric and additive uncertainties in the context of robust tube-based MPC (TMPC). For the proposed approach, most of the computations are carried out offline. Only a simple convex quadratic program (QP) is conducted online. Additionally a soft-constrained extension was briefly proposed. Simulation results are used to illustrate the good performance, closed-loop stability and recursive feasibility of the proposed approach despite uncertainties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2003.05962v4</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>American Control Conference, 2020</arxiv:journal_reference>
      <dc:creator>Joe Ismail, Steven Liu</dc:creator>
    </item>
    <item>
      <title>Robust Conic Satisficing</title>
      <link>https://arxiv.org/abs/2107.06714</link>
      <description>arXiv:2107.06714v4 Announce Type: replace 
Abstract: In practical optimization problems, we typically model uncertainty as a random variable though its true probability distribution is unobservable to the decision maker. Historical data provides some information of this distribution that we can use to approximately quantify the risk that depends on both the decision and the uncertainty. This empirical optimization approach is vulnerable to the issues of overfitting, which could be overcome by several data-driven robust optimization techniques. To tackle overfitting, Long et.al.(2022) propose a robust satisficing model, which is specified by a performance target and a penalty function that measures the deviation of the uncertainty from its nominal value, and yields solutions with superior out-of-sample performance. We generalize the robust satisficing framework to conic optimization problems with recourse, which has broad applications in predictive and prescriptive analytics. We derive an exact semidefinite optimization formulation for a biconvex quadratic evaluation function, with quadratic penalty and ellipsoidal support set. More importantly, under complete and bounded recourse, and a reasonably chosen polyhedral support set and penalty function, we propose safe approximations that are feasible for any reasonably chosen target. We then demonstrate that the assumption of complete and bounded recourse is not unimpeachable, and then introduce a novel perspective casting technique to derive an equivalent conic optimization problem satisfying the stated assumptions. Computationally, we showcase a study on data-driven portfolio optimization and demonstrate that the robust satisficing solutions can provide significant improvements over the solutions obtained by stochastic optimization models, including the celebrated Markowitz model, which is prone to overfitting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.06714v4</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arjun Ramachandra, Napat Rujeerapaiboon, Melvyn Sim</dc:creator>
    </item>
    <item>
      <title>Solving the Stock Option Forecast problem by a numerical method for the Black-Scholes Equation with Machine Learning Classification Model</title>
      <link>https://arxiv.org/abs/2209.03512</link>
      <description>arXiv:2209.03512v3 Announce Type: replace 
Abstract: We proposed classification models that utilize the result from the Quasi-Reversibility Method, which solves the Black-Scholes equation to forecast the option prices one day in advance. Combining the minimizer from QRM with our machine learning classifications, we can classify the option as an increase or decrease in value. Based on the different classifications of the options, we can apply various trading strategies which we aim to figure out ways to improve the results from QRM's extrapolations. To further test the viability of our model, we collected 23548 options data from the real-world market for our model, and we will then feed in the data along with the minimizer from QRM to form decision trees and random forests, which we will later test for accuracy, precision, and recall.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.03512v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Benjamin Jiang, Matthieu Durieux, Kirill V. Golubnichiy</dc:creator>
    </item>
    <item>
      <title>Fast convex optimization via closed-loop time scaling of gradient dynamics</title>
      <link>https://arxiv.org/abs/2301.00701</link>
      <description>arXiv:2301.00701v2 Announce Type: replace 
Abstract: In a Hilbert setting, for convex differentiable optimization, we develop a general framework for adaptive accelerated gradient methods. They are based on damped inertial dynamics where the coefficients are designed in a closed-loop way.
  Specifically, the damping is a feedback control of the velocity, or of the gradient of the objective function. For this, we develop a closed-loop version of the time scaling and averaging technique introduced by the authors. We thus obtain autonomous inertial dynamics which involve vanishing viscous damping and implicit Hessian driven damping. By simply using the convergence rates for the continuous steepest descent and Jensen's inequality, without the need for further Lyapunov analysis, we show that the trajectories have several remarkable properties at once: they ensure fast convergence of values, fast convergence of the gradients towards zero, and they converge to optimal solutions. Our approach leads to parallel algorithmic results, that we study in the case of proximal algorithms. These are among the very first general results of this type obtained using autonomous dynamics. Since the proposed numerical methods are based on proximal techniques, the results can be extended to a broader class, specifically to the problem of minimizing a proper, lower semicontinuous, and convex function. Numerical experiments are conducted to demonstrate the efficiency of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.00701v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hedy Attouch, Radu Ioan Bot, Dang-Khoa Nguyen</dc:creator>
    </item>
    <item>
      <title>Randomized Block-Coordinate Optimistic Gradient Algorithms for Root-Finding Problems</title>
      <link>https://arxiv.org/abs/2301.03113</link>
      <description>arXiv:2301.03113v4 Announce Type: replace 
Abstract: In this paper, we develop two new randomized block-coordinate optimistic gradient algorithms to approximate a solution of nonlinear equations in large-scale settings, which are called root-finding problems. Our first algorithm is non-accelerated with constant stepsizes, and achieves $\mathcal{O}(1/k)$ best-iterate convergence rate on $\mathbb{E}[ \Vert Gx^k\Vert^2]$ when the underlying operator $G$ is Lipschitz continuous and satisfies a weak Minty solution condition, where $\mathbb{E}[\cdot]$ is the expectation and $k$ is the iteration counter. Our second method is a new accelerated randomized block-coordinate optimistic gradient algorithm. We establish both $\mathcal{O}(1/k^2)$ and $o(1/k^2)$ last-iterate convergence rates on both $\mathbb{E}[ \Vert Gx^k\Vert^2]$ and $\mathbb{E}[ \Vert x^{k+1} - x^{k}\Vert^2]$ for this algorithm under the co-coerciveness of $G$. In addition, we prove that the iterate sequence $\{x^k\}$ converges to a solution almost surely, and $k\Vert Gx^k\Vert$ attains a $o(1/k)$ almost sure convergence rate. Then, we apply our methods to a class of large-scale finite-sum inclusions, which covers prominent applications in machine learning, statistical learning, and network optimization, especially in federated learning. We obtain two new federated learning-type algorithms and their convergence rate guarantees for solving this problem class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.03113v4</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quoc Tran-Dinh, Yang Luo</dc:creator>
    </item>
    <item>
      <title>Almost Surely $\sqrt{T}$ Regret for Adaptive LQR</title>
      <link>https://arxiv.org/abs/2301.05537</link>
      <description>arXiv:2301.05537v4 Announce Type: replace 
Abstract: The Linear-Quadratic Regulation (LQR) problem with unknown system parameters has been widely studied, but it has remained unclear whether $\tilde{ \mathcal{O}}(\sqrt{T})$ regret, which is the best known dependence on time, can be achieved almost surely. In this paper, we propose an adaptive LQR controller with almost surely $\tilde{ \mathcal{O}}(\sqrt{T})$ regret upper bound. The controller features a circuit-breaking mechanism, which circumvents potential safety breach and guarantees the convergence of the system parameter estimate, but is shown to be triggered only finitely often and hence has negligible effect on the asymptotic performance of the controller. The proposed controller is also validated via simulation on Tennessee Eastman Process~(TEP), a commonly used industrial process example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.05537v4</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TAC.2025.3535997</arxiv:DOI>
      <dc:creator>Yiwen Lu, Yilin Mo</dc:creator>
    </item>
    <item>
      <title>A successive convexification approach for robust receding horizon control</title>
      <link>https://arxiv.org/abs/2302.07744</link>
      <description>arXiv:2302.07744v3 Announce Type: replace 
Abstract: A novel robust nonlinear model predictive control strategy is proposed for systems with nonlinear dynamics and convex state and control constraints. Using a sequential convex approximation approach and a difference of convex functions representation, the scheme constructs tubes that contain predicted model trajectories, accounting for approximation errors and disturbances, and guaranteeing constraint satisfaction. An optimal control problem is solved as a sequence of convex programs. We develop the scheme initially in the absence of external disturbances and show that the proposed nominal approach is non-conservative, with the solutions of successive convex programs converging to a locally optimal solution for the original optimal control problem. We extend the approach to the case of additive disturbances using a novel strategy for selecting linearization points. As a result we formulate a robust receding horizon strategy with guarantees of recursive feasibility closed-loop system stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.07744v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yana Lishkova, Mark Cannon</dc:creator>
    </item>
    <item>
      <title>Continuous-Time Zeroth-Order Dynamics with Projection Maps: Model-Free Feedback Optimization with Safety Guarantees</title>
      <link>https://arxiv.org/abs/2303.06858</link>
      <description>arXiv:2303.06858v2 Announce Type: replace 
Abstract: This paper introduces a class of model-free feedback methods for solving generic constrained optimization problems where the specific mathematical forms of the objective and constraint functions are not available. The proposed methods, termed Projected Zeroth-Order (P-ZO) dynamics, incorporate projection maps into a class of continuous-time model-free dynamics that make use of periodic dithering for the purpose of gradient learning. In particular, the proposed P-ZO algorithms can be interpreted as new extremum-seeking algorithms that autonomously drive an unknown system toward a neighborhood of the set of solutions of an optimization problem using only output feedback, while systematically guaranteeing that the input trajectories remain in a feasible set for all times. In this way, the P-ZO algorithms can properly handle hard and asymptotical constraints in model-free optimization problems without using penalty terms or barrier functions. Moreover, the proposed dynamics have suitable robustness properties with respect to small bounded additive disturbances on the states and dynamics, a property that is fundamental for practical real-world implementations. Additional tracking results for time-varying and switching cost functions are also derived under stronger convexity and smoothness assumptions and using tools from hybrid dynamical systems. Numerical examples are presented throughout the paper to illustrate the above results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.06858v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xin Chen, Jorge I. Poveda, Na Li</dc:creator>
    </item>
    <item>
      <title>Model-Predictive Control with NUP Priors</title>
      <link>https://arxiv.org/abs/2303.15806</link>
      <description>arXiv:2303.15806v2 Announce Type: replace 
Abstract: Normals with unknown variance (NUV) and, more generally, normals with unknown parameters (NUP) can represent many useful priors including L_p norms and other sparsifying priors, and they blend well with linear-Gaussian models and Gaussian message passing algorithms. In this paper, we elaborate on recently proposed NUP representations of half-space constraints, box constraints, and finite-level constraints. We then demonstrate the use of such NUP representations for exemplary applications in model predictive control with a variety of constraints on the input, the output, or the internal state of the controlled system. In such applications, the computations boil down to iterations of Kalman-type forward-backward recursions, with a complexity (per iteration) that is linear in the planning horizon. In consequence, this approach can handle long planning horizons, which distinguishes it from the prior art. For nonconvex constraints, this approach has no claim to optimality, but it is empirically very effective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.15806v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raphael Keusch, Hans-Andrea Loeliger</dc:creator>
    </item>
    <item>
      <title>An output-polynomial time algorithm to determine all supported efficient solutions for multi-objective integer network flow problems</title>
      <link>https://arxiv.org/abs/2305.12867</link>
      <description>arXiv:2305.12867v3 Announce Type: replace 
Abstract: This paper addresses the problem of enumerating all supported efficient solutions for a linear multi-objective integer minimum cost flow problem (MOIMCF). It derives an output-polynomial time algorithm to determine all supported efficient solutions for MOIMCF problems. This is the first approach to solve this general problem in output-polynomial time. Moreover, we prove that the existence of an output-polynomial time algorithm to determine all weakly supported nondominated vectors (or all weakly supported efficient solutions) for a MOIMCF problem with a fixed number of d &gt;= 3 objectives can be excluded unless P = NP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.12867v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David K\"onen, Michael Stiglmayr</dc:creator>
    </item>
    <item>
      <title>A new mathematical model and algorithm for the optimal path to intercept a moving target</title>
      <link>https://arxiv.org/abs/2310.17856</link>
      <description>arXiv:2310.17856v2 Announce Type: replace 
Abstract: This paper is concerned with determining the shortest path for a pursuer aiming to intercept a moving target travelling at a constant speed. To address this challenge, we introduce an efficient mathematical model outlined as an optimal control problem. The proposed model is based on Dubin's path, where we concatenate two possible paths: a left-circular curve or a right-circular curve followed by a straight line. We develop and explore this model, providing a comprehensive geometric interpretation, and design an algorithm tailored to implement the proposed mathematical approach efficiently. Extensive numerical experiments involving diverse target positions highlight the strength of the model. The method exhibits a remarkably high convergence rate in finding solutions. We compare the proposed model and demonstrate its advantages through examples. For experiment purposes, we utilized the modelling software AMPL, employing a range of solvers to solve the problem. Subsequently, we simulated the obtained solutions using MATLAB, demonstrating the efficiency of the model in intercepting a moving target. The proposed model distinguishes itself by employing fewer parameters and making fewer assumptions, setting the model simplifies the complexities, and thus, makes it easier for experts to design optimal path plans.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17856v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Akter, M. M. Rizvi, M. Forkan</dc:creator>
    </item>
    <item>
      <title>De Finetti's Poissonian Dividend Control Problem under Spectrally Positive Markov Additive Process</title>
      <link>https://arxiv.org/abs/2311.04555</link>
      <description>arXiv:2311.04555v2 Announce Type: replace 
Abstract: We study a De Finetti's optimal dividend and capital injection problem under a Markov additive model. The surplus process without dividend and capital injection is assumed to follow a spectrally positive Markov additive process (MAP). Dividend payments are made at the jump times of an independent Poisson process and capitals are injected to avoid bankruptcy. The aim of the paper is to characterize an optimal dividend and capital injection strategy that maximizes the expected total discounted dividends subtracted by the total discounted costs of capital injection. Applying the fluctuation and excursion theory for Levy processes and the stochastic control theory, we first address an auxiliary dividend and capital injection control problem with a terminal payoff under the spectrally positive Levy model. Using results obtained for this auxiliary problem and a fixed point argument for iterations induced by dynamic program, we characterize the optimal strategy of our prime control problem as a regime-modulated double-barrier Poissonian-continuous-reflection dividend and capital injection strategy. Besides, a numerical example is provided to illustrate the features of the optimal strategies. The impacts of model parameters are also studied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04555v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lijun Bo, Wenyuan Wang, Kaixin Yan</dc:creator>
    </item>
    <item>
      <title>Optimality conditions in terms of Bouligand generalized differentials for a nonsmooth semilinear elliptic optimal control problem with distributed and boundary control pointwise constraints</title>
      <link>https://arxiv.org/abs/2311.15669</link>
      <description>arXiv:2311.15669v3 Announce Type: replace 
Abstract: This paper is concerned with an optimal control problem governed by nonsmooth semilinear elliptic partial differential equations with both distributed and boundary unilateral pointwise control constraints, in which the nonlinear coefficient in the state equation is not differentiable at one point.
  Therefore, the Bouligand subdifferential of this nonsmooth coefficient in every point consists of one or two elements that will be used to construct the two associated Bouligand generalized derivatives of the control-to-state operator in any admissible control.
  These Bouligand generalized derivatives appear in a novel optimality condition, which extends the purely primal optimality condition saying that the directional derivative of the reduced objective functional in admissible directions in nonnegative.
  We then establish the optimality conditions in the form of multiplier existence. There, in addition to the existence of the adjoint states and of the nonnegative multipliers associated with the unilateral pointwise constraints as usual, other nonnegative multipliers exist and correspond to the nondifferentiability of the control-to-state mapping.
  The latter type of optimality conditions shall be applied to an optimal control satisfying the so-called \emph{constraint qualification} to derive a \emph{strong} stationarity, where the sign of the associated adjoint state does not vary on the level set of the corresponding optimal state at the value of nondifferentiability.
  Finally, this strong stationarity is also shown to be equivalent to the purely primal optimality condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15669v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vu Huu Nhu, Nguyen Hai Son</dc:creator>
    </item>
    <item>
      <title>Universal generalization guarantees for Wasserstein distributionally robust models</title>
      <link>https://arxiv.org/abs/2402.11981</link>
      <description>arXiv:2402.11981v3 Announce Type: replace 
Abstract: Distributionally robust optimization has emerged as an attractive way to train robust machine learning models, capturing data uncertainty and distribution shifts. Recent statistical analyses have proved that generalization guarantees of robust models based on the Wasserstein distance have generalization guarantees that do not suffer from the curse of dimensionality. However, these results are either approximate, obtained in specific cases, or based on assumptions difficult to verify in practice. In contrast, we establish exact generalization guarantees that cover a wide range of cases, with arbitrary transport costs and parametric loss functions, including deep learning objectives with nonsmooth activations. We complete our analysis with an excess bound on the robust objective and an extension to Wasserstein robust models with entropic regularizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11981v3</guid>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tam Le (DAO), J\'er\^ome Malick (DAO)</dc:creator>
    </item>
    <item>
      <title>All You Need is Resistance: On the Equivalence of Effective Resistance and Certain Optimal Transport Problems on Graphs</title>
      <link>https://arxiv.org/abs/2404.15261</link>
      <description>arXiv:2404.15261v3 Announce Type: replace 
Abstract: The fields of effective resistance and optimal transport on graphs are filled with rich connections to combinatorics, geometry, machine learning, and beyond. In this article we put forth a bold claim: that the two fields should be understood as one and the same, up to a choice of $p$. We make this claim precise by introducing the parameterized family of $p$-Beckmann distances for probability measures on graphs and relate them sharply to certain Wasserstein distances. Then, we break open a suite of results including explicit connections to optimal stopping times and random walks on graphs, graph Sobolev spaces, and a Benamou-Brenier type formula for $2$-Beckmann distance. We further explore empirical implications in the world of unsupervised learning for graph data and propose further study of the usage of these metrics where Wasserstein distance may produce computational bottlenecks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15261v3</guid>
      <category>math.OC</category>
      <category>cs.DM</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sawyer Robertson, Zhengchao Wan, Alexander Cloninger</dc:creator>
    </item>
    <item>
      <title>Complexity of Minimizing Regularized Convex Quadratic Functions</title>
      <link>https://arxiv.org/abs/2404.17543</link>
      <description>arXiv:2404.17543v2 Announce Type: replace 
Abstract: In this work, we study the iteration complexity of gradient methods for minimizing convex quadratic functions regularized by powers of Euclidean norms. We show that, due to the uniform convexity of the objective, gradient methods have improved convergence rates. Thus, for the basic gradient descent with a novel step size, we prove a convergence rate of $O(N^{-p/(p - 2)})$ for the functional residual, where $N$ is the iteration number and $p &gt; 2$ is the power of the regularization term. We also show that this rate is tight by establishing a corresponding lower bound for one-step first-order methods. Then, for the general class of all multi-step methods, we establish that the rate of $O(N^{-2p/(p-2)})$ is optimal, providing a sharp analysis of the minimization of uniformly convex regularized quadratic functions. This rate is achieved by the fast gradient method. A special case of our problem class is $p=3$, which is the minimization of cubically regularized convex quadratic functions. It naturally appears as a subproblem at each iteration of the cubic Newton method. Therefore, our theory shows that the rate of $O(N^{-6})$ is optimal in this case. We also establish new lower bounds on minimizing the gradient norm within our framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17543v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Berg Thomsen, Nikita Doikov</dc:creator>
    </item>
    <item>
      <title>Logic-Based Discrete-Steepest Descent: A Solution Method for Process Synthesis Generalized Disjunctive Programs</title>
      <link>https://arxiv.org/abs/2405.05358</link>
      <description>arXiv:2405.05358v2 Announce Type: replace 
Abstract: The optimization of chemical processes is challenging due to the nonlinearities arising from process physics and discrete design decisions. In particular, optimal synthesis and design of chemical processes can be posed as a Generalized Disjunctive Programming (GDP) superstructure problem. Various solution methods are available to address these problems, such as reformulating them as Mixed-Integer Nonlinear Programming (MINLP) problems; nevertheless, algorithms explicitly designed to solve the GDP problem and potentially leverage its structure remain scarce. This paper presents the Logic-based Discrete-Steepest Descent Algorithm (LD-SDA) as a solution method for GDP problems involving ordered Boolean variables. The LD-SDA reformulates these ordered Boolean variables into integer decisions called external variables. The LD-SDA solves the reformulated GDP problem using a two-level decomposition approach where the upper-level subproblem determines external variable configurations. Subsequently, the remaining continuous and discrete variables are solved as a subproblem only involving those constraints relevant to the given external variable arrangement, effectively taking advantage of the structure of the GDP problem. The advantages of LD-SDA are illustrated through a batch processing case study, a reactor superstructure, a distillation column, and a catalytic distillation column, and its open-source implementation is available online. The results show convergence efficiency and solution quality improvements compared to conventional GDP and MINLP solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05358v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.compchemeng.2024.108993</arxiv:DOI>
      <arxiv:journal_reference>Computers &amp; Chemical Engineering, Volume 195, April 2025, Article Number 108993</arxiv:journal_reference>
      <dc:creator>Daniel Ovalle, David A. Li\~n\'an, Albert Lee, Jorge M. G\'omez, Luis Ricardez-Sandoval, Ignacio E. Grossmann, David E. Bernal Neira</dc:creator>
    </item>
    <item>
      <title>Fast Convergence of Frank-Wolfe algorithms on polytopes</title>
      <link>https://arxiv.org/abs/2406.18789</link>
      <description>arXiv:2406.18789v2 Announce Type: replace 
Abstract: We provide a template to derive convergence rates for the following popular versions of the Frank-Wolfe algorithm on polytopes: vanilla Frank-Wolfe, Frank-Wolfe with away steps, Frank-Wolfe with blended pairwise steps, and Frank-Wolfe with in-face directions. Our template shows how the convergence rates follow from two affine-invariant properties of the problem, namely, error bound and extended curvature. These properties depend solely on the polytope and objective function but not on any affine-dependent object like norms. For each one of the above algorithms, we derive rates of convergence ranging from sublinear to linear depending on the degree of the error bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18789v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elias Wirth, Javier Pena, Sebastian Pokutta</dc:creator>
    </item>
    <item>
      <title>Controllability problems of a neutral integro-differential equation with memory</title>
      <link>https://arxiv.org/abs/2407.07886</link>
      <description>arXiv:2407.07886v3 Announce Type: replace 
Abstract: The current study addresses the control problems posed by a semilinear neutral integro-differential equation with memory. The primary objectives of this study are to investigate the existence of a mild solution and approximate controllability of both linear and semilinear control systems in Banach spaces. To accomplish this, we begin by introducing the concept of a resolvent family associated with the homogeneous neutral integro-differential equation without memory. In the process, we establish some important properties of the resolvent family. Subsequently, we develop approximate controllability results for a linear control problem by constructing a linear-quadratic regulator problem. This involves establishing the existence of an optimal pair and determining the expression of the optimal control that produces the approximate controllability of the linear system. Furthermore, we deduce sufficient conditions for the existence of a mild solution and approximate controllability of a semilinear system in a reflexive Banach space with a uniformly convex dual. Additionally, we delve into the discussion of approximate controllability for a semilinear problem in general Banach space, assuming a Lipschitz type condition on the nonlinear term. Finally, we implement our findings to examine the approximate controllability of certain partial differential equations, demonstrating their practical relevance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07886v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sumit Arora, Akambadath Nandakumaran</dc:creator>
    </item>
    <item>
      <title>Approximate D-optimal design and equilibrium measure</title>
      <link>https://arxiv.org/abs/2409.04058</link>
      <description>arXiv:2409.04058v2 Announce Type: replace 
Abstract: We introduce a minor variant of  the approximate D-optimal design of experiments with a more general information matrixthat takes into account the representation of the design space S. The main motivation (and result) is that if S in R^d is the unit ball, the unit box or the canonical simplex, then remarkably, for every dimension d and every degree n, one obtains an optimal solution in closed form, namely the equilibrium measure of S (in pluripotential theory). Equivalently, for each degree n, the unique optimal solution is the vector of moments (up to degree 2n) of the equilibrium measure of S. Hence finding an optimal design reduces to finding a cubature for the equilibrium measure, with atoms in S, positive weights, and exact up to degree 2n. In addition, any resulting sequence of atomic D-optimal measures converges to the equilibrium measure of S for the weak-star topology, as n increases. Links with Fekete sets of points are also discussed. More general compact basic semi-algebraic sets are also considered, and a previously developed two-step design algorithm is easily adapted to this new variant of D-optimal design problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04058v2</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Didier Henrion (LAAS-POP), Jean Bernard Lasserre (LAAS-POP, TSE-R)</dc:creator>
    </item>
    <item>
      <title>Functionally Constrained Algorithm Solves Convex Simple Bilevel Problems</title>
      <link>https://arxiv.org/abs/2409.06530</link>
      <description>arXiv:2409.06530v3 Announce Type: replace 
Abstract: This paper studies simple bilevel problems, where a convex upper-level function is minimized over the optimal solutions of a convex lower-level problem. We first show the fundamental difficulty of simple bilevel problems, that the approximate optimal value of such problems is not obtainable by first-order zero-respecting algorithms. Then we follow recent works to pursue the weak approximate solutions. For this goal, we propose a novel method by reformulating them into functionally constrained problems. Our method achieves near-optimal rates for both smooth and nonsmooth problems. To the best of our knowledge, this is the first near-optimal algorithm that works under standard assumptions of smoothness or Lipschitz continuity for the objective functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06530v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huaqing Zhang, Lesi Chen, Jing Xu, Jingzhao Zhang</dc:creator>
    </item>
    <item>
      <title>Hierarchical Nash Equilibrium over Variational Equilibria via Fixed-point Set Expression of Quasi-nonexpansive Operator</title>
      <link>https://arxiv.org/abs/2409.11094</link>
      <description>arXiv:2409.11094v4 Announce Type: replace 
Abstract: The equilibrium selection problem in the generalized Nash equilibrium problem (GNEP) has recently been studied as an optimization problem, defined over the set of all variational equilibria achievable through a lower-level non-cooperative game among players. However, to make such a selection fair for every player, we have to rely on an unrealistic assumption, that is, the availability of a trusted center that does not induce any bias for every player. In this paper, we study a new equilibrium selection problem, named the hierarchical Nash equilibrium problem (HNEP), and propose an iterative algorithm for solving the HNEP. The HNEP is designed to ensure a fair selection without assuming any trusted center. More precisely, the HNEP is the GNEP for an upper-level non-cooperative game defined over the set of all variational equilibria of the lower-level non-cooperative game. The proposed algorithm for the HNEP is established by applying the hybrid steepest descent method to a variational inequality defined over the fixed point set of a quasi-nonexpansive operator. Numerical experiments show the effectiveness of the proposed equilibrium selection problem and its algorithmic solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11094v4</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shota Matsuo, Keita Kume, Isao Yamada</dc:creator>
    </item>
    <item>
      <title>Towards Robust Spacecraft Trajectory Optimization via Transformers</title>
      <link>https://arxiv.org/abs/2410.05585</link>
      <description>arXiv:2410.05585v2 Announce Type: replace 
Abstract: Future multi-spacecraft missions require robust autonomous trajectory optimization capabilities to ensure safe and efficient rendezvous operations. This capability hinges on solving non-convex optimal control problems in real-time, although traditional iterative methods such as sequential convex programming impose significant computational challenges. To mitigate this burden, the Autonomous Rendezvous Transformer (ART) introduced a generative model trained to provide near-optimal initial guesses. This approach provides convergence to better local optima (e.g., fuel optimality), improves feasibility rates, and results in faster convergence speed of optimization algorithms through warm-starting. This work extends the capabilities of ART to address robust chance-constrained optimal control problems. Specifically, ART is applied to challenging rendezvous scenarios in Low Earth Orbit (LEO), ensuring fault-tolerant behavior under uncertainty. Through extensive experimentation, the proposed warm-starting strategy is shown to consistently produce high-quality reference trajectories, achieving up to 30\% cost improvement and 50\% reduction in infeasible cases compared to conventional methods, demonstrating robust performance across multiple state representations. Additionally, a post hoc evaluation framework is proposed to assess the quality of generated trajectories and mitigate runtime failures, marking an initial step toward the reliable deployment of AI-driven solutions in safety-critical autonomous systems such as spacecraft.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05585v2</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuji Takubo, Tommaso Guffanti, Daniele Gammelli, Marco Pavone, Simone D'Amico</dc:creator>
    </item>
    <item>
      <title>ItsOPT: An inexact two-level smoothing framework for nonconvex optimization via high-order Moreau envelope</title>
      <link>https://arxiv.org/abs/2410.19928</link>
      <description>arXiv:2410.19928v3 Announce Type: replace 
Abstract: This paper introduces an inexact two-level smoothing optimization framework (ItsOPT)} for finding first-order critical points of nonsmooth and nonconvex functions. The framework involves two levels of methodologies: at the upper level, a first- or second-order method will be tailored to minimize a smooth approximation of the cost function; at the lower level, the high-order proximal auxiliary problems will be solved inexactly, generating an inexact oracle. As a smoothing technique, in particular, we here introduce the high-order Moreau envelope (HOME) and study its fundamental features under standard assumptions and its differential properties under a variant of prox-regularity. Next, introducing a high-order proximal-point algorithm (HiPPA) and its boosted variant (Boosted HiPPA) at the upper level and solving the proximal subproblem inexactly at the lower level lead to an instance method of the ItsOPT framework. Global and linear convergence results are established under the Kurdyka-{\L}ojasiewicz (KL) property of the cost and envelope functions, along with some reasonable conditions for the accuracy of the proximal terms. Preliminary numerical experiments on a robust low-rank matrix recovery problem indicate a promising performance of the proposed algorithm, validating our theoretical foundations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19928v3</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alireza Kabgani, Masoud Ahookhosh</dc:creator>
    </item>
    <item>
      <title>Sensitivity of ODE Solutions and Quantities of Interest with Respect to Component Functions in the Dynamics</title>
      <link>https://arxiv.org/abs/2411.09655</link>
      <description>arXiv:2411.09655v2 Announce Type: replace 
Abstract: This work analyzes the sensitivities of the solution of a system of ordinary differential equations (ODEs) and a corresponding quantity of interest (QoI) to perturbations in a state-dependent component function that appears in the governing ODEs. This extends existing ODE sensitivity results, which consider the sensitivity of the ODE solution with respect to state-independent parameters. It is shown that with Carath\'eodory-type assumptions on the ODEs, the Implicit Function Theorem can be applied to establish continuous Fr\'echet differentiability of the ODE solution with respect to the component function. These sensitivities are used to develop new estimates for the change in the ODE solution or QoI when the component function is perturbed. In applications, this new sensitivity-based bound on the ODE solution or QoI error is often much tighter than classical Gronwall-type error bounds. The sensitivity-based error bounds are applied to Zermelo's problem and to a trajectory simulation for a hypersonic vehicle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09655v2</guid>
      <category>math.OC</category>
      <category>math.CA</category>
      <category>math.DS</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan R. Cangelosi, Matthias Heinkenschloss</dc:creator>
    </item>
    <item>
      <title>Trade-off Invariance Principle for minimizers of regularized functionals</title>
      <link>https://arxiv.org/abs/2411.11639</link>
      <description>arXiv:2411.11639v4 Announce Type: replace 
Abstract: In this paper, we consider functionals of the form $H_\alpha(u)=F(u)+\alpha G(u)$ with $\alpha\in[0,+\infty)$, where $u$ varies in a set $U\neq\emptyset$ (without further structure). We first revisit a result stating that, excluding at most countably many values of $\alpha$, we have $\inf_{H_\alpha^\star}G= \sup_{H_\alpha^\star}G$, where $H_\alpha^\star := \arg\min_UH_\alpha$, which is assumed to be non-empty. Then, we prove a stronger result that concerns the invariance of the limiting value of the functional $G$ along minimizing sequences for $H_\alpha$, which extends the above Principle to the case $H_\alpha^\star= \emptyset$. Moreover, we show to what extent these findings generalize to multi-regularized functionals and -- in the presence of an underlying differentiable structure -- to critical points. Finally, the main result implies an unexpected consequence for functionals regularized with uniformly convex norms: excluding again at most countably many values of $\alpha$, it turns out that for a minimizing sequence, convergence to a minimizer in the weak or strong sense is equivalent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11639v4</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Massimo Fornasier, Jona Klemenc, Alessandro Scagliotti</dc:creator>
    </item>
    <item>
      <title>Average-Cost MDPs with Infinite State and Action Sets: New Sufficient Conditions for Optimality Inequalities and Equations</title>
      <link>https://arxiv.org/abs/2412.01594</link>
      <description>arXiv:2412.01594v2 Announce Type: replace 
Abstract: This paper studies discrete-time average-cost infinite-horizon Markov decision processes (MDPs) with Borel state and action sets. It introduces new sufficient conditions for { the} validity of optimality inequalities and optimality equations for MDPs with weakly and setwise continuous transition probabilities. These inequalities and equations imply the existence of deterministic optimal policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01594v2</guid>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Eugene A. Feinberg, Pavlo O. Kasyanov, Liliia S. Paliichuk</dc:creator>
    </item>
    <item>
      <title>Deep Distributed Optimization for Large-Scale Quadratic Programming</title>
      <link>https://arxiv.org/abs/2412.12156</link>
      <description>arXiv:2412.12156v2 Announce Type: replace 
Abstract: Quadratic programming (QP) forms a crucial foundation in optimization, encompassing a broad spectrum of domains and serving as the basis for more advanced algorithms. Consequently, as the scale and complexity of modern applications continue to grow, the development of efficient and reliable QP algorithms is becoming increasingly vital. In this context, this paper introduces a novel deep learning-aided distributed optimization architecture designed for tackling large-scale QP problems. First, we combine the state-of-the-art Operator Splitting QP (OSQP) method with a consensus approach to derive DistributedQP, a new method tailored for network-structured problems, with convergence guarantees to optimality. Subsequently, we unfold this optimizer into a deep learning framework, leading to DeepDistributedQP, which leverages learned policies to accelerate reaching to desired accuracy within a restricted amount of iterations. Our approach is also theoretically grounded through Probably Approximately Correct (PAC)-Bayes theory, providing generalization bounds on the expected optimality gap for unseen problems. The proposed framework, as well as its centralized version DeepQP, significantly outperform their standard optimization counterparts on a variety of tasks such as randomly generated problems, optimal control, linear regression, transportation networks and others. Notably, DeepDistributedQP demonstrates strong generalization by training on small problems and scaling to solve much larger ones (up to 50K variables and 150K constraints) using the same policy. Moreover, it achieves orders-of-magnitude improvements in wall-clock time compared to OSQP. The certifiable performance guarantees of our approach are also demonstrated, ensuring higher-quality solutions over traditional optimizers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12156v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Augustinos D. Saravanos, Hunter Kuperman, Alex Oshin, Arshiya Taj Abdul, Vincent Pacelli, Evangelos A. Theodorou</dc:creator>
    </item>
    <item>
      <title>Comments and extensions on "State-equivalent form and minimum-order compensator design for rectangular descriptor systems"</title>
      <link>https://arxiv.org/abs/2501.13445</link>
      <description>arXiv:2501.13445v2 Announce Type: replace 
Abstract: This technical note presents a counterexample showing that the equivalence conditions proposed by Geng et al. (IEEE Trans. Automat. Control, 2024), which use a minimum-order compensator (MOC) to achieve desired designs, including generalized regularity or both generalized regularity and free of impulse, are sufficient but not necessary. Furthermore, revised equivalence conditions are introduced, along with an equivalence condition ensuring the closed-loop system remains generalized regular, impulse-free, and stable using MOC. Additionally, it is shown that output feedback can replace the MOC, achieving the same design without increasing dimensionality. These findings are validated through a circuit example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13445v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuo Shi, Juan Zhang</dc:creator>
    </item>
    <item>
      <title>Sample-Based Piecewise Linear Power Flow Approximations Using Second-Order Sensitivities</title>
      <link>https://arxiv.org/abs/2501.13825</link>
      <description>arXiv:2501.13825v2 Announce Type: replace 
Abstract: The inherent nonlinearity of the power flow equations poses significant challenges in accurately modeling power systems, particularly when employing linearized approximations. Although power flow linearizations provide computational efficiency, they can fail to fully capture nonlinear behavior across diverse operating conditions. To improve approximation accuracy, we propose conservative piecewise linear approximations (CPLA) of the power flow equations, which are designed to consistently over- or under-estimate the quantity of interest, ensuring conservative behavior in optimization. The flexibility provided by piecewise linear functions can yield improved accuracy relative to standard linear approximations. However, applying CPLA across all dimensions of the power flow equations could introduce significant computational complexity, especially for large-scale optimization problems. In this paper, we propose a strategy that selectively targets dimensions exhibiting significant nonlinearities. Using a second-order sensitivity analysis, we identify the directions where the power flow equations exhibit the most significant curvature and tailor the CPLAs to improve accuracy in these specific directions. This approach reduces the computational burden while maintaining high accuracy, making it particularly well-suited for mixed-integer programming problems involving the power flow equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13825v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paprapee Buason, Sidhant Misra, Daniel K. Molzahn</dc:creator>
    </item>
    <item>
      <title>The Third Evolution Equation for Optimal Control Computation</title>
      <link>https://arxiv.org/abs/1802.04663</link>
      <description>arXiv:1802.04663v2 Announce Type: replace-cross 
Abstract: The Variation Evolving Method (VEM) that originates from the continuous-time dynamics stability theory seeks the optimal solutions with variation evolution principle. After establishing the first and the second evolution equations within its frame, the third evolution equation is developed. This equation only solves the control variables along the variation time to get the optimal solution, and its definite conditions may be arbitrary since the equation can eliminate possible infeasibilities. With this equation, the dimension of the resulting Initial-value Problem (IVP), transformed via the semi-discrete method, is greatly reduced. Therefore it might relieve the computation burden in seeking solutions. Illustrative examples are solved and it is shown that the proposed equation may produce more precise numerical solutions than the second evolution equation, and its computation time may be shorter for the dense discretization.</description>
      <guid isPermaLink="false">oai:arXiv.org:1802.04663v2</guid>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sheng Zhang, Fei Liao, Kai-Feng He</dc:creator>
    </item>
    <item>
      <title>Utility maximization in multivariate Volterra models</title>
      <link>https://arxiv.org/abs/2111.02191</link>
      <description>arXiv:2111.02191v4 Announce Type: replace-cross 
Abstract: This paper is concerned with portfolio selection for an investor with power utility in multi-asset financial markets in a rough stochastic environment. We investigate Merton's portfolio problem for different multivariate Volterra models, covering the rough Heston model. First we consider a class of multivariate affine Volterra models introduced in [E. Abi Jaber et al., SIAM J. Financial Math., 12, 369-409, (2021)]. Based on the classical Wishart model described in [N. B\"auerle and Li, Z., J. Appl. Probab., 50, 1025-1043 (2013)], we then introduce a new matrix-valued stochastic volatility model, where the volatility is driven by a Volterra-Wishart process. Due to the non-Markovianity of the underlying processes, the classical stochastic control approach cannot be applied in these settings. To overcome this issue, we provide a verification argument using calculus of convolutions and resolvents. The resulting optimal strategy can then be expressed explicitly in terms of the solution of a multivariate Riccati-Volterra equation. We thus extend the results obtained by Han and Wong to the multivariate case, avoiding restrictions on the correlation structure linked to the martingale distortion transformation used in [B. Han and Wong, H. Y., Finance Res. Lett., 39 (2021)]. We also provide existence and uniqueness theorems for the occurring Volterra processes and illustrate our results with a numerical study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.02191v4</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Florian Aichinger, Sascha Desmettre</dc:creator>
    </item>
    <item>
      <title>The Strong Maximum Circulation Algorithm: A New Method for Aggregating Preference Rankings</title>
      <link>https://arxiv.org/abs/2307.15702</link>
      <description>arXiv:2307.15702v4 Announce Type: replace-cross 
Abstract: We present a new optimization-based method for aggregating preferences in settings where each voter expresses preferences over pairs of alternatives. Our approach to identifying a consensus partial order is motivated by the observation that collections of votes that form a cycle can be treated as collective ties. Our approach then removes unions of cycles of votes, or circulations, from the vote graph and determines aggregate preferences from the remainder. Specifically, we study the removal of maximal circulations attained by any union of cycles the removal of which leaves an acyclic graph. We introduce the strong maximum circulation, the removal of which guarantees a unique outcome in terms of the induced partial order, called the strong partial order. The strong maximum circulation also satisfies strong complementary slackness conditions, and is shown to be solved efficiently as a network flow problem. We further establish the relationship between the dual of the maximum circulation problem and Kemeny's method, a popular optimization-based approach for preference aggregation. We also show that identifying a minimum maximal circulation -- i.e., a maximal circulation containing the smallest number of votes -- is an NP-hard problem. Further an instance of the minimum maximal circulation may have multiple optimal solutions whose removal results in conflicting partial orders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15702v4</guid>
      <category>cs.SI</category>
      <category>econ.TH</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Atkinson, Scott C. Ganz, Dorit S. Hochbaum, James B. Orlin</dc:creator>
    </item>
    <item>
      <title>Learning Exactly Linearizable Deep Dynamics Models</title>
      <link>https://arxiv.org/abs/2311.18261</link>
      <description>arXiv:2311.18261v2 Announce Type: replace-cross 
Abstract: Research on control using models based on machine-learning methods has now shifted to the practical engineering stage. Achieving high performance and theoretically guaranteeing the safety of the system is critical for such applications. In this paper, we propose a learning method for exactly linearizable dynamical models that can easily apply various control theories to ensure stability, reliability, etc., and to provide a high degree of freedom of expression. As an example, we present a design that combines simple linear control and control barrier functions. The proposed model is employed for the real-time control of an automotive engine, and the results demonstrate good predictive performance and stable control under constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18261v2</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryuta Moriyasu, Masayuki Kusunoki, Kenji Kashima</dc:creator>
    </item>
    <item>
      <title>Gradient Networks</title>
      <link>https://arxiv.org/abs/2404.07361</link>
      <description>arXiv:2404.07361v3 Announce Type: replace-cross 
Abstract: Directly parameterizing and learning gradients of functions has widespread significance, with specific applications in inverse problems, generative modeling, and optimal transport. This paper introduces gradient networks (GradNets): novel neural network architectures that parameterize gradients of various function classes. GradNets exhibit specialized architectural constraints that ensure correspondence to gradient functions. We provide a comprehensive GradNet design framework that includes methods for transforming GradNets into monotone gradient networks (mGradNets), which are guaranteed to represent gradients of convex functions. Our results establish that our proposed GradNet (and mGradNet) universally approximate the gradients of (convex) functions. Furthermore, these networks can be customized to correspond to specific spaces of potential functions, including transformed sums of (convex) ridge functions. Our analysis leads to two distinct GradNet architectures, GradNet-C and GradNet-M, and we describe the corresponding monotone versions, mGradNet-C and mGradNet-M. Our empirical results demonstrate that these architectures provide efficient parameterizations and outperform existing methods by up to 15 dB in gradient field tasks and by up to 11 dB in Hamiltonian dynamics learning tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07361v3</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>eess.SP</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TSP.2024.3496692</arxiv:DOI>
      <arxiv:journal_reference>in IEEE Transactions on Signal Processing, vol. 73, pp. 324-339, 2025</arxiv:journal_reference>
      <dc:creator>Shreyas Chaudhari, Srinivasa Pranav, Jos\'e M. F. Moura</dc:creator>
    </item>
    <item>
      <title>Sensor-Based Distributionally Robust Control for Safe Robot Navigation in Dynamic Environments</title>
      <link>https://arxiv.org/abs/2405.18251</link>
      <description>arXiv:2405.18251v3 Announce Type: replace-cross 
Abstract: We introduce a novel method for mobile robot navigation in dynamic, unknown environments, leveraging onboard sensing and distributionally robust optimization to impose probabilistic safety constraints. Our method introduces a distributionally robust control barrier function (DR-CBF) that directly integrates noisy sensor measurements and state estimates to define safety constraints. This approach is applicable to a wide range of control-affine dynamics, generalizable to robots with complex geometries, and capable of operating at real-time control frequencies. Coupled with a control Lyapunov function (CLF) for path following, the proposed CLF-DR-CBF control synthesis method achieves safe, robust, and efficient navigation in challenging environments. We demonstrate the effectiveness and robustness of our approach for safe autonomous navigation under uncertainty in simulations and real-world experiments with differential-drive robots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18251v3</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kehan Long, Yinzhuang Yi, Zhirui Dai, Sylvia Herbert, Jorge Cort\'es, Nikolay Atanasov</dc:creator>
    </item>
    <item>
      <title>Path-dependent Hamilton-Jacobi equations with u-dependence and time-measurable Hamiltonians</title>
      <link>https://arxiv.org/abs/2408.02145</link>
      <description>arXiv:2408.02145v2 Announce Type: replace-cross 
Abstract: We establish existence and uniqueness of minimax solutions for a fairly general class of path-dependent Hamilton-Jacobi equations. In particular, the relevant Hamiltonians can contain the solution and they only need to be measurable with respect to time. We apply our results to optimal control problems of (delay) functional differential equations with cost functionals that have discount factors and with time-measurable data. Our main results are also crucial for our companion paper Bandini and Keller [arXiv preprint arXiv:2408.02147 (2024)], where non-local path-dependent Hamilton-Jacobi-Bellman equations associated to the stochastic optimal control of non-Markovian piecewise deterministic processes are studied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02145v2</guid>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elena Bandini, Christian Keller</dc:creator>
    </item>
    <item>
      <title>Beyond the Neural Fog: Interpretable Learning for AC Optimal Power Flow</title>
      <link>https://arxiv.org/abs/2408.05228</link>
      <description>arXiv:2408.05228v2 Announce Type: replace-cross 
Abstract: The AC optimal power flow (AC-OPF) problem is essential for power system operations, but its non-convex nature makes it challenging to solve. A widely used simplification is the linearized DC optimal power flow (DC-OPF) problem, which can be solved to global optimality, but whose optimal solution is always infeasible in the original AC-OPF problem. Recently, neural networks (NN) have been introduced for solving the AC-OPF problem at significantly faster computation times. However, these methods necessitate extensive datasets, are difficult to train, and are often viewed as black boxes, leading to resistance from operators who prefer more transparent and interpretable solutions. In this paper, we introduce a novel learning-based approach that merges simplicity and interpretability, providing a bridge between traditional approximation methods and black-box learning techniques. Our approach not only provides transparency for operators but also achieves competitive accuracy. Numerical results across various power networks demonstrate that our method provides accuracy comparable to, and often surpassing, that of neural networks, particularly when training datasets are limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05228v2</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Salvador Pineda, Juan P\'erez-Ruiz, Juan Miguel Morales</dc:creator>
    </item>
    <item>
      <title>String Diagram of Optimal Transports</title>
      <link>https://arxiv.org/abs/2408.08550</link>
      <description>arXiv:2408.08550v2 Announce Type: replace-cross 
Abstract: We present a novel hierarchical framework for optimal transport (OT) using string diagrams, namely string diagrams of optimal transports. This framework reduces complex hierarchical OT problems to standard OT problems, allowing efficient synthesis of optimal hierarchical transportation plans. Our approach uses algebraic compositions of cost matrices to effectively model hierarchical structures. We also study an adversarial situation with multiple choices in the cost matrices, where we present a polynomial-time algorithm for a relaxation of the problem. Experimental results confirm the efficiency and performance advantages of our proposed algorithm over the naive method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08550v2</guid>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazuki Watanabe, Noboru Isobe</dc:creator>
    </item>
    <item>
      <title>Two-Timescale Gradient Descent Ascent Algorithms for Nonconvex Minimax Optimization</title>
      <link>https://arxiv.org/abs/2408.11974</link>
      <description>arXiv:2408.11974v3 Announce Type: replace-cross 
Abstract: We provide a unified analysis of two-timescale gradient descent ascent (TTGDA) for solving structured nonconvex minimax optimization problems in the form of $\min_\textbf{x} \max_{\textbf{y} \in Y} f(\textbf{x}, \textbf{y})$, where the objective function $f(\textbf{x}, \textbf{y})$ is nonconvex in $\textbf{x}$ and concave in $\textbf{y}$, and the constraint set $Y \subseteq \mathbb{R}^n$ is convex and bounded. In the convex-concave setting, the single-timescale gradient descent ascent (GDA) algorithm is widely used in applications and has been shown to have strong convergence guarantees. In more general settings, however, it can fail to converge. Our contribution is to design TTGDA algorithms that are effective beyond the convex-concave setting, efficiently finding a stationary point of the function $\Phi(\cdot) := \max_{\textbf{y} \in Y} f(\cdot, \textbf{y})$. We also establish theoretical bounds on the complexity of solving both smooth and nonsmooth nonconvex-concave minimax optimization problems. To the best of our knowledge, this is the first systematic analysis of TTGDA for nonconvex minimax optimization, shedding light on its superior performance in training generative adversarial networks (GANs) and in other real-world application problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11974v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyi Lin, Chi Jin, Michael. I. Jordan</dc:creator>
    </item>
    <item>
      <title>S-CFE: Simple Counterfactual Explanations</title>
      <link>https://arxiv.org/abs/2410.15723</link>
      <description>arXiv:2410.15723v4 Announce Type: replace-cross 
Abstract: We study the problem of finding optimal sparse, manifold-aligned counterfactual explanations for classifiers. Canonically, this can be formulated as an optimization problem with multiple non-convex components, including classifier loss functions and manifold alignment (or \emph{plausibility}) metrics. The added complexity of enforcing \emph{sparsity}, or shorter explanations, complicates the problem further. Existing methods often focus on specific models and plausibility measures, relying on convex $\ell_1$ regularizers to enforce sparsity. In this paper, we tackle the canonical formulation using the accelerated proximal gradient (APG) method, a simple yet efficient first-order procedure capable of handling smooth non-convex objectives and non-smooth $\ell_p$ (where $0 \leq p &lt; 1$) regularizers. This enables our approach to seamlessly incorporate various classifiers and plausibility measures while producing sparser solutions. Our algorithm only requires differentiable data-manifold regularizers and supports box constraints for bounded feature ranges, ensuring the generated counterfactuals remain \emph{actionable}. Finally, experiments on real-world datasets demonstrate that our approach effectively produces sparse, manifold-aligned counterfactual explanations while maintaining proximity to the factual data and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15723v4</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shpresim Sadiku, Moritz Wagner, Sai Ganesh Nagarajan, Sebastian Pokutta</dc:creator>
    </item>
    <item>
      <title>A parameterized linear formulation of the integer hull</title>
      <link>https://arxiv.org/abs/2501.02347</link>
      <description>arXiv:2501.02347v2 Announce Type: replace-cross 
Abstract: Let $A \in \mathbb{Z}^{m \times n}$ be an integer matrix with components bounded by $\Delta$ in absolute value. Cook et al.~(1986) have shown that there exists a universal matrix $B \in \mathbb{Z}^{m' \times n}$ with the following property: For each $b \in \mathbb{Z}^m$, there exists $t \in \mathbb{Z}^{m'}$ such that the integer hull of the polyhedron $P = \{ x \in \mathbb{R}^n \colon Ax \leq b\}$ is described by $P_I = \{ x \in \mathbb{R}^n \colon Bx \leq t\}$. Our \emph{main result} is that $t$ is an \emph{affine} function of $b$ as long as $b$ is from a fixed equivalence class of the lattice $D \cdot \mathbb{Z}^m$. Here $D \in \mathbb{N}$ is a number that depends on $n$ and $\Delta$ only. Furthermore, $D$ as well as the matrix $B$ can be computed in time depending on $\Delta$ and $n$ only. An application of this result is the solution of an open problem posed by Cslovjecsek et al.~(SODA 2024) concerning the complexity of \emph{2-stage-stochastic integer programming} problems. The main tool of our proof is the classical theory of \emph{Gomory-Chv\'atal cutting planes} and the \emph{elementary closure} of rational polyhedra.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02347v2</guid>
      <category>cs.CC</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Friedrich Eisenbrand, Thomas Rothvoss</dc:creator>
    </item>
    <item>
      <title>Soft regression trees: a model variant and a decomposition training algorithm</title>
      <link>https://arxiv.org/abs/2501.05942</link>
      <description>arXiv:2501.05942v2 Announce Type: replace-cross 
Abstract: Decision trees are widely used for classification and regression tasks in a variety of application fields due to their interpretability and good accuracy. During the past decade, growing attention has been devoted to globally optimized decision trees with deterministic or soft splitting rules at branch nodes, which are trained by optimizing the error function over all the tree parameters. In this work, we propose a new variant of soft multivariate regression trees (SRTs) where, for every input vector, the prediction is defined as the linear regression associated to a single leaf node, namely, the leaf node obtained by routing the input vector from the root along the branches with higher probability. SRTs exhibit the conditional computational property, i.e., each prediction depends on a small number of nodes (parameters), and our nonlinear optimization formulation for training them is amenable to decomposition. After showing a universal approximation result for SRTs, we present a decomposition training algorithm including a clustering-based initialization procedure and a heuristic for reassigning the input vectors along the tree. Under mild assumptions, we establish asymptotic convergence guarantees. Experiments on 15 wellknown datasets indicate that our SRTs and decomposition algorithm yield higher accuracy and robustness compared with traditional soft regression trees trained using the nonlinear optimization formulation of Blanquero et al., and a significant reduction in training times as well as a slightly better average accuracy compared with the mixed-integer optimization approach of Bertsimas and Dunn. We also report a comparison with the Random Forest ensemble method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05942v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Antonio Consolo, Edoardo Amaldi, Andrea Manno</dc:creator>
    </item>
    <item>
      <title>High-order Accurate Inference on Manifolds</title>
      <link>https://arxiv.org/abs/2501.06652</link>
      <description>arXiv:2501.06652v2 Announce Type: replace-cross 
Abstract: We present a new framework for statistical inference on Riemannian manifolds that achieves high-order accuracy, addressing the challenges posed by non-Euclidean parameter spaces frequently encountered in modern data science. Our approach leverages a novel and computationally efficient procedure to reach higher-order asymptotic precision. In particular, we develop a bootstrap algorithm on Riemannian manifolds that is both computationally efficient and accurate for hypothesis testing and confidence region construction. Although locational hypothesis testing can be reformulated as a standard Euclidean problem, constructing high-order accurate confidence regions necessitates careful treatment of manifold geometry. To this end, we establish high-order asymptotics under a fixed normal chart centered at the true parameter, thereby enabling precise expansions that incorporate curvature effects. We demonstrate the versatility of this framework across various manifold settings-including spheres, the Stiefel manifold, fixed-rank matrices manifolds, and rank-one tensor manifolds-and, for Euclidean submanifolds, introduce a class of projection-like coordinate charts with strong consistency properties. Finally, numerical studies confirm the practical merits of the proposed procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06652v2</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chengzhu Huang, Anru R. Zhang</dc:creator>
    </item>
  </channel>
</rss>
