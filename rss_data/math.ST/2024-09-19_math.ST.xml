<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Sep 2024 04:00:54 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Two New Families of Local Asymptotically Minimax Lower Bounds in Parameter Estimation</title>
      <link>https://arxiv.org/abs/2409.12491</link>
      <description>arXiv:2409.12491v1 Announce Type: new 
Abstract: We propose two families of asymptotically local minimax lower bounds on parameter estimation performance. The first family of bounds applies to any convex, symmetric loss function that depends solely on the difference between the estimate and the true underlying parameter value (i.e., the estimation error), whereas the second is more specifically oriented to the moments of the estimation error. The proposed bounds are relatively easy to calculate numerically (in the sense that their optimization is over relatively few auxiliary parameters), yet they turn out to be tighter (sometimes significantly so) than previously reported bounds that are associated with similar calculation efforts, across a variety of application examples. In addition to their relative simplicity, they also have the following advantages: (i) Essentially no regularity conditions are required regarding the parametric family of distributions; (ii) The bounds are local (in a sense to be specified); (iii) The bounds provide the correct order of decay as functions of the number of observations, at least in all examples examined; (iv) At least the first family of bounds extends straightforwardly to vector parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12491v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neri Merhav</dc:creator>
    </item>
    <item>
      <title>Mitigating Extremal Risks: A Network-Based Portfolio Strategy</title>
      <link>https://arxiv.org/abs/2409.12208</link>
      <description>arXiv:2409.12208v1 Announce Type: cross 
Abstract: In financial markets marked by inherent volatility, extreme events can result in substantial investor losses. This paper proposes a portfolio strategy designed to mitigate extremal risks. By applying extreme value theory, we evaluate the extremal dependence between stocks and develop a network model reflecting these dependencies. We use a threshold-based approach to construct this complex network and analyze its structural properties. To improve risk diversification, we utilize the concept of the maximum independent set from graph theory to develop suitable portfolio strategies. Since finding the maximum independent set in a given graph is NP-hard, we further partition the network using either sector-based or community-based approaches. Additionally, we use value at risk and expected shortfall as specific risk measures and compare the performance of the proposed portfolios with that of the market portfolio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12208v1</guid>
      <category>q-fin.PM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qian Hui, Tiandong Wang</dc:creator>
    </item>
    <item>
      <title>Neural Networks Generalize on Low Complexity Data</title>
      <link>https://arxiv.org/abs/2409.12446</link>
      <description>arXiv:2409.12446v1 Announce Type: cross 
Abstract: We show that feedforward neural networks with ReLU activation generalize on low complexity data, suitably defined. Given i.i.d. data generated from a simple programming language, the minimum description length (MDL) feedforward neural network which interpolates the data generalizes with high probability. We define this simple programming language, along with a notion of description length of such networks. We provide several examples on basic computational tasks, such as checking primality of a natural number, and more. For primality testing, our theorem shows the following. Suppose that we draw an i.i.d. sample of $\Theta(N^{\delta}\ln N)$ numbers uniformly at random from $1$ to $N$, where $\delta\in (0,1)$. For each number $x_i$, let $y_i = 1$ if $x_i$ is a prime and $0$ if it is not. Then with high probability, the MDL network fitted to this data accurately answers whether a newly drawn number between $1$ and $N$ is a prime or not, with test error $\leq O(N^{-\delta})$. Note that the network is not designed to detect primes; minimum description learning discovers a network which does so.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12446v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourav Chatterjee, Timothy Sudijono</dc:creator>
    </item>
    <item>
      <title>Neymanian inference in randomized experiments</title>
      <link>https://arxiv.org/abs/2409.12498</link>
      <description>arXiv:2409.12498v1 Announce Type: cross 
Abstract: In his seminal work in 1923, Neyman studied the variance estimation problem for the difference-in-means estimator of the average treatment effect in completely randomized experiments. He proposed a variance estimator that is conservative in general and unbiased when treatment effects are homogeneous. While widely used under complete randomization, there is no unique or natural way to extend this estimator to more complex designs. To this end, we show that Neyman's estimator can be alternatively derived in two ways, leading to two novel variance estimation approaches: the imputation approach and the contrast approach. While both approaches recover Neyman's estimator under complete randomization, they yield fundamentally different variance estimators for more general designs. In the imputation approach, the variance is expressed as a function of observed and missing potential outcomes and then estimated by imputing the missing potential outcomes, akin to Fisherian inference. In the contrast approach, the variance is expressed as a function of several unobservable contrasts of potential outcomes and then estimated by exchanging each unobservable contrast with an observable contrast. Unlike the imputation approach, the contrast approach does not require separately estimating the missing potential outcome for each unit. We examine the theoretical properties of both approaches, showing that for a large class of designs, each produces conservative variance estimators that are unbiased in finite samples or asymptotically under homogeneous treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12498v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ambarish Chattopadhyay, Guido W. Imbens</dc:creator>
    </item>
    <item>
      <title>Necessary and sufficient condition for CLT of linear spectral statistics of sample correlation matrices</title>
      <link>https://arxiv.org/abs/2409.12536</link>
      <description>arXiv:2409.12536v1 Announce Type: cross 
Abstract: In this paper, we establish the central limit theorem (CLT) for the linear spectral statistics (LSS) of sample correlation matrix $R$, constructed from a $p\times n$ data matrix $X$ with independent and identically distributed (i.i.d.) entries having mean zero, variance one, and infinite fourth moments in the high-dimensional regime $n/p\rightarrow \phi\in \mathbb{R}_+\backslash \{1\}$. We derive a necessary and sufficient condition for the CLT. More precisely, under the assumption that the identical distribution $\xi$ of the entries in $X$ satisfies $\mathbb{P}(|\xi|&gt;x)\sim l(x)x^{-\alpha}$ when $x\rightarrow \infty$ for $\alpha \in (2,4]$, where $l(x)$ is a slowly varying function, we conclude that: (i). When $\alpha\in(3,4]$, the universal asymptotic normality for the LSS of sample correlation matrix holds, with the same asymptotic mean and variance as in the finite fourth moment scenario; (ii) We identify a necessary and sufficient condition $\lim_{x\rightarrow\infty}x^3\mathbb{P}(|\xi|&gt;x)=0$ for the universal CLT; (iii) We establish a local law for $\alpha \in (2, 4]$. Overall, our proof strategy follows the routine of the matrix resampling, intermediate local law, Green function comparison, and characteristic function estimation. In various parts of the proof, we are required to come up with new approaches and ideas to solve the challenges posed by the special structure of sample correlation matrix. Our results also demonstrate that the symmetry condition is unnecessary for the CLT of LSS for sample correlation matrix, but the tail index $\alpha$ plays a crucial role in determining the asymptotic behaviors of LSS for $\alpha \in (2, 3)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12536v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanpeng Li, Guangming Pan, Jiahui Xie, Wang Zhou</dc:creator>
    </item>
    <item>
      <title>The Central Role of the Loss Function in Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2409.12799</link>
      <description>arXiv:2409.12799v1 Announce Type: cross 
Abstract: This paper illustrates the central role of loss functions in data-driven decision making, providing a comprehensive survey on their influence in cost-sensitive classification (CSC) and reinforcement learning (RL). We demonstrate how different regression loss functions affect the sample efficiency and adaptivity of value-based decision making algorithms. Across multiple settings, we prove that algorithms using the binary cross-entropy loss achieve first-order bounds scaling with the optimal policy's cost and are much more efficient than the commonly used squared loss. Moreover, we prove that distributional algorithms using the maximum likelihood loss achieve second-order bounds scaling with the policy variance and are even sharper than first-order bounds. This in particular proves the benefits of distributional RL. We hope that this paper serves as a guide analyzing decision making algorithms with varying loss functions, and can inspire the reader to seek out better loss functions to improve any decision making algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12799v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaiwen Wang, Nathan Kallus, Wen Sun</dc:creator>
    </item>
    <item>
      <title>Comparison and equality of generalized $\psi$-estimators</title>
      <link>https://arxiv.org/abs/2309.04773</link>
      <description>arXiv:2309.04773v2 Announce Type: replace 
Abstract: We solve the comparison problem for generalized $\psi$-estimators introduced in Barczy and P\'ales (2022). Namely, we derive several necessary and sufficient conditions under which a generalized $\psi$-estimator less than or equal to another $\psi$-estimator for any sample. We also solve the corresponding equality problem for generalized $\psi$-estimators. For applications, we solve the two problems in question for Bajraktarevi\'c-type- and quasi-arithmetic-type estimators. We also apply our results for some known statistical estimators such as for empirical expectiles and Mathieu-type estimators and for solutions of likelihood equations in case of normal, a Beta-type, Gamma, Lomax (Pareto type II), lognormal and Laplace distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04773v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matyas Barczy, Zsolt P\'ales</dc:creator>
    </item>
    <item>
      <title>Subsampling for Big Data Linear Models with Measurement Errors</title>
      <link>https://arxiv.org/abs/2403.04361</link>
      <description>arXiv:2403.04361v2 Announce Type: replace 
Abstract: Subsampling algorithms for various parametric regression models with massive data have been extensively investigated in recent years. However, all existing studies on subsampling heavily rely on clean massive data. In practical applications, the observed covariates may suffer from inaccuracies due to measurement errors. To address the challenge of large datasets with measurement errors, this study explores two subsampling algorithms based on the corrected likelihood approach: the optimal subsampling algorithm utilizing inverse probability weighting and the perturbation subsampling algorithm employing random weighting assuming a perfectly known distribution. Theoretical properties for both algorithms are provided. Numerical simulations and two real-world examples demonstrate the effectiveness of these proposed methods compared to other uncorrected algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04361v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiangshan Ju, Mingqiu Wang, Shengli Zhao</dc:creator>
    </item>
    <item>
      <title>The Asymptotics of Wide Remedians</title>
      <link>https://arxiv.org/abs/2409.09528</link>
      <description>arXiv:2409.09528v3 Announce Type: replace 
Abstract: The remedian uses a $k\times b$ matrix to approximate the median of $n\leq b^{k}$ streaming input values by recursively replacing buffers of $b$ values with their medians, thereby ignoring its $200(\lceil b/2\rceil / b)^{k}%$ most extreme inputs. Rousseeuw &amp; Bassett (1990) and Chao &amp; Lin (1993); Chen &amp; Chen (2005) study the remedian's distribution as $k\rightarrow\infty$ and as $k,b\rightarrow\infty$. The remedian's breakdown point vanishes as $k\rightarrow\infty$, but approaches $(1/2)^{k}$ as $b\rightarrow\infty$. We study the remedian's robust-regime distribution as $b\rightarrow\infty$, deriving a normal distribution for standardized (mean, median, remedian, remedian rank) as $b\rightarrow\infty$, thereby illuminating the remedian's accuracy in approximating the sample median. We derive the asymptotic efficiency of the remedian relative to the mean and the median. Finally, we discuss the estimation of more than one quantile at once, proposing an asymptotic distribution for the random vector that results when we apply remedian estimation in parallel to the components of i.i.d. random vectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09528v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philip T. Labo</dc:creator>
    </item>
    <item>
      <title>An accurate percentile method for parametric inference based on asymptotically biased estimators</title>
      <link>https://arxiv.org/abs/2405.05403</link>
      <description>arXiv:2405.05403v2 Announce Type: replace-cross 
Abstract: Inference methods for computing confidence intervals in parametric settings usually rely on consistent estimators of the parameter of interest. However, it may be computationally and/or analytically burdensome to obtain such estimators in various parametric settings, for example when the data exhibit certain features such as censoring, misclassification errors or outliers. To address these challenges, we propose a simulation-based inferential method, called the implicit bootstrap, that remains valid regardless of the potential asymptotic bias of the estimator on which the method is based. We demonstrate that this method allows for the construction of asymptotically valid percentile confidence intervals of the parameter of interest. Additionally, we show that these confidence intervals can also achieve second-order accuracy. We also show that the method is exact in three instances where the standard bootstrap fails. Using simulation studies, we illustrate the coverage accuracy of the method in three examples where standard parametric bootstrap procedures are computationally intensive and less accurate in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05403v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel Orso, Mucyo Karemera, Maria-Pia Victoria-Feser, St\'ephane Guerrier</dc:creator>
    </item>
    <item>
      <title>High Probability Latency Sequential Change Detection over an Unknown Finite Horizon</title>
      <link>https://arxiv.org/abs/2408.05817</link>
      <description>arXiv:2408.05817v2 Announce Type: replace-cross 
Abstract: A finite horizon variant of the quickest change detection problem is studied, in which the goal is to minimize a delay threshold (latency), under constraints on the probability of false alarm and the probability that the latency is exceeded. In addition, the horizon is not known to the change detector. A variant of the cumulative sum (CuSum) test with a threshold that increasing logarithmically with time is proposed as a candidate solution to the problem. An information-theoretic lower bound on the minimum value of the latency under the constraints is then developed. This lower bound is used to establish certain asymptotic optimality properties of the proposed test in terms of the horizon and the false alarm probability. Some experimental results are given to illustrate the performance of the test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05817v2</guid>
      <category>cs.DS</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 20 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu-Han Huang, Venugopal V. Veeravalli</dc:creator>
    </item>
  </channel>
</rss>
