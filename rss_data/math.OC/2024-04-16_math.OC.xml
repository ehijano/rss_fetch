<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Apr 2024 04:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 17 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Water Puzzle and Marginal Utility Optimization</title>
      <link>https://arxiv.org/abs/2404.10015</link>
      <description>arXiv:2404.10015v1 Announce Type: new 
Abstract: We present a variation of the water puzzle, which is related to a simple model of marginal utility. The problem has an exciting solution and can be extended in several directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10015v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaakov Malinovsky, Isaac M. Sonin</dc:creator>
    </item>
    <item>
      <title>Federated Learning on Riemannian Manifolds with Differential Privacy</title>
      <link>https://arxiv.org/abs/2404.10029</link>
      <description>arXiv:2404.10029v1 Announce Type: new 
Abstract: In recent years, federated learning (FL) has emerged as a prominent paradigm in distributed machine learning. Despite the partial safeguarding of agents' information within FL systems, a malicious adversary can potentially infer sensitive information through various means. In this paper, we propose a generic private FL framework defined on Riemannian manifolds (PriRFed) based on the differential privacy (DP) technique. We analyze the privacy guarantee while establishing the convergence properties. To the best of our knowledge, this is the first federated learning framework on Riemannian manifold with a privacy guarantee and convergence results. Numerical simulations are performed on synthetic and real-world datasets to showcase the efficacy of the proposed PriRFed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10029v1</guid>
      <category>math.OC</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenwei Huang, Wen Huang, Pratik Jawanpuria, Bamdev Mishra</dc:creator>
    </item>
    <item>
      <title>Feature selection in linear SVMs via hard cardinality constraint: a scalable SDP decomposition approach</title>
      <link>https://arxiv.org/abs/2404.10099</link>
      <description>arXiv:2404.10099v1 Announce Type: new 
Abstract: In this paper, we study the embedded feature selection problem in linear Support Vector Machines (SVMs), in which a cardinality constraint is employed, leading to a fully explainable selection model. The problem is NP-hard due to the presence of the cardinality constraint, even though the original linear SVM amounts to a problem solvable in polynomial time. To handle the hard problem, we first introduce two mixed-integer formulations for which novel SDP relaxations are proposed. Exploiting the sparsity pattern of the relaxations, we decompose the problems and obtain equivalent relaxations in a much smaller cone, making the conic approaches scalable. To make the best usage of the decomposed relaxations, we propose heuristics using the information of its optimal solution. Moreover, an exact procedure is proposed by solving a sequence of mixed-integer decomposed SDPs. Numerical results on classical benchmarking datasets are reported, showing the efficiency and effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10099v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Immanuel Bomze, Federico D'Onofrio, Laura Palagi, Bo Peng</dc:creator>
    </item>
    <item>
      <title>Optimizing Traffic Signal Control for Continuous-Flow Intersections: Benchmarking against a State-of-Practice Model</title>
      <link>https://arxiv.org/abs/2404.10215</link>
      <description>arXiv:2404.10215v1 Announce Type: new 
Abstract: Continuous-Flow Intersections (CFI), also known as Displaced Left-Turn (DLT) intersections, aim to improve the efficiency and safety of traffic junctions. A CFI introduces additional sub-intersections upstream of the main intersection to split the left-turn flow from the through movement before it arrives at the main intersection, decreasing the number of conflict points between left-turn and through movements. This study develops and examines a two-step optimization model for CFI traffic signal control design and demonstrates its performance across more than 300 different travel demand scenarios. The proposed model is compared against a benchmark state-of-practice CFI signal control model. Microsimulation results suggest that the proposed model reduces average delay by 17% and average queue length by 32% for a full CFI compared with the benchmark signal control model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10215v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yining Hu, David Rey, Reza Mohajerpoor, Meead Saberi</dc:creator>
    </item>
    <item>
      <title>Urban Water Sprinkler Routing: A Multi-Depot Mixed Capacitated Arc Routing Problem Incorporating Real-Time Demands</title>
      <link>https://arxiv.org/abs/2404.10230</link>
      <description>arXiv:2404.10230v1 Announce Type: new 
Abstract: Fugitive road dust (FRD), as one of the major pollutants in the city, poses great harm to the environment and the physical health of citizens. A common countermeasure adopted by government agencies is employing on-road water trucks (sprinklers) to spray water (sprinkle) on urban streets to reduce the FRD. Currently, the traveling routes of sprinklers are usually planned based on drivers' experience, which may lead low operation efficiency and could not respond to the real-time sprinkling demands. To address these issues, this study formulates the routes planning of sprinklers as a multi-depot mixed capacitated arc routing problem with real-time demands with the aim of minimizing the sprinklers' travel distance. We develop an improved adaptive large neighborhood search (ALNS) algorithm that incorporates a tabu-list and a perturbation mechanism to solve this problem. Furthermore, a problem-specific acceleration mechanism is designed to reduce unnecessary search domains to improve the efficiency of the algorithm. Empirical experiments are conducted based on various scenarios and the results demonstrate that the proposed algorithm generates solutions that are superior or at least comparable to the solutions generated by the traditional ALNS algorithm but with significantly lower computation time. Sensitivity analysis is conducted to explore the effects of relevant parameters on the results. This study is the first to incorporate real-time FRD pollution information, gathered through multiple data sources via IoT technology, into urban sprinkling operations, extending the traditional CARP from a tactical planning to a real-time operational environment. A real-world implementation case is also presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10230v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongtai Yang, Luna Liu, Ke Han, Boyi Lei</dc:creator>
    </item>
    <item>
      <title>Two system transformation data-driven algorithms for linear quadratic mean-field games</title>
      <link>https://arxiv.org/abs/2404.10285</link>
      <description>arXiv:2404.10285v1 Announce Type: new 
Abstract: This paper studies a class of continuous-time linear quadratic (LQ) mean-field game problems. We develop two system transformation data-driven algorithms to approximate the decentralized strategies of the LQ mean-field games. The main feature of the obtained data-driven algorithms is that they eliminate the requirement on all system matrices. First, we transform the original stochastic system into an ordinary differential equation (ODE). Subsequently, we construct some Kronecker product-based matrices by the input/state data of the ODE. By virtue of these matrices, we implement a model-based policy iteration (PI) algorithm and a model-based value iteration (VI) algorithm in a data-driven fashion. In addition, we also demonstrate the convergence of these two data-driven algorithms under some mild conditions. Finally, we illustrate the practicality of our algorithms via two numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10285v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xun Li, Guangchen Wang, Yu Wang, Jie Xiong, Heng Zhang</dc:creator>
    </item>
    <item>
      <title>Identification of Active Subfunctions in Finite-Max Minimisation via a Smooth Reformulation</title>
      <link>https://arxiv.org/abs/2404.10326</link>
      <description>arXiv:2404.10326v1 Announce Type: new 
Abstract: In this work, we consider a nonsmooth minimisation problem in which the objective function can be represented as the maximum of finitely many smooth ``subfunctions''. First, we study a smooth min-max reformulation of the problem. Due to this smoothness, the model provides enhanced capability of exploiting the structure of the problem, when compared to methods that attempt to tackle the nonsmooth problem directly. Then, we present several approaches to identify the set of active subfunctions at a minimiser, all within finitely many iterations of a first order method for solving the smooth model. As is well known, the problem can be equivalently rewritten in terms of these subfunctions, but a key challenge is to identify this set \textit{a priori}. Such an identification is clearly beneficial in an algorithmic sense, since one can apply this knowledge to create an equivalent problem with lower complexity, thus facilitating generally faster convergence. Finally, numerical results comparing the accuracy of each of these approaches are presented, along with the effect they have on reducing the complexity of the original problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10326v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charl Ras, Matthew Tam, Daniel Uteda</dc:creator>
    </item>
    <item>
      <title>Consensus-based algorithms for stochastic optimization problems</title>
      <link>https://arxiv.org/abs/2404.10372</link>
      <description>arXiv:2404.10372v1 Announce Type: new 
Abstract: We address an optimization problem where the cost function is the expectation of a random mapping. To tackle the problem two approaches based on the approximation of the objective function by consensus-based particle optimization methods on the search space are developed. The resulting methods are mathematically analyzed using a mean-field approximation and their connection is established. Several numerical experiments show the validity of the proposed algorithms and investigate their rates of convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10372v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sabrina Bonandin, Michael Herty</dc:creator>
    </item>
    <item>
      <title>Zero-Sum Games for Volterra Integral Equations and Viscosity Solutions of Path-Dependent Hamilton-Jacobi Equations</title>
      <link>https://arxiv.org/abs/2404.10428</link>
      <description>arXiv:2404.10428v1 Announce Type: new 
Abstract: We consider a game, in which the dynamics is described by a non-linear Volterra integral equation of Hammerstein type with a weakly-singular kernel and the goals of the first and second players are, respectively, to minimize and maximize a given cost functional. We propose a way of how the dynamic programming principle can be formalized and the theory of generalized (viscosity) solutions of path-dependent Hamilton--Jacobi equations can be developed in order to prove the existence of the game value, obtain a characterization of the value functional, and construct players' optimal feedback strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10428v1</guid>
      <category>math.OC</category>
      <category>math.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mikhail I. Gomoyunov</dc:creator>
    </item>
    <item>
      <title>Formulations of the continuous set-covering problem on networks: a comparative study</title>
      <link>https://arxiv.org/abs/2404.10467</link>
      <description>arXiv:2404.10467v1 Announce Type: new 
Abstract: We study the continuous set covering problem on networks and propose several new MILP formulations and valid inequalities. In contrast to state-of-the-art formulations, the new formulations only use edges to index installed points, and the formulation sizes are smaller. The covering conditions can be represented as multivariate piecewise linear concave constraints, which we formulate as disjunctive systems. We propose three MILP formulations based on indicator constraint, big-M, and disjunctive programming techniques for modeling the disjunctive system. Finally, we give a classification of new and old formulations, and conduct experiments to compare them computationally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10467v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liding Xu, Claudia D'Ambrosio</dc:creator>
    </item>
    <item>
      <title>Nonlinear kernel-free quadratic hyper-surface support vector machine with 0-1 loss function</title>
      <link>https://arxiv.org/abs/2404.10559</link>
      <description>arXiv:2404.10559v1 Announce Type: new 
Abstract: For the binary classification problem, a novel nonlinear kernel-free quadratic hyper-surface support vector machine with 0-1 loss function (QSSVM$_{0/1}$) is proposed. Specifically, the task of QSSVM$_{0/1}$ is to seek a quadratic separating hyper-surface to divide the samples into two categories. And it has better interpretability than the methods using kernel functions, since each feature of the sample acts both independently and synergistically. By introducing the 0-1 loss function to construct the optimization model makes the model obtain strong sample sparsity. The proximal stationary point of the optimization problem is defined by the proximal operator of the 0-1 loss function, which figures out the problem of non-convex discontinuity of the optimization problem due to the 0-1 loss function. A new iterative algorithm based on the alternating direction method of multipliers (ADMM) framework is designed to solve the optimization problem, which relates to the working set defined by support vectors. The computational complexity and convergence of the algorithm are discussed. Numerical experiments on 4 artificial datasets and 14 benchmark datasets demonstrate that our QSSVM$_{0/1}$ achieves higher classification accuracy, fewer support vectors and less CPU time cost than other state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10559v1</guid>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyang Wu, Zhixia Yang, Junyou Ye</dc:creator>
    </item>
    <item>
      <title>Swarm-Based Trajectory Generation and Optimization for Stress-Aligned 3D Printing</title>
      <link>https://arxiv.org/abs/2404.10686</link>
      <description>arXiv:2404.10686v1 Announce Type: new 
Abstract: In this study, we present a novel swarm-based approach for generating optimized stress-aligned trajectories for 3D printing applications. The method utilizes swarming dynamics to simulate the motion of virtual agents along the stress produced in a loaded part. Agent trajectories are then used as print trajectories. With this approach, the complex global trajectory generation problem is subdivided into a set of sequential and computationally efficient quadratic programs. Through comprehensive evaluations in both simulation and experiments, we compare our method with state-of-the-art approaches. Our results highlight a remarkable improvement in computational efficiency, achieving a 115x faster computation speed than existing methods. This efficiency, combined with the possibility to tune the trajectories spacing to match the deposition process constraints, makes the potential integration of our approach into existing 3D printing processes seamless. Additionally, the open-hole tensile specimen produced on a conventional fused filament fabrication set-up with our algorithm achieve a notable ~10% improvement in specific modulus compared to existing trajectory optimization methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10686v1</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xavier Guidetti, Efe C. Balta, John Lygeros</dc:creator>
    </item>
    <item>
      <title>Biomimicry in Radiation Therapy: Optimizing Patient Scheduling for Improved Treatment Outcomes</title>
      <link>https://arxiv.org/abs/2404.09996</link>
      <description>arXiv:2404.09996v1 Announce Type: cross 
Abstract: In the realm of medical science, the pursuit of enhancing treatment efficacy and patient outcomes continues to drive innovation. This study delves into the integration of biomimicry principles within the domain of Radiation Therapy (RT) to optimize patient scheduling, ultimately aiming to augment treatment results. RT stands as a vital medical technique for eradicating cancer cells and diminishing tumor sizes. Yet, the manual scheduling of patients for RT proves both laborious and intricate. In this research, the focus is on automating patient scheduling for RT through the application of optimization methodologies. Three bio-inspired algorithms are employed for optimization to tackle the complex online stochastic scheduling problem. These algorithms include the Genetic Algorithm (GA), Firefly Optimization (FFO), and Wolf Optimization (WO). These algorithms are harnessed to address the intricate challenges of online stochastic scheduling. Through rigorous evaluation, involving the scrutiny of convergence time, runtime, and objective values, the comparative performance of these algorithms is determined. The results of this study unveil the effectiveness of the applied bio-inspired algorithms in optimizing patient scheduling for RT. Among the algorithms examined, WO emerges as the frontrunner, consistently delivering superior outcomes across various evaluation criteria. The optimization approach showcased in this study holds the potential to streamline processes, reduce manual intervention, and ultimately improve treatment outcomes for patients undergoing RT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09996v1</guid>
      <category>cs.NE</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keshav Kumar K., NVSL Narasimham</dc:creator>
    </item>
    <item>
      <title>Safe Feature Identification Rule for Fused Lasso by An Extra Dual Variable</title>
      <link>https://arxiv.org/abs/2404.10262</link>
      <description>arXiv:2404.10262v1 Announce Type: cross 
Abstract: Fused Lasso was proposed to characterize the sparsity of the coefficients and the sparsity of their successive differences for the linear regression. Due to its wide applications, there are many existing algorithms to solve fused Lasso. However, the computation of this model is time-consuming in high-dimensional data sets. To accelerate the calculation of fused Lasso in high-dimension data sets, we build up the safe feature identification rule by introducing an extra dual variable. With a low computational cost, this rule can eliminate inactive features with zero coefficients and identify adjacent features with same coefficients in the solution. To the best of our knowledge, existing screening rules can not be applied to speed up the computation of fused Lasso and our work is the first one to deal with this problem. To emphasize our rule is a unique result that is capable of identifying adjacent features with same coefficients, we name the result as the safe feature identification rule. Numerical experiments on simulation and real data illustrate the efficiency of the rule, which means this rule can reduce the computational time of fused Lasso. In addition, our rule can be embedded into any efficient algorithm and speed up the computational process of fused Lasso.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10262v1</guid>
      <category>stat.CO</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pan Shang, Huangyue Chen, Lingchen Kong</dc:creator>
    </item>
    <item>
      <title>Optimal complexity solution of space-time finite element systems for state-based parabolic distributed optimal control problems</title>
      <link>https://arxiv.org/abs/2404.10350</link>
      <description>arXiv:2404.10350v1 Announce Type: cross 
Abstract: We consider a distributed optimal control problem subject to a parabolic evolution equation as constraint. The control will be considered in the energy norm of the anisotropic Sobolev space $[H_{0;,0}^{1,1/2}(Q)]^\ast$, such that the state equation of the partial differential equation defines an isomorphism onto $H^{1,1/2}_{0;0,}(Q)$. Thus, we can eliminate the control from the tracking type functional to be minimized, to derive the optimality system in order to determine the state. Since the appearing operator induces an equivalent norm in $H_{0;0,}^{1,1/2}(Q)$, we will replace it by a computable realization of the anisotropic Sobolev norm, using a modified Hilbert transformation. We are then able to link the cost or regularization parameter $\varrho&gt;0$ to the distance of the state and the desired target, solely depending on the regularity of the target. For a conforming space-time finite element discretization, this behavior carries over to the discrete setting, leading to an optimal choice $\varrho = h_x^2$ of the regularization parameter $\varrho$ to the spatial finite element mesh size $h_x$. Using a space-time tensor product mesh, error estimates for the distance of the computable state to the desired target are derived. The main advantage of this new approach is, that applying sparse factorization techniques, a solver of optimal, i.e., almost linear, complexity is proposed and analyzed. The theoretical results are complemented by numerical examples, including discontinuous and less regular targets. Moreover, this approach can be applied also to optimal control problems subject to non-linear state equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10350v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard L\"oscher, Michael Reichelt, Olaf Steinbach</dc:creator>
    </item>
    <item>
      <title>Problem of eigenvalues of stochastic Hamiltonian systems with boundary conditions and Markov chain</title>
      <link>https://arxiv.org/abs/2404.10398</link>
      <description>arXiv:2404.10398v1 Announce Type: cross 
Abstract: In this paper, we study the eigenvalue problem of stochastic Hamiltonian system driven by Brownian motion and Markov chain with boundary conditions and time-dependent coefficients. For any dimensional case, the existence of the first eigenvalue is proven and the corresponding eigenfunctions are constructed by virtue of dual transformation and generalized Riccati equation system. Furthermore, we have more finely characterized the existence of all eigenvalues and constructed the related eigenfunctions for one-dimensional Hamiltonian system. Moreover, the increasing order of these eigenvalues have also been given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10398v1</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tian Chen, Xijun Hu, Zhen Wu</dc:creator>
    </item>
    <item>
      <title>Tropical toric maximum likelihood estimation</title>
      <link>https://arxiv.org/abs/2404.10567</link>
      <description>arXiv:2404.10567v1 Announce Type: cross 
Abstract: We consider toric maximum likelihood estimation over the field of Puiseux series and study critical points of the likelihood function using tropical methods. This problem translates to finding the intersection points of a tropical affine space with a classical linear subspace. We derive new structural results on tropical affine spaces and use these to give a complete and explicit description of the tropical critical points in certain cases. In these cases, we associate tropical critical points to the simplices in a regular triangulation of the polytope giving rise to the toric model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10567v1</guid>
      <category>math.AG</category>
      <category>math.CO</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Boniface, Karel Devriendt, Serkan Ho\c{s}ten</dc:creator>
    </item>
    <item>
      <title>EMC$^2$: Efficient MCMC Negative Sampling for Contrastive Learning with Global Convergence</title>
      <link>https://arxiv.org/abs/2404.10575</link>
      <description>arXiv:2404.10575v1 Announce Type: cross 
Abstract: A key challenge in contrastive learning is to generate negative samples from a large sample set to contrast with positive samples, for learning better encoding of the data. These negative samples often follow a softmax distribution which are dynamically updated during the training process. However, sampling from this distribution is non-trivial due to the high computational costs in computing the partition function. In this paper, we propose an Efficient Markov Chain Monte Carlo negative sampling method for Contrastive learning (EMC$^2$). We follow the global contrastive learning loss as introduced in SogCLR, and propose EMC$^2$ which utilizes an adaptive Metropolis-Hastings subroutine to generate hardness-aware negative samples in an online fashion during the optimization. We prove that EMC$^2$ finds an $\mathcal{O}(1/\sqrt{T})$-stationary point of the global contrastive loss in $T$ iterations. Compared to prior works, EMC$^2$ is the first algorithm that exhibits global convergence (to stationarity) regardless of the choice of batch size while exhibiting low computation and memory cost. Numerical experiments validate that EMC$^2$ is effective with small batch training and achieves comparable or better performance than baseline algorithms. We report the results for pre-training image encoders on STL-10 and Imagenet-100.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10575v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chung-Yiu Yau, Hoi-To Wai, Parameswaran Raman, Soumajyoti Sarkar, Mingyi Hong</dc:creator>
    </item>
    <item>
      <title>Stochastic Linear-quadratic Control Problems with Affine Constraints</title>
      <link>https://arxiv.org/abs/2212.05248</link>
      <description>arXiv:2212.05248v2 Announce Type: replace 
Abstract: This paper investigates the stochastic linear-quadratic control problems with affine constraints, in which both equality and inequality constraints are involved. With the help of the Pontryagin maximum principle and Lagrangian duality theory, the dual problem of original problem is established and the state feedback form of the solution to the optimal control problem is obtained. Under the Slater condition, the equivalence is proved between the solutions to the original problem and the ones of the dual problem, and the KKT condition is also provided for solving original problem. Especially, a new sufficient condition is given for the invertibility assumption, which ensures the uniqueness of the solutions to the dual problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05248v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhun Gou, Nan-jing Huang, Xian-jun Long, Jian-hao Kang</dc:creator>
    </item>
    <item>
      <title>The Critical Node Game</title>
      <link>https://arxiv.org/abs/2303.05961</link>
      <description>arXiv:2303.05961v2 Announce Type: replace 
Abstract: In this work, we introduce a game-theoretic model that assesses the cyber-security risk of cloud networks and informs security experts on the optimal security strategies. Our approach combines game theory, combinatorial optimization, and cyber-security and aims to minimize the unexpected network disruptions caused by malicious cyber-attacks under uncertainty. Methodologically, we introduce the critical node game, a simultaneous and non-cooperative attacker-defender game where each player solves a combinatorial optimization problem parametrized in the variables of the other player. Each player simultaneously commits to a defensive (or attacking) strategy with limited knowledge about the choices of their adversary. We provide a realistic model for the critical node game and propose an algorithm to compute its stable solutions, i.e., its Nash equilibria. Practically, our approach enables security experts to assess the security posture of the cloud network and dynamically adapt the level of cyber-protection deployed on the network. We provide a detailed analysis of a real-world cloud network and demonstrate the efficacy of our approach through extensive computational tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.05961v2</guid>
      <category>math.OC</category>
      <category>cs.CR</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Combinatorial Optimization, 2024</arxiv:journal_reference>
      <dc:creator>Gabriele Dragotto, Amine Boukhtouta, Andrea Lodi, Mehdi Taobane</dc:creator>
    </item>
    <item>
      <title>Limited memory gradient methods for unconstrained optimization</title>
      <link>https://arxiv.org/abs/2308.15145</link>
      <description>arXiv:2308.15145v2 Announce Type: replace 
Abstract: The limited memory steepest descent method (Fletcher, 2012) for unconstrained optimization problems stores a few past gradients to compute multiple stepsizes at once. We review this method and propose new variants. For strictly convex quadratic objective functions, we study the numerical behavior of different techniques to compute new stepsizes. In particular, we introduce a method to improve the use of harmonic Ritz values. We also show the existence of a secant condition associated with LMSD, where the approximating Hessian is projected onto a low-dimensional space. In the general nonlinear case, we propose two new alternatives to Fletcher's method: first, the addition of symmetry constraints to the secant condition valid for the quadratic case; second, a perturbation of the last differences between consecutive gradients, to satisfy multiple secant equations simultaneously. We show that Fletcher's method can also be interpreted from this viewpoint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.15145v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giulia Ferrandi, Michiel E. Hochstenbach</dc:creator>
    </item>
    <item>
      <title>A Novel Gradient Methodology with Economical Objective Function Evaluations for Data Science Applications</title>
      <link>https://arxiv.org/abs/2309.10894</link>
      <description>arXiv:2309.10894v3 Announce Type: replace 
Abstract: Gradient methods are experiencing a growth in methodological and theoretical developments owing to the challenges posed by optimization problems arising in data science. However, such gradient methods face diverging optimality gaps or exploding objective evaluations when applied to optimization problems with realistic properties for data science applications. In this work, we address this gap by developing a generic methodology that economically uses objective function evaluations in a problem-driven manner to prevent optimality gap divergence and avoid explosions in objective evaluations. Our methodology allows for a variety of step size routines and search direction strategies. Furthermore, we develop a particular, novel step size selection methodology that is well-suited to our framework. We show that our specific procedure is highly competitive with standard optimization methods on CUTEst test problems. We then show our specific procedure is highly favorable relative to standard optimization methods on a particularly tough data science problem: learning the parameters in a generalized estimating equation model. Thus, we provide a novel gradient methodology that is better suited to optimization problems from this important class of data science applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10894v3</guid>
      <category>math.OC</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Varner, Vivak Patel</dc:creator>
    </item>
    <item>
      <title>Small-Disturbance Input-to-State Stability of Perturbed Gradient Flows: Applications to LQR Problem</title>
      <link>https://arxiv.org/abs/2310.02930</link>
      <description>arXiv:2310.02930v2 Announce Type: replace 
Abstract: This paper studies the effect of perturbations on the gradient flow of a general nonlinear programming problem, where the perturbation may arise from inaccurate gradient estimation in the setting of data-driven optimization. Under suitable conditions on the objective function, the perturbed gradient flow is shown to be small-disturbance input-to-state stable (ISS), which implies that, in the presence of a small-enough perturbation, the trajectories of the perturbed gradient flow must eventually enter a small neighborhood of the optimum. This work was motivated by the question of robustness of direct methods for the linear quadratic regulator problem, and specifically the analysis of the effect of perturbations caused by gradient estimation or round-off errors in policy optimization. We show small-disturbance ISS for three of the most common optimization algorithms: standard gradient flow, natural gradient flow, and Newton gradient flow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02930v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leilei Cui, Zhong-Ping Jiang, Eduardo D. Sontag</dc:creator>
    </item>
    <item>
      <title>Non-ergodic convergence rate of an inertial accelerated primal-dual algorithm for saddle point problems</title>
      <link>https://arxiv.org/abs/2311.11274</link>
      <description>arXiv:2311.11274v2 Announce Type: replace 
Abstract: In this paper, we design an inertial accelerated primal-dual algorithm to address the convex-concave saddle point problem, which is formulated as $\min_{x}\max_{y} f(x) + \langle Kx, y \rangle - g(y)$. Remarkably, both functions $f$ and $g$ exhibit a composite structure, combining ``nonsmooth'' + ``smooth'' components. Under the assumption of partially strong convexity in the sense that $f$ is convex and $g$ is strongly convex, we introduce a novel inertial accelerated primal-dual algorithm based on Nesterov's extrapolation. This algorithm can be reduced to two classical accelerated forward-backward methods for unconstrained optimization problem. We show that the proposed algorithm achieves a non-ergodic $\mathcal{O}(1/k^2)$ convergence rate, where $k$ represents the number of iterations. Several numerical experiments validate the efficiency of our proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11274v2</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>X. He, N. J. Huang, Y. P. Fang</dc:creator>
    </item>
    <item>
      <title>Discrete Consensus-Based Optimization</title>
      <link>https://arxiv.org/abs/2403.03430</link>
      <description>arXiv:2403.03430v3 Announce Type: replace 
Abstract: We propose Discrete Consensus-Based Optimization (DCBO), a fully discrete version of the Consensus-Based Optimization (CBO) framework. DCBO is a multi-agent method for the global optimization of possibly non-convex and non-differentiable functions. It aligns with the CBO paradigm, which promotes a consensus among agents towards a global optimum through simple stochastic dynamics amenable to rigorous mathematical analysis. Despite the promises, there has been a gap between the analysis of CBO and the actual behavior of the agents from its time-discrete implementation, as the former has focused on the system of continuous stochastic differential equations defining the model or its mean-field approximation. In particular, direct analysis of CBO-type algorithms with heterogeneous stochasticity is very challenging. DCBO distinguishes itself from these approaches in the sense that it has no continuous counterpart, thanks to the replacement of the "softmin" operator with the "hardmin" one, which is inherently discrete. Yet, it maintains the operational principles of CBO and allows for rich mathematical analysis. We present conditions, independent of the number of agents, for achieving a consensus or convergence and study the circumstances under which global optimization occurs. We test DCBO on a large number of benchmark functions to show its merits. We also demonstrate that DCBO is applicable to a diverse range of real-world problems, including neural network training, compressed sensing, and portfolio optimization, with competitive performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03430v3</guid>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junhyeok Byeon, Seung-Yeal Ha, Joong-Ho Won</dc:creator>
    </item>
    <item>
      <title>Exponential Stability of Parametric Optimization-Based Controllers via Lur'e Contractivity</title>
      <link>https://arxiv.org/abs/2403.08159</link>
      <description>arXiv:2403.08159v2 Announce Type: replace 
Abstract: In this letter, we investigate sufficient conditions for the exponential stability of LTI systems driven by controllers derived from parametric optimization problems. Our primary focus is on parametric projection controllers, namely parametric programs whose objective function is the squared distance to a nominal controller. Leveraging the virtual system method of analysis and a novel contractivity result for Lur'e systems, we establish a sufficient LMI condition for the exponential stability of an LTI system with a parametric projection-based controller. Separately, we prove additional results for single-integrator systems. Finally, we apply our results to state-dependent saturated control systems and control barrier function-based control and provide numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08159v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Davydov, Francesco Bullo</dc:creator>
    </item>
    <item>
      <title>Equitable Routing - Rethinking the Multiple Traveling Salesman Problem</title>
      <link>https://arxiv.org/abs/2404.08157</link>
      <description>arXiv:2404.08157v3 Announce Type: replace 
Abstract: The Multiple Traveling Salesman Problem (MTSP) with a single depot is a generalization of the well-known Traveling Salesman Problem (TSP) that involves an additional parameter, namely, the number of salesmen. In the MTSP, several salesmen at the depot need to visit a set of interconnected targets, such that each target is visited precisely once by at most one salesman while minimizing the total length of their tours. An equally important variant of the MTSP, the min-max MTSP, aims to distribute the workload (length of the individual tours) among salesmen by requiring the longest tour of all the salesmen to be as short as possible, i.e., minimizing the maximum tour length among all salesmen. The min-max MTSP appears in real-life applications to ensure a good balance of workloads for the salesmen. It is known in the literature that the min-max MTSP is notoriously difficult to solve to optimality due to the poor lower bounds its linear relaxations provide. In this paper, we formulate two novel parametric variants of the MTSP called the "fair-MTSP". One variant is formulated as a Mixed-Integer Second Order Cone Program (MISOCP), and the other as a Mixed Integer Linear Program (MILP). Both focus on enforcing the workloads for the salesmen to be equitable, i.e., the distribution of tour lengths for the salesmen to be fair while minimizing the total cost of their tours. We present algorithms to solve the two variants of the fair-MTSP to global optimality and computational results on benchmark and real-world test instances that make a case for fair-MTSP as a viable alternative to the min-max MTSP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08157v3</guid>
      <category>math.OC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhay Singh Bhadoriya, Deepjyoti Deka, Kaarthik Sundar</dc:creator>
    </item>
    <item>
      <title>A distributed framework for linear adaptive MPC</title>
      <link>https://arxiv.org/abs/2109.05777</link>
      <description>arXiv:2109.05777v2 Announce Type: replace-cross 
Abstract: Adaptive model predictive control (MPC) robustly ensures safety while reducing uncertainty during operation. In this paper, a distributed version is proposed to deal with network systems featuring multiple agents and limited communication. To solve the problem in a distributed manner, structure is imposed on the control design ingredients without sacrificing performance. Decentralized and distributed adaptation schemes that allow for a reduction of the uncertainty online compatibly with the network topology are also proposed. The algorithm ensures robust constraint satisfaction, recursive feasibility and finite gain $\ell_2$ stability, and yields lower closed-loop cost compared to robust distributed MPC in simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.05777v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/CDC45484.2021.9683290.</arxiv:DOI>
      <arxiv:journal_reference>Proc. 60th IEEE Conference on Decision and Control, Austin, TX, USA, 2021, pp. 460-465</arxiv:journal_reference>
      <dc:creator>Anilkumar Parsi, Ahmed Aboudonia, Andrea Iannelli, John Lygeros, Roy S. Smith</dc:creator>
    </item>
    <item>
      <title>Regularized Barzilai-Borwein method</title>
      <link>https://arxiv.org/abs/2211.06624</link>
      <description>arXiv:2211.06624v4 Announce Type: replace-cross 
Abstract: We develop a novel stepsize based on \BB method for solving some challenging optimization problems efficiently, named regularized \BB (RBB) stepsize. We indicate that RBB stepsize is the close solution to a $\ell_{2}^{2}$-regularized least squares problem. When the regularized item vanishes, the RBB stepsize reduces to the original \BB stepsize. RBB stepsize includes a class of valid stepsizes, such as another version of \BB stepsize. The global convergence of the corresponding RBB algorithm is proved in solving convex quadratic optimization problems. One scheme for adaptively generating regularization parameters was proposed, named adaptive two-step parameter. An enhanced RBB stepsize is used for solving quadratic and general optimization problems more efficiently. RBB stepsize could overcome the instability of BB stepsize in many ill-conditioned optimization problems. Moreover, RBB stepsize is more robust than BB stepsize in numerical experiments. Numerical examples show the advantage of using the proposed stepsize to solve some challenging optimization problems vividly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.06624v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Congpei An, Xin Xu</dc:creator>
    </item>
    <item>
      <title>Learning-Based Optimal Control with Performance Guarantees for Unknown Systems with Latent States</title>
      <link>https://arxiv.org/abs/2303.17963</link>
      <description>arXiv:2303.17963v3 Announce Type: replace-cross 
Abstract: As control engineering methods are applied to increasingly complex systems, data-driven approaches for system identification appear as a promising alternative to physics-based modeling. While the Bayesian approaches prevalent for safety-critical applications usually rely on the availability of state measurements, the states of a complex system are often not directly measurable. It may then be necessary to jointly estimate the dynamics and the latent state, making the quantification of uncertainties and the design of controllers with formal performance guarantees considerably more challenging. This paper proposes a novel method for the computation of an optimal input trajectory for unknown nonlinear systems with latent states based on a combination of particle Markov chain Monte Carlo methods and scenario theory. Probabilistic performance guarantees are derived for the resulting input trajectory, and an approach to validate the performance of arbitrary control laws is presented. The effectiveness of the proposed method is demonstrated in a numerical simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.17963v3</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Lefringhausen, Supitsana Srithasan, Armin Lederer, Sandra Hirche</dc:creator>
    </item>
    <item>
      <title>Universal Online Learning with Gradient Variations: A Multi-layer Online Ensemble Approach</title>
      <link>https://arxiv.org/abs/2307.08360</link>
      <description>arXiv:2307.08360v3 Announce Type: replace-cross 
Abstract: In this paper, we propose an online convex optimization approach with two different levels of adaptivity. On a higher level, our approach is agnostic to the unknown types and curvatures of the online functions, while at a lower level, it can exploit the unknown niceness of the environments and attain problem-dependent guarantees. Specifically, we obtain $\mathcal{O}(\log V_T)$, $\mathcal{O}(d \log V_T)$ and $\hat{\mathcal{O}}(\sqrt{V_T})$ regret bounds for strongly convex, exp-concave and convex loss functions, respectively, where $d$ is the dimension, $V_T$ denotes problem-dependent gradient variations and the $\hat{\mathcal{O}}(\cdot)$-notation omits $\log V_T$ factors. Our result not only safeguards the worst-case guarantees but also directly implies the small-loss bounds in analysis. Moreover, when applied to adversarial/stochastic convex optimization and game theory problems, our result enhances the existing universal guarantees. Our approach is based on a multi-layer online ensemble framework incorporating novel ingredients, including a carefully designed optimism for unifying diverse function types and cascaded corrections for algorithmic stability. Notably, despite its multi-layer structure, our algorithm necessitates only one gradient query per round, making it favorable when the gradient evaluation is time-consuming. This is facilitated by a novel regret decomposition equipped with carefully designed surrogate losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.08360v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu-Hu Yan, Peng Zhao, Zhi-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>The Marginal Value of Momentum for Small Learning Rate SGD</title>
      <link>https://arxiv.org/abs/2307.15196</link>
      <description>arXiv:2307.15196v2 Announce Type: replace-cross 
Abstract: Momentum is known to accelerate the convergence of gradient descent in strongly convex settings without stochastic gradient noise. In stochastic optimization, such as training neural networks, folklore suggests that momentum may help deep learning optimization by reducing the variance of the stochastic gradient update, but previous theoretical analyses do not find momentum to offer any provable acceleration. Theoretical results in this paper clarify the role of momentum in stochastic settings where the learning rate is small and gradient noise is the dominant source of instability, suggesting that SGD with and without momentum behave similarly in the short and long time horizons. Experiments show that momentum indeed has limited benefits for both optimization and generalization in practical training regimes where the optimal learning rate is not very large, including small- to medium-batch training from scratch on ImageNet and fine-tuning language models on downstream tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15196v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runzhe Wang, Sadhika Malladi, Tianhao Wang, Kaifeng Lyu, Zhiyuan Li</dc:creator>
    </item>
    <item>
      <title>Reconstructing the kinetic chemotaxis kernel using macroscopic data: well-posedness and ill-posedness</title>
      <link>https://arxiv.org/abs/2309.05004</link>
      <description>arXiv:2309.05004v2 Announce Type: replace-cross 
Abstract: Bacterial motion is guided by external stimuli (chemotaxis), and the motion described on the mesoscopic scale is uniquely determined by a parameter $K$ that models velocity change response from the bacteria. This parameter is termed chemotaxis kernel. In a practical setting, experimental data was collected to infer this kernel. In this article, a PDE-constrained optimization framework is deployed to perform this reconstruction using velocity-averaged, localized data taken in the interior of the domain. The problem can be well-posed or ill-posed depending on the data preparation and the experimental setup. In particular, we propose one specific design that guarantees numerical reconstructability and local convergence. This design is adapted to the discretization of $K$ in space and decouples the reconstruction of local values of $K$ into smaller cell problems, opening up parallelization opportunities. Numerical evidences support the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.05004v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <category>q-bio.CB</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kathrin Hellmuth, Christian Klingenberg, Qin Li, Min Tang</dc:creator>
    </item>
    <item>
      <title>LatticeML: A data-driven application for predicting the effective Young Modulus of high temperature graph based architected materials</title>
      <link>https://arxiv.org/abs/2404.09470</link>
      <description>arXiv:2404.09470v2 Announce Type: replace-cross 
Abstract: Architected materials with their unique topology and geometry offer the potential to modify physical and mechanical properties. Machine learning can accelerate the design and optimization of these materials by identifying optimal designs and forecasting performance. This work presents LatticeML, a data-driven application for predicting the effective Young's Modulus of high-temperature graph-based architected materials. The study considers eleven graph-based lattice structures with two high-temperature alloys, Ti-6Al-4V and Inconel 625. Finite element simulations were used to compute the effective Young's Modulus of the 2x2x2 unit cell configurations. A machine learning framework was developed to predict Young's Modulus, involving data collection, preprocessing, implementation of regression models, and deployment of the best-performing model. Five supervised learning algorithms were evaluated, with the XGBoost Regressor achieving the highest accuracy (MSE = 2.7993, MAE = 1.1521, R-squared = 0.9875). The application uses the Streamlit framework to create an interactive web interface, allowing users to input material and geometric parameters and obtain predicted Young's Modulus values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09470v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>math.OC</category>
      <category>physics.app-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akshansh Mishra</dc:creator>
    </item>
  </channel>
</rss>
