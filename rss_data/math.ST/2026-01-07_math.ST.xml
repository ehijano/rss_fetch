<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 07 Jan 2026 05:00:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Decision-Theoretic Robustness for Network Models</title>
      <link>https://arxiv.org/abs/2601.02811</link>
      <description>arXiv:2601.02811v1 Announce Type: new 
Abstract: Bayesian network models (Erdos Renyi, stochastic block models, random dot product graphs, graphons) are widely used in neuroscience, epidemiology, and the social sciences, yet real networks are sparse, heterogeneous, and exhibit higher-order dependence. How stable are network-based decisions, model selection, and policy recommendations to small model misspecification? We study local decision-theoretic robustness by allowing the posterior to vary within a small Kullback-Leibler neighborhood and choosing actions that minimize worst-case posterior expected loss. Exploiting low-dimensional functionals available under exchangeability, we (i) adapt decision-theoretic robustness to exchangeable graphs via graphon limits and derive sharp small-radius expansions of robust posterior risk; under squared loss the leading inflation is controlled by the posterior variance of the loss, and for robustness indices that diverge at percolation/fragmentation thresholds we obtain a universal critical exponent describing the explosion of decision uncertainty near criticality. (ii) Develop a nonparametric minimax theory for robust model selection between sparse Erdos-Renyi and block models, showing-via robustness error exponents-that no Bayesian or frequentist method can uniformly improve upon the decision-theoretic limits over configuration models and sparse graphon classes for percolation-type functionals. (iii) Propose a practical algorithm based on entropic tilting of posterior or variational samples, and demonstrate it on functional brain connectivity and Karnataka village social networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02811v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marios Papamichalis, Regina Ruane, Simon Lunagomez, Swati Chandna</dc:creator>
    </item>
    <item>
      <title>Collapsed Structured Block Models for Community Detection in Complex Networks</title>
      <link>https://arxiv.org/abs/2601.02828</link>
      <description>arXiv:2601.02828v1 Announce Type: new 
Abstract: Community detection seeks to recover mesoscopic structure from network data that may be binary, count-valued, signed, directed, weighted, or multilayer. The stochastic block model (SBM) explains such structure by positing a latent partition of nodes and block-specific edge distributions. In Bayesian SBMs, standard MCMC alternates between updating the partition and sampling block parameters, which can hinder mixing and complicate principled comparison across different partitions and numbers of communities. We develop a collapsed Bayesian SBM framework in which block-specific nuisance parameters are analytically integrated out under conjugate priors, so the marginal likelihood p(Y|z) depends only on the partition z and blockwise sufficient statistics. This yields fast local Gibbs/Metropolis updates based on ratios of closed-form integrated likelihoods and provides evidence-based complexity control that discourages gratuitous over-partitioning. We derive exact collapsed marginals for the most common SBM edge types-Beta-Bernoulli (binary), Gamma-Poisson (counts), and Normal-Inverse-Gamma (Gaussian weights)-and we extend collapsing to gap-constrained SBMs via truncated conjugate priors that enforce explicit upper bounds on between-community connectivity. We further show that the same collapsed strategy supports directed SBMs that model reciprocity through dyad states, signed SBMs via categorical block models, and multiplex SBMs where multiple layers contribute additive evidence for a shared partition. Across synthetic benchmarks and real networks (including email communication, hospital contact counts, and citation graphs), collapsed inference produces accurate partitions and interpretable posterior block summaries of within- and between-community interaction strengths while remaining computationally simple and modular.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02828v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marios Papamichalis, Regina Ruane</dc:creator>
    </item>
    <item>
      <title>A novel finite-sample testing procedure for composite null hypotheses via pointwise rejection</title>
      <link>https://arxiv.org/abs/2601.02529</link>
      <description>arXiv:2601.02529v1 Announce Type: cross 
Abstract: We propose a novel finite-sample procedure for testing composite null hypotheses. Traditional likelihood ratio tests based on asymptotic $\chi^2$ approximations often exhibit substantial bias in small samples. Our procedure rejects the composite null hypothesis $H_0: \theta \in \Theta_0$ if the simple null hypothesis $H_0: \theta = \theta_t$ is rejected for every $\theta_t$ in the null region $\Theta_0$, using an inflated significance level. We derive formulas that determine this inflated level so that the overall test approximately maintains the desired significance level even with small samples. Whereas the traditional likelihood ratio test applies when the null region is defined solely by equality constraints--that is, when it forms a manifold without boundary--the proposed approach extends to null hypotheses defined by both equality and inequality constraints. In addition, it accommodates null hypotheses expressed as unions of several component regions and can be applied to models involving nuisance parameters. Through several examples featuring nonstandard composite null hypotheses, we demonstrate numerically that the proposed test achieves accurate inference, exhibiting only a small gap between the actual and nominal significance levels for both small and large samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02529v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Joonha Park, Ming Wang</dc:creator>
    </item>
    <item>
      <title>Varadhan Functions, Variances, and Means on Compact Riemannian Manifolds</title>
      <link>https://arxiv.org/abs/2601.02832</link>
      <description>arXiv:2601.02832v1 Announce Type: cross 
Abstract: Motivated by Varadhan's theorem, we introduce Varadhan functions, variances, and means on compact Riemannian manifolds as smooth approximations to their Fr\'echet counterparts. Given independent and identically distributed samples, we prove uniform laws of large numbers for their empirical versions. Furthermore, we prove central limit theorems for Varadhan functions and variances for each fixed $t\ge0$, and for Varadhan means for each fixed $t&gt;0$. By studying small time asymptotics of gradients and Hessians of Varadhan functions, we build a strong connection to the central limit theorem for Fr\'echet means, without assumptions on the geometry of the cut locus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.02832v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yueqi Cao</dc:creator>
    </item>
    <item>
      <title>Using prior information to boost power in correlation structure support recovery</title>
      <link>https://arxiv.org/abs/2111.11278</link>
      <description>arXiv:2111.11278v2 Announce Type: replace-cross 
Abstract: Hypothesis testing of structure in correlation and covariance matrices is of broad interest in many application areas. In high dimensions and/or small to moderate sample sizes, high error rates in testing is a substantial concern. This article focuses on increasing power through a frequentist assisted by Bayes (FAB) procedure. This FAB approach boosts power by including prior information on the correlation parameters. In particular, we suppose there is one of two sources of prior information: (i) a prior dataset that is distinct from the current data but related enough that it may contain valuable information about the correlation structure in the current data; and (ii) knowledge about a tendency for the correlations in different parameters to be similar so that it is appropriate to consider a hierarchical model. When the prior information is relevant, the proposed FAB approach can have significant gains in power. A divide-and-conquer algorithm is developed to reduce computational complexity in massive testing dimensions. We show improvements in power for detecting correlated gene pairs in genomic studies while maintaining control of Type I error or false discover rate (FDR).</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.11278v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyang Ding, David Dunson</dc:creator>
    </item>
    <item>
      <title>A Method For Bounding Tail Probabilities</title>
      <link>https://arxiv.org/abs/2402.13662</link>
      <description>arXiv:2402.13662v3 Announce Type: replace-cross 
Abstract: We present a method for upper and lower bounding the right and the left tail probabilities of continuous random variables (RVs). For the right tail probability of RV $X$ with probability density function $f (x)$, this method requires first setting a continuous, positive, and strictly decreasing function $g (x)$ such that $-f (x)/g' (x)$ is a decreasing and increasing function, $\forall x&gt;x_0$, which results in upper and lower bounds, respectively, given in the form $-f (x) g (x)/g' (x)$, $\forall x&gt;x_0$, where $x_0$ is some point. Similarly, for the upper and lower bounds on the left tail probability of $X$, this method requires first setting a continuous, positive, and strictly increasing function $g (x)$ such that $f (x)/g' (x)$ is an increasing and decreasing function, $\forall x&lt;x_0$, which results in upper and lower bounds, respectively, given in the form $f (x) g (x)/g' (x)$, $\forall x&lt;x_0$. We provide some examples of good candidates for the function $g (x)$. We also establish connections between the new bounds and Markov's inequality and Chernoff's bound. In addition, we provide an iterative method for obtaining ever tighter lower and upper bounds, under certain conditions. As an application, we use the proposed method to derive a novel closed-form asymptotic expression of the converse bound on the capacity of the additive white Gaussian noise (AWGN) channel in the finite-blocklength regime, which is tighter than the closed-form asymptotic expression by Polyanskiy-Poor-Verd\'u. Finally, we provide numerical examples where we show the tightness of the bounds obtained by the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13662v3</guid>
      <category>math.PR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2026.3650974</arxiv:DOI>
      <arxiv:journal_reference>IEEE Access, 2026</arxiv:journal_reference>
      <dc:creator>Nikola Zlatanov</dc:creator>
    </item>
    <item>
      <title>Semiparametric fiducial inference for Cox models</title>
      <link>https://arxiv.org/abs/2404.18779</link>
      <description>arXiv:2404.18779v2 Announce Type: replace-cross 
Abstract: R. A. Fisher introduced the concept of fiducial as a potential replacement for the Bayesian posterior distribution in the 1930s. During the past century, fiducial approaches have been explored in various parametric and nonparametric settings. However, to the best of our knowledge, no fiducial inference has been developed in the realm of semiparametric statistics. In this paper, we propose a novel fiducial approach for semiparametric models. To streamline our presentation, we use the Cox proportional hazards model, which is the most popular model for the analysis of survival data, as a running example. Other models and extensions are also discussed. In our experiments, we find our method to perform well especially in situations when the maximum likelihood estimator fails.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18779v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Cui, Jan Hannig, Paul Edlefsen</dc:creator>
    </item>
    <item>
      <title>Source-Optimal Training is Transfer-Suboptimal</title>
      <link>https://arxiv.org/abs/2511.08401</link>
      <description>arXiv:2511.08401v4 Announce Type: replace-cross 
Abstract: We prove that training a source model optimally for its own task is generically suboptimal when the objective is downstream transfer. We study the source-side optimization problem in L2-SP ridge regression and show a fundamental mismatch between the source-optimal and transfer-optimal source regularization: outside of a measure-zero set, $\tau_0^* \neq \tau_S^*$. We characterize the transfer-optimal source penalty $\tau_0^*$ as a function of task alignment and identify an alignment-dependent reversal: with imperfect alignment ($0&lt;\rho&lt;1$), transfer benefits from stronger source regularization, while in super-aligned regimes ($\rho&gt;1$), transfer benefits from weaker regularization. Additionally, in isotropic settings, the decision of whether transfer helps is independent of the target sample size and noise, depending only on task alignment and source characteristics. We verify the linear predictions in a synthetic ridge regression experiment, and we present experiments on MNIST, CIFAR-10, and 20 Newsgroups as evidence that the source-optimal versus transfer-optimal mismatch persists in standard nonlinear transfer learning pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08401v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>C. Evans Hedges</dc:creator>
    </item>
    <item>
      <title>Exact inference via quasi-conjugacy in two-parameter Poisson-Dirichlet hidden Markov models</title>
      <link>https://arxiv.org/abs/2512.22098</link>
      <description>arXiv:2512.22098v2 Announce Type: replace-cross 
Abstract: We introduce a nonparametric model for time-evolving, unobserved probability distributions from discrete-time data consisting of unlabelled partitions. The latent process is a two-parameter Poisson-Dirichlet diffusion, and observations arise via exchangeable sampling. Applications include social and genetic data where only aggregate clustering summaries are observed. To address the intractable likelihood, we develop a tractable inferential framework that avoids label enumeration and direct simulation of the latent state. We exploit a duality between the diffusion and a pure-death process on partitions, together with coagulation operators that encode the effect of new data. These yield closed-form, recursive updates for forward and backward inference. We compute exact posterior distributions of the latent state at arbitrary times and predictive distributions of future or interpolated partitions. This enables online and offline inference and forecasting with full uncertainty quantification, bypassing MCMC and sequential Monte Carlo. Compared to particle filtering, our method achieves higher accuracy, lower variance, and substantial computational gains. We illustrate the methodology with synthetic experiments and a social network application, recovering interpretable patterns in time-varying heterozygosity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22098v2</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>q-bio.PE</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Dalla Pria, Matteo Ruggiero, Dario Span\`o</dc:creator>
    </item>
    <item>
      <title>A Novel Multiple Imputation Approach For Parameter Estimation in Observation-Driven Time Series Models With Missing Data</title>
      <link>https://arxiv.org/abs/2601.01259</link>
      <description>arXiv:2601.01259v2 Announce Type: replace-cross 
Abstract: Handling missing data in time series is a complex problem due to the presence of temporal dependence. General-purpose imputation methods, while widely used, often distort key statistical properties of the data, such as variance and dependence structure, leading to biased estimation and misleading inference. These issues become more pronounced in models that explicitly rely on capturing serial dependence, as standard imputation techniques fail to preserve the underlying dynamics. This paper proposes a novel multiple imputation method specifically designed for parameter estimation in observation-driven models (ODM). The approach takes advantage of the iterative nature of the systematic component in ODM to propagate the dependence structure through missing data, minimizing its impact on estimation. Unlike traditional imputation techniques, the proposed method accommodates continuous, discrete, and mixed-type data while preserving key distributional and dependence properties. We evaluate its performance through Monte Carlo simulations in the context of GARMA models, considering time series with up to 70\% missing data. An application to the proportion of stocked energy stored in South Brazil further demonstrates its practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01259v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 07 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guilherme Pumi, Taiane Schaedler Prass, Douglas Krauthein Verdum</dc:creator>
    </item>
  </channel>
</rss>
