<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 05 Nov 2024 04:09:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Segmentation algorithms and modeling of recurrent bursting events in neuronal and glial time series</title>
      <link>https://arxiv.org/abs/2411.00545</link>
      <description>arXiv:2411.00545v1 Announce Type: new 
Abstract: Long-time series of neuronal recordings are resulting from the activity of connected neuronal networks. Yet how neuronal properties can be extracted remains empirical. We review here the data analysis based on network models to recover physiological parameters from electrophysiological and calcium recordings in neurons and astrocytes. After, we present the recording techniques and activation events, such as burst and interburst and Up and Down states. We then describe time-serie segmentation methods developed to detect and to segment these events. To interpret the statistics extracted from time series, we present computational models of neuronal populations based on synaptic short-term plasticity and After hyperpolarization. We discuss how these models are calibrated so that they can reproduce the statistics observed in the experimental time series. They serve to extract specific parameters by comparing numerical and experimental statistical moment or entire distributions. Finally, we discus cases where calibrated models are used to predict the selective impact of some parameters on the circuit behavior, properties that would otherwise be difficult to dissect experimentally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00545v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lou Zonca, Elena Dossi, Nathalie Rouach, D. Holcman</dc:creator>
    </item>
    <item>
      <title>Comment on Lubineau et al. (2023) 'Does word flickering improve reading? Negative evidence from four experiments using low and high frequencies'</title>
      <link>https://arxiv.org/abs/2411.00019</link>
      <description>arXiv:2411.00019v1 Announce Type: cross 
Abstract: In 2023, Lubineau et al. published an article [1] detailing several experiments carried out with dyslexic readers. These authors attempted to measure the change in reading performance under different reading conditions using flickering devices. Beyond the low-frequency systems which have nevertheless shown their interest for some cases, we restrict here our response to the high-frequency systems, i.e. electronically controlled glasses (Lexilens) and lamps, designed and built upon recent work by Le Floch and Ropars [2]. Lubineau et al. found no significant change in reading performance at either low or high frequencies and concluded that these devices provide no or minor benefits. Unfortunately, experimental misunderstandings and some methodological issues invalidate namely the main conclusion concerning the high frequency systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00019v1</guid>
      <category>physics.soc-ph</category>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Michael Kodochian</dc:creator>
    </item>
    <item>
      <title>Spike-Adding Mechanisms in a Three-Timescale Fast-Slow System: Insights from the FitzHugh-Nagumo Model with Periodic Forcing</title>
      <link>https://arxiv.org/abs/2411.00152</link>
      <description>arXiv:2411.00152v1 Announce Type: cross 
Abstract: In this work, we investigate the spike-adding mechanism in a class of three-dimensional fast-slow systems with three distinct timescales, inspired by the FitzHugh-Nagumo (FHN) model driven by periodic input. First, we numerically generate a bifurcation diagram for the FHN model by varying the frequency and amplitude of the input, revealing that as the frequency decreases and the amplitude increases, the number of spikes within each burst grows. Next, we apply methods from geometric singular perturbation theory to compute critical and super-critical manifolds of the fast-slow system. We use them to characterize the emergence of new burst-spikes in the FHN model, when the periodic forcing resembles a low frequency-band brain rhythm. We then describe how the uncovered spike-adding mechanism defines the boundaries that separate regions with different spike counts in the parameter space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00152v1</guid>
      <category>math.DS</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pake Melland, Rodica Curtu, Zahra Aminzare</dc:creator>
    </item>
    <item>
      <title>Understanding the Limits of Vision Language Models Through the Lens of the Binding Problem</title>
      <link>https://arxiv.org/abs/2411.00238</link>
      <description>arXiv:2411.00238v1 Announce Type: cross 
Abstract: Recent work has documented striking heterogeneity in the performance of state-of-the-art vision language models (VLMs), including both multimodal language models and text-to-image models. These models are able to describe and generate a diverse array of complex, naturalistic images, yet they exhibit surprising failures on basic multi-object reasoning tasks -- such as counting, localization, and simple forms of visual analogy -- that humans perform with near perfect accuracy. To better understand this puzzling pattern of successes and failures, we turn to theoretical accounts of the binding problem in cognitive science and neuroscience, a fundamental problem that arises when a shared set of representational resources must be used to represent distinct entities (e.g., to represent multiple objects in an image), necessitating the use of serial processing to avoid interference. We find that many of the puzzling failures of state-of-the-art VLMs can be explained as arising due to the binding problem, and that these failure modes are strikingly similar to the limitations exhibited by rapid, feedforward processing in the human brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00238v1</guid>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Declan Campbell, Sunayana Rane, Tyler Giallanza, Nicol\`o De Sabbata, Kia Ghods, Amogh Joshi, Alexander Ku, Steven M. Frankland, Thomas L. Griffiths, Jonathan D. Cohen, Taylor W. Webb</dc:creator>
    </item>
    <item>
      <title>Unveiling Normative Trajectories of Lifespan Brain Maturation Using Quantitative MRI</title>
      <link>https://arxiv.org/abs/2411.00661</link>
      <description>arXiv:2411.00661v1 Announce Type: cross 
Abstract: Background: Brain maturation and aging involve significant microstructural changes, resulting in functional and cognitive alterations. Quantitative MRI (qMRI) can measure this evolution, distinguishing the physiological effects of normal aging from pathological deviations.
  Methods: We conducted a multicentre study using qMRI metrics (R1, R2*, and Quantitative Susceptibility Mapping) to model age trajectories across brain structures, including tractography-based white matter bundles (TWMB), superficial white matter (SWM), and cortical grey matter (CGM). MRI data from 537 healthy subjects, aged 8 to 79 years, were harmonized using two independent methods. We modeled age trajectories and performed regional analyses to capture maturation patterns and aging effects across the lifespan.
  Findings: Our findings revealed a distinct brain maturation gradient, with early qMRI peak values in TWMB, followed by SWM, and culminating in CGM regions. This gradient was observed as a posterior-to-anterior maturation pattern in the cortex and an inferior-to-superior maturation pattern in white matter tracts. R1 demonstrated the most robust age trajectories, while R2* and susceptibility exhibited greater variability and different patterns. The normative modeling framework confirmed the reliability of our age-modelled trajectories across datasets.
  Interpretation: Our study highlights the potential of multiparametric qMRI to capture complex, region-specific brain development patterns, addressing the need for comprehensive, age-spanning studies across multiple brain structures. Various harmonization strategies can merge qMRI cohorts, improving the robustness of qMRI-based age models and facilitating the understanding of normal patterns and disease-associated deviations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00661v1</guid>
      <category>physics.med-ph</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinjie Chen, Mario Ocampo-Pineda, Po-Jui Lu, Clara Ekerdt, Matthias Weigel, Michelle G. Jansen, Alessandro Cagol, Kwok-Shing Chan, Sabine Sch\"adelin, Marcel Zwiers, Joukje M. Oosterman, David G. Norris, Johanna M. M. Bayer, Andre F. Marquand, Willeke M. Menks, Jens Kuhle, Ludwig Kappos, Lester Melie-Garcia, Cristina Granziera, Jos\'e P. Marques</dc:creator>
    </item>
    <item>
      <title>Exploring Behavior-Relevant and Disentangled Neural Dynamics with Generative Diffusion Models</title>
      <link>https://arxiv.org/abs/2410.09614</link>
      <description>arXiv:2410.09614v2 Announce Type: replace 
Abstract: Understanding the neural basis of behavior is a fundamental goal in neuroscience. Current research in large-scale neuro-behavioral data analysis often relies on decoding models, which quantify behavioral information in neural data but lack details on behavior encoding. This raises an intriguing scientific question: ``how can we enable in-depth exploration of neural representations in behavioral tasks, revealing interpretable neural dynamics associated with behaviors''. However, addressing this issue is challenging due to the varied behavioral encoding across different brain regions and mixed selectivity at the population level. To tackle this limitation, our approach, named ``BeNeDiff'', first identifies a fine-grained and disentangled neural subspace using a behavior-informed latent variable model. It then employs state-of-the-art generative diffusion models to synthesize behavior videos that interpret the neural dynamics of each latent factor. We validate the method on multi-session datasets containing widefield calcium imaging recordings across the dorsal cortex. Through guiding the diffusion model to activate individual latent factors, we verify that the neural dynamics of latent factors in the disentangled neural subspace provide interpretable quantifications of the behaviors of interest. At the same time, the neural subspace in BeNeDiff demonstrates high disentanglement and neural reconstruction quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09614v2</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yule Wang, Chengrui Li, Weihan Li, Anqi Wu</dc:creator>
    </item>
    <item>
      <title>Long-Range Feedback Spiking Network Captures Dynamic and Static Representations of the Visual Cortex under Movie Stimuli</title>
      <link>https://arxiv.org/abs/2306.01354</link>
      <description>arXiv:2306.01354v2 Announce Type: replace-cross 
Abstract: Deep neural networks (DNNs) are widely used models for investigating biological visual representations. However, existing DNNs are mostly designed to analyze neural responses to static images, relying on feedforward structures and lacking physiological neuronal mechanisms. There is limited insight into how the visual cortex represents natural movie stimuli that contain context-rich information. To address these problems, this work proposes the long-range feedback spiking network (LoRaFB-SNet), which mimics top-down connections between cortical regions and incorporates spike information processing mechanisms inherent to biological neurons. Taking into account the temporal dependence of representations under movie stimuli, we present Time-Series Representational Similarity Analysis (TSRSA) to measure the similarity between model representations and visual cortical representations of mice. LoRaFB-SNet exhibits the highest level of representational similarity, outperforming other well-known and leading alternatives across various experimental paradigms, especially when representing long movie stimuli. We further conduct experiments to quantify how temporal structures (dynamic information) and static textures (static information) of the movie stimuli influence representational similarity, suggesting that our model benefits from long-range feedback to encode context-dependent representations just like the brain. Altogether, LoRaFB-SNet is highly competent in capturing both dynamic and static representations of the mouse visual cortex and contributes to the understanding of movie processing mechanisms of the visual system. Our codes are available at https://github.com/Grasshlw/SNN-Neural-Similarity-Movie.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.01354v2</guid>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liwei Huang, Zhengyu Ma, Liutao Yu, Huihui Zhou, Yonghong Tian</dc:creator>
    </item>
    <item>
      <title>Du-IN: Discrete units-guided mask modeling for decoding speech from Intracranial Neural signals</title>
      <link>https://arxiv.org/abs/2405.11459</link>
      <description>arXiv:2405.11459v3 Announce Type: replace-cross 
Abstract: Invasive brain-computer interfaces with Electrocorticography (ECoG) have shown promise for high-performance speech decoding in medical applications, but less damaging methods like intracranial stereo-electroencephalography (sEEG) remain underexplored. With rapid advances in representation learning, leveraging abundant recordings to enhance speech decoding is increasingly attractive. However, popular methods often pre-train temporal models based on brain-level tokens, overlooking that brain activities in different regions are highly desynchronized during tasks. Alternatively, they pre-train spatial-temporal models based on channel-level tokens but fail to evaluate them on challenging tasks like speech decoding, which requires intricate processing in specific language-related areas. To address this issue, we collected a well-annotated Chinese word-reading sEEG dataset targeting language-related brain networks from 12 subjects. Using this benchmark, we developed the Du-IN model, which extracts contextual embeddings based on region-level tokens through discrete codex-guided mask modeling. Our model achieves state-of-the-art performance on the 61-word classification task, surpassing all baselines. Model comparisons and ablation studies reveal that our design choices, including (i) temporal modeling based on region-level tokens by utilizing 1D depthwise convolution to fuse channels in the ventral sensorimotor cortex (vSMC) and superior temporal gyrus (STG) and (ii) self-supervision through discrete codex-guided mask modeling, significantly contribute to this performance. Overall, our approach -- inspired by neuroscience findings and capitalizing on region-level representations from specific brain regions -- is suitable for invasive brain modeling and represents a promising neuro-inspired AI approach in brain-computer interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11459v3</guid>
      <category>eess.SP</category>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hui Zheng, Hai-Teng Wang, Wei-Bang Jiang, Zhong-Tao Chen, Li He, Pei-Yang Lin, Peng-Hu Wei, Guo-Guang Zhao, Yun-Zhe Liu</dc:creator>
    </item>
    <item>
      <title>Leveraging Recurrent Neural Networks for Predicting Motor Movements from Primate Motor Cortex Neural Recordings</title>
      <link>https://arxiv.org/abs/2410.22283</link>
      <description>arXiv:2410.22283v2 Announce Type: replace-cross 
Abstract: This paper presents an efficient deep learning solution for decoding motor movements from neural recordings in non-human primates. An Autoencoder Gated Recurrent Unit (AEGRU) model was adopted as the model architecture for this task. The autoencoder is only used during the training stage to achieve better generalization. Together with the preprocessing techniques, our model achieved 0.71 $R^2$ score, surpassing the baseline models in Neurobench and is ranked first for $R^2$ in the IEEE BioCAS 2024 Grand Challenge on Neural Decoding. Model pruning is also applied leading to a reduction of 41.4% of the multiply-accumulate (MAC) operations with little change in the $R^2$ score compared to the unpruned model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22283v2</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuanxi Wang, Zuowen Wang, Shih-Chii Liu</dc:creator>
    </item>
    <item>
      <title>Dynamical similarity analysis uniquely captures how computations develop in RNNs</title>
      <link>https://arxiv.org/abs/2410.24070</link>
      <description>arXiv:2410.24070v2 Announce Type: replace-cross 
Abstract: Methods for analyzing representations in neural systems are increasingly popular tools in neuroscience and mechanistic interpretability. Measures comparing neural activations across conditions, architectures, and species give scalable ways to understand information transformation within different neural networks. However, recent findings show that some metrics respond to spurious signals, leading to misleading results. Establishing benchmark test cases is thus essential for identifying the most reliable metric and potential improvements. We propose that compositional learning in recurrent neural networks (RNNs) can provide a test case for dynamical representation alignment metrics. Implementing this case allows us to evaluate if metrics can identify representations that develop throughout learning and determine if representations identified by metrics reflect the network's actual computations. Building both attractor and RNN based test cases, we show that the recently proposed Dynamical Similarity Analysis (DSA) is more noise robust and reliably identifies behaviorally relevant representations compared to prior metrics (Procrustes, CKA). We also demonstrate how such test cases can extend beyond metric evaluation to study new architectures. Specifically, testing DSA in modern (Mamba) state space models suggests that these models, unlike RNNs, may not require changes in recurrent dynamics due to their expressive hidden states. Overall, we develop test cases that showcase how DSA's enhanced ability to detect dynamical motifs makes it highly effective for identifying ongoing computations in RNNs and revealing how networks learn tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.24070v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 04 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quentin Guilhot, Micha{\l} W\'ojcik, Jascha Achterberg, Rui Ponte Costa</dc:creator>
    </item>
  </channel>
</rss>
