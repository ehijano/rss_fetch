<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Feb 2025 04:15:53 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Meta-Cultural Competence: Climbing the Right Hill of Cultural Awareness</title>
      <link>https://arxiv.org/abs/2502.09637</link>
      <description>arXiv:2502.09637v1 Announce Type: new 
Abstract: Numerous recent studies have shown that Large Language Models (LLMs) are biased towards a Western and Anglo-centric worldview, which compromises their usefulness in non-Western cultural settings. However, "culture" is a complex, multifaceted topic, and its awareness, representation, and modeling in LLMs and LLM-based applications can be defined and measured in numerous ways. In this position paper, we ask what does it mean for an LLM to possess "cultural awareness", and through a thought experiment, which is an extension of the Octopus test proposed by Bender and Koller (2020), we argue that it is not cultural awareness or knowledge, rather meta-cultural competence, which is required of an LLM and LLM-based AI system that will make it useful across various, including completely unseen, cultures. We lay out the principles of meta-cultural competence AI systems, and discuss ways to measure and model those.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09637v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sougata Saha, Saurabh Kumar Pandey, Monojit Choudhury</dc:creator>
    </item>
    <item>
      <title>Genetic Data Governance in Crisis: Policy Recommendations for Safeguarding Privacy and Preventing Discrimination</title>
      <link>https://arxiv.org/abs/2502.09716</link>
      <description>arXiv:2502.09716v1 Announce Type: new 
Abstract: Genetic data collection has become ubiquitous today. The ability to meaningfully interpret genetic data has motivated its widespread use, providing crucial insights into human health and ancestry while driving important public health initiatives. Easy access to genetic testing has fueled a rapid expansion of recreational direct-to-consumer offerings. However, the growth of genetic datasets and their applications has created significant privacy and discrimination risks, as our understanding of the scientific basis for genetic traits continues to evolve. In this paper, we organize the uses of genetic data along four distinct "pillars": clinical practice, research, forensic and government use, and recreational use. Using our scientific understanding of genetics, genetic inference methods and their associated risks, and current public protections, we build a risk assessment framework that identifies key values that any governance system must preserve. We analyze case studies using this framework to assess how well existing regulatory frameworks preserve desired values. Our investigation reveals critical gaps in these frameworks and identifies specific threats to privacy and personal liberties, particularly through genetic discrimination. We propose comprehensive policy reforms to: (1) update the legal definition of genetic data to protect against modern technological capabilities, (2) expand the Genetic Information Nondiscrimination Act (GINA) to cover currently unprotected domains, and (3) establish a unified regulatory framework under a single governing body to oversee all applications of genetic data. We conclude with three open questions about genetic data: the challenges posed by its relational nature, including consent for relatives and minors; the complexities of international data transfer; and its potential integration into large language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09716v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vivek Ramanan, Ria Vinod, Cole Williams, Sohini Ramachandran, Suresh Venkatasubramanian</dc:creator>
    </item>
    <item>
      <title>Automation Bias in the AI Act: On the Legal Implications of Attempting to De-Bias Human Oversight of AI</title>
      <link>https://arxiv.org/abs/2502.10036</link>
      <description>arXiv:2502.10036v1 Announce Type: new 
Abstract: This paper examines the legal implications of the explicit mentioning of automation bias (AB) in the Artificial Intelligence Act (AIA). The AIA mandates human oversight for high-risk AI systems and requires providers to enable awareness of AB, i.e., the tendency to over-rely on AI outputs. The paper analyses how this extra-juridical concept is embedded in the AIA, the division of responsibility between AI providers and deployers, and the challenges of legally enforcing this novel awareness requirement. The analysis shows that the AIA's focus on providers does not adequately address design and context as causes of AB, and questions whether the AIA should directly regulate the risk of AB rather than just mandating awareness. As the AIA's approach requires a balance between legal mandates and behavioural science, the paper proposes that harmonised standards should reference the state of research on AB and human-AI interaction. Ultimately, further empirical research will be essential for effective safeguards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10036v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johann Laux, Hannah Ruschemeier</dc:creator>
    </item>
    <item>
      <title>Technical Risks of (Lethal) Autonomous Weapons Systems</title>
      <link>https://arxiv.org/abs/2502.10174</link>
      <description>arXiv:2502.10174v1 Announce Type: new 
Abstract: The autonomy and adaptability of (Lethal) Autonomous Weapons Systems, (L)AWS in short, promise unprecedented operational capabilities, but they also introduce profound risks that challenge the principles of control, accountability, and stability in international security. This report outlines the key technological risks associated with (L)AWS deployment, emphasizing their unpredictability, lack of transparency, and operational unreliability, which can lead to severe unintended consequences.
  Key Takeaways:
  1. Proposed advantages of (L)AWS can only be achieved through objectification and classification, but a range of systematic risks limit the reliability and predictability of classifying algorithms.
  2. These systematic risks include the black-box nature of AI decision-making, susceptibility to reward hacking, goal misgeneralization and potential for emergent behaviors that escape human control.
  3. (L)AWS could act in ways that are not just unexpected but also uncontrollable, undermining mission objectives and potentially escalating conflicts.
  4. Even rigorously tested systems may behave unpredictably and harmfully in real-world conditions, jeopardizing both strategic stability and humanitarian principles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10174v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heramb Podar, Alycia Colijn</dc:creator>
    </item>
    <item>
      <title>Merging public elementary schools to reduce racial/ethnic segregation</title>
      <link>https://arxiv.org/abs/2502.10193</link>
      <description>arXiv:2502.10193v1 Announce Type: new 
Abstract: Diverse schools can help address implicit biases and increase empathy, mutual respect, and reflective thought by fostering connections between students from different racial/ethnic, socioeconomic, and other backgrounds. Unfortunately, demographic segregation remains rampant in US public schools, despite over 70 years since the passing of federal legislation formally outlawing segregation by race. However, changing how students are assigned to schools can help foster more integrated learning environments. In this paper, we explore "school mergers" as one such under-explored, yet promising, student assignment policy change. School mergers involve merging the school attendance boundaries, or catchment areas, of schools and subsequently changing the grades each school offers. We develop an algorithm to simulate elementary school mergers across 200 large school districts serving 4.5 million elementary school students and find that pairing or tripling schools in this way could reduce racial/ethnic segregation by a median relative 20% -- and as much as nearly 60% in some districts -- while increasing driving times to schools by an average of a few minutes each way. Districts with many interfaces between racially/ethnically-disparate neighborhoods tend to be prime candidates for mergers. We also compare the expected results of school mergers to other typical integration policies, like redistricting, and find that different policies may be more or less suitable in different places. Finally, we make our results available through a public dashboard for policymakers and community members to explore further (https://mergers.schooldiversity.org). Together, our study offers new findings and tools to support integration policy-making across US public school districts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10193v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Madison Landry, Nabeel Gillani</dc:creator>
    </item>
    <item>
      <title>Assortment Optimization for Patient-Provider Matching</title>
      <link>https://arxiv.org/abs/2502.10353</link>
      <description>arXiv:2502.10353v1 Announce Type: new 
Abstract: Rising provider turnover forces healthcare administrators to frequently rematch patients to available providers, which can be cumbersome and labor-intensive. To reduce the burden of rematching, we study algorithms for matching patients and providers through assortment optimization. We develop a patient-provider matching model in which we simultaneously offer each patient a menu of providers, and patients subsequently respond and select providers. By offering assortments upfront, administrators can balance logistical ease and patient autonomy. We study policies for assortment optimization and characterize their performance under different problem settings. We demonstrate that the selection of assortment policy is highly dependent on problem specifics and, in particular, on a patient's willingness to match and the ratio between patients and providers. On real-world data, we show that our best policy can improve match quality by 13% over a greedy solution by tailoring assortment sizes based on patient characteristics. We conclude with recommendations for running a real-world patient-provider matching system inspired by our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10353v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naveen Raman, Holly Wiberg</dc:creator>
    </item>
    <item>
      <title>From Argumentation to Deliberation: Perspectivized Stance Vectors for Fine-grained (Dis)agreement Analysis</title>
      <link>https://arxiv.org/abs/2502.09644</link>
      <description>arXiv:2502.09644v1 Announce Type: cross 
Abstract: Debating over conflicting issues is a necessary first step towards resolving conflicts. However, intrinsic perspectives of an arguer are difficult to overcome by persuasive argumentation skills. Proceeding from a debate to a deliberative process, where we can identify actionable options for resolving a conflict requires a deeper analysis of arguments and the perspectives they are grounded in - as it is only from there that one can derive mutually agreeable resolution steps. In this work we develop a framework for a deliberative analysis of arguments in a computational argumentation setup. We conduct a fine-grained analysis of perspectivized stances expressed in the arguments of different arguers or stakeholders on a given issue, aiming not only to identify their opposing views, but also shared perspectives arising from their attitudes, values or needs. We formalize this analysis in Perspectivized Stance Vectors that characterize the individual perspectivized stances of all arguers on a given issue. We construct these vectors by determining issue- and argument-specific concepts, and predict an arguer's stance relative to each of them. The vectors allow us to measure a modulated (dis)agreement between arguers, structured by perspectives, which allows us to identify actionable points for conflict resolution, as a first step towards deliberation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09644v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moritz Plenz, Philipp Heinisch, Janosch Gehring, Philipp Cimiano, Anette Frank</dc:creator>
    </item>
    <item>
      <title>Language Shift or Maintenance? An Intergenerational Study of the Tibetan Community in Saudi Arabia</title>
      <link>https://arxiv.org/abs/2502.09646</link>
      <description>arXiv:2502.09646v1 Announce Type: cross 
Abstract: The present study provides the first-ever report on the language shift from Tibetan to Arabic among descendants of Tibetan families who migrated from the Tibet region to Saudi Arabia around 70 years ago. The aim of this study was to determine whether three age groups had adopted different practices in terms of maintaining Tibetan or shifting to Hijazi Arabic. To this end, 96 male and female members of the Tibetan community responded to a questionnaire in which they were asked about their code choice in different domains (home, neighbourhood, friends and relatives, expressing emotion, and performing religious rituals). The data revealed significant intergenerational differences between members of the community in terms of the extent of the shift to Arabic, with Tibetan rarely used by younger members and older members making only slightly more use of it. The difference between the three age groups was significant, at a p-value of .001.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09646v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.36892/ijlls.v5i3.1407</arxiv:DOI>
      <dc:creator>Sumaiyah Turkistani Mohammad Almoaily</dc:creator>
    </item>
    <item>
      <title>AI-VERDE: A Gateway for Egalitarian Access to Large Language Model-Based Resources For Educational Institutions</title>
      <link>https://arxiv.org/abs/2502.09651</link>
      <description>arXiv:2502.09651v1 Announce Type: cross 
Abstract: We present AI-VERDE, a unified LLM-as-a-platform service designed to facilitate seamless integration of commercial, cloud-hosted, and on-premise open LLMs in academic settings. AI-VERDE streamlines access management for instructional and research groups by providing features such as robust access control, privacy-preserving mechanisms, native Retrieval-Augmented Generation (RAG) support, budget management for third-party LLM services, and both a conversational web interface and API access. In a pilot deployment at a large public university, AI-VERDE demonstrated significant engagement across diverse educational and research groups, enabling activities that would typically require substantial budgets for commercial LLM services with limited user and team management capabilities. To the best of our knowledge, AI-Verde is the first platform to address both academic and research needs for LLMs within an higher education institutional framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09651v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Mithun, Enrique Noriega-Atala, Nirav Merchant, Edwin Skidmore</dc:creator>
    </item>
    <item>
      <title>Cancer Vaccine Adjuvant Name Recognition from Biomedical Literature using Large Language Models</title>
      <link>https://arxiv.org/abs/2502.09659</link>
      <description>arXiv:2502.09659v1 Announce Type: cross 
Abstract: Motivation: An adjuvant is a chemical incorporated into vaccines that enhances their efficacy by improving the immune response. Identifying adjuvant names from cancer vaccine studies is essential for furthering research and enhancing immunotherapies. However, the manual curation from the constantly expanding biomedical literature poses significant challenges. This study explores the automated recognition of vaccine adjuvant names using Large Language Models (LLMs), specifically Generative Pretrained Transformers (GPT) and Large Language Model Meta AI (Llama). Methods: We utilized two datasets: 97 clinical trial records from AdjuvareDB and 290 abstracts annotated with the Vaccine Adjuvant Compendium (VAC). GPT-4o and Llama 3.2 were employed in zero-shot and few-shot learning paradigms with up to four examples per prompt. Prompts explicitly targeted adjuvant names, testing the impact of contextual information such as substances or interventions. Outputs underwent automated and manual validation for accuracy and consistency. Results: GPT-4o attained 100% Precision across all situations while exhibiting notable improve in Recall and F1-scores, particularly with incorporating interventions. On the VAC dataset, GPT-4o achieved a maximum F1-score of 77.32% with interventions, surpassing Llama-3.2-3B by approximately 2%. On the AdjuvareDB dataset, GPT-4o reached an F1-score of 81.67% for three-shot prompting with interventions, surpassing Llama-3.2-3 B's maximum F1-score of 65.62%. Conclusion: Our findings demonstrate that LLMs excel at identifying adjuvant names, including rare variations of naming representation. This study emphasizes the capability of LLMs to enhance cancer vaccine development by efficiently extracting insights. Future work aims to broaden the framework to encompass various biomedical literature and enhance model generalizability across various vaccines and adjuvants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09659v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hasin Rehana, Jie Zheng, Leo Yeh, Benu Bansal, Nur Bengisu \c{C}am, Christianah Jemiyo, Brett McGregor, Arzucan \"Ozg\"ur, Yongqun He, Junguk Hur</dc:creator>
    </item>
    <item>
      <title>Large Language Models and Provenance Metadata for Determining the Relevance of Images and Videos in News Stories</title>
      <link>https://arxiv.org/abs/2502.09689</link>
      <description>arXiv:2502.09689v1 Announce Type: cross 
Abstract: The most effective misinformation campaigns are multimodal, often combining text with images and videos taken out of context -- or fabricating them entirely -- to support a given narrative. Contemporary methods for detecting misinformation, whether in deepfakes or text articles, often miss the interplay between multiple modalities. Built around a large language model, the system proposed in this paper addresses these challenges. It analyzes both the article's text and the provenance metadata of included images and videos to determine whether they are relevant. We open-source the system prototype and interactive web interface.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09689v1</guid>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tomas Peterka, Matyas Bohacek</dc:creator>
    </item>
    <item>
      <title>Carbon- and Precedence-Aware Scheduling for Data Processing Clusters</title>
      <link>https://arxiv.org/abs/2502.09717</link>
      <description>arXiv:2502.09717v1 Announce Type: cross 
Abstract: As large-scale data processing workloads continue to grow, their carbon footprint raises concerns. Prior research on carbon-aware schedulers has focused on shifting computation to align with availability of low-carbon energy, but these approaches assume that each task can be executed independently. In contrast, data processing jobs have precedence constraints (i.e., outputs of one task are inputs for another) that complicate decisions, since delaying an upstream ``bottleneck'' task to a low-carbon period will also block downstream tasks, impacting the entire job's completion time. In this paper, we show that carbon-aware scheduling for data processing benefits from knowledge of both time-varying carbon and precedence constraints. Our main contribution is $\texttt{PCAPS}$, a carbon-aware scheduler that interfaces with modern ML scheduling policies to explicitly consider the precedence-driven importance of each task in addition to carbon. To illustrate the gains due to fine-grained task information, we also study $\texttt{CAP}$, a wrapper for any carbon-agnostic scheduler that adapts the key provisioning ideas of $\texttt{PCAPS}$. Our schedulers enable a configurable priority between carbon reduction and job completion time, and we give analytical results characterizing the trade-off between the two. Furthermore, our Spark prototype on a 100-node Kubernetes cluster shows that a moderate configuration of $\texttt{PCAPS}$ reduces carbon footprint by up to 32.9% without significantly impacting the cluster's total efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09717v1</guid>
      <category>cs.DC</category>
      <category>cs.CY</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Lechowicz, Rohan Shenoy, Noman Bashir, Mohammad Hajiesmaili, Adam Wierman, Christina Delimitrou</dc:creator>
    </item>
    <item>
      <title>Co-designing Large Language Model Tools for Project-Based Learning with K12 Educators</title>
      <link>https://arxiv.org/abs/2502.09799</link>
      <description>arXiv:2502.09799v1 Announce Type: cross 
Abstract: The emergence of generative AI, particularly large language models (LLMs), has opened the door for student-centered and active learning methods like project-based learning (PBL). However, PBL poses practical implementation challenges for educators around project design and management, assessment, and balancing student guidance with student autonomy. The following research documents a co-design process with interdisciplinary K-12 teachers to explore and address the current PBL challenges they face. Through teacher-driven interviews, collaborative workshops, and iterative design of wireframes, we gathered evidence for ways LLMs can support teachers in implementing high-quality PBL pedagogy by automating routine tasks and enhancing personalized learning. Teachers in the study advocated for supporting their professional growth and augmenting their current roles without replacing them. They also identified affordances and challenges around classroom integration, including resource requirements and constraints, ethical concerns, and potential immediate and long-term impacts. Drawing on these, we propose design guidelines for future deployment of LLM tools in PBL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09799v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713971</arxiv:DOI>
      <arxiv:journal_reference>CHI Conference on Human Factors in Computing Systems (CHI '25), April 26-May 01, 2025, Yokohama, Japan. ACM, New York, NY, USA</arxiv:journal_reference>
      <dc:creator>Prerna Ravi, John Masla, Gisella Kakoti, Grace Lin, Emma Anderson, Matt Taylor, Anastasia Ostrowski, Cynthia Breazeal, Eric Klopfer, Hal Abelson</dc:creator>
    </item>
    <item>
      <title>How Users Who are Blind or Low Vision Play Mobile Games: Perceptions, Challenges, and Strategies</title>
      <link>https://arxiv.org/abs/2502.09866</link>
      <description>arXiv:2502.09866v1 Announce Type: cross 
Abstract: As blind and low-vision (BLV) players engage more deeply with games, accessibility features have become essential. While some research has explored tools and strategies to enhance game accessibility, the specific experiences of these players with mobile games remain underexamined. This study addresses this gap by investigating how BLV users experience mobile games with varying accessibility levels. Through interviews with 32 experienced BLV mobile players, we explore their perceptions, challenges, and strategies for engaging with mobile games. Our findings reveal that BLV players turn to mobile games to alleviate boredom, achieve a sense of accomplishment, and build social connections, but face barriers depending on the game's accessibility level. We also compare mobile games to other forms of gaming, highlighting the relative advantages of mobile games, such as the inherent accessibility of smartphones. This study contributes to understanding BLV mobile gaming experiences and provides insights for enhancing accessible mobile game design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09866v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihe Ran, Xiyu Li, Qing Xiao, Xianzhe Fan, Franklin Mingzhe Li, Yanyun Wang, Zhicong Lu</dc:creator>
    </item>
    <item>
      <title>The Blind Men and the Elephant: Mapping Interdisciplinarity in Research on Decentralized Autonomous Organizations</title>
      <link>https://arxiv.org/abs/2502.09949</link>
      <description>arXiv:2502.09949v1 Announce Type: cross 
Abstract: Decentralized Autonomous Organizations (DAOs) are attracting interdisciplinary interest, particularly in business, economics, and computer science. However, much like the parable of the blind men and the elephant, where each observer perceives only a fragment of the whole, DAO research remains fragmented across disciplines, limiting a comprehensive understanding of their potential. This paper assesses the maturity of interdisciplinary research on DAOs by analyzing knowledge flows between Business &amp; Economics and Computer Science through citation network analysis, topic modelling, and outlet analysis. Our findings reveal that while DAOs serve as a vibrant topic of interdisciplinary discourse, current research remains predominantly applied and case-driven, with limited theoretical integration. Strengthening the alignment between organizational and technical insights is crucial for advancing DAO research and fostering a more cohesive interdisciplinary framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09949v1</guid>
      <category>cs.DC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giorgia Samp\`o, Oliver Baumann, Marco Peressotti</dc:creator>
    </item>
    <item>
      <title>Revisiting the Berkeley Admissions data: Statistical Tests for Causal Hypotheses</title>
      <link>https://arxiv.org/abs/2502.10161</link>
      <description>arXiv:2502.10161v1 Announce Type: cross 
Abstract: Reasoning about fairness through correlation-based notions is rife with pitfalls. The 1973 University of California, Berkeley graduate school admissions case from Bickel et. al. (1975) is a classic example of one such pitfall, namely Simpson's paradox. The discrepancy in admission rates among males and female applicants, in the aggregate data over all departments, vanishes when admission rates per department are examined. We reason about the Berkeley graduate school admissions case through a causal lens. In the process, we introduce a statistical test for causal hypothesis testing based on Pearl's instrumental-variable inequalities (Pearl 1995). We compare different causal notions of fairness that are based on graphical, counterfactual and interventional queries on the causal model, and develop statistical tests for these notions that use only observational data. We study the logical relations between notions, and show that while notions may not be equivalent, their corresponding statistical tests coincide for the case at hand. We believe that a thorough case-based causal analysis helps develop a more principled understanding of both causal hypothesis testing and fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10161v1</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourbh Bhadane, Joris M. Mooij, Philip Boeken, Onno Zoeter</dc:creator>
    </item>
    <item>
      <title>"It's Like Not Being Able to Read and Write": Narrowing the Digital Divide for Older Adults and Leveraging the Role of Digital Educators in Ireland</title>
      <link>https://arxiv.org/abs/2502.10166</link>
      <description>arXiv:2502.10166v1 Announce Type: cross 
Abstract: As digital services increasingly replace traditional analogue systems, ensuring that older adults are not left behind is critical to fostering inclusive access. This study explores how digital educators support older adults in developing essential digital skills, drawing insights from interviews with $34$ educators in Ireland. These educators, both professional and volunteer, offer instruction through a range of formats, including workshops, remote calls, and in-person sessions. Our findings highlight the importance of personalized, step-by-step guidance tailored to older adults' learning needs, as well as fostering confidence through hands-on engagement with technology. Key challenges identified include limited transportation options, poor internet connectivity, outdated devices, and a lack of familial support for learning. To address these barriers, we propose enhanced public funding, expanded access to resources, and sustainable strategies such as providing relevant and practical course materials. Additionally, innovative tools like simulated online platforms for practicing digital transactions can help reduce anxiety and enhance digital literacy among older adults. This study underscores the vital role that digital educators play in bridging the digital divide, creating a more inclusive, human-centered approach to digital learning for older adults.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10166v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3689050.3704945</arxiv:DOI>
      <dc:creator>Melanie Gruben, Ashley Sheil, Sanchari Das, Michelle O Keeffe, Jacob Camilleri, Moya Cronin, Hazel Murray</dc:creator>
    </item>
    <item>
      <title>Large Language Models and Synthetic Data for Monitoring Dataset Mentions in Research Papers</title>
      <link>https://arxiv.org/abs/2502.10263</link>
      <description>arXiv:2502.10263v1 Announce Type: cross 
Abstract: Tracking how data is mentioned and used in research papers provides critical insights for improving data discoverability, quality, and production. However, manually identifying and classifying dataset mentions across vast academic literature is resource-intensive and not scalable. This paper presents a machine learning framework that automates dataset mention detection across research domains by leveraging large language models (LLMs), synthetic data, and a two-stage fine-tuning process. We employ zero-shot extraction from research papers, an LLM-as-a-Judge for quality assessment, and a reasoning agent for refinement to generate a weakly supervised synthetic dataset. The Phi-3.5-mini instruct model is pre-fine-tuned on this dataset, followed by fine-tuning on a manually annotated subset. At inference, a ModernBERT-based classifier efficiently filters dataset mentions, reducing computational overhead while maintaining high recall. Evaluated on a held-out manually annotated sample, our fine-tuned model outperforms NuExtract-v1.5 and GLiNER-large-v2.1 in dataset extraction accuracy. Our results highlight how LLM-generated synthetic data can effectively address training data scarcity, improving generalization in low-resource settings. This framework offers a pathway toward scalable monitoring of dataset usage, enhancing transparency, and supporting researchers, funders, and policymakers in identifying data gaps and strengthening data accessibility for informed decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10263v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aivin V. Solatorio, Rafael Macalaba, James Liounis</dc:creator>
    </item>
    <item>
      <title>A Roadmap to Address Burnout in the Cybersecurity Profession: Outcomes from a Multifaceted Workshop</title>
      <link>https://arxiv.org/abs/2502.10293</link>
      <description>arXiv:2502.10293v1 Announce Type: cross 
Abstract: This paper addresses the critical issue of burnout among cybersecurity professionals, a growing concern that threatens the effectiveness of digital defense systems. As the industry faces a significant attrition crisis, with nearly 46% of cybersecurity leaders contemplating departure from their roles, it is imperative to explore the causes and consequences of burnout through a socio-technical lens. These challenges were discussed by experts from academia and industry in a multi-disciplinary workshop at the 26th International Conference on Human-Computer Interaction to address broad antecedents of burnout, manifestation and its consequences among cybersecurity professionals, as well as programs to mitigate impacts from burnout. Central to the analysis is an empirical study of former National Security Agency (NSA) tactical cyber operators. This paper presents key insights in the following areas based on discussions in the workshop: lessons for public and private sectors from the NSA study, a comparative review of addressing burnout in the healthcare profession. It also outlines a roadmap for future collaborative research, thereby informing interdisciplinary studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10293v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ann Rangarajan, Calvin Nobles, Josiah Dykstra, Margaret Cunningham, Nikki Robinson, Tammie Hollis, Celeste Lyn Paul, Charles Gulotta</dc:creator>
    </item>
    <item>
      <title>Object Detection and Tracking</title>
      <link>https://arxiv.org/abs/2502.10310</link>
      <description>arXiv:2502.10310v1 Announce Type: cross 
Abstract: Efficient and accurate object detection is an important topic in the development of computer vision systems. With the advent of deep learning techniques, the accuracy of object detection has increased significantly. The project aims to integrate a modern technique for object detection with the aim of achieving high accuracy with real-time performance. The reliance on other computer vision algorithms in many object identification systems, which results in poor and ineffective performance, is a significant obstacle. In this research, we solve the end-to-end object detection problem entirely using deep learning techniques. The network is trained using the most difficult publicly available dataset, which is used for an annual item detection challenge. Applications that need object detection can benefit the system's quick and precise finding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10310v1</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Md Pranto, Omar Faruk</dc:creator>
    </item>
    <item>
      <title>Robustness tests for biomedical foundation models should tailor to specification</title>
      <link>https://arxiv.org/abs/2502.10374</link>
      <description>arXiv:2502.10374v1 Announce Type: cross 
Abstract: Existing regulatory frameworks for biomedical AI include robustness as a key component but lack detailed implementational guidance. The recent rise of biomedical foundation models creates new hurdles in testing and certification given their broad capabilities and susceptibility to complex distribution shifts. To balance test feasibility and effectiveness, we suggest a priority-based, task-oriented approach to tailor robustness evaluation objectives to a predefined specification. We urge concrete policies to adopt a granular categorization of robustness concepts in the specification. Our approach promotes the standardization of risk assessment and monitoring, which guides technical developments and mitigation efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10374v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>R. Patrick Xian, Noah R. Baker, Tom David, Qiming Cui, A. Jay Holmgren, Stefan Bauer, Madhumita Sushil, Reza Abbasi-Asl</dc:creator>
    </item>
    <item>
      <title>Willingness to Read AI-Generated News Is Not Driven by Their Perceived Quality</title>
      <link>https://arxiv.org/abs/2409.03500</link>
      <description>arXiv:2409.03500v3 Announce Type: replace 
Abstract: The advancement of artificial intelligence has led to its application in many areas, including news media, which makes it crucial to understand public reception of AI-generated news. This preregistered study investigates (i) the perceived quality of AI-assisted and AI-generated versus human-generated news articles, (ii) whether disclosure of AI's involvement in generating these news articles influences engagement with them, and (iii) whether such awareness affects the willingness to read AI-generated articles in the future. We conducted a survey experiment with 599 Swiss participants, who evaluated the credibility, readability, and expertise of news articles either written by journalists (control group), rewritten by AI (AI-assisted group), or entirely written by AI (AI-generated group). Our results indicate that all articles were perceived to be of equal quality. When participants in the treatment groups were subsequently made aware of AI's role, they expressed a higher willingness to continue reading the articles than participants in the control group. However, they were not more willing to read AI-generated news in the future. These results suggest that aversion to AI usage in news media is not primarily rooted in a perceived lack of quality, and that by disclosing using AI, journalists could induce more short-term engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03500v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabrizio Gilardi, Sabrina Di Lorenzo, Juri Ezzaini, Beryl Santa, Benjamin Streiff, Eric Zurfluh, Emma Hoes</dc:creator>
    </item>
    <item>
      <title>Consumer Segmentation and Participation Drivers in Community-Supported Agriculture: A Choice Experiment and PLS-SEM Approach</title>
      <link>https://arxiv.org/abs/2411.00010</link>
      <description>arXiv:2411.00010v2 Announce Type: replace 
Abstract: As the global food system faces increasing challenges from sustainability, climate change, and food security issues, alternative food networks like Community-Supported Agriculture (CSA) play an essential role in fostering stronger connections between consumers and producers. However, understanding consumer engagement with CSA is fragmented, particularly in Japan where CSA participation is still emerging. This study aims to identify potential CSA participants in Japan and validate existing theories on CSA participation through a quantitative analysis of 2,484 Japanese consumers. Using choice experiments, Latent Class Analysis, and Partial Least Squares Structural Equation Modeling, we identified five distinct consumer segments. The "Sustainable Food Seekers" group showed the highest positive utility for CSA, driven primarily by "Food Education and Learning Opportunities" and "Contribution to Environmental and Social Issues." These factors were consistently significant across all segments, suggesting that many Japanese consumers value CSA for its educational and environmental benefits. In contrast, factors related to "Variety of Ingredients" were less influential in determining participation intentions. The findings suggest that promoting CSA in Japan may be most effective by emphasizing its role in environmental and social impact, rather than focusing solely on product attributes like organic certification, which is readily available in supermarkets. This reflects a key distinction between CSA adoption in Japan and in other cultural contexts, where access to organic produce is a primary driver. For "Sustainable Food Seekers," CSA offers a way to contribute to broader societal goals rather than just securing organic products.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00010v2</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sota Takagi, Miki Saijo, Takumi Ohashi</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Framework to Operationalize Social Stereotypes for Responsible AI Evaluations</title>
      <link>https://arxiv.org/abs/2501.02074</link>
      <description>arXiv:2501.02074v2 Announce Type: replace 
Abstract: Societal stereotypes are at the center of a myriad of responsible AI interventions targeted at reducing the generation and propagation of potentially harmful outcomes. While these efforts are much needed, they tend to be fragmented and often address different parts of the issue without taking in a unified or holistic approach about social stereotypes and how they impact various parts of the machine learning pipeline. As a result, it fails to capitalize on the underlying mechanisms that are common across different types of stereotypes, and to anchor on particular aspects that are relevant in certain cases. In this paper, we draw on social psychological research, and build on NLP data and methods, to propose a unified framework to operationalize stereotypes in generative AI evaluations. Our framework identifies key components of stereotypes that are crucial in AI evaluation, including the target group, associated attribute, relationship characteristics, perceiving group, and relevant context. We also provide considerations and recommendations for its responsible use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02074v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aida Davani, Sunipa Dev, H\'ector P\'erez-Urbina, Vinodkumar Prabhakaran</dc:creator>
    </item>
    <item>
      <title>A Simulation-Based Framework for Leveraging Shared Autonomous Vehicles to Enhance Disaster Evacuations in Rural Regions with a Focus on Vulnerable Populations</title>
      <link>https://arxiv.org/abs/2502.07787</link>
      <description>arXiv:2502.07787v2 Announce Type: replace 
Abstract: Rapid advancements in autonomous vehicles (AVs) are poised to revolutionize transportation and communities, including disaster evacuations, particularly through the deployment of Shared Autonomous Vehicles (SAVs). Despite the potential, the use of SAVs in rural disaster evacuations remains an underexplored area. To address this gap, this study proposes a simulation-based framework that integrates both mathematical programming and SUMO traffic simulation to deploy SAVs in pre- and post-disaster evacuations in rural areas. The framework prioritizes the needs of vulnerable groups, including individuals with disabilities, limited English proficiency, and elderly residents. Sumter County, Florida, serves as the case study due to its unique characteristics: a high concentration of vulnerable individuals and limited access to public transportation, making it one of the most transportation-insecure counties in the state. These conditions present significant challenges for evacuation planning in the region. To explore potential solutions, we conducted mass evacuation simulations by incorporating SAVs across seven scenarios. These scenarios represented varying SAV penetration levels, ranging from 20% to 100% of the vulnerable population, and were compared to a baseline scenario using only passenger cars. Additionally, we examined both pre-disaster and post-disaster conditions, accounting for infrastructure failures and road closures. According to the simulation results, higher SAV integration significantly improves traffic distribution and reduces congestion. Scenarios featuring more SAVs exhibited lower congestion peaks and more stable traffic flow. Conversely, mixed traffic environments demonstrate reduced average speeds attributable to interactions between SAVs and passenger cars, while exclusive use of SAVs results in higher speeds and more stable travel patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07787v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Alican Sevim, Qian-wen Guo, Eren Erman Ozguven</dc:creator>
    </item>
    <item>
      <title>AI Safety for Everyone</title>
      <link>https://arxiv.org/abs/2502.09288</link>
      <description>arXiv:2502.09288v2 Announce Type: replace 
Abstract: Recent discussions and research in AI safety have increasingly emphasized the deep connection between AI safety and existential risk from advanced AI systems, suggesting that work on AI safety necessarily entails serious consideration of potential existential threats. However, this framing has three potential drawbacks: it may exclude researchers and practitioners who are committed to AI safety but approach the field from different angles; it could lead the public to mistakenly view AI safety as focused solely on existential scenarios rather than addressing a wide spectrum of safety challenges; and it risks creating resistance to safety measures among those who disagree with predictions of existential AI risks. Through a systematic literature review of primarily peer-reviewed research, we find a vast array of concrete safety work that addresses immediate and practical concerns with current AI systems. This includes crucial areas like adversarial robustness and interpretability, highlighting how AI safety research naturally extends existing technological and systems safety concerns and practices. Our findings suggest the need for an epistemically inclusive and pluralistic conception of AI safety that can accommodate the full range of safety considerations, motivations, and perspectives that currently shape the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09288v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Balint Gyevnar, Atoosa Kasirzadeh</dc:creator>
    </item>
    <item>
      <title>Pitfalls of Evidence-Based AI Policy</title>
      <link>https://arxiv.org/abs/2502.09618</link>
      <description>arXiv:2502.09618v2 Announce Type: replace 
Abstract: Nations across the world are working to govern AI. However, from a technical perspective, there is uncertainty and disagreement on the best way to do this. Meanwhile, recent debates over AI regulation have led to calls for "evidence-based AI policy" which emphasize holding regulatory action to a high evidentiary standard. Evidence is of irreplaceable value to policymaking. However, holding regulatory action to too high an evidentiary standard can lead to systematic neglect of certain risks. In historical policy debates (e.g., over tobacco ca. 1965 and fossil fuels ca. 1985) "evidence-based policy" rhetoric is also a well-precedented strategy to downplay the urgency of action, delay regulation, and protect industry interests. Here, we argue that if the goal is evidence-based AI policy, the first regulatory objective must be to actively facilitate the process of identifying, studying, and deliberating about AI risks. We discuss a set of 15 regulatory goals to facilitate this and show that Brazil, Canada, China, the EU, South Korea, the UK, and the USA all have substantial opportunities to adopt further evidence-seeking policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.09618v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Casper, David Krueger, Dylan Hadfield-Menell</dc:creator>
    </item>
    <item>
      <title>Accuracy and Political Bias of News Source Credibility Ratings by Large Language Models</title>
      <link>https://arxiv.org/abs/2304.00228</link>
      <description>arXiv:2304.00228v3 Announce Type: replace-cross 
Abstract: Search engines increasingly leverage large language models (LLMs) to generate direct answers, and AI chatbots now access the Internet for fresh data. As information curators for billions of users, LLMs must assess the accuracy and reliability of different sources. This paper audits nine widely used LLMs from three leading providers -- OpenAI, Google, and Meta -- to evaluate their ability to discern credible and high-quality information sources from low-credibility ones. We find that while LLMs can rate most tested news outlets, larger models more frequently refuse to provide ratings due to insufficient information, whereas smaller models are more prone to making errors in their ratings. For sources where ratings are provided, LLMs exhibit a high level of agreement among themselves (average Spearman's $\rho = 0.79$), but their ratings align only moderately with human expert evaluations (average $\rho = 0.50$). Analyzing news sources with different political leanings in the US, we observe a liberal bias in credibility ratings yielded by all LLMs in default configurations. Additionally, assigning partisan roles to LLMs consistently induces strong politically congruent bias in their ratings. These findings have important implications for the use of LLMs in curating news and political information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.00228v3</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3717867.3717903</arxiv:DOI>
      <dc:creator>Kai-Cheng Yang, Filippo Menczer</dc:creator>
    </item>
    <item>
      <title>Delving into LLM-assisted writing in biomedical publications through excess vocabulary</title>
      <link>https://arxiv.org/abs/2406.07016</link>
      <description>arXiv:2406.07016v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) like ChatGPT can generate and revise text with human-level performance. These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused. Yet, many scientists use them for their scholarly writing. But how wide-spread is such LLM usage in the academic literature? To answer this question for the field of biomedical research, we present an unbiased, large-scale approach: we study vocabulary changes in over 15 million biomedical abstracts from 2010--2024 indexed by PubMed, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. This excess word analysis suggests that at least 13.5% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, reaching 40% for some subcorpora. We show that LLMs have had an unprecedented impact on scientific writing in biomedical research, surpassing the effect of major world events such as the Covid pandemic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07016v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.SI</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Dmitry Kobak, Rita Gonz\'alez-M\'arquez, Em\H{o}ke-\'Agnes Horv\'at, Jan Lause</dc:creator>
    </item>
    <item>
      <title>Leveraging Large Language Models to Enhance Machine Learning Interpretability and Predictive Performance: A Case Study on Emergency Department Returns for Mental Health Patients</title>
      <link>https://arxiv.org/abs/2502.00025</link>
      <description>arXiv:2502.00025v3 Announce Type: replace-cross 
Abstract: Importance: Emergency department (ED) returns for mental health conditions pose a major healthcare burden, with 24-27% of patients returning within 30 days. Traditional machine learning models for predicting these returns often lack interpretability for clinical use.
  Objective: To assess whether integrating large language models (LLMs) with machine learning improves predictive accuracy and clinical interpretability of ED mental health return risk models.
  Methods: This retrospective cohort study analyzed 42,464 ED visits for 27,904 unique mental health patients at an academic medical center in the Deep South from January 2018 to December 2022.
  Main Outcomes and Measures: Two primary outcomes were evaluated: (1) 30-day ED return prediction accuracy and (2) model interpretability using a novel LLM-enhanced framework integrating SHAP (SHapley Additive exPlanations) values with clinical knowledge.
  Results: For chief complaint classification, LLaMA 3 (8B) with 10-shot learning outperformed traditional models (accuracy: 0.882, F1-score: 0.86). In SDoH classification, LLM-based models achieved 0.95 accuracy and 0.96 F1-score, with Alcohol, Tobacco, and Substance Abuse performing best (F1: 0.96-0.89), while Exercise and Home Environment showed lower performance (F1: 0.70-0.67). The LLM-based interpretability framework achieved 99% accuracy in translating model predictions into clinically relevant explanations. LLM-extracted features improved XGBoost AUC from 0.74 to 0.76 and AUC-PR from 0.58 to 0.61.
  Conclusions and Relevance: Integrating LLMs with machine learning models yielded modest but consistent accuracy gains while significantly enhancing interpretability through automated, clinically relevant explanations. This approach provides a framework for translating predictive analytics into actionable clinical insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00025v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 17 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdulaziz Ahmed, Mohammad Saleem, Mohammed Alzeen, Badari Birur, Rachel E Fargason, Bradley G Burk, Hannah Rose Harkins, Ahmed Alhassan, Mohammed Ali Al-Garadi</dc:creator>
    </item>
  </channel>
</rss>
