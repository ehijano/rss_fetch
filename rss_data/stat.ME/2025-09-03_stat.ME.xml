<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Sep 2025 01:33:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>LHS in LHS: A new expansion strategy for Latin hypercube sampling in simulation design</title>
      <link>https://arxiv.org/abs/2509.00159</link>
      <description>arXiv:2509.00159v1 Announce Type: new 
Abstract: Latin Hypercube Sampling (LHS) is a prominent tool in simulation design, with a variety of applications in high-dimensional and computationally expensive problems. LHS allows for various optimization strategies, most notably to ensure space-filling properties. However, LHS is a single-stage algorithm that requires a priori knowledge of the targeted sample size. In this work, we present LHS in LHS, a new expansion algorithm for LHS that enables the addition of new samples to an existing LHS-distributed set while (approximately) preserving its properties. In summary, the algorithm identifies regions of the parameter space that are far from the initial set, draws a new LHS within those regions, and then merges it with the original samples. As a by-product, we introduce a new metric, the LHS degree, which quantifies the deviation of a given design from an LHS distribution. Our public implementation is distributed via the Python package expandLHS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00159v1</guid>
      <category>stat.ME</category>
      <category>astro-ph.HE</category>
      <category>cs.DS</category>
      <category>gr-qc</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.softx.2025.102294</arxiv:DOI>
      <arxiv:journal_reference>SoftwareX 31, 102294 (2025)</arxiv:journal_reference>
      <dc:creator>Matteo Boschini, Davide Gerosa, Alessandro Crespi, Matteo Falcone</dc:creator>
    </item>
    <item>
      <title>G-HIVE: Parameter Estimation and Approximate Inference for Multivariate Response Generalized Linear Models with Hidden Variables</title>
      <link>https://arxiv.org/abs/2509.00196</link>
      <description>arXiv:2509.00196v1 Announce Type: new 
Abstract: In practice, there often exist unobserved variables, also termed hidden variables, associated with both the response and covariates. Existing works in the literature mostly focus on linear regression with hidden variables. However, when the regression model is non-linear, the presence of hidden variables leads to new challenges in parameter identification, estimation, and statistical inference. This paper studies multivariate response generalized linear models (GLMs) with hidden variables. We propose a unified framework for parameter estimation and statistical inference called G-HIVE, short for 'G'eneralized - 'HI'dden 'V'ariable adjusted 'E'stimation. Specifically, based on factor model assumptions, we propose a modified quasi-likelihood approach to estimate an intermediate parameter, defined through a set of reweighted estimating equations. The key of our approach is to construct the proper weight, so that the first-order asymptotic bias of the estimator can be removed by orthogonal projection. Moreover, we propose an approximate inference framework for uncertainty quantification. Theoretically, we establish the first-order and second-order asymptotic bias and the convergence rate of our estimator. In addition, we characterize the accuracy of the Gaussian approximation of our estimator via the Berry-Esseen bound, which justifies the validity of the proposed approximate inference approach. Extensive simulations and real data analysis results show that G-HIVE is feasibly implementable and can outperform the baseline method that ignores hidden variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00196v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Inbeom Lee, Yang Ning</dc:creator>
    </item>
    <item>
      <title>On the Use of Weighting for Personalized and Transparent Evidence Synthesis</title>
      <link>https://arxiv.org/abs/2509.00228</link>
      <description>arXiv:2509.00228v1 Announce Type: new 
Abstract: Over the past few decades, statistical methods for causal inference have made impressive strides, enabling progress across a range of scientific fields. However, these methodological advances have often been confined to individual studies, limiting their ability to draw more generalizable conclusions. Achieving a thorough understanding of cause and effect typically relies on the integration, reconciliation, and synthesis from diverse study designs and multiple data sources. Furthermore, it is crucial to direct this synthesis effort toward understanding the effect of treatments for specific patient populations. To address these challenges, we present a weighting framework for evidence synthesis that handles both individual- and aggregate-level data, encompassing and extending conventional regression-based meta-analysis methods. We use this approach to tailor meta-analyses, targeting the covariate profiles of patients in a target population in a sample-bounded manner, thereby enhancing their personalization and robustness. We propose a technique to detect studies that meaningfully deviate from the target population, suggesting when it might be prudent to exclude them from the analysis. We establish multiple consistency conditions and demonstrate asymptotic normality for the proposed estimator. We illustrate this approach using simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00228v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenqi Shi, Jos\'e R. Zubizarreta</dc:creator>
    </item>
    <item>
      <title>A Bayesian Optimal Phase II Design for Randomized Immunotherapy Trials with Delayed Treatment Effects</title>
      <link>https://arxiv.org/abs/2509.00238</link>
      <description>arXiv:2509.00238v1 Announce Type: new 
Abstract: Immunotherapy has transformed cancer treatment, yet its delayed therapeutic effects often lead to non-proportional hazards, rendering many conventional phase II designs underpowered and prone to type I error inflation. To address this issue, we propose a novel Bayesian Optimal Phase II design (DTE-BOP2) that explicitly models the uncertainty in the separation timing of treatment effect. The treatment separation timepoint (denoted by S) is endowed with a truncated-Gamma prior, whose parameters can be elicited from experts or inferred from historical data, with default settings available when prior knowledge is scarce. Built upon the BOP2 framework (Zhou et al. 2017, 2020), our design retains operational simplicity while incorporating type I error control and maintaining the power. Extensive simulations demonstrate that DTE-BOP2 uniformly controls type I error at the nominal level across a wide range of treatment effect separation timepoint S. We further observe that the power decreases monotonically as S increases. Importantly, we find that the power is primarily driven by the relative magnitude of treatment benefit before and after the separation time, i.e., the ratio of medians, rather than their absolute values. Compared to the original BOP2, the piecewise weighted log-rank, and the conventional log-rank tests, DTE-BOP2 achieves higher power with smaller sample sizes while preserving type I error robustness across plausible delay scenarios. An open-source R package, DTEBOP2 (CRAN), with detailed vignettes, enables investigators to implement the design and analyse phase-II trials exhibiting delayed treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00238v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongheng Cai, Haitao Pan</dc:creator>
    </item>
    <item>
      <title>Universal inference for variance components</title>
      <link>https://arxiv.org/abs/2509.00255</link>
      <description>arXiv:2509.00255v1 Announce Type: new 
Abstract: We consider universal inference in variance components models, focusing on settings where the parameter is near or at the boundary of the parameter set. Two cases, which are not handled by existing state-of-the-art methods, are of particular interest: (i) inference on a variance component when other variance components are near or at the boundary, and (ii) inference on near-unity proportions of variability, that is, one variance component divided by the sum of all variance components. Case (i) is relevant, for example, for the construction of componentwise confidence intervals, as often used by practitioners. Case (ii) is particularly relevant when making inferences about heritability in modern genetics. For both cases, we show how to construct confidence intervals that are uniformly valid in finite samples. We propose algorithms which, by exploiting the structure of variance components models, lead to substantially faster computing than naive implementations of universal inference. The usefulness of the proposed methods is illustrated by simulations and a data example with crossed random effects, which are known to be complicated for conventional inference procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00255v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiqiao Zhang, Karl Oskar Ekvall, Aaron J. Molstad</dc:creator>
    </item>
    <item>
      <title>Modeling Human Spatial Mobility Patterns with the L\'evy Flight Cluster Model</title>
      <link>https://arxiv.org/abs/2509.00298</link>
      <description>arXiv:2509.00298v1 Announce Type: new 
Abstract: Despite the extensive collection of individual mobility data over the past decade, fueled by the widespread use of GPS-enabled personal devices, the existing statistical literature on estimating human spatial mobility patterns from temporally irregular location data remains limited. In this paper, we introduce the L\'{e}vy Flight Cluster Model (LFCM), a hierarchical Bayesian mixture model designed to analyze an individual's activity distribution. The LFCM can be utilized to determine probabilistic overlaps between individuals' activity patterns and serves as an anonymization tool to generate synthetic location data. We present our methodology using real-world human location data, demonstrating its ability to accurately capture the key characteristics of human movement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00298v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Malcolm Wolff, Adrian Dobra, Anton H. Westveld, Grace S. Chiu</dc:creator>
    </item>
    <item>
      <title>Multiply Robust Inference of Average Treatment Effects by High-dimensional Empirical Likelihood</title>
      <link>https://arxiv.org/abs/2509.00312</link>
      <description>arXiv:2509.00312v1 Announce Type: new 
Abstract: In this paper, we develop a multiply robust inference procedure of the average treatment effect (ATE) for data with high-dimensional covariates. We consider the case where it is difficult to correctly specify a single parametric model for the propensity scores (PS). For example, the target population is formed from heterogeneous sources with different treatment assignment mechanisms. We propose a novel high-dimensional empirical likelihood weighting method under soft covariate balancing constraints to combine multiple working PS models. An extended set of calibration functions is used, and a regularized augmented outcome regression is developed to correct the bias due to non-exact covariate balancing. Those two approaches provide a new way to construct the Neyman orthogonal score of the ATE. The proposed confidence interval for the ATE achieves asymptotically valid nominal coverage under high-dimensional covariates if any of the PS models, their linear combination, or the outcome regression model is correctly specified. The proposed method is extended to generalized linear models for the outcome variable. Specifically, we consider estimating the ATE for data with unknown clusters, where multiple working PS models can be fitted based on the estimated clusters. Our proposed approach enables robust inference of the ATE for clustered data. We demonstrate the advantages of the proposed approach over the existing doubly robust inference methods under high-dimensional covariates via simulation studies. We analyzed the right heart catheterization dataset, initially collected from five medical centers and two different phases of studies, to demonstrate the effectiveness of the proposed method in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00312v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xintao Xia, Yumou Qiu</dc:creator>
    </item>
    <item>
      <title>An adaptive design for optimizing treatment assignment in randomized clinical trials</title>
      <link>https://arxiv.org/abs/2509.00429</link>
      <description>arXiv:2509.00429v1 Announce Type: new 
Abstract: The treatment assignment mechanism in a randomized clinical trial can be optimized for statistical efficiency within a specified class of randomization mechanisms. Optimal designs of this type have been characterized in terms of the variances of potential outcomes conditional on baseline covariates. Approximating these optimal designs requires information about the conditional variance functions, which is often unavailable or unreliable at the design stage. As a practical solution to this dilemma, we propose a multi-stage adaptive design that allows the treatment assignment mechanism to be modified at interim analyses based on accruing information about the conditional variance functions. This adaptation has profound implications on the distribution of trial data, which need to be accounted for in treatment effect estimation. We consider a class of treatment effect estimators that are consistent and asymptotically normal, identify the most efficient estimator within this class, and approximate the most efficient estimator by substituting estimates of unknown quantities. Simulation results indicate that, when there is little or no prior information available, the proposed design can bring substantial efficiency gains over conventional one-stage designs based on the same prior information. The methodology is illustrated with real data from a completed trial in stroke.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00429v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wei Zhang, Zhiwei Zhang, Aiyi Liu</dc:creator>
    </item>
    <item>
      <title>Semiparametric model averaging for high-dimensional quantile regression with nonignorable nonresponse</title>
      <link>https://arxiv.org/abs/2509.00464</link>
      <description>arXiv:2509.00464v1 Announce Type: new 
Abstract: Model averaging has demonstrated superior performance for ensemble forecasting in high-dimensional framework, its extension to incomplete datasets remains a critical but underexplored challenge. Moreover, identifying the parsimonious model through averaging procedure in quantile regression demands urgent methodological innovation. In this paper, we propose a novel model averaging method for high-dimensional quantile regression with nonignorable missingness. The idea is to relax the parametric constraint on the conditional distribution of respondents, which is constructed through the two-phase scheme: (i) a semiparametric likelihood-based estimation for the missing mechanism, and (ii) a semiparametric weighting procedure to combine candidate models. One of pivotal advantages is our SMA estimator can asymptotically concentrate on the optimally correct model when the candidate set involves at least one correct model. Theoretical results show that the estimator achieves asymptotic optimality even under complex missingness conditions. Empirical conclusions illustrate the efficiency of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00464v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Xiong, Dianliang Deng, Dehui Wang</dc:creator>
    </item>
    <item>
      <title>Adaptive CUSUM Chart for Simultaneous Monitoring of Mean and Variance</title>
      <link>https://arxiv.org/abs/2509.00514</link>
      <description>arXiv:2509.00514v1 Announce Type: new 
Abstract: Simultaneously monitoring changes in both the mean and variance is a fundamental problem in Statistical Process Control, and numerous methods have been developed to address it. However, many existing approaches face notable limitations: some rely on tuning parameters that can significantly affect performance; others are biased toward detecting increases in variance while performing poorly for decreases; and some are computationally burdensome. To address these limitations, we propose a novel adaptive CUSUM chart for jointly monitoring the mean and variance of a Gaussian process. The proposed method is free of tuning parameters, efficient in detecting a broad range of shifts in both mean and variance, and well-suited for real-time monitoring due to its recursive structure. It also has a built-in post-signal diagnostics function that can identify what kind of distributional changes have occurred after an alarm. Simulation results show that, compared to existing methods, the proposed chart achieves the most favorable balance between detection power and computational efficiency, delivering the best overall performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00514v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gokul Parakulum, Jun Li</dc:creator>
    </item>
    <item>
      <title>Adaptive CUSUM Chart for Real-Time Monitoring of Bivariate Event Data</title>
      <link>https://arxiv.org/abs/2509.00535</link>
      <description>arXiv:2509.00535v1 Announce Type: new 
Abstract: Monitoring time-between-events (TBE) data, where the goal is to track the time between consecutive events, has important applications across various fields. Many existing schemes for monitoring multivariate TBE data suffer from inherent delays, as they require waiting until all components of the observation vector are available before making inferences about the process state. In practice, however, these components are rarely recorded simultaneously. To address this issue, Zwetsloot et al. proposed a Shewhart chart for bivariate TBE data that updates the process status as individual observations arrive. However, like most Shewhart-type charts, their method evaluates the process based solely on the most recent observation and does not incorporate historical information. As a result, it is ineffective in detecting small to moderate changes. To overcome this limitation, we develop an adaptive CUSUM chart that updates with each incoming observation while also accumulating information over time. Simulation studies and real-data applications demonstrate that our method substantially outperforms the Shewhart chart of Zwetsloot et al., offering a robust and effective tool for real-time monitoring of bivariate TBE data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00535v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gokul Parakulum, Jun Li</dc:creator>
    </item>
    <item>
      <title>Measure Selection for Functional Linear Model</title>
      <link>https://arxiv.org/abs/2509.00583</link>
      <description>arXiv:2509.00583v1 Announce Type: new 
Abstract: Advancements in modern science have led to an increased prevalence of functional data, which are usually viewed as elements of the space of square-integrable functions $L^2$. Core methods in functional data analysis, such as functional principal component analysis, are typically grounded in the Hilbert structure of $L^2$ and rely on inner products based on integrals with respect to the Lebesgue measure over a fixed domain. A more flexible framework is proposed, where the measure can be arbitrary, allowing natural extensions to unbounded domains and prompting the question of optimal measure choice. Specifically, a novel functional linear model is introduced that incorporates a data-adaptive choice of the measure that defines the space, alongside an enhanced function principal component analysis. Selecting a good measure can improve the model's predictive performance, especially when the underlying processes are not well-represented when adopting the default Lebesgue measure. Simulations, as well as applications to COVID-19 data and the National Health and Nutrition Examination Survey data, show that the proposed approach consistently outperforms the conventional functional linear model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00583v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Su I Iao, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>MOPED: A moving sum method for change point detection in pairwise extremal dependence</title>
      <link>https://arxiv.org/abs/2509.00585</link>
      <description>arXiv:2509.00585v1 Announce Type: new 
Abstract: It is increasingly the case with modern time series that many data sets of practical interest contain abrupt changes in structure. These changes may occur in complex characteristics such as the extremal dependence structure, and identifying such structural breaks remains a challenging problem. Many existing change point detection algorithms focus on changes in dependence across the entire distribution, rather than the tails, and approaches that are tailored to extremes typically make strict parametric assumptions or they are only applicable to bivariate data. We propose a nonparametric MOving sum-based approach for detecting multiple changes in the Pairwise Extremal Dependence (MOPED) of multivariate regularly varying data. To avoid the classical problem of threshold selection in the study of multivariate extremes, we further propose a multiscale, multi-threshold variant of MOPED that pools change point estimates across choices of the threshold and the bandwidth used in local estimation. Good performance of MOPED is illustrated in a simulation study, and we showcase its ability to identify subtle changes in tail dependence class in the absence of correlation changes. We further demonstrate the usefulness of MOPED by identifying changes in the extremal connectivity of electroencephalogram (EEG) signals of seizure-prone neonates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00585v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Euan T. McGonigle, Matthew Pawley, Jordan Richards, Christian Rohrbeck</dc:creator>
    </item>
    <item>
      <title>Mode-Matched Inverse Gamma Priors for Variance Components in Bayesian Multilevel Models</title>
      <link>https://arxiv.org/abs/2509.00636</link>
      <description>arXiv:2509.00636v1 Announce Type: new 
Abstract: We introduce a strategy for specifying informative inverse-gamma (IG) priors for variance components in Bayesian multilevel models (MLMs), derived via transformations from chi-square to gamma to inverse-gamma distributions. A Monte Carlo simulation compared frequentist (maximum likelihood) estimation and Bayesian estimation using uninformative, weakly informative, and strongly informative variance priors across varied conditions (clusters J = 10, 30, 100; cluster sizes M = 5, 30; intraclass correlations 0.01, 0.20, 0.40; levels of explained variance at L1 and L2 R^2 = 0, 0.2, 0.4). The simulation results indicated that strongly informative IG priors (with hyperparameters set so the prior mode equals a plausible true variance) yielded more accurate and stable variance estimates with reduced bias and narrower credible intervals than flat/uninformative or weak IG(0.01, 0.01) priors. In an empirical example using the TIMSS 2019 Grade 8 science achievement data, both the full sample (273 schools) and small subsample (30 schools) were analyzed. The small-sample analysis with an informative variance prior anchored near the full-sample variance while considerably reducing the uncertainty of estimates. Findings suggest that carefully calibrated informative variance priors improve the precision and accuracy of parameter estimates, particularly when the number of higher-level units is limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00636v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liu Liu</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Spatial Autoregression with Latent Factors by Diversified Projections</title>
      <link>https://arxiv.org/abs/2509.00742</link>
      <description>arXiv:2509.00742v1 Announce Type: new 
Abstract: We study one particular type of multivariate spatial autoregression (MSAR) model with diverging dimensions in both responses and covariates. This makes the usual MSAR models no longer applicable due to the high computational cost. To address this issue, we propose a factor-augmented spatial autoregression (FSAR) model. FSAR is a special case of MSAR but with a novel factor structure imposed on the high-dimensional random error vector. The latent factors of FSAR are assumed to be of a fixed dimension. Therefore, they can be estimated consistently by the diversified projections method \citep{fan2022learning}, as long as the dimension of the multivariate response is diverging. Once the fixed-dimensional latent factors are consistently estimated, they are then fed back into the original SAR model and serve as exogenous covariates. This leads to a novel FSAR model. Thereafter, different components of the high-dimensional response can be modeled separately. To handle the high-dimensional feature, a smoothly clipped absolute deviation (SCAD) type penalized estimator is developed for each response component. We show theoretically that the resulting SCAD estimator is uniformly selection consistent, as long as the tuning parameter is selected appropriately. For practical selection of the tuning parameter, a novel BIC method is developed. Extensive numerical studies are conducted to demonstrate the finite sample performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00742v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaxin Shi, Xuening Zhu, Jing Zhou, Baichen Yu, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>FBMS: An R Package for Flexible Bayesian Model Selection and Model Averaging</title>
      <link>https://arxiv.org/abs/2509.00753</link>
      <description>arXiv:2509.00753v1 Announce Type: new 
Abstract: The FBMS R package facilitates Bayesian model selection and model averaging in complex regression settings by employing a variety of Monte Carlo model exploration methods. At its core, the package implements an efficient Mode Jumping Markov Chain Monte Carlo (MJMCMC) algorithm, designed to improve mixing in multi-modal posterior landscapes within Bayesian generalized linear models. In addition, it provides a genetically modified MJMCMC (GMJMCMC) algorithm that introduces nonlinear feature generation, thereby enabling the estimation of Bayesian generalized nonlinear models (BGNLMs). Within this framework, the algorithm maintains and updates populations of transformed features, computes their posterior probabilities, and evaluates the posteriors of models constructed from them. We demonstrate the effective use of FBMS for both inferential and predictive modeling in Gaussian regression, focusing on different instances of the BGNLM class of models. Furthermore, through a broad set of applications, we illustrate how the methodology can be extended to increasingly complex modeling scenarios, extending to other response distributions and mixed effect models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00753v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Frommlet, Jon Lachmann, Geir Storvik, Aliaksandr Hubin</dc:creator>
    </item>
    <item>
      <title>A Hybrid APIM-CFGM Model for Longitudinal Non-Exchangeable Dyads: Demonstrating and Comparing Estimation Approaches Using Multilevel Modeling</title>
      <link>https://arxiv.org/abs/2509.00993</link>
      <description>arXiv:2509.00993v1 Announce Type: new 
Abstract: Understanding change over time within dyads, such as mentor-mentee or therapist-client pairs, poses unique challenges, particularly in studies with small samples and distinguishable roles. This paper introduces a flexible hybrid longitudinal modeling that integrates features of the Actor-Partner Interdependence Model (APIM) and the Common Fate Growth Model (CFGM) to simultaneously estimate individual-level and shared dyad-level effects. Using a hypothetical peer-mentoring example (novices paired with experts), the model addresses three key issues: (1) how the interpretation of model parameters when role is dummy-coded versus effect-coded; (2) how model performance is affected by small sample sizes; and (3) how results differ between Maximum Likelihood (ML) and Bayesian estimation. Simulated data for 50 dyads across five time points are analyzed, with subsampling at 30 and 5 dyads. Models are estimated in a multilevel modeling framework using R (lme4 for ML and brms for Bayesian inference). Results show that dummy and effect coding reflect distinct interpretations: dummy coding expresses effects relative to a reference group, whereas effect coding centers parameters on the grand mean across roles. Although model fit remains unchanged, the choice of coding impacts how group-level effects and interactions are interpreted. Very small samples (e.g., 5 dyads) lead to unstable estimates, whereas Bayesian credible intervals more accurately reflect uncertainty in such cases. In larger samples, ML and Bayesian estimates converge. This APIM-CFGM hybrid offers a practical tool for researchers analyzing longitudinal dyadic data with distinguishable roles and smaller sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00993v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liu Liu</dc:creator>
    </item>
    <item>
      <title>Generalized promotion time cure model: A new modeling framework to identify cell-type-specific genes and improve survival prognosis</title>
      <link>https://arxiv.org/abs/2509.01001</link>
      <description>arXiv:2509.01001v1 Announce Type: new 
Abstract: Single-cell technologies provide an unprecedented opportunity for dissecting the interplay between the cancer cells and the associated tumor microenvironment, and the produced high-dimensional omics data should also augment existing survival modeling approaches for identifying tumor cell type-specific genes predictive of cancer patient survival. However, there is no statistical model to integrate multiscale data including individual-level survival data, multicellular-level cell composition data and cellular-level single-cell omics covariates. We propose a class of Bayesian generalized promotion time cure models (GPTCMs) for the multiscale data integration to identify cell-type-specific genes and improve cancer prognosis. We demonstrate with simulations in both low- and high-dimensional settings that the proposed Bayesian GPTCMs are able to identify cell-type-associated covariates and improve survival prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01001v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>stat.ML</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhi Zhao, Fatih Kizilaslan, Shixiong Wang, Manuela Zucknick</dc:creator>
    </item>
    <item>
      <title>Testing maximum entropy models with e-values</title>
      <link>https://arxiv.org/abs/2509.01064</link>
      <description>arXiv:2509.01064v1 Announce Type: new 
Abstract: E-values have recently emerged as a robust and flexible alternative to p-values for hypothesis testing, especially under optional continuation, i.e., when additional data from further experiments are collected. In this work, we define optimal e-values for testing between maximum entropy models, both in the microcanonical (hard constraints) and canonical (soft constraints) settings. We show that, when testing between two hypotheses that are both microcanonical, the so-called growth-rate optimal e-variable admits an exact analytical expression, which also serves as a valid e-variable in the canonical case. For canonical tests, where exact solutions are typically unavailable, we introduce a microcanonical approximation and verify its excellent performance via both theoretical arguments and numerical simulations. We then consider constrained binary models, focusing on $2 \times k$ contingency tables -- an essential framework in statistics and a natural representation for various models of complex systems. Our microcanonical optimal e-variable performs well in both settings, constituting a new tool that remains effective even in the challenging case when the number $k$ of groups grows with the sample size, as in models with growing features used for the analysis of real-world heterogeneous networks and time-series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01064v1</guid>
      <category>stat.ME</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesca Giuffrida, Diego Garlaschelli, Peter Gr\"unwald</dc:creator>
    </item>
    <item>
      <title>Specifying Composites in Structural Equation Modeling: Traditional, Recent and New Approaches</title>
      <link>https://arxiv.org/abs/2509.01292</link>
      <description>arXiv:2509.01292v1 Announce Type: new 
Abstract: Composites, or linear combinations of variables, play an important role in multivariate behavioral research. They appear in the form of indices, inventories, formative constructs, parcels, and emergent variables. Although structural equation modeling is widely used to study relations between variables, current approaches to incorporating composites have one or more limitations. These limitations include not modeling composite creation, not employing weights as model parameters, not being able to estimate weights, not allowing for composites in endogenous model positions, and not being able to assess whether a composite fully transmits the effects of or on its components. To address these limitations, we propose two novel composite specifications. The first specification combines the refined H-O specification of composites with phantom variables and uses the inverse of the weights as free parameters. The second specification blends the H-O specification with the pseudo-indicator approach of Rose et al. and uses the weights of all but one indicator as free parameters. We demonstrate the applicability of these specifications using an empirical example. The paper concludes with recommendations for choosing among the available composite specifications, as well as suggestions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01292v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J\"org Henseler, Xi Yu, Tamara Schamberger, Gregory R. Hancock, Florian Schuberth</dc:creator>
    </item>
    <item>
      <title>On the interplay between prior weight and variance of the robustification component in Robust Mixture Prior Bayesian Dynamic Borrowing approach</title>
      <link>https://arxiv.org/abs/2509.01435</link>
      <description>arXiv:2509.01435v1 Announce Type: new 
Abstract: Robust Mixture Prior (RMP) is a popular Bayesian dynamic borrowing method, which combines an informative historical distribution with a less informative component (referred as robustification component) in a mixture prior to enhance the efficiency of hybrid-control randomized trials. Current practice typically focuses solely on the selection of the prior weight that governs the relative influence of these two components, often fixing the variance of the robustification component to that of a single observation. In this study we demonstrate that the performance of RMPs critically depends on the joint selection of both weight and variance of the robustification component. In particular, we show that a wide range of weight-variance pairs can yield practically identical posterior inferences (in particular regions of the parameter space) and that large variance robust components may be employed without incurring in the so called Lindley's paradox. We further show that the use of large variance robustification components leads to improved asymptotic Type I error control and enhanced robustness of the RMP to the specification of the location parameter of the robustification component. Finally, we leverage these theoretical results to propose a novel and practical hyper-parameter elicitation routine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01435v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Ratta, Gaelle Saint-Hilary, Mauro Gasparini, Pavel Mozgunov</dc:creator>
    </item>
    <item>
      <title>Sampling as Bandits: Evaluation-Efficient Design for Black-Box Densities</title>
      <link>https://arxiv.org/abs/2509.01437</link>
      <description>arXiv:2509.01437v1 Announce Type: new 
Abstract: We introduce bandit importance sampling (BIS), a new class of importance sampling methods designed for settings where the target density is expensive to evaluate. In contrast to adaptive importance sampling, which optimises a proposal distribution, BIS directly designs the samples through a sequential strategy that combines space-filling designs with multi-armed bandits. Our method leverages Gaussian process surrogates to guide sample selection, enabling efficient exploration of the parameter space with minimal target evaluations. We establish theoretical guarantees on convergence and demonstrate the effectiveness of the method across a broad range of sampling tasks. BIS delivers accurate approximations with fewer target evaluations, outperforming competing approaches across multimodal, heavy-tailed distributions, and real-world applications to Bayesian inference of computationally expensive models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01437v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takuo Matsubara, Andrew Duncan, Simon Cotter, Konstantinos Zygalakis</dc:creator>
    </item>
    <item>
      <title>Discrete Fourier Transform versus Discrete Chi-Square Method</title>
      <link>https://arxiv.org/abs/2509.01540</link>
      <description>arXiv:2509.01540v1 Announce Type: new 
Abstract: We compare two time series analysis methods, the Discrete Fourier Transform (DFT) and our Discrete Chi-square Method (DCM). DCM is designed for detecting many signals superimposed on an unknown trend. The solution for the non-linear DCM model is an ill-posed problem. The backbone of DCM is the Gauss-Markov theorem that the least squares fit is the best unbiased estimator for linear regression models. DCM is a simple numerical time series analysis method that performs a massive number of linear least squares fits. We show that our numerical solution for the DCM model fulfills the three conditions of a well-posed problem: existence, uniqueness and stability. The Fisher-test is used to identify the best DCM model from all alternative tested DCM models. The correct DCM model must also pass our Predictivity-test. Our analyses of seven different simulated data samples expose the weaknesses of DFT and the efficiency of DCM. The DCM signal and trend detection depend only on the sample size and the accuracy of data. DCM is an ideal forecasting method because the time span of observations is irrelevant. We recommend fast sampling of large high quality datasets and the analysis of those datasets using numerical DCM parallel computation Python code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01540v1</guid>
      <category>stat.ME</category>
      <category>astro-ph.IM</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lauri Jetsu</dc:creator>
    </item>
    <item>
      <title>A Time-Series Model for Areal Data Using Spatially Correlated Gaussian Processes</title>
      <link>https://arxiv.org/abs/2509.01604</link>
      <description>arXiv:2509.01604v1 Announce Type: new 
Abstract: Traditional spatio-temporal models for areal data typically begin with spatial structure imposed at the level of random effects and later extend to include temporal dynamics. We propose an alternative hierarchical modeling framework that captures temporal trends in areal data through Gaussian processes that share spatial information via correlated variance components. This allows the model to better capture shared patterns of variability across regions while preserving local temporal dynamics, offering a more flexible representation of spatio-temporal processes.
  Specifically, we extend independent Gaussian-process models for time-series data to a spatially correlated framework by placing a conditional autoregressive (CAR) prior on the parameters governing the temporal variability and imposing a conditional dependence of the temporal range on the temporal variance. We apply this model to two case studies: monthly malaria incidence in Niassa, Mozambique, and weekly food insecurity prevalence in Cameroon. Inference is conducted within a Bayesian framework using approximate posterior sampling. Given the hierarchical structure of the model, we employ a combination of Markov chain Monte Carlo (MCMC) techniques, including the Metropolis-adjusted Langevin algorithm (MALA), Metropolis-Hastings, and Gibbs sampling.
  In both applications, the model demonstrates strong in-sample performance with narrow credible intervals and outperforms established spatio-temporal approaches in many regions when forecasting. These results underscore the model's ability to capture complex spatio-temporal dependencies while maintaining interpretability, key in settings with sparse data and policy relevance. By accounting for spatio-temporal variation through the evolution of temporal dynamics themselves, our approach offers a flexible and principled tool for many applied contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01604v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Rozo Posada, Oswaldo Gressani, Christel Faes, James Colborn, Baltazar Candrinho, Emanuele Giorgi, Thomas Neyens</dc:creator>
    </item>
    <item>
      <title>Finite-Sample Non-Parametric Bounds with an Application to the Causal Effect of Workforce Gender Diversity on Firm Performance</title>
      <link>https://arxiv.org/abs/2509.01622</link>
      <description>arXiv:2509.01622v1 Announce Type: new 
Abstract: Classical Manski bounds identify average treatment effects under minimal assumptions but, in finite samples, assume that latent conditional expectations are bounded by the sample's own extrema or that the population extrema are known a priori -- often untrue in firm-level data with heavy tails. We develop a finite-sample, concentration-driven band (concATE) that replaces that assumption with a Dvoretzky--Kiefer--Wolfowitz tail bound, combines it with delta-method variance, and allocates size via Bonferroni. The band extends to a group-sequential design that controls the family-wise error when the first ``significant'' diversity threshold is data-chosen. Applied to 945 listed firms (2015 Q2--2022 Q1), concATE shows that senior-level gender diversity raises Tobin's Q once representation exceeds approximately 30\% in growth sectors and approximately 65\% in cyclical sectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01622v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grace Lordan, Kaveh Salehzadeh Nobari</dc:creator>
    </item>
    <item>
      <title>Evaluation of Surrogate Endpoints Based on Meta-Analysis with Surrogate Indices</title>
      <link>https://arxiv.org/abs/2509.01737</link>
      <description>arXiv:2509.01737v1 Announce Type: new 
Abstract: We introduce in this paper an extension of the meta-analytic (MA) framework for evaluating surrogate endpoints. While the MA framework is regarded as the gold standard for surrogate endpoint evaluation, it is limited in its ability to handle complex surrogates and does not take into account possible differences in the distribution of baseline covariates across trials. By contrast, in the context of data fusion, the surrogate-index (SI) framework accommodates complex surrogates and allows for complex relationships between baseline covariates, surrogates, and clinical endpoints. However, the SI framework is not a surrogate evaluation framework and relies on strong identifying assumptions. To address the MA framework's limitations, we propose an extension that incorporates ideas from the SI framework. We first formalize the data-generating mechanism underlying the MA framework, providing a transparent description of the untestable assumptions required for valid inferences in any evaluation of trial-level surrogacy -- assumptions often left implicit in the MA framework. While this formalization is meaningful in its own right, it is also required for our main contribution: We propose to estimate a specific transformation of the baseline covariates and the surrogate, the so-called surrogate index. This estimated transformation serves as a new potential univariate surrogate and is optimal in a trial-level surrogacy sense under certain conditions. We show that, under weak additional conditions, this new univariate surrogate can be evaluated as a trial-level surrogate as if the transformation were known a priori. This approach enables the evaluation of the trial-level surrogacy of complex surrogates and can be implemented using standard software. We illustrate this approach with a set of COVID-19 vaccine trials where antibody markers are assessed as potential trial-level surrogate endpoints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01737v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Stijven, Peter B. Gilbert</dc:creator>
    </item>
    <item>
      <title>Monitoring Time Series for Relevant Changes</title>
      <link>https://arxiv.org/abs/2509.01756</link>
      <description>arXiv:2509.01756v1 Announce Type: new 
Abstract: We consider the problem of sequentially testing for changes in the mean parameter of a time series, compared to a benchmark period. Most tests in the literature focus on the null hypothesis of a constant mean versus the alternative of a single change at an unknown time. Yet in many applications it is unrealistic that no change occurs at all, or that after one change the time series remains stationary forever. We introduce a new setup, modeling the sequence of means as a piecewise constant function with arbitrarily many changes. Instead of testing for a change, we ask whether the evolving sequence of means, say $(\mu_n)_{n \geq 1}$, stays within a narrow corridor around its initial value, that is, $\mu_n \in [\mu_1-\Delta, \mu_1+\Delta]$ for all $n \ge 1$. Combining elements from multiple change point detection with a H\"older-type monitoring procedure, we develop a new online monitoring tool. A key challenge in both construction and proof of validity is that the risk of committing a type-I error after any time $n$ fundamentally depends on the unknown future of the time series. Simulations support our theoretical results and we present two real-world applications: (1) healthcare monitoring, with a focus on blood glucose tracking, and (2) political consensus analysis via citizen opinion polls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01756v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Bastian, Tim Kutta, Rupsa Basu, Holger Dette</dc:creator>
    </item>
    <item>
      <title>Generalized Correlation Regression for Disentangling Dependence in Clustered Data</title>
      <link>https://arxiv.org/abs/2509.01774</link>
      <description>arXiv:2509.01774v1 Announce Type: new 
Abstract: Clustered and longitudinal data are pervasive in scientific studies, from prenatal health programs to clinical trials and public health surveillance. Such data often involve non-Gaussian responses--including binary, categorical, and count outcomes--that exhibit complex correlation structures driven by multilevel clustering, covariates, over-dispersion, or zero inflation. Conventional approaches such as mixed-effects models and generalized estimating equations (GEEs) can capture some of these dependencies, but they are often too rigid or impose restrictive assumptions that limit interpretability and predictive performance.
  We investigate \emph{generalized correlation regression} (GCR), a unified framework that models correlations directly as functions of interpretable covariates while simultaneously estimating marginal means. By applying a generalized $z$-transformation, GCR guarantees valid correlation matrices, accommodates unbalanced cluster sizes, and flexibly incorporates covariates such as time, space, or group membership into the dependence structure. Through applications to modern prenatal care, a longitudinal toenail infection trial, and clustered health count data, we show that GCR not only achieves superior predictive performance over standard methods, but also reveals family-, community-, and individual-level drivers of dependence that are obscured under conventional modeling. These results demonstrate the broad applied value of GCR for analyzing binary, count, and categorical data in clustered and longitudinal settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01774v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yibo Wang, Chenlei Leng, Cheng Yong Tang</dc:creator>
    </item>
    <item>
      <title>Wrong Model, Right Uncertainty: Spatial Associations for Discrete Data with Misspecification</title>
      <link>https://arxiv.org/abs/2509.01776</link>
      <description>arXiv:2509.01776v1 Announce Type: new 
Abstract: Scientists are often interested in estimating an association between a covariate and a binary- or count-valued response. For instance, public health officials are interested in how much disease presence (a binary response per individual) varies as temperature or pollution (covariates) increases. Many existing methods can be used to estimate associations, and corresponding uncertainty intervals, but make unrealistic assumptions in the spatial domain. For instance, they incorrectly assume models are well-specified. Or they assume the training and target locations are i.i.d. -- whereas in practice, these locations are often not even randomly sampled. Some recent work avoids these assumptions but works only for continuous responses with spatially constant noise. In the present work, we provide the first confidence intervals with guaranteed asymptotic nominal coverage for spatial associations given discrete responses, even under simultaneous model misspecification and nonrandom sampling of spatial locations. To do so, we demonstrate how to handle spatially varying noise, provide a novel proof of consistency for our proposed estimator, and use a delta method argument with a Lyapunov central limit theorem. We show empirically that standard approaches can produce unreliable confidence intervals and can even get the sign of an association wrong, while our method reliably provides correct coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01776v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David R. Burt, Renato Berlinghieri, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>hdMTD: An R Package for High-Dimensional Mixture Transition Distribution Models</title>
      <link>https://arxiv.org/abs/2509.01808</link>
      <description>arXiv:2509.01808v1 Announce Type: new 
Abstract: Several natural phenomena exhibit long-range conditional dependencies. High-order mixture transition distribution (MTD) are parsimonious non-parametric models to study these phenomena. An MTD is a Markov chain in which the transition probabilities are expressed as a convex combination of lower-order conditional distributions. Despite their generality, inference for MTD models has traditionally been limited by the need to estimate high-dimensional joint distributions. In particular, for a sample of size n, the feasible order d of the MTD is typically restricted to d approximately O(log n). To overcome this limitation, Ost and Takahashi (2023) recently introduced a computationally efficient non-parametric inference method that identifies the relevant lags in high-order MTD models, even when d is approximately O(n), provided that the set of relevant lags is sparse. In this article, we introduce hdMTD, an R package allowing us to estimate parameters of such high-dimensional Markovian models. Given a sample from an MTD chain, hdMTD can retrieve the relevant past set using the BIC algorithm or the forward stepwise and cut algorithm described in Ost and Takahashi (2023). The package also computes the maximum likelihood estimate for transition probabilities and estimates high-order MTD parameters through the expectation-maximization algorithm. Additionally, hdMTD also allows for simulating an MTD chain from its stationary invariant distribution using the perfect (exact) sampling algorithm, enabling Monte Carlo simulation of the model. We illustrate the package's capabilities through simulated data and a real-world application involving temperature records from Brazil.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01808v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maiara Gripp, Giulio Iacobelli, Guilherme Ost, Daniel Y. Takahashi</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification for Ranking with Heterogeneous Preferences</title>
      <link>https://arxiv.org/abs/2509.01847</link>
      <description>arXiv:2509.01847v1 Announce Type: new 
Abstract: This paper studies human preference learning based on partially revealed choice behavior and formulates the problem as a generalized Bradley-Terry-Luce (BTL) ranking model that accounts for heterogeneous preferences. Specifically, we assume that each user is associated with a nonparametric preference function, and each item is characterized by a low-dimensional latent feature vector - their interaction defines the underlying low-rank score matrix. In this formulation, we propose an indirect regularization method for collaboratively learning the score matrix, which ensures entrywise $\ell_\infty$-norm error control - a novel contribution to the heterogeneous preference learning literature. This technique is based on sieve approximation and can be extended to a broader class of binary choice models where a smooth link function is adopted. In addition, by applying a single step of the Newton-Raphson method, we debias the regularized estimator and establish uncertainty quantification for item scores and rankings of items, both for the aggregated and individual preferences. Extensive simulation results from synthetic and real datasets corroborate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01847v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianqing Fan, Hyukjun Kwon, Xiaonan Zhu</dc:creator>
    </item>
    <item>
      <title>generalRSS: Sampling and Inference for Balanced and Unbalanced Ranked Set Sampling in R</title>
      <link>https://arxiv.org/abs/2509.02039</link>
      <description>arXiv:2509.02039v1 Announce Type: new 
Abstract: Ranked set sampling (RSS) is a stratified sampling method that improves efficiency over simple random sampling (SRS) by utilizing auxiliary information for ranking and stratification. While balanced RSS (BRSS) assumes equal allocation across strata, unbalanced RSS (URSS) allows unequal allocation, making it particularly effective for skewed distributions. The generalRSS package provides extensive tools for both BRSS and URSS, addressing limitations in existing RSS software that primarily focus on balanced designs. It supports RSS data generation, efficient sample allocation strategies for URSS, and statistical inference for both balanced and unbalanced designs. This paper presents the RSS methodology and demonstrates the utility of generalRSS through two medical data applications: a one-sample mean inference and a two-sample area under the curve (AUC) comparison using NHANES datasets. These applications illustrate the practical implementation of URSS and show how generalRSS facilitates ranked set sampling and inference in real-world data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02039v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chul Moon, Soohyun Ahn</dc:creator>
    </item>
    <item>
      <title>Bias Correction in Factor-Augmented Regression Models with Weak Factors</title>
      <link>https://arxiv.org/abs/2509.02066</link>
      <description>arXiv:2509.02066v1 Announce Type: new 
Abstract: In this paper, we study the asymptotic bias of the factor-augmented regression estimator and its reduction, which is augmented by the $r$ factors extracted from a large number of $N$ variables with $T$ observations. In particular, we consider general weak latent factor models with $r$ signal eigenvalues that may diverge at different rates, $N^{\alpha _{k}}$, $0&lt;\alpha _{k}\leq 1$, $k=1,\dots,r$. In the existing literature, the bias has been derived using an approximation for the estimated factors with a specific data-dependent rotation matrix $\hat{H}$ for the model with $\alpha_{k}=1$ for all $k$, whereas we derive the bias for weak factor models. In addition, we derive the bias using the approximation with a different rotation matrix $\hat{H}_q$, which generally has a smaller bias than with $\hat{H}$. We also derive the bias using our preferred approximation with a purely signal-dependent rotation $H$, which is unique and can be regarded as the population version of $\hat{H}$ and $\hat{H}_q$. Since this bias is parametrically inestimable, we propose a split-panel jackknife bias correction, and theory shows that it successfully reduces the bias. The extensive finite-sample experiments suggest that the proposed bias correction works very well, and the empirical application illustrates its usefulness in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02066v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peiyun Jiang, Yoshimasa Uematsu, Takashi Yamagata</dc:creator>
    </item>
    <item>
      <title>Biomarkers selection and combination based on the weighted Youden index</title>
      <link>https://arxiv.org/abs/2509.02090</link>
      <description>arXiv:2509.02090v1 Announce Type: new 
Abstract: In clinical practice, multiple biomarkers are used for disease diagnosis, but their individual accuracies are often suboptimal, with only a few proving directly relevant. Effectively selecting and combining biomarkers can significantly improve diagnostic accuracy. Existing methods often optimize metrics like the Area Under the ROC Curve (AUC) or the Youden index. However, optimizing AUC does not yield estimates for optimal cutoff values, and the Youden index assumes equal weighting of sensitivity and specificity, which may not reflect clinical priorities where these metrics are weighted differently. This highlights the need for methods that can flexibly accommodate such requirements. In this paper, we present a novel framework for selecting and combining biomarkers to maximize a weighted version of the Youden index. We introduce a smoothed estimator based on the weighted Youden index and propose a penalized version using the SCAD penalty to enhance variable selection. To handle the non-convexity of the objective function and the non-smoothness of the penalty, we develop an efficient algorithm, also applicable to other non-convex optimization problems. Simulation studies demonstrate the performance and efficiency of our method, and we apply it to construct a diagnostic scale for dermatitis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02090v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ao Sun, Zhanwang Deng, Jiahui Zhao, Hang Li, Xiao-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>Bayesian Estimation and Regularization Techniques in Categorical Data Analysis</title>
      <link>https://arxiv.org/abs/2509.02222</link>
      <description>arXiv:2509.02222v1 Announce Type: new 
Abstract: This paper explores Bayesian estimation for categorical data, focusing on simple yet effective models that provide a foundation for applying more advanced methods accurately and reliably in real-world applications. We begin by revisiting Bayesian estimators for the binomial distribution and investigating their properties. Next, we develop hypothesis tests for categorical data (sign test, homogeneity test, symmetry test) based on regularized maximum likelihood estimates of the probabilities. Finally, we formulate regularized versions of common association measures for contingency tables and study the regularized version of mutual information, particular for the situation where the regularized version can effectively handle zero counts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02222v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Kalina</dc:creator>
    </item>
    <item>
      <title>Causal Spatial Quantile Regression</title>
      <link>https://arxiv.org/abs/2509.02294</link>
      <description>arXiv:2509.02294v1 Announce Type: new 
Abstract: Treatment effects in a wide range of economic, environmental, and epidemiological applications often vary across space, and understanding the heterogeneity of causal effects across space and outcome quantiles is a critical challenge in spatial causal inference. To effectively capture spatial heterogeneity in distributional treatment effects, we propose a novel semiparametric neural network-based causal framework leveraging deep spatial quantile regression and then construct a plug-in estimator for spatial quantile treatment effects (SQTE). This framework incorporates an efficient adjustment procedure to mitigate the impact of spatial hidden confounders. Extensive simulations across various scenarios demonstrate that our methodology can accurately estimate SQTE, even with the presence of spatial hidden confounders. Additionally, the spatial confounding adjustment procedure effectively reduces neighborhood spatial patterns in the residuals. We apply this method to assess the spatially varying quantile treatment effects of maternal smoking on newborn birth weight in North Carolina, United States. Our findings consistently show negative effects across all birth weight quantiles, with particularly severe impacts observed in the lower quantile regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02294v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Gong, Reetam Majumder, Brian J. Reich, Rapha\"el Huser</dc:creator>
    </item>
    <item>
      <title>A nonparametric Bayesian analysis of independent and identically distributed observations of covariate-driven Poisson processes</title>
      <link>https://arxiv.org/abs/2509.02299</link>
      <description>arXiv:2509.02299v1 Announce Type: new 
Abstract: An important task in the statistical analysis of inhomogeneous point processes is to investigate the influence of a set of covariates on the point-generating mechanism. In this article, we consider the nonparametric Bayesian approach to this problem, assuming that $n$ independent and identically distributed realizations of the point pattern and the covariate random field are available. In many applications, different covariates are often vastly diverse in physical nature, resulting in anisotropic intensity functions whose variations along distinct directions occur at different smoothness levels. To model this scenario, we employ hierarchical prior distributions based on multi-bandwidth Gaussian processes. We prove that the resulting posterior distributions concentrate around the ground truth at optimal rate as $n\to\infty$, and achieve automatic adaptation to the anisotropic smoothness. Posterior inference is concretely implemented via a Metropolis-within-Gibbs Markov chain Monte Carlo algorithm that incorporates a dimension-robust sampling scheme to handle the functional component of the proposed nonparametric Bayesian model. Our theoretical results are supported by extensive numerical simulation studies. Further, we present an application to the analysis of a Canadian wildfire dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02299v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patric Dolmeta, Matteo Giordano</dc:creator>
    </item>
    <item>
      <title>Confirmatory Adaptive Hypothesis Tests in Markovian Illness-Death Models</title>
      <link>https://arxiv.org/abs/2509.02315</link>
      <description>arXiv:2509.02315v1 Announce Type: new 
Abstract: Classic adaptive designs for time-to-event trials are based on the log-rank statistic and its increments. Thereby, only information from the time-to-event endpoint on which the selected log-rank statistic is based may be used for data-dependent design modifications in interim analyses. Further information (e.g. surrogate parameters) may not be used. As pointed out in a letter by P. Bauer and M. Posch in 2004, adaptive tests on overall survival (OS) based on the log-rank statistic do in general not control the significance level if interim information on progression-free survival (PFS) is used for sample size adjustments, because progression is associated with increased risk of death.
  In contrast, in adaptive designs for time-to-event trials, which are constructed according to the principle of patient-wise separation, all trial data observed in interim analyses may be used for design modifications without compromizing type one error rate control. But by design, this comes at the price of incomplete use of the primary endpoint data in the final test decision or worst-case considerations which lead to a loss of power. Thus, the patient-wise separation approach cannot be regarded as a general solution to the problem described by Bauer and Posch.
  We address this problem within the framework of a comprehensive independent increments approach. We develop adaptive tests on OS in which sample size adjustments may be based on the observed interim data of both OS and PFS, while avoiding the problems of the patient-wise separation approach. We provide this methodology for both single-arm trials, in which a new therapy is compared with a pre-specified deterministic reference, and randomized trials, in which a new therapy is compared with a concurrent control group. The underlying assumption is that the joint distribution of OS and PFS is induced by a Markovian illness-death model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02315v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rene Schmidt, Moritz Fabian Danzer</dc:creator>
    </item>
    <item>
      <title>Resampling-based multi-resolution false discovery exceedance control</title>
      <link>https://arxiv.org/abs/2509.02376</link>
      <description>arXiv:2509.02376v1 Announce Type: new 
Abstract: MaxT is a highly popular resampling-based multiple testing procedure, which controls the familywise error rate (FWER) and is powerful under dependence. This paper generalizes maxT to what we term ``multi-resolution'' False Discovery eXceedance (FDX) control. FDX control means ensuring that the FDP -- the proportion of false discoveries among all rejections -- is at most $\gamma$ with probability at least $1-\alpha$. Here $\gamma$ and $\alpha$ are prespecified, small values between 0 and 1. If we take $\gamma=0$, FDX control is the same as FWER control. When $\gamma=0$, the new procedure reduces to maxT. For $\gamma&gt;0$, our method has much higher power. Our method is then strongly connected to maxT as well. For example, if our method rejects fewer than $\gamma^{-1}$ hypotheses, then maxT rejects the same. Further, the implied global tests of the two methods are the same, although the implied closed testing procedures differ. Finally, our method provides an easy-to-use simultaneous, multi-resolution guarantee: the procedure outputs a single rejection threshold $q$, but ensures that with probability $1-\alpha$, simultaneously over all stricter thresholds, the corresponding FDPs are also below $\gamma$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02376v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesse Hemerik</dc:creator>
    </item>
    <item>
      <title>Multiomics Tissue Segmentation via Spatially-Informed Nested Biclustering Methods</title>
      <link>https://arxiv.org/abs/2509.02482</link>
      <description>arXiv:2509.02482v1 Announce Type: new 
Abstract: Matrix-Assisted Laser Desorption/Ionisation Mass Spectrometry Imaging (MSI) is a powerful technique for spatially resolved molecular profiling and cancer biomarker discovery. Recent advances, including a novel multiomics workflow, enable multiple rounds of MSI on the same tissue section, extracting diverse molecular classes, e.g., lipids, peptides, and N-glycans, while preserving spatial resolution. This innovation is particularly valuable for studies with limited tissue sample availability, such as rare diseases or small tumors. However, the resulting data are high-dimensional, spatially structured, and the various molecular types share the same pixel grid. To address these challenges, we propose Poseidon, a Bayesian nonparametric nested biclustering model that simultaneously segments the common tissue pixels and clusters molecular signals within each molecular class, leveraging the shared spatial structure. A separately exchangeable framework is first considered, and then extended to handle spatial data via hidden Markov random fields. For scalability, we implement an efficient mean-field variational inference algorithm tailored for multi-dataset analysis. After validating the efficacy of our method on simulated scenarios, we demonstrate the applicability of our model in a real-world case study, where multiomics measurements were performed on kidney tissue affected by clear cell renal cell carcinoma. The nested, hierarchical structure of Poseidon, combined with its principled inferential framework, allows the extraction of interesting biological insights, such as clear tissue segmentation and biomarker detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02482v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Denti, Cecilia Balocchi, Vanna Denti, Giulia Capitoli</dc:creator>
    </item>
    <item>
      <title>Bringing Closure to False Discovery Rate Control: A General Principle for Multiple Testing</title>
      <link>https://arxiv.org/abs/2509.02517</link>
      <description>arXiv:2509.02517v1 Announce Type: new 
Abstract: We present a novel necessary and sufficient principle for multiple testing methods controlling an expected loss. This principle asserts that every such multiple testing method is a special case of a general closed testing procedure based on e-values. It generalizes the Closure Principle, known to underlie all methods controlling familywise error and tail probabilities of false discovery proportions, to a large class of error rates -- in particular to the false discovery rate (FDR). By writing existing methods as special cases of this procedure, we can achieve uniform improvements existing multiple testing methods such as the e-Benjamini-Hochberg and the Benjamini-Yekutieli procedures, and the self-consistent method of Su (2018). We also show that methods derived using the closure principle have several valuable properties. For example, they generally control their error rate not just for one rejected set, but simultaneously over many, allowing post hoc flexibility for the researcher. Moreover, we show that because all multiple testing methods for all error metrics are derived from the same procedure, researchers may even choose the error metric post hoc. Under certain conditions, this flexibility even extends to post hoc choice of the nominal error rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02517v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyu Xu, Aldo Solari, Lasse Fischer, Rianne de Heide, Aaditya Ramdas, Jelle Goeman</dc:creator>
    </item>
    <item>
      <title>Probit Monotone BART</title>
      <link>https://arxiv.org/abs/2509.00263</link>
      <description>arXiv:2509.00263v1 Announce Type: cross 
Abstract: Bayesian Additive Regression Trees (BART) of Chipman et al. (2010) has proven to be a powerful tool for nonparametric modeling and prediction. Monotone BART (Chipman et al., 2022) is a recent development that allows BART to be more precise in estimating monotonic functions. We further these developments by proposing probit monotone BART, which allows the monotone BART framework to estimate conditional mean functions when the outcome variable is binary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00263v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jared D. Fisher</dc:creator>
    </item>
    <item>
      <title>SANVI: A Fast Spectral-Assisted Network Variational Inference Method with an Extended Surrogate Likelihood Function</title>
      <link>https://arxiv.org/abs/2509.00562</link>
      <description>arXiv:2509.00562v1 Announce Type: cross 
Abstract: Bayesian inference has been broadly applied to statistical network analysis, but suffers from the expensive computational costs due to the nature of Markov chain Monte Carlo sampling algorithms. This paper proposes a novel and computationally efficient Spectral-Assisted Network Variational Inference (SANVI) method within the framework of the generalized random dot product graph. The key idea is a cleverly designed extended surrogate likelihood function that enjoys two convenient features. Firstly, it decouples the generalized inner product of latent positions in the random graph model. Secondly, it relaxes the complicated domain of the original likelihood function to the entire Euclidean space. Leveraging these features, we design a computationally efficient Gaussian variational inference algorithm via stochastic gradient descent. Furthermore, we show the asymptotic efficiency of the maximum extended surrogate likelihood estimator and the Bernstein-von Mises limit of the variational posterior distribution. Through extensive numerical studies, we demonstrate the usefulness of the proposed SANVI algorithm compared to the classical Markov chain Monte Carlo algorithm, including comparable estimation accuracy for the latent positions and less computational costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00562v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dingbo Wu, Fangzheng Xie</dc:creator>
    </item>
    <item>
      <title>ProCause: Generating Counterfactual Outcomes to Evaluate Prescriptive Process Monitoring Methods</title>
      <link>https://arxiv.org/abs/2509.00797</link>
      <description>arXiv:2509.00797v1 Announce Type: cross 
Abstract: Prescriptive Process Monitoring (PresPM) is the subfield of Process Mining that focuses on optimizing processes through real-time interventions based on event log data. Evaluating PresPM methods is challenging due to the lack of ground-truth outcomes for all intervention actions in datasets. A generative deep learning approach from the field of Causal Inference (CI), RealCause, has been commonly used to estimate the outcomes for proposed intervention actions to evaluate a new policy. However, RealCause overlooks the temporal dependencies in process data, and relies on a single CI model architecture, TARNet, limiting its effectiveness. To address both shortcomings, we introduce ProCause, a generative approach that supports both sequential (e.g., LSTMs) and non-sequential models while integrating multiple CI architectures (S-Learner, T-Learner, TARNet, and an ensemble). Our research using a simulator with known ground truths reveals that TARNet is not always the best choice; instead, an ensemble of models offers more consistent reliability, and leveraging LSTMs shows potential for improved evaluations when temporal dependencies are present. We further validate ProCause's practical effectiveness through a real-world data analysis, ensuring a more reliable evaluation of PresPM methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00797v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jakob De Moor, Hans Weytjens, Johannes De Smedt</dc:creator>
    </item>
    <item>
      <title>Causal SHAP: Feature Attribution with Dependency Awareness through Causal Discovery</title>
      <link>https://arxiv.org/abs/2509.00846</link>
      <description>arXiv:2509.00846v1 Announce Type: cross 
Abstract: Explaining machine learning (ML) predictions has become crucial as ML models are increasingly deployed in high-stakes domains such as healthcare. While SHapley Additive exPlanations (SHAP) is widely used for model interpretability, it fails to differentiate between causality and correlation, often misattributing feature importance when features are highly correlated. We propose Causal SHAP, a novel framework that integrates causal relationships into feature attribution while preserving many desirable properties of SHAP. By combining the Peter-Clark (PC) algorithm for causal discovery and the Intervention Calculus when the DAG is Absent (IDA) algorithm for causal strength quantification, our approach addresses the weakness of SHAP. Specifically, Causal SHAP reduces attribution scores for features that are merely correlated with the target, as validated through experiments on both synthetic and real-world datasets. This study contributes to the field of Explainable AI (XAI) by providing a practical framework for causal-aware model explanations. Our approach is particularly valuable in domains such as healthcare, where understanding true causal relationships is critical for informed decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00846v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Woon Yee Ng, Li Rong Wang, Siyuan Liu, Xiuyi Fan</dc:creator>
    </item>
    <item>
      <title>Suicide Mortality in Spain (2010-2022): Temporal Trends, Spatial Patterns, and Risk Factors</title>
      <link>https://arxiv.org/abs/2509.01342</link>
      <description>arXiv:2509.01342v1 Announce Type: cross 
Abstract: This paper investigates the spatial and temporal patterns of age-stratified suicide mortality rates in Spanish provinces from 2010 to 2022. We use mixed Poisson models to analyze these patterns and to determine the association between mortality rates and various socioeconomic and contextual factors, while controlling for spatial and temporal confounding effects. Our results indicate that a 10% increase in the proportion of people residing in rural areas is associated with an increase of over 5% in male suicide mortality. In addition, a 1% increase in the annual unemployment rate is linked to a 2.4% increase in female suicide mortality. An important finding is that despite male suicide rates consistently being higher than female rates, we observe a notable and steady upward trend in female suicide mortality over the study period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01342v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Adin, G. Retegui, A. S\'anchez Villegas, M. D. Ugarte</dc:creator>
    </item>
    <item>
      <title>Handling Sparse Non-negative Data in Finance</title>
      <link>https://arxiv.org/abs/2509.01478</link>
      <description>arXiv:2509.01478v1 Announce Type: cross 
Abstract: We show that Poisson regression, though often recommended over log-linear regression for modeling count and other non-negative variables in finance and economics, can be far from optimal when heteroskedasticity and sparsity -- two common features of such data -- are both present. We propose a general class of moment estimators, encompassing Poisson regression, that balances the bias-variance trade-off under these conditions. A simple cross-validation procedure selects the optimal estimator. Numerical simulations and applications to corporate finance data reveal that the best choice varies substantially across settings and often departs from Poisson regression, underscoring the need for a more flexible estimation framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01478v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Agostino Capponi, Zhaonan Qu</dc:creator>
    </item>
    <item>
      <title>Cohort-Anchored Robust Inference for Event-Study with Staggered Adoption</title>
      <link>https://arxiv.org/abs/2509.01829</link>
      <description>arXiv:2509.01829v1 Announce Type: cross 
Abstract: I propose a cohort-anchored framework for robust inference in event studies with staggered adoption. Robust inference based on aggregated event-study coefficients, as in Rambachan and Roth (2023), can be misleading because pre- and post-treatment coefficients are identified from different cohort compositions and the not-yet-treated control group changes over time. To address these issues, I work at the cohort-period level and introduce the \textit{block bias}-the parallel-trends violation for a cohort relative to its anchored initial control group-whose interpretation is consistent across pre- and post-treatment periods. For both the imputation estimator and the estimator in Callaway and Sant'Anna (2021) that uses not-yet-treated units as controls, I show an invertible decomposition linking these estimators' biases in post-treatment periods to block biases. This allows researchers to place transparent restrictions on block biases (e.g., Relative Magnitudes and Second Differences) and conduct robust inference using the algorithm from Rambachan and Roth (2023). In simulations, when parallel-trends violations differ across cohorts, my framework yields better-centered (and sometimes narrower) confidence sets than the aggregated approach. In a reanalysis of the effect of minimum-wage changes on teen employment in the Callaway and Sant'Anna (2021) application, my inference framework with the Second Differences restriction yields confidence sets centered well below zero, indicating robust negative effects, whereas inference based on aggregated coefficients yields sets centered near zero. The proposed framework is most useful when there are several cohorts, adequate within-cohort precision, and substantial cross-cohort heterogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01829v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyi Liu</dc:creator>
    </item>
    <item>
      <title>Non-Linear Model-Based Sequential Decision-Making in Agriculture</title>
      <link>https://arxiv.org/abs/2509.01924</link>
      <description>arXiv:2509.01924v1 Announce Type: cross 
Abstract: Sequential decision-making is central to sustainable agricultural management and precision agriculture, where resource inputs must be optimized under uncertainty and over time. However, such decisions must often be made with limited observations, whereas classical bandit and reinforcement learning approaches typically rely on either linear or black-box reward models that may misrepresent domain knowledge or require large amounts of data. We propose a family of nonlinear, model-based bandit algorithms that embed domain-specific response curves directly into the exploration-exploitation loop. By coupling (i) principled uncertainty quantification with (ii) closed-form or rapidly computable profit optima, these algorithms achieve sublinear regret and near-optimal sample complexity while preserving interpretability. Theoretical analysis establishes regret and sample complexity bounds, and extensive simulations emulating real-world fertilizer-rate decisions show consistent improvements over both linear and nonparametric baselines (such as linear UCB and $k$-NN UCB) in the low-sample regime, under both well-specified and shape-compatible misspecified models. Because our approach leverages mechanistic insight rather than large data volumes, it is especially suited to resource-constrained settings, supporting sustainable, inclusive, and transparent sequential decision-making across agriculture, environmental management, and allied applications. This methodology directly contributes to SDG 2 (Zero Hunger) and SDG 12 (Responsible Consumption and Production) by enabling data-driven, less wasteful agricultural practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01924v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sakshi Arya, Wentao Lin</dc:creator>
    </item>
    <item>
      <title>Mediation Analysis in the Presence of Sample Selection Bias with an Application to Disparities in Liver Transplantation Listing</title>
      <link>https://arxiv.org/abs/2509.01969</link>
      <description>arXiv:2509.01969v1 Announce Type: cross 
Abstract: The study of disparities in the liver transplantation process may focus on quantifying causal effects, particularly the average, direct, or indirect effects of various social determinants of health on being listed as a candidate for transplant. Selection bias arises when the data sample does not represent the target population, defined here as all individuals referred to the transplant clinic. Listing decisions are made for the subset of patients who complete the evaluation process, who may differ systematically from the referred population. There is evidence that selection is associated with patient characteristics that also impact outcomes. Using data only from the selected population may yield biased causal effect estimates. However, incorporating data from the referred population allows for analytic correction. This correction leverages hypothesized causal relationships among selection, the outcome (getting listed), exposures, and mediators. Using directed acyclic graphs (DAGs), we establish graphical conditions under which a reweighted mediation formula identifies effect of interest - direct, indirect, and path-specific effects - in the presence of sample selection. In a clinical case study, we investigate mediated and direct effects of a patient's socioeconomic position on being listed for transplant, allowing selection to depend on race, gender, age, and other social determinants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01969v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zain Khan, Lynnette Sequeira, Alexandra T. Strauss, Vedant Jain, Juliette Dixon, Eric Moughames, Tyrus Vong, Daniel Malinsky</dc:creator>
    </item>
    <item>
      <title>DEViaN-LM: An R Package for Detecting Abnormal Values in the Gaussian Linear Model</title>
      <link>https://arxiv.org/abs/2509.02202</link>
      <description>arXiv:2509.02202v1 Announce Type: cross 
Abstract: The DEViaN-LM is a R package that allows to detect the values poorly explained by a Gaussian linear model. The procedure is based on the maximum of the absolute value of the studentized residuals, which is a free statistic of the parameters of the model. This approach makes it possible to generalize several procedures used to detect abnormal values during longitudinal monitoring of certain biological markers. In this article, we describe the method used, and we show how to implement it on different real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02202v1</guid>
      <category>cs.MS</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Geoffroy Berthelot (IRMES - URP\_7329, RELAIS), Guillaume Sauli\`ere (IRMES - URP\_7329), J\'er\^ome Dedecker (MAP5 - UMR 8145)</dc:creator>
    </item>
    <item>
      <title>M-Estimation based on quasi-processes from discrete samples of Levy processes</title>
      <link>https://arxiv.org/abs/2112.08199</link>
      <description>arXiv:2112.08199v4 Announce Type: replace 
Abstract: We propose a novel estimation framework for path-dependent functionals of Levy processes from discretely observed data. Traditional approaches rely on Monte Carlo simulation of full paths, which requires complete model specification and heavy computation. In contrast, our quasi-process method constructs pseudo-paths directly from observed increments by random permutation, preserving the increment distribution while enabling repeated evaluation of functionals. Under a high-frequency, long-term sampling regime, we establish weak convergence of the quasi-process to the true Levy process and prove consistency and asymptotic normality of the resulting $M$-estimator. This bootstrap-like approach provides a practical and computationally efficient tool for inference from a single trajectory and offers promising extensions to multivariate modeling, machine learning integration, and risk management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.08199v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yasutaka Shimizu, Hiroshi Shiraishi</dc:creator>
    </item>
    <item>
      <title>On the Wasserstein median of probability measures</title>
      <link>https://arxiv.org/abs/2209.03318</link>
      <description>arXiv:2209.03318v5 Announce Type: replace 
Abstract: The primary choice to summarize a finite collection of random objects is by using measures of central tendency, such as mean and median. In the field of optimal transport, the Wasserstein barycenter corresponds to the Fr\'{e}chet or geometric mean of a set of probability measures, which is defined as a minimizer of the sum of squared distances to each element in a given set with respect to the Wasserstein distance of order 2. We introduce the Wasserstein median as a robust alternative to the Wasserstein barycenter. The Wasserstein median corresponds to the Fr\'{e}chet median under the 2-Wasserstein metric. The existence and consistency of the Wasserstein median are first established, along with its robustness property. In addition, we present a general computational pipeline that employs any recognized algorithms for the Wasserstein barycenter in an iterative fashion and demonstrate its convergence. The utility of the Wasserstein median as a robust measure of central tendency is demonstrated using real and simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.03318v5</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1080/10618600.2024.2374580</arxiv:DOI>
      <dc:creator>Kisung You, Dennis Shung, Mauro Giuffr\`e</dc:creator>
    </item>
    <item>
      <title>Distance and Kernel-Based Measures for Global and Local Two-Sample Conditional Distribution Testing</title>
      <link>https://arxiv.org/abs/2210.08149</link>
      <description>arXiv:2210.08149v3 Announce Type: replace 
Abstract: Testing the equality of two conditional distributions is crucial in various modern applications, including transfer learning and causal inference. Despite its importance, this fundamental problem has received surprisingly little attention in the literature, with existing works focusing exclusively on global two-sample conditional distribution testing. Based on distance and kernel methods, this paper presents the first unified framework for both global and local two-sample conditional distribution testing. To this end, we introduce distance and kernel-based measures that characterize the homogeneity of two conditional distributions. Drawing from the concept of conditional U-statistics, we propose consistent estimators for these measures. Theoretically, we derive the convergence rates and the asymptotic distributions of the estimators under both the null and alternative hypotheses. Utilizing these measures, along with a local bootstrap approach, we develop global and local tests that can detect discrepancies between two conditional distributions at global and local levels, respectively. Our tests demonstrate reliable performance through simulations and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.08149v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian Yan, Zhuoxi Li, Xianyang Zhang</dc:creator>
    </item>
    <item>
      <title>Adaptive greedy forward variable selection for linear regression models with incomplete data using multiple imputation</title>
      <link>https://arxiv.org/abs/2210.10967</link>
      <description>arXiv:2210.10967v3 Announce Type: replace 
Abstract: Variable selection is crucial for sparse modeling in this age of big data. Missing values are common in data, and make variable selection more complicated. The approach of multiple imputation (MI) results in multiply imputed datasets for missing values, and has been widely applied in various variable selection procedures. However, directly performing variable selection on the whole MI data or bootstrapped MI data may not be worthy in terms of computation cost. To fast identify the active variables in the linear regression model, we propose the adaptive grafting procedure with three pooling rules on MI data. The proposed methods proceed iteratively, which starts from finding the active variables based on the complete case subset and then expand the working data matrix with both the number of active variables and available observations. A comprehensive simulation study shows the selection accuracy in different aspects and computational efficiency of the proposed methods. Two real-life examples illustrate the strength of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.10967v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yong-Shiuan Lee</dc:creator>
    </item>
    <item>
      <title>Statistical Performance Guarantee for Subgroup Identification with Generic Machine Learning</title>
      <link>https://arxiv.org/abs/2310.07973</link>
      <description>arXiv:2310.07973v3 Announce Type: replace 
Abstract: Across a wide array of disciplines, many researchers use machine learning (ML) algorithms to identify a subgroup of individuals who are likely to benefit from a treatment the most (``exceptional responders'') or those who are harmed by it. A common approach to this subgroup identification problem consists of two steps. First, researchers estimate the conditional average treatment effect (CATE) using an ML algorithm. Next, they use the estimated CATE to select those individuals who are predicted to be most affected by the treatment, either positively or negatively. Unfortunately, CATE estimates are often biased and noisy. In addition, utilizing the same data to both identify a subgroup and estimate its group average treatment effect results in a multiple testing problem. To address these challenges, we develop uniform confidence bands for estimation of the group average treatment effect sorted by generic ML algorithm (GATES). Using these uniform confidence bands, researchers can identify, with a statistical guarantee, a subgroup whose GATES exceeds a certain effect size, regardless of how this effect size is chosen. The validity of the proposed methodology depends solely on randomization of treatment and random sampling of units. Importantly, our method does not require modeling assumptions and avoids a computationally intensive resampling procedure. A simulation study shows that the proposed uniform confidence bands are reasonably informative and have an appropriate empirical coverage even when the sample size is as small as 100. We analyze a clinical trial of late-stage prostate cancer and find a relatively large proportion of exceptional responders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07973v3</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Lingzhi Li, Kosuke Imai</dc:creator>
    </item>
    <item>
      <title>Semi-supervised linear regression: enhancing efficiency and robustness in high dimensions</title>
      <link>https://arxiv.org/abs/2311.17685</link>
      <description>arXiv:2311.17685v2 Announce Type: replace 
Abstract: In semi-supervised learning, the prevailing understanding suggests that observing additional unlabeled samples improves estimation accuracy for linear parameters only in the case of model misspecification. In this work, we challenge such a claim and show that additional unlabeled samples are beneficial in high-dimensional settings. Initially focusing on a dense scenario, we introduce robust semi-supervised estimators for the regression coefficient without relying on sparse structures in the population slope. Even when the true underlying model is linear, we show that leveraging information from large-scale unlabeled data helps reduce estimation bias, thereby improving both estimation accuracy and inference robustness. Moreover, we propose semi-supervised methods with further enhanced efficiency in scenarios with a sparse linear slope. The performance of the proposed methods is demonstrated through extensive numerical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17685v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/biomtc/ujaf113</arxiv:DOI>
      <arxiv:journal_reference>Biometrics, Volume 81, Issue 3, September 2025, ujaf113</arxiv:journal_reference>
      <dc:creator>Kai Chen, Yuqian Zhang</dc:creator>
    </item>
    <item>
      <title>Spectral estimation for spatial point processes and random fields</title>
      <link>https://arxiv.org/abs/2312.10176</link>
      <description>arXiv:2312.10176v3 Announce Type: replace 
Abstract: Spatial variables can be observed in many different forms, such as regularly sampled random fields (lattice data), point processes, and randomly sampled spatial processes. Joint analysis of such collections of observations is clearly desirable, but complicated by the lack of an easily implementable analysis framework. We fill this gap by providing a multitaper analysis framework using coupled discrete and continuous data tapers, combined with the discrete Fourier transform for inference. Using this set of tools is important, as it forms the backbone for practical spectral analysis. In higher dimensions it is important not to be constrained to Cartesian product domains, and so we develop the methodology for spectral analysis using irregular domain data tapers, and the tapered discrete Fourier transform. We discuss its fast implementation, and the asymptotic as well as large finite domain properties. Estimators of partial association between different spatial processes are provided as are principled methods to determine their significance, and we demonstrate their practical utility on a large-scale ecological dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10176v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jake P. Grainger, Tuomas A. Rajala, David J. Murrell, Sofia C. Olhede</dc:creator>
    </item>
    <item>
      <title>False Discovery Rate and Localizing Power</title>
      <link>https://arxiv.org/abs/2401.03554</link>
      <description>arXiv:2401.03554v2 Announce Type: replace 
Abstract: False discovery rate (FDR) is commonly used for correction for multiple testing in neuroimaging studies. However, when using two-tailed tests, making directional inferences about the results can lead to vastly inflated error rate, even approaching 100% in some cases. This happens because FDR control the error rate only globally, over all tests, not within subsets, such as among those in only one or another direction. Here we consider and evaluate different strategies for FDR control with two-tailed tests, using both synthetic and real imaging data. Approaches that separate the tests by direction of the hypothesis test, or by the direction of the resulting test statistic, more properly control the directional error rate and preserve FDR benefits, albeit with a doubled risk of errors under complete absence of signal. Strategies that combine tests in both directions, or that use simple two-tailed p-values, can lead to invalid directional conclusions, even if these tests remain globally valid. To enable valid thresholding for directional inference, we suggest that imaging software should allow the possibility that the user sets asymmetrical thresholds for the two sides of the statistical map. While FDR continues to be a valid, powerful procedure for multiple testing correction, care is needed when making directional inferences for two-tailed tests, or more broadly, when making any localized inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03554v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anderson M. Winkler, Paul A. Taylor, Thomas E. Nichols, Chris Rorden</dc:creator>
    </item>
    <item>
      <title>Combining Evidence Across Filtrations</title>
      <link>https://arxiv.org/abs/2402.09698</link>
      <description>arXiv:2402.09698v4 Announce Type: replace 
Abstract: In sequential anytime-valid inference, any admissible procedure must be based on e-processes: generalizations of test martingales that quantify the accumulated evidence against a composite null hypothesis at any stopping time. This paper proposes a method for combining e-processes constructed in different filtrations but for the same null. Although e-processes in the same filtration can be combined effortlessly (by averaging), e-processes in different filtrations cannot because their validity in a coarser filtration does not translate to a finer filtration. This issue arises in sequential tests of randomness and independence, as well as in the evaluation of sequential forecasters. We establish that a class of functions called adjusters can lift arbitrary e-processes across filtrations. The result yields a generally applicable "adjust-then-combine" procedure, which we demonstrate on the problem of testing randomness in real-world financial data. Furthermore, we prove a characterization theorem for adjusters that formalizes a sense in which using adjusters is necessary. There are two major implications. First, if we have a powerful e-process in a coarsened filtration, then we readily have a powerful e-process in the original filtration. Second, when we coarsen the filtration to construct an e-process, there is a logarithmic cost to recovering validity in the original filtration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09698v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yo Joong Choe, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Multilevel functional distributional models with application to continuous glucose monitoring in diabetes clinical trials</title>
      <link>https://arxiv.org/abs/2403.10514</link>
      <description>arXiv:2403.10514v2 Announce Type: replace 
Abstract: Continuous glucose monitoring (CGM) is a minimally invasive technology that measures blood glucose every few minutes for weeks or months at a time. CGM data are often collected in the free-living environment and is strongly related to sleep, physical activity and meal intake. As the timing of these activities varies substantially within- and between-individuals, it is difficult to model CGM trajectories as a function of time of day. Therefore, in practice, CGM trajectories are often reduced to one or two scalar summaries of the thousands of measurements collected for a study participant. To alleviate the potential loss of information, the cumulative distribution function (cdf) of the CGM time series was proposed as an alternative. Here we address the problem of conducting inference on cdfs in clinical trials with long follow up and frequent measurements. Our approach provides three major innovations: (1) modeling the entire cdf and preserving its monotonicity; (2) accounting for the cdfs correlation (because they are measured on the same individual), continuity (results are robust to the choice of the probability grid), and differential error (e.g., medians have lower variability than $0.99$ quantiles); and (3) preserving the family-wise error when the observed data are longitudinal samples of cdfs. We focus on modeling data collected by The Juvenile Diabetes Research Foundation Continuous Glucose Monitoring Group in a large clinical trial that collected CGM data every few minutes for 26 weeks. Our basic observation unit is the distribution of CGM observations in a four--week interval. The scientific goals are to: (1) identify and quantify the effects of factors that affect glycaemic control in type 1 diabetes patients (T1D); and (2) identify and characterize the patients who respond to treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10514v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcos Matabuena, Ciprian M. Crainiceanu</dc:creator>
    </item>
    <item>
      <title>Assessing Vaccine Effectiveness in Observational Studies via Nested Trial Emulation</title>
      <link>https://arxiv.org/abs/2403.18115</link>
      <description>arXiv:2403.18115v2 Announce Type: replace 
Abstract: Observational data are often used to estimate real-world effectiveness and durability of vaccines. A sequence of trials can be emulated to draw inference from such data while minimizing selection bias, immortal time bias, and confounding. Typically, when nested trial emulation (NTE) is employed, effect estimates are pooled across trials. However, such pooled estimates may lack a clear interpretation when the treatment effect is heterogeneous across trials. For vaccines against certain viruses, vaccine effectiveness may vary over calendar time due to newly emerging variants of the virus. This manuscript considers a NTE inverse probability weighted estimator of vaccine effectiveness that may vary over calendar time, time since vaccination, or both. Statistical testing of the trial effect homogeneity assumption is considered. As observed changes in vaccine effectiveness across trials may be attributable to variation in covariate distributions across trial-eligible populations, standardization of trial-specific inferences is also considered. Simulation studies are presented examining the finite-sample performance of the proposed methods under a variety of scenarios. The methods are used to estimate vaccine effectiveness against COVID-19 outcomes using observational data on over 110,000 residents of Abruzzo, Italy during 2021.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18115v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justin B. DeMonte, Bonnie E. Shook-Sa, Michael G. Hudgens</dc:creator>
    </item>
    <item>
      <title>Jacobi Prior: An Alternative Bayesian Method for Supervised Learning</title>
      <link>https://arxiv.org/abs/2404.11345</link>
      <description>arXiv:2404.11345v4 Announce Type: replace 
Abstract: The Jacobi prior offers an alternative Bayesian framework for predictive modelling, designed to achieve superior computational efficiency without compromising predictive performance. This scalable method is suitable for image classification and other computationally intensive tasks. Compared to widely used methods such as Lasso, Ridge, Elastic Net, uniLasso, the MCMC-based Horseshoe prior, and non-Bayesian machine learning methods including Support Vector Machines (SVM), Random Forests, and Extreme Gradient Boosting (XGBoost), the Jacobi prior achieves competitive or better accuracy with significantly reduced computational cost. The method is well suited to distributed computing environments, as it naturally accommodates partitioned data across multiple servers. We propose a parallelisable Monte Carlo algorithm to quantify the uncertainty in the estimated coefficients. We establish the theoretical foundations of the Jacobi estimator by studying its asymptotic properties. In particular, we prove a Bernstein--von Mises theorem for the Jacobi posterior. To demonstrate its practical utility, we conduct a comprehensive simulation study comprising seven experiments focused on statistical consistency, prediction accuracy, scalability, sensitivity analysis and robustness study. In the spine classification task, we extract last-layer features from a fine-tuned ResNet-50 model and evaluate multiple classifiers, including Jacobi-Multinomial logit regression, SVM, and Random Forest. The Jacobi prior achieves state-of-the-art results in recall and predictive stability, especially when paired with domain-specific features. This highlights its potential for scalable, high-dimensional learning in medical image analysis.
  All code and datasets used in this paper are available at: https://github.com/sourish-cmi/Jacobi-Prior/</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11345v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sourish Das, Shouvik Sardar</dc:creator>
    </item>
    <item>
      <title>Principal Component Analysis and biplots. A Back-to-Basics Comparison of Implementations</title>
      <link>https://arxiv.org/abs/2404.15115</link>
      <description>arXiv:2404.15115v2 Announce Type: replace 
Abstract: Principal Component Analysis and biplots are so well-established and readily implemented that it is just too tempting to give for granted their internal workings. In this note I get back to basics in comparing how PCA and biplots are implemented in base-R and contributed R packages, leveraging an implementation-agnostic understanding of the computational structure of each technique. I do so with a view to illustrating discrepancies that users might find elusive, as these arise from seemingly innocuous computational choices made under the hood. The proposed evaluation grid elevates aspects that are usually disregarded, including relationships that should hold if the computational rationale underpinning each technique is followed correctly. Strikingly, what is expected from these equivalences rarely follows without caveats from the output of specific implementations alone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15115v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11135-025-02266-9</arxiv:DOI>
      <arxiv:journal_reference>R. Qual Quant (2025)</arxiv:journal_reference>
      <dc:creator>Ettore Settanni</dc:creator>
    </item>
    <item>
      <title>Robust integration of external control data in randomized trials</title>
      <link>https://arxiv.org/abs/2406.17971</link>
      <description>arXiv:2406.17971v3 Announce Type: replace 
Abstract: One approach for increasing the efficiency of randomized trials is the use of "external controls" -- individuals who received the control treatment studied in the trial during routine practice or in prior experimental studies. Existing external control methods, however, can be biased if the populations underlying the trial and the external control data are not exchangeable. Here, we characterize a randomization-aware class of treatment effect estimators in the population underlying the trial that remain consistent and asymptotically normal when using external control data, even when exchangeability does not hold. We consider two members of this class of estimators: the well-known augmented inverse probability weighting trial-only estimator, which is the efficient estimator when only trial data are used; and a potentially more efficient member of the class when exchangeability holds and external control data are available, which we refer to as the optimized randomization-aware estimator. To achieve robust integration of external control data in trial analyses, we then propose a combined estimator based on the efficient trial-only estimator and the optimized randomization-aware estimator. We show that the combined estimator is consistent and no less efficient than the most efficient of the two component estimators, whether the exchangeability assumption holds or not. We examine the estimators' performance in simulations and we illustrate their use with data from two trials of paliperidone extended-release for schizophrenia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17971v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rickard Karlsson, Guanbo Wang, Piersilvio De Bartolomeis, Jesse H. Krijthe, Issa J. Dahabreh</dc:creator>
    </item>
    <item>
      <title>Metric Oja Depth, New Statistical Tool for Estimating the Most Central Objects</title>
      <link>https://arxiv.org/abs/2411.11580</link>
      <description>arXiv:2411.11580v2 Announce Type: replace 
Abstract: The Oja depth (simplicial volume depth) is one of the classical statistical techniques for measuring the central tendency of data in multivariate space. Despite the widespread emergence of object data like images, texts, matrices or graphs, a well-developed and suitable version of Oja depth for object data is lacking. To address this shortcoming, a novel measure of statistical depth, the metric Oja depth applicable to any object data, is proposed. Two competing strategies are used for optimizing metric depth functions, i.e., finding the deepest objects with respect to them. The performance of the metric Oja depth is compared with three other depth functions (half-space, lens, and spatial) in diverse data scenarios.
  Keywords: Object Data, Metric Oja depth, Statistical depth, Optimization, Metric statistics</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11580v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vida Zamanifarizhandi, Joni Virta</dc:creator>
    </item>
    <item>
      <title>Movement Dynamics in Elite Female Soccer Athletes: The Quantile Cube Approach</title>
      <link>https://arxiv.org/abs/2503.11815</link>
      <description>arXiv:2503.11815v2 Announce Type: replace 
Abstract: This paper presents the quantile cube, a novel three-dimensional summary representation designed to analyze external load using GPS-derived movement data. While broadly applicable, we demonstrate its utility through an application to data from elite female soccer athletes across 23 matches. The quantile cube segments athlete movements into discrete quantiles of velocity, acceleration, and movement angle across match halves, providing a structured and interpretable framework to capture complex movement dynamics. Statistical analysis revealed significant differences in movement distributions between the first and second halves for individual athletes across all matches. Principal Component Analysis identified matches with unique movement dynamics, particularly at the start and end of the season. Dirichlet-multinomial regression further explored how factors such as athlete position, playing time, and match characteristics influenced movement profiles. Our analysis reveals external load variations over time and provides insights into performance optimization. The integration of these statistical techniques demonstrates the potential of data-driven strategies to enhance athlete monitoring and workload management in women's soccer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11815v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kendall L. Thomas, Jan Hannig</dc:creator>
    </item>
    <item>
      <title>Quantifying structural uncertainty in chemical reaction network inference</title>
      <link>https://arxiv.org/abs/2505.15653</link>
      <description>arXiv:2505.15653v2 Announce Type: replace 
Abstract: Dynamical systems in chemistry and biology are complex, and one often does not have comprehensive knowledge about the interactions involved. Chemical reaction network (CRN) inference aims to identify, from observing species concentrations over time, the unknown reactions between the species. Existing approaches largely focus on identifying a single, most likely CRN, without addressing uncertainty about the network structure. However, it is important to quantify structural uncertainty to have confidence in our inference and predictions. In this work, we do so by inferring an approximate posterior distribution over CRN structures. This is done by keeping a large set of suboptimal solutions found through sparse optimisation, in contrast to existing optimisation approaches which discard suboptimal solutions. We find that inducing reaction sparsity with nonconvex penalty functions results in more parsimonious CRNs compared to the popular lasso regularisation. In a real-data example where multiple CRNs have been previously suggested, our method simultaneously recovers reactions proposed from different literature. Our emphasis on network-level probabilities enables a novel, hierarchical representation of structural ambiguities in the space of CRNs. This readily translates into alternative reaction pathways suggested by the available data, thus guiding the efforts of future experimental design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15653v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yong See Foo, Adriana Zanca, Jennifer A. Flegg, Ivo Siekmann</dc:creator>
    </item>
    <item>
      <title>Generalizable estimation of conditional average treatment effects using Causal Forest in randomized controlled trials</title>
      <link>https://arxiv.org/abs/2506.12296</link>
      <description>arXiv:2506.12296v2 Announce Type: replace 
Abstract: Estimating conditional average treatment effects (CATE) from randomized controlled trials (RCTs) and generalizing them to broader populations is essential for individualized treatment rules but is complicated by selection bias and high dimensional covariates. We evaluated Causal Forest based CATE estimation strategies that address trial selection bias. Specifically, we compared approaches of fitting Causal Forest with covariates of interest only, additionally including covariates that determine trial participation, and repeating these models with inverse probability weighting (IPW) to reweight trial samples to the source population. Identification theory suggests unbiased CATE estimation is possible when covariates related to trial participation are included. However, simulation studies demonstrated that, under realistic RCT sample sizes, variance inflation from high dimensional covariates often outweighed modest bias reduction. Including greater than 3 covariates related to participation substantially degraded precision unless sample sizes were large. In contrast, IPW methods consistently improved performance across scenarios, even when the weighting model was misspecified. Application to the VITAL trial of omega 3 fatty acids and coronary heart disease further illustrated how IPW shifts estimates toward source population effects and refines heterogeneity assessments. Our findings highlight a fundamental bias variance tradeoff in generalizing CATE from RCTs. While inclusion of trial selection variables ensures consistency in theory, in practice it may worsen performance in medical trials with sample size of 5000 or less. More efficient strategies are to limit CATE models to strong effect modifiers and address selection bias separately through IPW. These results provide practical guidance for applying CATE estimation in clinical and epidemiologic research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12296v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rikuta Hamaya, Etsuji Suzuki, Konan Hara</dc:creator>
    </item>
    <item>
      <title>A monotone single index model for spatially referenced multistate current status data</title>
      <link>https://arxiv.org/abs/2507.09057</link>
      <description>arXiv:2507.09057v2 Announce Type: replace 
Abstract: Assessment of multistate disease progression is commonplace in biomedical research, such as, in periodontal disease (PD). However, the presence of multistate current status endpoints, where only a single snapshot of each subject's progression through disease states is available at a random inspection time after a known starting state, complicates the inferential framework. In addition, these endpoints can be clustered, and spatially associated, where a group of proximally located teeth (within subjects) may experience similar PD status, compared to those distally located. Motivated by a clinical study recording PD progression, we propose a Bayesian semiparametric accelerated failure time model with an inverse-Wishart proposal for accommodating (spatial) random effects, and flexible errors that follow a Dirichlet process mixture of Gaussians. For clinical interpretability, the systematic component of the event times is modeled using a monotone single index model, with the (unknown) link function estimated via a novel integrated basis expansion and basis coefficients endowed with constrained Gaussian process priors. In addition to establishing parameter identifiability, we present scalable computing via a combination of elliptical slice sampling, fast circulant embedding techniques, and smoothing of hard constraints, leading to straightforward estimation of parameters, and state occupation and transition probabilities. Using synthetic data, we study the finite sample properties of our Bayesian estimates, and their performance under model misspecification. We also illustrate our method via application to the real clinical PD dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09057v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/biomtc/ujaf105</arxiv:DOI>
      <dc:creator>Snigdha Das, Minwoo Chae, Debdeep Pati, Dipankar Bandyopadhyay</dc:creator>
    </item>
    <item>
      <title>On Causal Inference for the Survivor Function</title>
      <link>https://arxiv.org/abs/2507.16691</link>
      <description>arXiv:2507.16691v2 Announce Type: replace 
Abstract: In this expository paper, we consider the problem of causal inference and efficient estimation for the counterfactual survivor function. This problem has previously been considered in the literature in several papers, each relying on the imposition of conditions meant to identify the desired estimand from the observed data. These conditions, generally referred to as either implying or satisfying coarsening at random, are inconsistently imposed across this literature and, in all cases, fail to imply coarsening at random. We establish the first general characterization of coarsening at random, and also sequential coarsening at random, for this estimation problem. Other contributions include the first general characterization of the set of all influence functions for the counterfactual survival probability under sequential coarsening at random, and the corresponding nonparametric efficient influence function. These characterizations are general in that neither impose continuity assumptions on either the underlying failure or censoring time distributions. We further show how the latter compares to alternative forms recently derived in the literature, including establishing the pointwise equivalence of the influence functions for our nonparametric efficient estimator and that recently given in Westling et al (2024, Journal of the American Statistical Association).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16691v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin R. Baer, Ashkan Ertefaie, Robert L. Strawderman</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Subgraph Frequencies of Exchangeable Hyperedge Models</title>
      <link>https://arxiv.org/abs/2508.13258</link>
      <description>arXiv:2508.13258v2 Announce Type: replace 
Abstract: In statistical network analysis, models for binary adjacency matrices satisfying vertex exchangeability are commonly used. However, such models may fail to capture key features of the data-generating process when interactions, rather than nodes, are fundamental units. We study statistical inference for subgraph counts under an exchangeable hyperedge model. We introduce several classes of subgraph statistics for hypergraphs and develop inferential tools for subgraph frequencies that account for edge multiplicity. We show that a subclass of these subgraph statistics is robust to the deletion of low-degree nodes, enabling inference in settings where low-degree nodes are more likely to be missing. We also examine a more traditional notion of subgraph frequency that ignores multiplicity, showing that while inference based on limiting distributions is feasible in some cases, a non-degenerate limiting distribution may not exist in others. Empirically, we assess our methods through simulations and newly collected real-world hypergraph data on academic and movie collaborations, where our inferential tools outperform traditional approaches based on binary adjacency matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.13258v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ayoushman Bhattacharya, Nilanjan Chakraborty, Robert Lunde</dc:creator>
    </item>
    <item>
      <title>Large-scale Multiple Testing: Fundamental Limits of False Discovery Rate Control and Compound Oracle</title>
      <link>https://arxiv.org/abs/2302.06809</link>
      <description>arXiv:2302.06809v3 Announce Type: replace-cross 
Abstract: The false discovery rate (FDR) and the false non-discovery rate (FNR), defined as the expected false discovery proportion (FDP) and the false non-discovery proportion (FNP), are the most popular benchmarks for multiple testing. Despite the theoretical and algorithmic advances in recent years, the optimal tradeoff between the FDR and the FNR has been largely unknown except for certain restricted classes of decision rules, e.g., separable rules, or for other performance metrics, e.g., the marginal FDR and the marginal FNR (mFDR and mFNR). In this paper, we determine the asymptotically optimal FDR-FNR tradeoff under the two-group random mixture model when the number of hypotheses tends to infinity. Distinct from the optimal mFDR-mFNR tradeoff, which is achieved by separable decision rules, the optimal FDR-FNR tradeoff requires compound rules even in the large-sample limit and for models as simple as the Gaussian location model. This suboptimality of separable rules also holds for other objectives, such as maximizing the expected number of true discoveries. Finally, to address the limitation of the FDR which only controls the expectation but not the fluctuation of the FDP, we also determine the optimal tradeoff when the FDP is controlled with high probability and show it coincides with that of the mFDR and the mFNR. Extensions to models with a fixed non-null proportion are also obtained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.06809v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yutong Nie, Yihong Wu</dc:creator>
    </item>
    <item>
      <title>K-Tensors: Clustering Positive Semi-Definite Matrices</title>
      <link>https://arxiv.org/abs/2306.06534</link>
      <description>arXiv:2306.06534v5 Announce Type: replace-cross 
Abstract: This paper presents a new clustering algorithm for symmetric positive semi-definite (SPSD) matrices, called K-Tensors. The method identifies structured subsets of the SPSD cone characterized by common principal component (CPC) representations, where each subset corresponds to matrices sharing a common eigenstructure. Unlike conventional clustering approaches that rely on vectorization or transformations of SPSD matrices, thereby losing critical geometric and spectral information, K-Tensors introduces a divergence that respects the intrinsic geometry of SPSD matrices. This divergence preserves the shape and eigenstructure information and yields principal SPSD tensors, defined as a set of representative matrices that summarize the distribution of SPSD matrices. By exploring its theoretical properties, we show that the proposed clustering algorithm is self-consistent under mild distribution assumptions and converges to a local optimum. We demonstrate the use of the algorithm through an application to resting-state functional magnetic resonance imaging (rs-fMRI) data from the Human Connectome Project, where we cluster brain connectivity matrices to discover groups of subjects with shared connectivity structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06534v5</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanchao Zhang, Xiaomeng Ju, Baoyi Shi, Lingsong Meng, Thaddeus Tarpey</dc:creator>
    </item>
    <item>
      <title>A Flexible Framework for Incorporating Patient Preferences Into Q-Learning</title>
      <link>https://arxiv.org/abs/2307.12022</link>
      <description>arXiv:2307.12022v2 Announce Type: replace-cross 
Abstract: In real-world healthcare settings, treatment decisions often involve optimizing for multivariate outcomes such as treatment efficacy and severity of side effects based on individual preferences. However, existing statistical methods for estimating dynamic treatment regimes (DTRs) usually assume a univariate outcome, and the few methods that deal with composite outcomes suffer from limitations such as restrictions to a single time point and limited theoretical guarantees. To address these limitations, we propose Latent Utility Q-Learning (LUQ-Learning), a latent model approach that adapts Q-learning to tackle the aforementioned difficulties. Our framework allows for an arbitrary finite number of decision points and outcomes, incorporates personal preferences, and achieves asymptotic performance guarantees with realistic assumptions. We conduct simulation experiments based on an ongoing trial for low back pain as well as a well-known trial for schizophrenia. In both settings, LUQ-Learning achieves highly competitive performance compared to alternative baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.12022v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua P. Zitovsky, Yating Zou, Leslie Wilson, Michael R. Kosorok</dc:creator>
    </item>
    <item>
      <title>Causal Effects in Matching Mechanisms with Strategically Reported Preferences</title>
      <link>https://arxiv.org/abs/2307.14282</link>
      <description>arXiv:2307.14282v3 Announce Type: replace-cross 
Abstract: A growing number of central authorities use assignment mechanisms to allocate students to schools in a way that reflects student preferences and school priorities. However, most real-world mechanisms incentivize students to strategically misreport their preferences. Misreporting complicates the identification of causal parameters that depend on true preferences, which are necessary inputs for a broad class of counterfactual analyses. In this paper, we provide an identification approach that is robust to strategic misreporting and derive sharp bounds on causal effects of school assignment on future outcomes. Our approach applies to any mechanism as long as there exist placement scores and cutoffs that characterize that mechanism's allocation rule. We use data from a deferred acceptance mechanism that assigns students to more than 1,000 university--major combinations in Chile. Matching theory predicts and empirical evidence suggests that students behave strategically in Chile because they face constraints on their submission of preferences and have good a priori information on the schools they will have access to. Our bounds are informative enough to reveal significant heterogeneity in graduation success with respect to preferences and school assignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.14282v3</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marinho Bertanha, Margaux Luflade, Ismael Mourifi\'e</dc:creator>
    </item>
    <item>
      <title>Optimal rates for estimating the covariance kernel from synchronously sampled functional data</title>
      <link>https://arxiv.org/abs/2407.13641</link>
      <description>arXiv:2407.13641v2 Announce Type: replace-cross 
Abstract: We obtain minimax-optimal convergence rates in the supremum norm, including information-theoretic lower bounds, for estimating the covariance kernel of a stochastic process which is repeatedly observed at discrete, synchronous design points. We focus on the supremum norm instead of the simpler $L_2$ norm, since it corresponds to the visualization of the estimation error and forms the basis for the construction of uniform confidence bands. For dense design, assuming H\"older-smooth sample paths we obtain the $\sqrt n$-rate of convergence in the supremum norm without additional logarithmic factors which typically occur in the results in the literature. Surprisingly, in the transition from dense to sparse design the rates do not reflect the two-dimensional nature of the covariance kernel but correspond to those for univariate mean function estimation. Our estimation method can make use of higher-order smoothness of the covariance kernel away from the diagonal, and does not require the same smoothness on the diagonal itself. Hence, our results cover covariance kernels of processes with rough, non-differentiable sample paths. Moreover, the estimator does not use mean function estimation to form residuals, and no smoothness assumptions on the mean have to be imposed. In the dense case we also obtain a central limit theorem in the supremum norm, which can be used as the basis for the construction of uniform confidence sets. Extensions to estimating partial derivatives as well as to asynchronous designs are also discussed. Simulations and real-data applications illustrate the practical usefulness of the methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13641v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Berger, Hajo Holzmann</dc:creator>
    </item>
    <item>
      <title>Causal inference and racial bias in policing: New estimands and the importance of mobility data</title>
      <link>https://arxiv.org/abs/2409.08059</link>
      <description>arXiv:2409.08059v2 Announce Type: replace-cross 
Abstract: Studying racial bias in policing is a critically important problem, but one that comes with a number of inherent difficulties due to the nature of the available data. In this manuscript we tackle multiple key issues in the causal analysis of racial bias in policing. First, we formalize race and place policing, the idea that individuals of one race are policed differently when they are in neighborhoods primarily made up of individuals of other races. We develop an estimand to study this question rigorously, show the assumptions necessary for causal identification, and develop sensitivity analyses to assess robustness to violations of key assumptions. Additionally, we investigate difficulties with existing estimands targeting racial bias in policing. We show for these estimands, and the estimands developed in this manuscript, that estimation can benefit from incorporating mobility data into analyses. We apply these ideas to a study in New York City, where we find a large amount of racial bias, as well as race and place policing, and that these findings are robust to large violations of untestable assumptions. We additionally show that mobility data can make substantial impacts on the resulting estimates, suggesting it should be used whenever possible in subsequent studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08059v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuochao Huang, Brenden Beck, Joseph Antonelli</dc:creator>
    </item>
    <item>
      <title>Two-Sided Nearest Neighbors: An adaptive and minimax optimal procedure for matrix completion</title>
      <link>https://arxiv.org/abs/2411.12965</link>
      <description>arXiv:2411.12965v2 Announce Type: replace-cross 
Abstract: Nearest neighbor (NN) algorithms have been extensively used for missing data problems in recommender systems and sequential decision-making systems. Prior theoretical analysis has established favorable guarantees for NN when the underlying data is sufficiently smooth and the missingness probabilities are lower bounded. Here we analyze NN with non-smooth non-linear functions with vast amounts of missingness. In particular, we consider matrix completion settings where the entries of the underlying matrix follow a latent non-linear factor model, with the non-linearity belonging to a \Holder function class that is less smooth than Lipschitz. Our results establish following favorable properties for a suitable two-sided NN: (1) The mean squared error (MSE) of NN adapts to the smoothness of the non-linearity, (2) under certain regularity conditions, the NN error rate matches the rate obtained by an oracle equipped with the knowledge of both the row and column latent factors, and finally (3) NN's MSE is non-trivial for a wide range of settings even when several matrix entries might be missing deterministically. We support our theoretical findings via extensive numerical simulations and a case study with data from a mobile health study, HeartSteps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12965v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tathagata Sadhukhan, Manit Paul, Raaz Dwivedi</dc:creator>
    </item>
    <item>
      <title>A Causal Lens for Evaluating Faithfulness Metrics</title>
      <link>https://arxiv.org/abs/2502.18848</link>
      <description>arXiv:2502.18848v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) offer natural language explanations as an alternative to feature attribution methods for model interpretability. However, despite their plausibility, they may not reflect the model's true reasoning faithfully, which is crucial for understanding the model's true decision-making processes. Although several faithfulness metrics have been proposed, they are often evaluated in isolation, making direct, principled comparisons between them difficult. Here, we present Causal Diagnosticity, a framework that serves as a common testbed to evaluate faithfulness metrics for natural language explanations. Our framework employs the concept of diagnosticity, and uses model-editing methods to generate faithful-unfaithful explanation pairs. Our benchmark includes four tasks: fact-checking, analogy, object counting, and multi-hop reasoning. We evaluate prominent faithfulness metrics, including post-hoc explanation and chain-of-thought-based methods. We find that diagnostic performance varies across tasks and models, with Filler Tokens performing best overall. Additionally, continuous metrics are generally more diagnostic than binary ones but can be sensitive to noise and model choice. Our results highlight the need for more robust faithfulness metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18848v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kerem Zaman, Shashank Srivastava</dc:creator>
    </item>
    <item>
      <title>G{\'e}n{\'e}ration de Matrices de Corr{\'e}lation avec des Structures de Graphe par Optimisation Convexe</title>
      <link>https://arxiv.org/abs/2503.21298</link>
      <description>arXiv:2503.21298v2 Announce Type: replace-cross 
Abstract: This work deals with the generation of theoretical correlation matrices with specific sparsity patterns, associated to graph structures. We present a novel approach based on convex optimization, offering greater flexibility compared to existing techniques, notably by controlling the mean of the entry distribution in the generated correlation matrices. This allows for the generation of correlation matrices that better represent realistic data and can be used to benchmark statistical methods for graph inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21298v2</guid>
      <category>eess.SP</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ali Fahkar (STATIFY), K\'evin Polisano (SVH), Ir\`ene Gannaz (G-SCOP\_GROG, G-SCOP), Sophie Achard (STATIFY)</dc:creator>
    </item>
    <item>
      <title>How Can I Publish My LLM Benchmark Without Giving the True Answers Away?</title>
      <link>https://arxiv.org/abs/2505.18102</link>
      <description>arXiv:2505.18102v4 Announce Type: replace-cross 
Abstract: Publishing a large language model (LLM) benchmark on the Internet risks contaminating future LLMs: the benchmark may be unintentionally (or intentionally) used to train or select a model. A common mitigation is to keep the benchmark private and let participants submit their models or predictions to the organizers. However, this strategy will require trust in a single organization and still permits test-set overfitting through repeated queries. To overcome this issue, we propose a way to publish benchmarks without completely disclosing the ground-truth answers to the questions, while still maintaining the ability to openly evaluate LLMs. Our main idea is to inject randomness to the answers by preparing several logically correct answers, and only include one of them as the solution in the benchmark. This reduces the best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is this helpful to keep us from disclosing the ground truth, but this approach also offers a test for detecting data contamination. In principle, even fully capable models should not surpass the Bayes accuracy. If a model surpasses this ceiling despite this expectation, this is a strong signal of data contamination. We present experimental evidence that our method can detect data contamination accurately on a wide range of benchmarks, models, and training methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18102v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takashi Ishida, Thanawat Lodkaew, Ikko Yamane</dc:creator>
    </item>
    <item>
      <title>Asymptotic Consistency and Generalization in Hybrid Models of Regularized Selection and Nonlinear Learning</title>
      <link>https://arxiv.org/abs/2508.07754</link>
      <description>arXiv:2508.07754v2 Announce Type: replace-cross 
Abstract: This study explores how different types of supervised models perform in the task of predicting and selecting relevant variables in high-dimensional contexts, especially when the data is very noisy. We analyzed three approaches: regularized models (such as Lasso, Ridge, and Elastic Net), black-box models (such as Random Forest, XGBoost, LightGBM, CatBoost, and H2O GBM), and hybrid models that combine both approaches: regularization with nonlinear algorithms. Based on simulations inspired by the Friedman equation, we evaluated 23 models using three complementary metrics: RMSE, Jaccard index, and recall rate. The results reveal that, although black-box models excel in predictive accuracy, they lack interpretability and simplicity, essential factors in many real-world contexts. Regularized models, on the other hand, proved to be more sensitive to an excess of irrelevant variables. In this scenario, hybrid models stood out for their balance: they maintain good predictive performance, identify relevant variables more consistently, and offer greater robustness, especially as the sample size increases. Therefore, we recommend using this hybrid framework in market applications, where it is essential that the results make sense in a practical context and support decisions with confidence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07754v2</guid>
      <category>stat.OT</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luciano Ribeiro Galv\~ao, Rafael de Andrade Mora</dc:creator>
    </item>
  </channel>
</rss>
