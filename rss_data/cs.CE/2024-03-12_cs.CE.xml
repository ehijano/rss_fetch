<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 12 Mar 2024 04:01:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Research progress on intelligent optimization techniques for energy-efficient design of ship hull forms</title>
      <link>https://arxiv.org/abs/2403.05832</link>
      <description>arXiv:2403.05832v1 Announce Type: new 
Abstract: The design optimization of ship hull form based on hydrodynamics theory and simulation-based design (SBD) technologies generally considers ship performance and energy efficiency performance as the design objective, which plays an important role in smart design and manufacturing of green ship. An optimal design of sustainable energy system requires multidisciplinary tools to build ships with the least resistance and energy consumption. Through a systematic approach, this paper presents the research progress of energy-efficient design of ship hull forms based on intelligent optimization techniques. We discuss different methods involved in the optimization procedure, especially the latest developments of intelligent optimization algorithms and surrogate models. Moreover, current development trends and technical challenges of multidisciplinary design optimization and surrogate-assisted evolutionary algorithms for ship design are further analyzed. We explore the gaps and potential future directions, so as to paving the way towards the design of the next generation of more energy-efficient ship hull form.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05832v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuwei Zhu, Siying Lv, Kaifeng Chen, Wei Fang, Leilei Cao</dc:creator>
    </item>
    <item>
      <title>Deciphering Crypto Twitter</title>
      <link>https://arxiv.org/abs/2403.06036</link>
      <description>arXiv:2403.06036v1 Announce Type: new 
Abstract: Cryptocurrency is a fast-moving space, with a continuous influx of new projects every year. However, an increasing number of incidents in the space, such as hacks and security breaches, threaten the growth of the community and the development of technology. This dynamic and often tumultuous landscape is vividly mirrored and shaped by discussions within Crypto Twitter, a key digital arena where investors, enthusiasts, and skeptics converge, revealing real-time sentiments and trends through social media interactions. We present our analysis on a Twitter dataset collected during a formative period of the cryptocurrency landscape. We collected 40 million tweets using cryptocurrency-related keywords and performed a nuanced analysis that involved grouping the tweets by semantic similarity and constructing a tweet and user network. We used sentence-level embeddings and autoencoders to create K-means clusters of tweets and identified six groups of tweets and their topics to examine different cryptocurrency-related interests and the change in sentiment over time. Moreover, we discovered sentiment indicators that point to real-life incidents in the crypto world, such as the FTX incident of November 2022. We also constructed and analyzed different networks of tweets and users in our dataset by considering the reply and quote relationships and analyzed the largest components of each network. Our networks reveal a structure of bot activity in Crypto Twitter and suggest that they can be detected and handled using a network-based approach. Our work sheds light on the potential of social media signals to detect and understand crypto events, benefiting investors, regulators, and curious observers alike, as well as the potential for bot detection in Crypto Twitter using a network-based approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06036v1</guid>
      <category>cs.CE</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3614419.3644026</arxiv:DOI>
      <dc:creator>Inwon Kang, Maruf Ahmed Mridul, Abraham Sanders, Yao Ma, Thilanka Munasinghe, Aparna Gupta, Oshani Seneviratne</dc:creator>
    </item>
    <item>
      <title>Generative LSTM Models and Asset Hierarchy Creation in Industrial Facilities</title>
      <link>https://arxiv.org/abs/2403.06103</link>
      <description>arXiv:2403.06103v1 Announce Type: new 
Abstract: In the evolving field of maintenance and reliability engineering, the organization of equipment into hierarchical structures presents both a challenge and a necessity, directly impacting the operational integrity of industrial facilities. This paper introduces an innovative approach employing machine learning, specifically Long Short-Term Memory (LSTM) models, to automate and enhance the creation and management of these hierarchies. By adapting techniques commonly used in natural language processing, the study explores the potential of LSTM models to interpret and predict relationships within equipment tags, offering a novel perspective on understanding facility design. This methodology involved character-wise tokenization of asset tags from approximately 29,000 entries across 50 upstream oil and gas facilities, followed by modeling these sequences using an LSTM-based recurrent neural network. The model's architecture capitalizes on LSTM's ability to learn long-term dependencies, facilitating the prediction of hierarchical relationships and contextual understanding of equipment tags.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06103v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Morgen Pronk</dc:creator>
    </item>
    <item>
      <title>RADS : Restricted Anisotropic Diffusion Spectrum model for Axonal Health quantification in Multiple Sclerosis</title>
      <link>https://arxiv.org/abs/2403.06140</link>
      <description>arXiv:2403.06140v1 Announce Type: new 
Abstract: Axonal damage is the primary pathological correlate of long-term impairment in multiple sclerosis (MS). Our previous work using our method - diffusion basis spectrum imaging (DBSI) - demonstrated a strong, quantitative relationship between axial diffusivity and axonal damage. In the present work, we develop an extension of DBSI which can be used to quantify the fraction of diseased and healthy axons in MS. In this method, we model the MRI signal with the axial diffusion (AD) spectrum for each fiber orientation. We use two component restricted anisotropic diffusion spectrum (RADS) to model the anisotropic component of the diffusion-weighted MRI signal. Diffusion coefficients and signal fractions are computed for the optimal model with the lowest Bayesian information criterion (BIC) score. This gives us the fractions of diseased and healthy axons based on the axial diffusivities of the diseased and healthy axons. We test our method using Monte-Carlo (MC) simulations with the MC simulation package developed as part of this work. First we test and validate our MC simulations for the basic RADS model. It accurately recovers the fiber and cell fractions simulated as well as the simulated diffusivities. For testing and validating RADS to quantify axonal loss, we simulate different fractions of diseased and healthy axons. Our method produces highly accurate quantification of diseased and healthy axons with Pearson's correlation (predicted vs true proportion) of $ r = 0.99 $ (p-value = 0.001); the one Sample t-test for proportion error gives the mean error of 2\% (p-value = 0.034). Furthermore, the method finds the axial diffusivities of the diseased and healthy axons very accurately with mean error of 4\% (p-value = 0.001). RADS modeling of the diffusion-weighted MRI signal has the potential to be used for Axonal Health quantification in Multiple Sclerosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06140v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nand Sharma</dc:creator>
    </item>
    <item>
      <title>No Language is an Island: Unifying Chinese and English in Financial Large Language Models, Instruction Data, and Benchmarks</title>
      <link>https://arxiv.org/abs/2403.06249</link>
      <description>arXiv:2403.06249v1 Announce Type: new 
Abstract: While the progression of Large Language Models (LLMs) has notably propelled financial analysis, their application has largely been confined to singular language realms, leaving untapped the potential of bilingual Chinese-English capacity. To bridge this chasm, we introduce ICE-PIXIU, seamlessly amalgamating the ICE-INTENT model and ICE-FLARE benchmark for bilingual financial analysis. ICE-PIXIU uniquely integrates a spectrum of Chinese tasks, alongside translated and original English datasets, enriching the breadth and depth of bilingual financial modeling. It provides unrestricted access to diverse model variants, a substantial compilation of diverse cross-lingual and multi-modal instruction data, and an evaluation benchmark with expert annotations, comprising 10 NLP tasks, 20 bilingual specific tasks, totaling 1,185k datasets. Our thorough evaluation emphasizes the advantages of incorporating these bilingual datasets, especially in translation tasks and utilizing original English data, enhancing both linguistic flexibility and analytical acuity in financial contexts. Notably, ICE-INTENT distinguishes itself by showcasing significant enhancements over conventional LLMs and existing financial LLMs in bilingual milieus, underscoring the profound impact of robust bilingual data on the accuracy and efficacy of financial NLP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06249v1</guid>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gang Hu, Ke Qin, Chenhan Yuan, Min Peng, Alejandro Lopez-Lira, Benyou Wang, Sophia Ananiadou, Wanlong Yu, Jimin Huang, Qianqian Xie</dc:creator>
    </item>
    <item>
      <title>When Crypto Economics Meet Graph Analytics and Learning</title>
      <link>https://arxiv.org/abs/2403.06454</link>
      <description>arXiv:2403.06454v1 Announce Type: new 
Abstract: Utilizing graph analytics and learning has proven to be an effective method for exploring aspects of crypto economics such as network effects, decentralization, tokenomics, and fraud detection. However, the majority of existing research predominantly focuses on leading cryptocurrencies, namely Bitcoin (BTC) and Ethereum (ETH), overlooking the vast diversity among the more than 10,000 cryptocurrency projects. This oversight may result in skewed insights. In our paper, we aim to broaden the scope of investigation to encompass the entire spectrum of cryptocurrencies, examining various coins across their entire life cycles. Furthermore, we intend to pioneer advanced methodologies, including graph transfer learning and the innovative concept of "graph of graphs". By extending our research beyond the confines of BTC and ETH, our goal is to enhance the depth of our understanding of crypto economics and to advance the development of more intricate graph-based techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06454v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bingqiao Luo</dc:creator>
    </item>
    <item>
      <title>Performance of Expansive Soil Stabilized with Bamboo Charcoal, Quarry Dust, and Lime for Use as Road Subgrade Material</title>
      <link>https://arxiv.org/abs/2403.06669</link>
      <description>arXiv:2403.06669v1 Announce Type: new 
Abstract: Expansive soils such as Black Cotton Soils (BCS) present significant challenges for road subgrade construction due to their high plasticity, swelling potential, and low strength. This study explores a triphasic stabilization method using Bamboo Charcoal (BC), Quarry Dust (QD), and Lime (L) to enhance the engineering properties of BCS for rural road applications. Initial soil characterization involved standard tests, including Atterberg limits, compaction, and Californian Bearing Ratio (CBR) assessments. The soil was treated with varying BC proportions (5% to 35% at 5% intervals) in the initial phase, leading to a progressive reduction in the Plasticity Index (PI) and swell index and an enhancement in the CBR up to 20% BC content. This further resulted in a soaked CBR value of 2.7%. In the second phase, additional treatment combined with BC and QD, incorporating diverse QD proportions (4% to 24%) relative to the optimal BC content. This further improved the CBR to 7.7% at 12% QD, but the PI exhibited a non-linear trend. Finally, 5% lime was introduced in the final phase. This minimized the PI to 11.2% and significantly increased the CBR to 19%. The optimal combination of 20% BC, 12% QD, and 5% Lime achieved optimal plasticity, compaction, and strength characteristics, demonstrating the viability of this approach for transforming BCS into a sustainable and cost-effective alternative for rural road subgrade construction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06669v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.14445/23488352/IJCE-V11I2P110</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Civil Engineering, vol. 11, no. 2, pp. 108-120, 2024</arxiv:journal_reference>
      <dc:creator>Essizewa Essowedeou Agate, Nyomboi Timothy, Ambassah O. Nathaniel, Ines Ngassam</dc:creator>
    </item>
    <item>
      <title>Numerical simulation of individual coil placement - A proof-of-concept study for the prediction of recurrence after aneurysm coiling</title>
      <link>https://arxiv.org/abs/2403.06889</link>
      <description>arXiv:2403.06889v1 Announce Type: new 
Abstract: Rupture of intracranial aneurysms results in severe subarachnoidal hemorrhage, which is associated with high morbidity and mortality. Neurointerventional occlusion of the aneurysm through coiling has evolved to a therapeutical standard. The choice of the specific coil has an important influence on secondary regrowth requiring retreatment. Aneurysm occlusion was simulated either through virtual implantation of a preshaped 3D coil or with a porous media approach. In this study, we used a recently developed numerical approach to simulate aneurysm shapes in specific challenging aneurysm anatomies and correlated these with aneurysm recurrence 6 months after treatment. The simulation showed a great variety of coil shapes depending on the variability in possible microcatheter positions. Aneurysms with a later recurrence showed a tendency for more successful coiling attempts. Results revealed further trends suggesting lower simulated packing densities in aneurysms with reoccurrence. Simulated packing densities did not correlate with those calculated by conventional software, indicating the potential for our approach to offer additional predictive value. Our study, therefore, pioneers a comprehensive numerical model for simulating aneurysm coiling, providing insights into individualized treatment strategies and outcome prediction. Future directions involve expanding the model's capabilities to simulate intraprocedural outcomes and long-term predictions, aiming to refine occlusion quality criteria and validate prediction parameters in larger patient cohorts. This simulation framework holds promise for enhancing clinical decision-making and optimizing patient outcomes in endovascular aneurysm treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06889v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julian Schwarting, Fabian Holzberger, Markus Muhr, Martin Renz, Tobias Boeckh-Behrens, Barbara Wohlmuth, Jan Kirschke</dc:creator>
    </item>
    <item>
      <title>Unified Occupancy on a Public Transport Network through Combination of AFC and APC Data</title>
      <link>https://arxiv.org/abs/2403.05546</link>
      <description>arXiv:2403.05546v1 Announce Type: cross 
Abstract: In a transport network, the onboard occupancy is key for gaining insights into travelers' habits and adjusting the offer. Traditionally, operators have relied on field studies to evaluate ridership of a typical workday. However, automated fare collection (AFC) and automatic passenger counting (APC) data, which provide complete temporal coverage, are often available but underexploited. It should be noted, however, that each data source comes with its own biases: AFC data may not account for fraud, while not all vehicles are equipped with APC systems.
  This paper introduces the unified occupancy method, a geostatistical model to extrapolate occupancy to every course of a public transportation network by combining AFC and APC data with partial coverage. Unified occupancy completes missing APC information for courses on lines where other courses have APC measures, as well as for courses on lines where no APC data is available at all. The accuracy of this method is evaluated on real data from several public transportation networks in France.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05546v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amir Dib, No\"elie Cherrier, Martin Graive, Baptiste R\'erolle, Eglantine Schmitt</dc:creator>
    </item>
    <item>
      <title>Spatial Clustering Approach for Vessel Path Identification</title>
      <link>https://arxiv.org/abs/2403.05778</link>
      <description>arXiv:2403.05778v1 Announce Type: cross 
Abstract: This paper addresses the challenge of identifying the paths for vessels with operating routes of repetitive paths, partially repetitive paths, and new paths. We propose a spatial clustering approach for labeling the vessel paths by using only position information. We develop a path clustering framework employing two methods: a distance-based path modeling and a likelihood estimation method. The former enhances the accuracy of path clustering through the integration of unsupervised machine learning techniques, while the latter focuses on likelihood-based path modeling and introduces segmentation for a more detailed analysis. The result findings highlight the superior performance and efficiency of the developed approach, as both methods for clustering vessel paths into five classes achieve a perfect F1-score. The approach aims to offer valuable insights for route planning, ultimately contributing to improving safety and efficiency in maritime transportation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05778v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed Abuella, M. Amine Atoui, Slawomir Nowaczyk, Simon Johansson, Ethan Faghan</dc:creator>
    </item>
    <item>
      <title>FMPAF: How Do Fed Chairs Affect the Financial Market? A Fine-grained Monetary Policy Analysis Framework on Their Language</title>
      <link>https://arxiv.org/abs/2403.06115</link>
      <description>arXiv:2403.06115v1 Announce Type: cross 
Abstract: The effectiveness of central bank communication is a crucial aspect of monetary policy transmission. While recent research has examined the influence of policy communication by the chairs of the Federal Reserve on various financial variables, much of the literature relies on rule-based or dictionary-based methods in parsing the language of the chairs, leaving nuanced information about policy stance contained in nonverbal emotion out of the analysis. In the current study, we propose the Fine-Grained Monetary Policy Analysis Framework (FMPAF), a novel approach that integrates large language models (LLMs) with regression analysis to provide a comprehensive analysis of the impact of the press-conference communications of chairs of the Federal Reserve on financial markets. We conduct extensive comparisons of model performance under different levels of granularity, modalities, and communication scenarios. Based on our preferred specification, a one-unit increase in the sentiment score is associated with an increase of the price of S\&amp;P 500 Exchange-Traded Fund by approximately 500 basis points, a 15-basis-point decrease in the policy interest rate, while not leading to a significant response in exchange rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06115v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yayue Deng, Mohan Xu, Yao Tang</dc:creator>
    </item>
    <item>
      <title>An Alternative to Stride-Based RNG for Monte Carlo Transport</title>
      <link>https://arxiv.org/abs/2403.06362</link>
      <description>arXiv:2403.06362v1 Announce Type: cross 
Abstract: The techniques used to generate pseudo-random numbers for Monte Carlo (MC) applications bear many implications on the quality and speed of that programs work. As a random number generator (RNG) slows, the production of random numbers begins to dominate runtime. As RNG output grows in correlation, the final product becomes less reliable.
  These difficulties are further compounded by the need for reproducibility and parallelism. For reproducibility, the numbers generated to determine any outcome must be the same each time a simulation is run. However, the concurrency that comes with most parallelism introduces race conditions. To have both reproducibility and concurrency, separate RNG states must be tracked for each independently schedulable unit of simulation, forming independent random number streams.
  We propose an alternative to the stride-based parallel LCG seeding approach that scales more practically with increased concurrency and workload by generating seeds through hashing and allowing for repeated outputs. Data gathered from normality tests of tally results from simple MC transport benchmark calculations indicates that the proposed hash-based RNG does not significantly affect the tally result normality property as compared to the conventional stride-based RNG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06362v1</guid>
      <category>physics.comp-ph</category>
      <category>cs.CE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Braxton S. Cuneo, Ilham Variansyah</dc:creator>
    </item>
    <item>
      <title>SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection</title>
      <link>https://arxiv.org/abs/2403.06534</link>
      <description>arXiv:2403.06534v1 Announce Type: cross 
Abstract: Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising &lt;2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining on RGB datasets and finetuning on SAR datasets in terms of both data domain and model structure. To bridge these gaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA) pretraining framework that tackles the problems from the perspective of data input, domain transition, and model migration. The proposed MSFA method significantly enhances the performance of SAR object detection models while demonstrating exceptional generalizability and flexibility across diverse models. This work aims to pave the way for further advancements in SAR object detection. The dataset and code is available at https://github.com/zcablii/SARDet_100K.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06534v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuxuan Li, Xiang Li, Weijie Li, Qibin Hou, Li Liu, Ming-Ming Cheng, Jian Yang</dc:creator>
    </item>
    <item>
      <title>Index-aware learning of circuits</title>
      <link>https://arxiv.org/abs/2309.00958</link>
      <description>arXiv:2309.00958v3 Announce Type: replace 
Abstract: Electrical circuits are present in a variety of technologies, making their design an important part of computer aided engineering. The growing number of parameters that affect the final design leads to a need for new approaches to quantify their impact. Machine learning may play a key role in this regard, however current approaches often make suboptimal use of existing knowledge about the system at hand. In terms of circuits, their description via modified nodal analysis is well-understood. This particular formulation leads to systems of differential-algebraic equations (DAEs) which bring with them a number of peculiarities, e.g. hidden constraints that the solution needs to fulfill. We use the recently introduced dissection index that can decouple a given system of DAEs into ordinary differential equations, only depending on differential variables, and purely algebraic equations, that describe the relations between differential and algebraic variables. The idea is to then only learn the differential variables and reconstruct the algebraic ones using the relations from the decoupling. This approach guarantees that the algebraic constraints are fulfilled up to the accuracy of the nonlinear system solver, and it may also reduce the learning effort as only the differential variables need to be learned.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.00958v3</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Idoia Cortes Garcia, Peter F\"orster, Lennart Jansen, Wil Schilders, Sebastian Sch\"ops</dc:creator>
    </item>
    <item>
      <title>A cost-benefit source-receptor framework for implementation of Blue-Green flood risk management</title>
      <link>https://arxiv.org/abs/2311.00420</link>
      <description>arXiv:2311.00420v2 Announce Type: replace 
Abstract: As floods are a major and growing source of risk in urban areas, there is a necessity to improve flood risk management frameworks and civil protection through planning interventions that modify surface flow pathways and introduce storage. Despite the complexity of densely urbanised areas, modern flood models can represent urban features and flow characteristics to help researchers, local authorities, and insurance companies to develop and improve efficient flood risk frameworks to achieve resilience in cities. A cost-benefit driven source-receptor flood risk framework is developed in this study to identify (1) locations contributing to surface flooding (sources), (2) buildings and locations at high flood risk (receptors), (3) the cost-benefit nexus between the source and the receptor, and finally (4) ways to mitigate flooding at the receptor by adding Blue-Green Infrastructure (BGI) in critical locations. The analysis is based on five steps to identify the source and the receptor in a study area based on the flood exposure of buildings, damages arising from flooding and available green spaces with the best potential to add sustainable and resilient solutions to reduce flooding. The framework was developed using the detailed hydrodynamic model CityCAT in a case study of the city centre of Newcastle upon Tyne, UK. The novelty of this analysis is that firstly, multiple storm magnitudes (i.e. small and large floods) are used combined with a method to locate the areas and the buildings at flood risk and a prioritized set of best places to add interventions upstream and downstream. Secondly, planning decisions are informed by considering the benefit from reduced damages to properties and the cost to construct resilient BGI options rather than a restricted hydraulic analysis considering only flood depths and storages in isolation from real-world economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00420v2</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christos Iliadis, Vassilis Glenis, Chris Kilsby</dc:creator>
    </item>
    <item>
      <title>Rediscovering the Mullins Effect With Deep Symbolic Regression</title>
      <link>https://arxiv.org/abs/2403.05495</link>
      <description>arXiv:2403.05495v2 Announce Type: replace 
Abstract: The Mullins effect represents a softening phenomenon observed in rubber-like materials and soft biological tissues. It is usually accompanied by many other inelastic effects like for example residual strain and induced anisotropy. In spite of the long term research and many material models proposed in literature, accurate modeling and prediction of this complex phenomenon still remain a challenging task.
  In this work, we present a novel approach using deep symbolic regression (DSR) to generate material models describing the Mullins effect in the context of nearly incompressible hyperelastic materials. The two step framework first identifies a strain energy function describing the primary loading. Subsequently, a damage function characterizing the softening behavior under cyclic loading is identified. The efficiency of the proposed approach is demonstrated through benchmark tests using the generalized the Mooney-Rivlin and the Ogden-Roxburgh model. The generalizability and robustness of the presented framework are thoroughly studied. In addition, the proposed methodology is extensively validated on a temperature-dependent data set, which demonstrates its versatile and reliable performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05495v2</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rasul Abdusalamov, Jendrik Weise, Mikhail Itskov</dc:creator>
    </item>
    <item>
      <title>Quantum-Inspired Tensor Neural Networks for Option Pricing</title>
      <link>https://arxiv.org/abs/2212.14076</link>
      <description>arXiv:2212.14076v2 Announce Type: replace-cross 
Abstract: Recent advances in deep learning have enabled us to address the curse of dimensionality (COD) by solving problems in higher dimensions. A subset of such approaches of addressing the COD has led us to solving high-dimensional PDEs. This has resulted in opening doors to solving a variety of real-world problems ranging from mathematical finance to stochastic control for industrial applications. Although feasible, these deep learning methods are still constrained by training time and memory. Tackling these shortcomings, Tensor Neural Networks (TNN) demonstrate that they can provide significant parameter savings while attaining the same accuracy as compared to the classical Dense Neural Network (DNN). In addition, we also show how TNN can be trained faster than DNN for the same accuracy. Besides TNN, we also introduce Tensor Network Initializer (TNN Init), a weight initialization scheme that leads to faster convergence with smaller variance for an equivalent parameter count as compared to a DNN. We benchmark TNN and TNN Init by applying them to solve the parabolic PDE associated with the Heston model, which is widely used in financial pricing theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.14076v2</guid>
      <category>q-fin.PR</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>quant-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raj G. Patel, Chia-Wei Hsing, Serkan Sahin, Samuel Palmer, Saeed S. Jahromi, Shivam Sharma, Tomas Dominguez, Kris Tziritas, Christophe Michel, Vincent Porte, Mustafa Abid, Stephane Aubert, Pierre Castellani, Samuel Mugel, Roman Orus</dc:creator>
    </item>
    <item>
      <title>Brain Effective Connectome based on fMRI and DTI Data: Bayesian Causal Learning and Assessment</title>
      <link>https://arxiv.org/abs/2302.05451</link>
      <description>arXiv:2302.05451v3 Announce Type: replace-cross 
Abstract: Neuroscientific studies aim to find an accurate and reliable brain Effective Connectome (EC). Although current EC discovery methods have contributed to our understanding of brain organization, their performances are severely constrained by the short sample size and poor temporal resolution of fMRI data, and high dimensionality of the brain connectome. By leveraging the DTI data as prior knowledge, we introduce two Bayesian causal discovery frameworks -- the Bayesian GOLEM (BGOLEM) and Bayesian FGES (BFGES) methods -- that offer significantly more accurate and reliable ECs and address the shortcomings of the existing causal discovery methods in discovering ECs based on only fMRI data. Through a series of simulation studies on synthetic and hybrid (DTI of the Human Connectome Project (HCP) subjects and synthetic fMRI) data, we demonstrate the effectiveness of the proposed methods in discovering EC. To numerically assess the improvement in the accuracy of ECs with our method on empirical data, we first introduce the Pseudo False Discovery Rate (PFDR) as a new computational accuracy metric for causal discovery in the brain. We show that our Bayesian methods achieve higher accuracy than traditional methods on HCP data. Additionally, we measure the reliability of discovered ECs using the Rogers-Tanimoto index for test-retest data and show that our Bayesian methods provide significantly more reproducible ECs than traditional methods. Overall, our study's numerical and graphical results highlight the potential for these frameworks to advance our understanding of brain function and organization significantly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.05451v3</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdolmahdi Bagheri, Mahdi Dehshiri, Yamin Bagheri, Alireza Akhondi-Asl, Babak Nadjar Araabi</dc:creator>
    </item>
    <item>
      <title>Application of Tensor Neural Networks to Pricing Bermudan Swaptions</title>
      <link>https://arxiv.org/abs/2304.09750</link>
      <description>arXiv:2304.09750v2 Announce Type: replace-cross 
Abstract: The Cheyette model is a quasi-Gaussian volatility interest rate model widely used to price interest rate derivatives such as European and Bermudan Swaptions for which Monte Carlo simulation has become the industry standard. In low dimensions, these approaches provide accurate and robust prices for European Swaptions but, even in this computationally simple setting, they are known to underestimate the value of Bermudan Swaptions when using the state variables as regressors. This is mainly due to the use of a finite number of predetermined basis functions in the regression. Moreover, in high-dimensional settings, these approaches succumb to the Curse of Dimensionality. To address these issues, Deep-learning techniques have been used to solve the backward Stochastic Differential Equation associated with the value process for European and Bermudan Swaptions; however, these methods are constrained by training time and memory. To overcome these limitations, we propose leveraging Tensor Neural Networks as they can provide significant parameter savings while attaining the same accuracy as classical Dense Neural Networks. In this paper we rigorously benchmark the performance of Tensor Neural Networks and Dense Neural Networks for pricing European and Bermudan Swaptions, and we show that Tensor Neural Networks can be trained faster than Dense Neural Networks and provide more accurate and robust prices than their Dense counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.09750v2</guid>
      <category>q-fin.CP</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>quant-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raj G. Patel, Tomas Dominguez, Mohammad Dib, Samuel Palmer, Andrea Cadarso, Fernando De Lope Contreras, Abdelkader Ratnani, Francisco Gomez Casanova, Senaida Hern\'andez-Santana, \'Alvaro D\'iaz-Fern\'andez, Eva Andr\'es, Jorge Luis-Hita, Escol\'astico S\'anchez-Mart\'inez, Samuel Mugel, Roman Orus</dc:creator>
    </item>
    <item>
      <title>Compositional Generative Inverse Design</title>
      <link>https://arxiv.org/abs/2401.13171</link>
      <description>arXiv:2401.13171v2 Announce Type: replace-cross 
Abstract: Inverse design, where we seek to design input variables in order to optimize an underlying objective function, is an important problem that arises across fields such as mechanical engineering to aerospace engineering. Inverse design is typically formulated as an optimization problem, with recent works leveraging optimization across learned dynamics models. However, as models are optimized they tend to fall into adversarial modes, preventing effective sampling. We illustrate that by instead optimizing over the learned energy function captured by the diffusion model, we can avoid such adversarial examples and significantly improve design performance. We further illustrate how such a design system is compositional, enabling us to combine multiple different diffusion models representing subcomponents of our desired system to design systems with every specified component. In an N-body interaction task and a challenging 2D multi-airfoil design task, we demonstrate that by composing the learned diffusion model at test time, our method allows us to design initial states and boundary shapes that are more complex than those in the training data. Our method generalizes to more objects for N-body dataset and discovers formation flying to minimize drag in the multi-airfoil design task. Project website and code can be found at https://github.com/AI4Science-WestlakeU/cindm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13171v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tailin Wu, Takashi Maruyama, Long Wei, Tao Zhang, Yilun Du, Gianluca Iaccarino, Jure Leskovec</dc:creator>
    </item>
  </channel>
</rss>
