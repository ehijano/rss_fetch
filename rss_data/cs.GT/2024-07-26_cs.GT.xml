<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 26 Jul 2024 04:00:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 26 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On Approximately Strategy-Proof Tournament Rules for Collusions of Size at Least Three</title>
      <link>https://arxiv.org/abs/2407.17569</link>
      <description>arXiv:2407.17569v1 Announce Type: new 
Abstract: A tournament organizer must select one of $n$ possible teams as the winner of a competition after observing all $\binom{n}{2}$ matches between them. The organizer would like to find a tournament rule that simultaneously satisfies the following desiderata. It must be Condorcet-consistent (henceforth, CC), meaning it selects as the winner the unique team that beats all other teams (if one exists). It must also be strongly non-manipulable for groups of size $k$ at probability $\alpha$ (henceforth, k-SNM-$\alpha$), meaning that no subset of $\leq k$ teams can fix the matches among themselves in order to increase the chances any of it's members being selected by more than $\alpha$. Our contributions are threefold. First, wee consider a natural generalization of the Randomized Single Elimination Bracket rule from [Schneider et al. 2017] to $d$-ary trees and provide upper bounds to its manipulability. Then, we propose a novel tournament rule that is CC and 3-SNM-1/2, a strict improvement upon the concurrent work of [Dinev and Weinberg, 2022] who proposed a CC and 3-SNM-31/60 rule. Finally, we initiate the study of reductions among tournament rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17569v1</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Mik\v{s}an\'ik, Ariel Schvartzman, Jan Soukup</dc:creator>
    </item>
    <item>
      <title>Nested replicator dynamics, nested logit choice, and similarity-based learning</title>
      <link>https://arxiv.org/abs/2407.17815</link>
      <description>arXiv:2407.17815v1 Announce Type: new 
Abstract: We consider a model of learning and evolution in games whose action sets are endowed with a partition-based similarity structure intended to capture exogenous similarities between strategies. In this model, revising agents have a higher probability of comparing their current strategy with other strategies that they deem similar, and they switch to the observed strategy with probability proportional to its payoff excess. Because of this implicit bias toward similar strategies, the resulting dynamics - which we call the nested replicator dynamics - do not satisfy any of the standard monotonicity postulates for imitative game dynamics; nonetheless, we show that they retain the main long-run rationality properties of the replicator dynamics, albeit at quantitatively different rates. We also show that the induced dynamics can be viewed as a stimulus-response model in the spirit of Erev &amp; Roth (1998), with choice probabilities given by the nested logit choice rule of Ben-Akiva (1973) and McFadden (1978). This result generalizes an existing relation between the replicator dynamics and the exponential weights algorithm in online learning, and provides an additional layer of interpretation to our analysis and results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17815v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Panayotis Mertikopoulos, William H. Sandholm</dc:creator>
    </item>
    <item>
      <title>Limited Voting for Better Representation?</title>
      <link>https://arxiv.org/abs/2407.17973</link>
      <description>arXiv:2407.17973v1 Announce Type: new 
Abstract: Limited Voting (LV) is an approval-based method for multi-winner elections where all ballots are required to have a same fixed size. While it appears to be used as voting method in corporate governance and has some political applications, to the best of our knowledge, no formal analysis of the rule exists to date. We provide such an analysis here, prompted by a request for advice about this voting rule by a health insurance company in the Netherlands, which uses it to elect its work council. We study conditions under which LV would improve representation over standard approval voting and when it would not. We establish the extent of such an improvement, or lack thereof, both in terms of diversity and proportionality notions. These results help us understand if, and how, LV may be used as a low-effort fix of approval voting in order to enhance representation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17973v1</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maaike Venema-Los, Zo\'e Christoff, Davide Grossi</dc:creator>
    </item>
    <item>
      <title>Principal-Agent Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2407.18074</link>
      <description>arXiv:2407.18074v1 Announce Type: new 
Abstract: Contracts are the economic framework which allows a principal to delegate a task to an agent -- despite misaligned interests, and even without directly observing the agent's actions. In many modern reinforcement learning settings, self-interested agents learn to perform a multi-stage task delegated to them by a principal. We explore the significant potential of utilizing contracts to incentivize the agents. We model the delegated task as an MDP, and study a stochastic game between the principal and agent where the principal learns what contracts to use, and the agent learns an MDP policy in response. We present a learning-based algorithm for optimizing the principal's contracts, which provably converges to the subgame-perfect equilibrium of the principal-agent game. A deep RL implementation allows us to apply our method to very large MDPs with unknown transition dynamics. We extend our approach to multiple agents, and demonstrate its relevance to resolving a canonical sequential social dilemma with minimal intervention to agent rewards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18074v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dima Ivanov, Paul D\"utting, Inbal Talgam-Cohen, Tonghan Wang, David C. Parkes</dc:creator>
    </item>
    <item>
      <title>Strategic Cost Selection in Participatory Budgeting</title>
      <link>https://arxiv.org/abs/2407.18092</link>
      <description>arXiv:2407.18092v1 Announce Type: new 
Abstract: We study strategic behavior of project proposers in the context of approval-based participatory budgeting (PB). In our model we assume that the votes are fixed and known and the proposers want to set as high project prices as possible, provided that their projects get selected and the prices are not below the minimum costs of their delivery. We study the existence of pure Nash equilibria (NE) in such games, focusing on the AV/Cost, Phragm\'en, and Method of Equal Shares rules. Furthermore, we report an experimental study of strategic cost selection on real-life PB election data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18092v1</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Piotr Faliszewski, {\L}ukasz Janeczko, Andrzej Kaczmarczyk, Grzegorz Lisowski, Piotr Skowron, Stanis{\l}aw Szufa</dc:creator>
    </item>
    <item>
      <title>Optimal Trade and Industrial Policies in the Global Economy: A Deep Learning Framework</title>
      <link>https://arxiv.org/abs/2407.17731</link>
      <description>arXiv:2407.17731v1 Announce Type: cross 
Abstract: We propose a deep learning framework, DL-opt, designed to efficiently solve for optimal policies in quantifiable general equilibrium trade models. DL-opt integrates (i) a nested fixed point (NFXP) formulation of the optimization problem, (ii) automatic implicit differentiation to enhance gradient descent for solving unilateral optimal policies, and (iii) a best-response dynamics approach for finding Nash equilibria. Utilizing DL-opt, we solve for non-cooperative tariffs and industrial subsidies across 7 economies and 44 sectors, incorporating sectoral external economies of scale. Our quantitative analysis reveals significant sectoral heterogeneity in Nash policies: Nash industrial subsidies increase with scale elasticities, whereas Nash tariffs decrease with trade elasticities. Moreover, we show that global dual competition, involving both tariffs and industrial subsidies, results in lower tariffs and higher welfare outcomes compared to a global tariff war. These findings highlight the importance of considering sectoral heterogeneity and policy combinations in understanding global economic competition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17731v1</guid>
      <category>econ.GN</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>q-fin.EC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zi Wang, Xingcheng Xu, Yanqing Yang, Xiaodong Zhu</dc:creator>
    </item>
    <item>
      <title>Truthful Budget Aggregation: Beyond Moving-Phantom Mechanisms</title>
      <link>https://arxiv.org/abs/2405.20303</link>
      <description>arXiv:2405.20303v2 Announce Type: replace 
Abstract: We study a budget-aggregation setting in which a number of voters report their ideal distribution of a budget over a set of alternatives, and a mechanism aggregates these reports into an allocation. Ideally, such mechanisms are truthful, i.e., voters should not be incentivized to misreport their preferences. For the case of two alternatives, the set of mechanisms that are truthful and additionally meet a range of basic desiderata (anonymity, neutrality, and continuity) exactly coincides with the so-called moving-phantom mechanisms, but whether this space is richer for more alternatives was repeatedly stated as an open question. We answer this question in the affirmative by presenting a class of truthful mechanisms that are not moving-phantoms but satisfy the three properties. Since moving-phantom mechanisms can only provide limited fairness guarantees (measured as the worst-case distance to a fair share solution), one motivation for broadening the class of truthful mechanisms is the hope for improved fairness guarantees. We dispel this hope by showing that lower bounds holding for the class of moving-phantom mechanisms extend to all truthful, anonymous, neutral, and continuous mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20303v2</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mark de Berg, Rupert Freeman, Ulrike Schmidt-Kraepelin, Markus Utke</dc:creator>
    </item>
  </channel>
</rss>
