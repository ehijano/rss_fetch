<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 26 Aug 2024 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 26 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Simopt -- Simulation pass for Speculative Optimisation of FPGA-CAD flow</title>
      <link>https://arxiv.org/abs/2408.12676</link>
      <description>arXiv:2408.12676v1 Announce Type: new 
Abstract: Behavioural simulation is deployed in CAD flow to verify the functional correctness of a Register Transfer Level (RTL) design. Metadata extracted from behavioural simulation could be used to optimise and/or speed up subsequent steps in the hardware design flow. In this paper, we propose Simopt, a tool flow that extracts simulation metadata to improve the timing performance of the design by introducing latency awareness during the placement phase and subsequently improving the routing time of the post-placed netlist using vendor tools. For our experiments, we adapt the open-source Yosys flow to perform Simopt-aware placement. Our results show that using the Simopt-pass in the design implementation flow results in up to 38.2% reduction in timing performance (latency) of the design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12676v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eashan Wadhwa, Shanker Shreejith</dc:creator>
    </item>
    <item>
      <title>An Architectural Error Metric for CNN-Oriented Approximate Multipliers</title>
      <link>https://arxiv.org/abs/2408.12836</link>
      <description>arXiv:2408.12836v1 Announce Type: new 
Abstract: As a potential alternative for implementing the large number of multiplications in convolutional neural networks (CNNs), approximate multipliers (AMs) promise both high hardware efficiency and accuracy. However, the characterization of accuracy and design of appropriate AMs are critical to an AM-based CNN (AM-CNN). In this work, the generation and propagation of errors in an AM-CNN are analyzed by considering the CNN architecture. Based on this analysis, a novel AM error metric is proposed to evaluate the accuracy degradation of an AM-CNN, denoted as the architectural mean error (AME). The effectiveness of the AME is assessed in VGG and ResNet on CIFAR-10, CIFAR-100, and ImageNet datasets. Experimental results show that AME exhibits a strong correlation with the accuracy of AM-CNNs, outperforming the other AM error metrics. To predict the accuracy of AM-CNNs, quadratic regression models are constructed based on the AME; the predictions show an average of 3% deviation from the ground-truth values. Compared with a GPU-based simulation, the AME-based prediction is about $10^{6}\times$ faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12836v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ao Liu, Jie Han, Qin Wang, Zhigang Mao, Honglan Jiang</dc:creator>
    </item>
    <item>
      <title>General-Purpose Multicore Architectures</title>
      <link>https://arxiv.org/abs/2408.12999</link>
      <description>arXiv:2408.12999v1 Announce Type: new 
Abstract: The first years of the 2000s led to an inflection point in computer architectures: while the number of available transistors on a chip continued to grow, crucial transistor scaling properties started to break down and result in increasing power consumption, while aggressive single-core performance optimizations were resulting in diminishing returns due to inherent limits in instruction-level parallelism. This led to the rise of multicore CPU architectures, which are now commonplace in modern computers at all scales. In this chapter, we discuss the evolution of multicore CPUs since their introduction. Starting with a historic overview of multiprocessing, we explore the basic microarchitecture of a multicore CPU, key challenges resulting from shared memory resources, operating system modifications to optimize multicore CPU support, popular metrics for multicore evaluation, and recent trends in multicore CPU design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12999v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saugata Ghose</dc:creator>
    </item>
    <item>
      <title>NAS-Cap: Deep-Learning Driven 3-D Capacitance Extraction with Neural Architecture Search and Data Augmentation</title>
      <link>https://arxiv.org/abs/2408.13195</link>
      <description>arXiv:2408.13195v1 Announce Type: new 
Abstract: More accurate capacitance extraction is demanded for designing integrated circuits under advanced process technology. The pattern matching approach and the field solver for capacitance extraction have the drawbacks of inaccuracy and large computational cost, respectively. Recent work \cite{yang2023cnn} proposes a grid-based data representation and a convolutional neural network (CNN) based capacitance models (called CNN-Cap), which opens the third way for 3-D capacitance extraction to get accurate results with much less time cost than field solver. In this work, the techniques of neural architecture search (NAS) and data augmentation are proposed to train better CNN models for 3-D capacitance extraction. Experimental results on datasets from different designs show that the obtained NAS-Cap models achieve remarkably higher accuracy than CNN-Cap, while consuming less runtime for inference and space for model storage. Meanwhile, the transferability of the NAS is validated, as the once searched architecture brought similar error reduction on coupling/total capacitance for the test cases from different design and/or process technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13195v1</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyuan Li, Dingcheng Yang, Chunyan Pei, Wenjian Yu</dc:creator>
    </item>
    <item>
      <title>TReX- Reusing Vision Transformer's Attention for Efficient Xbar-based Computing</title>
      <link>https://arxiv.org/abs/2408.12742</link>
      <description>arXiv:2408.12742v1 Announce Type: cross 
Abstract: Due to the high computation overhead of Vision Transformers (ViTs), In-memory Computing architectures are being researched towards energy-efficient deployment in edge-computing scenarios. Prior works have proposed efficient algorithm-hardware co-design and IMC-architectural improvements to improve the energy-efficiency of IMC-implemented ViTs. However, all prior works have neglected the overhead and co-depencence of attention blocks on the accuracy-energy-delay-area of IMC-implemented ViTs. To this end, we propose TReX- an attention-reuse-driven ViT optimization framework that effectively performs attention reuse in ViT models to achieve optimal accuracy-energy-delay-area tradeoffs. TReX optimally chooses the transformer encoders for attention reuse to achieve near iso-accuracy performance while meeting the user-specified delay requirement. Based on our analysis on the Imagenet-1k dataset, we find that TReX achieves 2.3x (2.19x) EDAP reduction and 1.86x (1.79x) TOPS/mm2 improvement with ~1% accuracy drop in case of DeiT-S (LV-ViT-S) ViT models. Additionally, TReX achieves high accuracy at high EDAP reduction compared to state-of-the-art token pruning and weight sharing approaches. On NLP tasks such as CoLA, TReX leads to 2% higher non-ideal accuracy compared to baseline at 1.6x lower EDAP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12742v1</guid>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhishek Moitra, Abhiroop Bhattacharjee, Youngeun Kim, Priyadarshini Panda</dc:creator>
    </item>
    <item>
      <title>When In-memory Computing Meets Spiking Neural Networks -- A Perspective on Device-Circuit-System-and-Algorithm Co-design</title>
      <link>https://arxiv.org/abs/2408.12767</link>
      <description>arXiv:2408.12767v1 Announce Type: cross 
Abstract: This review explores the intersection of bio-plausible artificial intelligence in the form of Spiking Neural Networks (SNNs) with the analog In-Memory Computing (IMC) domain, highlighting their collective potential for low-power edge computing environments. Through detailed investigation at the device, circuit, and system levels, we highlight the pivotal synergies between SNNs and IMC architectures. Additionally, we emphasize the critical need for comprehensive system-level analyses, considering the inter-dependencies between algorithms, devices, circuit &amp; system parameters, crucial for optimal performance. An in-depth analysis leads to identification of key system-level bottlenecks arising from device limitations which can be addressed using SNN-specific algorithm-hardware co-design techniques. This review underscores the imperative for holistic device to system design space co-exploration, highlighting the critical aspects of hardware and algorithm research endeavors for low-power neuromorphic solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12767v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhishek Moitra, Abhiroop Bhattacharjee, Yuhang Li, Youngeun Kim, Priyadarshini Panda</dc:creator>
    </item>
    <item>
      <title>Intelligent OPC Engineer Assistant for Semiconductor Manufacturing</title>
      <link>https://arxiv.org/abs/2408.12775</link>
      <description>arXiv:2408.12775v1 Announce Type: cross 
Abstract: Advancements in chip design and manufacturing have enabled the processing of complex tasks such as deep learning and natural language processing, paving the way for the development of artificial general intelligence (AGI). AI, on the other hand, can be leveraged to innovate and streamline semiconductor technology from planning and implementation to manufacturing. In this paper, we present \textit{Intelligent OPC Engineer Assistant}, an AI/LLM-powered methodology designed to solve the core manufacturing-aware optimization problem known as optical proximity correction (OPC). The methodology involves a reinforcement learning-based OPC recipe search and a customized multi-modal agent system for recipe summarization. Experiments demonstrate that our methodology can efficiently build OPC recipes on various chip designs with specially handled design topologies, a task that typically requires the full-time effort of OPC engineers with years of experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12775v1</guid>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guojin Chen, Haoyu Yang, Haoxing Ren, Bei Yu</dc:creator>
    </item>
    <item>
      <title>Multi-Objective Hardware-Mapping Co-Optimisation for Multi-DNN Workloads on Chiplet-based Accelerators</title>
      <link>https://arxiv.org/abs/2210.14657</link>
      <description>arXiv:2210.14657v2 Announce Type: replace 
Abstract: The need to efficiently execute different Deep Neural Networks (DNNs) on the same computing platform, coupled with the requirement for easy scalability, makes Multi-Chip Module (MCM)-based accelerators a preferred design choice. Such an accelerator brings together heterogeneous sub-accelerators in the form of chiplets, interconnected by a Network-on-Package (NoP). This paper addresses the challenge of selecting the most suitable sub-accelerators, configuring them, determining their optimal placement in the NoP, and mapping the layers of a predetermined set of DNNs spatially and temporally. The objective is to minimise execution time and energy consumption during parallel execution while also minimising the overall cost, specifically the silicon area, of the accelerator.
  This paper presents MOHaM, a framework for multi-objective hardware-mapping co-optimisation for multi-DNN workloads on chiplet-based accelerators. MOHaM exploits a multi-objective evolutionary algorithm that has been specialised for the given problem by incorporating several customised genetic operators. MOHaM is evaluated against state-of-the-art Design Space Exploration (DSE) frameworks on different multi-DNN workload scenarios. The solutions discovered by MOHaM are Pareto optimal compared to those by the state-of-the-art. Specifically, MOHaM-generated accelerator designs can reduce latency by up to $96\%$ and energy by up to $96.12\%$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.14657v2</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhijit Das, Enrico Russo, Maurizio Palesi</dc:creator>
    </item>
    <item>
      <title>Learning Generalizable Program and Architecture Representations for Performance Modeling</title>
      <link>https://arxiv.org/abs/2310.16792</link>
      <description>arXiv:2310.16792v3 Announce Type: replace-cross 
Abstract: Performance modeling is an essential tool in many areas, including performance characterization/optimization, design space exploration, and resource allocation problems, to name a few. However, existing performance modeling approaches have limitations, such as high computational cost for discrete-event simulators, narrow flexibility of hardware emulators, or restricted accuracy/generality of analytical/data-driven models. To address these limitations, this paper proposes PerfVec, a novel deep learning-based performance modeling framework that learns high-dimensional and independent/orthogonal program and microarchitecture representations. Once learned, a program representation can be used to predict its performance on any microarchitecture, and likewise, a microarchitecture representation can be applied in the performance prediction of any program. Additionally, PerfVec yields a foundation model that captures the performance essence of instructions, which can be directly used by developers in numerous performance modeling related tasks without incurring its training cost. The evaluation demonstrates that PerfVec is more general and efficient than previous approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16792v3</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingda Li, Thomas Flynn, Adolfy Hoisie</dc:creator>
    </item>
  </channel>
</rss>
