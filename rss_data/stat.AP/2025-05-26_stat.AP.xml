<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 May 2025 04:00:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Integrating Region-Specific SARS-CoV-2 Data for Statistical Wastewater Monitoring</title>
      <link>https://arxiv.org/abs/2505.18682</link>
      <description>arXiv:2505.18682v1 Announce Type: new 
Abstract: Wastewater data can be very useful for epidemic control during a disease outbreak and proper synthesis of different sources of information can be integrated towards an alerting system, that can be used for decision support. Wastewater data are considered to be of high quality, since they do not depend on testing and can take into account asymptomatic cases. However, little effort has been given into utilizing such information in statistical process control procedures, usually aimed at industrial problems. In this article, we demonstrate how wastewater data can be utilized in health surveillance and propose a statistical framework that can act as a decision support tool. Specifically, we analyze SARS-CoV-2 wastewater data from Austria, constructing summary variables to implicitly describe the Covid-19 prevalence and, based on them, we assess the effectiveness of the current sampling strategy of Austria. We propose a framework of a statistical process monitoring system to aid epidemic management procedures in case SARS-CoV-2 concentration gets dangerously high.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18682v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anastasios Apsemidis, Karin Weyermair, Hans Peter St\"uger, Sabrina Kuchling, Tadej Zerak, Oliver Alber</dc:creator>
    </item>
    <item>
      <title>Degradation-Aware and Machine Learning-Driven Uncertainty Quantification in Crystal Plasticity Finite Element: Texture-Driven Plasticity in 316L Stainless Steel</title>
      <link>https://arxiv.org/abs/2505.18891</link>
      <description>arXiv:2505.18891v1 Announce Type: new 
Abstract: The mechanical properties and long-term structural reliability of crystalline materials are strongly influenced by microstructural features such as grain size, morphology, and crystallographic texture. These characteristics not only determine the initial mechanical behavior but also govern the progression of degradation mechanisms, such as strain localization, fatigue damage, and microcrack initiation under service conditions. Variability in these microstructural attributes, introduced during manufacturing or evolving through in-service degradation, leads to uncertainty in material performance. Therefore, understanding and quantifying microstructure-sensitive plastic deformation is critical for assessing degradation risk in high-value mechanical systems. This study presents a first-of-its-kind machine learning-driven framework that couples high-fidelity crystal plasticity finite element (CPFE) simulations with data-driven surrogate modeling to accelerate degradation-aware uncertainty quantification in welded structural alloys. Specifically, the impact of crystallographic texture variability in 316L stainless steel weldments, characterized via high-throughput electron backscatter diffraction (EBSD), is examined through CPFE simulations on calibrated representative volume elements (RVEs). A polynomial chaos expansion-based surrogate model is then trained to efficiently emulate the CPFE response using only 200 simulations, reducing computational cost by several orders of magnitude compared to conventional Monte Carlo analysis. The surrogate enables rapid quantification of uncertainty in stress-strain behavior and identifies texture components such as Cube and Goss as key drivers of degradation-relevant plastic response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18891v1</guid>
      <category>stat.AP</category>
      <category>cond-mat.mtrl-sci</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dinesh Kumar, Eralp Demir, Julio Spadotto, Kazuma Kobayashi, Syed Bahauddin Alam, Brian Connolly, Ed Pickering, Paul Wilcox, David Knowles, Mahmoud Mostafavi</dc:creator>
    </item>
    <item>
      <title>Unsupervised cell segmentation by fast Gaussian Processes</title>
      <link>https://arxiv.org/abs/2505.18902</link>
      <description>arXiv:2505.18902v1 Announce Type: new 
Abstract: Cell boundary information is crucial for analyzing cell behaviors from time-lapse microscopy videos. Existing supervised cell segmentation tools, such as ImageJ, require tuning various parameters and rely on restrictive assumptions about the shape of the objects. While recent supervised segmentation tools based on convolutional neural networks enhance accuracy, they depend on high-quality labelled images, making them unsuitable for segmenting new types of objects not in the database. We developed a novel unsupervised cell segmentation algorithm based on fast Gaussian processes for noisy microscopy images without the need for parameter tuning or restrictive assumptions about the shape of the object. We derived robust thresholding criteria adaptive for heterogeneous images containing distinct brightness at different parts to separate objects from the background, and employed watershed segmentation to distinguish touching cell objects. Both simulated studies and real-data analysis of large microscopy images demonstrate the scalability and accuracy of our approach compared with the alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18902v1</guid>
      <category>stat.AP</category>
      <category>cs.CV</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laura Baracaldo, Blythe King, Haoran Yan, Yizi Lin, Nina Miolane, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>Online activity prediction via generalized Indian buffet process models</title>
      <link>https://arxiv.org/abs/2505.19643</link>
      <description>arXiv:2505.19643v1 Announce Type: new 
Abstract: Online A/B experiments generate millions of user-activity records each day, yet experimenters need timely forecasts to guide roll-outs and safeguard user experience. Motivated by the problem of activity prediction for A/B tests at Amazon, we introduce a Bayesian nonparametric model for predicting both first-time and repeat triggers in web experiments. The model is based on the stable beta-scaled process prior, which allows for capturing heavy-tailed behaviour without strict parametric assumptions. All posterior and predictive quantities are available in closed form, allowing for fast inference even on large-scale datasets. Simulation studies and a retrospective analysis of 1,774 production experiments show improved accuracy in forecasting new users and total triggers compared with state-of-the-art competitors, especially when only a few pilot days are observed. The framework enables shorter tests while preserving calibrated uncertainty estimates. Although motivated by Amazon's experimentation platform, the method extends to other applications that require rapid, distribution-free prediction of sparse count processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19643v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mario Beraha, Lorenzo Masoero, Stefano Favaro, Thomas S. Richardson</dc:creator>
    </item>
    <item>
      <title>Computation of statistical power and sample size for in vivo research models</title>
      <link>https://arxiv.org/abs/2505.19666</link>
      <description>arXiv:2505.19666v1 Announce Type: new 
Abstract: Sample size calculation is crucial in biomedical in vivo research investigations mainly for two reasons: to design the most resource-efficient studies and to safeguard ethical issues when alive animals are subjects of testing. In this context, power analysis has been widely applied to compute the sample size by predetermining the desired statistical power and the significance level. To verify whether the assumption of a null hypothesis is true, repeated measures analysis of variance (ANOVA) is used to test the differences between multiple experimental groups and control group(s). In this article, we focus on the a priori power analysis, for testing multiple parameters and calculating the power of experimental designs, which is suitable to compute the sample size of trial groups in repeated measures ANOVA. We first describe repeated measures ANOVA and the statistical power from a practical aspect of biomedical research. Furthermore, we apply the G*Power software to conduct the a priori power analysis using examples of repeated measures ANOVA with three groups and five time points. We aim not to use the typical technically adapted statistical language. This will enable experimentalists to confidently formulate power calculation and sample size calculation easier and more accurately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19666v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hasan Al-Nashash, Jiajin Wei, Ke Yang, Ayman Alzaatreh, Mohsen Adeli, Tiejun Tong, Angelo All</dc:creator>
    </item>
    <item>
      <title>The evolving categories multinomial distribution: introduction with applications to movement ecology and vote transfer</title>
      <link>https://arxiv.org/abs/2505.20151</link>
      <description>arXiv:2505.20151v1 Announce Type: new 
Abstract: We introduce the evolving categories multinomial (ECM) distribution for multivariate count data taken over time. This distribution models the counts of individuals following iid stochastic dynamics among categories, with the number and identity of the categories also evolving over time. We specify the one-time and two-times marginal distributions of the counts and the first and second order moments. When the total number of individuals is unknown, placing a Poisson prior on it yields a new distribution (ECM-Poisson), whose main properties we also describe. Since likelihoods are intractable or impractical, we propose two estimating functions for parameter estimation: a Gaussian pseudo-likelihood and a pairwise composite likelihood. We show two application scenarios: the inference of movement parameters of animals moving continuously in space-time with irregular survey regions, and the inference of vote transfer in two-rounds elections. We give three illustrations: a simulation study with Ornstein-Uhlenbeck moving individuals, paying special attention to the autocorrelation parameter; the inference of movement and behavior parameters of lesser prairie-chickens; and the estimation of vote transfer in the 2021 Chilean presidential election.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20151v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ricardo Carrizo Vergara, Marc K\'ery, Trevor Hefley</dc:creator>
    </item>
    <item>
      <title>Down to the Last Strike: The Effect of the Jury Lottery on Criminal Convictions</title>
      <link>https://arxiv.org/abs/2505.18431</link>
      <description>arXiv:2505.18431v1 Announce Type: cross 
Abstract: How much does luck matter to a criminal defendant in a jury trial? We use rich data on jury selection to causally estimate how parties who are randomly assigned a less favorable jury (as proxied by whether their attorneys exhaust their peremptory strikes) fare at trial. Our novel identification strategy is unique in that it captures variation in juror predisposition coming from variables unobserved by the econometrician but observed by attorneys. We find that criminal defendants who lose the "jury lottery" are more likely to be convicted than their similarly-situated counterparts, with a significant increase (~19 percentage points) for Black defendants. Our results suggest that a considerable number of cases would result in different verdicts if retried with new (counterfactual) random draws of the jury pool, raising concerns about the variance of justice in the criminal legal system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18431v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Scott Kostyshak, Neel U. Sukhatme</dc:creator>
    </item>
    <item>
      <title>DB-KSVD: Scalable Alternating Optimization for Disentangling High-Dimensional Embedding Spaces</title>
      <link>https://arxiv.org/abs/2505.18441</link>
      <description>arXiv:2505.18441v1 Announce Type: cross 
Abstract: Dictionary learning has recently emerged as a promising approach for mechanistic interpretability of large transformer models. Disentangling high-dimensional transformer embeddings, however, requires algorithms that scale to high-dimensional data with large sample sizes. Recent work has explored sparse autoencoders (SAEs) for this problem. However, SAEs use a simple linear encoder to solve the sparse encoding subproblem, which is known to be NP-hard. It is therefore interesting to understand whether this structure is sufficient to find good solutions to the dictionary learning problem or if a more sophisticated algorithm could find better solutions. In this work, we propose Double-Batch KSVD (DB-KSVD), a scalable dictionary learning algorithm that adapts the classic KSVD algorithm. DB-KSVD is informed by the rich theoretical foundations of KSVD but scales to datasets with millions of samples and thousands of dimensions. We demonstrate the efficacy of DB-KSVD by disentangling embeddings of the Gemma-2-2B model and evaluating on six metrics from the SAEBench benchmark, where we achieve competitive results when compared to established approaches based on SAEs. By matching SAE performance with an entirely different optimization approach, our results suggest that (i) SAEs do find strong solutions to the dictionary learning problem and (ii) that traditional optimization approaches can be scaled to the required problem sizes, offering a promising avenue for further research. We provide an implementation of DB-KSVD at https://github.com/RomeoV/KSVD.jl.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18441v1</guid>
      <category>cs.LG</category>
      <category>cs.MS</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Romeo Valentin, Sydney M. Katz, Vincent Vanhoucke, Mykel J. Kochenderfer</dc:creator>
    </item>
    <item>
      <title>LLMs for Supply Chain Management</title>
      <link>https://arxiv.org/abs/2505.18597</link>
      <description>arXiv:2505.18597v1 Announce Type: cross 
Abstract: The development of large language models (LLMs) has provided new tools for research in supply chain management (SCM). In this paper, we introduce a retrieval-augmented generation (RAG) framework that dynamically integrates external knowledge into the inference process, and develop a domain-specialized SCM LLM, which demonstrates expert-level competence by passing standardized SCM examinations and beer game tests. We further employ the use of LLMs to conduct horizontal and vertical supply chain games, in order to analyze competition and cooperation within supply chains. Our experiments show that RAG significantly improves performance on SCM tasks. Moreover, game-theoretic analysis reveals that the LLM can reproduce insights from the classical SCM literature, while also uncovering novel behaviors and offering fresh perspectives on phenomena such as the bullwhip effect. This paper opens the door for exploring cooperation and competition for complex supply chain network through the lens of LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18597v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haojie Wang, Jiuyun Jiang, L. Jeff Hong, Guangxin Jiang</dc:creator>
    </item>
    <item>
      <title>A concordance coefficient for lattice data: An application to poverty indices in Chile</title>
      <link>https://arxiv.org/abs/2505.18935</link>
      <description>arXiv:2505.18935v1 Announce Type: cross 
Abstract: This paper introduces a novel coefficient for measuring agreement between two lattice sequences observed in the same areal units, motivated by the analysis of different methodologies for measuring poverty rates in Chile. Building on the multivariate concordance coefficient framework, our approach accounts for dependencies in the multivariate lattice process using a non-negative definite matrix of weights, assuming a Multivariate Conditionally Autoregressive (GMCAR) process. We adopt a Bayesian perspective for inference, using summaries from Bayesian estimates. The methodology is illustrated through an analysis of poverty rates in the Metropolitan and Valpara\'iso regions of Chile, with High Posterior Density (HPD) intervals provided for the poverty rates. This work addresses a methodological gap in the understanding of agreement coefficients and enhances the usability of these measures in the context of social variables typically assessed in areal units.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18935v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronny Vallejos, Clemente Ferrer, Jorge Mateu</dc:creator>
    </item>
    <item>
      <title>Bayesian sparse modeling for interpretable prediction of hydroxide ion conductivity in anion-conductive polymer membranes</title>
      <link>https://arxiv.org/abs/2505.19044</link>
      <description>arXiv:2505.19044v1 Announce Type: cross 
Abstract: Anion-conductive polymer membranes have attracted considerable attention as solid electrolytes for alkaline fuel cells and electrolysis cells. Their hydroxide ion conductivity varies depending on factors such as the type and distribution of quaternary ammonium groups, as well as the structure and connectivity of hydrophilic and hydrophobic domains. In particular, the size and connectivity of hydrophilic domains significantly influence the mobility of hydroxide ions; however, this relationship has remained largely qualitative. In this study, we calculated the number of key constituent elements in the hydrophilic and hydrophobic units based on the copolymer composition, and investigated their relationship with hydroxide ion conductivity by using Bayesian sparse modeling. As a result, we successfully identified composition-derived features that are critical for accurately predicting hydroxide ion conductivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19044v1</guid>
      <category>cond-mat.soft</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryo Murakami, Kenji Miyatake, Ahmed Mohamed Ahmed Mahmoud, Hideki Yoshikawa, Kenji Nagata</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Variable Selection in Model-Based Clustering with Missing Not at Random</title>
      <link>https://arxiv.org/abs/2505.19093</link>
      <description>arXiv:2505.19093v1 Announce Type: cross 
Abstract: Model-based clustering integrated with variable selection is a powerful tool for uncovering latent structures within complex data. However, its effectiveness is often hindered by challenges such as identifying relevant variables that define heterogeneous subgroups and handling data that are missing not at random, a prevalent issue in fields like transcriptomics. While several notable methods have been proposed to address these problems, they typically tackle each issue in isolation, thereby limiting their flexibility and adaptability. This paper introduces a unified framework designed to address these challenges simultaneously. Our approach incorporates a data-driven penalty matrix into penalized clustering to enable more flexible variable selection, along with a mechanism that explicitly models the relationship between missingness and latent class membership. We demonstrate that, under certain regularity conditions, the proposed framework achieves both asymptotic consistency and selection consistency, even in the presence of missing data. This unified strategy significantly enhances the capability and efficiency of model-based clustering, advancing methodologies for identifying informative variables that define homogeneous subgroups in the presence of complex missing data patterns. The performance of the framework, including its computational efficiency, is evaluated through simulations and demonstrated using both synthetic and real-world transcriptomic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19093v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Binh H. Ho, Long Nguyen Chi, TrungTin Nguyen, Binh T. Nguyen, Van Ha Hoang, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>Do Large Language Models (Really) Need Statistical Foundations?</title>
      <link>https://arxiv.org/abs/2505.19145</link>
      <description>arXiv:2505.19145v1 Announce Type: cross 
Abstract: Large language models (LLMs) represent a new paradigm for processing unstructured data, with applications across an unprecedented range of domains. In this paper, we address, through two arguments, whether the development and application of LLMs would genuinely benefit from foundational contributions from the statistics discipline. First, we argue affirmatively, beginning with the observation that LLMs are inherently statistical models due to their profound data dependency and stochastic generation processes, where statistical insights are naturally essential for handling variability and uncertainty. Second, we argue that the persistent black-box nature of LLMs -- stemming from their immense scale, architectural complexity, and development practices often prioritizing empirical performance over theoretical interpretability -- renders closed-form or purely mechanistic analyses generally intractable, thereby necessitating statistical approaches due to their flexibility and often demonstrated effectiveness. To substantiate these arguments, the paper outlines several research areas -- including alignment, watermarking, uncertainty quantification, evaluation, and data mixture optimization -- where statistical methodologies are critically needed and are already beginning to make valuable contributions. We conclude with a discussion suggesting that statistical research concerning LLMs will likely form a diverse ``mosaic'' of specialized topics rather than deriving from a single unifying theory, and highlighting the importance of timely engagement by our statistics community in LLM research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19145v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weijie Su</dc:creator>
    </item>
    <item>
      <title>svc: An R package for Spatially Varying Coefficient Models</title>
      <link>https://arxiv.org/abs/2505.19287</link>
      <description>arXiv:2505.19287v1 Announce Type: cross 
Abstract: Traditional regression models assume stationary relationships between predictors and responses, failing to capture the spatial heterogeneity present in many environmental, epidemiological, and ecological processes. To address this limitation, we develop a scalable Bayesian framework for spatially varying coefficient (SVC) models, implemented in the \pkg{svc} R package (available at https://github.com/jdta95/svc), which allows regression coefficients to vary smoothly over space. Our approach combines three key computational innovations: (1) a subset Gaussian process approximation that reduces the computational burden from $O(n^3)$ to $O(m^3)$ with $m&lt;n$, while maintaining predictive accuracy; (2) a robust adaptive Metropolis (RAM) algorithm that automatically tunes proposal distributions for efficient MCMC sampling of spatial range parameters; and (3) optimized linear algebra operations leveraging precomputed distance matrices and Cholesky decompositions to accelerate covariance calculations. We present the model's theoretical foundation, prior specification, and Gibbs sampling algorithm, with a focus on practical implementation for large spatial datasets. Simulation studies demonstrate that our method outperforms existing approaches in computational efficiency while maintaining competitive estimation accuracy. We illustrate its application in an analysis of land surface temperature (LST) data, revealing spatially varying effects of vegetation and emissivity that would be obscured by traditional regression techniques. The \pkg{svc} package provides researchers with a flexible, efficient tool for uncovering and quantifying nonstationary spatial relationships across diverse scientific domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19287v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justice Akuoko-Frimpong, Edward Shao, Jonathan Ta</dc:creator>
    </item>
    <item>
      <title>Weighted Tail Random Variable: A Novel Framework with Stochastic Properties and Applications</title>
      <link>https://arxiv.org/abs/2505.19824</link>
      <description>arXiv:2505.19824v1 Announce Type: cross 
Abstract: This paper introduces a novel framework to construct the probability density function (PDF) of non-negative continuous random variables. The proposed framework uses two functions: one is the survival function (SF) of a non-negative continuous random variable, and the other is a weight function, which is an increasing and differentiable function satisfying some properties. The resulting random variable is referred to as the weighted tail random variable (WTRV) corresponding to the given random variable and the weight function. We investigate several reliability properties of the WTRV and establish various stochastic orderings between a random variable and its WTRV, as well as between two WTRVs. Using this framework, we construct a WTRV of the Kumaraswamy distribution. We conduct goodness-of-fit tests for two real-world datasets, applied to the Kumaraswamy distribution and its corresponding WTRV. The test results indicate that the WTRV offers a superior fit compared to the Kumaraswamy distribution, which demonstrates the utility of the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19824v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarikul Islam, Nitin Gupta</dc:creator>
    </item>
    <item>
      <title>Bayesian Dynamical Modeling of Fixational Eye Movements</title>
      <link>https://arxiv.org/abs/2303.11941</link>
      <description>arXiv:2303.11941v2 Announce Type: replace 
Abstract: Humans constantly move their eyes, even during visual fixations, where miniature (or fixational) eye movements occur involuntarily. Fixational eye movements comprise slow components (physiological drift and tremor) and fast components (microsaccades). The complex dynamics of physiological drift can be modeled qualitatively as a statistically self-avoiding random walk (SAW model, Engbert, Mergenthaler, Sinn, &amp; Pikovsky, 2011). In this study, we implement a data assimilation approach for the SAW model to explain statistics of fixational eye movements and microsaccades in experimental data obtained from high-resolution eye-tracking. We discuss and analyze the likelihood function for the SAW model, which allows us to apply Bayesian parameter estimation at the level of individual human observers. Based on model fitting, we find a relationship between the activation predicted by the SAW model and the occurrence of microsaccades. The model's latent activation relative to microsaccade onsets and offsets using experimental data lends support to the existence of a triggering mechanism for microsaccades. Our findings suggest that the SAW model can capture individual differences and serve as a tool for exploring the relationship between physiological drift and microsaccades as the two most essential components of fixational eye movements. Our results contribute to understanding individual variability in microsaccade behaviors and the role of fixational eye movements in visual information processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.11941v2</guid>
      <category>stat.AP</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lisa Schwetlick, Sebastian Reich, Ralf Engbert</dc:creator>
    </item>
    <item>
      <title>Analysis of Log Data from an International Online Educational Assessment System: A Multi-state Survival Modeling Approach to Reaction Time between and across Action Sequence</title>
      <link>https://arxiv.org/abs/2403.14908</link>
      <description>arXiv:2403.14908v2 Announce Type: replace 
Abstract: With increasingly available computer-based or online assessments, researchers have shown keen interest in analyzing log data to improve our understanding of test takers' problem-solving processes. In this paper, we propose a multi-state survival model (MSM) to action sequence data from log files, focusing on modeling test takers' reaction times between actions, in order to investigate which factors and how they influence test takers' transition speed between actions. We specifically identify the key actions that differentiate correct and incorrect answers, compare transition probabilities between these groups, and analyze their distinct problem-solving patterns. Through simulation studies and sensitivity analyses, we evaluate the robustness of our proposed model. We demonstrate the proposed approach using problem-solving items from the Programme for the International Assessment of Adult Competencies (PIAAC).</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14908v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jina Park, Ick Hoon Jin, Minjeong Jeon</dc:creator>
    </item>
    <item>
      <title>Analyzing and Forecasting Success in the Men's Ice Hockey World (Junior) Championships Using a Dynamic Ranking Model</title>
      <link>https://arxiv.org/abs/2409.05714</link>
      <description>arXiv:2409.05714v2 Announce Type: replace 
Abstract: What factors contribute to the success of national teams in the Men's Ice Hockey World Championships and the Men's Ice Hockey World Junior Championships? This study examines whether hosting the tournament provides a home advantage; the influence of past tournament performances; the impact of players' physical characteristics such as height, weight, and age; and the value of experience from the World Championships compared to the NHL and other leagues. We employ a dynamic ranking model based on the Plackett-Luce distribution with time-varying strength parameters driven by the score. The results show that experience in the IIHF tournaments outweighs experience in the NHL. Furthermore, in junior championships, there is a significant home advantage, and shorter, heavier players tend to have an edge. In senior championships, future success is linked to past achievements in both junior and senior championships, with younger teams performing better. Finally, we conduct a forecasting analysis to predict the probabilities of winning the tournament, earning a medal, and advancing to the playoff phase.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05714v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1515/jqas-2024-0137</arxiv:DOI>
      <arxiv:journal_reference>Hol\'y, V. (2025). Analyzing and Forecasting Success in the Men's Ice Hockey World (Junior) Championships Using a Dynamic Ranking Model. Journal of Quantitative Analysis in Sports</arxiv:journal_reference>
      <dc:creator>Vladim\'ir Hol\'y</dc:creator>
    </item>
    <item>
      <title>Bootstrap Prediction and Confidence Bands for Frequency Response Functions in Posturography</title>
      <link>https://arxiv.org/abs/2504.20588</link>
      <description>arXiv:2504.20588v2 Announce Type: replace 
Abstract: The frequency response function (FRF) is an established way to describe the outcome of experiments in posture control literature. The FRF is an empirical transfer function between an input stimulus and the induced body segment sway profile, represented as a vector of complex values associated with a vector of frequencies. For this reason, testing the components of the FRF independently with Bonferroni correction can result in a too-conservative approach. Performing statistics on scalar values defined on the FRF, e.g., comparing the averages, implies an arbitrary decision by the experimenter. This work proposes bootstrap prediction and confidence bands as general methods to evaluate the outcome of posture control experiments, overcoming the foretold limitations of previously used approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20588v2</guid>
      <category>stat.AP</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s40799-025-00808-2</arxiv:DOI>
      <dc:creator>Vittorio Lippi</dc:creator>
    </item>
    <item>
      <title>Out-of-distribution Reject Option Method for Dataset Shift Problem in Early Disease Onset Prediction</title>
      <link>https://arxiv.org/abs/2405.19864</link>
      <description>arXiv:2405.19864v2 Announce Type: replace-cross 
Abstract: Machine learning is increasingly used to predict lifestyle-related disease onset using health and medical data. However, its predictive accuracy for use is often hindered by dataset shift, which refers to discrepancies in data distribution between the training and testing datasets. This issue leads to the misclassification of out-of-distribution (OOD) data. To diminish dataset shift in real-world settings, this paper proposes the out-of-distribution reject option for prediction (ODROP). This method integrates an OOD detection model to preclude OOD data from the prediction phase. We used two real-world health checkup datasets (Hirosaki and Wakayama) with dataset shift, across three disease onset prediction tasks: diabetes, dyslipidemia, and hypertension. Both components of ODROP method -- the OOD detection model and the prediction model -- were trained on the Hirosaki dataset. We assessed the effectiveness of ODROP on the Wakayama dataset using AUROC-rejection rate curve plot. In the five OOD detection approaches (the variational autoencoder, neural network ensemble std, neural network ensemble epistemic, neural network energy, and neural network gaussian mixture based energy measurement), the variational autoencoder method demonstrated notably higher stability and a greater improvement in AUROC. For example, in the Wakayama dataset, the AUROC for diabetes onset increased from 0.80 without ODROP to 0.90 at a 31.1% rejection rate, and for dyslipidemia, it improved from 0.70 without ODROP to 0.76 at a 34% rejection rate. In addition, we categorized dataset shifts into two types using SHAP clustering -- those that considerably affect predictions and those that do not. This study is the first to apply OOD detection to actual health and medical data, demonstrating its potential to substantially improve the accuracy and reliability of disease prediction models amidst dataset shift.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19864v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taisei Tosaki, Eiichiro Uchino, Ryosuke Kojima, Yohei Mineharu, Yuji Okamoto, Mikio Arita, Nobuyuki Miyai, Yoshinori Tamada, Tatsuya Mikami, Koichi Murashita, Shigeyuki Nakaji, Yasushi Okuno</dc:creator>
    </item>
    <item>
      <title>Spatial Hyperspheric Models for Compositional Data</title>
      <link>https://arxiv.org/abs/2410.03648</link>
      <description>arXiv:2410.03648v3 Announce Type: replace-cross 
Abstract: Compositional observations are an increasingly prevalent data source in spatial statistics. Analysis of such data is typically done on log-ratio transformations or via Dirichlet regression. However, these approaches often make unnecessarily strong assumptions (e.g., strictly positive components, exclusively negative correlations). An alternative approach uses square-root transformed compositions and directional distributions. Such distributions naturally allow for zero-valued components and positive correlations, yet they may include support outside the non-negative orthant and are not generative for compositional data. To overcome this challenge, we truncate the elliptically symmetric angular Gaussian (ESAG) distribution to the non-negative orthant. Additionally, we propose a spatial hyperspheric regression model that contains fixed and random multivariate spatial effects. The proposed model also contains a term that can be used to propagate uncertainty that may arise from precursory stochastic models (i.e., machine learning classification). We used our model in a simulation study and for a spatial analysis of classified bioacoustic signals of the Dryobates pubescens (downy woodpecker).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03648v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael R. Schwob, Mevin B. Hooten, Nicholas M. Calzada, Timothy H. Keitt</dc:creator>
    </item>
    <item>
      <title>Scalable Bayesian Semiparametric Additive Regression Models For Microbiome Studies</title>
      <link>https://arxiv.org/abs/2410.03911</link>
      <description>arXiv:2410.03911v2 Announce Type: replace-cross 
Abstract: Statistical analysis of microbiome data is challenging. Bayesian multinomial logistic-normal (MLN) models have gained popularity due to their ability to account for the count compositional nature of these data, but existing approaches are either computationally intractable or restricted to purely parametric or non-parametric methods, which limit their flexibility and scalability. In this work, we introduce \textit{MultiAddGPs}, a novel semi-parametric framework that integrates additive Gaussian Process (GP) regression within a Bayesian MLN model to disentangle linear and non-linear covariate effects, including non-stationary dynamics. Our approach builds on the computationally efficient Collapse-Uncollapse (CU) sampler and additive GP regression, introducing a novel back-sampling algorithm and marginal likelihood approximation for efficient inference and hyperparameter estimation. Our models are over 240,000 times faster than alternatives while simultaneously producing more accurate posterior estimates. Additionally, we incorporate non-stationary kernel functions designed to model treatment interventions and disease effects. We demonstrate our approach using simulated and real data studies and produce novel biological insights from a previously published human gut microbiome study. Our methods are publicly available as part of the \textit{fido} software package on CRAN \footnotemark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03911v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tinghua Chen, Michelle Pistner Nixon, Justin D. Silverman</dc:creator>
    </item>
    <item>
      <title>Judging It, Washing It: Scoring and Greenwashing Corporate Climate Disclosures using Large Language Models</title>
      <link>https://arxiv.org/abs/2502.15094</link>
      <description>arXiv:2502.15094v2 Announce Type: replace-cross 
Abstract: We study the use of large language models (LLMs) to both evaluate and greenwash corporate climate disclosures. First, we investigate the use of the LLM-as-a-Judge (LLMJ) methodology for scoring company-submitted reports on emissions reduction targets and progress. Second, we probe the behavior of an LLM when it is prompted to greenwash a response subject to accuracy and length constraints. Finally, we test the robustness of the LLMJ methodology against responses that may be greenwashed using an LLM. We find that two LLMJ scoring systems, numerical rating and pairwise comparison, are effective in distinguishing high-performing companies from others, with the pairwise comparison system showing greater robustness against LLM-greenwashed responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15094v2</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marianne Chuang, Gabriel Chuang, Cheryl Chuang, John Chuang</dc:creator>
    </item>
  </channel>
</rss>
