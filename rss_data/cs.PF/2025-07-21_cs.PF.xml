<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Jul 2025 02:28:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Photonic Fabric Platform for AI Accelerators</title>
      <link>https://arxiv.org/abs/2507.14000</link>
      <description>arXiv:2507.14000v2 Announce Type: new 
Abstract: This paper presents the Photonic FabricTM and the Photonic Fabric ApplianceTM (PFA), a photonic-enabled switch and memory subsystem that delivers low latency, high bandwidth, and low per-bit energy. By integrating high-bandwidth HBM3E memory, an on-module photonic switch, and external DDR5 in a 2.5D electro-optical system-in-package, the PFA offers up to 32 TB of shared memory alongside 115 Tbps of all-to-all digital switching. The Photonic FabricTM enables distributed AI training and inference to execute parallelism strategies more efficiently. The Photonic Fabric removes the silicon beachfront constraint that limits the fixed memory-to-compute ratio observed in virtually all current XPU accelerator designs. Replacing a local HBM stack on an XPU with a chiplet that connects to the Photonic Fabric increases its memory capacity and correspondingly its memory bandwidth by offering a flexible path to scaling well beyond the limitations of on-package HBM alone. We introduce CelestiSim, a lightweight analytical simulator validated on NVIDIA H100 and H200 systems. It is used to evaluate the performance of LLM reference and energy savings on PFA, without any significant change to the GPU core design. With the PFA, the simulation results show that up to 3.66x throughput and 1.40x latency improvements in LLM inference at 405B parameters, up to 7.04x throughput and 1.41x latency improvements at 1T parameters, and 60-90% energy savings in data movement for heavy collective operations in all LLM training scenarios. While these results are shown for NVIDIA GPUs, they can be applied similarly to other AI accelerator designs (XPUs) that share the same fundamental limitation of fixed memory to compute.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14000v2</guid>
      <category>cs.PF</category>
      <category>cs.AI</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jing Ding, Trung Diep</dc:creator>
    </item>
    <item>
      <title>Leveraging Multi-Instance GPUs through moldable task scheduling</title>
      <link>https://arxiv.org/abs/2507.13601</link>
      <description>arXiv:2507.13601v1 Announce Type: cross 
Abstract: NVIDIA MIG (Multi-Instance GPU) allows partitioning a physical GPU into multiple logical instances with fully-isolated resources, which can be dynamically reconfigured. This work highlights the untapped potential of MIG through moldable task scheduling with dynamic reconfigurations. Specifically, we propose a makespan minimization problem for multi-task execution under MIG constraints. Our profiling shows that assuming monotonicity in task work with respect to resources is not viable, as is usual in multicore scheduling. Relying on a state-of-the-art proposal that does not require such an assumption, we present FAR, a 3-phase algorithm to solve the problem. Phase 1 of FAR builds on a classical task moldability method, phase 2 combines Longest Processing Time First and List Scheduling with a novel repartitioning tree heuristic tailored to MIG constraints, and phase 3 employs local search via task moves and swaps. FAR schedules tasks in batches offline, concatenating their schedules on the fly in an improved way that favors resource reuse. Excluding reconfiguration costs, the List Scheduling proof shows an approximation factor of 7/4 on the NVIDIA A30 model. We adapt the technique to the particular constraints of an NVIDIA A100/H100 to obtain an approximation factor of 2. Including the reconfiguration cost, our real-world experiments reveal a makespan with respect to the optimum no worse than 1.22x for a well-known suite of benchmarks, and 1.10x for synthetic inputs inspired by real kernels. We obtain good experimental results for each batch of tasks, but also in the concatenation of batches, with large improvements over the state-of-the-art and proposals without GPU reconfiguration. Beyond the algorithm, the paper demonstrates the research potential of the MIG technology and suggests useful metrics, workload characterizations and evaluation techniques for future work in this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13601v1</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.PF</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jpdc.2025.105128</arxiv:DOI>
      <arxiv:journal_reference>Journal of Parallel and Distributed Computing, vol. 204, pages 105128 (2025)</arxiv:journal_reference>
      <dc:creator>Jorge Villarrubia, Luis Costero, Francisco D. Igual, Katzalin Olcoz</dc:creator>
    </item>
    <item>
      <title>The Proportional Fair Scheduler in Wavelength-Multiplexed Quantum Networks</title>
      <link>https://arxiv.org/abs/2507.13999</link>
      <description>arXiv:2507.13999v1 Announce Type: cross 
Abstract: We address the problem of optimal pumping strategies in quantum networks. These networks enable secure communication by distributing entangled photon pairs to user (or node) pairs. Quantum Key Distribution (QKD) protocols, like BBM92, generate secret keys from entangled photons. While secure communication and error correction are essential for any quantum communication channel, resource contention, optimization, and fairness issues are critical for networks. In this article, we analyze the performance of quantum networks, proposing simple distributed algorithms for QKD networks generating secret keys.
  There are significant advantages of pumping entangled photons in QKD networks, but challenges arise in practical implementations. The underlying channels are inherently time-varying, and thus data rates fluctuate between nodes. Moreover, multiple edges (node pairs) can be pumped simultaneously, albeit at the cost of a reduced secret key rate (SKR). These temporal and spatial constraints yield a complex decision-making problem whose solutions may favor a small set of user pairs to the detriment of overall, long-run network performance.
  We design adaptive pumping strategies that address these challenges in QKD networks. In particular, we find that a proportional fairness pumping strategy (PF-PS) stands out by dynamically prioritizing users with lower average secret key rates and optimally balancing fairness with throughput. The proposed algorithm is a natural extension to quantum networks of the Proportional Fair Scheduler deployed in 4G LTE and 5G mobile networks. Both theoretical analysis and numerical simulations confirm that PF-PS is optimal for entangled state distribution, and thus, when adapted appropriately, proportional fair pumping is a strong candidate for efficient resource allocation in quantum networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13999v1</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>IEEE International Conference on Quantum Computing &amp; Engineering (QCE) 2025</arxiv:journal_reference>
      <dc:creator>Sanidhay Bhambay, Siddarth Koduru Joshi, Thirupathaiah Vasantam, Neil Walton</dc:creator>
    </item>
  </channel>
</rss>
