<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Apr 2024 04:00:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Replicability analysis of high dimensional data accounting for dependence</title>
      <link>https://arxiv.org/abs/2404.05808</link>
      <description>arXiv:2404.05808v1 Announce Type: new 
Abstract: Replicability is the cornerstone of scientific research. We study the replicability of data from high-throughput experiments, where tens of thousands of features are examined simultaneously. Existing replicability analysis methods either ignore the dependence among features or impose strong modelling assumptions, producing overly conservative or overly liberal results. Based on $p$-values from two studies, we use a four-state hidden Markov model to capture the structure of local dependence. Our method effectively borrows information from different features and studies while accounting for dependence among features and heterogeneity across studies. We show that the proposed method has better power than competing methods while controlling the false discovery rate, both empirically and theoretically. Analyzing datasets from genome-wide association studies reveals new biological insights that otherwise cannot be obtained by using existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05808v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengfei Lyu, Xianyang Zhang, Hongyuan Cao</dc:creator>
    </item>
    <item>
      <title>fastcpd: Fast Change Point Detection in R</title>
      <link>https://arxiv.org/abs/2404.05933</link>
      <description>arXiv:2404.05933v1 Announce Type: new 
Abstract: Change point analysis is concerned with detecting and locating structure breaks in the underlying model of a sequence of observations ordered by time, space or other variables. A widely adopted approach for change point analysis is to minimize an objective function with a penalty term on the number of change points. This framework includes several well-established procedures, such as the penalized log-likelihood using the (modified) Bayesian information criterion (BIC) or the minimum description length (MDL). The resulting optimization problem can be solved in polynomial time by dynamic programming or its improved version, such as the Pruned Exact Linear Time (PELT) algorithm (Killick, Fearnhead, and Eckley 2012). However, existing computational methods often suffer from two primary limitations: (1) methods based on direct implementation of dynamic programming or PELT are often time-consuming for long data sequences due to repeated computation of the cost value over different segments of the data sequence; (2) state-of-the-art R packages do not provide enough flexibility for users to handle different change point settings and models. In this work, we present the fastcpd package, aiming to provide an efficient and versatile framework for change point detection in several commonly encountered settings. The core of our algorithm is built upon PELT and the sequential gradient descent method recently proposed by Zhang and Dawn (2023). We illustrate the usage of the fastcpd package through several examples, including mean/variance changes in a (multivariate) Gaussian sequence, parameter changes in regression models, structural breaks in ARMA/GARCH/VAR models, and changes in user-specified models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05933v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingchi Li, Xianyang Zhang</dc:creator>
    </item>
    <item>
      <title>Constructing hierarchical time series through clustering: Is there an optimal way for forecasting?</title>
      <link>https://arxiv.org/abs/2404.06064</link>
      <description>arXiv:2404.06064v1 Announce Type: new 
Abstract: Forecast reconciliation has attracted significant research interest in recent years, with most studies taking the hierarchy of time series as given. We extend existing work that uses time series clustering to construct hierarchies, with the goal of improving forecast accuracy, in three ways. First, we investigate multiple approaches to clustering, including not only different clustering algorithms, but also the way time series are represented and how distance between time series is defined. We find that cluster-based hierarchies lead to improvements in forecast accuracy relative to two-level hierarchies. Second, we devise an approach based on random permutation of hierarchies, keeping the structure of the hierarchy fixed, while time series are randomly allocated to clusters. In doing so, we find that improvements in forecast accuracy that accrue from using clustering do not arise from grouping together similar series but from the structure of the hierarchy. Third, we propose an approach based on averaging forecasts across hierarchies constructed using different clustering methods, that is shown to outperform any single clustering method. All analysis is carried out on two benchmark datasets and a simulated dataset. Our findings provide new insights into the role of hierarchy construction in forecast reconciliation and offer valuable guidance on forecasting practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06064v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bohan Zhang, Anastasios Panagiotelis, Han Li</dc:creator>
    </item>
    <item>
      <title>Supervised Contamination Detection, with Flow Cytometry Application</title>
      <link>https://arxiv.org/abs/2404.06093</link>
      <description>arXiv:2404.06093v1 Announce Type: new 
Abstract: The contamination detection problem aims to determine whether a set of observations has been contaminated, i.e. whether it contains points drawn from a distribution different from the reference distribution. Here, we consider a supervised problem, where labeled samples drawn from both the reference distribution and the contamination distribution are available at training time. This problem is motivated by the detection of rare cells in flow cytometry. Compared to novelty detection problems or two-sample testing, where only samples from the reference distribution are available, the challenge lies in efficiently leveraging the observations from the contamination detection to design more powerful tests. In this article, we introduce a test for the supervised contamination detection problem. We provide non-asymptotic guarantees on its Type I error, and characterize its detection rate. The test relies on estimating reference and contamination densities using histograms, and its power depends strongly on the choice of the corresponding partition. We present an algorithm for judiciously choosing the partition that results in a powerful test. Simulations illustrate the good empirical performances of our partition selection algorithm and the efficiency of our test. Finally, we showcase our method and apply it to a real flow cytometry dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06093v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Solenne Gaucher (CREST, FAIRPLAY), Gilles Blanchard (LMO, CELESTE), Fr\'ed\'eric Chazal (LMO, DATASHAPE)</dc:creator>
    </item>
    <item>
      <title>Adaptive Unit Root Inference in Autoregressions using the Lasso Solution Path</title>
      <link>https://arxiv.org/abs/2404.06205</link>
      <description>arXiv:2404.06205v1 Announce Type: new 
Abstract: We show that the activation knot of a potentially non-stationary regressor on the adaptive Lasso solution path in autoregressions can be leveraged for selection-free inference about a unit root. The resulting test has asymptotic power against local alternatives in $1/T$ neighbourhoods, unlike post-selection inference methods based on consistent model selection. Exploiting the information enrichment principle devised by Reinschl\"ussel and Arnold arXiv:2402.16580 [stat.ME] to improve the Lasso-based selection of ADF models, we propose a composite statistic and analyse its asymptotic distribution and local power function. Monte Carlo evidence shows that the combined test dominates the comparable post-selection inference methods of Tibshirani et al. [JASA, 2016, 514, 600-620] and may surpass the power of established unit root tests against local alternatives. We apply the new tests to groundwater level time series for Germany and find evidence rejecting stochastic trends to explain observed long-term declines in mean water levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06205v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Martin C. Arnold, Thilo Reinschl\"ussel</dc:creator>
    </item>
    <item>
      <title>Self-Labeling in Multivariate Causality and Quantification for Adaptive Machine Learning</title>
      <link>https://arxiv.org/abs/2404.05809</link>
      <description>arXiv:2404.05809v1 Announce Type: cross 
Abstract: Adaptive machine learning (ML) aims to allow ML models to adapt to ever-changing environments with potential concept drift after model deployment. Traditionally, adaptive ML requires a new dataset to be manually labeled to tailor deployed models to altered data distributions. Recently, an interactive causality based self-labeling method was proposed to autonomously associate causally related data streams for domain adaptation, showing promising results compared to traditional feature similarity-based semi-supervised learning. Several unanswered research questions remain, including self-labeling's compatibility with multivariate causality and the quantitative analysis of the auxiliary models used in the self-labeling. The auxiliary models, the interaction time model (ITM) and the effect state detector (ESD), are vital to the success of self-labeling. This paper further develops the self-labeling framework and its theoretical foundations to address these research questions. A framework for the application of self-labeling to multivariate causal graphs is proposed using four basic causal relationships, and the impact of non-ideal ITM and ESD performance is analyzed. A simulated experiment is conducted based on a multivariate causal graph, validating the proposed theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05809v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yutian Ren, Aaron Haohua Yen, G. P. Li</dc:creator>
    </item>
    <item>
      <title>A feature-based information-theoretic approach for detecting interpretable, long-timescale pairwise interactions from time series</title>
      <link>https://arxiv.org/abs/2404.05929</link>
      <description>arXiv:2404.05929v1 Announce Type: cross 
Abstract: Quantifying relationships between components of a complex system is critical to understanding the rich network of interactions that characterize the behavior of the system. Traditional methods for detecting pairwise dependence of time series, such as Pearson correlation, Granger causality, and mutual information, are computed directly in the space of measured time-series values. But for systems in which interactions are mediated by statistical properties of the time series (`time-series features') over longer timescales, this approach can fail to capture the underlying dependence from limited and noisy time-series data, and can be challenging to interpret. Addressing these issues, here we introduce an information-theoretic method for detecting dependence between time series mediated by time-series features that provides interpretable insights into the nature of the interactions. Our method extracts a candidate set of time-series features from sliding windows of the source time series and assesses their role in mediating a relationship to values of the target process. Across simulations of three different generative processes, we demonstrate that our feature-based approach can outperform a traditional inference approach based on raw time-series values, especially in challenging scenarios characterized by short time-series lengths, high noise levels, and long interaction timescales. Our work introduces a new tool for inferring and interpreting feature-mediated interactions from time-series data, contributing to the broader landscape of quantitative analysis in complex systems research, with potential applications in various domains including but not limited to neuroscience, finance, climate science, and engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05929v1</guid>
      <category>physics.data-an</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aria Nguyen, Oscar McMullin, Joseph T. Lizier, Ben D. Fulcher</dc:creator>
    </item>
    <item>
      <title>A Cyber Manufacturing IoT System for Adaptive Machine Learning Model Deployment by Interactive Causality Enabled Self-Labeling</title>
      <link>https://arxiv.org/abs/2404.05976</link>
      <description>arXiv:2404.05976v1 Announce Type: cross 
Abstract: Machine Learning (ML) has been demonstrated to improve productivity in many manufacturing applications. To host these ML applications, several software and Industrial Internet of Things (IIoT) systems have been proposed for manufacturing applications to deploy ML applications and provide real-time intelligence. Recently, an interactive causality enabled self-labeling method has been proposed to advance adaptive ML applications in cyber-physical systems, especially manufacturing, by automatically adapting and personalizing ML models after deployment to counter data distribution shifts. The unique features of the self-labeling method require a novel software system to support dynamism at various levels.
  This paper proposes the AdaptIoT system, comprised of an end-to-end data streaming pipeline, ML service integration, and an automated self-labeling service. The self-labeling service consists of causal knowledge bases and automated full-cycle self-labeling workflows to adapt multiple ML models simultaneously. AdaptIoT employs a containerized microservice architecture to deliver a scalable and portable solution for small and medium-sized manufacturers. A field demonstration of a self-labeling adaptive ML application is conducted with a makerspace and shows reliable performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05976v1</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yutian Ren, Yuqi He, Xuyin Zhang, Aaron Yen, G. P. Li</dc:creator>
    </item>
    <item>
      <title>Least Squares-Based Permutation Tests in Time Series</title>
      <link>https://arxiv.org/abs/2404.06238</link>
      <description>arXiv:2404.06238v1 Announce Type: cross 
Abstract: This paper studies permutation tests for regression parameters in a time series setting, where the time series is assumed stationary but may exhibit an arbitrary (but weak) dependence structure. In such a setting, it is perhaps surprising that permutation tests can offer any type of inference guarantees, since permuting of covariates can destroy its relationship with the response. Indeed, the fundamental assumption of exchangeability of errors required for the finite-sample exactness of permutation tests, can easily fail. However, we show that permutation tests may be constructed which are asymptotically valid for a wide class of stationary processes, but remain exact when exchangeability holds. We also consider the problem of testing for no monotone trend and we construct asymptotically valid permutation tests in this setting as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06238v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph P. Romano, Marius A. Tirlea</dc:creator>
    </item>
    <item>
      <title>Permutation Testing for Monotone Trend</title>
      <link>https://arxiv.org/abs/2404.06239</link>
      <description>arXiv:2404.06239v1 Announce Type: cross 
Abstract: In this paper, we consider the fundamental problem of testing for monotone trend in a time series. While the term "trend" is commonly used and has an intuitive meaning, it is first crucial to specify its exact meaning in a hypothesis testing context. A commonly used well-known test is the Mann-Kendall test, which we show does not offer Type 1 error control even in large samples. On the other hand, by an appropriate studentization of the Mann-Kendall statistic, we construct permutation tests that offer asymptotic error control quite generally, but retain the exactness property of permutation tests for i.i.d. observations. We also introduce "local" Mann-Kendall statistics as a means of testing for local rather than global trend in a time series. Similar properties of permutation tests are obtained for these tests as well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06239v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph P. Romano, Marius A. Tirlea</dc:creator>
    </item>
    <item>
      <title>M-Estimation based on quasi-processes from discrete samples of Levy processes</title>
      <link>https://arxiv.org/abs/2112.08199</link>
      <description>arXiv:2112.08199v3 Announce Type: replace 
Abstract: We consider M-estimation problems, where the target value is determined using a minimizer of an expected functional of a Levy process. With discrete observations from the Levy process, we can produce a "quasi-path" by shuffling increments of the Levy process, we call it a quasi-process. Under a suitable sampling scheme, a quasi-process can converge weakly to the true process according to the properties of the stationary and independent increments. Using this resampling technique, we can estimate objective functionals similar to those estimated using the Monte Carlo simulations, and it is available as a contrast function. The M-estimator based on these quasi-processes can be consistent and asymptotically normal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.08199v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yasutaka Shimizu, Hiroshi Shiraishi</dc:creator>
    </item>
    <item>
      <title>Adaptive functional principal components analysis</title>
      <link>https://arxiv.org/abs/2306.16091</link>
      <description>arXiv:2306.16091v2 Announce Type: replace 
Abstract: Functional data analysis almost always involves smoothing discrete observations into curves, because they are never observed in continuous time and rarely without error. Although smoothing parameters affect the subsequent inference, data-driven methods for selecting these parameters are not well-developed, frustrated by the difficulty of using all the information shared by curves while being computationally efficient. On the one hand, smoothing individual curves in an isolated, albeit sophisticated way, ignores useful signals present in other curves. On the other hand, bandwidth selection by automatic procedures such as cross-validation after pooling all the curves together quickly become computationally unfeasible due to the large number of data points. In this paper we propose a new data-driven, adaptive kernel smoothing, specifically tailored for functional principal components analysis through the derivation of sharp, explicit risk bounds for the eigen-elements. The minimization of these quadratic risk bounds provide refined, yet computationally efficient bandwidth rules for each eigen-element separately. Both common and independent design cases are allowed. Rates of convergence for the estimators are derived. An extensive simulation study, designed in a versatile manner to closely mimic the characteristics of real data sets supports our methodological contribution. An illustration on a real data application is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16091v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sunny G. W. Wang, Valentin Patilea, Nicolas Klutchnikoff</dc:creator>
    </item>
    <item>
      <title>A Spatial Autoregressive Graphical Model with Applications in Intercropping</title>
      <link>https://arxiv.org/abs/2308.04325</link>
      <description>arXiv:2308.04325v3 Announce Type: replace 
Abstract: Within the statistical literature, a significant gap exists in methods capable of modeling asymmetric multivariate spatial effects that elucidate the relationships underlying complex spatial phenomena. For such a phenomenon, observations at any location are expected to arise from a combination of within- and between- location effects, where the latter exhibit asymmetry. This asymmetry is represented by heterogeneous spatial effects between locations pertaining to different categories, that is, a feature inherent to each location in the data, such that based on the feature label, asymmetric spatial relations are postulated between neighbouring locations with different labels. Our novel approach synergises the principles of multivariate spatial autoregressive models and the Gaussian graphical model. This synergy enables us to effectively address the gap by accommodating asymmetric spatial relations, overcoming the usual constraints in spatial analyses. Using a Bayesian-estimation framework, the model performance is assessed in a simulation study. We apply the model on intercropping data, where spatial effects between different crops are unlikely to be symmetric, in order to illustrate the usage of the proposed methodology. An R package containing the proposed methodology can be found on https://CRAN.R-project.org/package=SAGM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.04325v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sjoerd Hermes, Joost van Heerwaarden, Pariya Behrouzi</dc:creator>
    </item>
    <item>
      <title>Optimally weighted average derivative effects</title>
      <link>https://arxiv.org/abs/2308.05456</link>
      <description>arXiv:2308.05456v2 Announce Type: replace 
Abstract: Weighted average derivative effects (WADEs) are nonparametric estimands with uses in economics and causal inference. Debiased WADE estimators typically require learning the conditional mean outcome as well as a Riesz representer (RR) that characterises the requisite debiasing corrections. RR estimators for WADEs often rely on kernel estimators, introducing complicated bandwidth-dependant biases. In our work we propose a new class of RRs that are isomorphic to the class of WADEs and we derive the WADE weight that is optimal, in the sense of having minimum nonparametric efficiency bound. Our optimal WADE estimators require estimating conditional expectations only (e.g. using machine learning), thus overcoming the limitations of kernel estimators. Moreover, we connect our optimal WADE to projection parameters in partially linear models. We ascribe a causal interpretation to WADE and projection parameters in terms of so-called incremental effects. We propose efficient estimators for two WADE estimands in our class, which we evaluate in a numerical experiment and use to determine the effect of Warfarin dose on blood clotting function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05456v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oliver Hines, Karla Diaz-Ordaz, Stijn Vansteelandt</dc:creator>
    </item>
    <item>
      <title>Extremal graphical modeling with latent variables</title>
      <link>https://arxiv.org/abs/2403.09604</link>
      <description>arXiv:2403.09604v2 Announce Type: replace 
Abstract: Extremal graphical models encode the conditional independence structure of multivariate extremes and provide a powerful tool for quantifying the risk of rare events. Prior work on learning these graphs from data has focused on the setting where all relevant variables are observed. For the popular class of H\"usler-Reiss models, we propose the \texttt{eglatent} method, a tractable convex program for learning extremal graphical models in the presence of latent variables. Our approach decomposes the H\"usler-Reiss precision matrix into a sparse component encoding the graphical structure among the observed variables after conditioning on the latent variables, and a low-rank component encoding the effect of a few latent variables on the observed variables. We provide finite-sample guarantees of \texttt{eglatent} and show that it consistently recovers the conditional graph as well as the number of latent variables. We highlight the improved performances of our approach on synthetic and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09604v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Engelke, Armeen Taeb</dc:creator>
    </item>
    <item>
      <title>The Role of Mean Absolute Deviation Function in Obtaining Smooth Estimation for Distribution and Density Functions: Beta Regression Approach</title>
      <link>https://arxiv.org/abs/2403.16544</link>
      <description>arXiv:2403.16544v2 Announce Type: replace 
Abstract: Smooth Estimation of probability density and distribution functions from its sample is an attractive and an important problem that has applications in several fields such as, business, medicine, and environment. This article introduces a simple approach but novel for estimating both functions to have smooth curves for both via left mean absolute deviation (MAD) function and beta regression approach. Our approach explores estimation of both functions by smoothing the first derivative of left MAD function to obtain the final optimal smooth estimates. The final smooth estimates are obtained under the assumption that nondecreasing distribution function and the density function remains nonnegative. This is achieved by using beta regression with various link functions (logit, probit, cloglog, and cauchit) applied to a polynomial whose degree is determined by the first derivative of the left MAD function. The chosen polynomial degree minimizes the mean absolute regression errors, provided that the first derivative of the regression vector of expected values is nonnegative. Additionally, pointwise confidence limits for the distribution function are derived using the beta distribution. The method is utilized on simulated datasets featuring unimodal, bimodal, and tri-modal, with a demonstration on an actual dataset. The results suggest that this method exhibits strong performance relative to the kerne-based method, especially notable for its superior attributes in sample sizes and smoothness</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16544v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elsayed A. H. Elamir</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes When Estimation Precision Predicts Parameters</title>
      <link>https://arxiv.org/abs/2212.14444</link>
      <description>arXiv:2212.14444v4 Announce Type: replace-cross 
Abstract: Empirical Bayes methods usually maintain a prior independence assumption: The unknown parameters of interest are independent from the known standard errors of the estimates. This assumption is often theoretically questionable and empirically rejected. This paper instead models the conditional distribution of the parameter given the standard errors as a flexibly parametrized family of distributions, leading to a family of methods that we call CLOSE. This paper establishes that (i) CLOSE is rate-optimal for squared error Bayes regret, (ii) squared error regret control is sufficient for an important class of economic decision problems, and (iii) CLOSE is worst-case robust when our assumption on the conditional distribution is misspecified. Empirically, using CLOSE leads to sizable gains for selecting high-mobility Census tracts. Census tracts selected by CLOSE are substantially more mobile on average than those selected by the standard shrinkage method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.14444v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen</dc:creator>
    </item>
    <item>
      <title>Federated Transfer Learning with Differential Privacy</title>
      <link>https://arxiv.org/abs/2403.11343</link>
      <description>arXiv:2403.11343v2 Announce Type: replace-cross 
Abstract: Federated learning is gaining increasing popularity, with data heterogeneity and privacy being two prominent challenges. In this paper, we address both issues within a federated transfer learning framework, aiming to enhance learning on a target data set by leveraging information from multiple heterogeneous source data sets while adhering to privacy constraints. We rigorously formulate the notion of \textit{federated differential privacy}, which offers privacy guarantees for each data set without assuming a trusted central server. Under this privacy constraint, we study three classical statistical problems, namely univariate mean estimation, low-dimensional linear regression, and high-dimensional linear regression. By investigating the minimax rates and identifying the costs of privacy for these problems, we show that federated differential privacy is an intermediate privacy model between the well-established local and central models of differential privacy. Our analyses incorporate data heterogeneity and privacy, highlighting the fundamental costs of both in federated learning and underscoring the benefit of knowledge transfer across data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11343v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengchu Li, Ye Tian, Yang Feng, Yi Yu</dc:creator>
    </item>
  </channel>
</rss>
