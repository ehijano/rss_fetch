<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 22 Dec 2025 05:01:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A systematic assessment of Large Language Models for constructing two-level fractional factorial designs</title>
      <link>https://arxiv.org/abs/2512.17113</link>
      <description>arXiv:2512.17113v1 Announce Type: new 
Abstract: Two-level fractional factorial designs permit the study multiple factors using a limited number of runs. Traditionally, these designs are obtained from catalogs available in standard textbooks or statistical software. However, modern Large Language Models (LLMs) can now produce two-level fractional factorial designs, but the quality of these designs has not been previously assessed. In this paper, we perform a systematic evaluation of two popular classes of LLMs, namely GPT and Gemini models, to construct two-level fractional factorial designs with 8, 16, and 32 runs, and 4 to 26 factors. To this end, we use prompting techniques to develop a high-quality set of design construction tasks for the LLMs. We compare the designs obtained by the LLMs with the best-known designs in terms of resolution and minimum aberration criteria. We show that the LLMs can effectively construct optimal 8-, 16-, and 32-run designs with up to eight factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17113v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alan R. Vazquez, Kilian M. Rother, Marco V. Charles-Gonzalez</dc:creator>
    </item>
    <item>
      <title>A Synthetic Instrumental Variable Method: Using the Dual Tendency Condition for Coplanar Instruments</title>
      <link>https://arxiv.org/abs/2512.17301</link>
      <description>arXiv:2512.17301v1 Announce Type: new 
Abstract: Traditional instrumental variable (IV) methods often struggle with weak or invalid instruments and rely heavily on external data. We introduce a Synthetic Instrumental Variable (SIV) approach that constructs valid instruments using only existing data. Our method leverages a data-driven dual tendency (DT) condition to identify valid instruments without requiring external variables. SIV is robust to heteroscedasticity and can determine the true sign of the correlation between endogenous regressors and errors--an assumption typically imposed in empirical work. Through simulations and real-world applications, we show that SIV improves causal inference by mitigating common IV limitations and reducing dependence on scarce instruments. This approach has broad implications for economics, epidemiology, and policy evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17301v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ratbek Dzhumashev, Ainura Tursunalieva</dc:creator>
    </item>
    <item>
      <title>Penalized Fair Regression for Multiple Groups in Chronic Kidney Disease</title>
      <link>https://arxiv.org/abs/2512.17340</link>
      <description>arXiv:2512.17340v1 Announce Type: new 
Abstract: Fair regression methods have the potential to mitigate societal bias concerns in health care, but there has been little work on penalized fair regression when multiple groups experience such bias. We propose a general regression framework that addresses this gap with unfairness penalties for multiple groups. Our approach is demonstrated for binary outcomes with true positive rate disparity penalties. It can be efficiently implemented through reduction to a cost-sensitive classification problem. We additionally introduce novel score functions for automatically selecting penalty weights. Our penalized fair regression methods are empirically studied in simulations, where they achieve a fairness-accuracy frontier beyond that of existing comparison methods. Finally, we apply these methods to a national multi-site primary care study of chronic kidney disease to develop a fair classifier for end-stage renal disease. There we find substantial improvements in fairness for multiple race and ethnicity groups who experience societal bias in the health care system without any appreciable loss in overall fit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17340v1</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carter H. Nakamoto, Lucia Lushi Chen, Agata Foryciarz, Sherri Rose</dc:creator>
    </item>
    <item>
      <title>A General Stability Approach to False Discovery Rate Control</title>
      <link>https://arxiv.org/abs/2512.17401</link>
      <description>arXiv:2512.17401v1 Announce Type: new 
Abstract: Stability and reproducibility are essential considerations in various applications of statistical methods. False Discovery Rate (FDR) control methods are able to control false signals in scientific discoveries. However, many FDR control methods, such as Model-X knockoff and data-splitting approaches, yield unstable results due to the inherent randomness of the algorithms. To enhance the stability and reproducibility of statistical outcomes, we propose a general stability approach for FDR control in feature selection and multiple testing problems, named FDR Stabilizer. Taking feature selection as an example, our method first aggregates feature importance statistics obtained by multiple runs of the base FDR control procedure into a consensus ranking. Then, we construct a stabilized relaxed e-value for each feature and apply the e-BH procedure to these stabilized e-values to obtain the final selection set. We theoretically derive the finite-sample bounds for the FDR and the power of our method, and show that our method asymptotically controls the FDR without power loss. Moreover, we establish the stability of the proposed method, showing that the stabilized selection set converges to a deterministic limit as the number of repetitions increases. Extensive numerical experiments and applications to real datasets demonstrate that the proposed method generally outperforms existing alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17401v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiajun Sun, Zhanrui Cai, Wei Zhong</dc:creator>
    </item>
    <item>
      <title>Bayesian Markov-Switching Partial Reduced-Rank Regression</title>
      <link>https://arxiv.org/abs/2512.17471</link>
      <description>arXiv:2512.17471v1 Announce Type: new 
Abstract: Reduced-Rank (RR) regression is a powerful dimensionality reduction technique but it overlooks any possible group configuration among the responses by assuming a low-rank structure on the entire coefficient matrix. Moreover, the temporal change of the relations between predictors and responses in time series induce a possibly time-varying grouping structure in the responses. To address these limitations, a Bayesian Markov-switching partial RR (MS-PRR) model is proposed, where the response vector is partitioned in two groups to reflect different complexity of the relationship. A \textit{simple} group assumes a low-rank linear regression, while a \textit{complex} group exploits nonparametric regression via a Gaussian Process. Differently from traditional approaches, group assignments and rank are treated as unknown parameters to be estimated. Then temporal persistence in the regression function is accounted for by a Markov-switching process that drives the changes in the grouping structure and model parameters over time. Full Bayesian inference is preformed via a partially collapsed Gibbs sampler, which allows uncertainty quantification without the need for trans-dimensional moves. Applications to two real-world macroeconomic and commodity data demonstrate the evidence of time-varying grouping and different degrees of complexity both across states and within each state.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17471v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria F. Pintado, Matteo Iacopini, Luca Rossini, Alexander Y. Shestopaloff</dc:creator>
    </item>
    <item>
      <title>Inference on state occupancy in covariate-driven hidden Markov models</title>
      <link>https://arxiv.org/abs/2512.17496</link>
      <description>arXiv:2512.17496v1 Announce Type: new 
Abstract: Hidden Markov models (HMMs) are popular tools for analysing animal behaviour based on movement, acceleration and other sensor data. In particular, these models allow to infer how the animal's decision-making process interacts with internal and external drivers, by relating the probabilities of switching between distinct behavioural states to covariates. A key challenge arising in the statistical analysis of behavioural data using covariate-driven HMMs is the models' interpretation, especially when there are more than two states, as then several functional relationships between state-switching probabilities and covariates need to be jointly interpreted. The model-implied probabilities of occupying the different states, as a function of a covariate of interest, constitute a much simpler summary statistic. A pragmatic approximation of the state occupancy distribution, namely the hypothetical stationary distribution of the model's underlying Markov chain for fixed covariate values, has in fact routinely been reported in HMM-based analyses of ecological data. However, for stochastically varying covariates with relatively little persistence, we show that this approximation can be severely biased, potentially invalidating ecological inference. We develop two alternative approaches for obtaining the state occupancy distribution as a function of a covariate of interest - one based on resampling of the covariate process, the other obtained by regression analysis of the empirical state probabilities. The practical application of these approaches is demonstrated in simulations and a case study on Gal\'apagos tortoise (Chelonoidis niger) movement data. Our methods enable practitioners to conduct unbiased inference on the relationship between animal behaviour and general types of covariates, thus allowing to uncover the factors influencing behavioural decisions made by animals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17496v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maya N. Vienken, Jan-Ole Koslik, Roland Langrock</dc:creator>
    </item>
    <item>
      <title>Estimation and model errors in Gaussian-process-based Sensitivity Analysis of functional outputs</title>
      <link>https://arxiv.org/abs/2512.17635</link>
      <description>arXiv:2512.17635v1 Announce Type: new 
Abstract: Global sensitivity analysis (GSA) of functional-output models is usually performed by combining statistical techniques, such as basis expansions, metamodeling and sampling based estimation of sensitivity indices. By neglecting truncation error from basis expansion, two main sources of errors propagate to the final sensitivity indices: the metamodeling related error and the sampling-based, or pick-freeze (PF), estimation error. This work provides an efficient algorithm to estimate these errors in the frame of Gaussian processes (GP), based on the approach of Le Gratiet et al. [16]. The proposed algorithm takes advantage of the fact that the number of basis coefficients of expanded model outputs is significantly smaller than output dimensions. Basis coefficients are fitted by GP models and multiple conditional GP trajectories are sampled. Then, vector-valued PF estimation is used to speed-up the estimation of Sobol indices and generalized sensitivity indices (GSI). We illustrate the methodology on an analytical test case and on an application in non-Newtonian hydraulics, modelling an idealized dam-break flow. Numerical tests show an improvement of 15 times in the computational time when compared to the application of Le Gratiet et al. [16] algorithm separately over each output dimension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17635v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuri Taglieri S\'ao, Olivier Roustant, Geraldo de Freitas Maciel</dc:creator>
    </item>
    <item>
      <title>A Dependent Feature Allocation Model Based on Random Fields</title>
      <link>https://arxiv.org/abs/2512.17701</link>
      <description>arXiv:2512.17701v1 Announce Type: new 
Abstract: We introduce a flexible framework for modeling dependent feature allocations. Our approach addresses limitations in traditional nonparametric methods by directly modeling the logit-probability surface of the feature paintbox, enabling the explicit incorporation of covariates and complex but tractable dependence structures. The core of our model is a Gaussian Markov Random Field (GMRF), which we use to robustly decompose the latent field, separating a structural component based on the baseline covariates from intrinsic, unstructured heterogeneity. This structure is not a rigid grid but a sparse k-nearest neighbors graph derived from the latent geometry in the data, ensuring high-dimensional tractability. We extend this framework to a dynamic spatio-temporal process, allowing item effects to evolve via an Ornstein-Uhlenbeck process. Feature correlations are captured using a low-rank factorization of their joint prior. We demonstrate our model's utility by applying it to a polypharmacy dataset, successfully inferring latent health conditions from patient drug profiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17701v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernardo Flores, Yang Ni, Yanxun Xu, Peter M\"uller</dc:creator>
    </item>
    <item>
      <title>Recursive state estimation via approximate modal paths</title>
      <link>https://arxiv.org/abs/2512.17737</link>
      <description>arXiv:2512.17737v1 Announce Type: new 
Abstract: In this paper, a method for recursively computing approximate modal paths is developed. A recursive formulation of the modal path can be obtained either by backward or forward dynamic programming. By combining both methods, a ``two-filter'' formula is demonstrated. Both method involves a recursion over a so-called value function, which is intractable in general. This problem is overcome by quadratic approximation of the value function in the forward dynamic programming paradigm, resulting in both a filtering and smoothing method. The merit of the approach is verified in a simulation experiments, where it is shown to be on par or better than other modern algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17737v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filip Tronarp</dc:creator>
    </item>
    <item>
      <title>Principled Identification of Structural Dynamic Models</title>
      <link>https://arxiv.org/abs/2512.17005</link>
      <description>arXiv:2512.17005v1 Announce Type: cross 
Abstract: We take a new perspective on identification in structural dynamic models: rather than imposing restrictions, we optimize an objective. This provides new theoretical insights into traditional Cholesky identification. A correlation-maximizing objective yields an Order- and Scale-Invariant Identification Scheme (OASIS) that selects the orthogonal rotation that best aligns structural shocks with their reduced-form innovations. We revisit a large number of SVAR studies and find, across 22 published SVARs, that the correlations between structural and reduced-form shocks are generally high.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17005v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neville Francis, Peter Reinhard Hansen, Chen Tong</dc:creator>
    </item>
    <item>
      <title>Dirichlet Meets Horvitz and Thompson: Estimating Homophily in Large Networks via Sampling</title>
      <link>https://arxiv.org/abs/2512.17084</link>
      <description>arXiv:2512.17084v1 Announce Type: cross 
Abstract: Assessing homophily in large-scale networks is central to understanding structural regularities in graphs, and thus inform the choice of models (such as graph neural networks) adopted to learn from network data. Evaluation of smoothness metrics requires access to the entire network topology and node features, which may be impractical in several large-scale, dynamic, resource-limited, or privacy-constrained settings. In this work, we propose a sampling-based framework to estimate homophily via the Dirichlet energy (Laplacian-based total variation) of graph signals, leveraging the Horvitz-Thompson (HT) estimator for unbiased inference from partial graph observations. The Dirichlet energy is a so-termed total (of squared nodal feature deviations) over graph edges; hence, estimable under general network sampling designs for which edge-inclusion probabilities can be analytically derived and used as weights in the proposed HT estimator. We establish that the Dirichlet energy can be consistently estimated from sampled graphs, and empirically study other heterophily measures as well. Experiments on several heterophilic benchmark datasets demonstrate the effectiveness of the proposed HT estimators in reliably capturing homophilic structure (or lack thereof) from sampled network measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17084v1</guid>
      <category>eess.SP</category>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hamed Ajorlou, Gonzalo Mateos, Luana Ruiz</dc:creator>
    </item>
    <item>
      <title>Uncovering latent territorial structure in ICFES Saber 11 performance with Bayesian multilevel spatial models</title>
      <link>https://arxiv.org/abs/2512.17119</link>
      <description>arXiv:2512.17119v1 Announce Type: cross 
Abstract: This article develops a Bayesian hierarchical framework to analyze academic performance in the 2022 second semester Saber 11 examination in Colombia. Our approach combines multilevel regression with municipal and departmental spatial random effects, and it incorporates Ridge and Lasso regularization priors to compare the contribution of sociodemographic covariates. Inference is implemented in a fully open source workflow using Markov chain Monte Carlo methods, and model behavior is assessed through synthetic data that mirror key features of the observed data. Simulation results indicate that Ridge provides the most balanced performance in parameter recovery, predictive accuracy, and sampling efficiency, while Lasso shows weaker fit and posterior stability, with gains in predictive accuracy under stronger multicollinearity. In the application, posterior rankings show a strong centralization of performance, with higher scores in central departments and lower scores in peripheral territories, and the strongest correlates of scores are student level living conditions, maternal education, access to educational resources, gender, and ethnic background, while spatial random effects capture residual regional disparities. A hybrid Bayesian segmentation based on K means propagates posterior uncertainty into clustering at departmental, municipal, and spatial scales, revealing multiscale territorial patterns consistent with structural inequalities and informing territorial targeting in education policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17119v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Pardo, Juan Sosa</dc:creator>
    </item>
    <item>
      <title>Sharp Structure-Agnostic Lower Bounds for General Functional Estimation</title>
      <link>https://arxiv.org/abs/2512.17341</link>
      <description>arXiv:2512.17341v1 Announce Type: cross 
Abstract: The design of efficient nonparametric estimators has long been a central problem in statistics, machine learning, and decision making. Classical optimal procedures often rely on strong structural assumptions, which can be misspecified in practice and complicate deployment. This limitation has sparked growing interest in structure-agnostic approaches -- methods that debias black-box nuisance estimates without imposing structural priors. Understanding the fundamental limits of these methods is therefore crucial. This paper provides a systematic investigation of the optimal error rates achievable by structure-agnostic estimators. We first show that, for estimating the average treatment effect (ATE), a central parameter in causal inference, doubly robust learning attains optimal structure-agnostic error rates. We then extend our analysis to a general class of functionals that depend on unknown nuisance functions and establish the structure-agnostic optimality of debiased/double machine learning (DML). We distinguish two regimes -- one where double robustness is attainable and one where it is not -- leading to different optimal rates for first-order debiasing, and show that DML is optimal in both regimes. Finally, we instantiate our general lower bounds by deriving explicit optimal rates that recover existing results and extend to additional estimands of interest. Our results provide theoretical validation for widely used first-order debiasing methods and guidance for practitioners seeking optimal approaches in the absence of structural assumptions. This paper generalizes and subsumes the ATE lower bound established in \citet{jin2024structure} by the same authors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17341v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jikai Jin, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>meval: A Statistical Toolbox for Fine-Grained Model Performance Analysis</title>
      <link>https://arxiv.org/abs/2512.17409</link>
      <description>arXiv:2512.17409v1 Announce Type: cross 
Abstract: Analyzing machine learning model performance stratified by patient and recording properties is becoming the accepted norm and often yields crucial insights about important model failure modes. Performing such analyses in a statistically rigorous manner is non-trivial, however. Appropriate performance metrics must be selected that allow for valid comparisons between groups of different sample sizes and base rates; metric uncertainty must be determined and multiple comparisons be corrected for, in order to assess whether any observed differences may be purely due to chance; and in the case of intersectional analyses, mechanisms must be implemented to find the most `interesting' subgroups within combinatorially many subgroup combinations. We here present a statistical toolbox that addresses these challenges and enables practitioners to easily yet rigorously assess their models for potential subgroup performance disparities. While broadly applicable, the toolbox is specifically designed for medical imaging applications. The analyses provided by the toolbox are illustrated in two case studies, one in skin lesion malignancy classification on the ISIC2020 dataset and one in chest X-ray-based disease classification on the MIMIC-CXR dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17409v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-05870-6_19</arxiv:DOI>
      <arxiv:journal_reference>MICCAI FAIMI Workshop 2025</arxiv:journal_reference>
      <dc:creator>Dishantkumar Sutariya, Eike Petersen</dc:creator>
    </item>
    <item>
      <title>Inference for high dimensional repeated measure designs with the R package hdrm</title>
      <link>https://arxiv.org/abs/2512.17478</link>
      <description>arXiv:2512.17478v1 Announce Type: cross 
Abstract: Repeated-measure designs allow comparisons within a group as well as between groups, and are commonly referred to as split-plot designs. While originating in agricultural experiments, they are now widely used in medical research, psychology, and the life sciences, where repeated observations on the same subject are essential.
  Modern data collection often produces observation vectors with dimension $d$ comparable to or exceeding the sample size $N$. Although this can be advantageous in terms of cost efficiency, ethical considerations, and the study of rare diseases, it poses substantial challenges for statistical inference.
  Parametric methods based on multivariate normality provide a flexible framework that avoids restrictive assumptions on covariance structures or on the asymptotic relationship between $d$ and $N$. Within this framework, the freely available R-package hdrm enables the analysis of a wide range of hypotheses concerning expectation vectors in high-dimensional repeated-measure designs, covering both single-group and multi-group settings with homogeneous or heterogeneous covariance matrices.
  This paper describes the implemented tests, demonstrates their use through examples, and discusses their applicability in practical high-dimensional data scenarios. To address computational challenges arising for large $d$, the package incorporates efficient estimators and subsampling strategies that substantially reduce computation time while preserving statistical validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17478v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paavo Sattler, Nils Hichert</dc:creator>
    </item>
    <item>
      <title>Imputation Uncertainty in Interpretable Machine Learning Methods</title>
      <link>https://arxiv.org/abs/2512.17689</link>
      <description>arXiv:2512.17689v1 Announce Type: cross 
Abstract: In real data, missing values occur frequently, which affects the interpretation with interpretable machine learning (IML) methods. Recent work considers bias and shows that model explanations may differ between imputation methods, while ignoring additional imputation uncertainty and its influence on variance and confidence intervals. We therefore compare the effects of different imputation methods on the confidence interval coverage probabilities of the IML methods permutation feature importance, partial dependence plots and Shapley values. We show that single imputation leads to underestimation of variance and that, in most cases, only multiple imputation is close to nominal coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17689v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pegah Golchian, Marvin N. Wright</dc:creator>
    </item>
    <item>
      <title>Spatially-informed transformers: Injecting geostatistical covariance biases into self-attention for spatio-temporal forecasting</title>
      <link>https://arxiv.org/abs/2512.17696</link>
      <description>arXiv:2512.17696v1 Announce Type: cross 
Abstract: The modeling of high-dimensional spatio-temporal processes presents a fundamental dichotomy between the probabilistic rigor of classical geostatistics and the flexible, high-capacity representations of deep learning. While Gaussian processes offer theoretical consistency and exact uncertainty quantification, their prohibitive computational scaling renders them impractical for massive sensor networks. Conversely, modern transformer architectures excel at sequence modeling but inherently lack a geometric inductive bias, treating spatial sensors as permutation-invariant tokens without a native understanding of distance. In this work, we propose a spatially-informed transformer, a hybrid architecture that injects a geostatistical inductive bias directly into the self-attention mechanism via a learnable covariance kernel. By formally decomposing the attention structure into a stationary physical prior and a non-stationary data-driven residual, we impose a soft topological constraint that favors spatially proximal interactions while retaining the capacity to model complex dynamics. We demonstrate the phenomenon of ``Deep Variography'', where the network successfully recovers the true spatial decay parameters of the underlying process end-to-end via backpropagation. Extensive experiments on synthetic Gaussian random fields and real-world traffic benchmarks confirm that our method outperforms state-of-the-art graph neural networks. Furthermore, rigorous statistical validation confirms that the proposed method delivers not only superior predictive accuracy but also well-calibrated probabilistic forecasts, effectively bridging the gap between physics-aware modeling and data-driven learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17696v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuri Calleo</dc:creator>
    </item>
    <item>
      <title>Sparse Anomaly Detection Across Referentials: A Rank-Based Higher Criticism Approach</title>
      <link>https://arxiv.org/abs/2312.04924</link>
      <description>arXiv:2312.04924v2 Announce Type: replace 
Abstract: Detecting anomalies in large sets of observations is crucial in various applications, such as epidemiological studies, gene expression studies, and systems monitoring. We consider settings where the units of interest result in multiple independent observations from potentially distinct referentials. Scan statistics and related methods are commonly used in such settings, but rely on stringent modeling assumptions for proper calibration. We instead propose a rank-based variant of the higher criticism statistic that only requires independent observations originating from ordered spaces. We show under what conditions the resulting methodology is able to detect the presence of anomalies. These conditions are stated in a general, non-parametric manner, and depend solely on the probabilities of anomalous observations exceeding nominal observations. The analysis requires a refined understanding of the distribution of the ranks under the presence of anomalies, and in particular of the rank-induced dependencies. The methodology is robust against heavy-tailed distributions through the use of ranks. Within the exponential family and a family of convolutional models, we analytically quantify the asymptotic performance of our methodology and the performance of the oracle, and show the difference is small for many common models. Simulations confirm these results. We show the applicability of the methodology through an analysis of quality control data of a pharmaceutical manufacturing process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04924v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/24-AOS2477</arxiv:DOI>
      <arxiv:journal_reference>Annals of Statistics 2025, Vol. 53, No. 2, 676--702</arxiv:journal_reference>
      <dc:creator>Ivo V. Stoepker, Rui M. Castro, Ery Arias-Castro</dc:creator>
    </item>
    <item>
      <title>Efficient Sampling in Disease Surveillance through Subpopulations: Sampling Canaries in the Coal Mine</title>
      <link>https://arxiv.org/abs/2405.10742</link>
      <description>arXiv:2405.10742v2 Announce Type: replace 
Abstract: We consider outbreak detection settings of endemic diseases where the population under study consists of various subpopulations available for stratified surveillance. These subpopulations can for example be based on age cohorts, but may also correspond to other subgroups of the population under study such as international travellers. Rather than sampling uniformly across the population, one may elevate the effectiveness of the detection methodology by optimally choosing a sampling subpopulation. We show (under some assumptions) the relative sampling efficiency between two subpopulations is inversely proportional to the ratio of their respective baseline disease risks. This implies one can increase sampling efficiency by sampling from the subpopulation with higher baseline disease risk. Our results require careful treatment of the power curves of exact binomial tests as a function of their sample size, which are non-monotonic due to the underlying discreteness. A case study of COVID-19 cases in the Netherlands illustrates our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10742v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spl.2025.110384</arxiv:DOI>
      <arxiv:journal_reference>Statistics and Probablity Letters 2025, Vol. 222, 110384</arxiv:journal_reference>
      <dc:creator>Ivo V. Stoepker</dc:creator>
    </item>
    <item>
      <title>Operational Dosage: Implications of Capacity Constraints for the Design and Interpretation of Experiments</title>
      <link>https://arxiv.org/abs/2407.21322</link>
      <description>arXiv:2407.21322v3 Announce Type: replace 
Abstract: We study RCTs that evaluate the impact of service interventions, for example, teachers or advisors conducting proactive outreach to at-risk students, medical providers giving medication adherence support by calling or texting, or social workers that conduct home visits. A defining feature of service interventions is that they are delivered by a capacity-constrained resource -- teachers, healthcare providers, or social workers -- whose limited availability creates causal inference complications. Because participants share a finite service capacity, adding more participants can reduce the timeliness or intensity of the service that others receive, introducing interference across participants. This generates hidden variation in the treatment itself, which we term operational dosage. We provide a mathematical model of service interventions using techniques from queueing theory and study the impact of capacity constraints on experimental outcomes. Our main insight is that treatment effects are both capacity- and sample-size-dependent, as well as decreasing in sample size once a critical threshold is exceeded. Interestingly, an implication is that statistical power of service intervention RCTs peaks at intermediate sample sizes -- directly contradicting conventional power calculations that assume monotonically increasing power with sample size. We instantiate our insights using simulations calibrated to a real-world trial evaluating a behavioral health intervention for tuberculosis patients in Kenya. Our simulation results suggest that a trial with high service capacity but limited sample size can obtain the same statistical power as a trial with lower service capacity but large sample size. Taken together, our results highlight the importance of capacity selection in experiment design and provide a mechanism for why experiments may fail to replicate or perform at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21322v3</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justin Boutilier, Jonas Oddur Jonasson, Hannah Li, Erez Yoeli</dc:creator>
    </item>
    <item>
      <title>A Survey on Archetypal Analysis</title>
      <link>https://arxiv.org/abs/2504.12392</link>
      <description>arXiv:2504.12392v2 Announce Type: replace 
Abstract: Archetypal analysis (AA) was originally proposed in 1994 by Adele Cutler and Leo Breiman as a computational procedure for extracting distinct aspects, so-called archetypes, from observations, with each observational record approximated as a mixture (i.e., convex combination) of these archetypes. AA thereby provides straightforward, interpretable, and explainable representations for feature extraction and dimensionality reduction, facilitating the understanding of the structure of high-dimensional data and enabling wide applications across the sciences. However, AA also faces challenges, particularly as the associated optimization problem is non-convex. This is the first survey that provides researchers and data mining practitioners with an overview of the methodologies and opportunities that AA offers, surveying the many applications of AA across disparate fields of science, as well as best practices for modeling data with AA and its limitations. The survey concludes by explaining crucial future research directions concerning AA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.12392v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aleix Alcacer, Irene Epifanio, Sebastian Mair, Morten M{\o}rup</dc:creator>
    </item>
    <item>
      <title>On Bayes factor functions</title>
      <link>https://arxiv.org/abs/2506.16674</link>
      <description>arXiv:2506.16674v3 Announce Type: replace 
Abstract: We describe Bayes factors functions based on the sampling distributions of \emph{z}, \emph{t}, $\chi^2$, and \emph{F} statistics, using a class of inverse-moment prior distributions to define alternative hypotheses. These non-local alternative prior distributions are centered on standardized effects, which serve as indices for the Bayes factor function. We compare the conclusions drawn from resulting Bayes factor functions to those drawn from Bayes factors defined using local alternative prior specifications and examine their frequentist operating characteristics. Finally, an application of Bayes factor functions to replicated experimental designs in psychology is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16674v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saptati Datta, Riana Guha, Rachael Shudde, Valen E. Johnson</dc:creator>
    </item>
    <item>
      <title>Constructing Large Orthogonal Minimally Aliased Response Surface Designs by Concatenating Two Definitive Screening Designs</title>
      <link>https://arxiv.org/abs/2511.02984</link>
      <description>arXiv:2511.02984v2 Announce Type: replace 
Abstract: Orthogonal minimally aliased response surface (OMARS) designs permit the study of quantitative factors at three levels using an economical number of runs. In these designs, the linear effects of the factors are neither aliased with each other nor with the quadratic effects and the two-factor interactions. Complete catalogs of OMARS designs with up to five factors have been obtained using an enumeration algorithm. However, the algorithm is computationally demanding for designs with many factors and runs. To overcome this issue, we propose a construction method for large OMARS designs that concatenates two definitive screening designs and improves the statistical features of its parent designs. The concatenation employs an algorithm that minimizes the aliasing among the second-order effects using foldover techniques and column permutations for one of the parent designs. We study the properties of the new OMARS designs and compare them with alternative designs in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02984v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alan R. Vazquez, Peter Goos, Eric D. Schoen</dc:creator>
    </item>
    <item>
      <title>Conformalized Bayesian Inference, with Applications to Random Partition Models</title>
      <link>https://arxiv.org/abs/2511.05746</link>
      <description>arXiv:2511.05746v2 Announce Type: replace 
Abstract: Bayesian posterior distributions naturally represent parameter uncertainty informed by data. However, when the parameter space is complex, as in many nonparametric settings where it is infinite-dimensional or combinatorially large, standard summaries such as posterior means, credible intervals, or simple notions of multimodality are often unavailable, hindering interpretable posterior uncertainty quantification. We introduce Conformalized Bayesian Inference (CBI), a broadly applicable and computationally efficient framework for posterior inference on nonstandard parameter spaces. CBI yields a point estimate, a credible region with assumption-free posterior coverage guarantees, and a principled analysis of posterior multimodality, requiring only Monte Carlo samples from the posterior and a notion of discrepancy between parameters. The method builds a pseudo-density score for each parameter value, yielding a MAP-like point estimate and a credible region derived from conformal prediction principles. The key conceptual step underlying this construction is the reinterpretation of posterior inference as prediction on the parameter space. A final density-based clustering step identifies representative posterior modes. We investigate a number of theoretical and methodological properties of CBI and demonstrate its practicality, scalability, and versatility in simulated and real data clustering applications with random partition models. An accompanying Python library, cbi_partitions, is available at github.com/nbariletto/cbi_partitions_repo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05746v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicola Bariletto, Nhat Ho, Alessandro Rinaldo</dc:creator>
    </item>
    <item>
      <title>Modeling Issues with Eye Tracking Data</title>
      <link>https://arxiv.org/abs/2512.15950</link>
      <description>arXiv:2512.15950v2 Announce Type: replace 
Abstract: I describe and compare procedures for binary eye-tracking (ET) data. These procedures are applied to both raw and compressed data. The basic GLMM model is a logistic mixed model combined with random effects for persons and items. Additional models address autocorrelation eye-tracking serial observations. In particular, two novel approaches are illustrated that address serial without the use of an observed lag-1 predictor: a first-order autoregressive model obtained with generalized estimating equations, and a recurrent two-state survival model. Altogether, the results of four different analyses point to unresolved issues in the analysis of eye-tracking data and new directions for analytic development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15950v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregory Camilli</dc:creator>
    </item>
    <item>
      <title>Targeted Learning for Variable Importance</title>
      <link>https://arxiv.org/abs/2411.02221</link>
      <description>arXiv:2411.02221v2 Announce Type: replace-cross 
Abstract: Variable importance is one of the most widely used measures for interpreting machine learning with significant interest from both statistics and machine learning communities. Recently, increasing attention has been directed toward uncertainty quantification in these metrics. Current approaches largely rely on one-step procedures, which, while asymptotically efficient, can present higher sensitivity and instability in finite sample settings. To address these limitations, we propose a novel method by employing the targeted learning (TL) framework, designed to enhance robustness in inference for variable importance metrics. Our approach is particularly suited for conditional permutation variable importance. We show that it (i) retains the asymptotic efficiency of traditional methods, (ii) maintains comparable computational complexity, and (iii) delivers improved accuracy, especially in finite sample contexts. We further support these findings with numerical experiments that illustrate the practical advantages of our method and validate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02221v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaohan Wang, Yunzhe Zhou, Giles Hooker</dc:creator>
    </item>
    <item>
      <title>Clustering and Pruning in Causal Data Fusion</title>
      <link>https://arxiv.org/abs/2505.15215</link>
      <description>arXiv:2505.15215v2 Announce Type: replace-cross 
Abstract: Data fusion, the process of combining observational and experimental data, can enable the identification of causal effects that would otherwise remain non-identifiable. Although identification algorithms have been developed for specific scenarios, do-calculus remains the only general-purpose tool for causal data fusion, particularly when variables are present in some data sources but not others. However, approaches based on do-calculus may encounter computational challenges as the number of variables increases and the causal graph grows in complexity. Consequently, there exists a need to reduce the size of such models while preserving the essential features. For this purpose, we propose pruning (removing unnecessary variables) and clustering (combining variables) as preprocessing operations for causal data fusion. We generalize earlier results on a single data source and derive conditions for applying pruning and clustering in the case of multiple data sources. We give sufficient conditions for inferring the identifiability or non-identifiability of a causal effect in a larger graph based on a smaller graph and show how to obtain the corresponding identifying functional for identifiable causal effects. Examples from epidemiology and social science demonstrate the use of the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15215v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Otto Tabell, Santtu Tikka, Juha Karvanen</dc:creator>
    </item>
  </channel>
</rss>
