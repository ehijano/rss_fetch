<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Jun 2024 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Routing Algorithm for Software Defined Network Based on Boxcovering Algorithm</title>
      <link>https://arxiv.org/abs/2406.05528</link>
      <description>arXiv:2406.05528v1 Announce Type: new 
Abstract: A routing algorithm is the most fundamental problem in complex network communication. In complex networks, the amount of computation increases as the number of nodes increases which reduces routing performance. In this paper, we propose a routing algorithm for software-defined networking (SDN) based on a box-covering (BC) algorithm. It is known that using the BC algorithm it is possible to increase performance in complex SDN. We partition the entire SDN network into subnets using three existing box-covering methods such as MEMB, GC, and CIEA, then we use Dijkstra\textquotesingle s algorithm to find the shortest path between subnets and within each subnet. We compared all box-covering algorithms and found that the GC algorithm has the highest performance for SDN routing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05528v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/WINCOM59760.2023.10322960</arxiv:DOI>
      <dc:creator>Dana Turlykozhayeva, Sayat Akhtanov, Nurzhan Ussipov, Almat Akhmetali, Aslan Bolysbay, Yerkin Shabdan</dc:creator>
    </item>
    <item>
      <title>Single Gateway Placement in Wireless Mesh Networks</title>
      <link>https://arxiv.org/abs/2406.05698</link>
      <description>arXiv:2406.05698v1 Announce Type: new 
Abstract: Wireless Mesh Networks (WMNs) are crucial for various sectors due to their adaptability and scalability, providing robust connectivity where traditional wired networks are impractical. WMNs facilitate smart city initiatives, disaster recovery efforts, and industrial automation, playing a pivotal role in modern networking applications. Their versatility also extends to rural connectivity, highlighting their relevance in diverse scenarios. Recent research in WMNs has focused on optimizing gateway placement and selection to enhance network performance and ensure efficient data transmission. This paper introduces a novel approach to maximize average throughput by strategically positioning gateways within the mesh topology. Inspired by Coulomb's law, which has been used in network analysis, this approach aims to improve network performance through strategic gateway positioning. Comprehensive simulations and analyses demonstrate the effectiveness of the proposed method in enhancing both throughput and network efficiency. By leveraging physics-based models like Coulomb's law, the study offers an objective means to optimize gateway placement, a critical component in WMN design. These findings provide valuable insights for network designers and operators, guiding informed decision-making for gateway deployment across various WMN deployments. This research significantly contributes to the ongoing evolution of WMN optimization strategies, reaffirming the essential role of gateway placement in establishing resilient and efficient wireless communication infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05698v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>D. A. Turlykozhayeva, W. Waldemar, A. B. Akhmetali, N. M. Ussipov, S. A. Temesheva, S. N. Akhtanov</dc:creator>
    </item>
    <item>
      <title>Revisiting Multi-User Downlink in IEEE 802.11ax: A Designers Guide to MU-MIMO</title>
      <link>https://arxiv.org/abs/2406.05913</link>
      <description>arXiv:2406.05913v1 Announce Type: new 
Abstract: Downlink (DL) Multi-User (MU) Multiple Input Multiple Output (MU-MIMO) is a key technology that allows multiple concurrent data transmissions from an Access Point (AP) to a selected sub-set of clients for higher network efficiency in IEEE 802.11ax. However, DL MU-MIMO feature is typically turned off as the default setting in AP vendors' products, that is, turning on the DL MU-MIMO may not help increase the network efficiency, which is counter-intuitive. In this article, we provide a sufficiently deep understanding of the interplay between the various underlying factors, i.e., CSI overhead and spatial correlation, which result in negative results when turning on the DL MU-MIMO. Furthermore, we provide a fundamental guideline as a function of operational scenarios to address the fundamental question "when the DL MU-MIMO should be turned on/off".</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05913v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liu Cao, Lyutianyang Zhang, Sumit Roy, Sian Jin</dc:creator>
    </item>
    <item>
      <title>LLM-Based Intent Processing and Network Optimization Using Attention-Based Hierarchical Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2406.06059</link>
      <description>arXiv:2406.06059v1 Announce Type: new 
Abstract: Intent-based network automation is a promising tool to enable easier network management however certain challenges need to be effectively addressed. These are: 1) processing intents, i.e., identification of logic and necessary parameters to fulfill an intent, 2) validating an intent to align it with current network status, and 3) satisfying intents via network optimizing functions like xApps and rApps in O-RAN. This paper addresses these points via a three-fold strategy to introduce intent-based automation for O-RAN. First, intents are processed via a lightweight Large Language Model (LLM). Secondly, once an intent is processed, it is validated against future incoming traffic volume profiles (high or low). Finally, a series of network optimization applications (rApps and xApps) have been developed. With their machine learning-based functionalities, they can improve certain key performance indicators such as throughput, delay, and energy efficiency. In this final stage, using an attention-based hierarchical reinforcement learning algorithm, these applications are optimally initiated to satisfy the intent of an operator. Our simulations show that the proposed method can achieve at least 12% increase in throughput, 17.1% increase in energy efficiency, and 26.5% decrease in network delay compared to the baseline algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06059v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Arafat Habib, Pedro Enrique Iturria Rivera, Yigit Ozcan, Medhat Elsayed, Majid Bavand, Raimundus Gaigalas, Melike Erol-Kantarci</dc:creator>
    </item>
    <item>
      <title>Instability of Self-Driving Satellite Mega-Constellation: From Theory to Practical Impacts on Network Lifetime and Capacity</title>
      <link>https://arxiv.org/abs/2406.06068</link>
      <description>arXiv:2406.06068v1 Announce Type: new 
Abstract: Low Earth Orbit (LEO) satellite mega-constellations aim to enable high-speed Internet for numerous users anywhere on Earth. To safeguard their network infrastructure in congested outer space, they perform automatic orbital maneuvers to avoid collisions with external debris and satellites. However, our control-theoretic analysis and empirical validation using Starlink's space situational awareness datasets discover that, these safety-oriented maneuvers themselves can threaten safety and networking via cascaded collision avoidance inside the mega-constellation. This domino effect forces a dilemma between long-term LEO network lifetime and short-term LEO network capacity. Its root cause is that, the decades-old local pairwise maneuver paradigm for standalone satellites is inherently unstable if scaled out to recent mega-constellation networks. We thus propose an alternative bilateral maneuver control that stabilizes self-driving mega-constellations for concurrent network lifetime and capacity boosts. Our operational trace-driven emulation shows a 8$\times$ network lifetime extension in Starlink without limiting its network capacity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06068v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yimei Chen, Yuanjie Li, Hewu Li, Lixin Liu, Li Ouyang, Jiabo Yang, Junyi Li, Jianping Wu, Qian Wu, Jun Liu, Zeqi Lai</dc:creator>
    </item>
    <item>
      <title>The Evolution of Applications, Hardware Design, and Channel Modeling for Terahertz (THz) Band Communications and Sensing: Ready for 6G?</title>
      <link>https://arxiv.org/abs/2406.06105</link>
      <description>arXiv:2406.06105v1 Announce Type: new 
Abstract: For decades, the terahertz (THz) frequency band had been primarily explored in the context of radar, imaging, and spectroscopy, where multi-gigahertz (GHz) and even THz-wide channels and the properties of terahertz photons offered attractive target accuracy, resolution, and classification capabilities. Meanwhile, the exploitation of the terahertz band for wireless communication had originally been limited due to several reasons, including (i) no immediate need for such high data rates available via terahertz bands and (ii) challenges in designing sufficiently high power terahertz systems at reasonable cost and efficiency, leading to what was often referred to as "the terahertz gap". This roadmap paper first reviews the evolution of the hardware design approaches for terahertz systems, including electronic, photonic, and plasmonic approaches, and the understanding of the terahertz channel itself, in diverse scenarios, ranging from common indoors and outdoors scenarios to intra-body and outer-space environments. The article then summarizes the lessons learned during this multi-decade process and the cutting-edge state-of-the-art findings, including novel methods to quantify power efficiency, which will become more important in making design choices. Finally, the manuscript presents the authors' perspective and insights on how the evolution of terahertz systems design will continue toward enabling efficient terahertz communications and sensing solutions as an integral part of next-generation wireless systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06105v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Josep M. Jornet, Vitaly Petrov, Hua Wang, Zoya Popovic, Dipankar Shakya, Jose V. Siles, Theodore S. Rappaport</dc:creator>
    </item>
    <item>
      <title>A Taxonomy and Comparative Analysis of IPv4 ID Selection Correctness, Security, and Performance</title>
      <link>https://arxiv.org/abs/2406.06483</link>
      <description>arXiv:2406.06483v1 Announce Type: new 
Abstract: The battle for a more secure Internet is waged on many fronts, including the most basic of networking protocols. Our focus is the IPv4 Identifier (IPID), an IPv4 header field as old as the Internet with an equally long history as an exploited side channel for scanning network properties, inferring off-path connections, and poisoning DNS caches. This article taxonomizes the 25-year history of IPID-based exploits and the corresponding changes to IPID selection methods. By mathematically analyzing these methods' correctness and security and empirically evaluating their performance, we reveal recommendations for best practice as well as shortcomings of current operating system implementations, emphasizing the value of systematic evaluations in network security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06483v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joshua J. Daymude, Antonio M. Espinoza, Sean Bergen, Benjamin Mixon-Baca, Jeffrey Knockel, Jedidiah R. Crandall</dc:creator>
    </item>
    <item>
      <title>Multi-attribute Auction-based Resource Allocation for Twins Migration in Vehicular Metaverses: A GPT-based DRL Approach</title>
      <link>https://arxiv.org/abs/2406.05418</link>
      <description>arXiv:2406.05418v1 Announce Type: cross 
Abstract: Vehicular Metaverses are developed to enhance the modern automotive industry with an immersive and safe experience among connected vehicles and roadside infrastructures, e.g., RoadSide Units (RSUs). For seamless synchronization with virtual spaces, Vehicle Twins (VTs) are constructed as digital representations of physical entities. However, resource-intensive VTs updating and high mobility of vehicles require intensive computation, communication, and storage resources, especially for their migration among RSUs with limited coverages. To address these issues, we propose an attribute-aware auction-based mechanism to optimize resource allocation during VTs migration by considering both price and non-monetary attributes, e.g., location and reputation. In this mechanism, we propose a two-stage matching for vehicular users and Metaverse service providers in multi-attribute resource markets. First, the resource attributes matching algorithm obtains the resource attributes perfect matching, namely, buyers and sellers can participate in a double Dutch auction (DDA). Then, we train a DDA auctioneer using a generative pre-trained transformer (GPT)-based deep reinforcement learning (DRL) algorithm to adjust the auction clocks efficiently during the auction process. We compare the performance of social welfare and auction information exchange costs with state-of-the-art baselines under different settings. Simulation results show that our proposed GPT-based DRL auction schemes have better performance than others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05418v1</guid>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongju Tong, Junlong Chen, Minrui Xu, Jiawen Kang, Zehui Xiong, Dusit Niyato, Chau Yuen, Zhu Han</dc:creator>
    </item>
    <item>
      <title>Novel Approach to Intrusion Detection: Introducing GAN-MSCNN-BILSTM with LIME Predictions</title>
      <link>https://arxiv.org/abs/2406.05443</link>
      <description>arXiv:2406.05443v1 Announce Type: cross 
Abstract: This paper introduces an innovative intrusion detection system that harnesses Generative Adversarial Networks (GANs), Multi-Scale Convolutional Neural Networks (MSCNNs), and Bidirectional Long Short-Term Memory (BiLSTM) networks, supplemented by Local Interpretable Model-Agnostic Explanations (LIME) for interpretability. Employing a GAN, the system generates realistic network traffic data, encompassing both normal and attack patterns. This synthesized data is then fed into an MSCNN-BiLSTM architecture for intrusion detection. The MSCNN layer extracts features from the network traffic data at different scales, while the BiLSTM layer captures temporal dependencies within the traffic sequences. Integration of LIME allows for explaining the model's decisions. Evaluation on the Hogzilla dataset, a standard benchmark, showcases an impressive accuracy of 99.16\% for multi-class classification and 99.10\% for binary classification, while ensuring interpretability through LIME. This fusion of deep learning and interpretability presents a promising avenue for enhancing intrusion detection systems by improving transparency and decision support in network security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05443v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.56294/dm2023202</arxiv:DOI>
      <arxiv:journal_reference>Data and Metadata, 2023 Dec. 28</arxiv:journal_reference>
      <dc:creator>Asmaa Benchama, Khalid Zebbara</dc:creator>
    </item>
    <item>
      <title>Uplink resource allocation optimization for user-centric cell-free MIMO networks</title>
      <link>https://arxiv.org/abs/2406.05576</link>
      <description>arXiv:2406.05576v1 Announce Type: cross 
Abstract: We examine the problem of optimizing resource allocation in the uplink for a user-centric, cell-free, multi-input multi-output network. We start by modeling and developing resource allocation algorithms for two standard network operation modes. The centralized mode provides high data rates but suffers multiple issues, including scalability. On the other hand, the distributed mode has the opposite problem: relatively low rates, but is scalable. To address these challenges, we combine the strength of the two standard modes, creating a new semi-distributed operation mode. To avoid the need for information exchange between access points, we introduce a new quality of service metric to decentralize the resource allocation algorithms. Our results show that we can eliminate the need for information exchange with a relatively small penalty on data rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05576v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TWC.2024.3393869</arxiv:DOI>
      <dc:creator>Zehua Li, Raviraj Adve</dc:creator>
    </item>
    <item>
      <title>Sequential Binary Classification for Intrusion Detection in Software Defined Networks</title>
      <link>https://arxiv.org/abs/2406.06099</link>
      <description>arXiv:2406.06099v1 Announce Type: cross 
Abstract: Software-Defined Networks (SDN) are the standard architecture for network deployment. Intrusion Detection Systems (IDS) are a pivotal part of this technology as networks become more vulnerable to new and sophisticated attacks. Machine Learning (ML)-based IDS are increasingly seen as the most effective approach to handle this issue. However, IDS datasets suffer from high class imbalance, which impacts the performance of standard ML models. We propose Sequential Binary Classification (SBC) - an algorithm for multi-class classification to address this issue. SBC is a hierarchical cascade of base classifiers, each of which can be modelled on any general binary classifier. Extensive experiments are reported on benchmark datasets that evaluate the performance of SBC under different scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06099v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ishan Chokshi, Shrihari Vasudevan, Nachiappan Sundaram, Raaghul Ranganathan</dc:creator>
    </item>
    <item>
      <title>Federated Machine Reasoning for Resource Provisioning in 6G O-RAN</title>
      <link>https://arxiv.org/abs/2406.06128</link>
      <description>arXiv:2406.06128v1 Announce Type: cross 
Abstract: O-RAN specifications reshape RANs with function disaggregation and open interfaces, driven by RAN Intelligent Controllers. This enables data-driven management through AI/ML but poses trust challenges due to human operators' limited understanding of AI/ML decision-making. Balancing resource provisioning and avoiding overprovisioning and underprovisioning is critical, especially among the multiple virtualized base station(vBS) instances. Thus, we propose a novel Federated Machine Reasoning (FLMR) framework, a neurosymbolic method for federated reasoning, learning, and querying. FLMR optimizes CPU demand prediction based on contextual information and vBS configuration using local monitoring data from virtual base stations (vBS) on a shared O-Cloud platform.This optimization is critical, as insufficient computing resources can result in synchronization loss and significantly reduce network throughput. In the telecom domain, particularly in the virtual Radio Access Network (vRAN) sector, predicting and managing the CPU load of vBSs poses a significant challenge for network operators. Our proposed FLMR framework ensures transparency and human understanding in AI/ML decisions and addresses the evolving demands of the 6G O-RAN landscape, where reliability and performance are paramount. Furthermore, we performed a comparative analysis using \textit{DeepCog} as the baseline method. The outcomes highlight how our proposed approach outperforms the baseline and strikes a better balance between resource overprovisioning and underprovisioning. Our method notably lowers both provisioning relative to the baseline by a factor of 6.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06128v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Swastika Roy, Hatim Chergui, Adlen Ksentini, Christos Verikoukis</dc:creator>
    </item>
    <item>
      <title>Early Acceptance Matching Game for User-Centric Clustering in Scalable Cell-free MIMO Networks</title>
      <link>https://arxiv.org/abs/2406.06402</link>
      <description>arXiv:2406.06402v1 Announce Type: cross 
Abstract: The canonical setup is the primary approach adopted in cell-free multiple-input multiple-output (MIMO) networks, in which all access points (APs) jointly serve every user equipment (UE). This approach is not scalable in terms of computational complexity and fronthaul signaling becoming impractical in large networks. This work adopts a user-centric approach, a scalable alternative in which only a set of preferred APs jointly serve a UE. Forming the optimal cluster of APs for each UE is a challenging task, especially, when it needs to be dynamically adjusted to meet the quality of service (QoS) requirements of the UE. This complexity is even exacerbated when considering the constrained fronthaul capacity of the UE and the AP. We solve this problem with a novel many-to-many matching game. More specifically, we devise an early acceptance matching algorithm, which immediately admits or rejects UEs based on their requests and available radio resources. The proposed solution significantly reduces the fronthaul signaling while satisfying the maximum of UEs in terms of requested QoS compared to state-of-the-art approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06402v1</guid>
      <category>eess.SP</category>
      <category>cs.MA</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ala Eddine Nouali, Mohamed Sana, Jean-Paul Jamont</dc:creator>
    </item>
    <item>
      <title>Unveiling Energy Efficiency in Deep Learning: Measurement, Prediction, and Scoring across Edge Devices</title>
      <link>https://arxiv.org/abs/2310.18329</link>
      <description>arXiv:2310.18329v2 Announce Type: replace 
Abstract: Today, deep learning optimization is primarily driven by research focused on achieving high inference accuracy and reducing latency. However, the energy efficiency aspect is often overlooked, possibly due to a lack of sustainability mindset in the field and the absence of a holistic energy dataset. In this paper, we conduct a threefold study, including energy measurement, prediction, and efficiency scoring, with an objective to foster transparency in power and energy consumption within deep learning across various edge devices. Firstly, we present a detailed, first-of-its-kind measurement study that uncovers the energy consumption characteristics of on-device deep learning. This study results in the creation of three extensive energy datasets for edge devices, covering a wide range of kernels, state-of-the-art DNN models, and popular AI applications. Secondly, we design and implement the first kernel-level energy predictors for edge devices based on our kernel-level energy dataset. Evaluation results demonstrate the ability of our predictors to provide consistent and accurate energy estimations on unseen DNN models. Lastly, we introduce two scoring metrics, PCS and IECS, developed to convert complex power and energy consumption data of an edge device into an easily understandable manner for edge device end-users. We hope our work can help shift the mindset of both end-users and the research community towards sustainability in edge computing, a principle that drives our research. Find data, code, and more up-to-date information at https://amai-gsu.github.io/DeepEn2023.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18329v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3583740.3628442</arxiv:DOI>
      <dc:creator>Xiaolong Tu, Anik Mallik, Dawei Chen, Kyungtae Han, Onur Altintas, Haoxin Wang, Jiang Xie</dc:creator>
    </item>
    <item>
      <title>EdgeLoc: A Communication-Adaptive Parallel System for Real-Time Localization in Infrastructure-Assisted Autonomous Driving</title>
      <link>https://arxiv.org/abs/2405.12120</link>
      <description>arXiv:2405.12120v2 Announce Type: replace-cross 
Abstract: This paper presents EdgeLoc, an infrastructure-assisted, real-time localization system for autonomous driving that addresses the incompatibility between traditional localization methods and deep learning approaches. The system is built on top of the Robot Operating System (ROS) and combines the real-time performance of traditional methods with the high accuracy of deep learning approaches. The system leverages edge computing capabilities of roadside units (RSUs) for precise localization to enhance on-vehicle localization that is based on the real-time visual odometry. EdgeLoc is a parallel processing system, utilizing a proposed uncertainty-aware pose fusion solution. It achieves communication adaptivity through online learning and addresses fluctuations via window-based detection. Moreover, it achieves optimal latency and maximum improvement by utilizing auto-splitting vehicle-infrastructure collaborative inference, as well as online distribution learning for decision-making. Even with the most basic end-to-end deep neural network for localization estimation, EdgeLoc realizes a 67.75\% reduction in the localization error for real-time local visual odometry, a 29.95\% reduction for non-real-time collaborative inference, and a 30.26\% reduction compared to Kalman filtering. Finally, accuracy-to-latency conversion was experimentally validated, and an overall experiment was conducted on a practical cellular network. The system is open sourced at https://github.com/LoganCome/EdgeAssistedLocalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12120v2</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Boyi Liu, Jingwen Tong, Yufan Zhuang</dc:creator>
    </item>
  </channel>
</rss>
