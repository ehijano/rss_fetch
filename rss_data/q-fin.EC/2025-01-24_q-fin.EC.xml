<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.EC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.EC</link>
    <description>q-fin.EC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.EC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Jan 2025 05:15:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>People Reduce Workers' Compensation for Using Artificial Intelligence (AI)</title>
      <link>https://arxiv.org/abs/2501.13228</link>
      <description>arXiv:2501.13228v1 Announce Type: new 
Abstract: We investigate whether and why people might reduce compensation for workers who use AI tools. Across 10 studies (N = 3,346), participants consistently lowered compensation for workers who used AI tools. This "AI Penalization" effect was robust across (1) different types of work and worker statuses and worker statuses (e.g., full-time, part-time, or freelance), (2) different forms of compensation (e.g., required payments or optional bonuses) and their timing, (3) various methods of eliciting compensation (e.g., slider scale, multiple choice, and numeric entry), and (4) conditions where workers' output quality was held constant, subject to varying inferences, or controlled for. Moreover, the effect emerged not only in hypothetical compensation scenarios (Studies 1-5) but also with real gig workers and real monetary compensation (Study 6). People reduced compensation for workers using AI tools because they believed these workers deserved less credit than those who did not use AI (Studies 3 and 4). This effect weakened when it is less permissible to reduce worker compensation, such as when employment contracts provide stricter constraints (Study 4). Our findings suggest that adoption of AI tools in the workplace may exacerbate inequality among workers, as those protected by structured contracts face less vulnerability to compensation reductions, while those without such protections risk greater financial penalties for using AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13228v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jin Kim, Shane Schweitzer, Christoph Riedl, David De Cremer</dc:creator>
    </item>
    <item>
      <title>A Model of Enclosures: Coordination, Conflict, and Efficiency in the Transformation of Land Property Rights</title>
      <link>https://arxiv.org/abs/2311.01592</link>
      <description>arXiv:2311.01592v3 Announce Type: replace 
Abstract: Economists, historians, and social scientists have long debated how open-access areas, frontier regions, and customary landholding regimes came to be enclosed or otherwise transformed into private property. This paper analyzes decentralized enclosure processes using the theory of aggregative games, examining how population density, enclosure costs, potential productivity gains, and the broader physical, institutional, and policy environment jointly determine the property regime. Changes to any of these factors can lead to smooth or abrupt changes in equilibria that can result in inefficiently high, inefficiently low, or efficient levels of enclosure and associated technological transformation. Inefficient outcomes generally fall short of second-best. While policies to strengthen customary governance or compensate displaced stakeholders can realign incentives, addressing one market failure while neglecting others can worsen outcomes. Our analysis provides a unified framework for evaluating mechanisms emphasized in Neoclassical, Neo-institutional, and Marxian interpretations of historical enclosure processes and contemporary land formalization policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01592v3</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew J. Baker, Jonathan Conning</dc:creator>
    </item>
    <item>
      <title>Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina</title>
      <link>https://arxiv.org/abs/2410.19599</link>
      <description>arXiv:2410.19599v3 Announce Type: replace 
Abstract: Recent studies suggest large language models (LLMs) can exhibit human-like reasoning, aligning with human behavior in economic experiments, surveys, and political discourse. This has led many to propose that LLMs can be used as surrogates or simulations for humans in social science research. However, LLMs differ fundamentally from humans, relying on probabilistic patterns, absent the embodied experiences or survival objectives that shape human cognition. We assess the reasoning depth of LLMs using the 11-20 money request game. Nearly all advanced approaches fail to replicate human behavior distributions across many models. Causes of failure are diverse and unpredictable, relating to input language, roles, and safeguarding. These results advise caution when using LLMs to study human behavior or as surrogates or simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19599v3</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Gao, Dokyun Lee, Gordon Burtch, Sina Fazelpour</dc:creator>
    </item>
  </channel>
</rss>
