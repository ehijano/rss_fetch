<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Apr 2025 01:51:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Regularized Generalized Covariance (RGCov) Estimator</title>
      <link>https://arxiv.org/abs/2504.18678</link>
      <description>arXiv:2504.18678v1 Announce Type: new 
Abstract: We introduce a regularized Generalized Covariance (RGCov) estimator as an extension of the GCov estimator to high dimensional setting that results either from high-dimensional data or a large number of nonlinear transformations used in the objective function. The approach relies on a ridge-type regularization for high-dimensional matrix inversion in the objective function of the GCov. The RGCov estimator is consistent and asymptotically normally distributed. We provide the conditions under which it can reach semiparametric efficiency and discuss the selection of the optimal regularization parameter. We also examine the diagonal GCov estimator, which simplifies the computation of the objective function. The GCov-based specification test, and the test for nonlinear serial dependence (NLSD) are extended to the regularized RGCov specification and RNLSD tests with asymptotic Chi-square distributions. Simulation studies show that the RGCov estimator and the regularized tests perform well in the high dimensional setting. We apply the RGCov to estimate the mixed causal and noncausal VAR model of stock prices of green energy companies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18678v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Giancaterini, Alain Hecq, Joann Jasiak, Aryan Manafi Neyazi</dc:creator>
    </item>
    <item>
      <title>Inference in High-Dimensional Panel Models: Two-Way Dependence and Unobserved Heterogeneity</title>
      <link>https://arxiv.org/abs/2504.18772</link>
      <description>arXiv:2504.18772v1 Announce Type: new 
Abstract: Panel data allows for the modeling of unobserved heterogeneity, significantly raising the number of nuisance parameters and making high dimensionality a practical issue. Meanwhile, temporal and cross-sectional dependence in panel data further complicates high-dimensional estimation and inference. This paper proposes a toolkit for high-dimensional panel models with large cross-sectional and time sample sizes. To reduce the dimensionality, I propose a weighted LASSO using two-way cluster-robust penalty weights. Although consistent, the convergence rate of LASSO is slow due to the cluster dependence, rendering inference challenging in general. Nevertheless, asymptotic normality can be established in a semiparametric moment-restriction model by leveraging a clustered-panel cross-fitting approach and, as a special case, in a partial linear model using the full sample. In a panel estimation of the government spending multiplier, I demonstrate how high dimensionality could be hidden and how the proposed toolkit enables flexible modeling and robust inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18772v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaicheng Chen</dc:creator>
    </item>
    <item>
      <title>Identifying the Frontier Structural Function and Bounding Mean Deviations</title>
      <link>https://arxiv.org/abs/2504.19832</link>
      <description>arXiv:2504.19832v1 Announce Type: new 
Abstract: This paper analyzes a model in which an outcome variable equals the difference between a frontier function of inputs and a nonnegative unobserved deviation. If zero is in the support of the deviation at a given input value, then the frontier function is identified by the maximum outcome there. This obviates the need for instrumental variables. Implementation requires allowing for the distribution of deviations to depend on inputs, thus not ruling out endogenous inputs and ensuring the estimated frontier is not merely a constant shift of a biased conditional expectation. Including random errors results in a stochastic frontier analysis model generalized to allow the joint distribution of deviations and errors to depend on inputs. If the minimum deviation is a function of inputs, then we derive a lower bound for the mean deviation using variance and skewness, without making parametric distributional assumptions. We apply our results to a frontier production function, with deviations representing inefficiencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19832v1</guid>
      <category>econ.EM</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dan Ben-Moshe, David Genesove</dc:creator>
    </item>
    <item>
      <title>Inference with few treated units</title>
      <link>https://arxiv.org/abs/2504.19841</link>
      <description>arXiv:2504.19841v1 Announce Type: new 
Abstract: In many causal inference applications, only one or a few units (or clusters of units) are treated. An important challenge in such settings is that standard inference methods that rely on asymptotic theory may be unreliable, even when the total number of units is large. This survey reviews and categorizes inference methods that are designed to accommodate few treated units, considering both cross-sectional and panel data methods. We discuss trade-offs and connections between different approaches. In doing so, we propose slight modifications to improve the finite-sample validity of some methods, and we also provide theoretical justifications for existing heuristic approaches that have been proposed in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19841v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luis Alvarez, Bruno Ferman, Kaspar W\"uthrich</dc:creator>
    </item>
    <item>
      <title>Finite-Sample Properties of Generalized Ridge Estimators for Nonlinear Models</title>
      <link>https://arxiv.org/abs/2504.19018</link>
      <description>arXiv:2504.19018v1 Announce Type: cross 
Abstract: Parameter estimation can result in substantial mean squared error (MSE), even when consistent estimators are used and the sample size is large. This paper addresses the longstanding statistical challenge of analyzing the bias and MSE of ridge-type estimators in nonlinear models, including duration, Poisson, and multinomial choice models, where theoretical results have been scarce. Employing a finite-sample approximation technique developed in the econometrics literature, this study derives new theoretical results showing that the generalized ridge maximum likelihood estimator (MLE) achieves lower finite-sample MSE than the conventional MLE across a broad class of nonlinear models. Importantly, the analysis extends beyond parameter estimation to model-based prediction, demonstrating that the generalized ridge estimator improves predictive accuracy relative to the generic MLE for sufficiently small penalty terms, regardless of the validity of the incorporated hypotheses. Extensive simulation studies and an empirical application involving the estimation of marginal mean and quantile treatment effects further support the superior performance and practical applicability of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19018v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masamune Iwasawa</dc:creator>
    </item>
    <item>
      <title>Semiparametric Conditional Factor Models in Asset Pricing</title>
      <link>https://arxiv.org/abs/2112.07121</link>
      <description>arXiv:2112.07121v5 Announce Type: replace 
Abstract: We introduce a simple and tractable methodology for estimating semiparametric conditional latent factor models. Our approach disentangles the roles of characteristics in capturing factor betas of asset returns from ``alpha.'' We construct factors by extracting principal components from Fama-MacBeth managed portfolios. Applying this methodology to the cross-section of U.S. individual stock returns, we find compelling evidence of substantial nonzero pricing errors, even though our factors demonstrate superior performance in standard asset pricing tests. Unexplained ``arbitrage'' portfolios earn high Sharpe ratios, which decline over time. Combining factors with these orthogonal portfolios produces out-of-sample Sharpe ratios exceeding 4.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.07121v5</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qihui Chen, Nikolai Roussanov, Xiaoliang Wang</dc:creator>
    </item>
    <item>
      <title>Negative Control Falsification Tests for Instrumental Variable Designs</title>
      <link>https://arxiv.org/abs/2312.15624</link>
      <description>arXiv:2312.15624v3 Announce Type: replace 
Abstract: The validity of instrumental variable (IV) designs is typically tested using two types of falsification tests. We characterize these tests as conditional independence tests between negative control variables -- proxies for unobserved variables posing a threat to the identification -- and the IV or the outcome. We describe the conditions that variables must satisfy in order to serve as negative controls. We show that these falsification tests examine not only independence and the exclusion restriction, but also functional form assumptions. Our analysis reveals that conventional applications of these tests may flag problems even in valid IV designs. We offer implementation guidance to address these issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15624v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oren Danieli, Daniel Nevo, Itai Walk, Bar Weinstein, Dan Zeltzer</dc:creator>
    </item>
    <item>
      <title>Panel Stochastic Frontier Models with Latent Group Structures</title>
      <link>https://arxiv.org/abs/2412.08831</link>
      <description>arXiv:2412.08831v2 Announce Type: replace 
Abstract: Stochastic frontier models have attracted significant interest over the years due to their unique feature of including a distinct inefficiency term alongside the usual error term. To effectively separate these two components, strong distributional assumptions are often necessary. To overcome this limitation, numerous studies have sought to relax or generalize these models for more robust estimation. In line with these efforts, we introduce a latent group structure that accommodates heterogeneity across firms, addressing not only the stochastic frontiers but also the distribution of the inefficiency term. This framework accounts for the distinctive features of stochastic frontier models, and we propose a practical estimation procedure to implement it. Simulation studies demonstrate the strong performance of our proposed method, which is further illustrated through an application to study the cost efficiency of the U.S. commercial banking sector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08831v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Kazuki Tomioka, Thomas T. Yang, Xibin Zhang</dc:creator>
    </item>
    <item>
      <title>Causal Q-Aggregation for CATE Model Selection</title>
      <link>https://arxiv.org/abs/2310.16945</link>
      <description>arXiv:2310.16945v5 Announce Type: replace-cross 
Abstract: Accurate estimation of conditional average treatment effects (CATE) is at the core of personalized decision making. While there is a plethora of models for CATE estimation, model selection is a nontrivial task, due to the fundamental problem of causal inference. Recent empirical work provides evidence in favor of proxy loss metrics with double robust properties and in favor of model ensembling. However, theoretical understanding is lacking. Direct application of prior theoretical work leads to suboptimal oracle model selection rates due to the non-convexity of the model selection problem. We provide regret rates for the major existing CATE ensembling approaches and propose a new CATE model ensembling approach based on Q-aggregation using the doubly robust loss. Our main result shows that causal Q-aggregation achieves statistically optimal oracle model selection regret rates of $\frac{\log(M)}{n}$ (with $M$ models and $n$ samples), with the addition of higher-order estimation error terms related to products of errors in the nuisance functions. Crucially, our regret rate does not require that any of the candidate CATE models be close to the truth. We validate our new method on many semi-synthetic datasets and also provide extensions of our work to CATE model selection with instrumental variables and unobserved confounding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16945v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Hui Lan, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>The Risk-Neutral Equivalent Pricing of Model-Uncertainty</title>
      <link>https://arxiv.org/abs/2502.13744</link>
      <description>arXiv:2502.13744v4 Announce Type: replace-cross 
Abstract: Existing approaches to asset-pricing under model-uncertainty adapt classical utility-maximization frameworks and seek theoretical comprehensiveness. We move toward practice by considering binary model-uncertainties and by switching attention from 'preference' to 'constraints'. This decomposes economic asset-pricing into the viable pricing of model-risk and non-model risk separately such that the former has a unique and intuitive formula with convenient properties. Its parameter, dynamically conserved under model-risk inference, allows an integrated representation of ex-ante risk-pricing and bias, with ex-post price-effects that can be disentangled, through well-known anomalies, Momentum and Low-Risk, whose risk-reward curves acquire a new significance: peak-rewards measure ex-ante risk-pricing, and peak-locations, bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13744v4</guid>
      <category>q-fin.MF</category>
      <category>econ.EM</category>
      <pubDate>Tue, 29 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ken Kangda Wren</dc:creator>
    </item>
  </channel>
</rss>
