<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Mar 2025 01:54:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Online Disjoint Spanning Trees and Polymatroid Bases</title>
      <link>https://arxiv.org/abs/2503.19999</link>
      <description>arXiv:2503.19999v1 Announce Type: new 
Abstract: Finding the maximum number of disjoint spanning trees in a given graph is a well-studied problem with several applications and connections. The Tutte-Nash-Williams theorem provides a min-max relation for this problem which also extends to disjoint bases in a matroid and leads to efficient algorithms. Several other packing problems such as element disjoint Steiner trees, disjoint set covers, and disjoint dominating sets are NP-Hard but admit an $O(\log n)$-approximation. C\u{a}linescu, Chekuri, and Vondr\'ak viewed all these packing problems as packing bases of a polymatroid and provided a unified perspective. Motivated by applications in wireless networks, recent works have studied the problem of packing set covers in the online model. The online model poses new challenges for packing problems. In particular, it is not clear how to pack a maximum number of disjoint spanning trees in a graph when edges arrive online. Motivated by these applications and theoretical considerations, we formulate an online model for packing bases of a polymatroid, and describe a randomized algorithm with a polylogarithmic competitive ratio. Our algorithm is based on interesting connections to the notion of quotients of a polymatroid that has recently seen applications in polymatroid sparsification. We generalize the previously known result for the online disjoint set cover problem and also address several other packing problems in a unified fashion. For the special case of packing disjoint spanning trees in a graph (or a hypergraph) whose edges arrive online, we provide an alternative to our general algorithm that is simpler and faster while achieving the same poly-logarithmic competitive ratio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19999v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karthekeyan Chandrasekaran, Chandra Chekuri, Weihao Zhu</dc:creator>
    </item>
    <item>
      <title>Beyond Worst-Case Subset Sum: An Adaptive, Structure-Aware Solver with Sub-$2^{n/2}$ Enumeration</title>
      <link>https://arxiv.org/abs/2503.20162</link>
      <description>arXiv:2503.20162v1 Announce Type: new 
Abstract: The Subset Sum problem, which asks whether a set of $n$ integers has a subset summing to a target $t$, is a fundamental NP-complete problem in cryptography and combinatorial optimization. The classical meet-in-the-middle (MIM) algorithm of Horowitz--Sahni runs in $\widetilde{\mathcal{O}}\bigl(2^{n/2}\bigr)$, still the best-known deterministic bound. Yet many instances exhibit abundant collisions in partial sums, so actual hardness often depends on the number of unique sums ($U$).
  We present a structure-aware, adaptive solver that enumerates only distinct sums, pruning duplicates on the fly, thus running in $\widetilde{\mathcal{O}}(U)$ when $U \ll 2^n$. Its core is a unique-subset-sums enumerator combined with a double meet-in-the-middle strategy and lightweight dynamic programming, avoiding the classical MIM's expensive merge. We also introduce combinatorial tree compression to guarantee strictly sub-$2^{n/2}$ enumeration even on unstructured inputs, shaving a nontrivial constant from the exponent.
  Our solver supports anytime and online modes, producing partial solutions early and adapting to newly added elements. Theoretical analysis and experiments show that for structured instances -- e.g. with small doubling constants, high additive energy, or significant redundancy -- our method can far outperform classical approaches, often nearing dynamic-programming efficiency. Even in the worst case, it remains within $\widetilde{\mathcal{O}}\bigl(2^{n/2}\bigr)$, and its compression-based pruning yields a real constant-factor speedup over naive MIM. We conclude by discussing how this instance-specific adaptivity refines the Subset Sum complexity landscape and suggesting future adaptive-exponential directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20162v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesus Salas</dc:creator>
    </item>
    <item>
      <title>Global vs. s-t Vertex Connectivity Beyond Sequential: Almost-Perfect Reductions &amp; Near-Optimal Separations</title>
      <link>https://arxiv.org/abs/2503.20366</link>
      <description>arXiv:2503.20366v1 Announce Type: new 
Abstract: A recent breakthrough by [LNPSY STOC'21] showed that solving s-t vertex connectivity is sufficient (up to polylogarithmic factors) to solve (global) vertex connectivity in the sequential model. This raises a natural question: What is the relationship between s-t and global vertex connectivity in other computational models? In this paper, we demonstrate that the connection between global and s-t variants behaves very differently across computational models:
  1.In parallel and distributed models, we obtain almost tight reductions from global to s-t vertex connectivity. In PRAM, this leads to a $n^{\omega+o(1)}$-work and $n^{o(1)}$-depth algorithm for vertex connectivity, improving over the 35-year-old $\tilde O(n^{\omega+1})$-work $O(\log^2n)$-depth algorithm by [LLW FOCS'86], where $\omega$ is the matrix multiplication exponent and $n$ is the number of vertices. In CONGEST, the reduction implies the first sublinear-round (when the diameter is moderately small) vertex connectivity algorithm. This answers an open question in [JM STOC'23].
  2. In contrast, we show that global vertex connectivity is strictly harder than s-t vertex connectivity in the two-party communication setting, requiring $\tilde \Theta (n^{1.5})$ bits of communication. The s-t variant was known to be solvable in $\tilde O(n)$ communication [BvdBEMN FOCS'22]. Our results resolve open problems raised by [MN STOC'20, BvdBEMN FOCS'22, AS SOSA'23].
  At the heart of our results is a new graph decomposition framework we call \emph{common-neighborhood clustering}, which can be applied in multiple models. Finally, we observe that global vertex connectivity cannot be solved without using s-t vertex connectivity, by proving an s-t to global reduction in dense graphs, in the PRAM and communication models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20366v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joakim Blikstad, Yonggang Jiang, Sagnik Mukhopadhyay, Sorrachai Yingchareonthawornchai</dc:creator>
    </item>
    <item>
      <title>Finding Near-Optimal Maximum Set of Disjoint $k$-Cliques in Real-World Social Networks</title>
      <link>https://arxiv.org/abs/2503.20299</link>
      <description>arXiv:2503.20299v1 Announce Type: cross 
Abstract: A $k$-clique is a dense graph, consisting of $k$ fully-connected nodes, that finds numerous applications, such as community detection and network analysis. In this paper, we study a new problem, that finds a maximum set of disjoint $k$-cliques in a given large real-world graph with a user-defined fixed number $k$, which can contribute to a good performance of teaming collaborative events in online games. However, this problem is NP-hard when $k \geq 3$, making it difficult to solve. To address that, we propose an efficient lightweight method that avoids significant overheads and achieves a $k$-approximation to the optimal, which is equipped with several optimization techniques, including the ordering method, degree estimation in the clique graph, and a lightweight implementation. Besides, to handle dynamic graphs that are widely seen in real-world social networks, we devise an efficient indexing method with careful swapping operations, leading to the efficient maintenance of a near-optimal result with frequent updates in the graph. In various experiments on several large graphs, our proposed approaches significantly outperform the competitors by up to 2 orders of magnitude in running time and 13.3\% in the number of computed disjoint $k$-cliques, which demonstrates the superiority of the proposed approaches in terms of efficiency and effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20299v1</guid>
      <category>cs.SI</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenqing Lin, Xin Chen, Haoxuan Xie, Sibo Wang, Siqiang Luo</dc:creator>
    </item>
    <item>
      <title>Factorised Representations of Join Queries: Tight Bounds and a New Dichotomy</title>
      <link>https://arxiv.org/abs/2503.20438</link>
      <description>arXiv:2503.20438v1 Announce Type: cross 
Abstract: A common theme in factorised databases and knowledge compilation is the representation of solution sets in a useful yet succinct data structure. In this paper, we study the representation of the result of join queries (or, equivalently, the set of homomorphisms between two relational structures). We focus on the very general format of $\{\cup, \times\}$-circuits -- also known as d-representations or DNNF circuits -- and aim to find the limits of this approach.
  In prior work, it has been shown that there always exists a $\{\cup, \times\}$-circuits-circuit of size $N^{O(subw)}$ representing the query result, where N is the size of the database and subw the submodular width of the query. If the arity of all relations is bounded by a constant, then subw is linear in the treewidth tw of the query. In this setting, the authors of this paper proved a lower bound of $N^{\Omega(tw^{\varepsilon})}$ on the circuit size (ICALP 2023), where $\varepsilon&gt;0$ depends on the excluded grid theorem.
  Our first main contribution is to improve this lower bound to $N^{\Omega(tw)}$, which is tight up to a constant factor in the exponent. Our second contribution is a $N^{\Omega(subw^{1/4})}$ lower bound on the circuit size for join queries over relations of unbounded arity. Both lower bounds are unconditional lower bounds on the circuit size for well-chosen database instances. Their proofs use a combination of structural (hyper)graph theory with communication complexity in a simple yet novel way. While the second lower bound is asymptotically equivalent to Marx's conditional bound on the decision complexity (JACM 2013), our $N^{\Theta(tw)}$ bound in the bounded-arity setting is tight, while the best conditional bound on the decision complexity is $N^{\Omega(tw/\log tw)}$. Note that, removing this logarithmic factor in the decision setting is a major open problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20438v1</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Berkholz, Harry Vinall-Smeeth</dc:creator>
    </item>
    <item>
      <title>Adaptive Local Clustering over Attributed Graphs</title>
      <link>https://arxiv.org/abs/2503.20488</link>
      <description>arXiv:2503.20488v1 Announce Type: cross 
Abstract: Given a graph $G$ and a seed node $v_s$, the objective of local graph clustering (LGC) is to identify a subgraph $C_s \in G$ (a.k.a. local cluster) surrounding $v_s$ in time roughly linear with the size of $C_s$. This approach yields personalized clusters without needing to access the entire graph, which makes it highly suitable for numerous applications involving large graphs. However, most existing solutions merely rely on the topological connectivity between nodes in $G$, rendering them vulnerable to missing or noisy links that are commonly present in real-world graphs.
  To address this issue, this paper resorts to leveraging the complementary nature of graph topology and node attributes to enhance local clustering quality. To effectively exploit the attribute information, we first formulate the LGC as an estimation of the bidirectional diffusion distribution (BDD), which is specialized for capturing the multi-hop affinity between nodes in the presence of attributes. Furthermore, we propose LACA, an efficient and effective approach for LGC that achieves superb empirical performance on multiple real datasets while maintaining strong locality. The core components of LACA include (i) a fast and theoretically-grounded preprocessing technique for node attributes, (ii) an adaptive algorithm for diffusing any vectors over $G$ with rigorous theoretical guarantees and expedited convergence, and (iii) an effective three-step scheme for BDD approximation. Extensive experiments, comparing 17 competitors on 8 real datasets, show that LACA outperforms all competitors in terms of result quality measured against ground truth local clusters, while also being up to orders of magnitude faster. The code is available at https://github.com/HaoranZ99/alac.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20488v1</guid>
      <category>cs.SI</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoran Zheng, Renchi Yang, Jianliang Xu</dc:creator>
    </item>
    <item>
      <title>Scalable Pattern Matching in Computation Graphs</title>
      <link>https://arxiv.org/abs/2402.13065</link>
      <description>arXiv:2402.13065v2 Announce Type: replace 
Abstract: Graph rewriting is a popular tool for the optimisation and modification of graph expressions in domains such as compilers, machine learning and quantum computing. The underlying data structures are often port graphs - graphs with labels at edge endpoints. A pre-requisite for graph rewriting is the ability to find graph patterns. We propose a new solution to pattern matching in port graphs. Its novelty lies in the use of a pre-computed data structure that makes the pattern matching runtime complexity independent of the number of patterns. This offers a significant advantage over existing solutions for use cases with large sets of small patterns.
  Our approach is particularly well-suited for quantum superoptimisation. We provide an implementation and benchmarks showing that our algorithm offers a 20x speedup over current implementations on a dataset of 10000 real world patterns describing quantum circuits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13065v2</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <category>quant-ph</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.417.5</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 417, 2025, pp. 71-95</arxiv:journal_reference>
      <dc:creator>Luca Mondada (University of Oxford), Pablo Andr\'es-Mart\'inez (Quantinuum Ltd)</dc:creator>
    </item>
    <item>
      <title>A Clique Partitioning-Based Algorithm for Graph Compression</title>
      <link>https://arxiv.org/abs/2502.02477</link>
      <description>arXiv:2502.02477v2 Announce Type: replace 
Abstract: Reducing the running time of graph algorithms is vital for tackling real-world problems such as shortest paths and matching in large-scale graphs, where path information plays a crucial role. This paper addresses this critical challenge of reducing the running time of graph algorithms by proposing a new graph compression algorithm that partitions the graph into bipartite cliques and uses the partition to obtain a compressed graph having a smaller number of edges while preserving the path information. This compressed graph can then be used as input to other graph algorithms for which path information is essential, leading to a significant reduction of their running time, especially for large, dense graphs. The running time of the proposed algorithm is $O(mn^\delta)$, where $0 \leq \delta \leq 1$, which is better than $O(mn^\delta \log^2 n)$, the running time of the best existing clique partitioning-based graph compression algorithm (the Feder-Motwani (\textsf{FM}) algorithm). Our extensive experimental analysis show that our algorithm achieves a compression ratio of up to $26\%$ greater and executes up to 105.18 times faster than the \textsf{FM} algorithm. In addition, on large graphs with up to 1.05 billion edges, it achieves a compression ratio of up to 3.9, reducing the number of edges up to $74.36\%$. Finally, our tests with a matching algorithm on sufficiently large, dense graphs, demonstrate a reduction in the running time of up to 72.83\% when the input is the compressed graph obtained by our algorithm, compared to the case where the input is the original uncompressed graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02477v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akshar Chavan, Sanaz Rabinia, Daniel Grosu, Marco Brocanelli</dc:creator>
    </item>
    <item>
      <title>{\epsilon}-Cost Sharding: Scaling Hypergraph-Based Static Functions and Filters to Trillions of Keys</title>
      <link>https://arxiv.org/abs/2503.18397</link>
      <description>arXiv:2503.18397v2 Announce Type: replace 
Abstract: We describe a simple and yet very scalable implementation of static functions (VFunc) and of static filters (VFilter) based on hypergraphs. We introduce the idea of {\epsilon}-cost sharding, which allows us to build structures that can manage trillions of keys, at the same time increasing memory locality in hypergraph-based constructions. Contrarily to the commonly used HEM sharding method, {\epsilon}-cost sharding does not require to store of additional information, and does not introduce dependencies in the computation chain; its only cost is that of few arithmetical instructions, and of a relative increase {\epsilon} in space usage. We apply {\epsilon}-cost sharding to the classical MWHC construction, but we obtain the best result by combining Dietzfelbinger and Walzer's fuse graphs for large shards with lazy Gaussian elimination for small shards. We obtain large structures with an overhead of 10.5% with respect to the information-theoretical lower bound and with a query time that is a few nanoseconds away from the query time of the non-sharded version, which is the fastest currently available within the same space bounds. Besides comparing our structures with a non-sharded version, we contrast its tradeoffs with bumped ribbon constructions, a space-saving alternative to hypergraph-based static functions and filters, which provide optimum space consumption but slow construction and query time (though construction can be parallelized very efficiently). We build offline a trillion-key filter using commodity hardware in just 60 ns/key.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18397v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastiano Vigna</dc:creator>
    </item>
    <item>
      <title>Long Arithmetic Progressions in Sumsets and Subset Sums: Constructive Proofs and Efficient Witnesses</title>
      <link>https://arxiv.org/abs/2503.19299</link>
      <description>arXiv:2503.19299v2 Announce Type: replace 
Abstract: Existence of long arithmetic progression in sumsets and subset sums has been studied extensively in the field of additive combinatorics. These additive combinatorics results play a central role in the recent progress of fundamental problems in theoretical computer science including Knapsack and Subset Sum. The non-constructiveness of relevant additive combinatorics results affects their application in algorithms. In particular, several additive combinatorics-based algorithms for Subset Sum work only for the decision version of the problem, but not for the search version.
  We provide constructive proofs for finite addition theorems [S\'ark\H{o}zy'89 '94], which are fundamental results in additive combinatorics concerning the existence of long arithmetic progression in sumsets and subset sums. Our constructive proofs yield a near-linear time algorithm that returns an arithmetic progression explicitly, and moreover, for each term in the arithmetic progression, it also returns its representation as the sum of elements in the base set.
  As an application, we obtain an $\tilde{O}(n)$-time algorithm for the search version of dense subset sum now. Another application of our result is Unbounded Subset Sum, where each input integer can be used an infinite number of times. A classic result on the Frobenius problem [Erd\H{o}s and Graham '72] implies that for all $t \geq 2a^2_{\max}/n$, the decision version can be solved trivially in linear time. It remains unknown whether the search version can be solved in the same time. Our result implies that for all $t \geq ca^2_{\max}/n$ for some constant $c$, a solution for Unbounded Subset Sum can be obtained in $O(n \log a_{\max})$ time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19299v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Chen, Yuchen Mao, Guochuan Zhang</dc:creator>
    </item>
    <item>
      <title>Unbalanced Triangle Detection and Enumeration Hardness for Unions of Conjunctive Queries</title>
      <link>https://arxiv.org/abs/2210.11996</link>
      <description>arXiv:2210.11996v5 Announce Type: replace-cross 
Abstract: We study the enumeration of answers to Unions of Conjunctive Queries (UCQs) with optimal time guarantees. More precisely, we wish to identify the queries that can be solved with linear preprocessing time and constant delay. Despite the basic nature of this problem, it was shown only recently that UCQs can be solved within these time bounds if they admit free-connex union extensions, even if all individual CQs in the union are intractable with respect to the same complexity measure. Our goal is to understand whether there exist additional tractable UCQs, not covered by the currently known algorithms. As a first step, we show that some previously unclassified UCQs are hard using the classic 3SUM hypothesis, via a known reduction from 3SUM to triangle listing in graphs. As a second step, we identify a question about a variant of this graph task that is unavoidable if we want to classify all self-join-free UCQs: is it possible to decide the existence of a triangle in a vertex-unbalanced tripartite graph in linear time? We prove that this task is equivalent in hardness to some family of UCQs. Finally, we show a dichotomy for unions of two self-join-free CQs if we assume the answer to this question is negative. In conclusion, this paper pinpoints a computational barrier in the form of a single decision problem that is key to advancing our understanding of the enumeration complexity of many UCQs. Without a breakthrough for unbalanced triangle detection, we have no hope of finding an efficient algorithm for additional unions of two self-join-free CQs. On the other hand, a sufficiently efficient unbalanced triangle detection algorithm can be turned into an efficient algorithm for a family of UCQs currently not known to be tractable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.11996v5</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karl Bringmann, Nofar Carmeli</dc:creator>
    </item>
    <item>
      <title>The Algorithmic Phase Transition of Random Graph Alignment Problem</title>
      <link>https://arxiv.org/abs/2307.06590</link>
      <description>arXiv:2307.06590v2 Announce Type: replace-cross 
Abstract: We study the graph alignment problem over two independent Erd\H{o}s-R\'enyi graphs on $n$ vertices, with edge density $p$ falling into two regimes separated by the critical window around $p_c=\sqrt{\log n/n}$. Our result reveals an algorithmic phase transition for this random optimization problem: polynomial-time approximation schemes exist in the sparse regime, while statistical-computational gap emerges in the dense regime. Additionally, we establish a sharp transition on the performance of online algorithms for this problem when $p$ lies in the dense regime, resulting in a $\sqrt{8/9}$ multiplicative constant factor gap between achievable and optimal solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.06590v2</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <pubDate>Thu, 27 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hang Du, Shuyang Gong, Rundong Huang</dc:creator>
    </item>
  </channel>
</rss>
