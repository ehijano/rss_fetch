<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Dec 2024 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Human-centred test and evaluation of military AI</title>
      <link>https://arxiv.org/abs/2412.01978</link>
      <description>arXiv:2412.01978v1 Announce Type: new 
Abstract: The REAIM 2024 Blueprint for Action states that AI applications in the military domain should be ethical and human-centric and that humans must remain responsible and accountable for their use and effects. Developing rigorous test and evaluation, verification and validation (TEVV) frameworks will contribute to robust oversight mechanisms. TEVV in the development and deployment of AI systems needs to involve human users throughout the lifecycle. Traditional human-centred test and evaluation methods from human factors need to be adapted for deployed AI systems that require ongoing monitoring and evaluation. The language around AI-enabled systems should be shifted to inclusion of the human(s) as a component of the system. Standards and requirements supporting this adjusted definition are needed, as are metrics and means to evaluate them. The need for dialogue between technologists and policymakers on human-centred TEVV will be evergreen, but dialogue needs to be initiated with an objective in mind for it to be productive. Development of TEVV throughout system lifecycle is critical to support this evolution including the issue of human scalability and impact on scale of achievable testing. Communication between technical and non technical communities must be improved to ensure operators and policy-makers understand risk assumed by system use and to better inform research and development. Test and evaluation in support of responsible AI deployment must include the effect of the human to reflect operationally realised system performance. Means of communicating the results of TEVV to those using and making decisions regarding the use of AI based systems will be key in informing risk based decisions regarding use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01978v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Helmer, Michael Boardman, S. Kate Conroy, Adam J. Hepworth, Manoj Harjani</dc:creator>
    </item>
    <item>
      <title>ChatCollab: Exploring Collaboration Between Humans and AI Agents in Software Teams</title>
      <link>https://arxiv.org/abs/2412.01992</link>
      <description>arXiv:2412.01992v1 Announce Type: new 
Abstract: We explore the potential for productive team-based collaboration between humans and Artificial Intelligence (AI) by presenting and conducting initial tests with a general framework that enables multiple human and AI agents to work together as peers. ChatCollab's novel architecture allows agents - human or AI - to join collaborations in any role, autonomously engage in tasks and communication within Slack, and remain agnostic to whether their collaborators are human or AI. Using software engineering as a case study, we find that our AI agents successfully identify their roles and responsibilities, coordinate with other agents, and await requested inputs or deliverables before proceeding. In relation to three prior multi-agent AI systems for software development, we find ChatCollab AI agents produce comparable or better software in an interactive game development task. We also propose an automated method for analyzing collaboration dynamics that effectively identifies behavioral characteristics of agents with distinct roles, allowing us to quantitatively compare collaboration dynamics in a range of experimental conditions. For example, in comparing ChatCollab AI agents, we find that an AI CEO agent generally provides suggestions 2-4 times more often than an AI product manager or AI developer, suggesting agents within ChatCollab can meaningfully adopt differentiated collaborative roles. Our code and data can be found at: https://github.com/ChatCollab.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01992v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Klieger, Charis Charitsis, Miroslav Suzara, Sierra Wang, Nick Haber, John C. Mitchell</dc:creator>
    </item>
    <item>
      <title>Persian Version of Wayfinding Questionnaire</title>
      <link>https://arxiv.org/abs/2412.02143</link>
      <description>arXiv:2412.02143v1 Announce Type: new 
Abstract: Spatial navigation ability is essential for daily functioning, and the Wayfinding Questionnaire (WQ) is a validated self-report tool assessing this ability through 22 items across three subscales: Navigation and Orientation (11 items), Distance Estimation (3 items), and Spatial Anxiety (8 items). This study introduces the Persian translation of the WQ, adapted for Persian-speaking populations using a rigorous forward-backward translation, cognitive debriefing, and cultural adaptation process to ensure alignment with the original tool's reliability and validity. The Persian WQ provides a complete assessment of spatial navigation skills and identifies potential navigation challenges. Furthermore, the Persian WQ serves as a valuable resource for future research exploring spatial navigation and memory in diverse populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02143v1</guid>
      <category>cs.HC</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mobina Zibandehpoor, Mehdi Delrobaei</dc:creator>
    </item>
    <item>
      <title>Dynamic Prompt Middleware: Contextual Prompt Refinement Controls for Comprehension Tasks</title>
      <link>https://arxiv.org/abs/2412.02357</link>
      <description>arXiv:2412.02357v1 Announce Type: new 
Abstract: Effective prompting of generative AI is challenging for many users, particularly in expressing context for comprehension tasks such as explaining spreadsheet formulas, Python code, and text passages. Prompt middleware aims to address this barrier by assisting in prompt construction, but barriers remain for users in expressing adequate control so that they can receive AI-responses that match their preferences.
  We conduct a formative survey (n=38) investigating user needs for control over AI-generated explanations in comprehension tasks, which uncovers a trade-off between standardized but predictable support for prompting, and adaptive but unpredictable support tailored to the user and task. To explore this trade-off, we implement two prompt middleware approaches: Dynamic Prompt Refinement Control (Dynamic PRC) and Static Prompt Refinement Control (Static PRC). The Dynamic PRC approach generates context-specific UI elements that provide prompt refinements based on the user's prompt and user needs from the AI, while the Static PRC approach offers a preset list of generally applicable refinements.
  We evaluate these two approaches with a controlled user study (n=16) to assess the impact of these approaches on user control of AI responses for crafting better explanations. Results show a preference for the Dynamic PRC approach as it afforded more control, lowered barriers to providing context, and encouraged exploration and reflection of the tasks, but that reasoning about the effects of different generated controls on the final output remains challenging. Drawing on participant feedback, we discuss design implications for future Dynamic PRC systems that enhance user control of AI responses. Our findings suggest that dynamic prompt middleware can improve the user experience of generative AI workflows by affording greater control and guide users to a better AI response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02357v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ian Drosos, Jack Williams, Advait Sarkar, Nicholas Wilson</dc:creator>
    </item>
    <item>
      <title>Leveraging User Experience and Learning Analytics for Enhanced Student Well-being</title>
      <link>https://arxiv.org/abs/2412.02457</link>
      <description>arXiv:2412.02457v1 Announce Type: new 
Abstract: This study explores the design and preliminary evaluation of the "Well-being Journey" (WB Journey), a digital tool aimed at enhancing student well-being within educational environments through tailored recommendations for students. The study examines the WB Journey prototype's user experience and its effectiveness in meeting learning analytics goals related to student preferences. To achieve both goals, we employ a mixed-methods approach, combining quantitative data from the User Experience Questionnaire (UEQ) and the Student Expectations of Learning Analytics Questionnaire (SELAQ) with qualitative feedback from a student discussion. Conducted among 25 students from an engineering school in a Spanish University, the study's data collection involved a 120-minute workshop. The findings suggest opportunities for enhancing the prototype, highlighting the importance of aligning similar digital tools with student needs and preferences for a supportive learning environment, which can be achieved by leveraging tools such as UEQ and SELAQ.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02457v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Khadija El Aadmi-Laamech, Patricia Santos, Davinia Hern\'andez-Leo</dc:creator>
    </item>
    <item>
      <title>Generative AI as a Tool for Enhancing Reflective Learning in Students</title>
      <link>https://arxiv.org/abs/2412.02603</link>
      <description>arXiv:2412.02603v1 Announce Type: new 
Abstract: Reflection is widely recognized as a cornerstone of student development, fostering critical thinking, self-regulation, and deep conceptual understanding. Traditionally, reflective skills are cultivated through structured feedback, mentorship, and guided self-assessment. However, these approaches often face challenges such as limited scalability, difficulties in delivering individualized feedback, and a shortage of instructors proficient in facilitating meaningful reflection. This study pioneers the exploration of generative AI, specifically large language models (LLMs), as an innovative solution to these limitations. By leveraging the capacity of LLMs to provide personalized, context-sensitive feedback at scale, this research examines their potential to serve as effective facilitators of reflective exercises, maintaining the depth of engagement and promoting critical thinking. Through an in-depth analysis of prompt engineering strategies and the efficacy of LLMs in simulated multi-turn dialogues between tutors and students, this study demonstrates that, with pedagogically aligned prompts, LLMs can function as accessible and adaptive tools for automating reflective guidance and objectively assessing the performance of both tutors and students. This work also contributes to the evolving understanding of AI's role in reflective pedagogy and highlights new possibilities for AI-driven intelligent tutoring systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02603v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bo Yuan, Jiazi Hu</dc:creator>
    </item>
    <item>
      <title>SEMANTIC SEE-THROUGH GOGGLES: Wearing Linguistic Virtual Reality in (Artificial) Intelligence</title>
      <link>https://arxiv.org/abs/2412.02641</link>
      <description>arXiv:2412.02641v1 Announce Type: new 
Abstract: When language is utilized as a medium to store and communicate sensory information, there arises a kind of radical virtual reality, namely "the realities that are reduced into the same sentence are virtual/equivalent." In the current era, in which artificial intelligence engages in the linguistic mediation of sensory information, it is imperative to re-examine the various issues pertaining to this potential VR, particularly in relation to bias and (dis)communication. Semantic See-through Goggles represent an experimental framework for glasses through which the view is fully verbalized and re-depicted into the wearer's view. The participants wear the goggles equipped with a camera and head-mounted display (HMD). In real-time, the image captured by the camera is converted by the AI into a single line of text, which is then transformed into an image and presented to the user's eyes. This process enables users to perceive and interact with the real physical world through this redrawn view. We constructed a prototype of these goggles, examined their fundamental characteristics, and then conducted a qualitative analysis of the wearer's experience. This project investigates a methodology for subjectively capturing the situation in which AI serves as a proxy for our perception of the world. At the same time, It also attempts to appropriate some of the energy of today's debate over artificial intelligence for a classical inquiry around the fact that "intelligence can only see the world under meaning."</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02641v1</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Goki Muramoto, Yuri Yasui, Hirosuke Asahi</dc:creator>
    </item>
    <item>
      <title>The Impact of Featuring Comments in Online Discussions</title>
      <link>https://arxiv.org/abs/2412.02369</link>
      <description>arXiv:2412.02369v1 Announce Type: cross 
Abstract: A widespread moderation strategy by online news platforms is to feature what the platform deems high quality comments, usually called editor picks or featured comments. In this paper, we compare online discussions of news articles in which certain comments are featured, versus discussions in which no comments are featured. We measure the impact of featuring comments on the discussion, by estimating and comparing the quality of discussions from the perspective of the user base and the platform itself. Our analysis shows that the impact on discussion quality is limited. However, we do observe an increase in discussion activity after the first comments are featured by moderators, suggesting that the moderation strategy might be used to increase user engagement and to postpone the natural decline in user activity over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02369v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cedric Waterschoot, Ernst van den Hemel, Antal van den Bosch</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Survey on Dynamic Software Updating Techniques in IoTs</title>
      <link>https://arxiv.org/abs/2412.02450</link>
      <description>arXiv:2412.02450v1 Announce Type: cross 
Abstract: This comprehensive survey paper provides an in-depth analysis of Dynamic Software Updating (DSU) techniques in the Internet of Things (IoT). This study critically examines eight significant research papers that employ diverse methodologies to address the challenges of DSU in IoT devices. The primary objectives include comparative analysis to identify the application domains of DSU tools, classification of program alterations accommodated by these systems, evaluation of the advantages and disadvantages of various DSU tools, and identification of potential paths for future research. This paper emphasizes the critical function of DSU in improving energy efficiency, extending operational durability, and bolstering security within IoT environments that demand high availability, including applications in smart cities and connected vehicles. It delves into the basic approaches and mechanisms of DSU, ranging from traditional methods to advanced practices like Over-the-Air updates and container-based solutions. This survey highlights the evolving nature of DSU techniques, balancing operational efficiency, security, and adaptability amidst the complexities of diverse IoT applications. Through this exploration, the paper aims to guide future developments in DSU strategies, enhancing IoT devices' resilience, functionality, and sustainability in a connected world. The insights from this survey are pivotal for researchers, practitioners, and policymakers in shaping effective DSU strategies to meet the growing needs of the IoT ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02450v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <category>cs.NI</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Madhav Neupane</dc:creator>
    </item>
    <item>
      <title>Towards a Quality Approach to Hierarchical Color Maps</title>
      <link>https://arxiv.org/abs/2407.08287</link>
      <description>arXiv:2407.08287v3 Announce Type: replace 
Abstract: To improve the perception of hierarchical structures in data sets, several color map generation algorithms have been proposed to take this structure into account. But the design of hierarchical color maps elicits different requirements to those of color maps for tabular data. Within this paper, we make an initial effort to put design rules from the color map literature into the context of hierarchical color maps. We investigate the impact of several design decisions and provide recommendations for various analysis scenarios. Thus, we lay the foundation for objective quality criteria to evaluate hierarchical color maps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08287v3</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/VIS55277.2024.00052</arxiv:DOI>
      <arxiv:journal_reference>2024 IEEE Visualization and Visual Analytics (VIS), St. Pete Beach, FL, USA, 2024, pp. 221-225</arxiv:journal_reference>
      <dc:creator>Tobias Mertz (Fraunhofer IGD), J\"orn Kohlhammer (Fraunhofer IGD, TU Darmstadt)</dc:creator>
    </item>
    <item>
      <title>What Guides Our Choices? Modeling Developers' Trust and Behavioral Intentions Towards GenAI</title>
      <link>https://arxiv.org/abs/2409.04099</link>
      <description>arXiv:2409.04099v2 Announce Type: replace 
Abstract: Generative AI (genAI) tools, such as ChatGPT or Copilot, are advertised to improve developer productivity and are being integrated into software development. However, misaligned trust, skepticism, and usability concerns can impede the adoption of such tools. Research also indicates that AI can be exclusionary, failing to support diverse users adequately. One such aspect of diversity is cognitive diversity -- variations in users' cognitive styles -- that leads to divergence in perspectives and interaction styles. When an individual's cognitive style is unsupported, it creates barriers to technology adoption. Therefore, to understand how to effectively integrate genAI tools into software development, it is first important to model what factors affect developers' trust and intentions to adopt genAI tools in practice?
  We developed a theoretically grounded statistical model to (1) identify factors that influence developers' trust in genAI tools and (2) examine the relationship between developers' trust, cognitive styles, and their intentions to use these tools in their work. We surveyed software developers (N=238) at two major global tech organizations: GitHub Inc. and Microsoft; and employed Partial Least Squares-Structural Equation Modeling (PLS-SEM) to evaluate our model. Our findings reveal that genAI's system/output quality, functional value, and goal maintenance significantly influence developers' trust in these tools. Furthermore, developers' trust and cognitive styles influence their intentions to use these tools in their work. We offer practical suggestions for designing genAI tools for effective use and inclusive user experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04099v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>47th IEEE/ACM International Conference on Software Engineering (ICSE 2025)</arxiv:journal_reference>
      <dc:creator>Rudrajit Choudhuri, Bianca Trinkenreich, Rahul Pandita, Eirini Kalliamvakou, Igor Steinmacher, Marco Gerosa, Christopher Sanchez, Anita Sarma</dc:creator>
    </item>
    <item>
      <title>DuetML: Human-LLM Collaborative Machine Learning Framework for Non-Expert Users</title>
      <link>https://arxiv.org/abs/2411.18908</link>
      <description>arXiv:2411.18908v2 Announce Type: replace 
Abstract: Machine learning (ML) models have significantly impacted various domains in our everyday lives. While large language models (LLMs) offer intuitive interfaces and versatility, task-specific ML models remain valuable for their efficiency and focused performance in specialized tasks. However, developing these models requires technical expertise, making it particularly challenging for non-expert users to customize them for their unique needs. Although interactive machine learning (IML) aims to democratize ML development through user-friendly interfaces, users struggle to translate their requirements into appropriate ML tasks. We propose human-LLM collaborative ML as a new paradigm bridging human-driven IML and machine-driven LLM approaches. To realize this vision, we introduce DuetML, a framework that integrates multimodal LLMs (MLLMs) as interactive agents collaborating with users throughout the ML process. Our system carefully balances MLLM capabilities with user agency by implementing both reactive and proactive interactions between users and MLLM agents. Through a comparative user study, we demonstrate that DuetML enables non-expert users to define training data that better aligns with target tasks without increasing cognitive load, while offering opportunities for deeper engagement with ML task formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18908v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wataru Kawabe, Yusuke Sugano</dc:creator>
    </item>
    <item>
      <title>Towards Understanding the Impact of Guidance in Data Visualization Systems for Domain Experts</title>
      <link>https://arxiv.org/abs/2412.01024</link>
      <description>arXiv:2412.01024v2 Announce Type: replace 
Abstract: Guided data visualization systems are highly useful for domain experts to highlight important trends in their large-scale and complex datasets. However, more work is needed to understand the impact of guidance on interpreting data visualizations as well as on the resulting use of visualizations when communicating insights. We conducted two user studies with domain experts and found that experts benefit from a guided coarse-to-fine structure when using data visualization systems, as this is the same structure in which they communicate findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01024v2</guid>
      <category>cs.HC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sherry Qiu, Holly Rushmeier, Kim R. M. Blenman</dc:creator>
    </item>
    <item>
      <title>ProACT: An Augmented Reality Testbed for Intelligent Prosthetic Arms</title>
      <link>https://arxiv.org/abs/2407.05025</link>
      <description>arXiv:2407.05025v2 Announce Type: replace-cross 
Abstract: Upper-limb amputees face tremendous difficulty in operating dexterous powered prostheses. Previous work has shown that aspects of prosthetic hand, wrist, or elbow control can be improved through "intelligent" control, by combining movement-based or gaze-based intent estimation with low-level robotic autonomy. However, no such solutions exist for whole-arm control. Moreover, hardware platforms for advanced prosthetic control are expensive, and existing simulation platforms are not well-designed for integration with robotics software frameworks. We present the Prosthetic Arm Control Testbed (ProACT), a platform for evaluating intelligent control methods for prosthetic arms in an immersive (Augmented Reality) simulation setting. We demonstrate the use of ProACT through preliminary studies, with non-amputee participants performing an adapted Box-and-Blocks task with and without intent estimation. We further discuss how our observations may inform the design of prosthesis control methods, as well as the design of future studies using the platform. To the best of our knowledge, this constitutes the first study of semi-autonomous control for complex whole-arm prostheses, the first study including sequential task modeling in the context of wearable prosthetic arms, and the first testbed of its kind. Towards the goal of supporting future research in intelligent prosthetics, the system is built upon on existing open-source frameworks for robotics, and is available at https://arm.stanford.edu/proact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05025v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shivani Guptasarma, Monroe D. Kennedy III</dc:creator>
    </item>
    <item>
      <title>Breaking Down the Barriers: Investigating Non-Expert User Experiences in Robotic Teleoperation in UK and Japan</title>
      <link>https://arxiv.org/abs/2410.18727</link>
      <description>arXiv:2410.18727v2 Announce Type: replace-cross 
Abstract: Robots are being created each year with the goal of integrating them into our daily lives. As such, there is an interest in research in evaluating the trust of humans toward robots. In addition, teleoperating robotic arms can be challenging for non-experts. To reduce the strain put on the user, we created TELESIM, a modular and plug-and-play framework that enables direct teleoperation of any robotic arm using a digital twin as the interface between users and the robotic system. We evaluated our framework using a user survey with three robots and control methods and recorded the user's workload and performance at completing a tower stacking task. However, an analysis of the strain on the user and their ability to trust robots was omitted. This paper addresses these omissions by presenting the additional results of our user survey of 37 participants carried out in United Kingdom. In addition, we present the results of an additional user survey, under similar conditions performed in Japan, with the goal of addressing the limitations of our previous approach, by interfacing a VR controller with a UR5e. Our experimental results show that the UR5e has more towers built. Additionally, the UR5e gives the least amount of cognitive stress, while the combination of Senseglove and UR3 provides the user with the highest physical strain and causes the user to feel more frustrated. Finally, the Japanese participants seem more trusting of robots than the British participants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18727v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florent P Audonnet, Andrew Hamilton, Yakiyasu Domae, Ixchel G Ramirez-Alpizar, Gerardo Aragon-Camarasa</dc:creator>
    </item>
    <item>
      <title>Large Language Model-Brained GUI Agents: A Survey</title>
      <link>https://arxiv.org/abs/2411.18279</link>
      <description>arXiv:2411.18279v3 Announce Type: replace-cross 
Abstract: GUIs have long been central to human-computer interaction, providing an intuitive and visually-driven way to access and interact with digital systems. The advent of LLMs, particularly multimodal models, has ushered in a new era of GUI automation. They have demonstrated exceptional capabilities in natural language understanding, code generation, and visual processing. This has paved the way for a new generation of LLM-brained GUI agents capable of interpreting complex GUI elements and autonomously executing actions based on natural language instructions. These agents represent a paradigm shift, enabling users to perform intricate, multi-step tasks through simple conversational commands. Their applications span across web navigation, mobile app interactions, and desktop automation, offering a transformative user experience that revolutionizes how individuals interact with software. This emerging field is rapidly advancing, with significant progress in both research and industry.
  To provide a structured understanding of this trend, this paper presents a comprehensive survey of LLM-brained GUI agents, exploring their historical evolution, core components, and advanced techniques. We address research questions such as existing GUI agent frameworks, the collection and utilization of data for training specialized GUI agents, the development of large action models tailored for GUI tasks, and the evaluation metrics and benchmarks necessary to assess their effectiveness. Additionally, we examine emerging applications powered by these agents. Through a detailed analysis, this survey identifies key research gaps and outlines a roadmap for future advancements in the field. By consolidating foundational knowledge and state-of-the-art developments, this work aims to guide both researchers and practitioners in overcoming challenges and unlocking the full potential of LLM-brained GUI agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18279v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, Qi Zhang</dc:creator>
    </item>
    <item>
      <title>Advancing Speech Language Models by Scaling Supervised Fine-Tuning with Over 60,000 Hours of Synthetic Speech Dialogue Data</title>
      <link>https://arxiv.org/abs/2412.01078</link>
      <description>arXiv:2412.01078v2 Announce Type: replace-cross 
Abstract: The GPT-4o represents a significant milestone in enabling real-time interaction with large language models (LLMs) through speech, its remarkable low latency and high fluency not only capture attention but also stimulate research interest in the field. This real-time speech interaction is particularly valuable in scenarios requiring rapid feedback and immediate responses, dramatically enhancing user experience. However, there is a notable lack of research focused on real-time large speech language models, particularly for Chinese. In this work, we present KE-Omni, a seamless large speech language model built upon Ke-SpeechChat, a large-scale high-quality synthetic speech interaction dataset consisting of 7 million Chinese and English conversations, featuring 42,002 speakers, and totaling over 60,000 hours, This contributes significantly to the advancement of research and development in this field. The demos can be accessed at \url{https://huggingface.co/spaces/KE-Team/KE-Omni}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01078v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuaijiang Zhao, Tingwei Guo, Bajian Xiang, Tongtang Wan, Qiang Niu, Wei Zou, Xiangang Li</dc:creator>
    </item>
  </channel>
</rss>
