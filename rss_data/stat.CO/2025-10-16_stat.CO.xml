<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Oct 2025 01:51:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Escaping Neal's Funnel: a multi-stage sampling method for hierarchical models</title>
      <link>https://arxiv.org/abs/2510.12917</link>
      <description>arXiv:2510.12917v1 Announce Type: cross 
Abstract: Neal's funnel refers to an exponential tapering in probability densities common to Bayesian hierarchical models. Usual sampling methods, such as Markov Chain Monte Carlo, struggle to efficiently sample the funnel. Reparameterizing the model or analytically marginalizing local parameters are common techniques to remedy sampling pathologies in distributions exhibiting Neal's funnel. In this paper, we show that the challenges of Neal's funnel can be avoided by performing the hierarchical analysis, well, hierarchically. That is, instead of sampling all parameters of the hierarchical model jointly, we break the sampling into multiple stages. The first stage samples a generalized (higher-dimensional) hierarchical model which is parameterized to lessen the sharpness of the funnel. The next stage samples from the estimated density of the first stage, but under a constraint which restricts the sampling to recover the marginal distributions on the hyper-parameters of the original (lower-dimensional) hierarchical model. A normalizing flow can be used to represent the distribution from the first stage, such that it can easily be sampled from for the second stage of the analysis. This technique is useful when effective reparameterizations are computationally expensive to calculate, or a generalized hierarchical model already exists from which it is easy to sample.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12917v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aiden Gundersen, Neil J. Cornish</dc:creator>
    </item>
    <item>
      <title>Scalable Bayesian inference for high-dimensional mixed-type multivariate spatial data</title>
      <link>https://arxiv.org/abs/2510.13233</link>
      <description>arXiv:2510.13233v1 Announce Type: cross 
Abstract: Spatial generalized linear mixed-effects methods are popularly used to model spatially indexed univariate responses. However, with modern technology, it is common to observe vector-valued mixed-type responses, e.g., a combination of binary, count, or continuous types, at each location. Methods that allow joint modeling of such mixed-type multivariate spatial responses are rare. Using latent multivariate Gaussian processes (GPs), we present a class of Bayesian spatial methods that can be employed for any combination of exponential family responses. Since multivariate GP-based methods can suffer from computational bottlenecks when the number of spatial locations is high, we further employ a computationally efficient Vecchia approximation for fast posterior inference and prediction. Key theoretical properties of the proposed model, such as identifiability and the structure of the induced covariance, are established. Our approach employs a Markov chain Monte Carlo-based inference method that utilizes elliptical slice sampling in a blocked Metropolis-within-Gibbs sampling framework. We illustrate the efficacy of the proposed method through simulation studies and a real-data application on joint modeling of wildfire counts and burnt areas across the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13233v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arghya Mukherjee, Arnab Hazra, Dootika Vats</dc:creator>
    </item>
    <item>
      <title>Leveraging Nested MLMC for Sequential Neural Posterior Estimation with Intractable Likelihoods</title>
      <link>https://arxiv.org/abs/2401.16776</link>
      <description>arXiv:2401.16776v3 Announce Type: replace 
Abstract: There is a growing interest in studying sequential neural posterior estimation (SNPE) techniques due to their advantages for simulation-based models with intractable likelihoods. The methods aim to learn the posterior from adaptively proposed simulations using neural network-based conditional density estimators. As an SNPE technique, the automatic posterior transformation (APT) method proposed by Greenberg et al. (2019) performs well and scales to high-dimensional data. However, the APT method requires computing the expectation of the logarithm of an intractable normalizing constant, i.e., a nested expectation. Although atomic proposals were used to render an analytical normalizing constant, it remains challenging to analyze the convergence of learning. In this paper, we reformulate APT as a nested estimation problem. Building on this, we construct several multilevel Monte Carlo (MLMC) estimators for the loss function and its gradients to accommodate different scenarios, including two unbiased estimators, and a biased estimator that trades a small bias for reduced variance and controlled runtime and memory usage. We also provide convergence results of stochastic gradient descent to quantify the interaction of the bias and variance of the gradient estimator. Numerical experiments for approximating complex posteriors with multimodality in moderate dimensions are provided to examine the effectiveness of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16776v3</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiliang Yang, Yifei Xiong, Zhijian He</dc:creator>
    </item>
    <item>
      <title>A coupling-based approach to f-divergences diagnostics for Markov chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2510.07559</link>
      <description>arXiv:2510.07559v2 Announce Type: replace 
Abstract: A long-standing gap exists between the theoretical analysis of Markov chain Monte Carlo convergence, which is often based on statistical divergences, and the diagnostics used in practice. We introduce the first general convergence diagnostics for Markov chain Monte Carlo based on any f-divergence, allowing users to directly monitor, among others, the Kullback--Leibler and the $\chi^2$ divergences as well as the Hellinger and the total variation distances. Our first key contribution is a coupling-based `weight harmonization' scheme that produces a direct, computable, and consistent weighting of interacting Markov chains with respect to their target distribution. The second key contribution is to show how such consistent weightings of empirical measures can be used to provide upper bounds to f-divergences in general. We prove that these bounds are guaranteed to tighten over time and converge to zero as the chains approach stationarity, providing a concrete diagnostic. Numerical experiments demonstrate that our method is a practical and competitive diagnostic tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07559v2</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Corenflos, Hai-Dang Dau</dc:creator>
    </item>
    <item>
      <title>Conditional Distribution Compression via the Kernel Conditional Mean Embedding</title>
      <link>https://arxiv.org/abs/2504.10139</link>
      <description>arXiv:2504.10139v3 Announce Type: replace-cross 
Abstract: Existing distribution compression methods, like Kernel Herding (KH), were originally developed for unlabelled data. However, no existing approach directly compresses the conditional distribution of labelled data. To address this gap, we first introduce the Average Maximum Conditional Mean Discrepancy (AMCMD), a natural metric for comparing conditional distributions. We then derive a consistent estimator for the AMCMD and establish its rate of convergence. Next, we make a key observation: in the context of distribution compression, the cost of constructing a compressed set targeting the AMCMD can be reduced from $\mathcal{O}(n^3)$ to $\mathcal{O}(n)$. Building on this, we extend the idea of KH to develop Average Conditional Kernel Herding (ACKH), a linear-time greedy algorithm that constructs a compressed set targeting the AMCMD. To better understand the advantages of directly compressing the conditional distribution rather than doing so via the joint distribution, we introduce Joint Kernel Herding (JKH), a straightforward adaptation of KH designed to compress the joint distribution of labelled data. While herding methods provide a simple and interpretable selection process, they rely on a greedy heuristic. To explore alternative optimisation strategies, we propose Joint Kernel Inducing Points (JKIP) and Average Conditional Kernel Inducing Points (ACKIP), which jointly optimise the compressed set while maintaining linear complexity. Experiments show that directly preserving conditional distributions with ACKIP outperforms both joint distribution compression (via JKH and JKIP) and the greedy selection used in ACKH. Moreover, we see that JKIP consistently outperforms JKH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10139v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominic Broadbent, Nick Whiteley, Robert Allison, Tom Lovett</dc:creator>
    </item>
    <item>
      <title>Bayesian Double Descent</title>
      <link>https://arxiv.org/abs/2507.07338</link>
      <description>arXiv:2507.07338v3 Announce Type: replace-cross 
Abstract: Double descent is a phenomenon of over-parameterized statistical models such as deep neural networks which have a re-descending property in their risk function. As the complexity of the model increases, risk exhibits a U-shaped region due to the traditional bias-variance trade-off, then as the number of parameters equals the number of observations and the model becomes one of interpolation where the risk can be unbounded and finally, in the over-parameterized region, it re-descends -- the double descent effect. Our goal is to show that this has a natural Bayesian interpretation. We also show that this is not in conflict with the traditional Occam's razor -- simpler models are preferred to complex ones, all else being equal. Our theoretical foundations use Bayesian model selection, the Dickey-Savage density ratio, and connect generalized ridge regression and global-local shrinkage methods with double descent. We illustrate our approach for high dimensional neural networks and provide detailed treatments of infinite Gaussian means models and non-parametric regression. Finally, we conclude with directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07338v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Nick Polson, Vadim Sokolov</dc:creator>
    </item>
    <item>
      <title>Examining the Interface Design of Tidyverse</title>
      <link>https://arxiv.org/abs/2510.10382</link>
      <description>arXiv:2510.10382v3 Announce Type: replace-cross 
Abstract: The tidyverse is a popular meta-package comprising several core R packages to aid in various data science tasks, including data import, manipulation and visualisation. Although functionalities offered by the tidyverse can generally be replicated using other packages, its widespread adoption in both teaching and practice indicates there are factors contributing to its preference, despite some debate over its usage. This suggests that particular aspects, such as interface design, may play a significant role in its selection. Examining the interface design can potentially reveal aspects that aid the design process for developers. While Tidyverse has been lauded for adopting a user-centered design, arguably some elements of the design focus on the work domain instead of the end-user. We examine the Tidyverse interface design via the lens of human computer interaction, with an emphasis on data visualisation and data wrangling, to identify factors that might serve as a model for developers designing their packages. We recommend that developers adopt an iterative design that is informed by user feedback, analysis and complete coverage of the work domain, and ensure perceptual visibility of system constraints and relationships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10382v3</guid>
      <category>stat.OT</category>
      <category>stat.CO</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emi Tanaka</dc:creator>
    </item>
  </channel>
</rss>
