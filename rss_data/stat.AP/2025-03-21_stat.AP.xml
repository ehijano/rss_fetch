<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Mar 2025 04:00:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Capacity drop induced by stability modifications in stochastic dynamic systems</title>
      <link>https://arxiv.org/abs/2503.15717</link>
      <description>arXiv:2503.15717v1 Announce Type: new 
Abstract: It is well-known that the fundamental diagram in a realistic traffic system is featured by capacity drop. From a mesoscopic approach, we demonstrate that such a phenomenon is linked to the unique properties of stochastic noise, which, when considered a specific perturbation, may counterintuitively enhance the stability of the originally deterministic system. We argue that the system achieves its asymptotic behavior through a trade-off between the relaxation towards the stable attractors of the underlying deterministic system and the stochastic perturbations that non-trivially affect such a process. Utilizing It\^o calculus, the present study analyzes the threshold of the relevant perturbations that appropriately give rise to such a physical picture. In particular, we devise a scenario for which the stochastic noise is introduced in a minimized fashion to a deterministic fold model, which is known to reproduce the main feature of the fundamental diagram successfully. Our results show that the sudden increase in vehicle flow variance and the onset of capacity drop are intrinsically triggered by stochastic noise. Somewhat counterintuitively, we point out that the prolongation of the free flow state's asymptotic stability that forms the inverse lambda shape in the fundamental diagram can also be attributed to the random process, which typically destabilizes the underlying system. The asymptotic behaviors of the system are examined, corroborating well with the main features observed in the experimental data. The implications of the present findings are also addressed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15717v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mariana Pereira de Melo, Leon Alexander Valencia, Wei-Liang Qian</dc:creator>
    </item>
    <item>
      <title>A note on goodness of fit testing for the Poisson distribution</title>
      <link>https://arxiv.org/abs/2503.15757</link>
      <description>arXiv:2503.15757v1 Announce Type: new 
Abstract: Since its introduction in 1950, Fisher's dispersion test has become a standard means of deciding whether or not count data follow the Poisson distribution. The test is based on a characteristic property of the Poisson distribution, and discriminates well between the Poisson and the natural alternative hypotheses of binomial and negative binomial distributions.
  While the test is commonly used to test for general deviations from Poissonity, its performance against more general alternatives has not been widely investigated. This paper presents realistic alternative hypotheses for which general goodness of fit tests perform much better than the Fisher dispersion test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15757v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erekle Khurodze, Leigh A Roberts</dc:creator>
    </item>
    <item>
      <title>Temporal Point Process Modeling of Aggressive Behavior Onset in Psychiatric Inpatient Youths with Autism</title>
      <link>https://arxiv.org/abs/2503.15821</link>
      <description>arXiv:2503.15821v1 Announce Type: new 
Abstract: Aggressive behavior, including aggression towards others and self-injury, occurs in up to 80% of children and adolescents with autism, making it a leading cause of behavioral health referrals and a major driver of healthcare costs. Predicting when autistic youth will exhibit aggression is challenging due to their communication difficulties. Many are minimally verbal or have poor emotional insight. Recent advances in Machine Learning and wearable biosensing enable short-term aggression predictions within a limited future window (typically one to three minutes). However, existing models do not estimate aggression probability within longer future windows nor the expected number of aggression onsets over such a period. To address these limitations, we employ Temporal Point Processes (TPPs) to model the generative process of aggressive behavior onsets in inpatient youths with autism. We hypothesize that aggressive behavior onsets follow a self-exciting process driven by short-term history, making them well-suited for Hawkes Point Process modeling. We establish a benchmark and demonstrate through Goodness-of-Fit statistics and predictive metrics that TPPs perform well modeling aggressive behavior onsets in inpatient youths with autism. Additionally, we gain insights into the onset generative process, like the branching factor near criticality, and suggest TPPs may enhance future clinical decision-making and preemptive interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15821v1</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Potter, Michael Everett, Ashutosh Singh, Georgios Stratis, Yuna Watanabe, Ahmet Demirkaya, Deniz Erdogmus, Tales Imbiriba, Matthew S. Goodwin</dc:creator>
    </item>
    <item>
      <title>Copula-based spatio-temporal modeling of air pollutant data incorporating covariate dependencies</title>
      <link>https://arxiv.org/abs/2503.15935</link>
      <description>arXiv:2503.15935v1 Announce Type: new 
Abstract: Elevated levels of PM10 are known to cause severe respiratory and cardiovascular diseases, and, in extreme cases, cancer and mortality. Despite various reduction policies implemented across different sectors, PM10 concentrations in South Korea continue to exceed the annual recommended limits set by the World Health Organization. Spatio-temporal PM10 concentrations may exhibit both spatial and temporal dependencies. Additionally, interactions between PM10 and environmental factors can further influence the variability in PM10. Therefore, this study proposes a method that incorporates the spatio-temporal neighbors of covariates alongside those of PM10 by adopting an approach that explains spatio-temporal interactions through spatio-temporal neighbors. Vine copulas are used to integrate the pairwise dependence structures between a given location and its surrounding spatio-temporal neighbors. We applied the model to weekly average PM10 data from South Korea in 2019, using PM2.5 and CO as covariates. Given that all three variables exhibited skewness, we assumed the Gumbel and Generalized Extreme Value distributions as marginal distributions. The proposed model outperformed a traditional Bayesian spatio-temporal model, a kriging method, and an alternative copula-based approach, particularly in predicting the top 5% of extreme values, by effectively capturing tail dependencies crucial for extreme value analysis. This study highlights the importance of utilizing vine copulas to effectively model diverse dependency structures in spatio-temporal data while simultaneously accommodating spatial and temporal dimensions, including spatio-temporal dependencies among covariates. The results underscore the broader applicability of the proposed approach to other fields where complex dependency structures are present.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15935v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soyun Jeon, Jungsoon Choi</dc:creator>
    </item>
    <item>
      <title>Estimation of Piecewise Continuous Regression Function in Finite Dimension using Oblique Regression Tree with Applications in Image Denoising</title>
      <link>https://arxiv.org/abs/2503.16007</link>
      <description>arXiv:2503.16007v1 Announce Type: new 
Abstract: Decision trees are one of the most widely used nonparametric method for regression and classification. In existing literature, decision tree-based methods have been used for estimating continuous functions or piecewise-constant functions. However, they are not flexible enough to estimate the complex shapes of jump location curves (JLCs) in two dimensional regression functions. In this article, we explore the Oblique-axis Regression Tree (ORT) and propose a method to efficiently estimate piece-wise continuous functions in a general finite dimension with fixed design points. The central idea involves clustering the local pixel intensities by recursive tree partitioning, and using the local leaf-only averaging for estimation of the regression function at a given pixel. The proposed method can preserve complex shapes of the JLCs well in a finite dimensional regression function. Given that a two-dimensional grayscale image can be represented as a piecewise-continuous regression function, we apply the proposed algorithm to remove noise from noisy images. Theoretical analysis and numerical results, particularly with image intensity functions, indicate that the proposed method effectively preserves complicated edge structures while efficiently removing noise from piecewise continuous regression surfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16007v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Subhasish Basak, Anik Roy, Partha Sarathi Mukherjee</dc:creator>
    </item>
    <item>
      <title>Forecasting Extreme Temperatures in Siberia Using Supervised Learning and Conformal Prediction Regions</title>
      <link>https://arxiv.org/abs/2503.16118</link>
      <description>arXiv:2503.16118v1 Announce Type: new 
Abstract: In this paper, we step back from a variety of competing heat wave definitions and forecast directly unusually high temperatures. Our testbed is the Russian Far East in the summers of 2022 and 2023. Remotely sensed data from NASA's Aqua spacecraft are organized into a within-subject design that can reduce nuisance variation in forecasted temperatures. Spatial grid cells are the study units. Each is exposed to precursors of a faux heat wave in 2022 and to precursors of a reported heat wave in 2023. The precursors are used to forecast temperatures two weeks in the future for each of 31 consecutive days. Algorithmic fitting procedures produce forecasts with promise and relatively small conformal prediction regions having a coverage probability of at least .75. Spatial and temporal dependence are manageable. At worst, there is weak dependence such that conformal prediction inference is only asymptotically valid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16118v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Richard A. Berk, Amy Braverman</dc:creator>
    </item>
    <item>
      <title>Active Learning For Repairable Hardware Systems With Partial Coverage</title>
      <link>https://arxiv.org/abs/2503.16315</link>
      <description>arXiv:2503.16315v1 Announce Type: new 
Abstract: Identifying the optimal diagnostic test and hardware system instance to infer reliability characteristics using field data is challenging, especially when constrained by fixed budgets and minimal maintenance cycles. Active Learning (AL) has shown promise for parameter inference with limited data and budget constraints in machine learning/deep learning tasks. However, AL for reliability model parameter inference remains underexplored for repairable hardware systems. It requires specialized AL Acquisition Functions (AFs) that consider hardware aging and the fact that a hardware system consists of multiple sub-systems, which may undergo only partial testing during a given diagnostic test. To address these challenges, we propose a relaxed Mixed Integer Semidefinite Program (MISDP) AL AF that incorporates Diagnostic Coverage (DC), Fisher Information Matrices (FIMs), and diagnostic testing budgets. Furthermore, we design empirical-based simulation experiments focusing on two diagnostic testing scenarios: (1) partial tests of a hardware system with overlapping subsystem coverage, and (2) partial tests where one diagnostic test fully subsumes the subsystem coverage of another. We evaluate our proposed approach against the most widely used AL AF in the literature (entropy), as well as several intuitive AL AFs tailored for reliability model parameter inference. Our proposed AF ranked best on average among the alternative AFs across 6,000 experimental configurations, with respect to Area Under the Curve (AUC) of the Absolute Total Expected Event Error (ATEER) and Mean Squared Error (MSE) curves, with statistical significance calculated at a 0.05 alpha level using a Friedman hypothesis test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16315v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Potter, Beyza Kalkanl{\i}, Deniz Erdo\u{g}mu\c{s}, Michael Everett</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Heterogeneous Treatment Effect with Right-censored Data from Synthesizing Randomized Clinical Trials and Real-world Data</title>
      <link>https://arxiv.org/abs/2503.15745</link>
      <description>arXiv:2503.15745v1 Announce Type: cross 
Abstract: The heterogeneous treatment effect plays a crucial role in precision medicine. There is evidence that real-world data, even subject to biases, can be employed as supplementary evidence for randomized clinical trials to improve the statistical efficiency of the heterogeneous treatment effect estimation. In this paper, for survival data with right censoring, we consider estimating the heterogeneous treatment effect, defined as the difference of the treatment-specific conditional restricted mean survival times given covariates, by synthesizing evidence from randomized clinical trials and the real-world data with possible biases. We define an omnibus confounding function to characterize the effect of biases caused by unmeasured confounders, censoring, outcome heterogeneity, and measurement error, and further, identify it by combining the trial and real-world data. We propose a penalized sieve method to estimate the heterogeneous treatment effect and the confounding function and further study the theoretical properties of the proposed integrative estimators based on the theory of reproducing kernel Hilbert space and empirical process. The proposed methodology is shown to outperform the approach solely based on the trial data through simulation studies and an integrative analysis of the data from a randomized trial and a real-world registry on early-stage non-small-cell lung cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15745v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guangcai Mao, Shu Yang, Xiaofei Wang</dc:creator>
    </item>
    <item>
      <title>Alignment of Continuous Brain Connectivity</title>
      <link>https://arxiv.org/abs/2503.15830</link>
      <description>arXiv:2503.15830v1 Announce Type: cross 
Abstract: Brain networks are typically represented by adjacency matrices, where each node corresponds to a brain region. In traditional brain network analysis, nodes are assumed to be matched across individuals, but the methods used for node matching often overlook the underlying connectivity information. This oversight can result in inaccurate node alignment, leading to inflated edge variability and reduced statistical power in downstream connectivity analyses. To overcome this challenge, we propose a novel framework for registering high resolution continuous connectivity (ConCon), defined as a continuous function on a product manifold space specifically, the cortical surface capturing structural connectivity between all pairs of cortical points. Leveraging ConCon, we formulate an optimal diffeomorphism problem to align both connectivity profiles and cortical surfaces simultaneously. We introduce an efficient algorithm to solve this problem and validate our approach using data from the Human Connectome Project (HCP). Results demonstrate that our method substantially improves the accuracy and robustness of connectome-based analyses compared to existing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15830v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Cole, Yang Xiang, Will Consagra, Anuj Srivastava, Xing Qiu, Zhengwu Zhang</dc:creator>
    </item>
    <item>
      <title>Distribution of Deep Gaussian process Gradients and Sequential Design for Simulators with Sharp Variations</title>
      <link>https://arxiv.org/abs/2503.16027</link>
      <description>arXiv:2503.16027v1 Announce Type: cross 
Abstract: Deep Gaussian Processes (DGPs), multi-layered extensions of GPs, better emulate simulators with regime transitions or sharp changes than standard GPs. Gradient information is crucial for tasks like sensitivity analysis and dimension reduction. Although gradient posteriors are well-defined in GPs, extending them to DGPs is challenging due to their hierarchical structure. We propose a novel method to approximate the DGP emulator's gradient distribution, enabling efficient gradient computation with uncertainty quantification (UQ). Our approach derives an analytical gradient mean and the covariance. The numerical results show that our method outperforms GP and DGP with finite difference methods in gradient accuracy, offering the extra unique benefit of UQ. Based on the gradient information, we further propose a sequential design criterion to identify the sharp variation regions efficiently, with the gradient norm as a key indicator whose distribution can be readily evaluated in our framework. We evaluated the proposed sequential design using synthetic examples and empirical applications, demonstrating its superior performance in emulating functions with sharp changes compared to existing design methods. The DGP gradient computation is seamlessly integrated into the advanced Python package dgpsi for DGP emulation, along with the proposed sequential design available at https://github.com/yyimingucl/DGP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16027v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiming Yang, Deyu Ming, Serge Guillas</dc:creator>
    </item>
    <item>
      <title>Cognitive factor-based selection increases power in Alzheimer's dementia randomized clinical trials</title>
      <link>https://arxiv.org/abs/2503.16044</link>
      <description>arXiv:2503.16044v1 Announce Type: cross 
Abstract: Alzheimer's dementia (AD) is of increasing concern as populations achieve longer lifespans. Many of the recent failed AD clinical trials recruiting cognitively intact individuals had a low number of AD events and were thus underpowered. Previous trials have attempted to address this issue by requiring signs of cognitive decline in brain imaging for trial enrollment. However, this method systematically excludes people of color and those without access to healthcare and results in a selected sample that is not representative of the target population. We therefore propose the use of a predictive model based on cognitive test scores to enroll cognitively normal yet high risk participants in a hypothetical clinical trial. Cognitive test scores are a widely accessible tool, so their use in enrollment would be less likely to exclude marginalized populations than biomarkers (such as imaging), which are overwhelmingly available to exclusively high-income patients. We developed a novel longitudinal factor model to predict AD conversion within a 3-year window based on data from the National Alzheimer's Coordinating Center. Through simulation, we demonstrate that our predictive model provides substantial improvements in statistical power and required sample size in hypothetical clinical trials across a range of drug effects compared to other methods of subject selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16044v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julia Gallini, Zach Baucom, Yorghos Tripodis</dc:creator>
    </item>
    <item>
      <title>Doing More With Less: Mismatch-Based Risk-Limiting Audits</title>
      <link>https://arxiv.org/abs/2503.16104</link>
      <description>arXiv:2503.16104v1 Announce Type: cross 
Abstract: One approach to risk-limiting audits (RLAs) compares randomly selected cast vote records (CVRs) to votes read by human auditors from the corresponding ballot cards. Historically, such methods reduce audit sample sizes by considering how each sampled CVR differs from the corresponding true vote, not merely whether they differ. Here we investigate the latter approach, auditing by testing whether the total number of mismatches in the full set of CVRs exceeds the minimum number of CVR errors required for the reported outcome to be wrong (the "CVR margin"). This strategy makes it possible to audit more social choice functions and simplifies RLAs conceptually, which makes it easier to explain than some other RLA approaches. The cost is larger sample sizes. "Mismatch-based RLAs" only require a lower bound on the CVR margin, which for some social choice functions is easier to calculate than the effect of particular errors. When the population rate of mismatches is low and the lower bound on the CVR margin is close to the true CVR margin, the increase in sample size is small. However, the increase may be very large when errors include errors that, if corrected, would widen the CVR margin rather than narrow it; errors affect the margin between candidates other than the reported winner with the fewest votes and the reported loser with the most votes; or errors that affect different margins.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16104v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Ek, Michelle Blom, Philip B. Stark, Peter J. Stuckey, Vanessa J. Teague, Damjan Vukcevic</dc:creator>
    </item>
    <item>
      <title>Affective Polarization Amongst Swedish Politicians</title>
      <link>https://arxiv.org/abs/2503.16193</link>
      <description>arXiv:2503.16193v1 Announce Type: cross 
Abstract: This study investigates affective polarization among Swedish politicians on Twitter from 2021 to 2023, including the September 2022 parliamentary election. Analyzing over 25,000 tweets and employing large language models (LLMs) for sentiment and political classification, we distinguish between positive partisanship (support of allies) and negative partisanship (criticism of opponents).
  Our findings are contingent on the definition of the in-group. When political in-groups are defined at the ideological bloc level, negative and positive partisanship occur at similar rates. However, when the in-group is defined at the party level, negative partisanship becomes significantly more dominant and is 1.51 times more likely (1.45, 1.58). This effect is even stronger among extreme politicians, who engage in negativity more than their moderate counterparts. Negative partisanship also proves to be a strategic choice for online visibility, attracting 3.18 more likes and 1.69 more retweets on average.
  By adapting methods developed for two-party systems and leveraging LLMs for Swedish-language analysis, we provide novel insights into how multiparty politics shapes polarizing discourse. Our results underscore both the strategic appeal of negativity in digital spaces and the growing potential of LLMs for large-scale, non-English political research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16193v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fran\c{c}ois t'Serstevens, Roberto Cerina, Gustav Pepper</dc:creator>
    </item>
    <item>
      <title>Comparison study of variable selection procedures in high-dimensional Gaussian linear regression</title>
      <link>https://arxiv.org/abs/2109.12006</link>
      <description>arXiv:2109.12006v3 Announce Type: replace 
Abstract: We propose an extensive simulation study to compare some variable selection procedures in a high-dimensional framework. Assuming that the relationship between the actives variables and the response variable is linear, the high-dimensional Gaussian linear regression provides a relevant statistical framework to identify active variables related to the response variable. Many variable selection procedures exist, and in this article, we focus on methods based on regularization paths. We perform a comparison study by considering different simulation settings with various dependency structures for variables and evaluate the performance of the methods by computing several metrics. As expected, no method is optimal for all the evaluated performances but we provide recommendations for the best procedures according to the metric to control. Lastly, we test the importance of some assumptions of the model, especially the high dimensionality and the Gaussian ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.12006v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Perrine Lacroix, M\'elina Gallopin, Marie-Laure Martin</dc:creator>
    </item>
    <item>
      <title>Adapting tree-based multiple imputation methods for multi-level data? A simulation study</title>
      <link>https://arxiv.org/abs/2401.14161</link>
      <description>arXiv:2401.14161v2 Announce Type: replace 
Abstract: When data have a hierarchical structure, such as students nested within classrooms, ignoring dependencies between observations can compromise the validity of imputation procedures. Standard tree-based imputation methods implicitly assume independence between observations, limiting their applicability in multilevel data settings. Although Multivariate Imputation by Chained Equations (MICE) is widely used for hierarchical data, it has limitations, including sensitivity to model specification and computational complexity. Alternative tree-based approaches have shown promise for individual-level data, but remain largely unexplored for hierarchical contexts. In this simulation study, we systematically evaluate the performance of novel tree-based methods--Chained Random Forests and Extreme Gradient Boosting (mixgb)--explicitly adapted for multi-level data by incorporating dummy variables indicating cluster membership. We compare these tree-based methods and their adapted versions with traditional MICE imputation in terms of coefficient estimation bias, type I error rates and statistical power, under different cluster sizes, missingness mechanisms and missingness rates, using both random intercept and random slope data generation models. The results show that MICE provides robust and accurate inference for level 2 variables, especially at low missingness rates. However, the adapted boosting approach (mixgb with cluster dummies) consistently outperforms other methods for Level-1 variables at higher missingness rates (30%, 50%). For level 2 variables, while MICE retains better power at moderate missingness (30%), adapted boosting becomes superior at high missingness (50%), regardless of the missingness mechanism or cluster size. These findings highlight the potential of appropriately adapted tree-based imputation methods as effective alternatives to conventional MICE in multilevel data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14161v2</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nico F\"oge, Jakob Schwerter, Ketevan Gurtskaia, Markus Pauly, Philipp Doebler</dc:creator>
    </item>
    <item>
      <title>Estimating the Number of Street Vendors in New York City</title>
      <link>https://arxiv.org/abs/2406.00527</link>
      <description>arXiv:2406.00527v5 Announce Type: replace 
Abstract: We estimate the number of street vendors in New York City. We first summarize the process by which vendors receive licenses and permits to legally operate in New York City. We then describe a survey that was administered by the Street Vendor Project while distributing Coronavirus relief aid to vendors operating in New York City both with and without a license or permit. Finally, we calculate the total number of vendors using ratio estimation. We find that approximately 23,000 street vendors operate in New York City: 20,500 mobile food vendors and 2,300 general merchandise vendors. One third are located in just six ZIP Codes: 11368 (16%), 11372 (3%), and 11354 (3%) in North and West Queens and 10036 (5%), 10019 (4%), and 10001 (3%) in the Chelsea and Clinton neighborhoods of Manhattan. We also provide a theoretical justification of our estimates based on the theory of point processes and a discussion of their accuracy and implications. In particular, our estimates suggest the American Community Survey fails to cover the majority of New York City street vendors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00527v5</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Auerbach</dc:creator>
    </item>
    <item>
      <title>Bayesian Inference General Procedures for A Single-subject Test Study</title>
      <link>https://arxiv.org/abs/2408.15419</link>
      <description>arXiv:2408.15419v5 Announce Type: replace 
Abstract: Abnormality detection in identifying a single-subject which deviates from the majority of a control group dataset is a fundamental problem. Typically, the control group is characterised using standard Normal statistics, and the detection of a single abnormal subject is in that context. However, in many situations, the control group cannot be described by Normal statistics, making standard statistical methods inappropriate. This paper presents a Bayesian Inference General Procedures for A Single-subject Test (BIGPAST) designed to mitigate the effects of skewness under the assumption that the dataset of the control group comes from the skewed Student \( t \) distribution. BIGPAST operates under the null hypothesis that the single-subject follows the same distribution as the control group. We assess BIGPAST's performance against other methods through simulation studies. The results demonstrate that BIGPAST is robust against deviations from normality and outperforms the existing approaches in accuracy nearest to the nominal accuracy 0.95.
  BIGPAST can reduce model misspecification errors under the skewed Student
  $t$ assumption by up to 12 times, as demonstrated in Section 3.3. We
  apply BIGPAST to a Magnetoencephalography (MEG) dataset consisting of an
  individual with mild traumatic brain injury and an age and gender-matched
  control group. For example, the previous method failed to detect abnormalities
  in 8 brain areas, whereas BIGPAST successfully identified them, demonstrating
  its effectiveness in detecting abnormalities in a single-subject.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15419v5</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.neuri.2025.100195</arxiv:DOI>
      <arxiv:journal_reference>Neuroscience Informatics Volume 5, Issue 2, June 2025, 100195 Neuroscience Informatics, Volume 5, Issue 2</arxiv:journal_reference>
      <dc:creator>Jie Li, Gary Green, Sarah J. A. Carr, Peng Liu, Jian Zhang</dc:creator>
    </item>
    <item>
      <title>Consistent model selection for estimating functional interactions among stochastic neurons with variable-length memory</title>
      <link>https://arxiv.org/abs/2411.08205</link>
      <description>arXiv:2411.08205v2 Announce Type: replace 
Abstract: We address the problem of identifying functional interactions among stochastic neurons with variable-length memory from their spiking activity. The neuronal network is modeled by a stochastic system of interacting point processes with variable-length memory. Each chain describes the activity of a single neuron, indicating whether it spikes at a given time. One neuron's influence on another can be either excitatory or inhibitory. To identify the existence and nature of an interaction between a neuron and its postsynaptic counterpart, we propose a model selection procedure based on the observation of the spike activity of a finite set of neurons over a finite time. The proposed procedure is also based on the maximum likelihood estimator for the synaptic weight matrix of the network neuronal model. In this sense, we prove the consistency of the maximum likelihood estimator {followed} by a proof of the consistency of the neighborhood interaction estimation procedure. The effectiveness of the proposed model selection procedure is demonstrated using simulated data, which validates the underlying theory. The method is also applied to analyze spike train data recorded from hippocampal neurons in rats during a visual attention task, where a computational model reconstructs the spiking activity and the results reveal interesting and biologically relevant information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08205v2</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ricardo F. Ferreira, Matheus E. Pacola, Vitor G. Schiavone, Rodrigo F. O. Pena</dc:creator>
    </item>
    <item>
      <title>The Impact of Meteorological Factors on Crop Price Volatility in India: Case studies of Soybean and Brinjal</title>
      <link>https://arxiv.org/abs/2503.11690</link>
      <description>arXiv:2503.11690v2 Announce Type: replace 
Abstract: Climate is an evolving complex system with dynamic interactions and non-linear feedback mechanisms, shaping environmental and socio-economic outcomes. Crop production is highly sensitive to such climatic fluctuations. This paper studies the price volatility of agricultural crops as influenced by meteorological variables (and many other environmental, social and governance factors), which is a critical challenge in sustainable finance, agricultural planning, and policy-making. As case studies, we choose the two Indian states of Madhya Pradesh (for Soybean) and Odisha (for Brinjal). We employ an Exponential Generalized Autoregressive Conditional Heteroskedasticity (EGARCH) model to estimate the conditional volatility of the log returns of crop prices from 2012 to 2024. This study further explores the cross-correlations between volatility and the meteorological variables. Further, a Granger-causality test is carried out to analyze the causal effect of meteorological variables on the price volatility. Finally, the Seasonal Auto-Regressive Integrated Moving Average with Exogenous Regressors (SARIMAX) and Long Short-Term Memory (LSTM) models are implemented as simple machine learning models of price volatility with meteorological factors as exogenous variables. We believe that this will illustrate the usefulness of simple machine learning models in agricultural finance, and help the farmers to make informed decisions by considering climate patterns and making beneficial decisions with regard to crop rotation or allocations. In general, incorporating meteorological factors to assess agricultural performance could help to understand and reduce price volatility and possibly lead to economic stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11690v2</guid>
      <category>stat.AP</category>
      <category>econ.GN</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ashok Kumar, Abbinav Sankar Kailasam, Anish Rai, Sudeep Shukla, Sourish Das, Anirban Chakraborti</dc:creator>
    </item>
    <item>
      <title>Asymptotic non-linear shrinkage and eigenvector overlap for weighted sample covariance</title>
      <link>https://arxiv.org/abs/2410.14420</link>
      <description>arXiv:2410.14420v2 Announce Type: replace-cross 
Abstract: We compute asymptotic non-linear shrinkage formulas for covariance and precision matrix estimators for weighted sample covariances, and the joint sample-population eigenvector overlap distribution, in the spirit of Ledoit and P\'ech\'e. We detail explicitly the formulas for exponentially-weighted sample covariances. We propose an algorithm to numerically compute those formulas. Experimentally, we show the performance of the asymptotic non-linear shrinkage estimators. Finally, we test the robustness of the theory to a heavy-tailed distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14420v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Benoit Oriol</dc:creator>
    </item>
  </channel>
</rss>
