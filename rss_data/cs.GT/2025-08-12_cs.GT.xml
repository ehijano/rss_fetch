<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Aug 2025 01:28:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Generative Bid Shading in Real-Time Bidding Advertising</title>
      <link>https://arxiv.org/abs/2508.06550</link>
      <description>arXiv:2508.06550v1 Announce Type: new 
Abstract: Bid shading plays a crucial role in Real-Time Bidding~(RTB) by adaptively adjusting the bid to avoid advertisers overspending. Existing mainstream two-stage methods, which first model bid landscapes and then optimize surplus using operations research techniques, are constrained by unimodal assumptions that fail to adapt for non-convex surplus curves and are vulnerable to cascading errors in sequential workflows. Additionally, existing discretization models of continuous values ignore the dependence between discrete intervals, reducing the model's error correction ability, while sample selection bias in bidding scenarios presents further challenges for prediction. To address these issues, this paper introduces Generative Bid Shading~(GBS), which comprises two primary components: (1) an end-to-end generative model that utilizes an autoregressive approach to generate shading ratios by stepwise residuals, capturing complex value dependencies without relying on predefined priors; and (2) a reward preference alignment system, which incorporates a channel-aware hierarchical dynamic network~(CHNet) as the reward model to extract fine-grained features, along with modules for surplus optimization and exploration utility reward alignment, ultimately optimizing both short-term and long-term surplus using group relative policy optimization~(GRPO). Extensive experiments on both offline and online A/B tests validate GBS's effectiveness. Moreover, GBS has been deployed on the Meituan DSP platform, serving billions of bid requests daily.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06550v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinqiu Huang, Hao Ma, Wenshuai Chen, Shuli Wang, Yongqiang Zhang, Xue Wei, Yinhua Zhu, Haitao Wang, Xingxing Wang</dc:creator>
    </item>
    <item>
      <title>Algorithmic Delegated Choice: An Annotated Reading List</title>
      <link>https://arxiv.org/abs/2508.06562</link>
      <description>arXiv:2508.06562v2 Announce Type: new 
Abstract: The problem of delegated choice has been of long interest in economics and recently on computer science. We overview a list of papers on delegated choice problem, from classic works to recent papers with algorithmic perspectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06562v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>econ.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad T. Hajiaghayi, Suho Shin</dc:creator>
    </item>
    <item>
      <title>Asymmetric Network Games: $\alpha$-Potential Function and Learning</title>
      <link>https://arxiv.org/abs/2508.06619</link>
      <description>arXiv:2508.06619v1 Announce Type: new 
Abstract: In a network game, players interact over a network and the utility of each player depends on his own action and on an aggregate of his neighbours' actions. Many real world networks of interest are asymmetric and involve a large number of heterogeneous players. This paper analyzes static network games using the framework of $\alpha$-potential games. Under mild assumptions on the action sets (compact intervals) and the utility functions (twice continuously differentiable) of the players, we derive an expression for an inexact potential function of the game, called the $\alpha$-potential function. Using such a function, we show that modified versions of the sequential best-response algorithm and the simultaneous gradient play algorithm achieve convergence of players' actions to a $2\alpha$-Nash equilibrium. For linear-quadratic network games, we show that $\alpha$ depends on the maximum asymmetry in the network and is well-behaved for a wide range of networks of practical interest. Further, we derive bounds on the social welfare of the $\alpha$-Nash equilibrium corresponding to the maximum of the $\alpha$-potential function, under suitable assumptions. We numerically illustrate the convergence of the proposed algorithms and properties of the learned $2\alpha$-Nash equilibria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06619v1</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <category>cs.SI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kiran Rokade, Adit Jain, Francesca Parise, Vikram Krishnamurthy, Eva Tardos</dc:creator>
    </item>
    <item>
      <title>Convergence of Fast Policy Iteration in Markov Games and Robust MDPs</title>
      <link>https://arxiv.org/abs/2508.06661</link>
      <description>arXiv:2508.06661v1 Announce Type: new 
Abstract: Markov games and robust MDPs are closely related models that involve computing a pair of saddle point policies. As part of the long-standing effort to develop efficient algorithms for these models, the Filar-Tolwinski (FT) algorithm has shown considerable promise. As our first contribution, we demonstrate that FT may fail to converge to a saddle point and may loop indefinitely, even in small games. This observation contradicts the proof of FT's convergence to a saddle point in the original paper. As our second contribution, we propose Residual Conditioned Policy Iteration (RCPI). RCPI builds on FT, but is guaranteed to converge to a saddle point. Our numerical results show that RCPI outperforms other convergent algorithms by several orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06661v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keith Badger, Marek Petrik, Jefferson Huang</dc:creator>
    </item>
    <item>
      <title>Emergence of Cooperation and Commitment in Optional Prisoner's Dilemma</title>
      <link>https://arxiv.org/abs/2508.06702</link>
      <description>arXiv:2508.06702v1 Announce Type: new 
Abstract: Commitment is a well-established mechanism for fostering cooperation in human society and multi-agent systems. However, existing research has predominantly focused on the commitment that neglects the freedom of players to abstain from an interaction, limiting their applicability to many real-world scenarios where participation is often voluntary. In this paper, we present a two-stage game model to investigate the evolution of commitment-based behaviours and cooperation within the framework of the optional Prisoner's Dilemma game. In the pre-game stage, players decide whether to accept a mutual commitment. Once in the game, they choose among cooperation, defection, or exiting, depending on the formation of a pre-game commitment. We find that optional participation boosts commitment acceptance but fails to foster cooperation, leading instead to widespread exit behaviour. To address this, we then introduce and compare two institutional incentive approaches: i) a strict one (STRICT-COM) that rewards only committed players who cooperate in the game, and ii) a flexible one (FLEXIBLE-COM) that rewards any committed players who do not defect in the game. The results reveal that, while the strict approach is demonstrably better for promoting cooperation as the flexible rule creates a loophole for an opportunistic exit after committing, the flexible rule offers an efficient alternative for enhancing social welfare when such opportunistic behaviour results in a high gain. This study highlights the limitations of relying solely on voluntary participation and commitment to resolving social dilemmas, emphasising the importance of well-designed institutional incentives to promote cooperation and social welfare effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06702v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhao Song, The Anh Han</dc:creator>
    </item>
    <item>
      <title>When Competition Helps: Achieving Optimal Traffic Flow with Multiple Autonomous Planners</title>
      <link>https://arxiv.org/abs/2508.07145</link>
      <description>arXiv:2508.07145v1 Announce Type: new 
Abstract: The inefficiency of selfish routing in congested networks is a classical problem in algorithmic game theory, often captured by the Price of Anarchy (i.e., the ratio between the social cost of decentralized decisions and that of a centrally optimized solution.) With the advent of autonomous vehicles, capable of receiving and executing centrally assigned routes, it is natural to ask whether their deployment can eliminate this inefficiency. At first glance, a central authority could simply compute an optimal traffic assignment and instruct each vehicle to follow its assigned path. However, this vision overlooks critical challenges: routes must be individually rational (no vehicle has an incentive to deviate), and in practice, multiple planning agents (e.g., different companies) may coexist and compete. Surprisingly, we show that such competition is not merely an obstacle but a necessary ingredient for achieving optimal outcomes. In this work, we design a routing mechanism that embraces competition and converges to an optimal assignment, starting from the classical Pigou network as a foundational case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07145v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivan Geffner, Erez Karpas, Moshe Tennenholtz</dc:creator>
    </item>
    <item>
      <title>Maximizing Social Welfare with Side Payments</title>
      <link>https://arxiv.org/abs/2508.07147</link>
      <description>arXiv:2508.07147v1 Announce Type: new 
Abstract: We examine normal-form games in which players may \emph{pre-commit} to outcome-contingent transfers before choosing their actions. In the one-shot version of this model, Jackson and Wilkie showed that side contracting can backfire: even a game with a Pareto-optimal Nash equilibrium can devolve into inefficient equilibria once unbounded, simultaneous commitments are allowed. The root cause is a prisoner's dilemma effect, where each player can exploit her commitment power to reshape the equilibrium in her favor, harming overall welfare.
  To circumvent this problem we introduce a \emph{staged-commitment} protocol. Players may pledge transfers only in small, capped increments over multiple rounds, and the phase continues only with unanimous consent. We prove that, starting from any finite game $\Gamma$ with a non-degenerate Nash equilibrium $\vec{\sigma}$, this protocol implements every welfare-maximizing payoff profile that \emph{strictly} Pareto-improves $\vec{\sigma}$. Thus, gradual and bounded commitments restore the full efficiency potential of side payments while avoiding the inefficiencies identified by Jackson and Wilkie.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07147v1</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ivan Geffner, Caspar Oesterheld, Vincent Conitzer</dc:creator>
    </item>
    <item>
      <title>Last-Iterate Convergence in Adaptive Regret Minimization for Approximate Extensive-Form Perfect Equilibrium</title>
      <link>https://arxiv.org/abs/2508.07699</link>
      <description>arXiv:2508.07699v1 Announce Type: new 
Abstract: The Nash Equilibrium (NE) assumes rational play in imperfect-information Extensive-Form Games (EFGs) but fails to ensure optimal strategies for off-equilibrium branches of the game tree, potentially leading to suboptimal outcomes in practical settings. To address this, the Extensive-Form Perfect Equilibrium (EFPE), a refinement of NE, introduces controlled perturbations to model potential player errors. However, existing EFPE-finding algorithms, which typically rely on average strategy convergence and fixed perturbations, face significant limitations: computing average strategies incurs high computational costs and approximation errors, while fixed perturbations create a trade-off between NE approximation accuracy and the convergence rate of NE refinements.
  To tackle these challenges, we propose an efficient adaptive regret minimization algorithm for computing approximate EFPE, achieving last-iterate convergence in two-player zero-sum EFGs. Our approach introduces Reward Transformation Counterfactual Regret Minimization (RTCFR) to solve perturbed games and defines a novel metric, the Information Set Nash Equilibrium (ISNE), to dynamically adjust perturbations. Theoretical analysis confirms convergence to EFPE, and experimental results demonstrate that our method significantly outperforms state-of-the-art algorithms in both NE and EFPE-finding tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07699v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hang Ren, Xiaozhen Sun, Tianzi Ma, Jiajia Zhang, Xuan Wang</dc:creator>
    </item>
    <item>
      <title>Truthful Two-Obnoxious-Facility Location Games with Optional Preferences and Minimum Distance Constraint</title>
      <link>https://arxiv.org/abs/2508.08036</link>
      <description>arXiv:2508.08036v1 Announce Type: new 
Abstract: In this paper, we study a truthful two-obnoxious-facility location problem, in which each agent has a private location in [0, 1] and a public optional preference over two obnoxious facilities, and there is a minimum distance constraint d between the two facilities. Each agent wants to be as far away as possible from the facilities that affect her, and the utility of each agent is the total distance from her to these facilities. The goal is to decide how to place the facilities in [0, 1] so as to incentivize agents to report their private locations truthfully as well as maximize the social utility. First, we consider the special setting where d = 0, that is, the two facilities can be located at any point in [0, 1]. We propose a deterministic strategyproof mechanism with approximation ratio of at most 4 and a randomized strategyproof mechanism with approximation ratio of at most 2, respectively. Then we study the general setting. We propose a deterministic strategyproof mechanism with approximation ratio of at most 8 and a randomized strategyproof mechanism with approximation ratio of at most 4, respectively. Furthermore, we provide lower bounds of 2 and 14/13 on the approximation ratio for any deterministic and any randomized strategyproof mechanism, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08036v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaojia Han, Wenjing Liu, Qizhi Fang</dc:creator>
    </item>
    <item>
      <title>Constrained Distributed Heterogeneous Two-Facility Location Problems with Max-Variant Cost</title>
      <link>https://arxiv.org/abs/2508.08045</link>
      <description>arXiv:2508.08045v1 Announce Type: new 
Abstract: We study a constrained distributed heterogeneous two-facility location problem, where a set of agents with private locations on the real line are divided into disjoint groups. The constraint means that the facilities can only be built in a given multiset of candidate locations and at most one facility can be built at each candidate location. Given the locations of the two facilities, the cost of an agent is the distance from her location to the farthest facility (referred to as max-variant). Our goal is to design strategyproof distributed mechanisms that can incentivize all agents to truthfully report their locations and approximately optimize some social objective. A distributed mechanism consists of two steps: for each group, the mechanism chooses two candidate locations as the representatives of the group based only on the locations reported by agents therein; then, it outputs two facility locations among all the representatives. We focus on a class of deterministic strategyproof distributed mechanisms and analyze upper and lower bounds on the distortion under the Average-of-Average cost (average of the average individual cost of agents in each group), the Max-of-Max cost (maximum individual cost among all agents), the Average-of-Max cost (average of the maximum individual cost among all agents in each group) and the Max-of-Average cost (maximum of the average individual cost of all agents in each group). Under four social objectives, we obtain constant upper and lower distortion bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08045v1</guid>
      <category>cs.GT</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinru Xu, Wenjing Liu, Qizhi Fang</dc:creator>
    </item>
    <item>
      <title>Solving Pasur Using GPU-Accelerated Counterfactual Regret Minimization</title>
      <link>https://arxiv.org/abs/2508.06559</link>
      <description>arXiv:2508.06559v1 Announce Type: cross 
Abstract: Pasur is a fishing card game played over six rounds and is played similarly to games such as Cassino and Scopa, and Bastra. This paper introduces a CUDA-accelerated computational framework for simulating Pasur, emphasizing efficient memory management. We use our framework to compute near-Nash equilibria via Counterfactual Regret Minimization (CFR), a well-known algorithm for solving large imperfect-information games.
  Solving Pasur presents unique challenges due to its intricate rules and the large size of its game tree. We handle rule complexity using PyTorch CUDA tensors and to address the memory-intensive nature of the game, we decompose the game tree into two key components: (1) actual game states, and (2) inherited scores from previous rounds. We construct the Full Game Tree by pairing card states with accumulated scores in the Unfolding Process. This design reduces memory overhead by storing only essential strategy values and node connections. To further manage computational complexity, we apply a round-by-round backward training strategy, starting from the final round and recursively propagating average utilities to earlier stages. Our approach constructs the complete game tree, which on average consists of over $10^9$ nodes. We provide detailed implementation snippets.
  After computing a near-Nash equilibrium strategy, we train a tree-based model to predict these strategies for use during gameplay. We then estimate the fair value of each deck through large-scale self-play between equilibrium strategies by simulating, for instance, 10,000 games per matchup, executed in parallel using GPU acceleration.
  Similar frameworks can be extended to other reinforcement learning algorithms where the action tree naturally decomposes into multiple rounds such as turn-based strategy games or sequential trading decisions in financial markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06559v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sina Baghal</dc:creator>
    </item>
    <item>
      <title>Inter-role reciprocity in evolutionary trust game on square lattices</title>
      <link>https://arxiv.org/abs/2508.06685</link>
      <description>arXiv:2508.06685v1 Announce Type: cross 
Abstract: Simulating bipartite games, such as the trust game, is not straightforward due to the lack of a natural way to distinguish roles in a single population. The square lattice topology can provide a simple yet elegant solution by alternating trustors and trustees. For even lattice sizes, it creates two disjoint diagonal sub-lattices for strategy learning, while game interactions can take place on the original lattice. This setup ensures a minimal spatial structure that allows interactions across roles and learning within roles. By simulations on this setup, we detect an inter-role spatial reciprocity mechanism, through which trust can emerge. In particular, a moderate return ratio allows investing trustors and trustworthy trustees to form inter-role clusters and thus save trust. If the return is too high, it harms the survival of trustees; if too low, it harms trustors. The proposed simulation framework is also applicable to any bipartite game to uncover potential inter-role spatial mechanisms across various scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06685v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.GT</category>
      <category>nlin.CG</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <arxiv:DOI>10.1063/5.0285064</arxiv:DOI>
      <arxiv:journal_reference>Chaos 35, 083117 (2025)</arxiv:journal_reference>
      <dc:creator>Chaoqian Wang, Wei Zhang, Xinwei Wang, Attila Szolnoki</dc:creator>
    </item>
    <item>
      <title>Strategic Incentivization for Locally Differentially Private Federated Learning</title>
      <link>https://arxiv.org/abs/2508.07138</link>
      <description>arXiv:2508.07138v1 Announce Type: cross 
Abstract: In Federated Learning (FL), multiple clients jointly train a machine learning model by sharing gradient information, instead of raw data, with a server over multiple rounds. To address the possibility of information leakage in spite of sharing only the gradients, Local Differential Privacy (LDP) is often used. In LDP, clients add a selective amount of noise to the gradients before sending the same to the server. Although such noise addition protects the privacy of clients, it leads to a degradation in global model accuracy. In this paper, we model this privacy-accuracy trade-off as a game, where the sever incentivizes the clients to add a lower degree of noise for achieving higher accuracy, while the clients attempt to preserve their privacy at the cost of a potential loss in accuracy. A token based incentivization mechanism is introduced in which the quantum of tokens credited to a client in an FL round is a function of the degree of perturbation of its gradients. The client can later access a newly updated global model only after acquiring enough tokens, which are to be deducted from its balance. We identify the players, their actions and payoff, and perform a strategic analysis of the game. Extensive experiments were carried out to study the impact of different parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07138v1</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yashwant Krishna Pagoti, Arunesh Sinha, Shamik Sural</dc:creator>
    </item>
    <item>
      <title>Multi-Hop Privacy Propagation for Differentially Private Federated Learning in Social Networks</title>
      <link>https://arxiv.org/abs/2508.07676</link>
      <description>arXiv:2508.07676v1 Announce Type: cross 
Abstract: Federated learning (FL) enables collaborative model training across decentralized clients without sharing local data, thereby enhancing privacy and facilitating collaboration among clients connected via social networks. However, these social connections introduce privacy externalities: a client's privacy loss depends not only on its privacy protection strategy but also on the privacy decisions of others, propagated through the network via multi-hop interactions. In this work, we propose a socially-aware privacy-preserving FL mechanism that systematically quantifies indirect privacy leakage through a multi-hop propagation model. We formulate the server-client interaction as a two-stage Stackelberg game, where the server, as the leader, optimizes incentive policies, and clients, as followers, strategically select their privacy budgets, which determine their privacy-preserving levels by controlling the magnitude of added noise. To mitigate information asymmetry in networked privacy estimation, we introduce a mean-field estimator to approximate the average external privacy risk. We theoretically prove the existence and convergence of the fixed point of the mean-field estimator and derive closed-form expressions for the Stackelberg Nash Equilibrium. Despite being designed from a client-centric incentive perspective, our mechanism achieves approximately-optimal social welfare, as revealed by Price of Anarchy (PoA) analysis. Experiments on diverse datasets demonstrate that our approach significantly improves client utilities and reduces server costs while maintaining model performance, outperforming both Social-Agnostic (SA) baselines and methods that account for social externalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07676v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.GT</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenchen Lin, Xuehe Wang</dc:creator>
    </item>
    <item>
      <title>Asynchronous Majority Dynamics on Binomial Random Graphs</title>
      <link>https://arxiv.org/abs/2309.04691</link>
      <description>arXiv:2309.04691v2 Announce Type: replace 
Abstract: We study information aggregation in networks when agents interact to learn a binary state of the world. Initially each agent privately observes an independent signal which is "correct" with probability $\frac{1}{2}+\delta$ for some $\delta &gt; 0$. At each round, a node is selected uniformly at random to update their public opinion to match the majority of their neighbours (breaking ties in favour of their initial private signal). Our main result shows that for sparse and connected binomial random graphs $\mathcal G(n,p)$ the process stabilizes in a "correct" consensus in $\mathcal O(n\log^2 n/\log\log n)$ steps with high probability. In fact, when $\log n/n \ll p = o(1)$ the process terminates at time $\hat T = (1+o(1))n\log n$, where $\hat T$ is the first time when all nodes have been selected at least once. However, in dense binomial random graphs with $p=\Omega(1)$, there is an information cascade where the process terminates in the "incorrect" consensus with probability bounded away from zero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04691v2</guid>
      <category>cs.GT</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Divyarthi Mohan, Pawel Pralat</dc:creator>
    </item>
    <item>
      <title>The Condorcet Dimension of Metric Spaces</title>
      <link>https://arxiv.org/abs/2410.09201</link>
      <description>arXiv:2410.09201v3 Announce Type: replace 
Abstract: A Condorcet winning set is a set of candidates such that no other candidate is preferred by at least half the voters over all members of the set. The Condorcet dimension, which is the minimum cardinality of a Condorcet winning set, is known to be at most logarithmic in the number of candidates. We study the case of elections where voters and candidates are located in a $2$-dimensional space with preferences based upon proximity voting. Our main result is that the Condorcet dimension is at most $3$, under both the Manhattan norm and the infinity norm, natural measures in electoral systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09201v3</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandra Lassota, Adrian Vetta, Bernhard von Stengel</dc:creator>
    </item>
    <item>
      <title>A Direct Proof of the Short-Side Advantage in Random Matching Markets</title>
      <link>https://arxiv.org/abs/2501.05574</link>
      <description>arXiv:2501.05574v2 Announce Type: replace 
Abstract: We study the stable matching problem under the random matching model where the preferences of the doctors and hospitals are sampled uniformly and independently at random. In a balanced market with $n$ doctors and $n$ hospitals, the doctor-proposal deferred-acceptance algorithm gives doctors an expected rank of order $\log n$ for their partners and hospitals an expected rank of order $\frac{n}{\log n}$ for their partners. This situation is reversed in an unbalanced market with $n+1$ doctors and $n$ hospitals, a phenomenon known as the short-side advantage. The current proofs of this fact are indirect, counter-intuitively being based upon analyzing the hospital-proposal deferred-acceptance algorithm. In this paper we provide a direct proof of the short-side advantage, explicitly analyzing the doctor-proposal deferred-acceptance algorithm. Our proof sheds light on how and why the phenomenon arises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05574v2</guid>
      <category>cs.GT</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Mauras, Pawel Pralat, Adrian Vetta</dc:creator>
    </item>
    <item>
      <title>Inequality in the Age of Pseudonymity</title>
      <link>https://arxiv.org/abs/2508.04668</link>
      <description>arXiv:2508.04668v3 Announce Type: replace 
Abstract: Inequality measures such as the Gini coefficient are used to inform and motivate policymaking, and are increasingly applied to digital platforms. We analyze how measures fare in pseudonymous settings that are common in the digital age. One key challenge of such environments is the ability of actors to create fake identities under fictitious false names, also known as ``Sybils.'' While some actors may do so to preserve their privacy, we show that this can inadvertently hamper inequality measurements. As we prove, it is impossible for measures satisfying the literature's canonical set of desired properties to assess the inequality of an economy that may harbor Sybils. We characterize the class of all Sybil-proof measures, and prove that they must satisfy relaxed version of the aforementioned properties. Furthermore, we show that the structure imposed restricts the ability to assess inequality at a fine-grained level. By applying our results, we prove that large classes of popular measures are not Sybil-proof, with the famous Gini coefficient being but one example out of many. Finally, we examine the dynamics leading to the creation of Sybils in digital and traditional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04668v3</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <category>econ.TH</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aviv Yaish, Nir Chemaya, Lin William Cong, Dahlia Malkhi</dc:creator>
    </item>
    <item>
      <title>Computing a Fixed Point of Contraction Maps in Polynomial Queries</title>
      <link>https://arxiv.org/abs/2403.19911</link>
      <description>arXiv:2403.19911v2 Announce Type: replace-cross 
Abstract: We give an algorithm for finding an $\epsilon$-fixed point of a contraction map $f:[0,1]^k\mapsto[0,1]^k$ under the $\ell_\infty$-norm with query complexity $O (k\log (1/\epsilon ) )$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19911v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3744738</arxiv:DOI>
      <dc:creator>Xi Chen, Yuhao Li, Mihalis Yannakakis</dc:creator>
    </item>
    <item>
      <title>Observation Interference in Partially Observable Assistance Games</title>
      <link>https://arxiv.org/abs/2412.17797</link>
      <description>arXiv:2412.17797v2 Announce Type: replace-cross 
Abstract: We study partially observable assistance games (POAGs), a model of the human-AI value alignment problem which allows the human and the AI assistant to have partial observations. Motivated by concerns of AI deception, we study a qualitatively new phenomenon made possible by partial observability: would an AI assistant ever have an incentive to interfere with the human's observations? First, we prove that sometimes an optimal assistant must take observation-interfering actions, even when the human is playing optimally, and even when there are otherwise-equivalent actions available that do not interfere with observations. Though this result seems to contradict the classic theorem from single-agent decision making that the value of information is nonnegative, we resolve this seeming contradiction by developing a notion of interference defined on entire policies. This can be viewed as an extension of the classic result that the value of information is nonnegative into the cooperative multiagent setting. Second, we prove that if the human is simply making decisions based on their immediate outcomes, the assistant might need to interfere with observations as a way to query the human's preferences. We show that this incentive for interference goes away if the human is playing optimally, or if we introduce a communication channel for the human to communicate their preferences to the assistant. Third, we show that if the human acts according to the Boltzmann model of irrationality, this can create an incentive for the assistant to interfere with observations. Finally, we use an experimental model to analyze tradeoffs faced by the AI assistant in practice when considering whether or not to take observation-interfering actions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17797v2</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Scott Emmons, Caspar Oesterheld, Vincent Conitzer, Stuart Russell</dc:creator>
    </item>
    <item>
      <title>ElementaryNet: A Non-Strategic Neural Network for Predicting Human Behavior in Normal-Form Games</title>
      <link>https://arxiv.org/abs/2503.05925</link>
      <description>arXiv:2503.05925v2 Announce Type: replace-cross 
Abstract: Behavioral game theory models serve two purposes: yielding insights into how human decision-making works, and predicting how people would behave in novel strategic settings. A system called GameNet represents the state of the art for predicting human behavior in the setting of unrepeated simultaneous-move games, combining a simple "level-k" model of strategic reasoning with a complex neural network model of non-strategic "level-0" behavior. Although this reliance on well-established ideas from cognitive science ought to make GameNet interpretable, the flexibility of its level-0 model raises the possibility that it is able to emulate strategic reasoning. In this work, we prove that GameNet's level-0 model is indeed too general. We then introduce ElementaryNet, a novel neural network that is provably incapable of expressing strategic behavior. We show that these additional restrictions are empirically harmless, leading ElementaryNet to statistically indistinguishable predictive performance vs GameNet. We then show how it is possible to derive insights about human behavior by varying ElementaryNet's features and interpreting its parameters, finding evidence of iterative reasoning, learning about the depth of this reasoning process, and showing the value of a rich level-0 specification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05925v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Greg d'Eon, Hala Murad, Kevin Leyton-Brown, James R. Wright</dc:creator>
    </item>
    <item>
      <title>Game Reasoning Arena: A Framework and Benchmark for Assessing Reasoning Capabilites of Large Language Models via Game Play</title>
      <link>https://arxiv.org/abs/2508.03368</link>
      <description>arXiv:2508.03368v2 Announce Type: replace-cross 
Abstract: The Game Reasoning Arena library provides a framework for evaluating the decision making abilities of large language models (LLMs) through strategic board games implemented in Google OpenSpiel library. The framework enables systematic comparisons between LLM based agents and other agents (random, heuristic, reinforcement learning agents, etc.) in various game scenarios by wrapping multiple board and matrix games and supporting different agent types. It integrates API access to models via liteLLM, local model deployment via vLLM, and offers distributed execution through Ray. This paper summarises the library structure, key characteristics, and motivation of the repository, highlighting how it contributes to the empirical evaluation of the reasoning of LLM and game theoretic behaviour.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03368v2</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 12 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucia Cipolina-Kun, Marianna Nezhurina, Jenia Jitsev</dc:creator>
    </item>
  </channel>
</rss>
