<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Jul 2024 02:43:58 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 22 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Persuading while Learning</title>
      <link>https://arxiv.org/abs/2407.13964</link>
      <description>arXiv:2407.13964v1 Announce Type: new 
Abstract: We propose a dynamic product adoption persuasion model involving an impatient partially informed sender who gradually learns the state. In this model, the sender gathers information over time, and hence her posteriors' sequence forms a discrete-time martingale. The sender commits to a dynamic revelation policy to persuade the agent to adopt a product. We demonstrate that under the assumption that the sender's martingale possesses Blackwell-preserving kernels, the family of optimal strategies for the sender takes an interval form; namely, in every period the set of martingale realizations in which adoption occurs is an interval. Utilizing this, we prove that if the sender is sufficiently impatient, then under a random walk martingale, the optimal policy is fully transparent up to the moment of adoption; namely, the sender reveals the entire information she privately holds in every period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13964v1</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Itai Arieli, Yakov Babichenko, Dimitry Shaiderman, Xianwen Shi</dc:creator>
    </item>
    <item>
      <title>People use fast, goal-directed simulation to reason about novel games</title>
      <link>https://arxiv.org/abs/2407.14095</link>
      <description>arXiv:2407.14095v1 Announce Type: new 
Abstract: We can evaluate features of problems and their potential solutions well before we can effectively solve them. When considering a game we have never played, for instance, we might infer whether it is likely to be challenging, fair, or fun simply from hearing the game rules, prior to deciding whether to invest time in learning the game or trying to play it well. Many studies of game play have focused on optimality and expertise, characterizing how people and computational models play based on moderate to extensive search and after playing a game dozens (if not thousands or millions) of times. Here, we study how people reason about a range of simple but novel connect-n style board games. We ask people to judge how fair and how fun the games are from very little experience: just thinking about the game for a minute or so, before they have ever actually played with anyone else, and we propose a resource-limited model that captures their judgments using only a small number of partial game simulations and almost no lookahead search.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14095v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cedegao E. Zhang, Katherine M. Collins, Lionel Wong, Adrian Weller, Joshua B. Tenenbaum</dc:creator>
    </item>
    <item>
      <title>Unravelling in Collaborative Learning</title>
      <link>https://arxiv.org/abs/2407.14332</link>
      <description>arXiv:2407.14332v1 Announce Type: new 
Abstract: Collaborative learning offers a promising avenue for leveraging decentralized data. However, collaboration in groups of strategic learners is not a given. In this work, we consider strategic agents who wish to train a model together but have sampling distributions of different quality. The collaboration is organized by a benevolent aggregator who gathers samples so as to maximize total welfare, but is unaware of data quality. This setting allows us to shed light on the deleterious effect of adverse selection in collaborative learning. More precisely, we demonstrate that when data quality indices are private, the coalition may undergo a phenomenon known as unravelling, wherein it shrinks up to the point that it becomes empty or solely comprised of the worst agent. We show how this issue can be addressed without making use of external transfers, by proposing a novel method inspired by probabilistic verification. This approach makes the grand coalition a Nash equilibrium with high probability despite information asymmetry, thereby breaking unravelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14332v1</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aymeric Capitaine, Etienne Boursier, Antoine Scheid, Eric Moulines, Michael I. Jordan, El-Mahdi El-Mhamdi, Alain Durmus</dc:creator>
    </item>
    <item>
      <title>Integrated Resource Allocation and Strategy Synthesis in Safety Games on Graphs with Deception</title>
      <link>https://arxiv.org/abs/2407.14436</link>
      <description>arXiv:2407.14436v1 Announce Type: new 
Abstract: Deception plays a crucial role in strategic interactions with incomplete information. Motivated by security applications, we study a class of two-player turn-based deterministic games with one-sided incomplete information, in which player 1 (P1) aims to prevent player 2 (P2) from reaching a set of target states. In addition to actions, P1 can place two kinds of deception resources: "traps" and "fake targets" to disinform P2 about the transition dynamics and payoff of the game. Traps "hide the real" by making trap states appear normal, while fake targets "reveal the fiction" by advertising non-target states as targets. We are interested in jointly synthesizing optimal decoy placement and deceptive defense strategies for P1 that exploits P2's misinformation. We introduce a novel hypergame on graph model and two solution concepts: stealthy deceptive sure winning and stealthy deceptive almost-sure winning. These identify states from which P1 can prevent P2 from reaching the target in a finite number of steps or with probability one without allowing P2 to become aware that it is being deceived. Consequently, determining the optimal decoy placement corresponds to maximizing the size of P1's deceptive winning region. Considering the combinatorial complexity of exploring all decoy allocations, we utilize compositional synthesis concepts to show that the objective function for decoy placement is monotone, non-decreasing, and, in certain cases, sub- or super-modular. This leads to a greedy algorithm for decoy placement, achieving a $(1 - 1/e)$-approximation when the objective function is sub- or super-modular. The proposed hypergame model and solution concepts contribute to understanding the optimal deception resource allocation and deception strategies in various security applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14436v1</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhishek N. Kulkarni, Matthew S. Cohen, Charles A. Kamhoua, Jie Fu</dc:creator>
    </item>
    <item>
      <title>On sybil-proof mechanisms</title>
      <link>https://arxiv.org/abs/2407.14485</link>
      <description>arXiv:2407.14485v2 Announce Type: new 
Abstract: We show that in the single-parameter mechanism design environment, the only non-wasteful, symmetric, incentive compatible and sybil-proof mechanism is a second price auction with symmetric tie-breaking. Thus, if there is private information, lotteries or other mechanisms that do not always allocate to a highest-value bidder are not sybil-proof or not incentive compatible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14485v2</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minghao Pan, Akaki Mamageishvili, Christoph Schlegel</dc:creator>
    </item>
    <item>
      <title>User-Creator Feature Dynamics in Recommender Systems with Dual Influence</title>
      <link>https://arxiv.org/abs/2407.14094</link>
      <description>arXiv:2407.14094v1 Announce Type: cross 
Abstract: Recommender systems present relevant contents to users and help content creators reach their target audience. The dual nature of these systems influences both users and creators: users' preferences are affected by the items they are recommended, while creators are incentivized to alter their contents such that it is recommended more frequently. We define a model, called user-creator feature dynamics, to capture the dual influences of recommender systems. We prove that a recommender system with dual influence is guaranteed to polarize, causing diversity loss in the system. We then investigate, both theoretically and empirically, approaches for mitigating polarization and promoting diversity in recommender systems. Unexpectedly, we find that common diversity-promoting approaches do not work in the presence of dual influence, while relevancy-optimizing methods like top-$k$ recommendation can prevent polarization and improve diversity of the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14094v1</guid>
      <category>cs.IR</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tao Lin, Kun Jin, Andrew Estornell, Xiaoying Zhang, Yiling Chen, Yang Liu</dc:creator>
    </item>
    <item>
      <title>Stackelberg POMDP: A Reinforcement Learning Approach for Economic Design</title>
      <link>https://arxiv.org/abs/2210.03852</link>
      <description>arXiv:2210.03852v4 Announce Type: replace 
Abstract: We introduce a reinforcement learning framework for economic design where the interaction between the environment designer and the participants is modeled as a Stackelberg game. In this game, the designer (leader) sets up the rules of the economic system, while the participants (followers) respond strategically. We integrate algorithms for determining followers' response strategies into the leader's learning environment, providing a formulation of the leader's learning problem as a POMDP that we call the Stackelberg POMDP. We prove that the optimal leader's strategy in the Stackelberg game is the optimal policy in our Stackelberg POMDP under a limited set of possible policies, establishing a connection between solving POMDPs and Stackelberg games. We solve our POMDP under a limited set of policy options via the centralized training with decentralized execution framework. For the specific case of followers that are modeled as no-regret learners, we solve an array of increasingly complex settings, including problems of indirect mechanism design where there is turn-taking and limited communication by agents. We demonstrate the effectiveness of our training framework through ablation studies. We also give convergence results for no-regret learners to a Bayesian version of a coarse-correlated equilibrium, extending known results to correlated types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.03852v4</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gianluca Brero, Alon Eden, Darshan Chakrabarti, Matthias Gerstgrasser, Amy Greenwald, Vincent Li, David C. Parkes</dc:creator>
    </item>
    <item>
      <title>Optimal Strategies in Ranked Choice Voting</title>
      <link>https://arxiv.org/abs/2407.13661</link>
      <description>arXiv:2407.13661v2 Announce Type: replace 
Abstract: Ranked Choice Voting (RCV) and Single Transferable Voting (STV) are widely valued; but are complex to understand due to intricate per-round vote transfers. Questions like determining how far a candidate is from winning or identifying effective election strategies are computationally challenging as minor changes in voter rankings can lead to significant ripple effects - for example, lending support to a losing candidate can prevent their votes from transferring to a more competitive opponent. We study optimal strategies - persuading voters to change their ballots or adding new voters - both algorithmically and theoretically. Algorithmically, we develop efficient methods to reduce election instances while maintaining optimization accuracy, effectively circumventing the computational complexity barrier. Theoretically, we analyze the effectiveness of strategies under both perfect and imperfect polling information. Our algorithmic approach applies to the ranked-choice polling data on the US 2024 Republican Primary, finding, for example, that several candidates would have been optimally served by boosting another candidate instead of themselves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13661v2</guid>
      <category>cs.GT</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanyukta Deshpande, Nikhil Garg, Sheldon Jacobson</dc:creator>
    </item>
    <item>
      <title>Learning in Repeated Interactions on Networks</title>
      <link>https://arxiv.org/abs/2112.14265</link>
      <description>arXiv:2112.14265v5 Announce Type: replace-cross 
Abstract: We study how long-lived, rational agents learn in a social network. In every period, after observing the past actions of his neighbors, each agent receives a private signal, and chooses an action whose payoff depends only on the state. Since equilibrium actions depend on higher order beliefs, it is difficult to characterize behavior. Nevertheless, we show that regardless of the size and shape of the network, the utility function, and the patience of the agents, the speed of learning in any equilibrium is bounded from above by a constant that only depends on the private signal distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.14265v5</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wanying Huang, Philipp Strack, Omer Tamuz</dc:creator>
    </item>
    <item>
      <title>Algorithmically Fair Maximization of Multiple Submodular Objective Functions</title>
      <link>https://arxiv.org/abs/2402.15155</link>
      <description>arXiv:2402.15155v2 Announce Type: replace-cross 
Abstract: Constrained maximization of submodular functions poses a central problem in combinatorial optimization. In many realistic scenarios, a number of agents need to maximize multiple submodular objectives over the same ground set. We study such a setting, where the different solutions must be disjoint, and thus, questions of algorithmic fairness arise. Inspired from the fair division literature, we suggest a simple round-robin protocol, where agents are allowed to build their solutions one item at a time by taking turns. Unlike what is typical in fair division, however, the prime goal here is to provide a fair algorithmic environment; each agent is allowed to use any algorithm for constructing their respective solutions. We show that just by following simple greedy policies, agents have solid guarantees for both monotone and non-monotone objectives, and for combinatorial constraints as general as $p$-systems (which capture cardinality and matroid intersection constraints). In the monotone case, our results include approximate EF1-type guarantees and their implications in fair division may be of independent interest. Further, although following a greedy policy may not be optimal in general, we show that consistently performing better than that is computationally hard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15155v2</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Georgios Amanatidis, Georgios Birmpas, Philip Lazos, Stefano Leonardi, Rebecca Reiffenh\"auser</dc:creator>
    </item>
    <item>
      <title>Some Characterizations of TTC in Multiple-Object Reallocation Problems</title>
      <link>https://arxiv.org/abs/2404.04822</link>
      <description>arXiv:2404.04822v3 Announce Type: replace-cross 
Abstract: This paper considers exchange of indivisible objects when agents are endowed with and can consume any bundles. We focus on efficient allocation rules that satisfy a novel participation requirement, the weak endowment lower bound, and which defend against simple manipulation heuristics: drop strategies and truncation strategies. Based on these properties, we obtain characterizations of a generalized version of Top Trading Cycles (TTC) on several domains. On the lexicographic and conditionally lexicographic domains, TTC is characterized by Pareto efficiency, balancedness, the weak endowment lower bound, and truncation-proofness (or drop strategy-proofness). On the domain of responsive preferences, similar characterizations are obtained by restricting attention to rules that are ``individual-good-based'' and weakening Pareto efficiency to individual-good efficiency. For the Shapley-Scarf model, TTC is characterized by Pareto efficiency, individual rationality, and truncation-proofness. The lexicographic and conditionally lexicographic domains are maximal domains on which Pareto efficiency coincides with individual-good efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04822v3</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob Coreno, Di Feng</dc:creator>
    </item>
  </channel>
</rss>
