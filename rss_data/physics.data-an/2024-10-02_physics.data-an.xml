<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Oct 2024 02:10:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Statistics of drops generated from ensembles of randomly corrugated ligaments</title>
      <link>https://arxiv.org/abs/2106.16192</link>
      <description>arXiv:2106.16192v4 Announce Type: replace-cross 
Abstract: The size of drops generated by the capillary-driven disintegration of liquid ligaments plays a fundamental role in several important natural phenomena, ranging from heat and mass transfer at the ocean-atmosphere interface to pathogen transmission. The inherent non-linearity of the equations governing the ligament destabilization leads to significant differences in the resulting drop sizes, owing to small fluctuations in the myriad initial conditions. Previous experiments and simulations reveal a variety of drop size distributions, corresponding to competing underlying physical interpretations. Here, we perform numerical simulations of individual ligaments, the deterministic breakup of which is triggered by random initial surface corrugations. The simulations are grouped in a large ensemble, each corresponding to a random initial configuration. The resulting probability distributions reveal three stable drop sizes, generated via a sequence of two distinct stages of breakup. Four different distributions are tested, volume-based Poisson, Gaussian, Gamma and Log-Normal. Depending on the time, range of droplet sizes and criteria for success, each distribution has successes and failures. However the Log-Normal distribution roughly describes the data when fitting both the primary peak and the tail of the distribution while the number of droplets generated is the highest, while the Gamma and Log-Normal distributions perform equally well when fitting the tail. The study demonstrates a precisely controllable and reproducible framework, which can be employed to investigate the mechanisms responsible for the polydispersity of drop sizes found in complex fluid fragmentation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.16192v4</guid>
      <category>physics.flu-dyn</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sagar Pal, Cesar Pairetti, Marco Crialesi-Esposito, Daniel Fuster, St\'ephane Zaleski</dc:creator>
    </item>
    <item>
      <title>Statistical signatures of abstraction in deep neural networks</title>
      <link>https://arxiv.org/abs/2407.01656</link>
      <description>arXiv:2407.01656v2 Announce Type: replace-cross 
Abstract: We study how abstract representations emerge in a Deep Belief Network (DBN) trained on benchmark datasets. Our analysis targets the principles of learning in the early stages of information processing, starting from the "primordial soup" of the under-sampling regime. As the data is processed by deeper and deeper layers, features are detected and removed, transferring more and more "context-invariant" information to deeper layers. We show that the representation approaches an universal model -- the Hierarchical Feature Model (HFM) -- determined by the principle of maximal relevance. Relevance quantifies the uncertainty on the model of the data, thus suggesting that "meaning" -- i.e. syntactic information -- is that part of the data which is not yet captured by a model. Our analysis shows that shallow layers are well described by pairwise Ising models, which provide a representation of the data in terms of generic, low order features. We also show that plasticity increases with depth, in a similar way as it does in the brain. These findings suggest that DBNs are capable of extracting a hierarchy of features from the data which is consistent with the principle of maximal relevance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01656v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Wed, 02 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlo Orientale Caputo, Matteo Marsili</dc:creator>
    </item>
  </channel>
</rss>
