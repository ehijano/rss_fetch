<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Nov 2024 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Goetterfunke: Creativity in Machinae Sapiens. About the Qualitative Shift in Generative AI with a Focus of Text-To-Image</title>
      <link>https://arxiv.org/abs/2411.10448</link>
      <description>arXiv:2411.10448v1 Announce Type: new 
Abstract: The year 2022 marks a watershed in technology, and arguably in human history, with the release of powerful generative AIs capable of convincingly performing creative tasks. With the help of these systems, anyone can create something that would previously have been considered a remarkable work of art. In human-AI collaboration, the computer seems to have become more than a tool. Many who have made their first contact with current generative AIs see them as "creativity machines" while for others the term "machine creativity" remains an oxymoron. This article is about (the possibility of) creativity in computers within the current Machine Learning paradigm. It outlines some of the key concepts behind the technologies and the innovations that have contributed to this qualitative shift, with a focus on text-to-image systems. The nature of Artificial Creativity as such is discussed, as well as what this might mean for art. AI may become a responsible collaborator with elements of independent machine authorship in the artistic process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10448v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jens Knappe</dc:creator>
    </item>
    <item>
      <title>Love in Action: Gamifying Public Video Cameras for Fostering Social Relationships in Real World</title>
      <link>https://arxiv.org/abs/2411.10449</link>
      <description>arXiv:2411.10449v1 Announce Type: new 
Abstract: In this paper, we create "Love in Action" (LIA), a body language-based social game utilizing video cameras installed in public spaces to enhance social relationships in real-world. In the game, participants assume dual roles, i.e., requesters, who issue social requests, and performers, who respond social requests through performing specified body languages. To mediate the communication between participants, we build an AI-enhanced video analysis system incorporating multiple visual analysis modules like person detection, attribute recognition, and action recognition, to assess the performer's body language quality. A two-week field study involving 27 participants shows significant improvements in their social friendships, as indicated by self-reported questionnaires. Moreover, user experiences are investigated to highlight the potential of public video cameras as a novel communication medium for socializing in public spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10449v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhang Zhang, Da Li, Geng Wu, Yaoning Li, Xiaobing Sun, Liang Wang</dc:creator>
    </item>
    <item>
      <title>Directing Avatars in Live Performances -- an Autonomy Simulacrum of Virtual Entities</title>
      <link>https://arxiv.org/abs/2411.10452</link>
      <description>arXiv:2411.10452v1 Announce Type: new 
Abstract: We propose to review the main stages in the computer history of virtual actors, with a view to the exploration of virtual reality and discussion on different approaches to human simulation. The notion of autonomy emerges as a key issue for the virtual entities. We then explore one way of building elements of autonomy and conclude with an example of avatar stage direction leading to a simulacrum of autonomy in a live performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10452v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>R{\'e}jane Dreifuss; Simon Hagemann; Izabella Pluta. Live Performance and Video Games, transcript Verlag, 2024, 978-3-8376-7173-5</arxiv:journal_reference>
      <dc:creator>Georges Gagner\'e (INREV, AIAC)</dc:creator>
    </item>
    <item>
      <title>Biotic Browser: Applying StreamingLLM as a Persistent Web Browsing Co-Pilot</title>
      <link>https://arxiv.org/abs/2411.10454</link>
      <description>arXiv:2411.10454v1 Announce Type: new 
Abstract: This paper presents "Biotic Browser," an innovative AI assistant leveraging StreamingLLM to transform web navigation and task execution. Characterized by its ability to simulate the experience of a passenger in an autonomous vehicle, the Biotic Browser excels in managing extended interactions and complex, multi-step web-based tasks. It marks a significant advancement in AI technology, particularly in the realm of long-term context management, and offers promising applications for enhancing productivity and efficiency in both personal and professional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10454v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kevin F. Dunnell, Andrew P. Stoddard</dc:creator>
    </item>
    <item>
      <title>Utilizing Human Behavior Modeling to Manipulate Explanations in AI-Assisted Decision Making: The Good, the Bad, and the Scary</title>
      <link>https://arxiv.org/abs/2411.10461</link>
      <description>arXiv:2411.10461v1 Announce Type: new 
Abstract: Recent advances in AI models have increased the integration of AI-based decision aids into the human decision making process. To fully unlock the potential of AI-assisted decision making, researchers have computationally modeled how humans incorporate AI recommendations into their final decisions, and utilized these models to improve human-AI team performance. Meanwhile, due to the ``black-box'' nature of AI models, providing AI explanations to human decision makers to help them rely on AI recommendations more appropriately has become a common practice. In this paper, we explore whether we can quantitatively model how humans integrate both AI recommendations and explanations into their decision process, and whether this quantitative understanding of human behavior from the learned model can be utilized to manipulate AI explanations, thereby nudging individuals towards making targeted decisions. Our extensive human experiments across various tasks demonstrate that human behavior can be easily influenced by these manipulated explanations towards targeted outcomes, regardless of the intent being adversarial or benign. Furthermore, individuals often fail to detect any anomalies in these explanations, despite their decisions being affected by them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10461v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuoyan Li, Ming Yin</dc:creator>
    </item>
    <item>
      <title>Gamification and AI: Enhancing User Engagement through Intelligent Systems</title>
      <link>https://arxiv.org/abs/2411.10462</link>
      <description>arXiv:2411.10462v1 Announce Type: new 
Abstract: Gamification applies game mechanics to non-game environments to motivate and engage users. Artificial Intelligence (AI) offers powerful tools for personalizing and optimizing gamification, adapting to users' needs, preferences, and performance levels. By integrating AI with gamification, systems can dynamically adjust game mechanics, deliver personalized feedback, and predict user behavior, significantly enhancing the effectiveness of gamification efforts. This paper examines the intersection of gamification and AI, exploring AI's methods to optimize gamified experiences and proposing mathematical models for adaptive and predictive gamification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10462v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos J. Costa, Joao Tiago Aparicio, Manuela Aparicio, Sofia Aparicio</dc:creator>
    </item>
    <item>
      <title>Unexploited Information Value in Human-AI Collaboration</title>
      <link>https://arxiv.org/abs/2411.10463</link>
      <description>arXiv:2411.10463v1 Announce Type: new 
Abstract: Humans and AIs are often paired on decision tasks with the expectation of achieving complementary performance -- where the combination of human and AI outperforms either one alone. However, how to improve performance of a human-AI team is often not clear without knowing more about what particular information and strategies each agent employs. In this paper, we propose a model based in statistically decision theory to analyze human-AI collaboration from the perspective of what information could be used to improve a human or AI decision. We demonstrate our model on a deepfake detection task to investigate seven video-level features by their unexploited value of information. We compare the human alone, AI alone and human-AI team and offer insights on how the AI assistance impacts people's usage of the information and what information that the AI exploits well might be useful for improving human decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10463v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyang Guo, Yifan Wu, Jason Hartline, Jessica Hullman</dc:creator>
    </item>
    <item>
      <title>Detecting Student Disengagement in Online Classes Using Deep Learning: A Review</title>
      <link>https://arxiv.org/abs/2411.10464</link>
      <description>arXiv:2411.10464v1 Announce Type: new 
Abstract: Student disengagement in online learning has become a critical challenge, particularly post-pandemic. This review explores deep learning techniques used to detect disengagement, emphasizing computer vision and affective computing as effective approaches. We examine recent studies focusing on facial expressions, eye movements, and posture to assess student attention, along with non-face-based indicators like mouse activity. A systematic review of 38 selected studies outlines the indicators, methods, and models employed in this field, providing insights for future research on real-time engagement monitoring in online classrooms</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10464v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmed Mohamed, Mostafa Ali, Shahd Ahmed, Nouran Hani, Mohammed Hisham, Meram Mahmoud</dc:creator>
    </item>
    <item>
      <title>MICA: Medical Intelligent Conversational AgentHow to optimize medical teleconsultations for sports patients via a conversational agent?</title>
      <link>https://arxiv.org/abs/2411.10465</link>
      <description>arXiv:2411.10465v1 Announce Type: new 
Abstract: The coronavirus crisis has increased medical teleconsultations (in France) by 10 times between 2019 and the first 3 months of 2020. However, the consultation and teleconsultation process have remained much the same. To improve the patient pathway and the efficiency of a session, we have developed a conversational assistant to optimize the time spent with the patient, the quality of the information transmitted to the physicians and thus the accuracy of diagnosis. One of the chatbot's interests is to give the physicians at the beginning of the consultation key elements for further inquiry. In this way, the system aims to help the patients concentrate and well describe their problem, which leads to more meaningful interactions with physicians and helps to enable them to operate more efficiently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10465v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laurent Cervoni, Tonie Fares, Mehdi Roudesli</dc:creator>
    </item>
    <item>
      <title>Prolegomena to a Post-Aesthetics of Artificial Imaginations</title>
      <link>https://arxiv.org/abs/2411.10467</link>
      <description>arXiv:2411.10467v1 Announce Type: new 
Abstract: The acceleration of the use of generative artificial intelligences (AI), since 2015 and the turning point operated by Deepdream, tends to obscure a real analysis of what could be defined as artificial imagination. AIs are either reduced to simple instruments or thought of according to a form of techno-theologism. Our research tends to suspend any form of judgment in order to phenomenally grasp the emergence of these AIs. By taking up the question of Hegel's aesthetics and of art as the free production of the mind, but by moving it towards the question of generative AIs and therefore of a post-aesthetics, this article will show the phenomenal specificity of images generatedby AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10467v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippe Boisnard (CITU)</dc:creator>
    </item>
    <item>
      <title>User-wise Perturbations for User Identity Protection in EEG-Based BCIs</title>
      <link>https://arxiv.org/abs/2411.10469</link>
      <description>arXiv:2411.10469v1 Announce Type: new 
Abstract: Objective: An electroencephalogram (EEG)-based brain-computer interface (BCI) is a direct communication pathway between the human brain and a computer. Most research so far studied more accurate BCIs, but much less attention has been paid to the ethics of BCIs. Aside from task-specific information, EEG signals also contain rich private information, e.g., user identity, emotion, disorders, etc., which should be protected. Approach: We show for the first time that adding user-wise perturbations can make identity information in EEG unlearnable. We propose four types of user-wise privacy-preserving perturbations, i.e., random noise, synthetic noise, error minimization noise, and error maximization noise. After adding the proposed perturbations to EEG training data, the user identity information in the data becomes unlearnable, while the BCI task information remains unaffected. Main results: Experiments on six EEG datasets using three neural network classifiers and various traditional machine learning models demonstrated the robustness and practicability of the proposed perturbations. Significance: Our research shows the feasibility of hiding user identity information in EEG data without impacting the primary BCI task information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10469v1</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoqing Chen, Siyang Li, Yunlu Tu, Ziwei Wang, Dongrui Wu</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Review on the Advancement of Home Automation System</title>
      <link>https://arxiv.org/abs/2411.10472</link>
      <description>arXiv:2411.10472v1 Announce Type: new 
Abstract: In light of its many benefits, home automation systems are one of the subjects that are becoming ever more prevalent. The term "home automation" describes the remote monitoring and management of household equipment. The Internet and its usages are constantly expanding, which means there is a lot of room for remote access, management, and surveillance of these network-enabled systems. Nowadays, scientists and researchers are developing cutting-edge prototypes of home automation system which includes smart lighting system, smart kitchen, smart fire protection devices, smart lawn mower, smart health monitor system and so on. Each automated system's primary objective is its ability to reduce human labor, effort, time, and mistakes brought on by carelessness. The objective of this study is to provide in-depth evaluation of the newly developed home automation system. Moreover, state-of-the-art home automation topologies such as ZigBee, Z-wave, Wi-Fi and Bluetooth are also discussed here. The authors are optimistic that this study would have a major impact on the present advances in home automation technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10472v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Md. Rawshan Habib, Md Abu Yusuf, W. M. H Nimsara Warnasuriya, Kumar Sunny, Mohammed Mahbubur Rahaman, Md Rezaul Karim Khan, Partha Pratim Saha, Mohammad Tanzimul Alam</dc:creator>
    </item>
    <item>
      <title>PhDGPT: Introducing a psychometric and linguistic dataset about how large language models perceive graduate students and professors in psychology</title>
      <link>https://arxiv.org/abs/2411.10473</link>
      <description>arXiv:2411.10473v1 Announce Type: new 
Abstract: Machine psychology aims to reconstruct the mindset of Large Language Models (LLMs), i.e. how these artificial intelligences perceive and associate ideas. This work introduces PhDGPT, a prompting framework and synthetic dataset that encapsulates the machine psychology of PhD researchers and professors as perceived by OpenAI's GPT-3.5. The dataset consists of 756,000 datapoints, counting 300 iterations repeated across 15 academic events, 2 biological genders, 2 career levels and 42 unique item responses of the Depression, Anxiety, and Stress Scale (DASS-42). PhDGPT integrates these psychometric scores with their explanations in plain language. This synergy of scores and texts offers a dual, comprehensive perspective on the emotional well-being of simulated academics, e.g. male/female PhD students or professors. By combining network psychometrics and psycholinguistic dimensions, this study identifies several similarities and distinctions between human and LLM data. The psychometric networks of simulated male professors do not differ between physical and emotional anxiety subscales, unlike humans. Other LLMs' personification can reconstruct human DASS factors with a purity up to 80%. Furthemore, LLM-generated personifications across different scenarios are found to elicit explanations lower in concreteness and imageability in items coding for anxiety, in agreement with past studies about human psychology. Our findings indicate an advanced yet incomplete ability for LLMs to reproduce the complexity of human psychometric data, unveiling convenient advantages and limitations in using LLMs to replace human participants. PhDGPT also intriguingly capture the ability for LLMs to adapt and change language patterns according to prompted mental distress contextual features, opening new quantitative opportunities for assessing the machine psychology of these artificial intelligences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10473v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edoardo Sebastiano De Duro, Enrique Taietta, Riccardo Improta, Massimo Stella</dc:creator>
    </item>
    <item>
      <title>Correcting User Decisions Based on Incorrect Machine Learning Decisions</title>
      <link>https://arxiv.org/abs/2411.10474</link>
      <description>arXiv:2411.10474v1 Announce Type: new 
Abstract: . It is typically assumed that for the successful use of machine learning algorithms, these algorithms should have a higher accuracy than a human expert. Moreover, if the average accuracy of ML algorithms is lower than that of a human expert, such algorithms should not be considered and are counter-productive. However, this is not always true. We provide strong statistical evidence that shows that even if a human expert is more accurate than a machine, an interaction with such a machine is beneficial when communication with the machine is non-public. The existence of a conflict between the user and ML model, and the private nature of user-AI communication will have the effect of making the user think about their decision and hence increase overall accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10474v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-54053-0_2</arxiv:DOI>
      <arxiv:journal_reference>Advances in Information and Communication Proceedings of the 2024 Future of Information and Communication Conference (FICC), Volume 3</arxiv:journal_reference>
      <dc:creator>Saveli Goldberg, Lev Salnikov, Noor Kaiser, Tushar Srivastava, Eugene Pinsky</dc:creator>
    </item>
    <item>
      <title>Beyond object identification: How train drivers evaluate the risk of collision</title>
      <link>https://arxiv.org/abs/2411.10475</link>
      <description>arXiv:2411.10475v1 Announce Type: new 
Abstract: When trains collide with obstacles, the consequences are often severe. To assess how artificial intelligence might contribute to avoiding collisions, we need to understand how train drivers do it. What aspects of a situation do they consider when evaluating the risk of collision? In the present study, we assumed that train drivers do not only identify potential obstacles but interpret what they see in order to anticipate how the situation might unfold. However, to date it is unclear how exactly this is accomplished. Therefore, we assessed which cues train drivers use and what inferences they make. To this end, image-based expert interviews were conducted with 33 train drivers. Participants saw images with potential obstacles, rated the risk of collision, and explained their evaluation. Moreover, they were asked how the situation would need to change to decrease or increase collision risk. From their verbal reports, we extracted concepts about the potential obstacles, contexts, or consequences, and assigned these concepts to various categories (e.g., people's identity, location, movement, action, physical features, and mental states). The results revealed that especially for people, train drivers reason about their actions and mental states, and draw relations between concepts to make further inferences. These inferences systematically differ between situations. Our findings emphasise the need to understand train drivers' risk evaluation processes when aiming to enhance the safety of both human and automatic train operation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10475v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Romy M\"uller, Judith Schmidt</dc:creator>
    </item>
    <item>
      <title>The Noisy Work of Uncertainty Visualisation Research: A Review</title>
      <link>https://arxiv.org/abs/2411.10482</link>
      <description>arXiv:2411.10482v1 Announce Type: new 
Abstract: Uncertainty visualisation is quickly becomming a hot topic in information visualisation. Exisiting reviews in the field take the definition and purpose of an uncertainty visualisation to be self evident which results in a large amout of conflicting information. This conflict largely stems from a conflation between uncertainty visualisations designed for decision making and those designed to prevent false conclusions. We coin the term "signal suppression" to describe a visualisation that is designed for preventing false conclusions, as the approach demands that the signal (i.e. the collective take away of the estimates) is suppressed by the noise (i.e. the variance on those estimates). We argue that the current standards in visualisation suggest that uncertainty visualisations designed for decision making should not be considered uncertainty visualisations at all. Therefore, future work should focus on signal suppression. Effective signal suppression requires us to communicate the signal and the noise as a single "validity of signal" variable, and doing so proves to be difficult with current methods. We illustrate current approaches to uncertainty visualisation by showing how they would change the visual apprearance of a choropleth map. These maps allow us to see why some methods succeed at signal suppression, while others fall short. Evaluating visualisations on how well they perform signal suppression also proves to be difficult, as it involves measuring the effect of noise, a variable we typically try to ignore. We suggest authors use qualitative studies or compare uncertainty visualisations to the relevant hypothesis tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10482v1</guid>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harriet Mason, Dianne Cook, Sarah Goodwin, Emi Tanaka, Susan Vanderplus</dc:creator>
    </item>
    <item>
      <title>iFlow: An Interactive Max-Flow/Min-Cut Algorithms Visualizer</title>
      <link>https://arxiv.org/abs/2411.10484</link>
      <description>arXiv:2411.10484v1 Announce Type: new 
Abstract: The Max-Flow/Min-Cut problem is a fundamental tool in graph theory, with applications in many domains, including data mining, image segmentation, transportation planning, and many types of assignment problems, in addition to being an essential building block for many other algorithms. The Ford-Fulkerson Algorithm for Max-Flow/Min-Cut and its variants are therefore commonly taught in undergraduate and beginning graduate algorithms classes. However, these algorithms -- and in particular the so-called residual graphs they utilize -- often pose significant challenges for students.
  To help students achieve a deeper understanding, we developed iFlow, an interactive visualization tool for the Ford-Fulkerson Algorithm and its variants. iFlow lets users design or import flow networks, and execute the algorithm by hand. In particular, the user can select an augmentation path and amount, and then update the residual graph. The user is given detailed feedback on mistakes, and can also have iFlow auto-complete each step, to use it as a demonstration tool while still in the initial learning stages. iFlow has been made publicly available and open-sourced.
  We deployed iFlow in an undergraduate algorithms class, and collected students' self-reported learning benefits via an optional survey. All respondents considered the tool at least somewhat useful and engaging, with most rating it either as useful/engaging or very useful/engaging. Students also generally reported a significant increase in understanding of the algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10484v1</guid>
      <category>cs.HC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muyang Ye, Tianrui Xia, Tianxin Zu, Qian Wang, David Kempe</dc:creator>
    </item>
    <item>
      <title>Gait Kinematics in Healthy Participants: A Motion Capture Dataset Under Weight Load and Knee Brace Conditions</title>
      <link>https://arxiv.org/abs/2411.10485</link>
      <description>arXiv:2411.10485v1 Announce Type: new 
Abstract: The objective assessment of gait kinematics is crucial in evaluating human movement, informing clinical decisions, and advancing rehabilitation and assistive technologies. Assessing gait symmetry, in particular, holds significant importance in clinical rehabilitation, as it reflects the intricate coordination between nerves and muscles during human walking. In this research, a dataset has been compiled to improve the understanding of gait kinematics. The dataset encompasses motion capture data of the walking patterns of eleven healthy participants who were tasked with completing various activities on a circular path. These activities included normal walking, walking with a weighted dominant hand, walking with a braced dominant leg, and walking with both weight and brace. The walking tasks involving weight and brace were designed to emulate the asymmetry associated with common health conditions, shedding light on irregularities in individuals' walking patterns and reflecting the coordination between nerves and muscles. All tasks were performed at regular and fast speeds, offering valuable insights into upper and lower body kinematics. The dataset comprises raw sensor data, providing information on joint dynamics, angular velocities, and orientation changes during walking, as well as analyzed data, including processed data, Euler angles, and joint kinematics spanning various body segments. This dataset will serve as a valuable resource for researchers, clinicians, and engineers, facilitating the analysis of gait patterns and extracting relevant indices on mobility and balance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10485v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hanieh Moradi, Yas Vaseghi, Arash Abbasi Larki, Akram Shojaei, Mehdi Delrobaei</dc:creator>
    </item>
    <item>
      <title>The Future of Skill: What Is It to Be Skilled at Work?</title>
      <link>https://arxiv.org/abs/2411.10488</link>
      <description>arXiv:2411.10488v1 Announce Type: new 
Abstract: In this short paper, we introduce work that is aiming to purposefully venture into this mesh of questions from a different starting point. Interjecting into the conversation, we want to ask: 'What is it to be skilled at work?' Building on work from scholars like Tim Ingold, and strands of longstanding research in workplace studies and CSCW, our interest is in turning the attention to the active work of 'being good', or 'being skilled', at what we as workers do. As we see it, skill provides a counterpoint to the version of intelligence that appears to be easily blackboxed in systems like Slack, and that ultimately reduces much of what people do to work well together. To put it slightly differently, skill - as we will argue below - gives us a way into thinking about work as a much more entangled endeavour, unfolding through multiple and interweaving sets of practices, places, tools and collaborations. In this vein, designing for the future of work seems to be about much more than where work is done or how we might bolt on discrete containers of intelligence. More fruitful would be attending to how we succeed in threading so many entities together to do our jobs well - in 'coming to be skilled'.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10488v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Axel Niklasson, Sean Rintel, Stephann Makri, Alex Taylor</dc:creator>
    </item>
    <item>
      <title>AI-Spectra: A Visual Dashboard for Model Multiplicity to Enhance Informed and Transparent Decision-Making</title>
      <link>https://arxiv.org/abs/2411.10490</link>
      <description>arXiv:2411.10490v1 Announce Type: new 
Abstract: We present an approach, AI-Spectra, to leverage model multiplicity for interactive systems. Model multiplicity means using slightly different AI models yielding equally valid outcomes or predictions for the same task, thus relying on many simultaneous "expert advisors" that can have different opinions. Dealing with multiple AI models that generate potentially divergent results for the same task is challenging for users to deal with. It helps users understand and identify AI models are not always correct and might differ, but it can also result in an information overload when being confronted with multiple results instead of one. AI-Spectra leverages model multiplicity by using a visual dashboard designed for conveying what AI models generate which results while minimizing the cognitive effort to detect consensus among models and what type of models might have different opinions. We use a custom adaptation of Chernoff faces for AI-Spectra; Chernoff Bots. This visualization technique lets users quickly interpret complex, multivariate model configurations and compare predictions across multiple models. Our design is informed by building on established Human-AI Interaction guidelines and well know practices in information visualization. We validated our approach through a series of experiments training a wide variation of models with the MNIST dataset to perform number recognition. Our work contributes to the growing discourse on making AI systems more transparent, trustworthy, and effective through the strategic use of multiple models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10490v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Gilles Eerlings, Sebe Vanbrabant, Jori Liesenborgs, Gustavo Rovelo Ruiz, Davy Vanacken, Kris Luyten</dc:creator>
    </item>
    <item>
      <title>Chain of Alignment: Integrating Public Will with Expert Intelligence for Language Model Alignment</title>
      <link>https://arxiv.org/abs/2411.10534</link>
      <description>arXiv:2411.10534v1 Announce Type: new 
Abstract: We introduce a method to measure the alignment between public will and language model (LM) behavior that can be applied to fine-tuning, online oversight, and pre-release safety checks. Our `chain of alignment' (CoA) approach produces a rule based reward (RBR) by creating model behavior $\textit{rules}$ aligned to normative $\textit{objectives}$ aligned to $\textit{public will}$. This factoring enables a nonexpert public to directly specify their will through the normative objectives, while expert intelligence is used to figure out rules entailing model behavior that best achieves those objectives. We validate our approach by applying it across three different domains of LM prompts related to mental health. We demonstrate a public input process built on collective dialogues and bridging-based ranking that reliably produces normative objectives supported by at least $96\% \pm 2\%$ of the US public. We then show that rules developed by mental health experts to achieve those objectives enable a RBR that evaluates an LM response's alignment with the objectives similarly to human experts (Pearson's $r=0.841$, $AUC=0.964$). By measuring alignment with objectives that have near unanimous public support, these CoA RBRs provide an approximate measure of alignment between LM behavior and public will.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10534v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Konya, Aviv Ovadya, Kevin Feng, Quan Ze Chen, Lisa Schirch, Colin Irwin, Amy X. Zhang</dc:creator>
    </item>
    <item>
      <title>Pedagogical Design Considerations for Mobile Augmented Reality Serious Games (MARSGs): A Literature Review</title>
      <link>https://arxiv.org/abs/2411.10655</link>
      <description>arXiv:2411.10655v1 Announce Type: new 
Abstract: As technology advances, conceptualizations of effective strategies for teaching and learning shift. Due in part to their facilitation of unique affordances for learning, mobile devices, augmented reality, and games are all becoming more prominent elements in learning environments. In this work, we examine mobile augmented reality serious games (MARSGs) as the intersection of these technology-based experiences and to what effect their combination can yield even greater learning outcomes. We present a PRISMA review of 23 papers (from 610) spanning the entire literature timeline from 2002 to 2023. Among these works, there is wide variability in the realized application of game elements and pedagogical theories underpinning the game experience. For an educational tool to be effective, it must be designed to facilitate learning while anchored by pedagogical theory. Given that most MARSG developers are not pedagogical experts, this review further provides design considerations regarding which game elements might proffer the best of three major pedagogical theories for modern learning (cognitive constructivism, social constructivism, and behaviorism) based on existing applications. We will also briefly touch on radical constructivism and the instructional elements embedded within MARSGs. Lastly, this work offers a synthesis of current MARSG findings and extended future directions for MARSG development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10655v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.3390/electronics12214524</arxiv:DOI>
      <arxiv:journal_reference>Electron. - Spec. Issue Serious Games XR, vol. 12, no. 21, 2023</arxiv:journal_reference>
      <dc:creator>Cassidy R. Nelson, Joseph L. Gabbard</dc:creator>
    </item>
    <item>
      <title>EDBooks: AI-Enhanced Interactive Narratives for Programming Education</title>
      <link>https://arxiv.org/abs/2411.10687</link>
      <description>arXiv:2411.10687v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have shown the potential to be valuable teaching tools, with the potential of giving every student a personalized tutor. However, one challenge with using LLMs to learn new concepts is that when learning a topic in an unfamiliar domain, it can be difficult to know what questions to ask. Further, language models do not always encourage "active learning" where students can test and assess their understanding. In this paper, we propose ways to combine large language models with "traditional" learning materials (like e-books) to give readers the benefits of working with LLMs (the ability to ask personally interesting questions and receive personalized answers) with the benefits of a traditional e-book (having a structure and content that is pedagogically sound). This work shows one way that LLMs have the potential to improve learning materials and make personalized programming education more accessible to a broader audience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10687v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steve Oney, Yue Shen, Fei Wu, Young Suh Hong, Ziang Wang, Yamini Khajekar, Jiacheng Zhang, April Yi Wang</dc:creator>
    </item>
    <item>
      <title>A Monocular SLAM-based Multi-User Positioning System with Image Occlusion in Augmented Reality</title>
      <link>https://arxiv.org/abs/2411.10940</link>
      <description>arXiv:2411.10940v1 Announce Type: new 
Abstract: In recent years, with the rapid development of augmented reality (AR) technology, there is an increasing demand for multi-user collaborative experiences. Unlike for single-user experiences, ensuring the spatial localization of every user and maintaining synchronization and consistency of positioning and orientation across multiple users is a significant challenge. In this paper, we propose a multi-user localization system based on ORB-SLAM2 using monocular RGB images as a development platform based on the Unity 3D game engine. This system not only performs user localization but also places a common virtual object on a planar surface (such as table) in the environment so that every user holds a proper perspective view of the object. These generated virtual objects serve as reference points for multi-user position synchronization. The positioning information is passed among every user's AR devices via a central server, based on which the relative position and movement of other users in the space of a specific user are presented via virtual avatars all with respect to these virtual objects. In addition, we use deep learning techniques to estimate the depth map of an image from a single RGB image to solve occlusion problems in AR applications, making virtual objects appear more natural in AR scenes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10940v1</guid>
      <category>cs.HC</category>
      <category>cs.CV</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei-Hsiang Lien, Benedictus Kent Chandra, Robin Fischer, Ya-Hui Tang, Shiann-Jang Wang, Wei-En Hsu, Li-Chen Fu</dc:creator>
    </item>
    <item>
      <title>Exploring Device-Oriented Video Encryption for Hierarchical Privacy Protection in AR Content Sharing</title>
      <link>https://arxiv.org/abs/2411.10964</link>
      <description>arXiv:2411.10964v1 Announce Type: new 
Abstract: Content sharing across multiple Augmented Reality (AR) displays is becoming commonplace, enhancing team communication and collaboration through devices like smartphones and AR glasses. However, this practice raises significant privacy concerns, especially concerning the physical environment visible in AR, which may include sensitive personal details like facial features and identifiable information. Our research focuses on protecting privacy within AR environments, particularly the physical backgrounds visible during content sharing across three common AR display methods: projection, smartphone, and AR glasses. We analyze the potential privacy risks associated with each method and employ a Region Of Interest (ROI) video encryption system to hierarchically encrypt the physical backdrop based on its safety rating. This study pioneers the integration of ROI video encryption at the bitstream level within AR contexts, providing a more efficient solution than traditional pixel-level encryption by enhancing encryption speed and reducing the required space. Our adaptive system dynamically adjusts the encryption intensity based on the AR display method, ensuring tailored privacy protection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10964v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yongquan Hu, Dongsheng Zheng, Kexin Nie, Junyan Zhang, Wen Hu, Aaron Quigley</dc:creator>
    </item>
    <item>
      <title>Exploring the Impact of Non-Verbal Virtual Agent Behavior on User Engagement in Argumentative Dialogues</title>
      <link>https://arxiv.org/abs/2411.11102</link>
      <description>arXiv:2411.11102v1 Announce Type: new 
Abstract: Engaging in discussions that involve diverse perspectives and exchanging arguments on a controversial issue is a natural way for humans to form opinions. In this process, the way arguments are presented plays a crucial role in determining how engaged users are, whether the interaction takes place solely among humans or within human-agent teams. This is of great importance as user engagement plays a crucial role in determining the success or failure of cooperative argumentative discussions. One main goal is to maintain the user's motivation to participate in a reflective opinion-building process, even when addressing contradicting viewpoints. This work investigates how non-verbal agent behavior, specifically co-speech gestures, influences the user's engagement and interest during an ongoing argumentative interaction. The results of a laboratory study conducted with 56 participants demonstrate that the agent's co-speech gestures have a substantial impact on user engagement and interest and the overall perception of the system. Therefore, this research offers valuable insights for the design of future cooperative argumentative virtual agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11102v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3687272.3688315</arxiv:DOI>
      <dc:creator>Annalena Bea Aicher, Yuki Matsuda, Keichii Yasumoto, Wolfgang Minker, Elisabeth Andr\'e, Stefan Ultes</dc:creator>
    </item>
    <item>
      <title>Factors in Crowdsourcing for Evaluation of Complex Dialogue Systems</title>
      <link>https://arxiv.org/abs/2411.11137</link>
      <description>arXiv:2411.11137v1 Announce Type: new 
Abstract: In the last decade, crowdsourcing has become a popular method for conducting quantitative empirical studies in human-machine interaction. The remote work on a given task in crowdworking settings suits the character of typical speech/language-based interactive systems for instance with regard to argumentative conversations and information retrieval. Thus, crowdworking promises a valuable opportunity to study and evaluate the usability and user experience of real humans in interactions with such interactive systems. In contrast to physical attendance in laboratory studies, crowdsourcing studies offer much more flexible and easier access to large numbers of heterogeneous participants with a specific background, e.g., native speakers or domain expertise. On the other hand, the experimental and environmental conditions as well as the participant's compliance and reliability (at least better monitoring of the latter) are much better controllable in a laboratory.
  This paper seeks to present a (self-)critical examination of crowdsourcing-based studies in the context of complex (spoken) dialogue systems. It describes and discusses observed issues in crowdsourcing studies involving complex tasks and suggests solutions to improve and ensure the quality of the study results. Thereby, our work contributes to a better understanding and what needs to be considered when designing and evaluating studies with crowdworkers for complex dialogue systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11137v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Annalena Aicher, Stefan Hillmann, Isabel Feustel, Thilo Michael, Sebastian M\"oller, Wolfgang Minker</dc:creator>
    </item>
    <item>
      <title>From 2D Document Interactions into Immersive Information Experience: An Example-Based Design by Augmenting Content, Spatializing Placement, Enriching Long-Term Interactions, and Simplifying Content Creations</title>
      <link>https://arxiv.org/abs/2411.11145</link>
      <description>arXiv:2411.11145v1 Announce Type: new 
Abstract: Documents serve as a crucial and indispensable medium for everyday workplace tasks. However, understanding, interacting and creating such documents on today's planar interfaces without any intelligent support are challenging due to our natural cognitive constraints on remembering, processing, understanding and interacting with these information. My doctorate research investigates how to bring 2D document interactions into immersive information experience using multiple of today's emergent technologies. With the examples of four specific types of documents -- medical scans, instruction document, self-report diary survey, and reference images for visual artists -- my research demonstrates how to transform such of today's 2D document interactions into an immersive information experience, by augmenting content with virtual reality, spatializing document placements with mixed reality, enriching long-term and continuous interactions with voice assistants, and simplify document creation workflow with generative AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11145v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Chen Chen</dc:creator>
    </item>
    <item>
      <title>Towards Personalized Brain-Computer Interface Application Based on Endogenous EEG Paradigms</title>
      <link>https://arxiv.org/abs/2411.11302</link>
      <description>arXiv:2411.11302v1 Announce Type: new 
Abstract: In this paper, we propose a conceptual framework for personalized brain-computer interface (BCI) applications, which can offer an enhanced user experience by customizing services to individual preferences and needs, based on endogenous electroencephalography (EEG) paradigms including motor imagery (MI), speech imagery (SI), and visual imagery. The framework includes two essential components: user identification and intention classification, which enable personalized services by identifying individual users and recognizing their intended actions through EEG signals. We validate the feasibility of our framework using a private EEG dataset collected from eight subjects, employing the ShallowConvNet architecture to decode EEG features. The experimental results demonstrate that user identification achieved an average classification accuracy of 0.995, while intention classification achieved 0.47 accuracy across all paradigms, with MI demonstrating the best performance. These findings indicate that EEG signals can effectively support personalized BCI applications, offering robust identification and reliable intention decoding, especially for MI and SI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11302v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heon-Gyu Kwak, Gi-Hwan Shin, Yeon-Woo Choi, Dong-Hoon Lee, Yoo-In Jeon, Jun-Su Kang, Seong-Whan Lee</dc:creator>
    </item>
    <item>
      <title>Quantifying Haptic Affection of Car Door through Data-Driven Analysis of Force Profile</title>
      <link>https://arxiv.org/abs/2411.11382</link>
      <description>arXiv:2411.11382v1 Announce Type: new 
Abstract: Haptic affection plays a crucial role in user experience, particularly in the automotive industry where the tactile quality of components can influence customer satisfaction. This study aims to accurately predict the affective property of a car door by only watching the force or torque profile of it when opening. To this end, a deep learning model is designed to capture the underlying relationships between force profiles and user-defined adjective ratings, providing insights into the door-opening experience. The dataset employed in this research includes force profiles and user adjective ratings collected from six distinct car models, reflecting a diverse set of door-opening characteristics and tactile feedback. The model's performance is assessed using Leave-One-Out Cross-Validation, a method that measures its generalization capability on unseen data. The results demonstrate that the proposed model achieves a high level of prediction accuracy, indicating its potential in various applications related to haptic affection and design optimization in the automotive industry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11382v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Waseem Hassan, Mudassir Ibrahim Awan, Ahsan Raza, Ki-Uk Kyung, Seokhee Jeon</dc:creator>
    </item>
    <item>
      <title>Deliberative XAI: How Explanations Impact Understanding and Decision-Making of AI Novices in Collective and Individual Settings</title>
      <link>https://arxiv.org/abs/2411.11449</link>
      <description>arXiv:2411.11449v1 Announce Type: new 
Abstract: XAI research often focuses on settings where people learn about and assess algorithmic systems individually. However, as more public AI systems are deployed, it becomes essential for XAI to facilitate collective understanding and deliberation. We conducted a task-based interview study involving 8 focus groups and 12 individual interviews to explore how explanations can support AI novices in understanding and forming opinions about AI systems. Participants received a collection of explanations organized into four information categories to solve tasks and decide about a system's deployment. These explanations improved or calibrated participants' self-reported understanding and decision confidence and facilitated group discussions. Participants valued both technical and contextual information and the self-directed and modular explanation structure. Our contributions include an explanation approach that facilitates both individual and collaborative interaction and explanation design recommendations, including active and controllable exploration, different levels of information detail and breadth, and adaptations to the needs of decision subjects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11449v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timoth\'ee Schmude, Laura Koesten, Torsten M\"oller, Sebastian Tschiatschek</dc:creator>
    </item>
    <item>
      <title>Introducing IHARDS-CNN: A Cutting-Edge Deep Learning Method for Human Activity Recognition Using Wearable Sensors</title>
      <link>https://arxiv.org/abs/2411.11658</link>
      <description>arXiv:2411.11658v1 Announce Type: new 
Abstract: Human activity recognition, facilitated by smart devices, has recently garnered significant attention. Deep learning algorithms have become pivotal in daily activities, sports, and healthcare. Nevertheless, addressing the challenge of extracting features from sensor data processing necessitates the utilization of diverse algorithms in isolation, subsequently transforming them into a standard mode. This research introduces a novel approach called IHARDS-CNN, amalgamating data from three distinct datasets (UCI-HAR, WISDM, and KU-HAR) for human activity recognition. The data collected from sensors embedded in smartwatches or smartphones encompass five daily activity classes. This study initially outlines the dataset integration approach, follows with a comprehensive statistical analysis, and assesses dataset accuracy. The proposed methodology employs a one-dimensional deep convolutional neural network for classification. Compared to extant activity recognition methods, this approach stands out for its high speed, reduced detection steps, and absence of the need to aggregate classified results. Despite fewer detection steps, empirical results demonstrate an impressive accuracy of nearly 100%, marking it the highest among existing methods. Evaluation outcomes further highlight superior classification performance when compared to analogous architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11658v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nazanin Sedaghati, Masoud Kargar, Sina Abbaskhani</dc:creator>
    </item>
    <item>
      <title>sMoRe: Enhancing Object Manipulation and Organization in Mixed Reality Spaces with LLMs and Generative AI</title>
      <link>https://arxiv.org/abs/2411.11752</link>
      <description>arXiv:2411.11752v1 Announce Type: new 
Abstract: In mixed reality (MR) environments, understanding space and creating virtual objects is crucial to providing an intuitive and rich user experience. This paper introduces sMoRe (Spatial Mapping and Object Rendering Environment), an MR application that combines Generative AI (GenAI) with large language models (LLMs) to assist users in creating, placing, and managing virtual objects within physical spaces. sMoRe allows users to use voice or typed text commands to create and place virtual objects using GenAI while specifying spatial constraints. The system leverages LLMs to interpret users' commands, analyze the current scene, and identify optimal locations. Additionally, sMoRe integrates text-to-3D generative AI to dynamically create 3D objects based on users' descriptions. Our user study demonstrates the effectiveness of sMoRe in enhancing user comprehension, interaction, and organization of the MR environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11752v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunhao Xing, Que Liu, Jingwu Wang, Diego Gomez-Zara</dc:creator>
    </item>
    <item>
      <title>AdaptLIL: A Gaze-Adaptive Visualization for Ontology Mapping</title>
      <link>https://arxiv.org/abs/2411.11768</link>
      <description>arXiv:2411.11768v1 Announce Type: new 
Abstract: This paper showcases AdaptLIL, a real-time adaptive link-indented list ontology mapping visualization that uses eye gaze as the primary input source. Through a multimodal combination of real-time systems, deep learning, and web development applications, this system uniquely curtails graphical overlays (adaptations) to pairwise mappings of link-indented list ontology visualizations for individual users based solely on their eye gaze.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11768v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas Chow, Bo Fu</dc:creator>
    </item>
    <item>
      <title>Exploring the Requirements of Clinicians for Explainable AI Decision Support Systems in Intensive Care</title>
      <link>https://arxiv.org/abs/2411.11774</link>
      <description>arXiv:2411.11774v1 Announce Type: new 
Abstract: There is a growing need to understand how digital systems can support clinical decision-making, particularly as artificial intelligence (AI) models become increasingly complex and less human-interpretable. This complexity raises concerns about trustworthiness, impacting safe and effective adoption of such technologies. Improved understanding of decision-making processes and requirements for explanations coming from decision support tools is a vital component in providing effective explainable solutions. This is particularly relevant in the data-intensive, fast-paced environments of intensive care units (ICUs). To explore these issues, group interviews were conducted with seven ICU clinicians, representing various roles and experience levels. Thematic analysis revealed three core themes: (T1) ICU decision-making relies on a wide range of factors, (T2) the complexity of patient state is challenging for shared decision-making, and (T3) requirements and capabilities of AI decision support systems. We include design recommendations from clinical input, providing insights to inform future AI systems for intensive care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11774v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeffrey N. Clark, Matthew Wragg, Emily Nielsen, Miquel Perello-Nieto, Nawid Keshtmand, Michael Ambler, Shiv Sharma, Christopher P. Bourdeaux, Amberly Brigden, Raul Santos-Rodriguez</dc:creator>
    </item>
    <item>
      <title>Describe Now: User-Driven Audio Description for Blind and Low Vision Individuals</title>
      <link>https://arxiv.org/abs/2411.11835</link>
      <description>arXiv:2411.11835v1 Announce Type: new 
Abstract: Audio descriptions (AD) make videos accessible for blind and low vision (BLV) users by describing visual elements that cannot be understood from the main audio track. AD created by professionals or novice describers is time-consuming and lacks scalability while offering little control to BLV viewers on description length and content and when they receive it. To address this gap, we explore user-driven AI-generated descriptions, where the BLV viewer controls when they receive descriptions. In a study, 20 BLV participants activated audio descriptions for seven different video genres with two levels of detail: concise and detailed. Our results show differences in AD frequency and level of detail BLV users wanted for different videos, their sense of control with this style of AD delivery, its limitations, and variations among BLV users in their AD needs and perception of AI-generated descriptions. We discuss the implications of our findings for future AI-based AD tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11835v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maryam Cheema, Hasti Seifi, Pooyan Fazli</dc:creator>
    </item>
    <item>
      <title>Dataset Refinement for Improving the Generalization Ability of the EEG Decoding Model</title>
      <link>https://arxiv.org/abs/2411.10450</link>
      <description>arXiv:2411.10450v1 Announce Type: cross 
Abstract: Electroencephalography (EEG) is a generally used neuroimaging approach in brain-computer interfaces due to its non-invasive characteristics and convenience, making it an effective tool for understanding human intentions. Therefore, recent research has focused on decoding human intentions from EEG signals utilizing deep learning methods. However, since EEG signals are highly susceptible to noise during acquisition, there is a high possibility of the existence of noisy data in the dataset. Although pioneer studies have generally assumed that the dataset is well-curated, this assumption is not always met in the EEG dataset. In this paper, we addressed this issue by designing a dataset refinement algorithm that can eliminate noisy data based on metrics evaluating data influence during the training process. We applied the proposed algorithm to two motor imagery EEG public datasets and three different models to perform dataset refinement. The results indicated that retraining the model with the refined dataset consistently led to better generalization performance compared to using the original dataset. Hence, we demonstrated that removing noisy data from the training dataset alone can effectively improve the generalization performance of deep learning models in the EEG domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10450v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sung-Jin Kim, Dae-Hyeok Lee, Hyeon-Taek Han</dc:creator>
    </item>
    <item>
      <title>Improvement in Facial Emotion Recognition using Synthetic Data Generated by Diffusion Model</title>
      <link>https://arxiv.org/abs/2411.10863</link>
      <description>arXiv:2411.10863v1 Announce Type: cross 
Abstract: Facial Emotion Recognition (FER) plays a crucial role in computer vision, with significant applications in human-computer interaction, affective computing, and areas such as mental health monitoring and personalized learning environments. However, a major challenge in FER task is the class imbalance commonly found in available datasets, which can hinder both model performance and generalization. In this paper, we tackle the issue of data imbalance by incorporating synthetic data augmentation and leveraging the ResEmoteNet model to enhance the overall performance on facial emotion recognition task. We employed Stable Diffusion 2 and Stable Diffusion 3 Medium models to generate synthetic facial emotion data, augmenting the training sets of the FER2013 and RAF-DB benchmark datasets. Training ResEmoteNet with these augmented datasets resulted in substantial performance improvements, achieving accuracies of 96.47% on FER2013 and 99.23% on RAF-DB. These findings shows an absolute improvement of 16.68% in FER2013, 4.47% in RAF-DB and highlight the efficacy of synthetic data augmentation in strengthening FER models and underscore the potential of advanced generative models in FER research and applications. The source code for ResEmoteNet is available at https://github.com/ArnabKumarRoy02/ResEmoteNet</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10863v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>eess.IV</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnab Kumar Roy, Hemant Kumar Kathania, Adhitiya Sharma</dc:creator>
    </item>
    <item>
      <title>Large Language Models (LLMs) as Traffic Control Systems at Urban Intersections: A New Paradigm</title>
      <link>https://arxiv.org/abs/2411.10869</link>
      <description>arXiv:2411.10869v1 Announce Type: cross 
Abstract: This study introduces a novel approach for traffic control systems by using Large Language Models (LLMs) as traffic controllers. The study utilizes their logical reasoning, scene understanding, and decision-making capabilities to optimize throughput and provide feedback based on traffic conditions in real-time. LLMs centralize traditionally disconnected traffic control processes and can integrate traffic data from diverse sources to provide context-aware decisions. LLMs can also deliver tailored outputs using various means such as wireless signals and visuals to drivers, infrastructures, and autonomous vehicles. To evaluate LLMs ability as traffic controllers, this study proposed a four-stage methodology. The methodology includes data creation and environment initialization, prompt engineering, conflict identification, and fine-tuning. We simulated multi-lane four-leg intersection scenarios and generates detailed datasets to enable conflict detection using LLMs and Python simulation as a ground truth. We used chain-of-thought prompts to lead LLMs in understanding the context, detecting conflicts, resolving them using traffic rules, and delivering context-sensitive traffic management solutions. We evaluated the prformance GPT-mini, Gemini, and Llama as traffic controllers. Results showed that the fine-tuned GPT-mini achieved 83% accuracy and an F1-score of 0.84. GPT-mini model exhibited a promising performance in generating actionable traffic management insights, with high ROUGE-L scores across conflict identification of 0.95, decision-making of 0.91, priority assignment of 0.94, and waiting time optimization of 0.92. We demonstrated that LLMs can offer precise recommendations to drivers in real-time including yielding, slowing, or stopping based on vehicle dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10869v1</guid>
      <category>cs.CL</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sari Masri, Huthaifa I. Ashqar, Mohammed Elhenawy</dc:creator>
    </item>
    <item>
      <title>Improving User Experience in Preference-Based Optimization of Reward Functions for Assistive Robots</title>
      <link>https://arxiv.org/abs/2411.11182</link>
      <description>arXiv:2411.11182v1 Announce Type: cross 
Abstract: Assistive robots interact with humans and must adapt to different users' preferences to be effective. An easy and effective technique to learn non-expert users' preferences is through rankings of robot behaviors, for example, robot movement trajectories or gestures. Existing techniques focus on generating trajectories for users to rank that maximize the outcome of the preference learning process. However, the generated trajectories do not appear to reflect the user's preference over repeated interactions. In this work, we design an algorithm to generate trajectories for users to rank that we call Covariance Matrix Adaptation Evolution Strategies with Information Gain (CMA-ES-IG). CMA-ES-IG prioritizes the user's experience of the preference learning process. We show that users find our algorithm more intuitive and easier to use than previous approaches across both physical and social robot tasks. This project's code is hosted at github.com/interaction-lab/CMA-ES-IG</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11182v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Dennler, Zhonghao Shi, Stefanos Nikolaidis, Maja Matari\'c</dc:creator>
    </item>
    <item>
      <title>Investigating the Use of Productive Failure as a Design Paradigm for Learning Introductory Python Programming</title>
      <link>https://arxiv.org/abs/2411.11227</link>
      <description>arXiv:2411.11227v1 Announce Type: cross 
Abstract: Productive Failure (PF) is a learning approach where students initially tackle novel problems targeting concepts they have not yet learned, followed by a consolidation phase where these concepts are taught. Recent application in STEM disciplines suggests that PF can help learners develop more robust conceptual knowledge. However, empirical validation of PF for programming education remains under-explored. In this paper, we investigate the use of PF to teach Python lists to undergraduate students with limited prior programming experience. We designed a novel PF-based learning activity that incorporated the unobtrusive collection of real-time heart-rate data from consumer-grade wearable sensors. This sensor data was used both to make the learning activity engaging and to infer cognitive load. We evaluated our approach with 20 participants, half of whom were taught Python concepts using Direct Instruction (DI), and the other half with PF. We found that although there was no difference in initial learning outcomes between the groups, students who followed the PF approach showed better knowledge retention and performance on delayed but similar tasks. In addition, physiological measurements indicated that these students also exhibited a larger decrease in cognitive load during their tasks after instruction. Our findings suggest that PF-based approaches may lead to more robust learning, and that future work should investigate similar activities at scale across a range of concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11227v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3641554.3701911</arxiv:DOI>
      <dc:creator>Hussel Suriyaarachchi, Paul Denny, Suranga Nanayakkara</dc:creator>
    </item>
    <item>
      <title>Mapping out the Space of Human Feedback for Reinforcement Learning: A Conceptual Framework</title>
      <link>https://arxiv.org/abs/2411.11761</link>
      <description>arXiv:2411.11761v1 Announce Type: cross 
Abstract: Reinforcement Learning from Human feedback (RLHF) has become a powerful tool to fine-tune or train agentic machine learning models. Similar to how humans interact in social contexts, we can use many types of feedback to communicate our preferences, intentions, and knowledge to an RL agent. However, applications of human feedback in RL are often limited in scope and disregard human factors. In this work, we bridge the gap between machine learning and human-computer interaction efforts by developing a shared understanding of human feedback in interactive learning scenarios. We first introduce a taxonomy of feedback types for reward-based learning from human feedback based on nine key dimensions. Our taxonomy allows for unifying human-centered, interface-centered, and model-centered aspects. In addition, we identify seven quality metrics of human feedback influencing both the human ability to express feedback and the agent's ability to learn from the feedback. Based on the feedback taxonomy and quality criteria, we derive requirements and design choices for systems learning from human feedback. We relate these requirements and design choices to existing work in interactive machine learning. In the process, we identify gaps in existing work and future research opportunities. We call for interdisciplinary collaboration to harness the full potential of reinforcement learning with data-driven co-adaptive modeling and varied interaction mechanics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11761v1</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yannick Metz, David Lindner, Rapha\"el Baur, Mennatallah El-Assady</dc:creator>
    </item>
    <item>
      <title>UniHands: Unifying Various Wild-Collected Keypoints for Personalized Hand Reconstruction</title>
      <link>https://arxiv.org/abs/2411.11845</link>
      <description>arXiv:2411.11845v1 Announce Type: cross 
Abstract: Accurate hand motion capture and standardized 3D representation are essential for various hand-related tasks. Collecting keypoints-only data, while efficient and cost-effective, results in low-fidelity representations and lacks surface information. Furthermore, data inconsistencies across sources challenge their integration and use. We present UniHands, a novel method for creating standardized yet personalized hand models from wild-collected keypoints from diverse sources. Unlike existing neural implicit representation methods, UniHands uses the widely-adopted parametric models MANO and NIMBLE, providing a more scalable and versatile solution. It also derives unified hand joints from the meshes, which facilitates seamless integration into various hand-related tasks. Experiments on the FreiHAND and InterHand2.6M datasets demonstrate its ability to precisely reconstruct hand mesh vertices and keypoints, effectively capturing high-degree articulation motions. Empirical studies involving nine participants show a clear preference for our unified joints over existing configurations for accuracy and naturalism (p-value 0.016).</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11845v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Menghe Zhang, Joonyeoup Kim, Yangwen Liang, Shuangquan Wang, Kee-Bong Song</dc:creator>
    </item>
    <item>
      <title>fNIRS Analysis of Interaction Techniques in Touchscreen-Based Educational Gaming</title>
      <link>https://arxiv.org/abs/2405.08906</link>
      <description>arXiv:2405.08906v3 Announce Type: replace 
Abstract: Touchscreens are becoming increasingly widespread in educational games, enhancing the quality of learner experience. Traditional metrics are often used to evaluate various input modalities, including hand and stylus. However, there exists a gap in understanding the cognitive impacts of these modalities during educational gameplay, which can be addressed through brain signal analysis to gain deeper insights into the underlying cognitive function and necessary brain resources for each condition. This facilitates a more precise comparison between conditions. In this study, we compared the brain signal and user experience of using hands and stylus on touchscreens while playing an educational game by analyzing hemodynamic response and self-reported measures. Participants engaged in a Unity-based educational quiz game using both hand and stylus on a touchscreen in a counterbalanced within-subject design. Oxygenated and deoxygenated hemoglobin data were collected using fNIRS, alongside quiz performance scores and standardized and customized user experience questionnaire ratings. Our findings show almost the same performance level with both input modalities, however, the hand requires less oxygen flow which suggests a lower cognitive effort than using a stylus while playing the educational game. Although the result shows that the stylus condition required more neural involvement than the hand condition, there is no significant difference between the use of both input modalities. However, there is a statistically significant difference in self-reported measures that support the findings mentioned above, favoring the hand that enhances understanding of modality effects in interactive educational environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08906v3</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shayla Sharmin, Elham Bakhshipour, Behdokht Kiafar, Md Fahim Abrar, Pinar Kullu, Nancy Getchell, Roghayeh Leila Barmaki</dc:creator>
    </item>
    <item>
      <title>MEEG and AT-DGNN: Improving EEG Emotion Recognition with Music Introducing and Graph-based Learning</title>
      <link>https://arxiv.org/abs/2407.05550</link>
      <description>arXiv:2407.05550v4 Announce Type: replace 
Abstract: We present the MEEG dataset, a multi-modal collection of music-induced electroencephalogram (EEG) recordings designed to capture emotional responses to various musical stimuli across different valence and arousal levels. This public dataset facilitates an in-depth examination of brainwave patterns within musical contexts, providing a robust foundation for studying brain network topology during emotional processing. Leveraging the MEEG dataset, we introduce the Attention-based Temporal Learner with Dynamic Graph Neural Network (AT-DGNN), a novel framework for EEG-based emotion recognition. This model combines an attention mechanism with a dynamic graph neural network (DGNN) to capture intricate EEG dynamics. The AT-DGNN achieves state-of-the-art (SOTA) performance with an accuracy of 83.74% in arousal recognition and 86.01% in valence recognition, outperforming existing SOTA methods. Comparative analysis with traditional datasets, such as DEAP, further validates the model's effectiveness and underscores the potency of music as an emotional stimulus. This study advances graph-based learning methodology in brain-computer interfaces (BCI), significantly improving the accuracy of EEG-based emotion recognition. The MEEG dataset and source code are publicly available at https://github.com/xmh1011/AT-DGNN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05550v4</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minghao Xiao, Zhengxi Zhu, Kang Xie, Bin Jiang</dc:creator>
    </item>
    <item>
      <title>Modulating Language Model Experiences through Frictions</title>
      <link>https://arxiv.org/abs/2407.12804</link>
      <description>arXiv:2407.12804v2 Announce Type: replace 
Abstract: Language models are transforming the ways that their users engage with the world. Despite impressive capabilities, over-consumption of language model outputs risks propagating unchecked errors in the short-term and damaging human capabilities for critical thinking in the long-term. How can we develop scaffolding around language models to curate more appropriate use? We propose selective frictions for language model experiences, inspired by behavioral science interventions, to dampen misuse. Frictions involve small modifications to a user's experience, e.g., the addition of a button impeding model access and reminding a user of their expertise relative to the model. Through a user study with real humans, we observe shifts in user behavior from the imposition of a friction over LLMs in the context of a multi-topic question-answering task as a representative task that people may use LLMs for, e.g., in education and information retrieval. We find that frictions modulate over-reliance by driving down users' click rates while minimally affecting accuracy for those topics. Yet, frictions may have unintended effects. We find marked differences in users' click behaviors even on topics where frictions were not provisioned. Our contributions motivate further study of human-AI behavioral interaction to inform more effective and appropriate LLM use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12804v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katherine M. Collins, Valerie Chen, Ilia Sucholutsky, Hannah Rose Kirk, Malak Sadek, Holli Sargeant, Ameet Talwalkar, Adrian Weller, Umang Bhatt</dc:creator>
    </item>
    <item>
      <title>OpenOmni: A Collaborative Open Source Tool for Building Future-Ready Multimodal Conversational Agents</title>
      <link>https://arxiv.org/abs/2408.03047</link>
      <description>arXiv:2408.03047v2 Announce Type: replace 
Abstract: Multimodal conversational agents are highly desirable because they offer natural and human-like interaction. However, there is a lack of comprehensive end-to-end solutions to support collaborative development and benchmarking. While proprietary systems like GPT-4o and Gemini demonstrating impressive integration of audio, video, and text with response times of 200-250ms, challenges remain in balancing latency, accuracy, cost, and data privacy. To better understand and quantify these issues, we developed OpenOmni, an open-source, end-to-end pipeline benchmarking tool that integrates advanced technologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented Generation, Large Language Models, along with the ability to integrate customized models. OpenOmni supports local and cloud deployment, ensuring data privacy and supporting latency and accuracy benchmarking. This flexible framework allows researchers to customize the pipeline, focusing on real bottlenecks and facilitating rapid proof-of-concept development. OpenOmni can significantly enhance applications like indoor assistance for visually impaired individuals, advancing human-computer interaction. Our demonstration video is available https://www.youtube.com/watch?v=zaSiT3clWqY, demo is available via https://openomni.ai4wa.com, code is available via https://github.com/AI4WA/OpenOmniFramework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03047v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>EMNLP 2024 (System Demonstrations), pp. 46-52</arxiv:journal_reference>
      <dc:creator>Qiang Sun, Yuanyi Luo, Sirui Li, Wenxiao Zhang, Wei Liu</dc:creator>
    </item>
    <item>
      <title>Bridging Player Intentions: Exploring the Potential of Synchronized Haptic Controllers in Multiplayer Game</title>
      <link>https://arxiv.org/abs/2411.05115</link>
      <description>arXiv:2411.05115v2 Announce Type: replace 
Abstract: In multiplayer cooperative video games, players traditionally use individual controllers, inferring others' actions through on-screen visuals and their own movements. This indirect understanding limits truly collaborative gameplay. Research in Joint Action shows that when manipulating a single object, motor performance improves when two people operate together while sensing each other's movements. Building on this, we developed a controller allowing multiple players to operate simultaneously while sharing haptic sensations. We showcased our system at exhibitions, gathering feedback from over 150 participants on how shared sensory input affects their gaming experience. This approach could transform player interaction, enhance cooperation, and redefine multiplayer gaming experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05115v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenta Hashiura, Kazuya Iida, Takeru Hashimoto, Youichi Kamiyama, Keita Watanabe, Kouta Minamizawa, Takuji Narumi</dc:creator>
    </item>
    <item>
      <title>From Complexity to Simplicity: Using Python Instead of PsychoPy for fNIRS Data Collection</title>
      <link>https://arxiv.org/abs/2411.06523</link>
      <description>arXiv:2411.06523v2 Announce Type: replace 
Abstract: Functional near-infrared spectroscopy (fNIRS) is a non-invasive optical technique that measures brain activity by estimating blood oxygenation using near-infrared light. Traditionally, PsychoPy is used in many studies to send task-specific markers, requiring a separate device to interface with the fNIRS data collection system. In this work, we present a Python-based implementation to send markers directly, eliminating the need for an additional device. This approach allows researchers to run both marker transmission and fNIRS data collection on the same computer, simplifying the setup and enhancing accessibility. This streamlined solution reduces hardware requirements and makes fNIRS studies more efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06523v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shayla Sharmin, Md Fahim Abrar, Roghayeh Leila Barmaki</dc:creator>
    </item>
    <item>
      <title>SimTube: Generating Simulated Video Comments through Multimodal AI and User Personas</title>
      <link>https://arxiv.org/abs/2411.09577</link>
      <description>arXiv:2411.09577v2 Announce Type: replace 
Abstract: Audience feedback is crucial for refining video content, yet it typically comes after publication, limiting creators' ability to make timely adjustments. To bridge this gap, we introduce SimTube, a generative AI system designed to simulate audience feedback in the form of video comments before a video's release. SimTube features a computational pipeline that integrates multimodal data from the video-such as visuals, audio, and metadata-with user personas derived from a broad and diverse corpus of audience demographics, generating varied and contextually relevant feedback. Furthermore, the system's UI allows creators to explore and customize the simulated comments. Through a comprehensive evaluation-comprising quantitative analysis, crowd-sourced assessments, and qualitative user studies-we show that SimTube's generated comments are not only relevant, believable, and diverse but often more detailed and informative than actual audience comments, highlighting its potential to help creators refine their content before release.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09577v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu-Kai Hung, Yun-Chien Huang, Ting-Yu Su, Yen-Ting Lin, Lung-Pan Cheng, Bryan Wang, Shao-Hua Sun</dc:creator>
    </item>
    <item>
      <title>Unsupervised Domain Adaptation for RF-based Gesture Recognition</title>
      <link>https://arxiv.org/abs/2111.10602</link>
      <description>arXiv:2111.10602v2 Announce Type: replace-cross 
Abstract: Human gesture recognition with Radio Frequency (RF) signals has attained acclaim due to the omnipresence, privacy protection, and broad coverage nature of RF signals. These gesture recognition systems rely on neural networks trained with a large number of labeled data. However, the recognition model trained with data under certain conditions would suffer from significant performance degradation when applied in practical deployment, which limits the application of gesture recognition systems. In this paper, we propose an unsupervised domain adaptation framework for RF-based gesture recognition aiming to enhance the performance of the recognition model in new conditions by making effective use of the unlabeled data from new conditions. We first propose pseudo-labeling and consistency regularization to utilize unlabeled data for model training and eliminate the feature discrepancies in different domains. Then we propose a confidence constraint loss to enhance the effectiveness of pseudo-labeling, and design two corresponding data augmentation methods based on the characteristic of the RF signals to strengthen the performance of the consistency regularization, which can make the framework more effective and robust. Furthermore, we propose a cross-match loss to integrate the pseudo-labeling and consistency regularization, which makes the whole framework simple yet effective. Extensive experiments demonstrate that the proposed framework could achieve 4.35% and 2.25% accuracy improvement comparing with the state-of-the-art methods on public WiFi dataset and millimeter wave (mmWave) radar dataset, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.10602v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/JIOT.2023.3284496</arxiv:DOI>
      <arxiv:journal_reference>IEEE Internet of Things Journal, 2023, 10(23): 21026-21038</arxiv:journal_reference>
      <dc:creator>Bin-Bin Zhang, Dongheng Zhang, Yadong Li, Yang Hu, Yan Chen</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Study of Knowledge Editing for Large Language Models</title>
      <link>https://arxiv.org/abs/2401.01286</link>
      <description>arXiv:2401.01286v5 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have shown extraordinary capabilities in understanding and generating text that closely mirrors human communication. However, a primary limitation lies in the significant computational demands during training, arising from their extensive parameterization. This challenge is further intensified by the dynamic nature of the world, necessitating frequent updates to LLMs to correct outdated information or integrate new knowledge, thereby ensuring their continued relevance. Note that many applications demand continual model adjustments post-training to address deficiencies or undesirable behaviors. There is an increasing interest in efficient, lightweight methods for on-the-fly model modifications. To this end, recent years have seen a burgeoning in the techniques of knowledge editing for LLMs, which aim to efficiently modify LLMs' behaviors within specific domains while preserving overall performance across various inputs. In this paper, we first define the knowledge editing problem and then provide a comprehensive review of cutting-edge approaches. Drawing inspiration from educational and cognitive research theories, we propose a unified categorization criterion that classifies knowledge editing methods into three groups: resorting to external knowledge, merging knowledge into the model, and editing intrinsic knowledge. Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive empirical evaluation of representative knowledge editing approaches. Additionally, we provide an in-depth analysis of knowledge location, which can give a deeper understanding of the knowledge structures inherent within LLMs. Finally, we discuss several potential applications of knowledge editing, outlining its broad and impactful implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01286v5</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen</dc:creator>
    </item>
    <item>
      <title>Enhancing Cross-Modal Contextual Congruence for Crowdfunding Success using Knowledge-infused Learning</title>
      <link>https://arxiv.org/abs/2402.03607</link>
      <description>arXiv:2402.03607v2 Announce Type: replace-cross 
Abstract: The digital landscape continually evolves with multimodality, enriching the online experience for users. Creators and marketers aim to weave subtle contextual cues from various modalities into congruent content to engage users with a harmonious message. This interplay of multimodal cues is often a crucial factor in attracting users' attention. However, this richness of multimodality presents a challenge to computational modeling, as the semantic contextual cues spanning across modalities need to be unified to capture the true holistic meaning of the multimodal content. This contextual meaning is critical in attracting user engagement as it conveys the intended message of the brand or the organization. In this work, we incorporate external commonsense knowledge from knowledge graphs to enhance the representation of multimodal data using compact Visual Language Models (VLMs) and predict the success of multi-modal crowdfunding campaigns. Our results show that external knowledge commonsense bridges the semantic gap between text and image modalities, and the enhanced knowledge-infused representations improve the predictive performance of models for campaign success upon the baselines without knowledge. Our findings highlight the significance of contextual congruence in online multimodal content for engaging and successful crowdfunding campaigns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03607v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>IEEE International Conference on Big Data 2024 (IEEE BigData 2024)</arxiv:journal_reference>
      <dc:creator>Trilok Padhi, Ugur Kursuncu, Yaman Kumar, Valerie L. Shalin, Lane Peterson Fronczek</dc:creator>
    </item>
    <item>
      <title>Designs for Enabling Collaboration in Human-Machine Teaming via Interactive and Explainable Systems</title>
      <link>https://arxiv.org/abs/2406.05003</link>
      <description>arXiv:2406.05003v2 Announce Type: replace-cross 
Abstract: Collaborative robots and machine learning-based virtual agents are increasingly entering the human workspace with the aim of increasing productivity and enhancing safety. Despite this, we show in a ubiquitous experimental domain, Overcooked-AI, that state-of-the-art techniques for human-machine teaming (HMT), which rely on imitation or reinforcement learning, are brittle and result in a machine agent that aims to decouple the machine and human's actions to act independently rather than in a synergistic fashion. To remedy this deficiency, we develop HMT approaches that enable iterative, mixed-initiative team development allowing end-users to interactively reprogram interpretable AI teammates. Our 50-subject study provides several findings that we summarize into guidelines. While all approaches underperform a simple collaborative heuristic (a critical, negative result for learning-based methods), we find that white-box approaches supported by interactive modification can lead to significant team development, outperforming white-box approaches alone, and that black-box approaches are easier to train and result in better HMT performance highlighting a tradeoff between explainability and interactivity versus ease-of-training. Together, these findings present three important future research directions: 1) Improving the ability to generate collaborative agents with white-box models, 2) Better learning methods to facilitate collaboration rather than individualized coordination, and 3) Mixed-initiative interfaces that enable users, who may vary in ability, to improve collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05003v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohan Paleja, Michael Munje, Kimberlee Chang, Reed Jensen, Matthew Gombolay</dc:creator>
    </item>
    <item>
      <title>Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina</title>
      <link>https://arxiv.org/abs/2410.19599</link>
      <description>arXiv:2410.19599v2 Announce Type: replace-cross 
Abstract: Recent studies suggest large language models (LLMs) can exhibit human-like reasoning, aligning with human behavior in economic experiments, surveys, and political discourse. This has led many to propose that LLMs can be used as surrogates or simulations for humans in social science research. However, LLMs differ fundamentally from humans, relying on probabilistic patterns, absent the embodied experiences or survival objectives that shape human cognition. We assess the reasoning depth of LLMs using the 11-20 money request game. Nearly all advanced approaches fail to replicate human behavior distributions across many models. Causes of failure are diverse and unpredictable, relating to input language, roles, and safeguarding. These results advise caution when using LLMs to study human behavior or as surrogates or simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19599v2</guid>
      <category>econ.GN</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Gao, Dokyun Lee, Gordon Burtch, Sina Fazelpour</dc:creator>
    </item>
    <item>
      <title>Effective Guidance for Model Attention with Simple Yes-no Annotations</title>
      <link>https://arxiv.org/abs/2410.22312</link>
      <description>arXiv:2410.22312v2 Announce Type: replace-cross 
Abstract: Modern deep learning models often make predictions by focusing on irrelevant areas, leading to biased performance and limited generalization. Existing methods aimed at rectifying model attention require explicit labels for irrelevant areas or complex pixel-wise ground truth attention maps. We present CRAYON (Correcting Reasoning with Annotations of Yes Or No), offering effective, scalable, and practical solutions to rectify model attention using simple yes-no annotations. CRAYON empowers classical and modern model interpretation techniques to identify and guide model reasoning: CRAYON-ATTENTION directs classic interpretations based on saliency maps to focus on relevant image regions, while CRAYON-PRUNING removes irrelevant neurons identified by modern concept-based methods to mitigate their influence. Through extensive experiments with both quantitative and human evaluation, we showcase CRAYON's effectiveness, scalability, and practicality in refining model attention. CRAYON achieves state-of-the-art performance, outperforming 12 methods across 3 benchmark datasets, surpassing approaches that require more complex annotations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22312v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seongmin Lee, Ali Payani, Duen Horng Chau</dc:creator>
    </item>
    <item>
      <title>ConvMixFormer- A Resource-efficient Convolution Mixer for Transformer-based Dynamic Hand Gesture Recognition</title>
      <link>https://arxiv.org/abs/2411.07118</link>
      <description>arXiv:2411.07118v2 Announce Type: replace-cross 
Abstract: Transformer models have demonstrated remarkable success in many domains such as natural language processing (NLP) and computer vision. With the growing interest in transformer-based architectures, they are now utilized for gesture recognition. So, we also explore and devise a novel ConvMixFormer architecture for dynamic hand gestures. The transformers use quadratic scaling of the attention features with the sequential data, due to which these models are computationally complex and heavy. We have considered this drawback of the transformer and designed a resource-efficient model that replaces the self-attention in the transformer with the simple convolutional layer-based token mixer. The computational cost and the parameters used for the convolution-based mixer are comparatively less than the quadratic self-attention. Convolution-mixer helps the model capture the local spatial features that self-attention struggles to capture due to their sequential processing nature. Further, an efficient gate mechanism is employed instead of a conventional feed-forward network in the transformer to help the model control the flow of features within different stages of the proposed model. This design uses fewer learnable parameters which is nearly half the vanilla transformer that helps in fast and efficient training. The proposed method is evaluated on NVidia Dynamic Hand Gesture and Briareo datasets and our model has achieved state-of-the-art results on single and multimodal inputs. We have also shown the parameter efficiency of the proposed ConvMixFormer model compared to other methods. The source code is available at https://github.com/mallikagarg/ConvMixFormer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07118v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 19 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Mallika Garg, Debashis Ghosh, Pyari Mohan Pradhan</dc:creator>
    </item>
  </channel>
</rss>
