<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Jan 2025 05:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Signal-to-noise ratio aware minimax analysis of sparse linear regression</title>
      <link>https://arxiv.org/abs/2501.13323</link>
      <description>arXiv:2501.13323v1 Announce Type: new 
Abstract: We consider parameter estimation under sparse linear regression -- an extensively studied problem in high-dimensional statistics and compressed sensing. While the minimax framework has been one of the most fundamental approaches for studying statistical optimality in this problem, we identify two important issues that the existing minimax analyses face: (i) The signal-to-noise ratio appears to have no effect on the minimax optimality, while it shows a major impact in numerical simulations. (ii) Estimators such as best subset selection and Lasso are shown to be minimax optimal, yet they exhibit significantly different performances in simulations. In this paper, we tackle the two issues by employing a minimax framework that accounts for variations in the signal-to-noise ratio (SNR), termed the SNR-aware minimax framework. We adopt a delicate higher-order asymptotic analysis technique to obtain the SNR-aware minimax risk. Our theoretical findings determine three distinct SNR regimes: low-SNR, medium-SNR, and high-SNR, wherein minimax optimal estimators exhibit markedly different behaviors. The new theory not only offers much better elaborations for empirical results, but also brings new insights to the estimation of sparse signals in noisy data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13323v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shubhangi Ghosh, Yilin Guo, Haolei Weng, Arian Maleki</dc:creator>
    </item>
    <item>
      <title>Consistent spectral clustering in sparse tensor block models</title>
      <link>https://arxiv.org/abs/2501.13820</link>
      <description>arXiv:2501.13820v1 Announce Type: new 
Abstract: High-order clustering aims to classify objects in multiway datasets that are prevalent in various fields such as bioinformatics, social network analysis, and recommendation systems. These tasks often involve data that is sparse and high-dimensional, presenting significant statistical and computational challenges. This paper introduces a tensor block model specifically designed for sparse integer-valued data tensors. We propose a simple spectral clustering algorithm augmented with a trimming step to mitigate noise fluctuations, and identify a density threshold that ensures the algorithm's consistency. Our approach models sparsity using a sub-Poisson noise concentration framework, accommodating heavier than sub-Gaussian tails. Remarkably, this natural class of tensor block models is closed under aggregation across arbitrary modes. Consequently, we obtain a comprehensive framework for evaluating the tradeoff between signal loss and noise reduction during data aggregation. The analysis is based on a novel concentration bound for sparse random Gram matrices. The theoretical findings are illustrated through simulation experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13820v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ian V\"alimaa, Lasse Leskel\"a</dc:creator>
    </item>
    <item>
      <title>Testing conditional independence under isotonicity</title>
      <link>https://arxiv.org/abs/2501.06133</link>
      <description>arXiv:2501.06133v1 Announce Type: cross 
Abstract: We propose a test of the conditional independence of random variables $X$ and $Y$ given $Z$ under the additional assumption that $X$ is stochastically increasing in $Z$. The well-documented hardness of testing conditional independence means that some further restriction on the null hypothesis parameter space is required, but in contrast to existing approaches based on parametric models, smoothness assumptions, or approximations to the conditional distribution of $X$ given $Z$ and/or $Y$ given $Z$, our test requires only the stochastic monotonicity assumption. Our procedure, called PairSwap-ICI, determines the significance of a statistic by randomly swapping the $X$ values within ordered pairs of $Z$ values. The matched pairs and the test statistic may depend on both $Y$ and $Z$, providing the analyst with significant flexibility in constructing a powerful test. Our test offers finite-sample Type I error control, and provably achieves high power against a large class of alternatives that are not too close to the null. We validate our theoretical findings through a series of simulations and real data experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06133v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rohan Hore, Jake A. Soloff, Rina Foygel Barber, Richard J. Samworth</dc:creator>
    </item>
    <item>
      <title>Sequential One-Sided Hypothesis Testing of Markov Chains</title>
      <link>https://arxiv.org/abs/2501.13187</link>
      <description>arXiv:2501.13187v1 Announce Type: cross 
Abstract: We study the problem of sequentially testing whether a given stochastic process is generated by a known Markov chain. Formally, given access to a stream of random variables, we want to quickly determine whether this sequence is a trajectory of a Markov chain with a known transition matrix $P$ (null hypothesis) or not (composite alternative hypothesis). This problem naturally arises in many engineering problems.
  The main technical challenge is to develop a sequential testing scheme that adapts its sample size to the unknown alternative. Indeed, if we knew the alternative distribution (that is, the transition matrix) $Q$, a natural approach would be to use a generalization of Wald's sequential probability ratio test (SPRT). Building on this intuition, we propose and analyze a family of one-sided SPRT-type tests for our problem that use a data-driven estimator $\hat{Q}$. In particular, we show that if the deployed estimator admits a worst-case regret guarantee scaling as $\mathcal{O}\left( \log{t} \right)$, then the performance of our test asymptotically matches that of SPRT in the simple hypothesis testing case. In other words, our test automatically adapts to the unknown hardness of the problem, without any prior information. We end with a discussion of known Markov chain estimators with $\mathcal{O}\left( \log{t} \right)$ regret.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13187v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Greg Fields, Tara Javidi, Shubhanshu Shekhar</dc:creator>
    </item>
    <item>
      <title>Continuity of the Distribution Function of the argmax of a Gaussian Process</title>
      <link>https://arxiv.org/abs/2501.13265</link>
      <description>arXiv:2501.13265v1 Announce Type: cross 
Abstract: An increasingly important class of estimators has members whose asymptotic distribution is non-Gaussian, yet characterizable as the argmax of a Gaussian process. This paper presents high-level sufficient conditions under which such asymptotic distributions admit a continuous distribution function. The plausibility of the sufficient conditions is demonstrated by verifying them in three prominent examples, namely maximum score estimation, empirical risk minimization, and threshold regression estimation. In turn, the continuity result buttresses several recently proposed inference procedures whose validity seems to require a result of the kind established herein. A notable feature of the high-level assumptions is that one of them is designed to enable us to employ the celebrated Cameron-Martin theorem. In a leading special case, the assumption in question is demonstrably weak and appears to be close to minimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13265v1</guid>
      <category>econ.EM</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Gregory Fletcher Cox, Michael Jansson, Kenichi Nagasawa</dc:creator>
    </item>
    <item>
      <title>Capturing heterogeneous time-variation in covariate effects in non-proportional hazard regression models</title>
      <link>https://arxiv.org/abs/2501.13525</link>
      <description>arXiv:2501.13525v1 Announce Type: cross 
Abstract: A central focus in survival analysis is examining how covariates influence survival time. These covariate effects are often found to be either time-varying, heterogeneous - such as being specific to patients, treatments, or subgroups - or exhibit both characteristics simultaneously. While the standard model, the Cox proportional hazards model, allows neither time-varying nor heterogeneous effects, several extensions to the Cox model as well as alternative modeling frameworks have been introduced. However, no unified framework for incorporating heterogeneously time-varying effects of covariates has been proposed yet. Such effects occur when a covariate influences survival not only in a heterogeneous and time-varying manner, but when the time-variation is also heterogeneous. We propose to model such effects by introducing heterogeneously time-varying coefficients to piecewise exponential additive mixed models. We deploy functional random effects, also known as factor smooths, to model such coefficients as the interaction effect of heterogeneity and time-variation. Our approach allows for non-linear time-effects due to being based on penalized splines and uses an efficient random effects basis to model the heterogeneity. Using a penalized basis prevents overfitting in case of absence of such effects. In addition, the penalization mostly solves the problem of choosing the number of intervals which is usually present in unregularized piecewise exponential approaches. We demonstrate the superiority of our approach in comparison to competitors by means of a simulation study. Finally, the practical application and relevance are outlined by presenting a brain tumor case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13525v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niklas Hagemann, Thomas Kneib, Kathrin M\"ollenhoff</dc:creator>
    </item>
    <item>
      <title>Information-theoretic limits and approximate message-passing for high-dimensional time series</title>
      <link>https://arxiv.org/abs/2501.13625</link>
      <description>arXiv:2501.13625v1 Announce Type: cross 
Abstract: High-dimensional time series appear in many scientific setups, demanding a nuanced approach to model and analyze the underlying dependence structure. However, theoretical advancements so far often rely on stringent assumptions regarding the sparsity of the underlying signals. In this contribution, we expand the scope by investigating a high-dimensional time series model wherein the number of features grows proportionally to the number of sampling points, without assuming sparsity in the signal. Specifically, we consider the stochastic regression model and derive a single-letter formula for the normalized mutual information between observations and the signal. We also empirically study the vector approximate message passing (VAMP) algorithm and show that, despite a lack of theoretical guarantees, its performance for inference in our time series model is robust and often statistically optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13625v1</guid>
      <category>cs.IT</category>
      <category>cond-mat.dis-nn</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daria Tieplova, Samriddha Lahiry, Jean Barbier</dc:creator>
    </item>
    <item>
      <title>Finite mixture representations of zero-&amp;-$N$-inflated distributions for count-compositional data</title>
      <link>https://arxiv.org/abs/2501.13879</link>
      <description>arXiv:2501.13879v1 Announce Type: cross 
Abstract: We provide novel probabilistic portrayals of two multivariate models designed to handle zero-inflation in count-compositional data. We develop a new unifying framework that represents both as finite mixture distributions. One of these distributions, based on Dirichlet-multinomial components, has been studied before, but has not yet been properly characterised as a sampling distribution of the counts. The other, based on multinomial components, is a new contribution. Using our finite mixture representations enables us to derive key statistical properties, including moments, marginal distributions, and special cases for both distributions. We develop enhanced Bayesian inference schemes with efficient Gibbs sampling updates, wherever possible, for parameters and auxiliary variables, demonstrating improvements over existing methods in the literature. We conduct simulation studies to evaluate the efficiency of the Bayesian inference procedures and to illustrate the practical utility of the proposed distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13879v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'e F. B. Menezes, Andrew C. Parnell, Keefe Murphy</dc:creator>
    </item>
    <item>
      <title>An Overview of Asymptotic Normality in Stochastic Blockmodels: Cluster Analysis and Inference</title>
      <link>https://arxiv.org/abs/2305.06353</link>
      <description>arXiv:2305.06353v2 Announce Type: replace 
Abstract: This paper provides a selective review of the statistical network analysis literature focused on clustering and inference problems for stochastic blockmodels and their variants. We survey asymptotic normality results for stochastic blockmodels as a means of thematically linking classical statistical concepts to contemporary research in network data analysis. Of note, multiple different forms of asymptotically Gaussian behavior arise in stochastic blockmodels and are useful for different purposes, pertaining to estimation and testing, the characterization of cluster structure in community detection, and understanding latent space geometry. This paper concludes with a discussion of open problems and ongoing research activities addressing asymptotic normality and its implications for statistical network modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.06353v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Agterberg, Joshua Cape</dc:creator>
    </item>
    <item>
      <title>Spectral Regularized Kernel Goodness-of-Fit Tests</title>
      <link>https://arxiv.org/abs/2308.04561</link>
      <description>arXiv:2308.04561v2 Announce Type: replace 
Abstract: Maximum mean discrepancy (MMD) has enjoyed a lot of success in many machine learning and statistical applications, including non-parametric hypothesis testing, because of its ability to handle non-Euclidean data. Recently, it has been demonstrated in Balasubramanian et al.(2021) that the goodness-of-fit test based on MMD is not minimax optimal while a Tikhonov regularized version of it is, for an appropriate choice of the regularization parameter. However, the results in Balasubramanian et al. (2021) are obtained under the restrictive assumptions of the mean element being zero, and the uniform boundedness condition on the eigenfunctions of the integral operator. Moreover, the test proposed in Balasubramanian et al. (2021) is not practical as it is not computable for many kernels. In this paper, we address these shortcomings and extend the results to general spectral regularizers that include Tikhonov regularization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.04561v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research, 25 (309): 1-52, 2024</arxiv:journal_reference>
      <dc:creator>Omar Hagrass, Bharath K. Sriperumbudur, Bing Li</dc:creator>
    </item>
    <item>
      <title>High-dimensional analysis of ridge regression for non-identically distributed data with a variance profile</title>
      <link>https://arxiv.org/abs/2403.20200</link>
      <description>arXiv:2403.20200v3 Announce Type: replace 
Abstract: High-dimensional linear regression has been thoroughly studied in the context of independent and identically distributed data. We propose to investigate high-dimensional regression models for independent but non-identically distributed data. To this end, we suppose that the set of observed predictors (or features) is a random matrix with a variance profile and with dimensions growing at a proportional rate. Assuming a random effect model, we study the predictive risk of the ridge estimator for linear regression with such a variance profile. In this setting, we provide deterministic equivalents of this risk and of the degree of freedom of the ridge estimator. For certain class of variance profile, our work highlights the emergence of the well-known double descent phenomenon in high-dimensional regression for the minimum norm least-squares estimator when the ridge regularization parameter goes to zero. We also exhibit variance profiles for which the shape of this predictive risk differs from double descent. The proofs of our results are based on tools from random matrix theory in the presence of a variance profile that have not been considered so far to study regression models. Numerical experiments are provided to show the accuracy of the aforementioned deterministic equivalents on the computation of the predictive risk of ridge regression. We also investigate the similarities and differences that exist with the standard setting of independent and identically distributed data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20200v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J\'er\'emie Bigot, Issa-Mbenard Dabo, Camille Male</dc:creator>
    </item>
    <item>
      <title>Minimax Optimal Goodness-of-Fit Testing with Kernel Stein Discrepancy</title>
      <link>https://arxiv.org/abs/2404.08278</link>
      <description>arXiv:2404.08278v3 Announce Type: replace 
Abstract: We explore the minimax optimality of goodness-of-fit tests on general domains using the kernelized Stein discrepancy (KSD). The KSD framework offers a flexible approach for goodness-of-fit testing, avoiding strong distributional assumptions, accommodating diverse data structures beyond Euclidean spaces, and relying only on partial knowledge of the reference distribution, while maintaining computational efficiency. Although KSD is a powerful framework for goodness-of-fit testing, only the consistency of the corresponding tests has been established so far, and their statistical optimality remains largely unexplored. In this paper, we develop a general framework and an operator-theoretic representation of the KSD, encompassing many existing KSD tests in the literature, which vary depending on the domain. Building on this representation, we propose a modified discrepancy by applying the concept of spectral regularization to the KSD framework. We establish the minimax optimality of the proposed regularized test for a wide range of the smoothness parameter $\theta$ under a specific alternative space, defined over general domains, using the $\chi^2$-divergence as the separation metric. In contrast, we demonstrate that the unregularized KSD test fails to achieve the minimax separation rate for the considered alternative space. Additionally, we introduce an adaptive test capable of achieving minimax optimality up to a logarithmic factor by adapting to unknown parameters. Through numerical experiments, we illustrate the superior performance of our proposed tests across various domains compared to their unregularized counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08278v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omar Hagrass, Bharath Sriperumbudur, Krishnakumar Balasubramanian</dc:creator>
    </item>
    <item>
      <title>Estimation of conditional inequality measures</title>
      <link>https://arxiv.org/abs/2412.20228</link>
      <description>arXiv:2412.20228v2 Announce Type: replace 
Abstract: Classical inequality measures such as the Gini index are often used to describe the sparsity of the distribution of a certain feature in a population. It is sometimes also used to compare the inequalities between some subpopulations, conditioned on certain values of the covariates. The concept of measuring inequality in subpopulation was described in the literature and it is strongly related to the decomposition of the Gini index. In this paper, the idea of conditional inequality measures is extended to the case where covariates are continuous. Curves of conditional inequality measures are introduced, especially, the curves of the conditional quantile versions of the Zenga and $D$ indices are considered. Various methods of their estimation based on quantile regression are presented. An approach using isotonic regression is used to prevent quantile crossing in quantile regression. The accuracy of the estimators considered is compared in simulation studies. Furthermore, an analysis of the growth in salary inequalities with respect to employee age is included to demonstrate the potential of conditional inequality measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20228v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alicja Jokiel-Rokita, Sylwester Pi\k{a}tek, Rafa{\l} Topolnicki</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Canonical Correlation Analysis</title>
      <link>https://arxiv.org/abs/2306.16393</link>
      <description>arXiv:2306.16393v3 Announce Type: replace-cross 
Abstract: This paper studies high-dimensional canonical correlation analysis (CCA) with an emphasis on the vectors that define canonical variables. The paper shows that when two dimensions of data grow to infinity jointly and proportionally, the classical CCA procedure for estimating those vectors fails to deliver a consistent estimate. This provides the first result on the impossibility of identification of canonical variables in the CCA procedure when all dimensions are large. As a countermeasure, the paper derives the magnitude of the estimation error, which can be used in practice to assess the precision of CCA estimates. Applications of the results to cyclical vs. non-cyclical stocks and to a limestone grassland data set are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16393v3</guid>
      <category>econ.EM</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Bykhovskaya, Vadim Gorin</dc:creator>
    </item>
    <item>
      <title>Fourier analysis of spatial point processes</title>
      <link>https://arxiv.org/abs/2401.06403</link>
      <description>arXiv:2401.06403v3 Announce Type: replace-cross 
Abstract: In this article, we develop comprehensive frequency domain methods for estimating and inferring the second-order structure of spatial point processes. The main element here is on utilizing the discrete Fourier transform (DFT) of the point pattern and its tapered counterpart. Under second-order stationarity, we show that both the DFTs and the tapered DFTs are asymptotically jointly independent Gaussian even when the DFTs share the same limiting frequencies. Based on these results, we establish an $\alpha$-mixing central limit theorem for a statistic formulated as a quadratic form of the tapered DFT. As applications, we derive the asymptotic distribution of the kernel spectral density estimator and establish a frequency domain inferential method for parametric stationary point processes. For the latter, the resulting model parameter estimator is computationally tractable and yields meaningful interpretations even in the case of model misspecification. We investigate the finite sample performance of our estimator through simulations, considering scenarios of both correctly specified and misspecified models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06403v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junho Yang, Yongtao Guan</dc:creator>
    </item>
    <item>
      <title>Structural adaptation via directional regularity: rate accelerated estimation in multivariate functional data</title>
      <link>https://arxiv.org/abs/2409.00817</link>
      <description>arXiv:2409.00817v4 Announce Type: replace-cross 
Abstract: We introduce directional regularity, a new definition of anisotropy for multivariate functional data. Instead of taking the conventional view which determines anisotropy as a notion of smoothness along a dimension, directional regularity additionally views anisotropy through the lens of directions. We show that faster rates of convergence can be obtained through a change-of-basis by adapting to the directional regularity of a multivariate process. An algorithm for the estimation and identification of the change-of-basis matrix is constructed, made possible due to the replication structure of functional data. Non-asymptotic bounds are provided for our algorithm, supplemented by numerical evidence from an extensive simulation study. Possible applications of the directional regularity approach are discussed, and we advocate its consideration as a standard pre-processing step in multivariate functional data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00817v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Omar Kassi, Sunny G. W. Wang</dc:creator>
    </item>
    <item>
      <title>High Dimensional Space Oddity</title>
      <link>https://arxiv.org/abs/2409.13046</link>
      <description>arXiv:2409.13046v2 Announce Type: replace-cross 
Abstract: In his 1996 paper, Talagrand highlighted that the Law of Large Numbers (LLN) for independent random variables can be viewed as a geometric property of multidimensional product spaces. This phenomenon is known as the concentration of measure. To illustrate this profound connection between geometry and probability theory, we consider a seemingly intractable geometric problem in multidimensional Euclidean space and solve it using standard probabilistic tools such as the LLN and the Central Limit Theorem (CLT).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13046v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haim Bar, Vladimir Pozdnyakov</dc:creator>
    </item>
    <item>
      <title>Local Learning for Covariate Selection in Nonparametric Causal Effect Estimation with Latent Variables</title>
      <link>https://arxiv.org/abs/2411.16315</link>
      <description>arXiv:2411.16315v3 Announce Type: replace-cross 
Abstract: Estimating causal effects from nonexperimental data is a fundamental problem in many fields of science. A key component of this task is selecting an appropriate set of covariates for confounding adjustment to avoid bias. Most existing methods for covariate selection often assume the absence of latent variables and rely on learning the global network structure among variables. However, identifying the global structure can be unnecessary and inefficient, especially when our primary interest lies in estimating the effect of a treatment variable on an outcome variable. To address this limitation, we propose a novel local learning approach for covariate selection in nonparametric causal effect estimation, which accounts for the presence of latent variables. Our approach leverages testable independence and dependence relationships among observed variables to identify a valid adjustment set for a target causal relationship, ensuring both soundness and completeness under standard assumptions. We validate the effectiveness of our algorithm through extensive experiments on both synthetic and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16315v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheng Li, Feng Xie, Xichen Guo, Yan Zeng, Hao Zhang, Zhi Geng</dc:creator>
    </item>
    <item>
      <title>Guaranteed Recovery of Unambiguous Clusters</title>
      <link>https://arxiv.org/abs/2501.13093</link>
      <description>arXiv:2501.13093v2 Announce Type: replace-cross 
Abstract: Clustering is often a challenging problem because of the inherent ambiguity in what the "correct" clustering should be. Even when the number of clusters $K$ is known, this ambiguity often still exists, particularly when there is variation in density among different clusters, and clusters have multiple relatively separated regions of high density. In this paper we propose an information-theoretic characterization of when a $K$-clustering is ambiguous, and design an algorithm that recovers the clustering whenever it is unambiguous. This characterization formalizes the situation when two high density regions within a cluster are separable enough that they look more like two distinct clusters than two truly distinct clusters in the clustering. The algorithm first identifies $K$ partial clusters (or "seeds") using a density-based approach, and then adds unclustered points to the initial $K$ partial clusters in a greedy manner to form a complete clustering. We implement and test a version of the algorithm that is modified to effectively handle overlapping clusters, and observe that it requires little parameter selection and displays improved performance on many datasets compared to widely used algorithms for non-convex cluster recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13093v2</guid>
      <category>cs.IT</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kayvon Mazooji, Ilan Shomorony</dc:creator>
    </item>
  </channel>
</rss>
