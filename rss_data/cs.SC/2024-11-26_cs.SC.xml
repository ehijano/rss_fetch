<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SC</link>
    <description>cs.SC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 27 Nov 2024 02:56:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Botfip-LLM: An Enhanced Multimodal Scientific Computing Framework Leveraging Knowledge Distillation from Large Language Models</title>
      <link>https://arxiv.org/abs/2411.15525</link>
      <description>arXiv:2411.15525v1 Announce Type: new 
Abstract: In recent years, the introduction of AI technologies has brought transformative changes to scientific computing. However, AI models typically focus on single-task and single-modal data processing, limiting their application. To address this, multimodal scientific computing frameworks have become a trend. The Botfip framework aligns function images with symbolic operation trees through multimodal training, extracting deep scientific information. However, Botfip struggles with processing Formula Strings, leading to inadequate understanding in multimodal learning. To enhance Botfip's learning of Formula Strings and expand its applicability to related tasks, we propose the Botfip-LLM framework based on knowledge distillation, incorporating pre-trained large language models for aligning symbolic tree data. Experimental analysis shows that the choice of LLM is crucial, with ChatGLM-2 outperforming others in training and testing. Botfip-LLM not only improves performance, generalization, and extrapolation over the original Botfip model but also significantly enhances applicability to Formula String-related tasks, enabling more diverse task handling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15525v1</guid>
      <category>cs.SC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianhao Chen, Pengbo Xu, Pengbo Xu</dc:creator>
    </item>
    <item>
      <title>Extracting Linear Relations from Gr\"obner Bases for Formal Verification of And-Inverter Graphs</title>
      <link>https://arxiv.org/abs/2411.16348</link>
      <description>arXiv:2411.16348v1 Announce Type: new 
Abstract: Formal verification techniques based on computer algebra have proven highly effective for circuit verification. The circuit, given as an and-inverter graph, is encoded as a set of polynomials that automatically generates a Gr\"obner basis with respect to a lexicographic term ordering. Correctness of the circuit can be derived by computing the polynomial remainder of the specification. However, the main obstacle is the monomial blow-up during the rewriting of the specification, which leads to the development of dedicated heuristics to overcome this issue. In this paper, we investigate an orthogonal approach and focus the computational effort on rewriting the Gr\"obner basis itself. Our goal is to ensure the basis contains linear polynomials that can be effectively used to rewrite the linearized specification. We first prove the soundness and completeness of this technique and then demonstrate its practical application. Our implementation of this method shows promising results on benchmarks related to multiplier verification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16348v1</guid>
      <category>cs.SC</category>
      <category>cs.LO</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniela Kaufmann, J\'er\'emy Berthomieu</dc:creator>
    </item>
    <item>
      <title>Bio-inspired AI: Integrating Biological Complexity into Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2411.15243</link>
      <description>arXiv:2411.15243v1 Announce Type: cross 
Abstract: The pursuit of creating artificial intelligence (AI) mirrors our longstanding fascination with understanding our own intelligence. From the myths of Talos to Aristotelian logic and Heron's inventions, we have sought to replicate the marvels of the mind. While recent advances in AI hold promise, singular approaches often fall short in capturing the essence of intelligence. This paper explores how fundamental principles from biological computation--particularly context-dependent, hierarchical information processing, trial-and-error heuristics, and multi-scale organization--can guide the design of truly intelligent systems. By examining the nuanced mechanisms of biological intelligence, such as top-down causality and adaptive interaction with the environment, we aim to illuminate potential limitations in artificial constructs. Our goal is to provide a framework inspired by biological systems for designing more adaptable and robust artificial intelligent systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15243v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.NE</category>
      <category>cs.SC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nima Dehghani, Michael Levin</dc:creator>
    </item>
    <item>
      <title>New Bounds on Quotient Polynomials with Applications to Exact Divisibility and Divisibility Testing of Sparse Polynomials</title>
      <link>https://arxiv.org/abs/2308.03885</link>
      <description>arXiv:2308.03885v3 Announce Type: replace 
Abstract: We prove that for monic polynomials $f, g \in \mathbb{C}[x]$ such that $g$ divides $f$, the $\ell_2$-norm of the quotient polynomial $f/g$ is bounded by $\lVert f \rVert_1 \cdot \tilde{O}(\lVert{g}\rVert_0^3\text{deg}^2{ f})^{\lVert{g}\rVert_0 - 1}$. This improves upon the previously known exponential (in $\text{deg}{ f}$) bounds for general polynomials. Our results implies that the trivial long division algorithm runs in quasi-linear time relative to the input size and number of terms of the quotient polynomial $f/g$, thus solving a long-standing problem on exact divisibility of sparse polynomials.
  We also study the problem of bounding the number of terms of $f/g$ in some special cases. When $f, g \in \mathbb{Z}[x]$ and $g$ is a cyclotomic-free (i.e., it has no cyclotomic factors) trinomial, we prove that $\lVert{f/g}\rVert_0 \leq O(\lVert{f}\rVert_0 \text{size}({f})^2 \cdot \log^6{\text{deg}{ g}})$. When $g$ is a cyclotomic-free binomial, we prove that the sparsity is at most $O(\lVert{f}\rVert_0 ( \log{\lVert{f}\rVert_0} + \log{\lVert{f}\rVert_{\infty}}))$. Both upper bounds are polynomial in the input-size. We leverage these results and give a polynomial time algorithm for deciding whether a cyclotomic-free trinomial divides a sparse polynomial over the integers.
  As our last result, we present a polynomial time algorithm for testing divisibility by pentanomials over small finite fields when $\text{deg}{ f} = \tilde{O}(\text{deg}{ g})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.03885v3</guid>
      <category>cs.SC</category>
      <category>cs.CC</category>
      <category>math.NT</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ido Nahshon, Amir Shpilka</dc:creator>
    </item>
    <item>
      <title>Federated Automatic Differentiation</title>
      <link>https://arxiv.org/abs/2301.07806</link>
      <description>arXiv:2301.07806v2 Announce Type: replace-cross 
Abstract: Federated learning (FL) is a general framework for learning across an axis of group partitioned data (heterogeneous clients) while preserving data privacy, under the orchestration of a central server. FL methods often compute gradients of loss functions purely locally (ie. entirely at each client, or entirely at the server), typically using automatic differentiation (AD) techniques. We propose a federated automatic differentiation (FAD) framework that 1) enables computing derivatives of functions involving client and server computation as well as communication between them and 2) operates in a manner compatible with existing federated technology. In other words, FAD computes derivatives across communication boundaries. We show, in analogy with traditional AD, that FAD may be implemented using various accumulation modes, which introduce distinct computation-communication trade-offs and systems requirements. Further, we show that a broad class of federated computations is closed under these various modes of FAD, implying in particular that if the original computation can be implemented using privacy-preserving primitives, its derivative may be computed using only these same primitives. We then show how FAD can be used to create algorithms that dynamically learn components of the algorithm itself. In particular, we show that FedAvg-style algorithms can exhibit significantly improved performance by using FAD to adjust the server optimization step automatically, or by using FAD to learn weighting schemes for computing weighted averages across clients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.07806v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.SC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keith Rush, Zachary Charles, Zachary Garrett</dc:creator>
    </item>
    <item>
      <title>Reasoning Abilities of Large Language Models: In-Depth Analysis on the Abstraction and Reasoning Corpus</title>
      <link>https://arxiv.org/abs/2403.11793</link>
      <description>arXiv:2403.11793v3 Announce Type: replace-cross 
Abstract: The existing methods for evaluating the inference abilities of Large Language Models (LLMs) have been predominantly results-centric, making it challenging to assess the inference process comprehensively. We introduce a novel approach using the Abstraction and Reasoning Corpus (ARC) benchmark to evaluate the inference and contextual understanding abilities of LLMs in a process-centric manner, focusing on three key components from the Language of Thought Hypothesis (LoTH): Logical Coherence, Compositionality, and Productivity. Our carefully designed experiments reveal that while LLMs demonstrate some inference capabilities, they still significantly lag behind human-level reasoning in these three aspects. The main contribution of this paper lies in introducing the LoTH perspective, which provides a method for evaluating the reasoning process that conventional results-oriented approaches fail to capture, thereby offering new insights into the development of human-level reasoning in artificial intelligence systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11793v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.SC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seungpil Lee, Woochang Sim, Donghyeon Shin, Wongyu Seo, Jiwon Park, Seokki Lee, Sanha Hwang, Sejin Kim, Sundong Kim</dc:creator>
    </item>
  </channel>
</rss>
