<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 May 2025 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Algebraic Approach for Orthomax Rotations</title>
      <link>https://arxiv.org/abs/2504.21288</link>
      <description>arXiv:2504.21288v1 Announce Type: new 
Abstract: In exploratory factor analysis, rotation techniques are employed to derive interpretable factor loading matrices. Factor rotations deal with equality-constrained optimization problems aimed at determining a loading matrix based on measure of simplicity, such as ``perfect simple structure'' and ``Thurstone simple structure.'' Numerous criteria have been proposed, since the concept of simple structure is fundamentally ambiguous and involves multiple distinct aspects. However, most rotation criteria may fail to consistently yield a simple structure that is optimal for analytical purposes, primarily due to two challenges. First, existing optimization techniques, including the gradient projection descent method, exhibit strong dependence on initial values and frequently become trapped in suboptimal local optima. Second, multifaceted nature of simple structure complicates the ability of any single criterion to ensure interpretability across all aspects. In certain cases, even when a global optimum is achieved, other rotations may exhibit simpler structures in specific aspects. To address these issues, obtaining all equality-constrained stationary points -- including both global and local optima -- is advantageous. Fortunately, many rotation criteria are expressed as algebraic functions, and the constraints in the optimization problems in factor rotations are formulated as algebraic equations. Therefore, we can employ computational algebra techniques that utilize operations within polynomial rings to derive exact all equality-constrained stationary points. Unlike existing optimization methods, the computational algebraic approach can determine global optima and all stationary points, independent of initial values. We conduct Monte Carlo simulations to examine the properties of the orthomax rotation criteria, which generalizes various orthogonal rotation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21288v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryoya Fukasaku, Michio Yamamoto, Yutaro Kabata, Yasuhiko Ikematsu, Kei Hirose</dc:creator>
    </item>
    <item>
      <title>The differential structure shared by probability and moment matching priors on non-regular statistical models via the Lie derivative</title>
      <link>https://arxiv.org/abs/2504.21363</link>
      <description>arXiv:2504.21363v1 Announce Type: new 
Abstract: In Bayesian statistics, the selection of noninformative priors is a crucial issue. There have been various discussions on theoretical justification, problems with the Jeffreys prior, and alternative objective priors. Among them, we focus on two types of matching priors consistent with frequentist theory: the probability matching priors and the moment matching priors. In particular, no clear relationship has been established between these two types of priors on non-regular statistical models, even though they share similar objectives.
  Considering information geometry on a one-sided truncated exponential family, a typical example of non-regular statistical models, we find that the Lie derivative along a particular vector field provides the conditions for both the probability and moment matching priors. Notably, this Lie derivative does not appear in regular models. These conditions require the invariance of a generalized volume element with respect to differentiation along the non-regular parameter. This invariance leads to a suitable decomposition of the one-sided truncated exponential family into one-dimensional submodels. This result promotes a unified understanding of probability and moment matching priors on non-regular models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21363v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masaki Yoshioka, Fuyuhiko Tanaka</dc:creator>
    </item>
    <item>
      <title>Non-parametric multiple change-point detection</title>
      <link>https://arxiv.org/abs/2504.21379</link>
      <description>arXiv:2504.21379v1 Announce Type: new 
Abstract: We introduce a methodology, labelled Non-Parametric Isolate-Detect (NPID), for the consistent estimation of the number and locations of multiple change-points in a non-parametric setting. The method can handle general distributional changes and is based on an isolation technique preventing the consideration of intervals that contain more than one change-point, which enhances the estimation accuracy. As stopping rules, we propose both thresholding and the optimization of an information criterion. In the scenarios tested, which cover a broad range of change types, NPID outperforms the state of the art. An R implementation is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21379v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andreas Anastasiou, Piotr Fryzlewicz</dc:creator>
    </item>
    <item>
      <title>Convergence rate for Nearest Neighbour matching: geometry of the domain and higher-order regularity</title>
      <link>https://arxiv.org/abs/2504.21633</link>
      <description>arXiv:2504.21633v1 Announce Type: new 
Abstract: Estimating some mathematical expectations from partially observed data and in particular missing outcomes is a central problem encountered in numerous fields such as transfer learning, counterfactual analysis or causal inference. Matching estimators, estimators based on k-nearest neighbours, are widely used in this context. It is known that the variance of such estimators can converge to zero at a parametric rate, but their bias can have a slower rate when the dimension of the covariates is larger than 2. This makes analysis of this bias particularly important. In this paper, we provide higher order properties of the bias. In contrast to the existing literature related to this problem, we do not assume that the support of the target distribution of the covariates is strictly included in that of the source, and we analyse two geometric conditions on the support that avoid such boundary bias problems. We show that these conditions are much more general than the usual convex support assumption, leading to an improvement of existing results. Furthermore, we show that the matching estimator studied by Abadie and Imbens (2006) for the average treatment effect can be asymptotically efficient when the dimension of the covariates is less than 4, a result only known in dimension 1.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21633v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Viel, Lionel Truquet, Ikko Yamane</dc:creator>
    </item>
    <item>
      <title>Estimation of discrete distributions in relative entropy, and the deviations of the missing mass</title>
      <link>https://arxiv.org/abs/2504.21787</link>
      <description>arXiv:2504.21787v1 Announce Type: new 
Abstract: We study the problem of estimating a distribution over a finite alphabet from an i.i.d. sample, with accuracy measured in relative entropy (Kullback-Leibler divergence). While optimal expected risk bounds are known, high-probability guarantees remain less well-understood. First, we analyze the classical Laplace (add-$1$) estimator, obtaining matching upper and lower bounds on its performance and showing its optimality among confidence-independent estimators. We then characterize the minimax-optimal high-probability risk achievable by any estimator, which is attained via a simple confidence-dependent smoothing technique. Interestingly, the optimal non-asymptotic risk contains an additional logarithmic factor over the ideal asymptotic risk. Next, motivated by scenarios where the alphabet exceeds the sample size, we investigate methods that adapt to the sparsity of the distribution at hand. We introduce an estimator using data-dependent smoothing, for which we establish a high-probability risk bound depending on two effective sparsity parameters. As part of the analysis, we also derive a sharp high-probability upper bound on the missing mass.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21787v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jaouad Mourtada</dc:creator>
    </item>
    <item>
      <title>Statistical comparison of Hidden Markov Models via Fragment Analysis</title>
      <link>https://arxiv.org/abs/2504.21046</link>
      <description>arXiv:2504.21046v1 Announce Type: cross 
Abstract: Standard practice in Hidden Markov Model (HMM) selection favors the candidate with the highest full-sequence likelihood, although this is equivalent to making a decision based on a single realization. We introduce a \emph{fragment-based} framework that redefines model selection as a formal statistical comparison. For an unknown true model $\mathrm{HMM}_0$ and a candidate $\mathrm{HMM}_j$, let $\mu_j(r)$ denote the probability that $\mathrm{HMM}_j$ and $\mathrm{HMM}_0$ generate the same sequence of length~$r$. We show that if $\mathrm{HMM}_i$ is closer to $\mathrm{HMM}_0$ than $\mathrm{HMM}_j$, there exists a threshold $r^{*}$ -- often small -- such that $\mu_i(r)&gt;\mu_j(r)$ for all $r\geq r^{*}$. Sampling $k$ independent fragments yields unbiased estimators $\hat{\mu}_j(r)$ whose differences are asymptotically normal, enabling a straightforward $Z$-test for the hypothesis $H_0\!:\,\mu_i(r)=\mu_j(r)$. By evaluating only short subsequences, the procedure circumvents full-sequence likelihood computation and provides valid $p$-values for model comparison.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21046v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos M. Hernandez-Suarez, Osval A. Montesinos-L\'opez</dc:creator>
    </item>
    <item>
      <title>Polyhedral Aspects of Maxoids</title>
      <link>https://arxiv.org/abs/2504.21068</link>
      <description>arXiv:2504.21068v1 Announce Type: cross 
Abstract: The conditional independence (CI) relation of a distribution in a max-linear Bayesian network depends on its weight matrix through the $C^\ast$-separation criterion. These CI models, which we call maxoids, are compositional graphoids which are in general not representable by Gaussian or discrete random variables. We prove that every maxoid can be obtained from a transitively closed weighted DAG and show that the stratification of generic weight matrices by their maxoids yields a polyhedral fan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21068v1</guid>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tobias Boege, Kamillo Ferry, Benjamin Hollering, Francesco Nowell</dc:creator>
    </item>
    <item>
      <title>Publication Design with Incentives in Mind</title>
      <link>https://arxiv.org/abs/2504.21156</link>
      <description>arXiv:2504.21156v1 Announce Type: cross 
Abstract: The publication process both determines which research receives the most attention, and influences the supply of research through its impact on the researcher's private incentives. We introduce a framework to study optimal publication decisions when researchers can choose (i) whether or how to conduct a study and (ii) whether or how to manipulate the research findings (e.g., via selective reporting or data manipulation). When manipulation is not possible, but research entails substantial private costs for the researchers, it may be optimal to incentivize cheaper research designs even if they are less accurate. When manipulation is possible, it is optimal to publish some manipulated results, as well as results that would have not received attention in the absence of manipulability. Even if it is possible to deter manipulation, such as by requiring pre-registered experiments instead of (potentially manipulable) observational studies, it is suboptimal to do so when experiments entail high research costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21156v1</guid>
      <category>econ.EM</category>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ravi Jagadeesan, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>Mining and Intervention of Social Networks Information Cocoon Based on Multi-Layer Network Community Detection</title>
      <link>https://arxiv.org/abs/2504.21357</link>
      <description>arXiv:2504.21357v1 Announce Type: cross 
Abstract: With the rapid development of information technology and the widespread utilization of recommendation algorithms, users are able to access information more conveniently, while the content they receive tends to be homogeneous. Homogeneous viewpoints and preferences tend to cluster users into sub-networks, leading to group polarization and increasing the likelihood of forming information cocoons. This paper aims to handle information cocoon phenomena in debates on social media. In order to investigate potential user connections, we construct a double-layer network that incorporates two dimensions: relational ties and feature-based similarity between users. Based on the structure of the multi-layer network, we promote two graph auto-encoder (GAE) based community detection algorithms, which can be applied to the partition and determination of information cocoons. This paper tests these two algorithms on Cora, Citeseer, and synthetic datasets, comparing them with existing multi-layer network unsupervised community detection algorithms. Numerical experiments illustrate that the algorithms proposed in this paper significantly improve prediction accuracy indicator NMI (normalized mutual information) and network topology indicator Q. Additionally, an influence-based intervention measure on which algorithms can operate is proposed. Through the Markov states transition model, we simulate the intervention effects, which illustrate that our community detection algorithms play a vital role in partitioning and determining information cocoons. Simultaneously, our intervention strategy alleviates the polarization of viewpoints and the formation of information cocoons with minimal intervention effort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21357v1</guid>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yang Suwen, Shi Lei</dc:creator>
    </item>
    <item>
      <title>Kernel Density Machines</title>
      <link>https://arxiv.org/abs/2504.21419</link>
      <description>arXiv:2504.21419v1 Announce Type: cross 
Abstract: We introduce kernel density machines (KDM), a novel density ratio estimator in a reproducing kernel Hilbert space setting. KDM applies to general probability measures on countably generated measurable spaces without restrictive assumptions on continuity, or the existence of a Lebesgue density. For computational efficiency, we incorporate a low-rank approximation with precisely controlled error that grants scalability to large-sample settings. We provide rigorous theoretical guarantees, including asymptotic consistency, a functional central limit theorem, and finite-sample error bounds, establishing a strong foundation for practical use. Empirical results based on simulated and real data demonstrate the efficacy and precision of KDM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21419v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Damir Filipovic, Paul Schneider</dc:creator>
    </item>
    <item>
      <title>An Aldous-Hoover type representation for row exchangeable arrays</title>
      <link>https://arxiv.org/abs/2504.21584</link>
      <description>arXiv:2504.21584v1 Announce Type: cross 
Abstract: In an array of random variables, each row can be regarded as a single, sequence-valued random variable. In this way, the array is seen as a sequence of sequences. Such an array is said to be row exchangeable if each row is an exchangeable sequence, and the entire array, viewed as a sequence of sequences, is exchangeable. We give a representation theorem, analogous to those of Aldous and Hoover, which characterizes row exchangeable arrays. We then use this representation theorem to address the problem of performing Bayesian inference on row exchangeable arrays.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21584v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evan Donald, Jason Swanson</dc:creator>
    </item>
    <item>
      <title>Conditional independence testing with a single realization of a multivariate nonstationary nonlinear time series</title>
      <link>https://arxiv.org/abs/2504.21647</link>
      <description>arXiv:2504.21647v1 Announce Type: cross 
Abstract: Identifying relationships among stochastic processes is a key goal in disciplines that deal with complex temporal systems, such as economics. While the standard toolkit for multivariate time series analysis has many advantages, it can be difficult to capture nonlinear dynamics using linear vector autoregressive models. This difficulty has motivated the development of methods for variable selection, causal discovery, and graphical modeling for nonlinear time series, which routinely employ nonparametric tests for conditional independence. In this paper, we introduce the first framework for conditional independence testing that works with a single realization of a nonstationary nonlinear process. The key technical ingredients are time-varying nonlinear regression, time-varying covariance estimation, and a distribution-uniform strong Gaussian approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21647v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Wieck-Sosa, Michel F. C. Haddad, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>On the Robustness of Mixture Models in the Presence of Hidden Markov Regimes with Covariate-Dependent Transition Probabilities</title>
      <link>https://arxiv.org/abs/2504.21669</link>
      <description>arXiv:2504.21669v1 Announce Type: cross 
Abstract: This paper studies the robustness of quasi-maximum-likelihood (QML) estimation in hidden Markov models (HMMs) when the regime-switching structure is misspecified. Specifically, we examine the case where the true data-generating process features a hidden Markov regime sequence with covariate-dependent transition probabilities, but estimation proceeds under a simplified mixture model that assumes regimes are independent and identically distributed. We show that the parameters governing the conditional distribution of the observables can still be consistently estimated under this misspecification, provided certain regularity conditions hold. Our results highlight a practical benefit of using computationally simpler mixture models in settings where regime dependence is complex or difficult to model directly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21669v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Demian Pouzo, Martin Sola, Zacharias Psaradakis</dc:creator>
    </item>
    <item>
      <title>Taylor's Theorem and Mean Value Theorem for Random Functions and Random Variables</title>
      <link>https://arxiv.org/abs/2102.10429</link>
      <description>arXiv:2102.10429v2 Announce Type: replace-cross 
Abstract: This study addresses the often-overlooked issue of measurability at intermediate points when applying Taylor's theorems to random functions and random vectors (e.g., likelihood functions with respect to estimators) in statistics. Classical Taylor-related theorems were originally developed for deterministic settings. Consequently, they do not directly extend to stochastic functions and variables and do not inherently guarantee the measurability of intermediate points. In statistical contexts, applying these theorems without properly accounting for randomness can lead to analyses that lack well-defined probabilistic interpretations. Elementary approaches, such as pointwise constructions, are insufficient for handling random quantities and establishing measurable intermediate points. Moreover, some statistical literature has implicitly disregarded this issue, often neglecting the stochastic nature of the problem and assuming that intermediate points are measurable. To address this gap, we develop multivariate Taylor's and mean value theorems tailored for random functions and random variables under mild assumptions. We provide illustrative examples demonstrating the applicability of our results to commonly used statistical methods, including maximum likelihood estimation, $M$-estimation, and profile estimation. Our findings contribute a rigorous foundation for the applications of Taylor expansions in statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2102.10429v2</guid>
      <category>stat.OT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yifan Yang, Xiaoyu Zhou, Ming Wang</dc:creator>
    </item>
    <item>
      <title>Flexible Bayesian Multiple Comparison Adjustment Using Dirichlet Process and Beta-Binomial Model Priors</title>
      <link>https://arxiv.org/abs/2208.07086</link>
      <description>arXiv:2208.07086v3 Announce Type: replace-cross 
Abstract: Researchers frequently wish to assess the equality or inequality of groups, but this poses the challenge of adequately adjusting for multiple comparisons. Statistically, all possible configurations of equality and inequality constraints can be uniquely represented as partitions of groups, where any number of groups are equal if they are in the same subset of the partition. In a Bayesian framework, one can adjust for multiple comparisons by constructing a suitable prior distribution over all possible partitions. Inspired by work on variable selection in regression, we propose a class of flexible beta-binomial priors for multiple comparison adjustment. We compare this prior setup to the Dirichlet process prior suggested by Gopalan and Berry (1998) and multiple comparison adjustment methods that do not specify a prior over partitions directly. Our approach not only allows researchers to assess pairwise equality constraints but simultaneously all possible equalities among all groups. Since the space of possible partitions grows rapidly -- for ten groups, there are already 115,975 possible partitions -- we use a stochastic search algorithm to efficiently explore the space. Our method is implemented in the Julia package EqualitySampler, and we illustrate it on examples related to the comparison of means, standard deviations, and proportions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.07086v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Don van den Bergh, Fabian Dablander</dc:creator>
    </item>
    <item>
      <title>Anytime-valid FDR control with the stopped e-BH procedure</title>
      <link>https://arxiv.org/abs/2502.08539</link>
      <description>arXiv:2502.08539v2 Announce Type: replace-cross 
Abstract: The recent e-Benjamini-Hochberg (e-BH) procedure for multiple hypothesis testing is known to control the false discovery rate (FDR) under arbitrary dependence between the input e-values. This paper points out an important subtlety when applying the e-BH procedure with e-processes, which are sequential generalizations of e-values (where the data are observed sequentially). Since adaptively stopped e-processes are e-values, the e-BH procedure can be repeatedly applied at every time step, and one can continuously monitor the e-processes and the rejection sets obtained. One would hope that the "stopped e-BH procedure" (se-BH) has an FDR guarantee for the rejection set obtained at any stopping time. However, while this is true if the data in different streams are independent, it is not true in full generality, because each stopped e-process is an e-value only for stopping times in its own local filtration, but the se-BH procedure employs a stopping time with respect to a global filtration. This can cause information to leak across time, allowing one stream to know its future by knowing past data of another stream. This paper formulates a simple causal condition under which local e-processes are also global e-processes and thus the se-BH procedure does indeed control the FDR. The condition excludes unobserved confounding from the past and is met under most reasonable scenarios including genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08539v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Wang, Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>An Improved Satterthwaite Effective Degrees of Freedom Correction for Weighted Syntheses of Variance</title>
      <link>https://arxiv.org/abs/2503.22080</link>
      <description>arXiv:2503.22080v3 Announce Type: replace-cross 
Abstract: This article presents an improved approximation for the effective degrees of freedom in the Satterthwaite (1941, 1946) method which estimates the distribution of a weighted combination of variance components The standard Satterthwaite approximation assumes a scaled chisquare distribution for the composite variance estimator but is known to be biased downward when component degrees of freedom are small. Building on recent work by von Davier (2025), we propose an adjusted estimator that corrects this bias by modifying both the numerator and denominator of the traditional formula. The new approximation incorporates a weighted average of component degrees of freedom and a scaling factor that ensures consistency as the number of components or their degrees of freedom increases. We demonstrate the utility of this adjustment in practical settings, including Rubin's (1987) total variance estimation in multiple imputations, where weighted variance combinations are common. The proposed estimator generalizes and further improves von Davier's (2025) unweighted case and more accurately approximates synthetic variance estimators with arbitrary weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22080v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matthias von Davier</dc:creator>
    </item>
    <item>
      <title>Markovian Continuity of the MMSE</title>
      <link>https://arxiv.org/abs/2504.14659</link>
      <description>arXiv:2504.14659v2 Announce Type: replace-cross 
Abstract: Minimum mean square error (MMSE) estimation is widely used in signal processing and related fields. While it is known to be non-continuous with respect to all standard notions of stochastic convergence, it remains robust in practical applications. In this work, we review the known counterexamples to the continuity of the MMSE. We observe that, in these counterexamples, the discontinuity arises from an element in the converging measurement sequence providing more information about the estimand than the limit of the measurement sequence. We argue that this behavior is uncharacteristic of real-world applications and introduce a new stochastic convergence notion, termed Markovian convergence, to address this issue. We prove that the MMSE is, in fact, continuous under this new notion. We supplement this result with semi-continuity and continuity guarantees of the MMSE in other settings and prove the continuity of the MMSE under linear estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14659v2</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Elad Domanovitz, Anatoly Khina</dc:creator>
    </item>
  </channel>
</rss>
