<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 16 Feb 2026 05:00:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Transformer-based CoVaR: Systemic Risk in Textual Information</title>
      <link>https://arxiv.org/abs/2602.12490</link>
      <description>arXiv:2602.12490v1 Announce Type: new 
Abstract: Conditional Value-at-Risk (CoVaR) quantifies systemic financial risk by measuring the loss quantile of one asset, conditional on another asset experiencing distress. We develop a Transformer-based methodology that integrates financial news articles directly with market data to improve CoVaR estimates. Unlike approaches that use predefined sentiment scores, our method incorporates raw text embeddings generated by a large language model (LLM). We prove explicit error bounds for our Transformer CoVaR estimator, showing that accurate CoVaR learning is possible even with small datasets. Using U.S. market returns and Reuters news items from 2006--2013, our out-of-sample results show that textual information impacts the CoVaR forecasts. With better predictive performance, we identify a pronounced negative dip during market stress periods across several equity assets when comparing the Transformer-based CoVaR to both the CoVaR without text and the CoVaR using traditional sentiment measures. Our results show that textual data can be used to effectively model systemic risk without requiring prohibitively large data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12490v1</guid>
      <category>econ.EM</category>
      <category>q-fin.RM</category>
      <category>stat.ML</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyu Chen, Tom Boot, Lingwei Kong, Weining Wang</dc:creator>
    </item>
    <item>
      <title>Toggling the Defiers to Relax Monotonicity: The Difference-in-Instrumental-Variables Estimand</title>
      <link>https://arxiv.org/abs/2602.12504</link>
      <description>arXiv:2602.12504v1 Announce Type: new 
Abstract: Standard instrumental variables (IV) methods identify a Local Average Treatment Effect under monotonicity, which rules out defiers. In many empirical environments, however, distinct instruments may induce heterogeneous and even opposing behavioral responses. This paper introduces the Difference-in-Instrumental-Variables (DIIV) estimand, which exploits two instruments with opposing compliance patterns to recover a point-identified and behaviorally interpretable causal effect without imposing monotonicity. The estimand yields a convex combination of the marginal treatment effects on compliers and defiers, with weights reflecting differential shifts in treatment take-up across instruments. When monotonicity holds, DIIV coincides with the standard IV estimand. The approach can be implemented using simple linear transformations and standard two-stage least squares procedures. Applications using replication data illustrate its applicability in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12504v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johann Caro-Burnett</dc:creator>
    </item>
    <item>
      <title>Empirical Validation of a Dual-Defense Mechanism Reshaping Wholesale Electricity Price Dynamics in Singapore</title>
      <link>https://arxiv.org/abs/2602.12782</link>
      <description>arXiv:2602.12782v1 Announce Type: cross 
Abstract: While ex-ante screening and static price caps are global standards for mitigating price volatility, Singapore's electricity market employs a unique dual-defense mechanism integrating vesting contracts (VC) with a temporary price cap (TPC). Using high-frequency data from 2021 to 2024, this paper evaluates this mechanism and yields three primary findings. First, a structural trade-off exists within the VC framework: while VC quantity (VCQ) suppresses average prices, it paradoxically exacerbates instability via liquidity squeezes. Conversely, VC price (VCP) functions as a tail-risk anchor, dominating at extreme quantiles where VCQ efficacy wanes. Second, a structural break around the 2023 reform reveals a fundamental re-mapping of price dynamics; the previously positive pass-through from offer ratios to clearing prices was largely neutralized post-reform. Furthermore, diagnostics near the TPC threshold show no systematic evidence of strategic bid shading, confirming the TPC's operational integrity. Third, the dual-defense mechanism exhibits a critical synergy that resolves the volatility trade-off. The TPC reverses the volatility penalty of high VCQ, shifting the elasticity of conditional volatility from a destabilizing 0.636 to a stabilizing -0.213. This synergy enables the framework to enhance tail-risk control while eliminating liquidity-related stability costs. We conclude that this dual-defense mechanism successfully decouples price suppression from liquidity risks, thereby maximizing market stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12782v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>econ.EM</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huang Zhenyu, Yuan Zhao</dc:creator>
    </item>
    <item>
      <title>An Introduction to Double/Debiased Machine Learning</title>
      <link>https://arxiv.org/abs/2504.08324</link>
      <description>arXiv:2504.08324v2 Announce Type: replace 
Abstract: This paper provides an introduction to Double/Debiased Machine Learning (DML). DML is a general approach to performing inference about a target parameter in the presence of nuisance functions: objects that are needed to identify the target parameter but are not of primary interest. Nuisance functions arise naturally in many settings, such as when controlling for confounding variables or leveraging instruments. The paper describes two biases that arise from nuisance function estimation and explains how DML alleviates these biases. Consequently, DML allows the use of flexible methods, including machine learning tools, for estimating nuisance functions, reducing the dependence on auxiliary functional form assumptions and enabling the use of complex non-tabular data, such as text or images. We illustrate the application of DML through simulations and empirical examples. We conclude with a discussion of recommended practices. A companion website includes additional examples with code and references to other resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08324v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Achim Ahrens, Victor Chernozhukov, Christian Hansen, Damian Kozbur, Mark Schaffer, Thomas Wiemann</dc:creator>
    </item>
    <item>
      <title>Boundary Discontinuity Designs: Theory and Practice</title>
      <link>https://arxiv.org/abs/2511.06474</link>
      <description>arXiv:2511.06474v3 Announce Type: replace 
Abstract: The boundary discontinuity (BD) design is a non-experimental method for identifying causal effects that exploits a thresholding rule based on a bivariate score and a boundary curve. This widely used method generalizes the univariate regression discontinuity design but introduces unique challenges arising from its multidimensional nature. We synthesize over 80 empirical papers that use the BD design, tracing the method's application from its formative stages to its implementation in modern research. We also overview ongoing theoretical and methodological research on identification, estimation, and inference for BD designs employing local polynomial regression, and offer recommendations for practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.06474v3</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rocio Titiunik, Ruiqi Rae Yu</dc:creator>
    </item>
    <item>
      <title>Anytime-Valid Inference in Adaptive Experiments: Covariate Adjustment and Balanced Power</title>
      <link>https://arxiv.org/abs/2506.20523</link>
      <description>arXiv:2506.20523v4 Announce Type: replace-cross 
Abstract: Adaptive experiments such as multi-armed bandits offer efficiency gains over traditional randomized experiments but pose two major challenges: invalid inference on the Average Treatment Effect (ATE) due to adaptive sampling and low statistical power for sub-optimal treatments. We address both issues by extending the Mixture Adaptive Design framework (arXiv:2311.05794). First, we propose MADCovar, a covariate-adjusted ATE estimator that is unbiased and preserves anytime-valid inference guarantees while substantially improving ATE precision. Second, we introduce MADMod, which dynamically reallocates samples to underpowered arms, enabling more balanced statistical power across treatments without sacrificing valid inference. Both methods retain MAD's core advantage of constructing asymptotic confidence sequences (CSs) that allow researchers to continuously monitor ATE estimates and stop data collection once a desired precision or significance criterion is met. Empirically, we validate both methods using simulations and real-world data. In simulations, MADCovar reduces CS width by up to 60% relative to MAD. In a large-scale political RCT with 32,000 participants, MADCovar achieves similar precision gains. MADMod improves statistical power and inferential precision across all treatment arms, particularly for suboptimal treatments. Simulations show that MADMod sharply reduces Type II error while preserving the efficiency benefits of adaptive allocation. Together, MADCovar and MADMod make adaptive experiments more practical, reliable, and efficient for applied researchers across many domains. Our proposed methods are implemented through an open-source software package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20523v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Molitor, Samantha Gold</dc:creator>
    </item>
  </channel>
</rss>
