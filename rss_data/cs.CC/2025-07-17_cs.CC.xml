<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Jul 2025 04:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Perfect diffusion is $\mathsf{TC}^0$ -- Bad diffusion is Turing-complete</title>
      <link>https://arxiv.org/abs/2507.12469</link>
      <description>arXiv:2507.12469v1 Announce Type: new 
Abstract: This paper explores the computational complexity of diffusion-based language modeling. We prove a dichotomy based on the quality of the score-matching network in a diffusion model. In one direction, a network that exactly computes the score function of some initial distribution can only perform language modeling within the $\mathsf{TC}^0$ complexity class, reflecting limitations tied to rapid convergence. In the other direction, we show that if there is no requirement for the network to match any score function, then diffusion modeling can simulate any Turing machine in a certain sense. This dichotomy provides a theoretical lens on the capabilities and limitations of diffusion models, particularly concerning tasks requiring sequential computation. We conjecture extensions of our theoretical results, including for the case where the diffusion model is not perfect, but merely good. We also discuss the wider context and practical implications, and hypothesize that a machine learning architecture that can interpolate between sequential and parallel modes of operation would be superior to both Transformers and diffusion models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12469v1</guid>
      <category>cs.CC</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yuxi Liu</dc:creator>
    </item>
    <item>
      <title>Computational-Statistical Tradeoffs from NP-hardness</title>
      <link>https://arxiv.org/abs/2507.13222</link>
      <description>arXiv:2507.13222v1 Announce Type: new 
Abstract: A central question in computer science and statistics is whether efficient algorithms can achieve the information-theoretic limits of statistical problems. Many computational-statistical tradeoffs have been shown under average-case assumptions, but since statistical problems are average-case in nature, it has been a challenge to base them on standard worst-case assumptions.
  In PAC learning where such tradeoffs were first studied, the question is whether computational efficiency can come at the cost of using more samples than information-theoretically necessary. We base such tradeoffs on $\mathsf{NP}$-hardness and obtain:
  $\circ$ Sharp computational-statistical tradeoffs assuming $\mathsf{NP}$ requires exponential time: For every polynomial $p(n)$, there is an $n$-variate class $C$ with VC dimension $1$ such that the sample complexity of time-efficiently learning $C$ is $\Theta(p(n))$.
  $\circ$ A characterization of $\mathsf{RP}$ vs. $\mathsf{NP}$ in terms of learning: $\mathsf{RP} = \mathsf{NP}$ iff every $\mathsf{NP}$-enumerable class is learnable with $O(\mathrm{VCdim}(C))$ samples in polynomial time. The forward implication has been known since (Pitt and Valiant, 1988); we prove the reverse implication.
  Notably, all our lower bounds hold against improper learners. These are the first $\mathsf{NP}$-hardness results for improperly learning a subclass of polynomial-size circuits, circumventing formal barriers of Applebaum, Barak, and Xiao (2008).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13222v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guy Blanc, Caleb Koch, Carmen Strassle, Li-Yang Tan</dc:creator>
    </item>
    <item>
      <title>The Serial Scaling Hypothesis</title>
      <link>https://arxiv.org/abs/2507.12549</link>
      <description>arXiv:2507.12549v1 Announce Type: cross 
Abstract: While machine learning has advanced through massive parallelization, we identify a critical blind spot: some problems are fundamentally sequential. These "inherently serial" problems-from mathematical reasoning to physical simulations to sequential decision-making-require dependent computational steps that cannot be parallelized. Drawing from complexity theory, we formalize this distinction and demonstrate that current parallel-centric architectures face fundamental limitations on such tasks. We argue that recognizing the serial nature of computation holds profound implications on machine learning, model design, hardware development. As AI tackles increasingly complex reasoning, deliberately scaling serial computation-not just parallel computation-is essential for continued progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12549v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxi Liu, Konpat Preechakul, Kananart Kuwaranancharoen, Yutong Bai</dc:creator>
    </item>
    <item>
      <title>Ranking Vectors Clustering: Theory and Applications</title>
      <link>https://arxiv.org/abs/2507.12583</link>
      <description>arXiv:2507.12583v1 Announce Type: cross 
Abstract: We study the problem of clustering ranking vectors, where each vector represents preferences as an ordered list of distinct integers. Specifically, we focus on the k-centroids ranking vectors clustering problem (KRC), which aims to partition a set of ranking vectors into k clusters and identify the centroid of each cluster. Unlike classical k-means clustering (KMC), KRC constrains both the observations and centroids to be ranking vectors. We establish the NP-hardness of KRC and characterize its feasible set. For the single-cluster case, we derive a closed-form analytical solution for the optimal centroid, which can be computed in linear time. To address the computational challenges of KRC, we develop an efficient approximation algorithm, KRCA, which iteratively refines initial solutions from KMC, referred to as the baseline solution. Additionally, we introduce a branch-and-bound (BnB) algorithm for efficient cluster reconstruction within KRCA, leveraging a decision tree framework to reduce computational time while incorporating a controlling parameter to balance solution quality and efficiency. We establish theoretical error bounds for KRCA and BnB. Through extensive numerical experiments on synthetic and real-world datasets, we demonstrate that KRCA consistently outperforms baseline solutions, delivering significant improvements in solution quality with fast computational times. This work highlights the practical significance of KRC for personalization and large-scale decision making, offering methodological advancements and insights that can be built upon in future studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12583v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Fattahi, Ali Eshragh, Babak Aslani, Meysam Rabiee</dc:creator>
    </item>
    <item>
      <title>FormulaOne: Measuring the Depth of Algorithmic Reasoning Beyond Competitive Programming</title>
      <link>https://arxiv.org/abs/2507.13337</link>
      <description>arXiv:2507.13337v1 Announce Type: cross 
Abstract: Frontier AI models demonstrate formidable breadth of knowledge. But how close are they to true human -- or superhuman -- expertise? Genuine experts can tackle the hardest problems and push the boundaries of scientific understanding. To illuminate the limits of frontier model capabilities, we turn away from contrived competitive programming puzzles, and instead focus on real-life research problems.
  We construct FormulaOne, a benchmark that lies at the intersection of graph theory, logic, and algorithms, all well within the training distribution of frontier models. Our problems are incredibly demanding, requiring an array of reasoning steps. The dataset has three key properties. First, it is of commercial interest and relates to practical large-scale optimisation problems, such as those arising in routing, scheduling, and network design. Second, it is generated from the highly expressive framework of Monadic Second-Order (MSO) logic on graphs, paving the way toward automatic problem generation at scale; ideal for building RL environments. Third, many of our problems are intimately related to the frontier of theoretical computer science, and to central conjectures therein, such as the Strong Exponential Time Hypothesis (SETH). As such, any significant algorithmic progress on our dataset, beyond known results, could carry profound theoretical implications.
  Remarkably, state-of-the-art models like OpenAI's o3 fail entirely on FormulaOne, solving less than 1% of the questions, even when given 10 attempts and explanatory fewshot examples -- highlighting how far they remain from expert-level understanding in some domains. To support further research, we additionally curate FormulaOne-Warmup, offering a set of simpler tasks, from the same distribution. We release the full corpus along with a comprehensive evaluation framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13337v1</guid>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>math.LO</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gal Beniamini, Yuval Dor, Alon Vinnikov, Shir Granot Peled, Or Weinstein, Or Sharir, Noam Wies, Tomer Nussbaum, Ido Ben Shaul, Tomer Zekharya, Yoav Levine, Shai Shalev-Shwartz, Amnon Shashua</dc:creator>
    </item>
    <item>
      <title>The Algebraic Cost of a Boolean Sum</title>
      <link>https://arxiv.org/abs/2502.02442</link>
      <description>arXiv:2502.02442v2 Announce Type: replace 
Abstract: It is a well-known fact that the permanent polynomial is complete for the complexity class VNP, and it is largely suspected that the determinant does not share this property, despite its similar expression. We study the question of why the VNP-completeness proof of the permanent fails for the determinant. We isolate three fundamental properties that are sufficient to prove a polynomial sequence is VNP-hard, of which two are shared by both the permanent and the determinant. We proceed to show that the permanent satisfies the third property, which we refer to as the ``cost of a boolean sum," while the determinant does not, showcasing the fundamental difference between the polynomial families. We further note that this differentiation also applies in the border complexity setting and that our results apply for counting complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02442v2</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ian Orzel, Srikanth Srinivasan, S\'ebastien Tavenas, Amir Yehudayoff</dc:creator>
    </item>
    <item>
      <title>Computational Complexity of Polynomial Subalgebras</title>
      <link>https://arxiv.org/abs/2502.05278</link>
      <description>arXiv:2502.05278v2 Announce Type: replace 
Abstract: The computational complexity of polynomial ideals and Gr\"obner bases has been studied since the 1980s. In recent years, the related notions of polynomial subalgebras and SAGBI bases have gained more and more attention in computational algebra, with a view towards effective algorithms. We investigate the computational complexity of the subalgebra membership problem and degree bounds. In particular, we show completeness for the complexity class EXPSPACE and prove PSPACE-completeness for homogeneous algebras. We highlight parallels and differences compared to the settings of ideals, and also look at important classes of polynomials such as monomial algebras.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05278v2</guid>
      <category>cs.CC</category>
      <category>math.AC</category>
      <category>math.AG</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonie Kayser</dc:creator>
    </item>
    <item>
      <title>Weighted Pseudorandom Generators for Read-Once Branching Programs via Weighted Pseudorandom Reductions</title>
      <link>https://arxiv.org/abs/2502.08272</link>
      <description>arXiv:2502.08272v3 Announce Type: replace 
Abstract: We study weighted pseudorandom generators (WPRGs) and derandomizations for read-once branching programs (ROBPs). Denote $n$ and $w$ as the length and the width of a ROBP. We have the following results.
  For standard ROBPs, we give an explicit $\varepsilon$-WPRG with seed length
  $$O\left(\frac{\log n\log (nw)}{\max\left\{1,\log\log w-\log\log n\right\}}+\log w \left(\log\log\log w-\log\log\max\left\{2,\frac{\log w}{\log \frac{n}{\varepsilon}}\right\}\right)+\log\frac{1}{\varepsilon}\right).$$
  For permutation ROBPs with unbounded widths and single accept nodes, we give an explicit $\varepsilon$-WPRG with seed length
  $$O\left( \log n\left( \log\log n + \sqrt{\log(1/\varepsilon)} \right)+\log(1/\varepsilon)\right). $$
  We also give a new Nisan-Zuckerman style derandomization for regular ROBPs with width $w$, length $n = 2^{O(\sqrt{\log w})}$, and multiple accept nodes. We attain optimal space complexity $O(\log w)$ for arbitrary approximation error $\varepsilon = 1/\text{poly} (w)$.
  All our results are based on iterative weighted pseudorandom reductions, which can iteratively reduce fooling long ROBPs to fooling short ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08272v3</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kuan Cheng, Ruiyang Wu</dc:creator>
    </item>
    <item>
      <title>Evolomino is NP-complete</title>
      <link>https://arxiv.org/abs/2503.07611</link>
      <description>arXiv:2503.07611v2 Announce Type: replace 
Abstract: Evolomino is a pencil-and-paper logic puzzle popularized by the Japanese publisher Nikoli (like Sudoku, Kakuro, Slitherlink, Masyu, and Fillomino). The puzzle's name reflects its core mechanic: the shapes of polyomino-like blocks that players must draw gradually "evolve" in the directions indicated by pre-drawn arrows. We prove, by reduction from 3-SAT, that the question of whether there exists at least one solution to an Evolomino puzzle satisfying the rules is NP-complete. Since our reduction is parsimonious, i.e., it preserves the number of distinct solutions, we also prove that counting the number of solutions to an Evolomino puzzle is #P-complete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07611v2</guid>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrei V. Nikolaev</dc:creator>
    </item>
    <item>
      <title>Closure Properties and Characterizations of TotP</title>
      <link>https://arxiv.org/abs/2504.20262</link>
      <description>arXiv:2504.20262v5 Announce Type: replace 
Abstract: The class TotP consists of functions that count the number of all paths of a nondeterministic polynomial-time Turing machine. In this paper, we give a predicate based definition of TotP, analogous to a standard definition of #P. From a new characterization of TotP it follows that many well known #P problems belong to TotP, and TotP = #P if and only if P = NP. We show that TotP has several closure properties of #P and GapP, and also properties that are not known to hold for #P and GapP. We also prove that the closure of TotP under left composition with FP+ is equivalent to TotP = FP+ and P = PP, and give examples of FP+-functions such that if TotP is closed under composition with them, then it is closed under composition with FP+.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20262v5</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yaroslav Ivanashev</dc:creator>
    </item>
    <item>
      <title>PLS-completeness of string permutations</title>
      <link>https://arxiv.org/abs/2505.02622</link>
      <description>arXiv:2505.02622v2 Announce Type: replace 
Abstract: Bitstrings can be permuted via permutations and compared via the lexicographic order. In this paper we study the complexity of finding a minimum of a bitstring via given permutations. As a global optima is known to be NP-complete, we study the local optima via the class PLS and show hardness for PLS. Additionally, we show that even for one permutation the global optimization is NP-complete and give a formula that has these permutation as symmetries. This answers an open question inspired from Kolodziejczyk and Thapen and stated at the SAT and interactions seminar in Dagstuhl.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02622v2</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dominik Scheder, Johannes Tantow</dc:creator>
    </item>
    <item>
      <title>Beyond Worst-Case Subset Sum: An Adaptive, Structure-Aware Solver with Sub-$2^{n/2}$ Enumeration</title>
      <link>https://arxiv.org/abs/2503.20162</link>
      <description>arXiv:2503.20162v2 Announce Type: replace-cross 
Abstract: The Subset Sum problem, which asks whether a set of $n$ integers has a subset summing to a target $t$, is a fundamental NP-complete problem in cryptography and combinatorial optimization. The classical meet-in-the-middle (MIM) algorithm of Horowitz--Sahni runs in $\mathcal{O}^*(2^{n/2})$, which remains the best-known deterministic bound. Yet in practice, many instances exhibit abundant collisions in partial sums, so the true difficulty is often governed by $U = |\Sigma(S)|$, the number of unique subset sums.
  We present a structure-aware, adaptive solver that enumerates only the distinct subset sums, pruning duplicates on the fly and achieving deterministic runtime $\mathcal{O}(U \cdot n^2)$ and expected randomized runtime $\mathcal{O}(U \cdot n)$. Its core is a canonical unique-subset-sums enumerator combined with a double meet-in-the-middle strategy, supporting anytime and online modes.
  To ensure worst-case gains even on unstructured inputs, we introduce a Controlled Aliasing technique that provably reduces the enumeration space by a fixed constant factor. This yields a guaranteed global runtime of $\mathcal{O}^*(2^{n/2 - \varepsilon})$ for some $\varepsilon &gt; 0$, strictly improving upon classical bounds.
  Empirical results show that the solver adapts efficiently to structured inputs with low entropy (e.g., instances with small doubling constants, duplicates, or additive progressions) often approaching near-dynamic programming performance. We conclude by outlining how this adaptive framework can be extended to other NP-complete problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20162v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesus Salas</dc:creator>
    </item>
    <item>
      <title>List Decoding Expander-Based Codes up to Capacity in Near-Linear Time</title>
      <link>https://arxiv.org/abs/2504.20333</link>
      <description>arXiv:2504.20333v2 Announce Type: replace-cross 
Abstract: We give a new framework based on graph regularity lemmas, for list decoding and list recovery of codes based on spectral expanders. Using existing algorithms for computing regularity decompositions of sparse graphs in (randomized) near-linear time, and appropriate choices for the constant-sized inner/base codes, we prove the following:
  - Expander-based codes constructed using the distance amplification technique of Alon, Edmonds and Luby [FOCS 1995] with rate $\rho$, can be list decoded to a radius $1 - \rho - \epsilon$ in near-linear time. By known results, the output list has size $O(1/\epsilon)$.
  - The above codes of Alon, Edmonds and Luby, with rate $\rho$, can also be list recovered to radius $1 - \rho - \epsilon$ in near-linear time, with constant-sized output lists.
  - The Tanner code construction of Sipser and Spielman [IEEE Trans. Inf. Theory 1996] with distance $\delta$, can be list decoded to radius $\delta - \epsilon$ in near-linear time, with constant-sized output lists.
  Our results imply novel combinatorial as well as algorithmic bounds for each of the above explicit constructions. All of these bounds are obtained via combinatorial rigidity phenomena, proved using (weak) graph regularity. The regularity framework allows us to lift the list decoding and list recovery properties for the local base codes, to the global codes obtained via the above constructions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20333v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 18 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shashank Srivastava, Madhur Tulsiani</dc:creator>
    </item>
  </channel>
</rss>
