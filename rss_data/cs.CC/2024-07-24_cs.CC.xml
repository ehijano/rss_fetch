<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Jul 2024 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Inference of rankings planted in random tournaments</title>
      <link>https://arxiv.org/abs/2407.16597</link>
      <description>arXiv:2407.16597v1 Announce Type: cross 
Abstract: We consider the problem of inferring an unknown ranking of $n$ items from a random tournament on $n$ vertices whose edge directions are correlated with the ranking. We establish, in terms of the strength of these correlations, the computational and statistical thresholds for detection (deciding whether an observed tournament is purely random or drawn correlated with a hidden ranking) and recovery (estimating the hidden ranking with small error in Spearman's footrule or Kendall's tau metric on permutations). Notably, we find that this problem provides a new instance of a detection-recovery gap: solving the detection problem requires much weaker correlations than solving the recovery problem. In establishing these thresholds, we also identify simple algorithms for detection (thresholding a degree 2 polynomial) and recovery (outputting a ranking by the number of "wins" of a tournament vertex, i.e., the out-degree) that achieve optimal performance up to constants in the correlation strength. For detection, we find that the above low-degree polynomial algorithm is superior to a natural spectral algorithm. We also find that, whenever it is possible to achieve strong recovery (i.e., to estimate with vanishing error in the above metrics) of the hidden ranking, then the above "Ranking By Wins" algorithm not only does so, but also outputs a close approximation of the maximum likelihood estimator, a task that is NP-hard in the worst case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16597v1</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dmitriy Kunisky, Daniel A. Spielman, Xifan Yu</dc:creator>
    </item>
    <item>
      <title>Regenerative Ulam-von Neumann Algorithm: An Innovative Markov chain Monte Carlo Method for Matrix Inversion</title>
      <link>https://arxiv.org/abs/2407.16661</link>
      <description>arXiv:2407.16661v1 Announce Type: cross 
Abstract: This paper presents an extension of the classical Ulan-von Neumann Markov chain Monte-Carlo algorithm for the computation of the matrix inverse. The algorithm presented in this paper, termed as \emph{regenerative Ulam-von Neumann algorithm}, utilizes the regenerative structure of classical, non-truncated Neumann series defined by a non-singular matrix and produces an unbiased estimator of the matrix inverse. Furthermore, the accuracy of the proposed algorithm depends on a single parameter that controls the total number of Markov transitions simulated thus avoiding the challenge of balancing between the total number of Markov chain replications and its corresponding length as in the classical Ulam-von Neumann algorithm. To efficiently utilize the Markov chain transition samples in the calculation of the regenerative quantities, the proposed algorithm quantifies automatically the contribution of each Markov transition to all regenerative quantities by a carefully designed updating scheme that utilized three separate matrices containing the current weights, total weights, and regenerative cycle count, respectively. A probabilistic analysis of the performance of the algorithm, including the variance of the estimator, is provided. Finally, numerical experiments verify the qualitative effectiveness of the proposed scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16661v1</guid>
      <category>math.NA</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumyadip Ghosh, Lior Horesh, Vassilis Kalantzis, Yingdong Lu, Tomasz Nowicki</dc:creator>
    </item>
    <item>
      <title>Separations in Proof Complexity and TFNP</title>
      <link>https://arxiv.org/abs/2205.02168</link>
      <description>arXiv:2205.02168v3 Announce Type: replace 
Abstract: It is well-known that Resolution proofs can be efficiently simulated by Sherali-Adams (SA) proofs. We show, however, that any such simulation needs to exploit huge coefficients: Resolution cannot be efficiently simulated by SA when the coefficients are written in unary. We also show that Reversible Resolution (a variant of MaxSAT Resolution) cannot be efficiently simulated by Nullstellensatz (NS).
  These results have consequences for total NP search problems. First, we characterise the classes PPADS, PPAD, SOPL by unary-SA, unary-NS, and Reversible Resolution, respectively. Second, we show that, relative to an oracle, PLS $\not\subseteq$ PPP, SOPL $\not\subseteq$ PPA, and EOPL $\not\subseteq$ UEOPL. In particular, together with prior work, this gives a complete picture of the black-box relationships between all classical TFNP classes introduced in the 1990s.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.02168v3</guid>
      <category>cs.CC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3663758</arxiv:DOI>
      <dc:creator>Mika G\"o\"os, Alexandros Hollender, Siddhartha Jain, Gilbert Maystre, William Pires, Robert Robere, Ran Tao</dc:creator>
    </item>
    <item>
      <title>Elementary Quantum Recursion Schemes That Capture Quantum Polylogarithmic Time Computability of Quantum Functions</title>
      <link>https://arxiv.org/abs/2311.15884</link>
      <description>arXiv:2311.15884v3 Announce Type: replace 
Abstract: Quantum computing has been studied over the past four decades based on two computational models of quantum circuits and quantum Turing machines. To capture quantum polynomial-time computability, a new recursion-theoretic approach was taken lately by Yamakami [J. Symb. Logic 80, pp.~1546--1587, 2020] by way of recursion schematic definition, which constitutes six initial quantum functions and three construction schemes of composition, branching, and multi-qubit quantum recursion. By taking a similar approach, we look into quantum polylogarithmic-time computability and further explore the expressing power of elementary schemes designed for such quantum computation. In particular, we introduce an elementary form of the quantum recursion, called the fast quantum recursion, and formulate $EQS$ (elementary quantum schemes) of ``elementary'' quantum functions. This class $EQS$ captures exactly quantum polylogarithmic-time computability, which forms the complexity class BQPOLYLOGTIME. We also demonstrate the separation of BQPOLYLOGTIME from NLOGTIME and PPOLYLOGTIME. As a natural extension of $EQS$, we further consider an algorithmic procedural scheme that implements the well-known divide-and-conquer strategy. This divide-and-conquer scheme helps compute the parity function but the scheme cannot be realized within our system $EQS$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15884v3</guid>
      <category>cs.CC</category>
      <category>quant-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomoyuki Yamakami</dc:creator>
    </item>
    <item>
      <title>Average-case deterministic query complexity of boolean functions with fixed weight</title>
      <link>https://arxiv.org/abs/2403.03530</link>
      <description>arXiv:2403.03530v2 Announce Type: replace 
Abstract: We explore the $\textit{average-case deterministic query complexity}$ of boolean functions under a $\textit{uniform distribution}$, denoted by $\mathrm{D}_\mathrm{ave}(f)$, the minimum average depth of zero-error decision tree computing a boolean function $f$. This measure has found several applications across diverse fields, yet its understanding is limited. We study $\mathrm{D}_\mathrm{ave}(f)$ of several functions, including the penalty shoot-out function, symmetric functions, linear threshold functions and the tribes functions. We prove $\mathrm{D}_\mathrm{ave}(f) \le \max \{ \log \frac{\mathrm{wt}(f)}{\log n} + O(\log \log \frac{\mathrm{wt}(f)}{\log n}), O(1) \}$ for every $n$-variable boolean function $f$, where $\mathrm{wt}(f)$ denotes the weight (the number of inputs on which $f$ outputs $1$). For any $4\log n \le m(n) \le 2^{n-1}$, we prove the upper bound is tight up to an additive logarithmic term for almost all $n$-variable boolean functions with weight $\mathrm{wt}(f) = m(n)$. Using H\r{a}stad's switching lemma or Rossman's switching lemma [Comput. Complexity Conf. 137, 2019], one can derive $\mathrm{D}_\mathrm{ave}(f) \leq n(1 - \frac{1}{O(w)})$ or $\mathrm{D}_\mathrm{ave}(f) \le n(1 - \frac{1}{O(\log s)})$ for CNF/DNF formulas of width $w$ or size $s$, respectively. We show that, for any $w \ge \log n + \log \log n + 3$, there exists a DNF formula of width $w$ and size $\lceil 2^w / w \rceil$ such that $\mathrm{D}_\mathrm{ave}(f) = n (1 - \frac{\log n}{\Theta(w)})$. In other words, we show the criticality upper bounds $O(w)$ and $O(\log s)$ are tight up to a multiplicative $\log n$ factor, providing evidence on the tightness of the switching lemmas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03530v2</guid>
      <category>cs.CC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Li, Haowei Wu, Yi Yang</dc:creator>
    </item>
    <item>
      <title>Delta-modular ILP Problems of Bounded Co-dimension, Discrepancy, and Convolution</title>
      <link>https://arxiv.org/abs/2405.17001</link>
      <description>arXiv:2405.17001v2 Announce Type: replace 
Abstract: For $k, n \geq 0$, and $c \in Z^n$, we consider ILP problems \begin{gather*}
  \max\bigl\{ c^\top x \colon A x = b,\, x \in Z^n_{\geq 0} \bigr\}\text{ with $A \in Z^{k \times n}$, $rank(A) = k$, $b \in Z^{k}$ and}
  \max\bigl\{ c^\top x \colon A x \leq b,\, x \in Z^n \bigr\} \text{ with $A \in Z^{(n+k) \times n}$, $rank(A) = n$, $b \in Z^{n+k}$.} \end{gather*} The first problem is called an \emph{ILP problem in the standard form of the codimension $k$}, and the second problem is called an \emph{ILP problem in the canonical form with $n+k$ constraints.} We show that, for any sufficiently large $\Delta$, both problems can be solved with $$ 2^{O(k)} \cdot (f_{k,d} \cdot \Delta)^2 / 2^{\Omega\bigl(\sqrt{\log(f_{k,d} \cdot \Delta)}\bigr)} $$ operations, where $
  f_{k,d} = \min \Bigl\{ k^{k/2},
  \bigl(\log k \cdot \log (d + k)\bigr)^{k/2}
  \Bigr\} $, $d$ is the dimension of a corresponding polyhedron and $\Delta$ is the maximum absolute value of $rank(A) \times rank(A)$ sub-determinants of $A$.
  As our second main result, we show that the feasibility variants of both problems can be solved with $$ 2^{O(k)} \cdot f_{k,d} \cdot \Delta \cdot \log^3(f_{k,d} \cdot \Delta) $$ operations. The constant $f_{k,d}$ can be replaced by other constant $g_{k,\Delta} = \bigl(\log k \cdot \log (k \Delta)\bigr)^{k/2}$ that depends only on $k$ and $\Delta$. Additionally, we consider different partial cases with $k=0$ and $k=1$, which have interesting applications.
  As a result of independent interest, we propose an $n^2/2^{\Omega\bigl(\sqrt{\log n}\bigr)}$-time algorithm for the tropical convolution problem on sequences, indexed by elements of a finite Abelian group of the order $n$. Additionally, we give a complete, self-contained error analysis of the generalized Discrete Fourier Transform for Abelian groups with respect to the Word-RAM computational model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17001v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.AC</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>D. Gribanov, D. Malyshev, P. M. Pardalos</dc:creator>
    </item>
    <item>
      <title>Equality cases of the Alexandrov--Fenchel inequality are not in the polynomial hierarchy</title>
      <link>https://arxiv.org/abs/2309.05764</link>
      <description>arXiv:2309.05764v3 Announce Type: replace-cross 
Abstract: Describing the equality conditions of the Alexandrov--Fenchel inequality has been a major open problem for decades. We prove that in the case of convex polytopes, this description is not in the polynomial hierarchy unless the polynomial hierarchy collapses to a finite level. This is the first hardness result for the problem, and is a complexity counterpart of the recent result by Shenfeld and van Handel (arXiv:archive/201104059), which gave a geometric characterization of the equality conditions. The proof involves Stanley's order polytopes and employs poset theoretic technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.05764v3</guid>
      <category>math.CO</category>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <category>cs.DM</category>
      <category>math.MG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Swee Hong Chan, Igor Pak</dc:creator>
    </item>
    <item>
      <title>The Computational Advantage of MIP* Vanishes in the Presence of Noise</title>
      <link>https://arxiv.org/abs/2312.04360</link>
      <description>arXiv:2312.04360v2 Announce Type: replace-cross 
Abstract: Quantum multiprover interactive proof systems with entanglement MIP* are much more powerful than its classical counterpart MIP (Babai et al. '91, Ji et al. '20): while MIP = NEXP, the quantum class MIP* is equal to RE, a class including the halting problem. This is because the provers in MIP* can share unbounded quantum entanglement. However, recent works of Qin and Yao '21 and '23 have shown that this advantage is significantly reduced if the provers' shared state contains noise. This paper attempts to exactly characterize the effect of noise on the computational power of quantum multiprover interactive proof systems. We investigate the quantum two-prover one-round interactive system MIP*[poly, O(1)], where the verifier sends polynomially many bits to the provers and the provers send back constantly many bits. We show noise completely destroys the computational advantage given by shared entanglement in this model. Specifically, we show that if the provers are allowed to share arbitrarily many noisy EPR states, where each EPR state is affected by an arbitrarily small constant amount of noise, the resulting complexity class is equivalent to NEXP = MIP. This improves significantly on the previous best-known bound of NEEEXP (nondeterministic triply exponential time) by Qin and Yao '21. We also show that this collapse in power is due to the noise, rather than the O(1) answer size, by showing that allowing for noiseless EPR states gives the class the full power of RE = MIP*[poly, poly]. Along the way, we develop two technical tools of independent interest. First, we give a new, deterministic tester for the positivity of an exponentially large matrix, provided it has a low-degree Fourier decomposition in terms of Pauli matrices. Secondly, we develop a new invariance principle for smooth matrix functions having bounded third-order Fr\'echet derivatives or which are Lipschitz continous.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04360v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yangjing Dong, Honghao Fu, Anand Natarajan, Minglong Qin, Haochen Xu, Penghui Yao</dc:creator>
    </item>
    <item>
      <title>Integer Programming Using A Single Atom</title>
      <link>https://arxiv.org/abs/2402.16541</link>
      <description>arXiv:2402.16541v3 Announce Type: replace-cross 
Abstract: Integer programming (IP), as the name suggests is an integer-variable-based approach commonly used to formulate real-world optimization problems with constraints. Currently, quantum algorithms reformulate the IP into an unconstrained form through the use of binary variables, which is an indirect and resource-consuming way of solving it. We develop an algorithm that maps and solves an IP problem in its original form to any quantum system possessing a large number of accessible internal degrees of freedom that are controlled with sufficient accuracy. This work leverages the principle of superposition to solve the optimization problem. Using a single Rydberg atom as an example, we associate the integer values to electronic states belonging to different manifolds and implement a selective superposition of different states to solve the full IP problem. The optimal solution is found within a few microseconds for prototypical IP problems with up to eight variables and four constraints. This also includes non-linear IP problems, which are usually harder to solve with classical algorithms when compared to their linear counterparts. Our algorithm for solving IP is benchmarked by a well-known classical algorithm (branch and bound) in terms of the number of steps needed for convergence to the solution. This approach carries the potential to improve the solutions obtained for larger-size problems using hybrid quantum-classical algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16541v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>math.OC</category>
      <category>physics.atom-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kapil Goswami, Peter Schmelcher, Rick Mukherjee</dc:creator>
    </item>
    <item>
      <title>Arborescences and Shortest Path Trees when Colors Matter</title>
      <link>https://arxiv.org/abs/2403.06580</link>
      <description>arXiv:2403.06580v2 Announce Type: replace-cross 
Abstract: Color-constrained subgraph problems are those where we are given an edge-colored (directed or undirected) graph and the task is to find a specific type of subgraph, like a spanning tree, an arborescence, a single-source shortest path tree, a perfect matching etc., with constraints on the number of edges of each color. Some of these problems, like color-constrained spanning tree, have elegant solutions and some of them, like color-constrained perfect matching, are longstanding open questions. In this work, we study color-constrained arborescences and shortest path trees. Computing a color-constrained shortest path tree on weighted digraphs turns out to be NP-hard in general but polynomial-time solvable when all cycles have positive weight. This polynomial-time solvability is due to the fact that the solution space is essentially the set of all color-constrained arborescences of a directed acyclic subgraph of the original graph. While finding color-constrained arborescence of digraphs is NP-hard in general, we give efficient algorithms when the input graph is acyclic. Consequently, a color-constrained shortest path tree on weighted digraphs having only positive weight cycles can be efficiently computed. Our algorithms also generalize to the problem of finding a color-constrained shortest path tree with minimum total weight. En route, we sight nice connections to colored matroids and color-constrained bases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.06580v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P. S. Ardra, Jasine Babu, Kritika Kashyap, R. Krithika, Sreejith K. Pallathumadam, Deepak Rajendraprasad</dc:creator>
    </item>
  </channel>
</rss>
