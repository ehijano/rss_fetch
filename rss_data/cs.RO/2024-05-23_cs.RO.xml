<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.RO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.RO</link>
    <description>cs.RO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.RO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 May 2024 04:00:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Pure Planning to Pure Policies and In Between with a Recursive Tree Planner</title>
      <link>https://arxiv.org/abs/2405.13130</link>
      <description>arXiv:2405.13130v1 Announce Type: new 
Abstract: A recursive tree planner (RTP) is designed to function as a pure planner without policies at one extreme and run a pure greedy policy at the other. In between, the RTP exploits policies to improve planning performance and improve zero-shot transfer from one class of planning problem to another. Policies are learned through imitation of the planner. These are then used by the planner to improve policies in a virtuous cycle. To improve planning performance and zero-shot transfer, the RTP incorporates previously learned tasks as generalized actions (GA) at any level of its hierarchy, and can refine those GA by adding primitive actions at any level too. For search, the RTP uses a generalized Dijkstra algorithm [Dijkstra 1959] which tries the greedy policy first and then searches over near-greedy paths and then farther away as necessary. The RPT can return multiple sub-goals from lower levels as well as boundary states near obstacles, and can exploit policies with background and object-number invariance. Policies at all levels of the hierarchy can be learned simultaneously or in any order or come from outside the framework. The RTP is tested here on a variety of Box2d [Cato 2022] problems, including the classic lunar lander [Farama 2022], and on the MuJoCo [Todorov et al 2012] inverted pendulum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13130v1</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. Norman Redlich</dc:creator>
    </item>
    <item>
      <title>Offline robot programming assisted by task demonstration: an AutomationML interoperable solution for glass adhesive application and welding</title>
      <link>https://arxiv.org/abs/2405.13141</link>
      <description>arXiv:2405.13141v1 Announce Type: new 
Abstract: Robots have been successfully deployed in both traditional and novel manufacturing processes. However, they are still difficult to program by non-experts, which limits their accessibility to a wider range of potential users. Programming robots requires expertise in both robotics and the specific manufacturing process in which they are applied. Robot programs created offline often lack parameters that represent relevant manufacturing skills when executing a specific task. These skills encompass aspects like robot orientation and velocity. This paper introduces an intuitive robot programming system designed to capture manufacturing skills from task demonstrations performed by skilled workers. Demonstration data, including orientations and velocities of the working paths, are acquired using a magnetic tracking system fixed to the tools used by the worker. Positional data are extracted from CAD/CAM. Robot path poses are transformed into Cartesian space and validated in simulation, subsequently leading to the generation of robot programs. PathML, an AutomationML-based syntax, integrates robot and manufacturing data across the heterogeneous elements and stages of the manufacturing systems considered. Experiments conducted on the glass adhesive application and welding processes showcased the intuitive nature of the system, with path errors falling within the functional tolerance range.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13141v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/0951192X.2024.2358042</arxiv:DOI>
      <dc:creator>M. Babcinschi, F. Cruz, N. Duarte, S. Santos, S. Alves, P. Neto</dc:creator>
    </item>
    <item>
      <title>One-Shot Imitation Learning with Invariance Matching for Robotic Manipulation</title>
      <link>https://arxiv.org/abs/2405.13178</link>
      <description>arXiv:2405.13178v1 Announce Type: new 
Abstract: Learning a single universal policy that can perform a diverse set of manipulation tasks is a promising new direction in robotics. However, existing techniques are limited to learning policies that can only perform tasks that are encountered during training, and require a large number of demonstrations to learn new tasks. Humans, on the other hand, often can learn a new task from a single unannotated demonstration. In this work, we propose the Invariance-Matching One-shot Policy Learning (IMOP) algorithm. In contrast to the standard practice of learning the end-effector's pose directly, IMOP first learns invariant regions of the state space for a given task, and then computes the end-effector's pose through matching the invariant regions between demonstrations and test scenes. Trained on the 18 RLBench tasks, IMOP achieves a success rate that outperforms the state-of-the-art consistently, by 4.5% on average over the 18 tasks. More importantly, IMOP can learn a novel task from a single unannotated demonstration, and without any fine-tuning, and achieves an average success rate improvement of $11.5\%$ over the state-of-the-art on 22 novel tasks selected across nine categories. IMOP can also generalize to new shapes and learn to manipulate objects that are different from those in the demonstration. Further, IMOP can perform one-shot sim-to-real transfer using a single real-robot demonstration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13178v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyu Zhang, Abdeslam Boularias</dc:creator>
    </item>
    <item>
      <title>BeadSight: An Inexpensive Tactile Sensor Using Hydro-Gel Beads</title>
      <link>https://arxiv.org/abs/2405.13204</link>
      <description>arXiv:2405.13204v1 Announce Type: new 
Abstract: In robotic manipulation, tactile sensors are indispensable, especially when dealing with soft objects, objects of varying dimensions, or those out of the robot's direct line of sight. Traditional tactile sensors often grapple with challenges related to cost and durability. To address these issues, our study introduces a novel approach to visuo-tactile sensing with an emphasis on economy and replacablity. Our proposed sensor, BeadSight, uses hydro-gel beads encased in a vinyl bag as an economical, easily replaceable sensing medium. When the sensor makes contact with a surface, the deformation of the hydrogel beads is observed using a rear camera. This observation is then passed through a U-net Neural Network to predict the forces acting on the surface of the bead bag, in the form of a pressure map. Our results show that the sensor can accurately predict these pressure maps, detecting the location and magnitude of forces applied to the surface. These abilities make BeadSight an effective, inexpensive, and easily replaceable tactile sensor, ideal for many robotics applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13204v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abraham George, Yibo Chen, Atharva Dikshit, Peter Pak, Amir Barati Farimani</dc:creator>
    </item>
    <item>
      <title>A Survey of Robotic Language Grounding: Tradeoffs Between Symbols and Embeddings</title>
      <link>https://arxiv.org/abs/2405.13245</link>
      <description>arXiv:2405.13245v1 Announce Type: new 
Abstract: With large language models, robots can understand language more flexibly and more capable than ever before. This survey reviews recent literature and situates it into a spectrum with two poles: 1) mapping between language and some manually defined formal representation of meaning, and 2) mapping between language and high-dimensional vector spaces that translate directly to low-level robot policy. Using a formal representation allows the meaning of the language to be precisely represented, limits the size of the learning problem, and leads to a framework for interpretability and formal safety guarantees. Methods that embed language and perceptual data into high-dimensional spaces avoid this manually specified symbolic structure and thus have the potential to be more general when fed enough data but require more data and computing to train. We discuss the benefits and tradeoffs of each approach and finish by providing directions for future work that achieves the best of both worlds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13245v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vanya Cohen, Jason Xinyu Liu, Raymond Mooney, Stefanie Tellex, David Watkins</dc:creator>
    </item>
    <item>
      <title>Deep Learning-Driven State Correction: A Hybrid Architecture for Radar-Based Dynamic Occupancy Grid Mapping</title>
      <link>https://arxiv.org/abs/2405.13307</link>
      <description>arXiv:2405.13307v1 Announce Type: new 
Abstract: This paper introduces a novel hybrid architecture that enhances radar-based Dynamic Occupancy Grid Mapping (DOGM) for autonomous vehicles, integrating deep learning for state-classification. Traditional radar-based DOGM often faces challenges in accurately distinguishing between static and dynamic objects. Our approach addresses this limitation by introducing a neural network-based DOGM state correction mechanism, designed as a semantic segmentation task, to refine the accuracy of the occupancy grid. Additionally a heuristic fusion approach is proposed which allows to enhance performance without compromising on safety. We extensively evaluate this hybrid architecture on the NuScenes Dataset, focusing on its ability to improve dynamic object detection as well grid quality. The results show clear improvements in the detection capabilities of dynamic objects, highlighting the effectiveness of the deep learning-enhanced state correction in radar-based DOGM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13307v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Peter Ronecker, Xavier Diaz, Michael Karner, Daniel Watzenig</dc:creator>
    </item>
    <item>
      <title>BenchNav: Simulation Platform for Benchmarking Off-road Navigation Algorithms with Probabilistic Traversability</title>
      <link>https://arxiv.org/abs/2405.13318</link>
      <description>arXiv:2405.13318v1 Announce Type: new 
Abstract: As robotic navigation techniques in perception and planning advance, mobile robots increasingly venture into off-road environments involving complex traversability. However, selecting suitable planning methods remains a challenge due to their algorithmic diversity, as each offers unique benefits. To aid in algorithm design, we introduce BenchNav, an open-source PyTorch-based simulation platform for benchmarking off-road navigation with uncertain traversability. Built upon Gymnasium, BenchNav provides three key features: 1) a data generation pipeline for preparing synthetic natural environments, 2) built-in machine learning models for traversability prediction, and 3) consistent execution of path and motion planning across different algorithms. We show BenchNav's versatility through simulation examples in off-road environments, employing three representative planning algorithms from different domains. https://github.com/masafumiendo/benchnav</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13318v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masafumi Endo, Kohei Honda, Genya Ishigami</dc:creator>
    </item>
    <item>
      <title>Autonomous Algorithm for Training Autonomous Vehicles with Minimal Human Intervention</title>
      <link>https://arxiv.org/abs/2405.13345</link>
      <description>arXiv:2405.13345v1 Announce Type: new 
Abstract: Reinforcement learning (RL) provides a compelling framework for enabling autonomous vehicles to continue to learn and improve diverse driving behaviors on their own. However, training real-world autonomous vehicles with current RL algorithms presents several challenges. One critical challenge, often overlooked in these algorithms, is the need to reset a driving environment between every episode. While resetting an environment after each episode is trivial in simulated settings, it demands significant human intervention in the real world. In this paper, we introduce a novel autonomous algorithm that allows off-the-shelf RL algorithms to train an autonomous vehicle with minimal human intervention. Our algorithm takes into account the learning progress of the autonomous vehicle to determine when to abort episodes before it enters unsafe states and where to reset it for subsequent episodes in order to gather informative transitions. The learning progress is estimated based on the novelty of both current and future states. We also take advantage of rule-based autonomous driving algorithms to safely reset an autonomous vehicle to an initial state. We evaluate our algorithm against baselines on diverse urban driving tasks. The experimental results show that our algorithm is task-agnostic and achieves better driving performance with fewer manual resets than baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13345v1</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sang-Hyun Lee, Daehyeok Kwon, Seung-Woo Seo</dc:creator>
    </item>
    <item>
      <title>Towards Safe Mid-Air Drone Interception: Strategies for Tracking &amp; Capture</title>
      <link>https://arxiv.org/abs/2405.13542</link>
      <description>arXiv:2405.13542v1 Announce Type: new 
Abstract: A unique approach for the mid-air autonomous aerial interception of non-cooperating UAV by a flying robot equipped with a net is presented in this paper. A novel interception guidance method dubbed EPN is proposed, designed to catch agile maneuvering targets while relying on onboard state estimation and tracking. The proposed method is compared with state-of-the-art approaches in simulations using 100 different trajectories of the target with varying complexity comprising almost 14 hours of flight data, and EPN demonstrates the shortest response time and the highest number of interceptions, which are key parameters of agile interception. To enable robust transfer from theory and simulation to a real-world implementation, we aim to avoid overfitting to specific assumptions about the target, and to tackle interception of a target following an unknown general trajectory. Furthermore, we identify several often overlooked problems related to tracking and estimation of the target's state that can have a significant influence on the overall performance of the system. We propose the use of a novel state estimation filter based on the IMM filter and a new measurement model. Simulated experiments show that the proposed solution provides significant improvements in estimation accuracy over the commonly employed KF approaches when considering general trajectories. Based on these results, we employ the proposed filtering and guidance methods to implement a complete autonomous interception system, which is thoroughly evaluated in realistic simulations and tested in real-world experiments with a maneuvering target going far beyond the performance of any state-of-the-art solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13542v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michal Pliska, Matou\v{s} Vrba, Tom\'a\v{s} B\'a\v{c}a, Martin Saska</dc:creator>
    </item>
    <item>
      <title>HighwayLLM: Decision-Making and Navigation in Highway Driving with RL-Informed Language Model</title>
      <link>https://arxiv.org/abs/2405.13547</link>
      <description>arXiv:2405.13547v1 Announce Type: new 
Abstract: Autonomous driving is a complex task which requires advanced decision making and control algorithms. Understanding the rationale behind the autonomous vehicles' decision is crucial to ensure their safe and effective operation on highway driving. This study presents a novel approach, HighwayLLM, which harnesses the reasoning capabilities of large language models (LLMs) to predict the future waypoints for ego-vehicle's navigation. Our approach also utilizes a pre-trained Reinforcement Learning (RL) model to serve as a high-level planner, making decisions on appropriate meta-level actions. The HighwayLLM combines the output from the RL model and the current state information to make safe, collision-free, and explainable predictions for the next states, thereby constructing a trajectory for the ego-vehicle. Subsequently, a PID-based controller guides the vehicle to the waypoints predicted by the LLM agent. This integration of LLM with RL and PID enhances the decision-making process and provides interpretability for highway autonomous driving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13547v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mustafa Yildirim, Barkin Dagda, Saber Fallah</dc:creator>
    </item>
    <item>
      <title>Learning Manipulation Skills through Robot Chain-of-Thought with Sparse Failure Guidance</title>
      <link>https://arxiv.org/abs/2405.13573</link>
      <description>arXiv:2405.13573v1 Announce Type: new 
Abstract: The acquisition of manipulation skills through language instruction remains an unresolved challenge. Recently, vision-language models have made significant progress in teaching robots these skills. However, their performance is restricted to a narrow range of simple tasks. In this paper, we propose that vision-language models can provide a superior source of rewards for agents. Our method decomposes complex tasks into simpler sub-goals, enabling better task comprehension and avoiding potential failures with sparse failure guidance. Empirical evidence demonstrates that our algorithm consistently outperforms baselines such as CLIP, LIV, and RoboCLIP. Specifically, our algorithm achieves a $5.4\times$ higher average success rate compared to the best baseline, RoboCLIP, across a series of manipulation tasks. It has shown a comprehensive understanding of a wide range of robotic manipulation tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13573v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaifeng Zhang, Zhao-Heng Yin, Weirui Ye, Yang Gao</dc:creator>
    </item>
    <item>
      <title>Waverider: Leveraging Hierarchical, Multi-Resolution Maps for Efficient and Reactive Obstacle Avoidance</title>
      <link>https://arxiv.org/abs/2405.13617</link>
      <description>arXiv:2405.13617v1 Announce Type: new 
Abstract: Fast and reliable obstacle avoidance is an important task for mobile robots. In this work, we propose an efficient reactive system that provides high-quality obstacle avoidance while running at hundreds of hertz with minimal resource usage. Our approach combines wavemap, a hierarchical volumetric map representation, with a novel hierarchical and parallelizable obstacle avoidance algorithm formulated through Riemannian Motion Policies (RMP). Leveraging multi-resolution obstacle avoidance policies, the proposed navigation system facilitates precise, low-latency (36ms), and extremely efficient obstacle avoidance with a very large perceptive radius (30m). We perform extensive statistical evaluations on indoor and outdoor maps, verifying that the proposed system compares favorably to fixed-resolution RMP variants and CHOMP. Finally, the RMP formulation allows the seamless fusion of obstacle avoidance with additional objectives, such as goal-seeking, to obtain a fully-fledged navigation system that is versatile and robust. We deploy the system on a Micro Aerial Vehicle and show how it navigates through an indoor obstacle course. Our complete implementation, called waverider, is made available as open source.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13617v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Reijgwart, Michael Pantic, Roland Siegwart, Lionel Ott</dc:creator>
    </item>
    <item>
      <title>Safe and Personalizable Logical Guidance for Trajectory Planning of Autonomous Driving</title>
      <link>https://arxiv.org/abs/2405.13704</link>
      <description>arXiv:2405.13704v1 Announce Type: new 
Abstract: Autonomous vehicles necessitate a delicate balance between safety, efficiency, and user preferences in trajectory planning. Existing traditional or learning-based methods face challenges in adequately addressing all these aspects. In response, this paper proposes a novel component termed the Logical Guidance Layer (LGL), designed for seamless integration into autonomous driving trajectory planning frameworks, specifically tailored for highway scenarios. The LGL guides the trajectory planning with a local target area determined through scenario reasoning, scenario evaluation, and guidance area calculation. Integrating the Responsibility-Sensitive Safety (RSS) model, the LGL ensures formal safety guarantees while accommodating various user preferences defined by logical formulae. Experimental validation demonstrates the effectiveness of the LGL in achieving a balance between safety and efficiency, and meeting user preferences in autonomous highway driving scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13704v1</guid>
      <category>cs.RO</category>
      <category>cs.LO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuejiao Xu, Ruolin Wang, Chengpeng Xu, Jianmin Ji</dc:creator>
    </item>
    <item>
      <title>Low Fidelity Digital Twin for Automated Driving Systems: Use Cases and Automatic Generation</title>
      <link>https://arxiv.org/abs/2405.13705</link>
      <description>arXiv:2405.13705v1 Announce Type: new 
Abstract: Automated driving systems are an integral part of the automotive industry. Tools such as Robot Operating System and simulators support their development. However, in the end, the developers must test their algorithms on a real vehicle. To better observe the difference between reality and simulation--the reality gap--digital twin technology offers real-time communication between the real vehicle and its model. We present low fidelity digital twin generator and describe situations where automatic generation is preferable to high fidelity simulation. We validated our approach of generating a virtual environment with a vehicle model by replaying the data recorded from the real vehicle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13705v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiri Vlasak, Jaroslav Klap\'alek, Adam Kollar\v{c}\'ik, Michal Sojka, Zden\v{e}k Hanz\'alek</dc:creator>
    </item>
    <item>
      <title>GameVLM: A Decision-making Framework for Robotic Task Planning Based on Visual Language Models and Zero-sum Games</title>
      <link>https://arxiv.org/abs/2405.13751</link>
      <description>arXiv:2405.13751v1 Announce Type: new 
Abstract: With their prominent scene understanding and reasoning capabilities, pre-trained visual-language models (VLMs) such as GPT-4V have attracted increasing attention in robotic task planning. Compared with traditional task planning strategies, VLMs are strong in multimodal information parsing and code generation and show remarkable efficiency. Although VLMs demonstrate great potential in robotic task planning, they suffer from challenges like hallucination, semantic complexity, and limited context. To handle such issues, this paper proposes a multi-agent framework, i.e., GameVLM, to enhance the decision-making process in robotic task planning. In this study, VLM-based decision and expert agents are presented to conduct the task planning. Specifically, decision agents are used to plan the task, and the expert agent is employed to evaluate these task plans. Zero-sum game theory is introduced to resolve inconsistencies among different agents and determine the optimal solution. Experimental results on real robots demonstrate the efficacy of the proposed framework, with an average success rate of 83.3%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13751v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aoran Mei, Jianhua Wang, Guo-Niu Zhu, Zhongxue Gan</dc:creator>
    </item>
    <item>
      <title>Expansion-GRR: Efficient Generation of Smooth Global Redundancy Resolution Roadmaps</title>
      <link>https://arxiv.org/abs/2405.13770</link>
      <description>arXiv:2405.13770v1 Announce Type: new 
Abstract: Global redundancy resolution (GRR) roadmap is a novel concept in robotics that facilitates the mapping from task space paths to configuration space paths in a legible, predictable, and repeatable way. Such roadmaps could find widespread utility in applications such as safe teleoperation, consistent path planning, and factory workcell design. However, the previous methods to compute GRR roadmaps often necessitate a lengthy computation time and produce non-smooth paths, limiting their practical efficacy. To address this challenge, we introduce a novel method Expansion-GRR that leverages efficient configuration space projections and enables a rapid generation of smooth roadmaps that satisfy the task constraints. Additionally, we propose a simple multi-seed strategy that further enhances the final quality. We conducted experiments in simulation with a 5-link planar manipulator and a Kinova arm. We were able to generate the GRR roadmaps up to 2 orders of magnitude faster while achieving higher smoothness. We also demonstrate the utility of the GRR roadmaps in teleoperation tasks where our method outperformed prior methods and reactive IK solvers in terms of success rate and solution quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13770v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuoyun Zhong, Zhi Li, Constantinos Chamzas</dc:creator>
    </item>
    <item>
      <title>Robot Explanation Identity</title>
      <link>https://arxiv.org/abs/2405.13841</link>
      <description>arXiv:2405.13841v1 Announce Type: new 
Abstract: To bring robots into human everyday life, their capacity for social interaction must increase. One way for robots to acquire social skills is by assigning them the concept of identity. This research focuses on the concept of \textit{Explanation Identity} within the broader context of robots' roles in society, particularly their ability to interact socially and explain decisions. Explanation Identity refers to the combination of characteristics and approaches robots use to justify their actions to humans. Drawing from different technical and social disciplines, we introduce Explanation Identity as a multidisciplinary concept and discuss its importance in Human-Robot Interaction. Our theoretical framework highlights the necessity for robots to adapt their explanations to the user's context, demonstrating empathy and ethical integrity. This research emphasizes the dynamic nature of robot identity and guides the integration of explanation capabilities in social robots, aiming to improve user engagement and acceptance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13841v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amar Halilovic, Senka Krivic</dc:creator>
    </item>
    <item>
      <title>Embodied Design for Enhanced Flipper-Based Locomotion in Complex Terrains</title>
      <link>https://arxiv.org/abs/2405.13948</link>
      <description>arXiv:2405.13948v1 Announce Type: new 
Abstract: Robots are becoming increasingly essential for traversing complex environments such as disaster areas, extraterrestrial terrains, and marine environments. Yet, their potential is often limited by mobility and adaptability constraints. In nature, various animals have evolved finely tuned designs and anatomical features that enable efficient locomotion in diverse environments. Sea turtles, for instance, possess specialized flippers that facilitate both long-distance underwater travel and adept maneuvers across a range of coastal terrains. Building on the principles of embodied intelligence and drawing inspiration from sea turtle hatchings, this paper examines the critical interplay between a robot's physical form and its environmental interactions, focusing on how morphological traits and locomotive behaviors affect terrestrial navigation. We present a bio-inspired robotic system and study the impacts of flipper/body morphology and gait patterns on its terrestrial mobility across diverse terrains ranging from sand to rocks. Evaluating key performance metrics such as speed and cost of transport, our experimental results highlight adaptive designs as crucial for multi-terrain robotic mobility to achieve not only speed and efficiency but also the versatility needed to tackle the varied and complex terrains encountered in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13948v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nnamdi Chikere, John McElroy, Yasemin Ozkan-Aydin</dc:creator>
    </item>
    <item>
      <title>Uncertainty-Aware DRL for Autonomous Vehicle Crowd Navigation in Shared Space</title>
      <link>https://arxiv.org/abs/2405.13969</link>
      <description>arXiv:2405.13969v1 Announce Type: new 
Abstract: Safe, socially compliant, and efficient navigation of low-speed autonomous vehicles (AVs) in pedestrian-rich environments necessitates considering pedestrians' future positions and interactions with the vehicle and others. Despite the inevitable uncertainties associated with pedestrians' predicted trajectories due to their unobserved states (e.g., intent), existing deep reinforcement learning (DRL) algorithms for crowd navigation often neglect these uncertainties when using predicted trajectories to guide policy learning. This omission limits the usability of predictions when diverging from ground truth. This work introduces an integrated prediction and planning approach that incorporates the uncertainties of predicted pedestrian states in the training of a model-free DRL algorithm. A novel reward function encourages the AV to respect pedestrians' personal space, decrease speed during close approaches, and minimize the collision probability with their predicted paths. Unlike previous DRL methods, our model, designed for AV operation in crowded spaces, is trained in a novel simulation environment that reflects realistic pedestrian behaviour in a shared space with vehicles. Results show a 40% decrease in collision rate and a 15% increase in minimum distance to pedestrians compared to the state of the art model that does not account for prediction uncertainty. Additionally, the approach outperforms model predictive control methods that incorporate the same prediction uncertainties in terms of both performance and computational time, while producing trajectories closer to human drivers in similar scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13969v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahsa Golchoubian, Moojan Ghafurian, Kerstin Dautenhahn, Nasser Lashgarian Azad</dc:creator>
    </item>
    <item>
      <title>Neural Scaling Laws for Embodied AI</title>
      <link>https://arxiv.org/abs/2405.14005</link>
      <description>arXiv:2405.14005v1 Announce Type: new 
Abstract: Scaling laws have driven remarkable progress across machine learning domains like language modeling and computer vision. However, the exploration of scaling laws in embodied AI and robotics has been limited, despite the rapidly increasing usage of machine learning in this field. This paper presents the first study to quantify scaling laws for Robot Foundation Models (RFMs) and the use of LLMs in robotics tasks. Through a meta-analysis spanning 198 research papers, we analyze how key factors like compute, model size, and training data quantity impact model performance across various robotic tasks. Our findings confirm that scaling laws apply to both RFMs and LLMs in robotics, with performance consistently improving as resources increase. The power law coefficients for RFMs closely match those of LLMs in robotics, resembling those found in computer vision and outperforming those for LLMs in the language domain. We also note that these coefficients vary with task complexity, with familiar tasks scaling more efficiently than unfamiliar ones, emphasizing the need for large and diverse datasets. Furthermore, we highlight the absence of standardized benchmarks in embodied AI. Most studies indicate diminishing returns, suggesting that significant resources are necessary to achieve high performance, posing challenges due to data and computational limitations. Finally, as models scale, we observe the emergence of new capabilities, particularly related to data and model size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14005v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Sartor, Neil Thompson</dc:creator>
    </item>
    <item>
      <title>A Survey on Vision-Language-Action Models for Embodied AI</title>
      <link>https://arxiv.org/abs/2405.14093</link>
      <description>arXiv:2405.14093v1 Announce Type: new 
Abstract: Deep learning has demonstrated remarkable success across many domains, including computer vision, natural language processing, and reinforcement learning. Representative artificial neural networks in these fields span convolutional neural networks, Transformers, and deep Q-networks. Built upon unimodal neural networks, numerous multi-modal models have been introduced to address a range of tasks such as visual question answering, image captioning, and speech recognition. The rise of instruction-following robotic policies in embodied AI has spurred the development of a novel category of multi-modal models known as vision-language-action models (VLAs). Their multi-modality capability has become a foundational element in robot learning. Various methods have been proposed to enhance traits such as versatility, dexterity, and generalizability. Some models focus on refining specific components through pretraining. Others aim to develop control policies adept at predicting low-level actions. Certain VLAs serve as high-level task planners capable of decomposing long-horizon tasks into executable subtasks. Over the past few years, a myriad of VLAs have emerged, reflecting the rapid advancement of embodied AI. Therefore, it is imperative to capture the evolving landscape through a comprehensive survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14093v1</guid>
      <category>cs.RO</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, Irwin King</dc:creator>
    </item>
    <item>
      <title>Learning Multimodal Confidence for Intention Recognition in Human-Robot Interaction</title>
      <link>https://arxiv.org/abs/2405.14116</link>
      <description>arXiv:2405.14116v1 Announce Type: new 
Abstract: The rapid development of collaborative robotics has provided a new possibility of helping the elderly who has difficulties in daily life, allowing robots to operate according to specific intentions. However, efficient human-robot cooperation requires natural, accurate and reliable intention recognition in shared environments. The current paramount challenge for this is reducing the uncertainty of multimodal fused intention to be recognized and reasoning adaptively a more reliable result despite current interactive condition. In this work we propose a novel learning-based multimodal fusion framework Batch Multimodal Confidence Learning for Opinion Pool (BMCLOP). Our approach combines Bayesian multimodal fusion method and batch confidence learning algorithm to improve accuracy, uncertainty reduction and success rate given the interactive condition. In particular, the generic and practical multimodal intention recognition framework can be easily extended further. Our desired assistive scenarios consider three modalities gestures, speech and gaze, all of which produce categorical distributions over all the finite intentions. The proposed method is validated with a six-DoF robot through extensive experiments and exhibits high performance compared to baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14116v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiyuan Zhao, Huijun Li, Tianyuan Miao, Xianyi Zhu, Zhikai Wei, Aiguo Song</dc:creator>
    </item>
    <item>
      <title>Transformers for Image-Goal Navigation</title>
      <link>https://arxiv.org/abs/2405.14128</link>
      <description>arXiv:2405.14128v1 Announce Type: new 
Abstract: Visual perception and navigation have emerged as major focus areas in the field of embodied artificial intelligence. We consider the task of image-goal navigation, where an agent is tasked to navigate to a goal specified by an image, relying only on images from an onboard camera. This task is particularly challenging since it demands robust scene understanding, goal-oriented planning and long-horizon navigation. Most existing approaches typically learn navigation policies reliant on recurrent neural networks trained via online reinforcement learning. However, training such policies requires substantial computational resources and time, and performance of these models is not reliable on long-horizon navigation. In this work, we present a generative Transformer based model that jointly models image goals, camera observations and the robot's past actions to predict future actions. We use state-of-the-art perception models and navigation policies to learn robust goal conditioned policies without the need for real-time interaction with the environment. Our model demonstrates capability in capturing and associating visual information across long time horizons, helping in effective navigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14128v1</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikhilanj Pelluri</dc:creator>
    </item>
    <item>
      <title>A Single Motor Nano Aerial Vehicle with Novel Peer-to-Peer Communication and Sensing Mechanism</title>
      <link>https://arxiv.org/abs/2405.14144</link>
      <description>arXiv:2405.14144v1 Announce Type: new 
Abstract: Communication and position sensing are among the most important capabilities for swarm robots to interact with their peers and perform tasks collaboratively. However, the hardware required to facilitate communication and position sensing is often too complicated, expensive, and bulky to be carried on swarm robots. Here we present Maneuverable Piccolissimo 3 (MP3), a minimalist, single motor drone capable of executing inter-robot communication via infrared light and triangulation-based sensing of relative bearing, distance, and elevation using message arrival time. Thanks to its novel design, MP3 can communicate with peers and localize itself using simple components, keeping its size and mass small and making it inherently safe for human interaction. Here we present the hardware and software design of MP3 and demonstrate its capability to localize itself, fly stably and maneuver in the environment using peer-to-peer communication and sensing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14144v1</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingxian Wang, Andrew G. Curtis, Mark Yim, Michael Rubenstein</dc:creator>
    </item>
    <item>
      <title>Skip-SCAR: A Modular Approach to ObjectGoal Navigation with Sparsity and Adaptive Skips</title>
      <link>https://arxiv.org/abs/2405.14154</link>
      <description>arXiv:2405.14154v1 Announce Type: new 
Abstract: In ObjectGoal navigation (ObjectNav), agents must locate specific objects within unseen environments, requiring effective observation, prediction, and navigation capabilities. This study found that traditional methods looking only for prediction accuracy often compromise on computational efficiency. To address this, we introduce "Skip-SCAR," a modular framework that enhances efficiency by leveraging sparsity and adaptive skips. The SparseConv-Augmented ResNet (SCAR) at the core of our approach uses sparse and dense feature processing in parallel, optimizing both the computation and memory footprint. Our adaptive skip technique further reduces computational demands by selectively bypassing unnecessary semantic segmentation steps based on environmental constancy. Tested on the HM3D ObjectNav datasets, Skip-SCAR not only minimizes resource use but also sets new performance benchmarks, demonstrating a robust method for improving efficiency and accuracy in robotic navigation tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14154v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yaotian Liu, Jeff Zhang</dc:creator>
    </item>
    <item>
      <title>Adaptive Teaching in Heterogeneous Agents: Balancing Surprise in Sparse Reward Scenarios</title>
      <link>https://arxiv.org/abs/2405.14199</link>
      <description>arXiv:2405.14199v1 Announce Type: new 
Abstract: Learning from Demonstration (LfD) can be an efficient way to train systems with analogous agents by enabling ``Student'' agents to learn from the demonstrations of the most experienced ``Teacher'' agent, instead of training their policy in parallel. However, when there are discrepancies in agent capabilities, such as divergent actuator power or joint angle constraints, naively replicating demonstrations that are out of bounds for the Student's capability can limit efficient learning. We present a Teacher-Student learning framework specifically tailored to address the challenge of heterogeneity between the Teacher and Student agents. Our framework is based on the concept of ``surprise'', inspired by its application in exploration incentivization in sparse-reward environments. Surprise is repurposed to enable the Teacher to detect and adapt to differences between itself and the Student. By focusing on maximizing its surprise in response to the environment while concurrently minimizing the Student's surprise in response to the demonstrations, the Teacher agent can effectively tailor its demonstrations to the Student's specific capabilities and constraints. We validate our method by demonstrating improvements in the Student's learning in control tasks within sparse-reward environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14199v1</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Emma Clark, Kanghyun Ryu, Negar Mehr</dc:creator>
    </item>
    <item>
      <title>Efficient Navigation of a Robotic Fish Swimming Across the Vortical Flow Field</title>
      <link>https://arxiv.org/abs/2405.14251</link>
      <description>arXiv:2405.14251v1 Announce Type: new 
Abstract: Navigating efficiently across vortical flow fields presents a significant challenge in various robotic applications. The dynamic and unsteady nature of vortical flows often disturbs the control of underwater robots, complicating their operation in hydrodynamic environments. Conventional control methods, which depend on accurate modeling, fail in these settings due to the complexity of fluid-structure interactions (FSI) caused by unsteady hydrodynamics. This study proposes a deep reinforcement learning (DRL) algorithm, trained in a data-driven manner, to enable efficient navigation of a robotic fish swimming across vortical flows. Our proposed algorithm incorporates the LSTM architecture and uses several recent consecutive observations as the state to address the issue of partial observation, often due to sensor limitations. We present a numerical study of navigation within a Karman vortex street, created by placing a stationary cylinder in a uniform flow, utilizing the immersed boundary-lattice Boltzmann method (IB-LBM). The aim is to train the robotic fish to discover efficient navigation policies, enabling it to reach a designated target point across the Karman vortex street from various initial positions. After training, the fish demonstrates the ability to rapidly reach the target from different initial positions, showcasing the effectiveness and robustness of our proposed algorithm. Analysis of the results reveals that the robotic fish can leverage velocity gains and pressure differences induced by the vortices to reach the target, underscoring the potential of our proposed algorithm in enhancing navigation in complex hydrodynamic environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14251v1</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haodong Feng, Dehan Yuan, Jiale Miao, Jie You, Yue Wang, Yi Zhu, Dixia Fan</dc:creator>
    </item>
    <item>
      <title>Optimal Whole Body Trajectory Planning for Mobile Manipulators in Planetary Exploration and Construction</title>
      <link>https://arxiv.org/abs/2405.14363</link>
      <description>arXiv:2405.14363v1 Announce Type: new 
Abstract: Space robotics poses unique challenges arising from the limitation of energy and computational resources, and the complexity of the environment and employed platforms. At the control center, offline motion planning is fundamental in the computation of optimized trajectories accounting for the system's constraints. Smooth movements, collision and forbidden areas avoidance, target visibility and energy consumption are all important factors to consider to be able to generate feasible and optimal plans. When mobile manipulators (terrestrial, aerial) are employed, the base and the arm movements are often separately planned, ultimately resulting in sub-optimal solutions. We propose an Optimal Whole Body Planner (OptiWB) based on Discrete Dynamic Programming (DDP) and optimal interpolation. Kinematic redundancy is exploited for collision and forbidden areas avoidance, and to improve target illumination and visibility from onboard cameras. The planner, implemented in ROS (Robot Operating System), interfaces 3DROCS, a mission planner used in several programs of the European Space Agency (ESA) to support planetary exploration surface missions and part of the ExoMars Rover's planning software. The proposed approach is exercised on a simplified version of the Analog-1 Interact rover by ESA, a 7-DOFs robotic arm mounted on a four wheels non-holonomic platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14363v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federica Storiale, Enrico Ferrentino, Federico Salvioli, Konstantinos Kapellos, Pasquale Chiacchio</dc:creator>
    </item>
    <item>
      <title>Multi-purpose robot for rehabilitation of small diameter water pipes</title>
      <link>https://arxiv.org/abs/2405.14382</link>
      <description>arXiv:2405.14382v1 Announce Type: new 
Abstract: Rehabilitating cast iron pipes through lining offers several advantages, including increased durability, reduced water leaks, and minimal disruption.This approach presents a cost effective and environmentally friendly solution by sealing cracks and joints, extending the pipeline's lifespan, and reducing water wastage, all while avoiding the need for trench excavation. However, due to the relining process, branch connections are sealed and need to be reestablished. To address the issue of rehabilitating small-diameter water pipes, we have designed a modular robot capable of traversing and working within 200 meter long, 100 mm diameter cast iron pipes. This robot is equipped with perception functions to detect, locate, and characterize the branch connections in cast iron pipes and relocate them after lining, as well as machining functions. A first prototype of this system has been developed and validated on an 8 meter long section, in a laboratory environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14382v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julien Feiguel, Mouhamed NDiaye, Pascal Chambaud, Adrien Chambellan, Pierre Blanc, Steve Bourgeois, Lucas Labarussiat, Clemence Dubois, Audrey Vigneron, Thomas Desrez, Alain Riwan, Caroline Vienne</dc:creator>
    </item>
    <item>
      <title>A Unification Between Deep-Learning Vision, Compartmental Dynamical Thermodynamics, and Robotic Manipulation for a Circular Economy</title>
      <link>https://arxiv.org/abs/2405.14406</link>
      <description>arXiv:2405.14406v1 Announce Type: new 
Abstract: The shift from a linear to a circular economy has the potential to simultaneously reduce uncertainties of material supplies and waste generation. To date, the development of robotic and, more generally, autonomous systems have been rarely integrated into circular economy implementation strategies. In this review, we merge deep-learning vision, compartmental dynamical thermodynamics, and robotic manipulation into a theoretically-coherent physics-based research framework to lay the foundations of circular flow designs of materials, and hence, to speed-up the transition from linearity to circularity. Then, we discuss opportunities for robotics in circular economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14406v1</guid>
      <category>cs.RO</category>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Zocco, Wassim M. Haddad, Andrea Corti, Monica Malvezzi</dc:creator>
    </item>
    <item>
      <title>Leveraging Natural Load Dynamics with Variable Gear-ratio Actuators</title>
      <link>https://arxiv.org/abs/2405.14441</link>
      <description>arXiv:2405.14441v1 Announce Type: new 
Abstract: This paper presents a robotic system where the gear-ratio of an actuator is dynamically changed to either leverage or attenuate the natural load dynamics. Based on this principle, lightweight robotic systems can be made fast and strong; exploiting the natural load dynamics for moving at higher speeds (small reduction ratio), while also able to bear a large load through the attenuation of the load dynamics (large reduction ratio). A model-based control algorithm to automatically select the optimal gear-ratios that minimize the total actuator torques for an arbitrary dynamic state and expected uncertainty level is proposed. Also, a novel 3-DoF robot arm using custom actuators with two discrete gear-ratios is presented. The advantages of gear-shifting dynamically are demonstrated through experiments and simulations. Results show that actively changing the gear-ratio using the proposed control algorithms can lead to an order-of-magnitude reduction of necessary actuator torque and power, and also increase robustness to disturbances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14441v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/LRA.2017.2651946</arxiv:DOI>
      <arxiv:journal_reference>IEEE Robotics and Automation Letters ( Volume: 2, Issue: 2, April 2017)</arxiv:journal_reference>
      <dc:creator>Alexandre Girard, H. Harry Asada</dc:creator>
    </item>
    <item>
      <title>Visuo-Tactile Keypoint Correspondences for Object Manipulation</title>
      <link>https://arxiv.org/abs/2405.14515</link>
      <description>arXiv:2405.14515v1 Announce Type: new 
Abstract: This paper presents a novel manipulation strategy that uses keypoint correspondences extracted from visuo-tactile sensor images to facilitate precise object manipulation. Our approach uses the visuo-tactile feedback to guide the robot's actions for accurate object grasping and placement, eliminating the need for post-grasp adjustments and extensive training. This method provides an improvement in deployment efficiency, addressing the challenges of manipulation tasks in environments where object locations are not predefined. We validate the effectiveness of our strategy through experiments demonstrating the extraction of keypoint correspondences and their application to real-world tasks such as block alignment and gear insertion, which require millimeter-level precision. The results show an average error margin significantly lower than that of traditional vision-based methods, which is sufficient to achieve the target tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14515v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeong-Jung Kim, Doo-Yeol Koh, Chang-Hyun Kim</dc:creator>
    </item>
    <item>
      <title>Towards Privacy-Aware and Personalised Assistive Robots: A User-Centred Approach</title>
      <link>https://arxiv.org/abs/2405.14528</link>
      <description>arXiv:2405.14528v1 Announce Type: new 
Abstract: The global increase in the elderly population necessitates innovative long-term care solutions to improve the quality of life for vulnerable individuals while reducing caregiver burdens. Assistive robots, leveraging advancements in Machine Learning, offer promising personalised support. However, their integration into daily life raises significant privacy concerns. Widely used frameworks like the Robot Operating System (ROS) historically lack inherent privacy mechanisms, complicating data-driven approaches in robotics. This research pioneers user-centric, privacy-aware technologies such as Federated Learning (FL) to advance assistive robotics. FL enables collaborative learning without sharing sensitive data, addressing privacy and scalability issues. This work includes developing solutions for smart wheelchair assistance, enhancing user independence and well-being. By tackling challenges related to non-stationary data and heterogeneous environments, the research aims to improve personalisation and user experience. Ultimately, it seeks to lead the responsible integration of assistive robots into society, enhancing the quality of life for elderly and care-dependent individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14528v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando E. Casado</dc:creator>
    </item>
    <item>
      <title>VINS-Multi: A Robust Asynchronous Multi-camera-IMU State Estimator</title>
      <link>https://arxiv.org/abs/2405.14539</link>
      <description>arXiv:2405.14539v1 Announce Type: new 
Abstract: State estimation is a critical foundational module in robotics applications, where robustness and performance are paramount. Although in recent years, many works have been focusing on improving one of the most widely adopted state estimation methods, visual inertial odometry (VIO), by incorporating multiple cameras, these efforts predominantly address synchronous camera systems. Asynchronous cameras, which offer simpler hardware configurations and enhanced resilience, have been largely overlooked. To fill this gap, this paper presents VINS-Multi, a novel multi-camera-IMU state estimator for asynchronous cameras. The estimator comprises parallel front ends, a front end coordinator, and a back end optimization module capable of handling asynchronous input frames. It utilizes the frames effectively through a dynamic feature number allocation and a frame priority coordination strategy. The proposed estimator is integrated into a customized quadrotor platform and tested in multiple realistic and challenging scenarios to validate its practicality. Additionally, comprehensive benchmark results are provided to showcase the robustness and superior performance of the proposed estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14539v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luqi Wang, Yang Xu, Shaojie Shen</dc:creator>
    </item>
    <item>
      <title>Task-Based Design and Policy Co-Optimization for Tendon-driven Underactuated Kinematic Chains</title>
      <link>https://arxiv.org/abs/2405.14566</link>
      <description>arXiv:2405.14566v1 Announce Type: new 
Abstract: Underactuated manipulators reduce the number of bulky motors, thereby enabling compact and mechanically robust designs. However, fewer actuators than joints means that the manipulator can only access a specific manifold within the joint space, which is particular to a given hardware configuration and can be low-dimensional and/or discontinuous. Determining an appropriate set of hardware parameters for this class of mechanisms, therefore, is difficult - even for traditional task-based co-optimization methods. In this paper, our goal is to implement a task-based design and policy co-optimization method for underactuated, tendon-driven manipulators. We first formulate a general model for an underactuated, tendon-driven transmission. We then use this model to co-optimize a three-link, two-actuator kinematic chain using reinforcement learning. We demonstrate that our optimized tendon transmission and control policy can be transferred reliably to physical hardware with real-world reaching experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14566v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sharfin Islam, Zhanpeng He, Matei Ciocarlie</dc:creator>
    </item>
    <item>
      <title>Efficient Robot Learning for Perception and Mapping</title>
      <link>https://arxiv.org/abs/2405.14688</link>
      <description>arXiv:2405.14688v1 Announce Type: new 
Abstract: Holistic scene understanding poses a fundamental contribution to the autonomous operation of a robotic agent in its environment. Key ingredients include a well-defined representation of the surroundings to capture its spatial structure as well as assigning semantic meaning while delineating individual objects. Classic components from the toolbox of roboticists to address these tasks are simultaneous localization and mapping (SLAM) and panoptic segmentation. Although recent methods demonstrate impressive advances, mostly due to employing deep learning, they commonly utilize in-domain training on large datasets. Since following such a paradigm substantially limits their real-world application, my research investigates how to minimize human effort in deploying perception-based robotic systems to previously unseen environments. In particular, I focus on leveraging continual learning and reducing human annotations for efficient learning. An overview of my work can be found at https://vniclas.github.io.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14688v1</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niclas V\"odisch</dc:creator>
    </item>
    <item>
      <title>Evolution and learning in differentiable robots</title>
      <link>https://arxiv.org/abs/2405.14712</link>
      <description>arXiv:2405.14712v1 Announce Type: new 
Abstract: The automatic design of robots has existed for 30 years but has been constricted by serial non-differentiable design evaluations, premature convergence to simple bodies or clumsy behaviors, and a lack of sim2real transfer to physical machines. Thus, here we employ massively-parallel differentiable simulations to rapidly and simultaneously optimize individual neural control of behavior across a large population of candidate body plans and return a fitness score for each design based on the performance of its fully optimized behavior. Non-differentiable changes to the mechanical structure of each robot in the population -- mutations that rearrange, combine, add, or remove body parts -- were applied by a genetic algorithm in an outer loop of search, generating a continuous flow of novel morphologies with highly-coordinated and graceful behaviors honed by gradient descent. This enabled the exploration of several orders-of-magnitude more designs than all previous methods, despite the fact that robots here have the potential to be much more complex, in terms of number of independent motors, than those in prior studies. We found that evolution reliably produces ``increasingly differentiable'' robots: body plans that smooth the loss landscape in which learning operates and thereby provide better training paths toward performant behaviors. Finally, one of the highly differentiable morphologies discovered in simulation was realized as a physical robot and shown to retain its optimized behavior. This provides a cyberphysical platform to investigate the relationship between evolution and learning in biological systems and broadens our understanding of how a robot's physical structure can influence the ability to train policies for it. Videos and code at https://sites.google.com/view/eldir.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14712v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Strgar, David Matthews, Tyler Hummer, Sam Kriegman</dc:creator>
    </item>
    <item>
      <title>CoPeD-Advancing Multi-Robot Collaborative Perception: A Comprehensive Dataset in Real-World Environments</title>
      <link>https://arxiv.org/abs/2405.14731</link>
      <description>arXiv:2405.14731v1 Announce Type: new 
Abstract: In the past decade, although single-robot perception has made significant advancements, the exploration of multi-robot collaborative perception remains largely unexplored. This involves fusing compressed, intermittent, limited, heterogeneous, and asynchronous environmental information across multiple robots to enhance overall perception, despite challenges like sensor noise, occlusions, and sensor failures. One major hurdle has been the lack of real-world datasets. This paper presents a pioneering and comprehensive real-world multi-robot collaborative perception dataset to boost research in this area. Our dataset leverages the untapped potential of air-ground robot collaboration featuring distinct spatial viewpoints, complementary robot mobilities, coverage ranges, and sensor modalities. It features raw sensor inputs, pose estimation, and optional high-level perception annotation, thus accommodating diverse research interests. Compared to existing datasets predominantly designed for Simultaneous Localization and Mapping (SLAM), our setup ensures a diverse range and adequate overlap of sensor views to facilitate the study of multi-robot collaborative perception algorithms. We demonstrate the value of this dataset qualitatively through multiple collaborative perception tasks. We believe this work will unlock the potential research of high-level scene understanding through multi-modal collaborative perception in multi-robot settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14731v1</guid>
      <category>cs.RO</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Zhou, Long Quang, Carlos Nieto-Granda, Giuseppe Loianno</dc:creator>
    </item>
    <item>
      <title>HR-INR: Continuous Space-Time Video Super-Resolution via Event Camera</title>
      <link>https://arxiv.org/abs/2405.13389</link>
      <description>arXiv:2405.13389v1 Announce Type: cross 
Abstract: Continuous space-time video super-resolution (C-STVSR) aims to simultaneously enhance video resolution and frame rate at an arbitrary scale. Recently, implicit neural representation (INR) has been applied to video restoration, representing videos as implicit fields that can be decoded at an arbitrary scale. However, the highly ill-posed nature of C-STVSR limits the effectiveness of current INR-based methods: they assume linear motion between frames and use interpolation or feature warping to generate features at arbitrary spatiotemporal positions with two consecutive frames. This restrains C-STVSR from capturing rapid and nonlinear motion and long-term dependencies (involving more than two frames) in complex dynamic scenes. In this paper, we propose a novel C-STVSR framework, called HR-INR, which captures both holistic dependencies and regional motions based on INR. It is assisted by an event camera, a novel sensor renowned for its high temporal resolution and low latency. To fully utilize the rich temporal information from events, we design a feature extraction consisting of (1) a regional event feature extractor - taking events as inputs via the proposed event temporal pyramid representation to capture the regional nonlinear motion and (2) a holistic event-frame feature extractor for long-term dependence and continuity motion. We then propose a novel INR-based decoder with spatiotemporal embeddings to capture long-term dependencies with a larger temporal perception field. We validate the effectiveness and generalization of our method on four datasets (both simulated and real data), showing the superiority of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13389v1</guid>
      <category>cs.CV</category>
      <category>cs.MM</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yunfan Lu, Zipeng Wang, Yusheng Wang, Hui Xiong</dc:creator>
    </item>
    <item>
      <title>AlabOS: A Python-based Reconfigurable Workflow Management Framework for Autonomous Laboratories</title>
      <link>https://arxiv.org/abs/2405.13930</link>
      <description>arXiv:2405.13930v1 Announce Type: cross 
Abstract: The recent advent of autonomous laboratories, coupled with algorithms for high-throughput screening and active learning, promises to accelerate materials discovery and innovation. As these autonomous systems grow in complexity, the demand for robust and efficient workflow management software becomes increasingly critical. In this paper, we introduce AlabOS, a general-purpose software framework for orchestrating experiments and managing resources, with an emphasis on automated laboratories for materials synthesis and characterization. We demonstrate the implementation of AlabOS in a prototype autonomous materials laboratory. AlabOS features a reconfigurable experiment workflow model, enabling the simultaneous execution of varied workflows composed of modular tasks. Therefore, AlabOS is well-suited to handle the rapidly changing experimental protocols defining the progress of self-driving laboratory development for materials research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13930v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.RO</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuxing Fei, Bernardus Rendy, Rishi Kumar, Olympia Dartsi, Hrushikesh P. Sahasrabuddhe, Matthew J. McDermott, Zheren Wang, Nathan J. Szymanski, Lauren N. Walters, David Milsted, Yan Zeng, Anubhav Jain, Gerbrand Ceder</dc:creator>
    </item>
    <item>
      <title>RadarOcc: Robust 3D Occupancy Prediction with 4D Imaging Radar</title>
      <link>https://arxiv.org/abs/2405.14014</link>
      <description>arXiv:2405.14014v1 Announce Type: cross 
Abstract: 3D occupancy-based perception pipeline has significantly advanced autonomous driving by capturing detailed scene descriptions and demonstrating strong generalizability across various object categories and shapes. Current methods predominantly rely on LiDAR or camera inputs for 3D occupancy prediction. These methods are susceptible to adverse weather conditions, limiting the all-weather deployment of self-driving cars. To improve perception robustness, we leverage the recent advances in automotive radars and introduce a novel approach that utilizes 4D imaging radar sensors for 3D occupancy prediction. Our method, RadarOcc, circumvents the limitations of sparse radar point clouds by directly processing the 4D radar tensor, thus preserving essential scene details. RadarOcc innovatively addresses the challenges associated with the voluminous and noisy 4D radar data by employing Doppler bins descriptors, sidelobe-aware spatial sparsification, and range-wise self-attention mechanisms. To minimize the interpolation errors associated with direct coordinate transformations, we also devise a spherical-based feature encoding followed by spherical-to-Cartesian feature aggregation. We benchmark various baseline methods based on distinct modalities on the public K-Radar dataset. The results demonstrate RadarOcc's state-of-the-art performance in radar-based 3D occupancy prediction and promising results even when compared with LiDAR- or camera-based methods. Additionally, we present qualitative evidence of the superior performance of 4D radar in adverse weather conditions and explore the impact of key pipeline components through ablation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14014v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fangqiang Ding, Xiangyu Wen, Yunzhou Zhu, Yiming Li, Chris Xiaoxuan Lu</dc:creator>
    </item>
    <item>
      <title>A New Method in Facial Registration in Clinics Based on Structure Light Images</title>
      <link>https://arxiv.org/abs/2405.14292</link>
      <description>arXiv:2405.14292v1 Announce Type: cross 
Abstract: Background and Objective: In neurosurgery, fusing clinical images and depth images that can improve the information and details is beneficial to surgery. We found that the registration of face depth images was invalid frequently using existing methods. To abundant traditional image methods with depth information, a method in registering with depth images and traditional clinical images was investigated. Methods: We used the dlib library, a C++ library that could be used in face recognition, and recognized the key points on faces from the structure light camera and CT image. The two key point clouds were registered for coarse registration by the ICP method. Fine registration was finished after coarse registration by the ICP method. Results: RMSE after coarse and fine registration is as low as 0.995913 mm. Compared with traditional methods, it also takes less time. Conclusions: The new method successfully registered the facial depth image from structure light images and CT with a low error, and that would be promising and efficient in clinical application of neurosurgery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14292v1</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengfei Li, Ziyue Ma, Hong Wang, Juan Deng, Yan Wang, Zhenyu Xu, Feng Yan, Wenjun Tu, Hong Sha</dc:creator>
    </item>
    <item>
      <title>Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration</title>
      <link>https://arxiv.org/abs/2405.14314</link>
      <description>arXiv:2405.14314v1 Announce Type: cross 
Abstract: Grounding the reasoning ability of large language models (LLMs) for embodied tasks is challenging due to the complexity of the physical world. Especially, LLM planning for multi-agent collaboration requires communication of agents or credit assignment as the feedback to re-adjust the proposed plans and achieve effective coordination. However, existing methods that overly rely on physical verification or self-reflection suffer from excessive and inefficient querying of LLMs. In this paper, we propose a novel framework for multi-agent collaboration that introduces Reinforced Advantage feedback (ReAd) for efficient self-refinement of plans. Specifically, we perform critic regression to learn a sequential advantage function from LLM-planned data, and then treat the LLM planner as an optimizer to generate actions that maximize the advantage function. It endows the LLM with the foresight to discern whether the action contributes to accomplishing the final task. We provide theoretical analysis by extending advantage-weighted regression in reinforcement learning to multi-agent systems. Experiments on Overcooked-AI and a difficult variant of RoCoBench show that ReAd surpasses baselines in success rate, and also significantly decreases the interaction steps of agents and query rounds of LLMs, demonstrating its high efficiency for grounding LLMs. More results are given at \url{https://read-llm.github.io/}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14314v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Xuelong Li, Zhen Wang</dc:creator>
    </item>
    <item>
      <title>Camera Relocalization in Shadow-free Neural Radiance Fields</title>
      <link>https://arxiv.org/abs/2405.14824</link>
      <description>arXiv:2405.14824v1 Announce Type: cross 
Abstract: Camera relocalization is a crucial problem in computer vision and robotics. Recent advancements in neural radiance fields (NeRFs) have shown promise in synthesizing photo-realistic images. Several works have utilized NeRFs for refining camera poses, but they do not account for lighting changes that can affect scene appearance and shadow regions, causing a degraded pose optimization process. In this paper, we propose a two-staged pipeline that normalizes images with varying lighting and shadow conditions to improve camera relocalization. We implement our scene representation upon a hash-encoded NeRF which significantly boosts up the pose optimization process. To account for the noisy image gradient computing problem in grid-based NeRFs, we further propose a re-devised truncated dynamic low-pass filter (TDLF) and a numerical gradient averaging technique to smoothen the process. Experimental results on several datasets with varying lighting conditions demonstrate that our method achieves state-of-the-art results in camera relocalization under varying lighting conditions. Code and data will be made publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14824v1</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiyao Xu, Caiyun Liu, Yuantao Chen, Zhenxin Zhu, Zike Yan, Yongliang Shi, Hao Zhao, Guyue Zhou</dc:creator>
    </item>
    <item>
      <title>Privileged Sensing Scaffolds Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2405.14853</link>
      <description>arXiv:2405.14853v1 Announce Type: cross 
Abstract: We need to look at our shoelaces as we first learn to tie them but having mastered this skill, can do it from touch alone. We call this phenomenon "sensory scaffolding": observation streams that are not needed by a master might yet aid a novice learner. We consider such sensory scaffolding setups for training artificial agents. For example, a robot arm may need to be deployed with just a low-cost, robust, general-purpose camera; yet its performance may improve by having privileged training-time-only access to informative albeit expensive and unwieldy motion capture rigs or fragile tactile sensors. For these settings, we propose "Scaffolder", a reinforcement learning approach which effectively exploits privileged sensing in critics, world models, reward estimators, and other such auxiliary components that are only used at training time, to improve the target policy. For evaluating sensory scaffolding agents, we design a new "S3" suite of ten diverse simulated robotic tasks that explore a wide range of practical sensor setups. Agents must use privileged camera sensing to train blind hurdlers, privileged active visual perception to help robot arms overcome visual occlusions, privileged touch sensors to train robot hands, and more. Scaffolder easily outperforms relevant prior baselines and frequently performs comparably even to policies that have test-time access to the privileged sensors. Website: https://penn-pal-lab.github.io/scaffolder/</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14853v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edward S. Hu, James Springer, Oleh Rybkin, Dinesh Jayaraman</dc:creator>
    </item>
    <item>
      <title>Generative Camera Dolly: Extreme Monocular Dynamic Novel View Synthesis</title>
      <link>https://arxiv.org/abs/2405.14868</link>
      <description>arXiv:2405.14868v1 Announce Type: cross 
Abstract: Accurate reconstruction of complex dynamic scenes from just a single viewpoint continues to be a challenging task in computer vision. Current dynamic novel view synthesis methods typically require videos from many different camera viewpoints, necessitating careful recording setups, and significantly restricting their utility in the wild as well as in terms of embodied AI applications. In this paper, we propose $\textbf{GCD}$, a controllable monocular dynamic view synthesis pipeline that leverages large-scale diffusion priors to, given a video of any scene, generate a synchronous video from any other chosen perspective, conditioned on a set of relative camera pose parameters. Our model does not require depth as input, and does not explicitly model 3D scene geometry, instead performing end-to-end video-to-video translation in order to achieve its goal efficiently. Despite being trained on synthetic multi-view video data only, zero-shot real-world generalization experiments show promising results in multiple domains, including robotics, object permanence, and driving environments. We believe our framework can potentially unlock powerful applications in rich dynamic scene understanding, perception for robotics, and interactive 3D video viewing experiences for virtual reality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14868v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Basile Van Hoorick, Rundi Wu, Ege Ozguroglu, Kyle Sargent, Ruoshi Liu, Pavel Tokmakov, Achal Dave, Changxi Zheng, Carl Vondrick</dc:creator>
    </item>
    <item>
      <title>An Empirical Study of Training State-of-the-Art LiDAR Segmentation Models</title>
      <link>https://arxiv.org/abs/2405.14870</link>
      <description>arXiv:2405.14870v1 Announce Type: cross 
Abstract: In the rapidly evolving field of autonomous driving, precise segmentation of LiDAR data is crucial for understanding complex 3D environments. Traditional approaches often rely on disparate, standalone codebases, hindering unified advancements and fair benchmarking across models. To address these challenges, we introduce MMDetection3D-lidarseg, a comprehensive toolbox designed for the efficient training and evaluation of state-of-the-art LiDAR segmentation models. We support a wide range of segmentation models and integrate advanced data augmentation techniques to enhance robustness and generalization. Additionally, the toolbox provides support for multiple leading sparse convolution backends, optimizing computational efficiency and performance. By fostering a unified framework, MMDetection3D-lidarseg streamlines development and benchmarking, setting new standards for research and application. Our extensive benchmark experiments on widely-used datasets demonstrate the effectiveness of the toolbox. The codebase and trained models have been publicly available, promoting further research and innovation in the field of LiDAR segmentation for autonomous driving.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14870v1</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiahao Sun, Xiang Xu, Lingdong Kong, Youquan Liu, Li Li, Chenming Zhu, Jingwei Zhang, Zeqi Xiao, Runnan Chen, Tai Wang, Wenwei Zhang, Kai Chen, Chunmei Qing</dc:creator>
    </item>
    <item>
      <title>Experimental investigation of a maneuver selection algorithm for vehicles in low adhesion conditions</title>
      <link>https://arxiv.org/abs/2205.15178</link>
      <description>arXiv:2205.15178v2 Announce Type: replace 
Abstract: Winter conditions, characterized by the presence of ice and snow on the ground, are more likely to lead to road accidents. This paper presents an experimental proof of concept, with a 1/5th scale car platform, of a maneuver selection scheme for low adhesion conditions. In the proposed approach, a model-based estimator first processes the high-dimensional sensors data of the IMU, LIDAR and encoders to estimate physically relevant vehicle and ground conditions parameters such as the inertial velocity of the vehicle $v$, the friction coefficient $\mu$, the cohesion $c$ and the internal shear angle $\phi$. Then, a data-driven predictor is trained to predict the optimal maneuver to perform in the situation characterized by the estimated parameters. Experimental results show that it is possible to 1) produce a real-time estimate of the relevant ground parameters, and 2) determine an optimal maneuver based on the estimated parameters between a limited set of maneuvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.15178v2</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TIV.2022.3188942</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Intelligent Vehicles ( Volume: 7, Issue: 3, September 2022)</arxiv:journal_reference>
      <dc:creator>Olivier Lecompte, William Therrien, Alexandre Girard</dc:creator>
    </item>
    <item>
      <title>Controller design and experimental evaluation of a motorised assistance for a patient transfer floor lift</title>
      <link>https://arxiv.org/abs/2205.15201</link>
      <description>arXiv:2205.15201v2 Announce Type: replace 
Abstract: Patient transfer is a challenging, critical task because it exposes caregivers to injury risks. Available transfer devices, like floor lifts, lead to improvements but are far from perfect. They do not eliminate the caregivers risk of musculoskeletal disorders, and they can be burdensome to use due to their poor maneuverability. This paper presents a new motorized floor lift with a single central motorized wheel connected to an instrumented handle. Admittance controllers are designed to 1) improve the device maneuverability, 2) reduce the required caregiver effort, and 3) ensure the security and comfort of patients. Two controller designs, one with a linear admittance law and a non-linear admittance law with variable damping, were developed and implemented on a prototype. Tests were performed on seven participants to evaluate the performance of the assistance system and the controllers. The experimental results show that 1) the motorized assistance with the variable damping controller improves maneuverability by 28%, 2) reduces the amount of effort required to push the lift by 66% and 3) provides the same level of patient comfort compared to a standard unassisted floor lift.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.15201v2</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Donatien Callon, Ian Lalonde, Mathieu Nadeau, Alexandre Girard</dc:creator>
    </item>
    <item>
      <title>Self-Supervised Depth Correction of Lidar Measurements from Map Consistency Loss</title>
      <link>https://arxiv.org/abs/2303.01123</link>
      <description>arXiv:2303.01123v4 Announce Type: replace 
Abstract: Depth perception is considered an invaluable source of information in the context of 3D mapping and various robotics applications. However, point cloud maps acquired using consumer-level light detection and ranging sensors (lidars) still suffer from bias related to local surface properties such as measuring beam-to-surface incidence angle, distance, texture, reflectance, or illumination conditions. This fact has recently motivated researchers to exploit traditional filters, as well as the deep learning paradigm, in order to suppress the aforementioned depth sensors error while preserving geometric and map consistency details. Despite the effort, depth correction of lidar measurements is still an open challenge mainly due to the lack of clean 3D data that could be used as ground truth. In this paper, we introduce two novel point cloud map consistency losses, which facilitate self-supervised learning on real data of lidar depth correction models. Specifically, the models exploit multiple point cloud measurements of the same scene from different view-points in order to learn to reduce the bias based on the constructed map consistency signal. Complementary to the removal of the bias from the measurements, we demonstrate that the depth correction models help to reduce localization drift. Additionally, we release a data set that contains point cloud data captured in an indoor corridor environment with precise localization and ground truth mapping information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.01123v4</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/LRA.2023.3287791</arxiv:DOI>
      <arxiv:journal_reference>20 June 2023</arxiv:journal_reference>
      <dc:creator>Ruslan Agishev, Tom\'a\v{s} P\v{e}t\v{r}\'i\v{c}ek, Karel Zimmermann</dc:creator>
    </item>
    <item>
      <title>Soft insoles for estimating 3D ground reaction forces using 3D printed foam-like sensors</title>
      <link>https://arxiv.org/abs/2303.04719</link>
      <description>arXiv:2303.04719v2 Announce Type: replace 
Abstract: Sensorized insoles provide a tool for gait studies and health monitoring during daily life. For users to accept such insoles they need to be comfortable and lightweight. Previous work has already demonstrated that estimation of ground reaction forces (GRFs) is possible with insoles. However, these are often assemblies of commercial components restricting design freedom and customization. Within this work, we investigate using four 3D-printed soft foam-like sensors to sensorize an insole. These sensors were combined with system identification of Hammerstein-Wiener models to estimate the 3D GRFs, which were compared to values from an instrumented treadmill as the golden standard. It was observed that the four sensors behaved in line with the expected change in pressure distribution during the gait cycle. In addition, the identified (personalized) Hammerstein-Wiener models showed the best estimation performance (on average RMS error 9.3%, R^2=0.85 and mean absolute error (MAE) 7%) of the vertical, mediolateral, and anteroposterior GRFs. Thereby showing that these sensors can estimate the resulting 3D force reasonably well. These results for nine participants were comparable to or outperformed other works that used commercial FSRs with machine learning. The identified models did decrease in estimation performance over time but stayed on average 11.35% RMS and 8.6% MAE after a week with the Hammerstein-Wiener model seeming consistent between days two and seven. These results show promise for using 3D-printed soft piezoresistive foam-like sensors with system identification to be a viable approach for applications that require softness, lightweight, and customization such as wearable (force) sensors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.04719v2</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nick Willemstein, Saivimal Sridar, Herman van der Kooij, Ali Sadeghi</dc:creator>
    </item>
    <item>
      <title>Spatiotemporal Receding Horizon Control with Proactive Interaction Towards Autonomous Driving in Dense Traffic</title>
      <link>https://arxiv.org/abs/2308.05929</link>
      <description>arXiv:2308.05929v3 Announce Type: replace 
Abstract: In dense traffic scenarios, ensuring safety while keeping high task performance for autonomous driving is a critical challenge. To address this problem, this paper proposes a computationally-efficient spatiotemporal receding horizon control (ST-RHC) scheme to generate a safe, dynamically feasible, energy-efficient trajectory in control space, where different driving tasks in dense traffic can be achieved with high accuracy and safety in real time. In particular, an embodied spatiotemporal safety barrier module considering proactive interactions is devised to mitigate the effects of inaccuracies resulting from the trajectory prediction of other vehicles. Subsequently, the motion planning and control problem is formulated as a constrained nonlinear optimization problem, which favorably facilitates the effective use of off-the-shelf optimization solvers in conjunction with multiple shooting. The effectiveness of the proposed ST-RHC scheme is demonstrated through comprehensive comparisons with state-of-the-art algorithms on synthetic and real-world traffic datasets under dense traffic, and the attendant outcome of superior performance in terms of accuracy, efficiency and safety is achieved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05929v3</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/tiv.2024.3389827</arxiv:DOI>
      <dc:creator>Lei Zheng, Rui Yang, Zengqi Peng, Michael Yu Wang, Jun Ma</dc:creator>
    </item>
    <item>
      <title>ViPlanner: Visual Semantic Imperative Learning for Local Navigation</title>
      <link>https://arxiv.org/abs/2310.00982</link>
      <description>arXiv:2310.00982v3 Announce Type: replace 
Abstract: Real-time path planning in outdoor environments still challenges modern robotic systems due to differences in terrain traversability, diverse obstacles, and the necessity for fast decision-making. Established approaches have primarily focused on geometric navigation solutions, which work well for structured geometric obstacles but have limitations regarding the semantic interpretation of different terrain types and their affordances. Moreover, these methods fail to identify traversable geometric occurrences, such as stairs. To overcome these issues, we introduce ViPlanner, a learned local path planning approach that generates local plans based on geometric and semantic information. The system is trained using the Imperative Learning paradigm, for which the network weights are optimized end-to-end based on the planning task objective. This optimization uses a differentiable formulation of a semantic costmap, which enables the planner to distinguish between the traversability of different terrains and accurately identify obstacles. The semantic information is represented in 30 classes using an RGB colorspace that can effectively encode the multiple levels of traversability. We show that the planner can adapt to diverse real-world environments without requiring any real-world training. In fact, the planner is trained purely in simulation, enabling a highly scalable training data generation. Experimental results demonstrate resistance to noise, zero-shot sim-to-real transfer, and a decrease of 38.02% in terms of traversability cost compared to purely geometric-based approaches. Code and models are made publicly available: https://github.com/leggedrobotics/viplanner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.00982v3</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pascal Roth, Julian Nubert, Fan Yang, Mayank Mittal, Marco Hutter</dc:creator>
    </item>
    <item>
      <title>Deep Bayesian Reinforcement Learning for Spacecraft Proximity Maneuvers and Docking</title>
      <link>https://arxiv.org/abs/2311.03680</link>
      <description>arXiv:2311.03680v2 Announce Type: replace 
Abstract: In the pursuit of autonomous spacecraft proximity maneuvers and docking(PMD), we introduce a novel Bayesian actor-critic reinforcement learning algorithm to learn a control policy with the stability guarantee. The PMD task is formulated as a Markov decision process that reflects the relative dynamic model, the docking cone and the cost function. Drawing from the principles of Lyapunov theory, we frame the temporal difference learning as a constrained Gaussian process regression problem. This innovative approach allows the state-value function to be expressed as a Lyapunov function, leveraging the Gaussian process and deep kernel learning. We develop a novel Bayesian quadrature policy optimization procedure to analytically compute the policy gradient while integrating Lyapunov-based stability constraints. This integration is pivotal in satisfying the rigorous safety demands of spaceflight missions. The proposed algorithm has been experimentally evaluated on a spacecraft air-bearing testbed and shows impressive and promising performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.03680v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Desong Du, Naiming Qi, Yanfang Liu, Wei Pan</dc:creator>
    </item>
    <item>
      <title>Multi-Contact Whole-Body Force Control for Position-Controlled Robots</title>
      <link>https://arxiv.org/abs/2312.16465</link>
      <description>arXiv:2312.16465v4 Announce Type: replace 
Abstract: Many humanoid and multi-legged robots are controlled in positions rather than in torques, which prevents direct control of contact forces, and hampers their ability to create multiple contacts to enhance their balance, such as placing a hand on a wall or a handrail. This letter introduces the SEIKO (Sequential Equilibrium Inverse Kinematic Optimization) pipeline, and proposes a unified formulation that exploits an explicit model of flexibility to indirectly control contact forces on traditional position-controlled robots. SEIKO formulates whole-body retargeting from Cartesian commands and admittance control using two quadratic programs solved in real-time. Our pipeline is validated with experiments on the real, full-scale humanoid robot Talos in various multi-contact scenarios, including pushing tasks, far-reaching tasks, stair climbing, and stepping on sloped surfaces. Code and videos are available at: https://hucebot.github.io/seiko_controller_website/</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16465v4</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/LRA.2024.3396094</arxiv:DOI>
      <arxiv:journal_reference>IEEE Robotics and Automation Letters, 2024, 9 (6), pp.5639-5646</arxiv:journal_reference>
      <dc:creator>Quentin Rouxel (LARSEN), Serena Ivaldi (LARSEN), Jean-Baptiste Mouret (LARSEN)</dc:creator>
    </item>
    <item>
      <title>Nigel -- Mechatronic Design and Robust Sim2Real Control of an Over-Actuated Autonomous Vehicle</title>
      <link>https://arxiv.org/abs/2401.11542</link>
      <description>arXiv:2401.11542v4 Announce Type: replace 
Abstract: Simulation to reality (sim2real) transfer from a dynamics and controls perspective usually involves re-tuning or adapting the designed algorithms to suit real-world operating conditions, which often violates the performance guarantees established originally. This work presents a generalizable framework for achieving reliable sim2real transfer of autonomy-oriented control systems using multi-model multi-objective robust optimal control synthesis, which lends well to uncertainty handling and disturbance rejection with theoretical guarantees. Particularly, this work is centered around a novel actuation-redundant scaled autonomous vehicle called Nigel, with independent all-wheel drive and independent all-wheel steering architecture, whose enhanced configuration space bodes well for robust control applications. To this end, we present the mechatronic design, dynamics modeling, parameter identification, and robust stabilizing as well as tracking control of Nigel using the proposed framework, with exhaustive experimentation and benchmarking in simulation as well as real-world settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11542v4</guid>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chinmay Vilas Samak, Tanmay Vilas Samak, Javad Mohammadpour Velni, Venkat Narayan Krovi</dc:creator>
    </item>
    <item>
      <title>Proto-MPC: An Encoder-Prototype-Decoder Approach for Quadrotor Control in Challenging Winds</title>
      <link>https://arxiv.org/abs/2401.15508</link>
      <description>arXiv:2401.15508v2 Announce Type: replace 
Abstract: Quadrotors are increasingly used in the evolving field of aerial robotics for their agility and mechanical simplicity. However, inherent uncertainties, such as aerodynamic effects coupled with quadrotors' operation in dynamically changing environments, pose significant challenges for traditional, nominal model-based control designs. We propose a multi-task meta-learning method called Encoder-Prototype-Decoder (EPD), which has the advantage of effectively balancing shared and distinctive representations across diverse training tasks. Subsequently, we integrate the EPD model into a model predictive control problem (Proto-MPC) to enhance the quadrotor's ability to adapt and operate across a spectrum of dynamically changing tasks with an efficient online implementation. We validate the proposed method in simulations, which demonstrates Proto-MPC's robust performance in trajectory tracking of a quadrotor being subject to static and spatially varying side winds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15508v2</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuliang Gu, Sheng Cheng, Naira Hovakimyan</dc:creator>
    </item>
    <item>
      <title>Dynamic Occupancy Grids for Object Detection: A Radar-Centric Approach</title>
      <link>https://arxiv.org/abs/2402.01488</link>
      <description>arXiv:2402.01488v2 Announce Type: replace 
Abstract: Dynamic Occupancy Grid Mapping is a technique used to generate a local map of the environment containing both static and dynamic information. Typically, these maps are primarily generated using lidar measurements. However, with improvements in radar sensing, resulting in better accuracy and higher resolution, radar is emerging as a viable alternative to lidar as the primary sensor for mapping. In this paper, we propose a radar-centric dynamic occupancy grid mapping algorithm with adaptations to the state computation, inverse sensor model, and field-of-view computation tailored to the specifics of radar measurements. We extensively evaluate our approach using real data to demonstrate its effectiveness and establish the first benchmark for radar-based dynamic occupancy grid mapping using the publicly available Radarscenes dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01488v2</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Peter Ronecker, Markus Schratter, Lukas Kuschnig, Daniel Watzenig</dc:creator>
    </item>
    <item>
      <title>Combining Constrained Diffusion Models and Numerical Solvers for Efficient and Robust Non-Convex Trajectory Optimization</title>
      <link>https://arxiv.org/abs/2403.05571</link>
      <description>arXiv:2403.05571v2 Announce Type: replace 
Abstract: Motivated by the need to solve open-loop optimal control problems with computational efficiency and reliable constraint satisfaction, we introduce a general framework that combines diffusion models and numerical optimization solvers. Optimal control problems are rarely solvable in closed form, hence they are often transcribed into numerical trajectory optimization problems, which then require initial guesses. These initial guesses are supplied in our framework by diffusion models. To mitigate the effect of samples that violate the problem constraints, we develop a novel constrained diffusion model to approximate the true distribution of locally optimal solutions with an additional constraint violation loss in training. To further enhance the robustness, the diffusion samples as initial guesses are fed to the numerical solver to refine and derive final optimal (and hence feasible) solutions. Experimental evaluations on three tasks verify the improved constraint satisfaction and computational efficiency with 4$\times$ to 30$\times$ acceleration using our proposed framework, which generalizes across trajectory optimization problems and scales well with problem complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05571v2</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anjian Li, Zihan Ding, Adji Bousso Dieng, Ryne Beeson</dc:creator>
    </item>
    <item>
      <title>Multi-AGV Path Planning Method via Reinforcement Learning and Particle Filters</title>
      <link>https://arxiv.org/abs/2403.18236</link>
      <description>arXiv:2403.18236v4 Announce Type: replace 
Abstract: Thanks to its robust learning and search stabilities,the reinforcement learning (RL) algorithm has garnered increasingly significant attention and been exten-sively applied in Automated Guided Vehicle (AGV) path planning. However, RL-based planning algorithms have been discovered to suffer from the substantial variance of neural networks caused by environmental instability and significant fluctua-tions in system structure. These challenges manifest in slow convergence speed and low learning efficiency. To tackle this issue, this paper presents a novel multi-AGV path planning method named Particle Filters - Double Deep Q-Network (PF-DDQN)via leveraging Particle Filters (PF) and RL algorithm. Firstly, the proposed method leverages the imprecise weight values of the network as state values to formulate thestate space equation.Subsequently, the DDQN model is optimized to acquire the optimal true weight values through the iterative fusion process of neural networksand PF in order to enhance the optimization efficiency of the proposedmethod. Lastly, the performance of the proposed method is validated by different numerical simulations. The simulation results demonstrate that the proposed methoddominates the traditional DDQN algorithm in terms of path planning superiority andtraining time indicator by 92.62% and 76.88%, respectively. Therefore, the proposedmethod could be considered as a vital alternative in the field of multi-AGV path planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18236v4</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shao Shuo</dc:creator>
    </item>
    <item>
      <title>Learning Visuotactile Skills with Two Multifingered Hands</title>
      <link>https://arxiv.org/abs/2404.16823</link>
      <description>arXiv:2404.16823v2 Announce Type: replace 
Abstract: Aiming to replicate human-like dexterity, perceptual experiences, and motion patterns, we explore learning from human demonstrations using a bimanual system with multifingered hands and visuotactile data. Two significant challenges exist: the lack of an affordable and accessible teleoperation system suitable for a dual-arm setup with multifingered hands, and the scarcity of multifingered hand hardware equipped with touch sensing. To tackle the first challenge, we develop HATO, a low-cost hands-arms teleoperation system that leverages off-the-shelf electronics, complemented with a software suite that enables efficient data collection; the comprehensive software suite also supports multimodal data processing, scalable policy learning, and smooth policy deployment. To tackle the latter challenge, we introduce a novel hardware adaptation by repurposing two prosthetic hands equipped with touch sensors for research. Using visuotactile data collected from our system, we learn skills to complete long-horizon, high-precision tasks which are difficult to achieve without multifingered dexterity and touch feedback. Furthermore, we empirically investigate the effects of dataset size, sensing modality, and visual input preprocessing on policy learning. Our results mark a promising step forward in bimanual multifingered manipulation from visuotactile data. Videos, code, and datasets can be found at https://toruowo.github.io/hato/ .</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16823v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toru Lin, Yu Zhang, Qiyang Li, Haozhi Qi, Brent Yi, Sergey Levine, Jitendra Malik</dc:creator>
    </item>
    <item>
      <title>PlanNetX: Learning an Efficient Neural Network Planner from MPC for Longitudinal Control</title>
      <link>https://arxiv.org/abs/2404.18863</link>
      <description>arXiv:2404.18863v2 Announce Type: replace 
Abstract: Model predictive control (MPC) is a powerful, optimization-based approach for controlling dynamical systems. However, the computational complexity of online optimization can be problematic on embedded devices. Especially, when we need to guarantee fixed control frequencies. Thus, previous work proposed to reduce the computational burden using imitation learning (IL) approximating the MPC policy by a neural network. In this work, we instead learn the whole planned trajectory of the MPC. We introduce a combination of a novel neural network architecture PlanNetX and a simple loss function based on the state trajectory that leverages the parameterized optimal control structure of the MPC. We validate our approach in the context of autonomous driving by learning a longitudinal planner and benchmarking it extensively in the CommonRoad simulator using synthetic scenarios and scenarios derived from real data. Our experimental results show that we can learn the open-loop MPC trajectory with high accuracy while improving the closed-loop performance of the learned control policy over other baselines like behavior cloning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18863v2</guid>
      <category>cs.RO</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jasper Hoffmann, Diego Fernandez, Julien Brosseit, Julian Bernhard, Klemens Esterle, Moritz Werling, Michael Karg, Joschka Boedecker</dc:creator>
    </item>
    <item>
      <title>NGM-SLAM: Gaussian Splatting SLAM with Radiance Field Submap</title>
      <link>https://arxiv.org/abs/2405.05702</link>
      <description>arXiv:2405.05702v3 Announce Type: replace 
Abstract: SLAM systems based on Gaussian Splatting have garnered attention due to their capabilities for rapid real-time rendering and high-fidelity mapping. However, current Gaussian Splatting SLAM systems usually struggle with large scene representation and lack effective loop closure detection. To address these issues, we introduce NGM-SLAM, the first 3DGS based SLAM system that utilizes neural radiance field submaps for progressive scene expression, effectively integrating the strengths of neural radiance fields and 3D Gaussian Splatting. We utilize neural radiance field submaps as supervision and achieve high-quality scene expression and online loop closure adjustments through Gaussian rendering of fused submaps. Our results on multiple real-world scenes and large-scale scene datasets demonstrate that our method can achieve accurate hole filling and high-quality scene expression, supporting monocular, stereo, and RGB-D inputs, and achieving state-of-the-art scene reconstruction and tracking performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05702v3</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingrui Li, Jingwei Huang, Lei Sun, Aaron Xuxiang Tian, Tianchen Deng, Hongyu Wang</dc:creator>
    </item>
    <item>
      <title>Fast Collision Probability Estimation for Automated Driving using Multi-circular Shape Approximations</title>
      <link>https://arxiv.org/abs/2405.10765</link>
      <description>arXiv:2405.10765v2 Announce Type: replace 
Abstract: Many state-of-the-art methods for safety assessment and motion planning for automated driving require estimation of the probability of collision (POC). To estimate the POC, a shape approximation of the colliding actors and probability density functions of the associated uncertain kinematic variables are required. Even with such information available, the derivation of the POC is in general, i.e., for any shape and density, only possible with Monte Carlo sampling (MCS). Random sampling of the POC, however, is challenging as computational resources are limited in real-world applications. We present expressions for the POC in the presence of Gaussian uncertainties, based on multi-circular shape approximations. In addition, we show that the proposed approach is computationally more efficient than MCS. Lastly, we provide a method for upper and lower bounding the estimation error for the POC induced by the used shape approximations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10765v2</guid>
      <category>cs.RO</category>
      <category>math.PR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leon Tolksdorf, Christian Birkner, Arturo Tejada, Nathan van de Wouw</dc:creator>
    </item>
    <item>
      <title>Visuo-Tactile based Predictive Cross Modal Perception for Object Exploration in Robotics</title>
      <link>https://arxiv.org/abs/2405.12634</link>
      <description>arXiv:2405.12634v2 Announce Type: replace 
Abstract: Autonomously exploring the unknown physical properties of novel objects such as stiffness, mass, center of mass, friction coefficient, and shape is crucial for autonomous robotic systems operating continuously in unstructured environments. We introduce a novel visuo-tactile based predictive cross-modal perception framework where initial visual observations (shape) aid in obtaining an initial prior over the object properties (mass). The initial prior improves the efficiency of the object property estimation, which is autonomously inferred via interactive non-prehensile pushing and using a dual filtering approach. The inferred properties are then used to enhance the predictive capability of the cross-modal function efficiently by using a human-inspired `surprise' formulation. We evaluated our proposed framework in the real-robotic scenario, demonstrating superior performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12634v2</guid>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anirvan Dutta, Etienne Burdet, Mohsen Kaboli</dc:creator>
    </item>
    <item>
      <title>Deep Reinforcement Learning for Time-Critical Wilderness Search And Rescue Using Drones</title>
      <link>https://arxiv.org/abs/2405.12800</link>
      <description>arXiv:2405.12800v2 Announce Type: replace 
Abstract: Traditional search and rescue methods in wilderness areas can be time-consuming and have limited coverage. Drones offer a faster and more flexible solution, but optimizing their search paths is crucial. This paper explores the use of deep reinforcement learning to create efficient search missions for drones in wilderness environments. Our approach leverages a priori data about the search area and the missing person in the form of a probability distribution map. This allows the deep reinforcement learning agent to learn optimal flight paths that maximize the probability of finding the missing person quickly. Experimental results show that our method achieves a significant improvement in search times compared to traditional coverage planning and search planning algorithms. In one comparison, deep reinforcement learning is found to outperform other algorithms by over $160\%$, a difference that can mean life or death in real-world search operations. Additionally, unlike previous work, our approach incorporates a continuous action space enabled by cubature, allowing for more nuanced flight patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12800v2</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan-Hendrik Ewers, David Anderson, Douglas Thomson</dc:creator>
    </item>
    <item>
      <title>Planar Friction Modelling with LuGre Dynamics and Limit Surfaces</title>
      <link>https://arxiv.org/abs/2308.01123</link>
      <description>arXiv:2308.01123v3 Announce Type: replace-cross 
Abstract: During planar motion, contact surfaces exhibit a coupling between tangential and rotational friction forces. This paper proposes planar friction models grounded in the LuGre model and limit surface theory. First, distributed planar extended state models are proposed and the Elasto-Plastic model is extended for multi-dimensional friction. Subsequently, we derive a reduced planar friction model, coupled with a pre-calculated limit surface, that offers reduced computational cost. The limit surface approximation through an ellipsoid is discussed. The properties of the planar friction models are assessed in various simulations, demonstrating that the reduced planar friction model achieves comparable performance to the distributed model while exhibiting ~80 times lower computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01123v3</guid>
      <category>eess.SY</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Arslan Waltersson, Yiannis Karayiannidis</dc:creator>
    </item>
    <item>
      <title>A Systematic Literature Review of Computer Vision Applications in Robotized Wire Harness Assembly</title>
      <link>https://arxiv.org/abs/2309.13744</link>
      <description>arXiv:2309.13744v3 Announce Type: replace-cross 
Abstract: This article provides a systematic literature review of computer vision applications in robotized wire harness assembly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13744v3</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.aei.2024.102596</arxiv:DOI>
      <dc:creator>Hao Wang, Omkar Salunkhe, Walter Quadrini, Dan L\"amkull, Fredrik Ore, M\'elanie Despeisse, Luca Fumagalli, Johan Stahre, Bj\"orn Johansson</dc:creator>
    </item>
    <item>
      <title>No Compromise in Solution Quality: Speeding Up Belief-dependent Continuous POMDPs via Adaptive Multilevel Simplification</title>
      <link>https://arxiv.org/abs/2310.10274</link>
      <description>arXiv:2310.10274v2 Announce Type: replace-cross 
Abstract: Continuous POMDPs with general belief-dependent rewards are notoriously difficult to solve online. In this paper, we present a complete provable theory of adaptive multilevel simplification for the setting of a given externally constructed belief tree and MCTS that constructs the belief tree on the fly using an exploration technique. Our theory allows to accelerate POMDP planning with belief-dependent rewards without any sacrifice in the quality of the obtained solution. We rigorously prove each theoretical claim in the proposed unified theory. Using the general theoretical results, we present three algorithms to accelerate continuous POMDP online planning with belief-dependent rewards. Our two algorithms, SITH-BSP and LAZY-SITH-BSP, can be utilized on top of any method that constructs a belief tree externally. The third algorithm, SITH-PFT, is an anytime MCTS method that permits to plug-in any exploration technique. All our methods are guaranteed to return exactly the same optimal action as their unsimplified equivalents. We replace the costly computation of information-theoretic rewards with novel adaptive upper and lower bounds which we derive in this paper, and are of independent interest. We show that they are easy to calculate and can be tightened by the demand of our algorithms. Our approach is general; namely, any bounds that monotonically converge to the reward can be utilized to achieve significant speedup without any loss in performance. Our theory and algorithms support the challenging setting of continuous states, actions, and observations. The beliefs can be parametric or general and represented by weighted particles. We demonstrate in simulation a significant speedup in planning compared to baseline approaches with guaranteed identical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.10274v2</guid>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrey Zhitnikov, Ori Sztyglic, Vadim Indelman</dc:creator>
    </item>
    <item>
      <title>EVI-SAM: Robust, Real-time, Tightly-coupled Event-Visual-Inertial State Estimation and 3D Dense Mapping</title>
      <link>https://arxiv.org/abs/2312.11911</link>
      <description>arXiv:2312.11911v3 Announce Type: replace-cross 
Abstract: Event cameras are bio-inspired, motion-activated sensors that demonstrate substantial potential in handling challenging situations, such as motion blur and high-dynamic range. In this paper, we proposed EVI-SAM to tackle the problem of 6 DoF pose tracking and 3D reconstruction using monocular event camera. A novel event-based hybrid tracking framework is designed to estimate the pose, leveraging the robustness of feature matching and the precision of direct alignment. Specifically, we develop an event-based 2D-2D alignment to construct the photometric constraint, and tightly integrate it with the event-based reprojection constraint. The mapping module recovers the dense and colorful depth of the scene through the image-guided event-based mapping method. Subsequently, the appearance, texture, and surface mesh of the 3D scene can be reconstructed by fusing the dense depth map from multiple viewpoints using truncated signed distance function (TSDF) fusion. To the best of our knowledge, this is the first non-learning work to realize event-based dense mapping. Numerical evaluations are performed on both publicly available and self-collected datasets, which qualitatively and quantitatively demonstrate the superior performance of our method. Our EVI-SAM effectively balances accuracy and robustness while maintaining computational efficiency, showcasing superior pose tracking and dense mapping performance in challenging scenarios. Video Demo: https://youtu.be/Nn40U4e5Si8.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.11911v3</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weipeng Guan, Peiyu Chen, Huibin Zhao, Yu Wang, Peng Lu</dc:creator>
    </item>
    <item>
      <title>MonoLSS: Learnable Sample Selection For Monocular 3D Detection</title>
      <link>https://arxiv.org/abs/2312.14474</link>
      <description>arXiv:2312.14474v2 Announce Type: replace-cross 
Abstract: In the field of autonomous driving, monocular 3D detection is a critical task which estimates 3D properties (depth, dimension, and orientation) of objects in a single RGB image. Previous works have used features in a heuristic way to learn 3D properties, without considering that inappropriate features could have adverse effects. In this paper, sample selection is introduced that only suitable samples should be trained to regress the 3D properties. To select samples adaptively, we propose a Learnable Sample Selection (LSS) module, which is based on Gumbel-Softmax and a relative-distance sample divider. The LSS module works under a warm-up strategy leading to an improvement in training stability. Additionally, since the LSS module dedicated to 3D property sample selection relies on object-level features, we further develop a data augmentation method named MixUp3D to enrich 3D property samples which conforms to imaging principles without introducing ambiguity. As two orthogonal methods, the LSS module and MixUp3D can be utilized independently or in conjunction. Sufficient experiments have shown that their combined use can lead to synergistic effects, yielding improvements that transcend the mere sum of their individual applications. Leveraging the LSS module and the MixUp3D, without any extra data, our method named MonoLSS ranks 1st in all three categories (Car, Cyclist, and Pedestrian) on KITTI 3D object detection benchmark, and achieves competitive results on both the Waymo dataset and KITTI-nuScenes cross-dataset evaluation. The code is included in the supplementary material and will be released to facilitate related academic and industrial studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14474v2</guid>
      <category>cs.CV</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenjia Li, Jinrang Jia, Yifeng Shi</dc:creator>
    </item>
    <item>
      <title>Don't Start from Scratch: Behavioral Refinement via Interpolant-based Policy Diffusion</title>
      <link>https://arxiv.org/abs/2402.16075</link>
      <description>arXiv:2402.16075v3 Announce Type: replace-cross 
Abstract: Imitation learning empowers artificial agents to mimic behavior by learning from demonstrations. Recently, diffusion models, which have the ability to model high-dimensional and multimodal distributions, have shown impressive performance on imitation learning tasks. These models learn to shape a policy by diffusing actions (or states) from standard Gaussian noise. However, the target policy to be learned is often significantly different from Gaussian and this mismatch can result in poor performance when using a small number of diffusion steps (to improve inference speed) and under limited data. The key idea in this work is that initiating from a more informative source than Gaussian enables diffusion methods to mitigate the above limitations. We contribute both theoretical results, a new method, and empirical findings that show the benefits of using an informative source policy. Our method, which we call BRIDGER, leverages the stochastic interpolants framework to bridge arbitrary policies, thus enabling a flexible approach towards imitation learning. It generalizes prior work in that standard Gaussians can still be applied, but other source policies can be used if available. In experiments on challenging simulation benchmarks and on real robots, BRIDGER outperforms state-of-the-art diffusion policies. We provide further analysis on design considerations when applying BRIDGER. https://clear-nus.github.io/blog/bridger</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16075v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Kaiqi Chen, Eugene Lim, Kelvin Lin, Yiyang Chen, Harold Soh</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification Metrics for Deep Regression</title>
      <link>https://arxiv.org/abs/2405.04278</link>
      <description>arXiv:2405.04278v3 Announce Type: replace-cross 
Abstract: When deploying deep neural networks on robots or other physical systems, the learned model should reliably quantify predictive uncertainty. A reliable uncertainty allows downstream modules to reason about the safety of its actions. In this work, we address metrics for evaluating such an uncertainty. Specifically, we focus on regression tasks, and investigate Area Under Sparsification Error (AUSE), Calibration Error, Spearman's Rank Correlation, and Negative Log-Likelihood (NLL). Using synthetic regression datasets, we look into how those metrics behave under four typical types of uncertainty, their stability regarding the size of the test set, and reveal their strengths and weaknesses. Our results indicate that Calibration Error is the most stable and interpretable metric, but AUSE and NLL also have their respective use cases. We discourage the usage of Spearman's Rank Correlation for evaluating uncertainties and recommend replacing it with AUSE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04278v3</guid>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Kristoffersson Lind, Ziliang Xiong, Per-Erik Forss\'en, Volker Kr\"uger</dc:creator>
    </item>
  </channel>
</rss>
